2024-02-06 16:29:25,613 Hello! This is Joey-NMT.
2024-02-06 16:29:25,622 Total params: 25642504
2024-02-06 16:29:25,623 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'gloss_output_layer.bias', 'gloss_output_layer.weight', 'sgn_embed.ln.bias', 'sgn_embed.ln.weight', 'sgn_embed.norm.norm.bias', 'sgn_embed.norm.norm.weight', 'txt_embed.norm.norm.bias', 'txt_embed.norm.norm.weight']
2024-02-06 16:29:26,845 cfg.name                           : sign_experiment
2024-02-06 16:29:26,846 cfg.data.data_path                 : ./data/Sports_dataset/8/
2024-02-06 16:29:26,846 cfg.data.version                   : phoenix_2014_trans
2024-02-06 16:29:26,846 cfg.data.sgn                       : sign
2024-02-06 16:29:26,846 cfg.data.txt                       : text
2024-02-06 16:29:26,846 cfg.data.gls                       : gloss
2024-02-06 16:29:26,846 cfg.data.train                     : excel_data.train
2024-02-06 16:29:26,846 cfg.data.dev                       : excel_data.dev
2024-02-06 16:29:26,846 cfg.data.test                      : excel_data.test
2024-02-06 16:29:26,846 cfg.data.feature_size              : 2560
2024-02-06 16:29:26,847 cfg.data.level                     : word
2024-02-06 16:29:26,847 cfg.data.txt_lowercase             : True
2024-02-06 16:29:26,847 cfg.data.max_sent_length           : 500
2024-02-06 16:29:26,847 cfg.data.random_train_subset       : -1
2024-02-06 16:29:26,847 cfg.data.random_dev_subset         : -1
2024-02-06 16:29:26,847 cfg.testing.recognition_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-06 16:29:26,847 cfg.testing.translation_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-06 16:29:26,847 cfg.testing.translation_beam_alphas : [-1, 0, 1, 2, 3, 4, 5]
2024-02-06 16:29:26,848 cfg.training.reset_best_ckpt       : False
2024-02-06 16:29:26,848 cfg.training.reset_scheduler       : False
2024-02-06 16:29:26,848 cfg.training.reset_optimizer       : False
2024-02-06 16:29:26,848 cfg.training.random_seed           : 42
2024-02-06 16:29:26,848 cfg.training.model_dir             : ./sign_sample_model/fold8/32head/256batch
2024-02-06 16:29:26,848 cfg.training.recognition_loss_weight : 1.0
2024-02-06 16:29:26,848 cfg.training.translation_loss_weight : 1.0
2024-02-06 16:29:26,848 cfg.training.eval_metric           : bleu
2024-02-06 16:29:26,849 cfg.training.optimizer             : adam
2024-02-06 16:29:26,849 cfg.training.learning_rate         : 0.0001
2024-02-06 16:29:26,849 cfg.training.batch_size            : 256
2024-02-06 16:29:26,849 cfg.training.num_valid_log         : 5
2024-02-06 16:29:26,849 cfg.training.epochs                : 50000
2024-02-06 16:29:26,849 cfg.training.early_stopping_metric : eval_metric
2024-02-06 16:29:26,849 cfg.training.batch_type            : sentence
2024-02-06 16:29:26,849 cfg.training.translation_normalization : batch
2024-02-06 16:29:26,849 cfg.training.eval_recognition_beam_size : 1
2024-02-06 16:29:26,850 cfg.training.eval_translation_beam_size : 1
2024-02-06 16:29:26,850 cfg.training.eval_translation_beam_alpha : -1
2024-02-06 16:29:26,850 cfg.training.overwrite             : True
2024-02-06 16:29:26,850 cfg.training.shuffle               : True
2024-02-06 16:29:26,850 cfg.training.use_cuda              : True
2024-02-06 16:29:26,850 cfg.training.translation_max_output_length : 40
2024-02-06 16:29:26,850 cfg.training.keep_last_ckpts       : 1
2024-02-06 16:29:26,850 cfg.training.batch_multiplier      : 1
2024-02-06 16:29:26,850 cfg.training.logging_freq          : 100
2024-02-06 16:29:26,851 cfg.training.validation_freq       : 2000
2024-02-06 16:29:26,851 cfg.training.betas                 : [0.9, 0.998]
2024-02-06 16:29:26,851 cfg.training.scheduling            : plateau
2024-02-06 16:29:26,851 cfg.training.learning_rate_min     : 1e-08
2024-02-06 16:29:26,851 cfg.training.weight_decay          : 0.0001
2024-02-06 16:29:26,851 cfg.training.patience              : 12
2024-02-06 16:29:26,851 cfg.training.decrease_factor       : 0.5
2024-02-06 16:29:26,851 cfg.training.label_smoothing       : 0.0
2024-02-06 16:29:26,852 cfg.model.initializer              : xavier
2024-02-06 16:29:26,852 cfg.model.bias_initializer         : zeros
2024-02-06 16:29:26,852 cfg.model.init_gain                : 1.0
2024-02-06 16:29:26,852 cfg.model.embed_initializer        : xavier
2024-02-06 16:29:26,852 cfg.model.embed_init_gain          : 1.0
2024-02-06 16:29:26,852 cfg.model.tied_softmax             : True
2024-02-06 16:29:26,852 cfg.model.encoder.type             : transformer
2024-02-06 16:29:26,852 cfg.model.encoder.num_layers       : 3
2024-02-06 16:29:26,853 cfg.model.encoder.num_heads        : 32
2024-02-06 16:29:26,853 cfg.model.encoder.embeddings.embedding_dim : 512
2024-02-06 16:29:26,853 cfg.model.encoder.embeddings.scale : False
2024-02-06 16:29:26,853 cfg.model.encoder.embeddings.dropout : 0.1
2024-02-06 16:29:26,853 cfg.model.encoder.embeddings.norm_type : batch
2024-02-06 16:29:26,853 cfg.model.encoder.embeddings.activation_type : softsign
2024-02-06 16:29:26,853 cfg.model.encoder.hidden_size      : 512
2024-02-06 16:29:26,853 cfg.model.encoder.ff_size          : 2048
2024-02-06 16:29:26,854 cfg.model.encoder.dropout          : 0.1
2024-02-06 16:29:26,854 cfg.model.decoder.type             : transformer
2024-02-06 16:29:26,854 cfg.model.decoder.num_layers       : 3
2024-02-06 16:29:26,854 cfg.model.decoder.num_heads        : 32
2024-02-06 16:29:26,854 cfg.model.decoder.embeddings.embedding_dim : 512
2024-02-06 16:29:26,854 cfg.model.decoder.embeddings.scale : False
2024-02-06 16:29:26,854 cfg.model.decoder.embeddings.dropout : 0.1
2024-02-06 16:29:26,854 cfg.model.decoder.embeddings.norm_type : batch
2024-02-06 16:29:26,854 cfg.model.decoder.embeddings.activation_type : softsign
2024-02-06 16:29:26,855 cfg.model.decoder.hidden_size      : 512
2024-02-06 16:29:26,855 cfg.model.decoder.ff_size          : 2048
2024-02-06 16:29:26,855 cfg.model.decoder.dropout          : 0.1
2024-02-06 16:29:26,855 Data set sizes: 
	train 2124,
	valid 708,
	test 708
2024-02-06 16:29:26,855 First training example:
	[GLS] A B C D E
	[TXT] how did she become a champion
2024-02-06 16:29:26,855 First 10 words (gls): (0) <si> (1) <unk> (2) <pad> (3) A (4) B (5) C (6) D (7) E
2024-02-06 16:29:26,855 First 10 words (txt): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) and (6) to (7) in (8) a (9) of
2024-02-06 16:29:26,855 Number of unique glosses (types): 8
2024-02-06 16:29:26,856 Number of unique words (types): 4402
2024-02-06 16:29:26,856 SignModel(
	encoder=TransformerEncoder(num_layers=3, num_heads=32),
	decoder=TransformerDecoder(num_layers=3, num_heads=32),
	sgn_embed=SpatialEmbeddings(embedding_dim=512, input_size=2560),
	txt_embed=Embeddings(embedding_dim=512, vocab_size=4402))
2024-02-06 16:29:26,859 EPOCH 1
2024-02-06 16:29:50,940 Epoch   1: Total Training Recognition Loss 84.41  Total Training Translation Loss 994.47 
2024-02-06 16:29:50,941 EPOCH 2
2024-02-06 16:30:12,162 Epoch   2: Total Training Recognition Loss 41.38  Total Training Translation Loss 919.63 
2024-02-06 16:30:12,162 EPOCH 3
2024-02-06 16:30:28,841 Epoch   3: Total Training Recognition Loss 35.91  Total Training Translation Loss 882.69 
2024-02-06 16:30:28,842 EPOCH 4
2024-02-06 16:30:45,684 Epoch   4: Total Training Recognition Loss 24.37  Total Training Translation Loss 853.91 
2024-02-06 16:30:45,684 EPOCH 5
2024-02-06 16:31:02,024 Epoch   5: Total Training Recognition Loss 18.90  Total Training Translation Loss 833.48 
2024-02-06 16:31:02,024 EPOCH 6
2024-02-06 16:31:18,479 Epoch   6: Total Training Recognition Loss 15.62  Total Training Translation Loss 820.54 
2024-02-06 16:31:18,479 EPOCH 7
2024-02-06 16:31:34,714 Epoch   7: Total Training Recognition Loss 13.45  Total Training Translation Loss 812.37 
2024-02-06 16:31:34,715 EPOCH 8
2024-02-06 16:31:51,442 Epoch   8: Total Training Recognition Loss 12.01  Total Training Translation Loss 807.51 
2024-02-06 16:31:51,443 EPOCH 9
2024-02-06 16:32:07,511 Epoch   9: Total Training Recognition Loss 10.75  Total Training Translation Loss 804.43 
2024-02-06 16:32:07,512 EPOCH 10
2024-02-06 16:32:23,996 Epoch  10: Total Training Recognition Loss 9.78  Total Training Translation Loss 801.18 
2024-02-06 16:32:23,997 EPOCH 11
2024-02-06 16:32:40,428 Epoch  11: Total Training Recognition Loss 9.01  Total Training Translation Loss 796.29 
2024-02-06 16:32:40,429 EPOCH 12
2024-02-06 16:32:46,620 [Epoch: 012 Step: 00000100] Batch Recognition Loss:   1.139669 => Gls Tokens per Sec:      207 || Batch Translation Loss: 110.594482 => Txt Tokens per Sec:      712 || Lr: 0.000100
2024-02-06 16:32:56,942 Epoch  12: Total Training Recognition Loss 8.24  Total Training Translation Loss 791.81 
2024-02-06 16:32:56,943 EPOCH 13
2024-02-06 16:33:13,165 Epoch  13: Total Training Recognition Loss 7.86  Total Training Translation Loss 784.89 
2024-02-06 16:33:13,166 EPOCH 14
2024-02-06 16:33:29,382 Epoch  14: Total Training Recognition Loss 7.22  Total Training Translation Loss 779.03 
2024-02-06 16:33:29,382 EPOCH 15
2024-02-06 16:33:45,804 Epoch  15: Total Training Recognition Loss 6.84  Total Training Translation Loss 770.49 
2024-02-06 16:33:45,804 EPOCH 16
2024-02-06 16:34:02,432 Epoch  16: Total Training Recognition Loss 6.39  Total Training Translation Loss 761.45 
2024-02-06 16:34:02,433 EPOCH 17
2024-02-06 16:34:18,842 Epoch  17: Total Training Recognition Loss 6.76  Total Training Translation Loss 753.10 
2024-02-06 16:34:18,843 EPOCH 18
2024-02-06 16:34:35,131 Epoch  18: Total Training Recognition Loss 6.53  Total Training Translation Loss 741.94 
2024-02-06 16:34:35,131 EPOCH 19
2024-02-06 16:34:51,551 Epoch  19: Total Training Recognition Loss 5.96  Total Training Translation Loss 733.82 
2024-02-06 16:34:51,551 EPOCH 20
2024-02-06 16:35:07,911 Epoch  20: Total Training Recognition Loss 5.97  Total Training Translation Loss 719.31 
2024-02-06 16:35:07,912 EPOCH 21
2024-02-06 16:35:24,052 Epoch  21: Total Training Recognition Loss 5.36  Total Training Translation Loss 707.77 
2024-02-06 16:35:24,052 EPOCH 22
2024-02-06 16:35:40,619 Epoch  22: Total Training Recognition Loss 5.47  Total Training Translation Loss 697.00 
2024-02-06 16:35:40,620 EPOCH 23
2024-02-06 16:35:47,286 [Epoch: 023 Step: 00000200] Batch Recognition Loss:   0.746001 => Gls Tokens per Sec:      384 || Batch Translation Loss:  80.147163 => Txt Tokens per Sec:     1217 || Lr: 0.000100
2024-02-06 16:35:56,864 Epoch  23: Total Training Recognition Loss 6.29  Total Training Translation Loss 686.00 
2024-02-06 16:35:56,865 EPOCH 24
2024-02-06 16:36:13,366 Epoch  24: Total Training Recognition Loss 10.51  Total Training Translation Loss 677.77 
2024-02-06 16:36:13,367 EPOCH 25
2024-02-06 16:36:29,482 Epoch  25: Total Training Recognition Loss 6.87  Total Training Translation Loss 666.14 
2024-02-06 16:36:29,483 EPOCH 26
2024-02-06 16:36:45,621 Epoch  26: Total Training Recognition Loss 5.91  Total Training Translation Loss 659.56 
2024-02-06 16:36:45,622 EPOCH 27
2024-02-06 16:37:02,180 Epoch  27: Total Training Recognition Loss 6.49  Total Training Translation Loss 650.37 
2024-02-06 16:37:02,181 EPOCH 28
2024-02-06 16:37:18,305 Epoch  28: Total Training Recognition Loss 4.60  Total Training Translation Loss 640.70 
2024-02-06 16:37:18,306 EPOCH 29
2024-02-06 16:37:34,526 Epoch  29: Total Training Recognition Loss 4.07  Total Training Translation Loss 634.41 
2024-02-06 16:37:34,527 EPOCH 30
2024-02-06 16:37:50,656 Epoch  30: Total Training Recognition Loss 4.08  Total Training Translation Loss 626.10 
2024-02-06 16:37:50,657 EPOCH 31
2024-02-06 16:38:07,017 Epoch  31: Total Training Recognition Loss 3.61  Total Training Translation Loss 614.22 
2024-02-06 16:38:07,017 EPOCH 32
2024-02-06 16:38:23,257 Epoch  32: Total Training Recognition Loss 3.52  Total Training Translation Loss 602.68 
2024-02-06 16:38:23,258 EPOCH 33
2024-02-06 16:38:39,408 Epoch  33: Total Training Recognition Loss 3.31  Total Training Translation Loss 593.55 
2024-02-06 16:38:39,408 EPOCH 34
2024-02-06 16:38:48,954 [Epoch: 034 Step: 00000300] Batch Recognition Loss:   0.623495 => Gls Tokens per Sec:      402 || Batch Translation Loss:  84.808983 => Txt Tokens per Sec:     1260 || Lr: 0.000100
2024-02-06 16:38:55,616 Epoch  34: Total Training Recognition Loss 3.40  Total Training Translation Loss 590.37 
2024-02-06 16:38:55,617 EPOCH 35
2024-02-06 16:39:11,898 Epoch  35: Total Training Recognition Loss 3.25  Total Training Translation Loss 584.03 
2024-02-06 16:39:11,899 EPOCH 36
2024-02-06 16:39:28,078 Epoch  36: Total Training Recognition Loss 3.04  Total Training Translation Loss 573.17 
2024-02-06 16:39:28,079 EPOCH 37
2024-02-06 16:39:44,272 Epoch  37: Total Training Recognition Loss 2.80  Total Training Translation Loss 563.28 
2024-02-06 16:39:44,272 EPOCH 38
2024-02-06 16:40:00,480 Epoch  38: Total Training Recognition Loss 2.72  Total Training Translation Loss 558.74 
2024-02-06 16:40:00,481 EPOCH 39
2024-02-06 16:40:16,628 Epoch  39: Total Training Recognition Loss 2.63  Total Training Translation Loss 558.25 
2024-02-06 16:40:16,629 EPOCH 40
2024-02-06 16:40:33,023 Epoch  40: Total Training Recognition Loss 2.61  Total Training Translation Loss 547.81 
2024-02-06 16:40:33,023 EPOCH 41
2024-02-06 16:40:49,269 Epoch  41: Total Training Recognition Loss 2.53  Total Training Translation Loss 537.88 
2024-02-06 16:40:49,270 EPOCH 42
2024-02-06 16:41:05,712 Epoch  42: Total Training Recognition Loss 2.32  Total Training Translation Loss 525.38 
2024-02-06 16:41:05,713 EPOCH 43
2024-02-06 16:41:21,728 Epoch  43: Total Training Recognition Loss 2.32  Total Training Translation Loss 519.81 
2024-02-06 16:41:21,729 EPOCH 44
2024-02-06 16:41:38,288 Epoch  44: Total Training Recognition Loss 2.23  Total Training Translation Loss 510.33 
2024-02-06 16:41:38,288 EPOCH 45
2024-02-06 16:41:49,491 [Epoch: 045 Step: 00000400] Batch Recognition Loss:   0.458852 => Gls Tokens per Sec:      377 || Batch Translation Loss:  73.955307 => Txt Tokens per Sec:     1160 || Lr: 0.000100
2024-02-06 16:41:54,540 Epoch  45: Total Training Recognition Loss 2.13  Total Training Translation Loss 502.51 
2024-02-06 16:41:54,541 EPOCH 46
2024-02-06 16:42:11,252 Epoch  46: Total Training Recognition Loss 2.04  Total Training Translation Loss 493.19 
2024-02-06 16:42:11,253 EPOCH 47
2024-02-06 16:42:27,428 Epoch  47: Total Training Recognition Loss 1.87  Total Training Translation Loss 485.99 
2024-02-06 16:42:27,429 EPOCH 48
2024-02-06 16:42:43,961 Epoch  48: Total Training Recognition Loss 1.82  Total Training Translation Loss 482.57 
2024-02-06 16:42:43,962 EPOCH 49
2024-02-06 16:43:00,376 Epoch  49: Total Training Recognition Loss 1.79  Total Training Translation Loss 472.81 
2024-02-06 16:43:00,376 EPOCH 50
2024-02-06 16:43:16,846 Epoch  50: Total Training Recognition Loss 1.77  Total Training Translation Loss 468.16 
2024-02-06 16:43:16,846 EPOCH 51
2024-02-06 16:43:33,177 Epoch  51: Total Training Recognition Loss 1.76  Total Training Translation Loss 463.58 
2024-02-06 16:43:33,178 EPOCH 52
2024-02-06 16:43:49,505 Epoch  52: Total Training Recognition Loss 1.78  Total Training Translation Loss 461.15 
2024-02-06 16:43:49,506 EPOCH 53
2024-02-06 16:44:05,754 Epoch  53: Total Training Recognition Loss 1.80  Total Training Translation Loss 451.32 
2024-02-06 16:44:05,755 EPOCH 54
2024-02-06 16:44:22,213 Epoch  54: Total Training Recognition Loss 1.66  Total Training Translation Loss 443.69 
2024-02-06 16:44:22,213 EPOCH 55
2024-02-06 16:44:38,360 Epoch  55: Total Training Recognition Loss 1.59  Total Training Translation Loss 435.70 
2024-02-06 16:44:38,360 EPOCH 56
2024-02-06 16:44:49,130 [Epoch: 056 Step: 00000500] Batch Recognition Loss:   0.166601 => Gls Tokens per Sec:      594 || Batch Translation Loss:  54.220665 => Txt Tokens per Sec:     1798 || Lr: 0.000100
2024-02-06 16:44:54,489 Epoch  56: Total Training Recognition Loss 1.56  Total Training Translation Loss 429.66 
2024-02-06 16:44:54,489 EPOCH 57
2024-02-06 16:45:10,834 Epoch  57: Total Training Recognition Loss 1.56  Total Training Translation Loss 427.60 
2024-02-06 16:45:10,834 EPOCH 58
2024-02-06 16:45:26,930 Epoch  58: Total Training Recognition Loss 1.59  Total Training Translation Loss 424.86 
2024-02-06 16:45:26,931 EPOCH 59
2024-02-06 16:45:43,202 Epoch  59: Total Training Recognition Loss 1.49  Total Training Translation Loss 421.20 
2024-02-06 16:45:43,203 EPOCH 60
2024-02-06 16:45:59,671 Epoch  60: Total Training Recognition Loss 1.51  Total Training Translation Loss 413.37 
2024-02-06 16:45:59,672 EPOCH 61
2024-02-06 16:46:16,019 Epoch  61: Total Training Recognition Loss 1.48  Total Training Translation Loss 408.74 
2024-02-06 16:46:16,020 EPOCH 62
2024-02-06 16:46:32,319 Epoch  62: Total Training Recognition Loss 1.48  Total Training Translation Loss 414.64 
2024-02-06 16:46:32,320 EPOCH 63
2024-02-06 16:46:48,378 Epoch  63: Total Training Recognition Loss 1.42  Total Training Translation Loss 398.54 
2024-02-06 16:46:48,379 EPOCH 64
2024-02-06 16:47:04,503 Epoch  64: Total Training Recognition Loss 1.43  Total Training Translation Loss 389.62 
2024-02-06 16:47:04,503 EPOCH 65
2024-02-06 16:47:20,408 Epoch  65: Total Training Recognition Loss 1.34  Total Training Translation Loss 379.86 
2024-02-06 16:47:20,409 EPOCH 66
2024-02-06 16:47:37,009 Epoch  66: Total Training Recognition Loss 1.33  Total Training Translation Loss 372.64 
2024-02-06 16:47:37,009 EPOCH 67
2024-02-06 16:47:49,324 [Epoch: 067 Step: 00000600] Batch Recognition Loss:   0.269614 => Gls Tokens per Sec:      551 || Batch Translation Loss:  55.315395 => Txt Tokens per Sec:     1555 || Lr: 0.000100
2024-02-06 16:47:53,229 Epoch  67: Total Training Recognition Loss 1.28  Total Training Translation Loss 366.04 
2024-02-06 16:47:53,230 EPOCH 68
2024-02-06 16:48:09,826 Epoch  68: Total Training Recognition Loss 1.22  Total Training Translation Loss 358.34 
2024-02-06 16:48:09,826 EPOCH 69
2024-02-06 16:48:26,086 Epoch  69: Total Training Recognition Loss 1.16  Total Training Translation Loss 352.11 
2024-02-06 16:48:26,087 EPOCH 70
2024-02-06 16:48:42,523 Epoch  70: Total Training Recognition Loss 1.21  Total Training Translation Loss 347.73 
2024-02-06 16:48:42,523 EPOCH 71
2024-02-06 16:48:58,653 Epoch  71: Total Training Recognition Loss 1.19  Total Training Translation Loss 344.75 
2024-02-06 16:48:58,654 EPOCH 72
2024-02-06 16:49:15,071 Epoch  72: Total Training Recognition Loss 1.17  Total Training Translation Loss 341.38 
2024-02-06 16:49:15,071 EPOCH 73
2024-02-06 16:49:31,117 Epoch  73: Total Training Recognition Loss 1.16  Total Training Translation Loss 335.35 
2024-02-06 16:49:31,118 EPOCH 74
2024-02-06 16:49:47,437 Epoch  74: Total Training Recognition Loss 1.16  Total Training Translation Loss 331.77 
2024-02-06 16:49:47,437 EPOCH 75
2024-02-06 16:50:03,584 Epoch  75: Total Training Recognition Loss 1.12  Total Training Translation Loss 325.27 
2024-02-06 16:50:03,584 EPOCH 76
2024-02-06 16:50:19,764 Epoch  76: Total Training Recognition Loss 1.12  Total Training Translation Loss 317.44 
2024-02-06 16:50:19,764 EPOCH 77
2024-02-06 16:50:35,946 Epoch  77: Total Training Recognition Loss 1.12  Total Training Translation Loss 310.08 
2024-02-06 16:50:35,946 EPOCH 78
2024-02-06 16:50:45,011 [Epoch: 078 Step: 00000700] Batch Recognition Loss:   0.144842 => Gls Tokens per Sec:      889 || Batch Translation Loss:  42.606747 => Txt Tokens per Sec:     2351 || Lr: 0.000100
2024-02-06 16:50:51,580 Epoch  78: Total Training Recognition Loss 1.04  Total Training Translation Loss 303.19 
2024-02-06 16:50:51,581 EPOCH 79
2024-02-06 16:51:08,059 Epoch  79: Total Training Recognition Loss 1.10  Total Training Translation Loss 299.83 
2024-02-06 16:51:08,060 EPOCH 80
2024-02-06 16:51:24,167 Epoch  80: Total Training Recognition Loss 1.01  Total Training Translation Loss 294.63 
2024-02-06 16:51:24,168 EPOCH 81
2024-02-06 16:51:40,789 Epoch  81: Total Training Recognition Loss 1.07  Total Training Translation Loss 291.59 
2024-02-06 16:51:40,790 EPOCH 82
2024-02-06 16:51:57,144 Epoch  82: Total Training Recognition Loss 1.03  Total Training Translation Loss 291.35 
2024-02-06 16:51:57,145 EPOCH 83
2024-02-06 16:52:13,588 Epoch  83: Total Training Recognition Loss 1.13  Total Training Translation Loss 301.81 
2024-02-06 16:52:13,588 EPOCH 84
2024-02-06 16:52:29,810 Epoch  84: Total Training Recognition Loss 1.11  Total Training Translation Loss 292.18 
2024-02-06 16:52:29,811 EPOCH 85
2024-02-06 16:52:45,847 Epoch  85: Total Training Recognition Loss 1.11  Total Training Translation Loss 282.87 
2024-02-06 16:52:45,847 EPOCH 86
2024-02-06 16:53:02,179 Epoch  86: Total Training Recognition Loss 1.05  Total Training Translation Loss 272.97 
2024-02-06 16:53:02,180 EPOCH 87
2024-02-06 16:53:18,152 Epoch  87: Total Training Recognition Loss 1.05  Total Training Translation Loss 267.34 
2024-02-06 16:53:18,153 EPOCH 88
2024-02-06 16:53:34,660 Epoch  88: Total Training Recognition Loss 1.06  Total Training Translation Loss 263.15 
2024-02-06 16:53:34,661 EPOCH 89
2024-02-06 16:53:50,468 [Epoch: 089 Step: 00000800] Batch Recognition Loss:   0.121871 => Gls Tokens per Sec:      591 || Batch Translation Loss:  15.676558 => Txt Tokens per Sec:     1713 || Lr: 0.000100
2024-02-06 16:53:50,798 Epoch  89: Total Training Recognition Loss 0.96  Total Training Translation Loss 265.12 
2024-02-06 16:53:50,799 EPOCH 90
2024-02-06 16:54:07,063 Epoch  90: Total Training Recognition Loss 1.02  Total Training Translation Loss 259.54 
2024-02-06 16:54:07,063 EPOCH 91
2024-02-06 16:54:23,234 Epoch  91: Total Training Recognition Loss 0.96  Total Training Translation Loss 251.60 
2024-02-06 16:54:23,235 EPOCH 92
2024-02-06 16:54:39,749 Epoch  92: Total Training Recognition Loss 0.96  Total Training Translation Loss 246.71 
2024-02-06 16:54:39,749 EPOCH 93
2024-02-06 16:54:56,123 Epoch  93: Total Training Recognition Loss 0.98  Total Training Translation Loss 243.40 
2024-02-06 16:54:56,123 EPOCH 94
2024-02-06 16:55:12,441 Epoch  94: Total Training Recognition Loss 0.96  Total Training Translation Loss 239.18 
2024-02-06 16:55:12,442 EPOCH 95
2024-02-06 16:55:28,626 Epoch  95: Total Training Recognition Loss 1.00  Total Training Translation Loss 241.72 
2024-02-06 16:55:28,626 EPOCH 96
2024-02-06 16:55:44,961 Epoch  96: Total Training Recognition Loss 1.02  Total Training Translation Loss 233.29 
2024-02-06 16:55:44,961 EPOCH 97
2024-02-06 16:56:01,162 Epoch  97: Total Training Recognition Loss 0.94  Total Training Translation Loss 224.78 
2024-02-06 16:56:01,162 EPOCH 98
2024-02-06 16:56:17,511 Epoch  98: Total Training Recognition Loss 0.94  Total Training Translation Loss 218.57 
2024-02-06 16:56:17,511 EPOCH 99
2024-02-06 16:56:33,569 Epoch  99: Total Training Recognition Loss 0.90  Total Training Translation Loss 217.38 
2024-02-06 16:56:33,569 EPOCH 100
2024-02-06 16:56:50,051 [Epoch: 100 Step: 00000900] Batch Recognition Loss:   0.141578 => Gls Tokens per Sec:      644 || Batch Translation Loss:  10.370111 => Txt Tokens per Sec:     1783 || Lr: 0.000100
2024-02-06 16:56:50,052 Epoch 100: Total Training Recognition Loss 0.95  Total Training Translation Loss 212.26 
2024-02-06 16:56:50,052 EPOCH 101
2024-02-06 16:57:06,309 Epoch 101: Total Training Recognition Loss 0.92  Total Training Translation Loss 210.27 
2024-02-06 16:57:06,309 EPOCH 102
2024-02-06 16:57:22,832 Epoch 102: Total Training Recognition Loss 0.91  Total Training Translation Loss 205.30 
2024-02-06 16:57:22,833 EPOCH 103
2024-02-06 16:57:38,980 Epoch 103: Total Training Recognition Loss 0.93  Total Training Translation Loss 202.93 
2024-02-06 16:57:38,981 EPOCH 104
2024-02-06 16:57:55,140 Epoch 104: Total Training Recognition Loss 0.87  Total Training Translation Loss 195.66 
2024-02-06 16:57:55,140 EPOCH 105
2024-02-06 16:58:11,271 Epoch 105: Total Training Recognition Loss 0.88  Total Training Translation Loss 191.13 
2024-02-06 16:58:11,272 EPOCH 106
2024-02-06 16:58:27,745 Epoch 106: Total Training Recognition Loss 0.83  Total Training Translation Loss 185.40 
2024-02-06 16:58:27,746 EPOCH 107
2024-02-06 16:58:43,769 Epoch 107: Total Training Recognition Loss 0.85  Total Training Translation Loss 182.99 
2024-02-06 16:58:43,770 EPOCH 108
2024-02-06 16:59:00,041 Epoch 108: Total Training Recognition Loss 0.87  Total Training Translation Loss 178.26 
2024-02-06 16:59:00,042 EPOCH 109
2024-02-06 16:59:16,090 Epoch 109: Total Training Recognition Loss 0.89  Total Training Translation Loss 178.21 
2024-02-06 16:59:16,091 EPOCH 110
2024-02-06 16:59:31,972 Epoch 110: Total Training Recognition Loss 0.88  Total Training Translation Loss 174.29 
2024-02-06 16:59:31,972 EPOCH 111
2024-02-06 16:59:48,434 Epoch 111: Total Training Recognition Loss 0.84  Total Training Translation Loss 168.74 
2024-02-06 16:59:48,434 EPOCH 112
2024-02-06 16:59:48,866 [Epoch: 112 Step: 00001000] Batch Recognition Loss:   0.061712 => Gls Tokens per Sec:     2970 || Batch Translation Loss:  20.029348 => Txt Tokens per Sec:     8183 || Lr: 0.000100
2024-02-06 17:00:04,702 Epoch 112: Total Training Recognition Loss 0.86  Total Training Translation Loss 164.43 
2024-02-06 17:00:04,703 EPOCH 113
2024-02-06 17:00:21,145 Epoch 113: Total Training Recognition Loss 0.83  Total Training Translation Loss 158.89 
2024-02-06 17:00:21,145 EPOCH 114
2024-02-06 17:00:37,175 Epoch 114: Total Training Recognition Loss 0.83  Total Training Translation Loss 154.09 
2024-02-06 17:00:37,176 EPOCH 115
2024-02-06 17:00:53,597 Epoch 115: Total Training Recognition Loss 0.78  Total Training Translation Loss 148.96 
2024-02-06 17:00:53,597 EPOCH 116
2024-02-06 17:01:09,973 Epoch 116: Total Training Recognition Loss 0.81  Total Training Translation Loss 145.30 
2024-02-06 17:01:09,974 EPOCH 117
2024-02-06 17:01:26,179 Epoch 117: Total Training Recognition Loss 0.77  Total Training Translation Loss 142.88 
2024-02-06 17:01:26,180 EPOCH 118
2024-02-06 17:01:42,658 Epoch 118: Total Training Recognition Loss 0.77  Total Training Translation Loss 139.02 
2024-02-06 17:01:42,659 EPOCH 119
2024-02-06 17:01:59,137 Epoch 119: Total Training Recognition Loss 0.78  Total Training Translation Loss 136.94 
2024-02-06 17:01:59,137 EPOCH 120
2024-02-06 17:02:15,264 Epoch 120: Total Training Recognition Loss 0.75  Total Training Translation Loss 134.62 
2024-02-06 17:02:15,265 EPOCH 121
2024-02-06 17:02:31,533 Epoch 121: Total Training Recognition Loss 0.75  Total Training Translation Loss 130.70 
2024-02-06 17:02:31,533 EPOCH 122
2024-02-06 17:02:47,928 Epoch 122: Total Training Recognition Loss 0.74  Total Training Translation Loss 127.83 
2024-02-06 17:02:47,928 EPOCH 123
2024-02-06 17:02:48,964 [Epoch: 123 Step: 00001100] Batch Recognition Loss:   0.078323 => Gls Tokens per Sec:     2476 || Batch Translation Loss:  11.786918 => Txt Tokens per Sec:     6185 || Lr: 0.000100
2024-02-06 17:03:04,420 Epoch 123: Total Training Recognition Loss 0.75  Total Training Translation Loss 128.54 
2024-02-06 17:03:04,420 EPOCH 124
2024-02-06 17:03:20,730 Epoch 124: Total Training Recognition Loss 0.76  Total Training Translation Loss 127.29 
2024-02-06 17:03:20,731 EPOCH 125
2024-02-06 17:03:37,199 Epoch 125: Total Training Recognition Loss 0.74  Total Training Translation Loss 123.83 
2024-02-06 17:03:37,200 EPOCH 126
2024-02-06 17:03:53,368 Epoch 126: Total Training Recognition Loss 0.77  Total Training Translation Loss 120.54 
2024-02-06 17:03:53,368 EPOCH 127
2024-02-06 17:04:09,734 Epoch 127: Total Training Recognition Loss 0.77  Total Training Translation Loss 122.78 
2024-02-06 17:04:09,734 EPOCH 128
2024-02-06 17:04:25,930 Epoch 128: Total Training Recognition Loss 0.77  Total Training Translation Loss 118.97 
2024-02-06 17:04:25,931 EPOCH 129
2024-02-06 17:04:41,833 Epoch 129: Total Training Recognition Loss 0.79  Total Training Translation Loss 113.89 
2024-02-06 17:04:41,834 EPOCH 130
2024-02-06 17:04:58,480 Epoch 130: Total Training Recognition Loss 0.74  Total Training Translation Loss 114.61 
2024-02-06 17:04:58,480 EPOCH 131
2024-02-06 17:05:18,928 Epoch 131: Total Training Recognition Loss 0.76  Total Training Translation Loss 111.63 
2024-02-06 17:05:18,928 EPOCH 132
2024-02-06 17:05:36,009 Epoch 132: Total Training Recognition Loss 0.79  Total Training Translation Loss 110.96 
2024-02-06 17:05:36,009 EPOCH 133
2024-02-06 17:05:52,563 Epoch 133: Total Training Recognition Loss 0.78  Total Training Translation Loss 104.44 
2024-02-06 17:05:52,564 EPOCH 134
2024-02-06 17:05:57,641 [Epoch: 134 Step: 00001200] Batch Recognition Loss:   0.103157 => Gls Tokens per Sec:      579 || Batch Translation Loss:   3.636260 => Txt Tokens per Sec:     1450 || Lr: 0.000100
2024-02-06 17:06:09,059 Epoch 134: Total Training Recognition Loss 0.77  Total Training Translation Loss 100.00 
2024-02-06 17:06:09,059 EPOCH 135
2024-02-06 17:06:25,511 Epoch 135: Total Training Recognition Loss 0.71  Total Training Translation Loss 96.81 
2024-02-06 17:06:25,511 EPOCH 136
2024-02-06 17:06:41,794 Epoch 136: Total Training Recognition Loss 0.73  Total Training Translation Loss 95.31 
2024-02-06 17:06:41,794 EPOCH 137
2024-02-06 17:06:58,410 Epoch 137: Total Training Recognition Loss 0.69  Total Training Translation Loss 96.92 
2024-02-06 17:06:58,410 EPOCH 138
2024-02-06 17:07:14,703 Epoch 138: Total Training Recognition Loss 0.73  Total Training Translation Loss 102.18 
2024-02-06 17:07:14,704 EPOCH 139
2024-02-06 17:07:31,234 Epoch 139: Total Training Recognition Loss 0.73  Total Training Translation Loss 98.03 
2024-02-06 17:07:31,235 EPOCH 140
2024-02-06 17:07:47,643 Epoch 140: Total Training Recognition Loss 0.70  Total Training Translation Loss 89.90 
2024-02-06 17:07:47,644 EPOCH 141
2024-02-06 17:08:03,944 Epoch 141: Total Training Recognition Loss 0.71  Total Training Translation Loss 84.43 
2024-02-06 17:08:03,944 EPOCH 142
2024-02-06 17:08:20,533 Epoch 142: Total Training Recognition Loss 0.72  Total Training Translation Loss 80.50 
2024-02-06 17:08:20,534 EPOCH 143
2024-02-06 17:08:37,028 Epoch 143: Total Training Recognition Loss 0.71  Total Training Translation Loss 77.23 
2024-02-06 17:08:37,028 EPOCH 144
2024-02-06 17:08:53,610 Epoch 144: Total Training Recognition Loss 0.65  Total Training Translation Loss 74.43 
2024-02-06 17:08:53,610 EPOCH 145
2024-02-06 17:08:55,248 [Epoch: 145 Step: 00001300] Batch Recognition Loss:   0.063095 => Gls Tokens per Sec:     3127 || Batch Translation Loss:   9.887820 => Txt Tokens per Sec:     8053 || Lr: 0.000100
2024-02-06 17:09:09,800 Epoch 145: Total Training Recognition Loss 0.62  Total Training Translation Loss 72.97 
2024-02-06 17:09:09,800 EPOCH 146
2024-02-06 17:09:26,233 Epoch 146: Total Training Recognition Loss 0.63  Total Training Translation Loss 69.76 
2024-02-06 17:09:26,233 EPOCH 147
2024-02-06 17:09:42,390 Epoch 147: Total Training Recognition Loss 0.59  Total Training Translation Loss 68.17 
2024-02-06 17:09:42,391 EPOCH 148
2024-02-06 17:09:58,937 Epoch 148: Total Training Recognition Loss 0.60  Total Training Translation Loss 68.03 
2024-02-06 17:09:58,938 EPOCH 149
2024-02-06 17:10:15,223 Epoch 149: Total Training Recognition Loss 0.60  Total Training Translation Loss 64.58 
2024-02-06 17:10:15,223 EPOCH 150
2024-02-06 17:10:31,408 Epoch 150: Total Training Recognition Loss 0.59  Total Training Translation Loss 63.16 
2024-02-06 17:10:31,408 EPOCH 151
2024-02-06 17:10:47,666 Epoch 151: Total Training Recognition Loss 0.58  Total Training Translation Loss 60.63 
2024-02-06 17:10:47,666 EPOCH 152
2024-02-06 17:11:04,353 Epoch 152: Total Training Recognition Loss 0.56  Total Training Translation Loss 59.88 
2024-02-06 17:11:04,353 EPOCH 153
2024-02-06 17:11:20,683 Epoch 153: Total Training Recognition Loss 0.55  Total Training Translation Loss 58.41 
2024-02-06 17:11:20,684 EPOCH 154
2024-02-06 17:11:37,167 Epoch 154: Total Training Recognition Loss 0.57  Total Training Translation Loss 58.91 
2024-02-06 17:11:37,167 EPOCH 155
2024-02-06 17:11:53,550 Epoch 155: Total Training Recognition Loss 0.57  Total Training Translation Loss 59.48 
2024-02-06 17:11:53,550 EPOCH 156
2024-02-06 17:11:59,534 [Epoch: 156 Step: 00001400] Batch Recognition Loss:   0.063583 => Gls Tokens per Sec:      919 || Batch Translation Loss:   5.507606 => Txt Tokens per Sec:     2255 || Lr: 0.000100
2024-02-06 17:12:09,690 Epoch 156: Total Training Recognition Loss 0.60  Total Training Translation Loss 57.86 
2024-02-06 17:12:09,690 EPOCH 157
2024-02-06 17:12:26,003 Epoch 157: Total Training Recognition Loss 0.60  Total Training Translation Loss 55.81 
2024-02-06 17:12:26,004 EPOCH 158
2024-02-06 17:12:42,304 Epoch 158: Total Training Recognition Loss 0.54  Total Training Translation Loss 53.41 
2024-02-06 17:12:42,304 EPOCH 159
2024-02-06 17:12:58,714 Epoch 159: Total Training Recognition Loss 0.57  Total Training Translation Loss 51.76 
2024-02-06 17:12:58,715 EPOCH 160
2024-02-06 17:13:14,680 Epoch 160: Total Training Recognition Loss 0.55  Total Training Translation Loss 51.14 
2024-02-06 17:13:14,680 EPOCH 161
2024-02-06 17:13:30,708 Epoch 161: Total Training Recognition Loss 0.55  Total Training Translation Loss 48.86 
2024-02-06 17:13:30,709 EPOCH 162
2024-02-06 17:13:47,128 Epoch 162: Total Training Recognition Loss 0.55  Total Training Translation Loss 49.12 
2024-02-06 17:13:47,128 EPOCH 163
2024-02-06 17:14:03,359 Epoch 163: Total Training Recognition Loss 0.55  Total Training Translation Loss 47.87 
2024-02-06 17:14:03,360 EPOCH 164
2024-02-06 17:14:19,657 Epoch 164: Total Training Recognition Loss 0.53  Total Training Translation Loss 45.04 
2024-02-06 17:14:19,657 EPOCH 165
2024-02-06 17:14:35,848 Epoch 165: Total Training Recognition Loss 0.50  Total Training Translation Loss 43.05 
2024-02-06 17:14:35,849 EPOCH 166
2024-02-06 17:14:52,333 Epoch 166: Total Training Recognition Loss 0.48  Total Training Translation Loss 41.56 
2024-02-06 17:14:52,334 EPOCH 167
2024-02-06 17:15:06,996 [Epoch: 167 Step: 00001500] Batch Recognition Loss:   0.035015 => Gls Tokens per Sec:      463 || Batch Translation Loss:   4.599648 => Txt Tokens per Sec:     1295 || Lr: 0.000100
2024-02-06 17:15:08,574 Epoch 167: Total Training Recognition Loss 0.48  Total Training Translation Loss 40.87 
2024-02-06 17:15:08,574 EPOCH 168
2024-02-06 17:15:24,756 Epoch 168: Total Training Recognition Loss 0.49  Total Training Translation Loss 39.26 
2024-02-06 17:15:24,757 EPOCH 169
2024-02-06 17:15:41,169 Epoch 169: Total Training Recognition Loss 0.52  Total Training Translation Loss 38.01 
2024-02-06 17:15:41,169 EPOCH 170
2024-02-06 17:15:57,204 Epoch 170: Total Training Recognition Loss 0.47  Total Training Translation Loss 38.02 
2024-02-06 17:15:57,204 EPOCH 171
2024-02-06 17:16:13,505 Epoch 171: Total Training Recognition Loss 0.50  Total Training Translation Loss 36.55 
2024-02-06 17:16:13,505 EPOCH 172
2024-02-06 17:16:29,565 Epoch 172: Total Training Recognition Loss 0.45  Total Training Translation Loss 36.31 
2024-02-06 17:16:29,566 EPOCH 173
2024-02-06 17:16:46,035 Epoch 173: Total Training Recognition Loss 0.45  Total Training Translation Loss 35.44 
2024-02-06 17:16:46,035 EPOCH 174
2024-02-06 17:17:02,031 Epoch 174: Total Training Recognition Loss 0.44  Total Training Translation Loss 33.79 
2024-02-06 17:17:02,032 EPOCH 175
2024-02-06 17:17:18,482 Epoch 175: Total Training Recognition Loss 0.46  Total Training Translation Loss 33.47 
2024-02-06 17:17:18,483 EPOCH 176
2024-02-06 17:17:34,725 Epoch 176: Total Training Recognition Loss 0.42  Total Training Translation Loss 32.52 
2024-02-06 17:17:34,725 EPOCH 177
2024-02-06 17:17:51,081 Epoch 177: Total Training Recognition Loss 0.41  Total Training Translation Loss 31.53 
2024-02-06 17:17:51,081 EPOCH 178
2024-02-06 17:18:06,378 [Epoch: 178 Step: 00001600] Batch Recognition Loss:   0.066248 => Gls Tokens per Sec:      527 || Batch Translation Loss:   2.177579 => Txt Tokens per Sec:     1479 || Lr: 0.000100
2024-02-06 17:18:07,448 Epoch 178: Total Training Recognition Loss 0.42  Total Training Translation Loss 31.00 
2024-02-06 17:18:07,448 EPOCH 179
2024-02-06 17:18:23,699 Epoch 179: Total Training Recognition Loss 0.41  Total Training Translation Loss 30.63 
2024-02-06 17:18:23,699 EPOCH 180
2024-02-06 17:18:39,885 Epoch 180: Total Training Recognition Loss 0.41  Total Training Translation Loss 28.97 
2024-02-06 17:18:39,885 EPOCH 181
2024-02-06 17:18:55,878 Epoch 181: Total Training Recognition Loss 0.42  Total Training Translation Loss 29.24 
2024-02-06 17:18:55,878 EPOCH 182
2024-02-06 17:19:12,139 Epoch 182: Total Training Recognition Loss 0.42  Total Training Translation Loss 29.04 
2024-02-06 17:19:12,139 EPOCH 183
2024-02-06 17:19:28,558 Epoch 183: Total Training Recognition Loss 0.41  Total Training Translation Loss 27.97 
2024-02-06 17:19:28,559 EPOCH 184
2024-02-06 17:19:45,244 Epoch 184: Total Training Recognition Loss 0.39  Total Training Translation Loss 27.35 
2024-02-06 17:19:45,245 EPOCH 185
2024-02-06 17:20:01,583 Epoch 185: Total Training Recognition Loss 0.41  Total Training Translation Loss 28.17 
2024-02-06 17:20:01,584 EPOCH 186
2024-02-06 17:20:17,862 Epoch 186: Total Training Recognition Loss 0.41  Total Training Translation Loss 26.29 
2024-02-06 17:20:17,863 EPOCH 187
2024-02-06 17:20:33,864 Epoch 187: Total Training Recognition Loss 0.37  Total Training Translation Loss 25.89 
2024-02-06 17:20:33,865 EPOCH 188
2024-02-06 17:20:50,260 Epoch 188: Total Training Recognition Loss 0.39  Total Training Translation Loss 25.07 
2024-02-06 17:20:50,260 EPOCH 189
2024-02-06 17:21:03,386 [Epoch: 189 Step: 00001700] Batch Recognition Loss:   0.020670 => Gls Tokens per Sec:      712 || Batch Translation Loss:   2.838775 => Txt Tokens per Sec:     1925 || Lr: 0.000100
2024-02-06 17:21:06,622 Epoch 189: Total Training Recognition Loss 0.39  Total Training Translation Loss 24.94 
2024-02-06 17:21:06,623 EPOCH 190
2024-02-06 17:21:22,936 Epoch 190: Total Training Recognition Loss 0.36  Total Training Translation Loss 24.42 
2024-02-06 17:21:22,937 EPOCH 191
2024-02-06 17:21:39,233 Epoch 191: Total Training Recognition Loss 0.36  Total Training Translation Loss 24.51 
2024-02-06 17:21:39,234 EPOCH 192
2024-02-06 17:21:55,675 Epoch 192: Total Training Recognition Loss 0.37  Total Training Translation Loss 23.48 
2024-02-06 17:21:55,675 EPOCH 193
2024-02-06 17:22:11,880 Epoch 193: Total Training Recognition Loss 0.36  Total Training Translation Loss 22.37 
2024-02-06 17:22:11,881 EPOCH 194
2024-02-06 17:22:28,524 Epoch 194: Total Training Recognition Loss 0.35  Total Training Translation Loss 21.98 
2024-02-06 17:22:28,525 EPOCH 195
2024-02-06 17:22:44,628 Epoch 195: Total Training Recognition Loss 0.33  Total Training Translation Loss 21.05 
2024-02-06 17:22:44,629 EPOCH 196
2024-02-06 17:23:01,046 Epoch 196: Total Training Recognition Loss 0.34  Total Training Translation Loss 20.57 
2024-02-06 17:23:01,047 EPOCH 197
2024-02-06 17:23:17,474 Epoch 197: Total Training Recognition Loss 0.32  Total Training Translation Loss 20.28 
2024-02-06 17:23:17,474 EPOCH 198
2024-02-06 17:23:33,548 Epoch 198: Total Training Recognition Loss 0.33  Total Training Translation Loss 19.92 
2024-02-06 17:23:33,549 EPOCH 199
2024-02-06 17:23:49,933 Epoch 199: Total Training Recognition Loss 0.31  Total Training Translation Loss 19.84 
2024-02-06 17:23:49,933 EPOCH 200
2024-02-06 17:24:06,186 [Epoch: 200 Step: 00001800] Batch Recognition Loss:   0.033208 => Gls Tokens per Sec:      654 || Batch Translation Loss:   2.961499 => Txt Tokens per Sec:     1808 || Lr: 0.000100
2024-02-06 17:24:06,187 Epoch 200: Total Training Recognition Loss 0.32  Total Training Translation Loss 20.30 
2024-02-06 17:24:06,187 EPOCH 201
2024-02-06 17:24:22,673 Epoch 201: Total Training Recognition Loss 0.32  Total Training Translation Loss 19.19 
2024-02-06 17:24:22,674 EPOCH 202
2024-02-06 17:24:38,968 Epoch 202: Total Training Recognition Loss 0.33  Total Training Translation Loss 19.21 
2024-02-06 17:24:38,968 EPOCH 203
2024-02-06 17:24:55,336 Epoch 203: Total Training Recognition Loss 0.32  Total Training Translation Loss 18.44 
2024-02-06 17:24:55,337 EPOCH 204
2024-02-06 17:25:11,717 Epoch 204: Total Training Recognition Loss 0.31  Total Training Translation Loss 17.58 
2024-02-06 17:25:11,717 EPOCH 205
2024-02-06 17:25:27,778 Epoch 205: Total Training Recognition Loss 0.30  Total Training Translation Loss 17.32 
2024-02-06 17:25:27,779 EPOCH 206
2024-02-06 17:25:43,868 Epoch 206: Total Training Recognition Loss 0.31  Total Training Translation Loss 16.77 
2024-02-06 17:25:43,868 EPOCH 207
2024-02-06 17:26:00,372 Epoch 207: Total Training Recognition Loss 0.30  Total Training Translation Loss 16.44 
2024-02-06 17:26:00,372 EPOCH 208
2024-02-06 17:26:16,605 Epoch 208: Total Training Recognition Loss 0.29  Total Training Translation Loss 15.91 
2024-02-06 17:26:16,605 EPOCH 209
2024-02-06 17:26:33,029 Epoch 209: Total Training Recognition Loss 0.29  Total Training Translation Loss 15.82 
2024-02-06 17:26:33,030 EPOCH 210
2024-02-06 17:26:48,909 Epoch 210: Total Training Recognition Loss 0.27  Total Training Translation Loss 15.92 
2024-02-06 17:26:48,910 EPOCH 211
2024-02-06 17:27:05,242 Epoch 211: Total Training Recognition Loss 0.29  Total Training Translation Loss 15.29 
2024-02-06 17:27:05,243 EPOCH 212
2024-02-06 17:27:05,860 [Epoch: 212 Step: 00001900] Batch Recognition Loss:   0.036859 => Gls Tokens per Sec:     2078 || Batch Translation Loss:   2.066308 => Txt Tokens per Sec:     6229 || Lr: 0.000100
2024-02-06 17:27:21,601 Epoch 212: Total Training Recognition Loss 0.29  Total Training Translation Loss 15.17 
2024-02-06 17:27:21,601 EPOCH 213
2024-02-06 17:27:37,692 Epoch 213: Total Training Recognition Loss 0.27  Total Training Translation Loss 14.93 
2024-02-06 17:27:37,692 EPOCH 214
2024-02-06 17:27:53,812 Epoch 214: Total Training Recognition Loss 0.30  Total Training Translation Loss 14.44 
2024-02-06 17:27:53,812 EPOCH 215
2024-02-06 17:28:10,067 Epoch 215: Total Training Recognition Loss 0.27  Total Training Translation Loss 14.45 
2024-02-06 17:28:10,068 EPOCH 216
2024-02-06 17:28:26,112 Epoch 216: Total Training Recognition Loss 0.27  Total Training Translation Loss 13.56 
2024-02-06 17:28:26,112 EPOCH 217
2024-02-06 17:28:42,382 Epoch 217: Total Training Recognition Loss 0.24  Total Training Translation Loss 13.73 
2024-02-06 17:28:42,382 EPOCH 218
2024-02-06 17:28:58,626 Epoch 218: Total Training Recognition Loss 0.28  Total Training Translation Loss 13.31 
2024-02-06 17:28:58,627 EPOCH 219
2024-02-06 17:29:14,752 Epoch 219: Total Training Recognition Loss 0.26  Total Training Translation Loss 13.46 
2024-02-06 17:29:14,753 EPOCH 220
2024-02-06 17:29:31,060 Epoch 220: Total Training Recognition Loss 0.26  Total Training Translation Loss 12.73 
2024-02-06 17:29:31,061 EPOCH 221
2024-02-06 17:29:46,992 Epoch 221: Total Training Recognition Loss 0.25  Total Training Translation Loss 12.72 
2024-02-06 17:29:46,993 EPOCH 222
2024-02-06 17:30:03,062 Epoch 222: Total Training Recognition Loss 0.22  Total Training Translation Loss 12.35 
2024-02-06 17:30:03,062 EPOCH 223
2024-02-06 17:30:03,704 [Epoch: 223 Step: 00002000] Batch Recognition Loss:   0.024515 => Gls Tokens per Sec:     3992 || Batch Translation Loss:   1.171681 => Txt Tokens per Sec:     9543 || Lr: 0.000100
2024-02-06 17:31:41,378 Hooray! New best validation result [eval_metric]!
2024-02-06 17:31:41,380 Saving new checkpoint.
2024-02-06 17:31:41,690 Validation result at epoch 223, step     2000: duration: 97.9857s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.55171	Translation Loss: 76063.57812	PPL: 1992.69031
	Eval Metric: BLEU
	WER 7.84	(DEL: 0.00,	INS: 0.00,	SUB: 7.84)
	BLEU-4 0.80	(BLEU-1: 13.23,	BLEU-2: 4.53,	BLEU-3: 1.79,	BLEU-4: 0.80)
	CHRF 17.64	ROUGE 10.72
2024-02-06 17:31:41,692 Logging Recognition and Translation Outputs
2024-02-06 17:31:41,692 ========================================================================================================================
2024-02-06 17:31:41,692 Logging Sequence: 165_414.00
2024-02-06 17:31:41,692 	Gloss Reference :	A B+C+D+E
2024-02-06 17:31:41,693 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 17:31:41,693 	Gloss Alignment :	         
2024-02-06 17:31:41,693 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 17:31:41,694 	Text Reference  :	he felt sachin was      lucky so       he always gave his     sweater to give it   to the umpire 
2024-02-06 17:31:41,695 	Text Hypothesis :	** **** delhi  capitals were  supposed to be     an   threats as      he not  take a  new zealand
2024-02-06 17:31:41,695 	Text Alignment  :	D  D    S      S        S     S        S  S      S    S       S       S  S    S    S  S   S      
2024-02-06 17:31:41,695 ========================================================================================================================
2024-02-06 17:31:41,695 Logging Sequence: 169_268.00
2024-02-06 17:31:41,695 	Gloss Reference :	A B+C+D+E
2024-02-06 17:31:41,695 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 17:31:41,696 	Gloss Alignment :	         
2024-02-06 17:31:41,696 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 17:31:41,696 	Text Reference  :	shami supports arshdeep and  many fans    supported him   as  well 
2024-02-06 17:31:41,697 	Text Hypothesis :	***** a        panel    will be   created to        solve the issue
2024-02-06 17:31:41,697 	Text Alignment  :	D     S        S        S    S    S       S         S     S   S    
2024-02-06 17:31:41,697 ========================================================================================================================
2024-02-06 17:31:41,697 Logging Sequence: 172_15.00
2024-02-06 17:31:41,697 	Gloss Reference :	A B+C+D+E
2024-02-06 17:31:41,697 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 17:31:41,697 	Gloss Alignment :	         
2024-02-06 17:31:41,698 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 17:31:41,699 	Text Reference  :	now in the final match on 28 may   2023   the  two teams were  up  against each other at       the same *** venue
2024-02-06 17:31:41,700 	Text Hypothesis :	*** ** *** ***** ***** ** ** after losing this was an    world cup trophy  he   was   unvieled the same and cafes
2024-02-06 17:31:41,700 	Text Alignment  :	D   D  D   D     D     D  D  S     S      S    S   S     S     S   S       S    S     S                 I   S    
2024-02-06 17:31:41,700 ========================================================================================================================
2024-02-06 17:31:41,700 Logging Sequence: 96_158.00
2024-02-06 17:31:41,700 	Gloss Reference :	A B+C+D+E
2024-02-06 17:31:41,700 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 17:31:41,700 	Gloss Alignment :	         
2024-02-06 17:31:41,700 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 17:31:41,701 	Text Reference  :	after this   pandya fell    on his knees in   disappointment
2024-02-06 17:31:41,701 	Text Hypothesis :	the   couple were   shocked to see the   next wicket        
2024-02-06 17:31:41,701 	Text Alignment  :	S     S      S      S       S  S   S     S    S             
2024-02-06 17:31:41,701 ========================================================================================================================
2024-02-06 17:31:41,702 Logging Sequence: 152_73.00
2024-02-06 17:31:41,702 	Gloss Reference :	A B+C+D+E
2024-02-06 17:31:41,702 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 17:31:41,702 	Gloss Alignment :	         
2024-02-06 17:31:41,702 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 17:31:41,703 	Text Reference  :	******* *** ***** *** ********* *** ******* ** *** ***** eventually he   too got  out by   shaheen afridi
2024-02-06 17:31:41,703 	Text Hypothesis :	however the match was cancelled and managed to bat first but        will be  held in  just 175     overs 
2024-02-06 17:31:41,703 	Text Alignment  :	I       I   I     I   I         I   I       I  I   I     S          S    S   S    S   S    S       S     
2024-02-06 17:31:41,703 ========================================================================================================================
2024-02-06 17:31:58,085 Epoch 223: Total Training Recognition Loss 0.23  Total Training Translation Loss 12.37 
2024-02-06 17:31:58,085 EPOCH 224
2024-02-06 17:32:14,818 Epoch 224: Total Training Recognition Loss 0.24  Total Training Translation Loss 12.19 
2024-02-06 17:32:14,819 EPOCH 225
2024-02-06 17:32:31,249 Epoch 225: Total Training Recognition Loss 0.25  Total Training Translation Loss 12.04 
2024-02-06 17:32:31,249 EPOCH 226
2024-02-06 17:32:47,509 Epoch 226: Total Training Recognition Loss 0.23  Total Training Translation Loss 11.83 
2024-02-06 17:32:47,510 EPOCH 227
2024-02-06 17:33:04,034 Epoch 227: Total Training Recognition Loss 0.24  Total Training Translation Loss 11.64 
2024-02-06 17:33:04,034 EPOCH 228
2024-02-06 17:33:20,444 Epoch 228: Total Training Recognition Loss 0.24  Total Training Translation Loss 11.29 
2024-02-06 17:33:20,445 EPOCH 229
2024-02-06 17:33:36,614 Epoch 229: Total Training Recognition Loss 0.21  Total Training Translation Loss 10.85 
2024-02-06 17:33:36,614 EPOCH 230
2024-02-06 17:33:52,881 Epoch 230: Total Training Recognition Loss 0.22  Total Training Translation Loss 10.80 
2024-02-06 17:33:52,882 EPOCH 231
2024-02-06 17:34:09,318 Epoch 231: Total Training Recognition Loss 0.22  Total Training Translation Loss 10.68 
2024-02-06 17:34:09,319 EPOCH 232
2024-02-06 17:34:25,399 Epoch 232: Total Training Recognition Loss 0.20  Total Training Translation Loss 10.44 
2024-02-06 17:34:25,399 EPOCH 233
2024-02-06 17:34:41,945 Epoch 233: Total Training Recognition Loss 0.22  Total Training Translation Loss 10.52 
2024-02-06 17:34:41,946 EPOCH 234
2024-02-06 17:34:45,656 [Epoch: 234 Step: 00002100] Batch Recognition Loss:   0.032524 => Gls Tokens per Sec:     1035 || Batch Translation Loss:   0.679814 => Txt Tokens per Sec:     2493 || Lr: 0.000100
2024-02-06 17:34:58,417 Epoch 234: Total Training Recognition Loss 0.21  Total Training Translation Loss 10.20 
2024-02-06 17:34:58,419 EPOCH 235
2024-02-06 17:35:14,795 Epoch 235: Total Training Recognition Loss 0.21  Total Training Translation Loss 10.19 
2024-02-06 17:35:14,796 EPOCH 236
2024-02-06 17:35:31,349 Epoch 236: Total Training Recognition Loss 0.21  Total Training Translation Loss 10.03 
2024-02-06 17:35:31,350 EPOCH 237
2024-02-06 17:35:47,459 Epoch 237: Total Training Recognition Loss 0.19  Total Training Translation Loss 9.78 
2024-02-06 17:35:47,459 EPOCH 238
2024-02-06 17:36:03,907 Epoch 238: Total Training Recognition Loss 0.20  Total Training Translation Loss 9.61 
2024-02-06 17:36:03,908 EPOCH 239
2024-02-06 17:36:20,373 Epoch 239: Total Training Recognition Loss 0.19  Total Training Translation Loss 9.67 
2024-02-06 17:36:20,373 EPOCH 240
2024-02-06 17:36:36,631 Epoch 240: Total Training Recognition Loss 0.18  Total Training Translation Loss 9.55 
2024-02-06 17:36:36,632 EPOCH 241
2024-02-06 17:36:52,975 Epoch 241: Total Training Recognition Loss 0.19  Total Training Translation Loss 9.27 
2024-02-06 17:36:52,975 EPOCH 242
2024-02-06 17:37:09,269 Epoch 242: Total Training Recognition Loss 0.18  Total Training Translation Loss 9.17 
2024-02-06 17:37:09,269 EPOCH 243
2024-02-06 17:37:25,784 Epoch 243: Total Training Recognition Loss 0.18  Total Training Translation Loss 9.16 
2024-02-06 17:37:25,785 EPOCH 244
2024-02-06 17:37:41,810 Epoch 244: Total Training Recognition Loss 0.18  Total Training Translation Loss 9.03 
2024-02-06 17:37:41,810 EPOCH 245
2024-02-06 17:37:52,082 [Epoch: 245 Step: 00002200] Batch Recognition Loss:   0.012449 => Gls Tokens per Sec:      498 || Batch Translation Loss:   1.040008 => Txt Tokens per Sec:     1533 || Lr: 0.000100
2024-02-06 17:37:58,172 Epoch 245: Total Training Recognition Loss 0.18  Total Training Translation Loss 8.65 
2024-02-06 17:37:58,173 EPOCH 246
2024-02-06 17:38:14,433 Epoch 246: Total Training Recognition Loss 0.17  Total Training Translation Loss 8.46 
2024-02-06 17:38:14,435 EPOCH 247
2024-02-06 17:38:30,846 Epoch 247: Total Training Recognition Loss 0.18  Total Training Translation Loss 8.45 
2024-02-06 17:38:30,847 EPOCH 248
2024-02-06 17:38:46,795 Epoch 248: Total Training Recognition Loss 0.19  Total Training Translation Loss 8.45 
2024-02-06 17:38:46,795 EPOCH 249
2024-02-06 17:39:03,372 Epoch 249: Total Training Recognition Loss 0.18  Total Training Translation Loss 8.35 
2024-02-06 17:39:03,372 EPOCH 250
2024-02-06 17:39:19,753 Epoch 250: Total Training Recognition Loss 0.19  Total Training Translation Loss 8.29 
2024-02-06 17:39:19,753 EPOCH 251
2024-02-06 17:39:36,240 Epoch 251: Total Training Recognition Loss 0.17  Total Training Translation Loss 8.06 
2024-02-06 17:39:36,241 EPOCH 252
2024-02-06 17:39:52,510 Epoch 252: Total Training Recognition Loss 0.16  Total Training Translation Loss 7.98 
2024-02-06 17:39:52,511 EPOCH 253
2024-02-06 17:40:09,051 Epoch 253: Total Training Recognition Loss 0.18  Total Training Translation Loss 7.88 
2024-02-06 17:40:09,052 EPOCH 254
2024-02-06 17:40:25,114 Epoch 254: Total Training Recognition Loss 0.16  Total Training Translation Loss 7.67 
2024-02-06 17:40:25,115 EPOCH 255
2024-02-06 17:40:41,288 Epoch 255: Total Training Recognition Loss 0.16  Total Training Translation Loss 7.73 
2024-02-06 17:40:41,288 EPOCH 256
2024-02-06 17:40:43,387 [Epoch: 256 Step: 00002300] Batch Recognition Loss:   0.012348 => Gls Tokens per Sec:     3051 || Batch Translation Loss:   0.915804 => Txt Tokens per Sec:     7456 || Lr: 0.000100
2024-02-06 17:40:57,674 Epoch 256: Total Training Recognition Loss 0.17  Total Training Translation Loss 7.67 
2024-02-06 17:40:57,674 EPOCH 257
2024-02-06 17:41:14,115 Epoch 257: Total Training Recognition Loss 0.17  Total Training Translation Loss 7.85 
2024-02-06 17:41:14,116 EPOCH 258
2024-02-06 17:41:30,531 Epoch 258: Total Training Recognition Loss 0.16  Total Training Translation Loss 7.33 
2024-02-06 17:41:30,532 EPOCH 259
2024-02-06 17:41:47,061 Epoch 259: Total Training Recognition Loss 0.17  Total Training Translation Loss 7.45 
2024-02-06 17:41:47,062 EPOCH 260
2024-02-06 17:42:03,384 Epoch 260: Total Training Recognition Loss 0.16  Total Training Translation Loss 7.27 
2024-02-06 17:42:03,384 EPOCH 261
2024-02-06 17:42:19,737 Epoch 261: Total Training Recognition Loss 0.15  Total Training Translation Loss 7.20 
2024-02-06 17:42:19,738 EPOCH 262
2024-02-06 17:42:36,132 Epoch 262: Total Training Recognition Loss 0.15  Total Training Translation Loss 6.99 
2024-02-06 17:42:36,133 EPOCH 263
2024-02-06 17:42:52,511 Epoch 263: Total Training Recognition Loss 0.16  Total Training Translation Loss 6.91 
2024-02-06 17:42:52,512 EPOCH 264
2024-02-06 17:43:09,095 Epoch 264: Total Training Recognition Loss 0.15  Total Training Translation Loss 6.68 
2024-02-06 17:43:09,095 EPOCH 265
2024-02-06 17:43:25,326 Epoch 265: Total Training Recognition Loss 0.16  Total Training Translation Loss 6.75 
2024-02-06 17:43:25,327 EPOCH 266
2024-02-06 17:43:41,720 Epoch 266: Total Training Recognition Loss 0.15  Total Training Translation Loss 6.54 
2024-02-06 17:43:41,720 EPOCH 267
2024-02-06 17:43:50,993 [Epoch: 267 Step: 00002400] Batch Recognition Loss:   0.015596 => Gls Tokens per Sec:      731 || Batch Translation Loss:   0.949863 => Txt Tokens per Sec:     1970 || Lr: 0.000100
2024-02-06 17:43:58,187 Epoch 267: Total Training Recognition Loss 0.14  Total Training Translation Loss 6.48 
2024-02-06 17:43:58,188 EPOCH 268
2024-02-06 17:44:14,649 Epoch 268: Total Training Recognition Loss 0.15  Total Training Translation Loss 6.50 
2024-02-06 17:44:14,649 EPOCH 269
2024-02-06 17:44:30,856 Epoch 269: Total Training Recognition Loss 0.16  Total Training Translation Loss 6.56 
2024-02-06 17:44:30,856 EPOCH 270
2024-02-06 17:44:47,447 Epoch 270: Total Training Recognition Loss 0.15  Total Training Translation Loss 6.31 
2024-02-06 17:44:47,448 EPOCH 271
2024-02-06 17:45:03,544 Epoch 271: Total Training Recognition Loss 0.15  Total Training Translation Loss 6.27 
2024-02-06 17:45:03,545 EPOCH 272
2024-02-06 17:45:20,235 Epoch 272: Total Training Recognition Loss 0.13  Total Training Translation Loss 6.24 
2024-02-06 17:45:20,237 EPOCH 273
2024-02-06 17:45:36,832 Epoch 273: Total Training Recognition Loss 0.14  Total Training Translation Loss 6.07 
2024-02-06 17:45:36,832 EPOCH 274
2024-02-06 17:45:53,340 Epoch 274: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.77 
2024-02-06 17:45:53,341 EPOCH 275
2024-02-06 17:46:09,966 Epoch 275: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.96 
2024-02-06 17:46:09,967 EPOCH 276
2024-02-06 17:46:26,323 Epoch 276: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.79 
2024-02-06 17:46:26,323 EPOCH 277
2024-02-06 17:46:42,551 Epoch 277: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.78 
2024-02-06 17:46:42,552 EPOCH 278
2024-02-06 17:46:52,310 [Epoch: 278 Step: 00002500] Batch Recognition Loss:   0.013395 => Gls Tokens per Sec:      826 || Batch Translation Loss:   0.762261 => Txt Tokens per Sec:     2225 || Lr: 0.000100
2024-02-06 17:46:59,090 Epoch 278: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.57 
2024-02-06 17:46:59,091 EPOCH 279
2024-02-06 17:47:15,513 Epoch 279: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.64 
2024-02-06 17:47:15,513 EPOCH 280
2024-02-06 17:47:31,906 Epoch 280: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.99 
2024-02-06 17:47:31,906 EPOCH 281
2024-02-06 17:47:48,239 Epoch 281: Total Training Recognition Loss 0.12  Total Training Translation Loss 5.89 
2024-02-06 17:47:48,239 EPOCH 282
2024-02-06 17:48:04,541 Epoch 282: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.59 
2024-02-06 17:48:04,541 EPOCH 283
2024-02-06 17:48:20,885 Epoch 283: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.41 
2024-02-06 17:48:20,886 EPOCH 284
2024-02-06 17:48:37,400 Epoch 284: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.46 
2024-02-06 17:48:37,400 EPOCH 285
2024-02-06 17:48:53,827 Epoch 285: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.40 
2024-02-06 17:48:53,828 EPOCH 286
2024-02-06 17:49:09,969 Epoch 286: Total Training Recognition Loss 0.11  Total Training Translation Loss 5.14 
2024-02-06 17:49:09,970 EPOCH 287
2024-02-06 17:49:26,452 Epoch 287: Total Training Recognition Loss 0.12  Total Training Translation Loss 5.59 
2024-02-06 17:49:26,452 EPOCH 288
2024-02-06 17:49:42,631 Epoch 288: Total Training Recognition Loss 0.12  Total Training Translation Loss 5.67 
2024-02-06 17:49:42,631 EPOCH 289
2024-02-06 17:49:58,834 [Epoch: 289 Step: 00002600] Batch Recognition Loss:   0.025227 => Gls Tokens per Sec:      577 || Batch Translation Loss:   0.381376 => Txt Tokens per Sec:     1611 || Lr: 0.000100
2024-02-06 17:49:59,260 Epoch 289: Total Training Recognition Loss 0.12  Total Training Translation Loss 5.50 
2024-02-06 17:49:59,260 EPOCH 290
2024-02-06 17:50:15,602 Epoch 290: Total Training Recognition Loss 0.11  Total Training Translation Loss 6.10 
2024-02-06 17:50:15,603 EPOCH 291
2024-02-06 17:50:31,949 Epoch 291: Total Training Recognition Loss 0.12  Total Training Translation Loss 6.79 
2024-02-06 17:50:31,950 EPOCH 292
2024-02-06 17:50:48,386 Epoch 292: Total Training Recognition Loss 0.16  Total Training Translation Loss 17.16 
2024-02-06 17:50:48,386 EPOCH 293
2024-02-06 17:51:04,752 Epoch 293: Total Training Recognition Loss 0.43  Total Training Translation Loss 50.39 
2024-02-06 17:51:04,753 EPOCH 294
2024-02-06 17:51:21,305 Epoch 294: Total Training Recognition Loss 0.37  Total Training Translation Loss 40.32 
2024-02-06 17:51:21,305 EPOCH 295
2024-02-06 17:51:37,348 Epoch 295: Total Training Recognition Loss 0.42  Total Training Translation Loss 28.52 
2024-02-06 17:51:37,348 EPOCH 296
2024-02-06 17:51:53,795 Epoch 296: Total Training Recognition Loss 0.36  Total Training Translation Loss 18.00 
2024-02-06 17:51:53,795 EPOCH 297
2024-02-06 17:52:09,934 Epoch 297: Total Training Recognition Loss 0.40  Total Training Translation Loss 13.42 
2024-02-06 17:52:09,934 EPOCH 298
2024-02-06 17:52:26,807 Epoch 298: Total Training Recognition Loss 0.32  Total Training Translation Loss 10.50 
2024-02-06 17:52:26,808 EPOCH 299
2024-02-06 17:52:42,996 Epoch 299: Total Training Recognition Loss 0.25  Total Training Translation Loss 8.87 
2024-02-06 17:52:42,997 EPOCH 300
2024-02-06 17:52:59,114 [Epoch: 300 Step: 00002700] Batch Recognition Loss:   0.033360 => Gls Tokens per Sec:      659 || Batch Translation Loss:   1.025169 => Txt Tokens per Sec:     1823 || Lr: 0.000100
2024-02-06 17:52:59,114 Epoch 300: Total Training Recognition Loss 0.19  Total Training Translation Loss 7.26 
2024-02-06 17:52:59,114 EPOCH 301
2024-02-06 17:53:15,797 Epoch 301: Total Training Recognition Loss 0.16  Total Training Translation Loss 6.71 
2024-02-06 17:53:15,799 EPOCH 302
2024-02-06 17:53:32,248 Epoch 302: Total Training Recognition Loss 0.16  Total Training Translation Loss 5.90 
2024-02-06 17:53:32,250 EPOCH 303
2024-02-06 17:53:48,450 Epoch 303: Total Training Recognition Loss 0.17  Total Training Translation Loss 5.51 
2024-02-06 17:53:48,451 EPOCH 304
2024-02-06 17:54:04,817 Epoch 304: Total Training Recognition Loss 0.15  Total Training Translation Loss 5.23 
2024-02-06 17:54:04,817 EPOCH 305
2024-02-06 17:54:21,218 Epoch 305: Total Training Recognition Loss 0.16  Total Training Translation Loss 4.88 
2024-02-06 17:54:21,219 EPOCH 306
2024-02-06 17:54:37,810 Epoch 306: Total Training Recognition Loss 0.14  Total Training Translation Loss 4.78 
2024-02-06 17:54:37,810 EPOCH 307
2024-02-06 17:54:54,220 Epoch 307: Total Training Recognition Loss 0.13  Total Training Translation Loss 4.63 
2024-02-06 17:54:54,221 EPOCH 308
2024-02-06 17:55:10,942 Epoch 308: Total Training Recognition Loss 0.14  Total Training Translation Loss 4.44 
2024-02-06 17:55:10,942 EPOCH 309
2024-02-06 17:55:27,165 Epoch 309: Total Training Recognition Loss 0.13  Total Training Translation Loss 4.34 
2024-02-06 17:55:27,166 EPOCH 310
2024-02-06 17:55:43,574 Epoch 310: Total Training Recognition Loss 0.12  Total Training Translation Loss 4.27 
2024-02-06 17:55:43,574 EPOCH 311
2024-02-06 17:55:59,804 Epoch 311: Total Training Recognition Loss 0.13  Total Training Translation Loss 4.23 
2024-02-06 17:55:59,804 EPOCH 312
2024-02-06 17:56:00,122 [Epoch: 312 Step: 00002800] Batch Recognition Loss:   0.008578 => Gls Tokens per Sec:     4038 || Batch Translation Loss:   0.439157 => Txt Tokens per Sec:    10309 || Lr: 0.000100
2024-02-06 17:56:15,977 Epoch 312: Total Training Recognition Loss 0.14  Total Training Translation Loss 4.10 
2024-02-06 17:56:15,977 EPOCH 313
2024-02-06 17:56:32,813 Epoch 313: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.98 
2024-02-06 17:56:32,814 EPOCH 314
2024-02-06 17:56:49,006 Epoch 314: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.92 
2024-02-06 17:56:49,007 EPOCH 315
2024-02-06 17:57:05,472 Epoch 315: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.87 
2024-02-06 17:57:05,472 EPOCH 316
2024-02-06 17:57:21,617 Epoch 316: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.82 
2024-02-06 17:57:21,617 EPOCH 317
2024-02-06 17:57:38,249 Epoch 317: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.91 
2024-02-06 17:57:38,251 EPOCH 318
2024-02-06 17:57:54,409 Epoch 318: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.64 
2024-02-06 17:57:54,409 EPOCH 319
2024-02-06 17:58:10,601 Epoch 319: Total Training Recognition Loss 0.13  Total Training Translation Loss 3.65 
2024-02-06 17:58:10,602 EPOCH 320
2024-02-06 17:58:26,821 Epoch 320: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.67 
2024-02-06 17:58:26,821 EPOCH 321
2024-02-06 17:58:43,273 Epoch 321: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.54 
2024-02-06 17:58:43,274 EPOCH 322
2024-02-06 17:59:00,064 Epoch 322: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.44 
2024-02-06 17:59:00,065 EPOCH 323
2024-02-06 17:59:01,096 [Epoch: 323 Step: 00002900] Batch Recognition Loss:   0.014959 => Gls Tokens per Sec:     2486 || Batch Translation Loss:   0.252264 => Txt Tokens per Sec:     6002 || Lr: 0.000100
2024-02-06 17:59:16,306 Epoch 323: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.38 
2024-02-06 17:59:16,306 EPOCH 324
2024-02-06 17:59:32,601 Epoch 324: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.38 
2024-02-06 17:59:32,602 EPOCH 325
2024-02-06 17:59:49,390 Epoch 325: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.39 
2024-02-06 17:59:49,391 EPOCH 326
2024-02-06 18:00:05,632 Epoch 326: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.29 
2024-02-06 18:00:05,632 EPOCH 327
2024-02-06 18:00:22,054 Epoch 327: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.32 
2024-02-06 18:00:22,055 EPOCH 328
2024-02-06 18:00:38,348 Epoch 328: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.17 
2024-02-06 18:00:38,349 EPOCH 329
2024-02-06 18:00:54,680 Epoch 329: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.11 
2024-02-06 18:00:54,680 EPOCH 330
2024-02-06 18:01:11,319 Epoch 330: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.07 
2024-02-06 18:01:11,320 EPOCH 331
2024-02-06 18:01:27,918 Epoch 331: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.99 
2024-02-06 18:01:27,918 EPOCH 332
2024-02-06 18:01:44,194 Epoch 332: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.06 
2024-02-06 18:01:44,195 EPOCH 333
2024-02-06 18:02:00,479 Epoch 333: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.07 
2024-02-06 18:02:00,479 EPOCH 334
2024-02-06 18:02:01,723 [Epoch: 334 Step: 00003000] Batch Recognition Loss:   0.006111 => Gls Tokens per Sec:     3092 || Batch Translation Loss:   0.387442 => Txt Tokens per Sec:     8051 || Lr: 0.000100
2024-02-06 18:02:16,563 Epoch 334: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.06 
2024-02-06 18:02:16,565 EPOCH 335
2024-02-06 18:02:32,744 Epoch 335: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.99 
2024-02-06 18:02:32,746 EPOCH 336
2024-02-06 18:02:49,337 Epoch 336: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.94 
2024-02-06 18:02:49,338 EPOCH 337
2024-02-06 18:03:05,730 Epoch 337: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.94 
2024-02-06 18:03:05,731 EPOCH 338
2024-02-06 18:03:21,964 Epoch 338: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.94 
2024-02-06 18:03:21,965 EPOCH 339
2024-02-06 18:03:38,338 Epoch 339: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.99 
2024-02-06 18:03:38,338 EPOCH 340
2024-02-06 18:03:54,537 Epoch 340: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.86 
2024-02-06 18:03:54,537 EPOCH 341
2024-02-06 18:04:10,625 Epoch 341: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.80 
2024-02-06 18:04:10,626 EPOCH 342
2024-02-06 18:04:27,225 Epoch 342: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.87 
2024-02-06 18:04:27,225 EPOCH 343
2024-02-06 18:04:43,900 Epoch 343: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.82 
2024-02-06 18:04:43,901 EPOCH 344
2024-02-06 18:05:00,326 Epoch 344: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.73 
2024-02-06 18:05:00,327 EPOCH 345
2024-02-06 18:05:11,345 [Epoch: 345 Step: 00003100] Batch Recognition Loss:   0.019042 => Gls Tokens per Sec:      383 || Batch Translation Loss:   0.397592 => Txt Tokens per Sec:     1052 || Lr: 0.000100
2024-02-06 18:05:16,726 Epoch 345: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.66 
2024-02-06 18:05:16,727 EPOCH 346
2024-02-06 18:05:33,117 Epoch 346: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.69 
2024-02-06 18:05:33,118 EPOCH 347
2024-02-06 18:05:49,460 Epoch 347: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.62 
2024-02-06 18:05:49,461 EPOCH 348
2024-02-06 18:06:05,886 Epoch 348: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.56 
2024-02-06 18:06:05,887 EPOCH 349
2024-02-06 18:06:22,388 Epoch 349: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.58 
2024-02-06 18:06:22,388 EPOCH 350
2024-02-06 18:06:38,517 Epoch 350: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.57 
2024-02-06 18:06:38,517 EPOCH 351
2024-02-06 18:06:54,959 Epoch 351: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.48 
2024-02-06 18:06:54,960 EPOCH 352
2024-02-06 18:07:11,227 Epoch 352: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.55 
2024-02-06 18:07:11,227 EPOCH 353
2024-02-06 18:07:27,433 Epoch 353: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.51 
2024-02-06 18:07:27,434 EPOCH 354
2024-02-06 18:07:43,614 Epoch 354: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.42 
2024-02-06 18:07:43,615 EPOCH 355
2024-02-06 18:07:59,722 Epoch 355: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.57 
2024-02-06 18:07:59,723 EPOCH 356
2024-02-06 18:08:08,000 [Epoch: 356 Step: 00003200] Batch Recognition Loss:   0.003808 => Gls Tokens per Sec:      773 || Batch Translation Loss:   0.246455 => Txt Tokens per Sec:     2117 || Lr: 0.000100
2024-02-06 18:08:16,533 Epoch 356: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.41 
2024-02-06 18:08:16,533 EPOCH 357
2024-02-06 18:08:33,187 Epoch 357: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.38 
2024-02-06 18:08:33,189 EPOCH 358
2024-02-06 18:08:49,688 Epoch 358: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.31 
2024-02-06 18:08:49,689 EPOCH 359
2024-02-06 18:09:06,498 Epoch 359: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.41 
2024-02-06 18:09:06,499 EPOCH 360
2024-02-06 18:09:23,079 Epoch 360: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.37 
2024-02-06 18:09:23,080 EPOCH 361
2024-02-06 18:09:39,418 Epoch 361: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.40 
2024-02-06 18:09:39,420 EPOCH 362
2024-02-06 18:09:55,784 Epoch 362: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.36 
2024-02-06 18:09:55,784 EPOCH 363
2024-02-06 18:10:12,379 Epoch 363: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.29 
2024-02-06 18:10:12,380 EPOCH 364
2024-02-06 18:10:28,547 Epoch 364: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.29 
2024-02-06 18:10:28,547 EPOCH 365
2024-02-06 18:10:44,997 Epoch 365: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.30 
2024-02-06 18:10:44,997 EPOCH 366
2024-02-06 18:11:01,173 Epoch 366: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.26 
2024-02-06 18:11:01,173 EPOCH 367
2024-02-06 18:11:16,018 [Epoch: 367 Step: 00003300] Batch Recognition Loss:   0.014008 => Gls Tokens per Sec:      457 || Batch Translation Loss:   0.360855 => Txt Tokens per Sec:     1303 || Lr: 0.000100
2024-02-06 18:11:17,634 Epoch 367: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.30 
2024-02-06 18:11:17,634 EPOCH 368
2024-02-06 18:11:33,798 Epoch 368: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.21 
2024-02-06 18:11:33,799 EPOCH 369
2024-02-06 18:11:50,091 Epoch 369: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.22 
2024-02-06 18:11:50,092 EPOCH 370
2024-02-06 18:12:06,545 Epoch 370: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.16 
2024-02-06 18:12:06,546 EPOCH 371
2024-02-06 18:12:22,582 Epoch 371: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.23 
2024-02-06 18:12:22,583 EPOCH 372
2024-02-06 18:12:38,853 Epoch 372: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.10 
2024-02-06 18:12:38,854 EPOCH 373
2024-02-06 18:12:54,943 Epoch 373: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.09 
2024-02-06 18:12:54,944 EPOCH 374
2024-02-06 18:13:11,498 Epoch 374: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.05 
2024-02-06 18:13:11,498 EPOCH 375
2024-02-06 18:13:27,697 Epoch 375: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.08 
2024-02-06 18:13:27,698 EPOCH 376
2024-02-06 18:13:44,232 Epoch 376: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.98 
2024-02-06 18:13:44,233 EPOCH 377
2024-02-06 18:14:00,376 Epoch 377: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.10 
2024-02-06 18:14:00,376 EPOCH 378
2024-02-06 18:14:16,071 [Epoch: 378 Step: 00003400] Batch Recognition Loss:   0.003452 => Gls Tokens per Sec:      514 || Batch Translation Loss:   0.269743 => Txt Tokens per Sec:     1417 || Lr: 0.000100
2024-02-06 18:14:17,191 Epoch 378: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.16 
2024-02-06 18:14:17,191 EPOCH 379
2024-02-06 18:14:33,316 Epoch 379: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.28 
2024-02-06 18:14:33,317 EPOCH 380
2024-02-06 18:14:49,907 Epoch 380: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.35 
2024-02-06 18:14:49,907 EPOCH 381
2024-02-06 18:15:06,355 Epoch 381: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.48 
2024-02-06 18:15:06,357 EPOCH 382
2024-02-06 18:15:23,126 Epoch 382: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.32 
2024-02-06 18:15:23,126 EPOCH 383
2024-02-06 18:15:39,384 Epoch 383: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.23 
2024-02-06 18:15:39,384 EPOCH 384
2024-02-06 18:15:55,905 Epoch 384: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.20 
2024-02-06 18:15:55,905 EPOCH 385
2024-02-06 18:16:12,171 Epoch 385: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.04 
2024-02-06 18:16:12,172 EPOCH 386
2024-02-06 18:16:28,605 Epoch 386: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.05 
2024-02-06 18:16:28,606 EPOCH 387
2024-02-06 18:16:44,996 Epoch 387: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.18 
2024-02-06 18:16:44,997 EPOCH 388
2024-02-06 18:17:01,424 Epoch 388: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.14 
2024-02-06 18:17:01,425 EPOCH 389
2024-02-06 18:17:17,546 [Epoch: 389 Step: 00003500] Batch Recognition Loss:   0.005307 => Gls Tokens per Sec:      579 || Batch Translation Loss:   0.206062 => Txt Tokens per Sec:     1680 || Lr: 0.000100
2024-02-06 18:17:17,789 Epoch 389: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.98 
2024-02-06 18:17:17,789 EPOCH 390
2024-02-06 18:17:34,047 Epoch 390: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.06 
2024-02-06 18:17:34,047 EPOCH 391
2024-02-06 18:17:50,546 Epoch 391: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.10 
2024-02-06 18:17:50,547 EPOCH 392
2024-02-06 18:18:06,702 Epoch 392: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.93 
2024-02-06 18:18:06,704 EPOCH 393
2024-02-06 18:18:23,072 Epoch 393: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.94 
2024-02-06 18:18:23,073 EPOCH 394
2024-02-06 18:18:39,489 Epoch 394: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.94 
2024-02-06 18:18:39,489 EPOCH 395
2024-02-06 18:18:55,564 Epoch 395: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.80 
2024-02-06 18:18:55,564 EPOCH 396
2024-02-06 18:19:11,980 Epoch 396: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.74 
2024-02-06 18:19:11,980 EPOCH 397
2024-02-06 18:19:28,103 Epoch 397: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.86 
2024-02-06 18:19:28,104 EPOCH 398
2024-02-06 18:19:44,791 Epoch 398: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.88 
2024-02-06 18:19:44,791 EPOCH 399
2024-02-06 18:20:01,590 Epoch 399: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.71 
2024-02-06 18:20:01,591 EPOCH 400
2024-02-06 18:20:18,128 [Epoch: 400 Step: 00003600] Batch Recognition Loss:   0.006489 => Gls Tokens per Sec:      642 || Batch Translation Loss:   0.242859 => Txt Tokens per Sec:     1777 || Lr: 0.000100
2024-02-06 18:20:18,129 Epoch 400: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.72 
2024-02-06 18:20:18,129 EPOCH 401
2024-02-06 18:20:34,315 Epoch 401: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.72 
2024-02-06 18:20:34,317 EPOCH 402
2024-02-06 18:20:50,703 Epoch 402: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.80 
2024-02-06 18:20:50,703 EPOCH 403
2024-02-06 18:21:07,234 Epoch 403: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.79 
2024-02-06 18:21:07,235 EPOCH 404
2024-02-06 18:21:23,469 Epoch 404: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.79 
2024-02-06 18:21:23,470 EPOCH 405
2024-02-06 18:21:39,575 Epoch 405: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.81 
2024-02-06 18:21:39,575 EPOCH 406
2024-02-06 18:21:56,120 Epoch 406: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.86 
2024-02-06 18:21:56,122 EPOCH 407
2024-02-06 18:22:12,205 Epoch 407: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.84 
2024-02-06 18:22:12,205 EPOCH 408
2024-02-06 18:22:28,677 Epoch 408: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.01 
2024-02-06 18:22:28,678 EPOCH 409
2024-02-06 18:22:44,998 Epoch 409: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.50 
2024-02-06 18:22:45,000 EPOCH 410
2024-02-06 18:23:01,560 Epoch 410: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.61 
2024-02-06 18:23:01,560 EPOCH 411
2024-02-06 18:23:17,862 Epoch 411: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.71 
2024-02-06 18:23:17,863 EPOCH 412
2024-02-06 18:23:23,850 [Epoch: 412 Step: 00003700] Batch Recognition Loss:   0.009205 => Gls Tokens per Sec:      214 || Batch Translation Loss:   1.825756 => Txt Tokens per Sec:      735 || Lr: 0.000100
2024-02-06 18:23:34,219 Epoch 412: Total Training Recognition Loss 0.07  Total Training Translation Loss 7.43 
2024-02-06 18:23:34,220 EPOCH 413
2024-02-06 18:23:50,650 Epoch 413: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.61 
2024-02-06 18:23:50,652 EPOCH 414
2024-02-06 18:24:07,122 Epoch 414: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.99 
2024-02-06 18:24:07,122 EPOCH 415
2024-02-06 18:24:23,077 Epoch 415: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.31 
2024-02-06 18:24:23,078 EPOCH 416
2024-02-06 18:24:39,605 Epoch 416: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.70 
2024-02-06 18:24:39,606 EPOCH 417
2024-02-06 18:24:56,112 Epoch 417: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.62 
2024-02-06 18:24:56,113 EPOCH 418
2024-02-06 18:25:12,695 Epoch 418: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.78 
2024-02-06 18:25:12,696 EPOCH 419
2024-02-06 18:25:29,206 Epoch 419: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.77 
2024-02-06 18:25:29,207 EPOCH 420
2024-02-06 18:25:45,337 Epoch 420: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.73 
2024-02-06 18:25:45,337 EPOCH 421
2024-02-06 18:26:01,807 Epoch 421: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.28 
2024-02-06 18:26:01,808 EPOCH 422
2024-02-06 18:26:18,184 Epoch 422: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.97 
2024-02-06 18:26:18,184 EPOCH 423
2024-02-06 18:26:19,193 [Epoch: 423 Step: 00003800] Batch Recognition Loss:   0.004883 => Gls Tokens per Sec:     2540 || Batch Translation Loss:   0.235060 => Txt Tokens per Sec:     6660 || Lr: 0.000100
2024-02-06 18:26:34,378 Epoch 423: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.58 
2024-02-06 18:26:34,378 EPOCH 424
2024-02-06 18:26:50,908 Epoch 424: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.50 
2024-02-06 18:26:50,909 EPOCH 425
2024-02-06 18:27:06,791 Epoch 425: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.21 
2024-02-06 18:27:06,792 EPOCH 426
2024-02-06 18:27:23,376 Epoch 426: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.39 
2024-02-06 18:27:23,376 EPOCH 427
2024-02-06 18:27:39,519 Epoch 427: Total Training Recognition Loss 0.07  Total Training Translation Loss 5.33 
2024-02-06 18:27:39,519 EPOCH 428
2024-02-06 18:27:55,957 Epoch 428: Total Training Recognition Loss 0.10  Total Training Translation Loss 5.02 
2024-02-06 18:27:55,958 EPOCH 429
2024-02-06 18:28:12,247 Epoch 429: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.90 
2024-02-06 18:28:12,248 EPOCH 430
2024-02-06 18:28:28,644 Epoch 430: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.22 
2024-02-06 18:28:28,645 EPOCH 431
2024-02-06 18:28:44,861 Epoch 431: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.52 
2024-02-06 18:28:44,861 EPOCH 432
2024-02-06 18:29:01,340 Epoch 432: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.21 
2024-02-06 18:29:01,341 EPOCH 433
2024-02-06 18:29:17,558 Epoch 433: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.88 
2024-02-06 18:29:17,559 EPOCH 434
2024-02-06 18:29:21,593 [Epoch: 434 Step: 00003900] Batch Recognition Loss:   0.003482 => Gls Tokens per Sec:      952 || Batch Translation Loss:   0.195440 => Txt Tokens per Sec:     2702 || Lr: 0.000100
2024-02-06 18:29:33,785 Epoch 434: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.68 
2024-02-06 18:29:33,786 EPOCH 435
2024-02-06 18:29:50,239 Epoch 435: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.57 
2024-02-06 18:29:50,240 EPOCH 436
2024-02-06 18:30:06,590 Epoch 436: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.59 
2024-02-06 18:30:06,590 EPOCH 437
2024-02-06 18:30:23,479 Epoch 437: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.44 
2024-02-06 18:30:23,481 EPOCH 438
2024-02-06 18:30:39,760 Epoch 438: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.40 
2024-02-06 18:30:39,761 EPOCH 439
2024-02-06 18:30:56,270 Epoch 439: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.35 
2024-02-06 18:30:56,272 EPOCH 440
2024-02-06 18:31:13,030 Epoch 440: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.32 
2024-02-06 18:31:13,032 EPOCH 441
2024-02-06 18:31:29,205 Epoch 441: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.28 
2024-02-06 18:31:29,206 EPOCH 442
2024-02-06 18:31:45,763 Epoch 442: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.23 
2024-02-06 18:31:45,763 EPOCH 443
2024-02-06 18:32:02,367 Epoch 443: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.23 
2024-02-06 18:32:02,368 EPOCH 444
2024-02-06 18:32:18,622 Epoch 444: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.26 
2024-02-06 18:32:18,623 EPOCH 445
2024-02-06 18:32:25,512 [Epoch: 445 Step: 00004000] Batch Recognition Loss:   0.005814 => Gls Tokens per Sec:      743 || Batch Translation Loss:   0.187525 => Txt Tokens per Sec:     1864 || Lr: 0.000100
2024-02-06 18:33:33,896 Validation result at epoch 445, step     4000: duration: 68.3825s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.54551	Translation Loss: 83530.37500	PPL: 4200.77881
	Eval Metric: BLEU
	WER 6.64	(DEL: 0.00,	INS: 0.00,	SUB: 6.64)
	BLEU-4 0.44	(BLEU-1: 12.55,	BLEU-2: 3.99,	BLEU-3: 1.38,	BLEU-4: 0.44)
	CHRF 17.88	ROUGE 10.19
2024-02-06 18:33:33,897 Logging Recognition and Translation Outputs
2024-02-06 18:33:33,898 ========================================================================================================================
2024-02-06 18:33:33,898 Logging Sequence: 112_165.00
2024-02-06 18:33:33,899 	Gloss Reference :	A B+C+D+E
2024-02-06 18:33:33,899 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 18:33:33,899 	Gloss Alignment :	         
2024-02-06 18:33:33,899 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 18:33:33,900 	Text Reference  :	**** ***** ******** the **** narendra modi stadium will  be    the  home  for the     ahmedabad-based franchise
2024-02-06 18:33:33,900 	Text Hypothesis :	this group includes the same as       the  delhi   final match semi final and weekend double          headers  
2024-02-06 18:33:33,901 	Text Alignment  :	I    I     I            I    S        S    S       S     S     S    S     S   S       S               S        
2024-02-06 18:33:33,901 ========================================================================================================================
2024-02-06 18:33:33,901 Logging Sequence: 176_154.00
2024-02-06 18:33:33,901 	Gloss Reference :	A B+C+D+E
2024-02-06 18:33:33,901 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 18:33:33,901 	Gloss Alignment :	         
2024-02-06 18:33:33,902 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 18:33:33,902 	Text Reference  :	***** dahiya could  potentially bring home india's second gold medal 
2024-02-06 18:33:33,903 	Text Hypothesis :	sadly the    indian team        lost  to   qualify for    the  finals
2024-02-06 18:33:33,903 	Text Alignment  :	I     S      S      S           S     S    S       S      S    S     
2024-02-06 18:33:33,903 ========================================================================================================================
2024-02-06 18:33:33,903 Logging Sequence: 94_2.00
2024-02-06 18:33:33,903 	Gloss Reference :	A B+C+D+E
2024-02-06 18:33:33,903 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 18:33:33,903 	Gloss Alignment :	         
2024-02-06 18:33:33,903 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 18:33:33,905 	Text Reference  :	***** ** *** ** the icc odi    men'    world cup 2023 will be hosted by india on    5th october 2023 
2024-02-06 18:33:33,905 	Text Hypothesis :	india is now at the *** women' cricket world cup **** **** ** ****** ** 6     balls two tough   balls
2024-02-06 18:33:33,905 	Text Alignment  :	I     I  I   I      D   S      S                 D    D    D  D      D  S     S     S   S       S    
2024-02-06 18:33:33,905 ========================================================================================================================
2024-02-06 18:33:33,905 Logging Sequence: 165_453.00
2024-02-06 18:33:33,906 	Gloss Reference :	A B+C+D+E
2024-02-06 18:33:33,906 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 18:33:33,906 	Gloss Alignment :	         
2024-02-06 18:33:33,906 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 18:33:33,907 	Text Reference  :	*** **** icc did not **** *** agree to sehwag' decision of  wearing a    numberless jersey
2024-02-06 18:33:33,907 	Text Hypothesis :	sai said he  did not know who is    a  strong  but      did not     find the        umpire
2024-02-06 18:33:33,908 	Text Alignment  :	I   I    S           I    I   S     S  S       S        S   S       S    S          S     
2024-02-06 18:33:33,908 ========================================================================================================================
2024-02-06 18:33:33,908 Logging Sequence: 139_46.00
2024-02-06 18:33:33,908 	Gloss Reference :	A B+C+D+E
2024-02-06 18:33:33,908 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 18:33:33,908 	Gloss Alignment :	         
2024-02-06 18:33:33,908 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 18:33:33,910 	Text Reference  :	everyone thought it would be a         one    sided match   because    morocco is       an amateur team    and belgium ranks 2nd in the        world
2024-02-06 18:33:33,910 	Text Hypothesis :	******** ******* ** ***** ** moroccans living in    belgium celebrated by      resorted to arson   rioting and ******* ***** *** ** destroying cars 
2024-02-06 18:33:33,910 	Text Alignment  :	D        D       D  D     D  S         S      S     S       S          S       S        S  S       S           D       D     D   D  S          S    
2024-02-06 18:33:33,910 ========================================================================================================================
2024-02-06 18:33:43,627 Epoch 445: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.39 
2024-02-06 18:33:43,627 EPOCH 446
2024-02-06 18:34:00,611 Epoch 446: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.31 
2024-02-06 18:34:00,612 EPOCH 447
2024-02-06 18:34:16,889 Epoch 447: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.33 
2024-02-06 18:34:16,889 EPOCH 448
2024-02-06 18:34:33,076 Epoch 448: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.22 
2024-02-06 18:34:33,077 EPOCH 449
2024-02-06 18:34:49,513 Epoch 449: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.14 
2024-02-06 18:34:49,513 EPOCH 450
2024-02-06 18:35:06,079 Epoch 450: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.14 
2024-02-06 18:35:06,080 EPOCH 451
2024-02-06 18:35:22,445 Epoch 451: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.10 
2024-02-06 18:35:22,445 EPOCH 452
2024-02-06 18:35:38,534 Epoch 452: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.07 
2024-02-06 18:35:38,534 EPOCH 453
2024-02-06 18:35:55,023 Epoch 453: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.06 
2024-02-06 18:35:55,023 EPOCH 454
2024-02-06 18:36:11,658 Epoch 454: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.03 
2024-02-06 18:36:11,659 EPOCH 455
2024-02-06 18:36:27,736 Epoch 455: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.05 
2024-02-06 18:36:27,737 EPOCH 456
2024-02-06 18:36:38,892 [Epoch: 456 Step: 00004100] Batch Recognition Loss:   0.004497 => Gls Tokens per Sec:      574 || Batch Translation Loss:   0.150436 => Txt Tokens per Sec:     1757 || Lr: 0.000100
2024-02-06 18:36:44,264 Epoch 456: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.08 
2024-02-06 18:36:44,264 EPOCH 457
2024-02-06 18:37:00,562 Epoch 457: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.07 
2024-02-06 18:37:00,562 EPOCH 458
2024-02-06 18:37:16,747 Epoch 458: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.04 
2024-02-06 18:37:16,748 EPOCH 459
2024-02-06 18:37:33,324 Epoch 459: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.02 
2024-02-06 18:37:33,325 EPOCH 460
2024-02-06 18:37:49,672 Epoch 460: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.99 
2024-02-06 18:37:49,672 EPOCH 461
2024-02-06 18:38:05,820 Epoch 461: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.07 
2024-02-06 18:38:05,821 EPOCH 462
2024-02-06 18:38:22,013 Epoch 462: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.00 
2024-02-06 18:38:22,014 EPOCH 463
2024-02-06 18:38:38,417 Epoch 463: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.97 
2024-02-06 18:38:38,417 EPOCH 464
2024-02-06 18:38:55,063 Epoch 464: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.97 
2024-02-06 18:38:55,064 EPOCH 465
2024-02-06 18:39:11,664 Epoch 465: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.00 
2024-02-06 18:39:11,665 EPOCH 466
2024-02-06 18:39:28,093 Epoch 466: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.98 
2024-02-06 18:39:28,093 EPOCH 467
2024-02-06 18:39:39,560 [Epoch: 467 Step: 00004200] Batch Recognition Loss:   0.003798 => Gls Tokens per Sec:      670 || Batch Translation Loss:   0.132766 => Txt Tokens per Sec:     1940 || Lr: 0.000100
2024-02-06 18:39:44,543 Epoch 467: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.95 
2024-02-06 18:39:44,543 EPOCH 468
2024-02-06 18:40:01,106 Epoch 468: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.94 
2024-02-06 18:40:01,106 EPOCH 469
2024-02-06 18:40:17,176 Epoch 469: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.98 
2024-02-06 18:40:17,177 EPOCH 470
2024-02-06 18:40:33,774 Epoch 470: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.93 
2024-02-06 18:40:33,775 EPOCH 471
2024-02-06 18:40:49,922 Epoch 471: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.92 
2024-02-06 18:40:49,923 EPOCH 472
2024-02-06 18:41:06,306 Epoch 472: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.92 
2024-02-06 18:41:06,307 EPOCH 473
2024-02-06 18:41:22,546 Epoch 473: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.96 
2024-02-06 18:41:22,546 EPOCH 474
2024-02-06 18:41:39,168 Epoch 474: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.95 
2024-02-06 18:41:39,169 EPOCH 475
2024-02-06 18:41:55,539 Epoch 475: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.99 
2024-02-06 18:41:55,540 EPOCH 476
2024-02-06 18:42:11,910 Epoch 476: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.95 
2024-02-06 18:42:11,911 EPOCH 477
2024-02-06 18:42:28,264 Epoch 477: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.09 
2024-02-06 18:42:28,264 EPOCH 478
2024-02-06 18:42:40,154 [Epoch: 478 Step: 00004300] Batch Recognition Loss:   0.005186 => Gls Tokens per Sec:      754 || Batch Translation Loss:   0.089509 => Txt Tokens per Sec:     2163 || Lr: 0.000100
2024-02-06 18:42:44,717 Epoch 478: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.94 
2024-02-06 18:42:44,717 EPOCH 479
2024-02-06 18:43:01,224 Epoch 479: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.98 
2024-02-06 18:43:01,225 EPOCH 480
2024-02-06 18:43:17,636 Epoch 480: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.01 
2024-02-06 18:43:17,636 EPOCH 481
2024-02-06 18:43:34,127 Epoch 481: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.04 
2024-02-06 18:43:34,127 EPOCH 482
2024-02-06 18:43:50,365 Epoch 482: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.01 
2024-02-06 18:43:50,366 EPOCH 483
2024-02-06 18:44:06,880 Epoch 483: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.98 
2024-02-06 18:44:06,881 EPOCH 484
2024-02-06 18:44:23,006 Epoch 484: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.98 
2024-02-06 18:44:23,007 EPOCH 485
2024-02-06 18:44:39,671 Epoch 485: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.98 
2024-02-06 18:44:39,672 EPOCH 486
2024-02-06 18:44:56,019 Epoch 486: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.99 
2024-02-06 18:44:56,020 EPOCH 487
2024-02-06 18:45:12,703 Epoch 487: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.02 
2024-02-06 18:45:12,704 EPOCH 488
2024-02-06 18:45:28,782 Epoch 488: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.17 
2024-02-06 18:45:28,783 EPOCH 489
2024-02-06 18:45:45,163 [Epoch: 489 Step: 00004400] Batch Recognition Loss:   0.001340 => Gls Tokens per Sec:      570 || Batch Translation Loss:   0.153856 => Txt Tokens per Sec:     1619 || Lr: 0.000100
2024-02-06 18:45:45,486 Epoch 489: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.24 
2024-02-06 18:45:45,487 EPOCH 490
2024-02-06 18:46:01,697 Epoch 490: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.16 
2024-02-06 18:46:01,697 EPOCH 491
2024-02-06 18:46:18,304 Epoch 491: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.27 
2024-02-06 18:46:18,305 EPOCH 492
2024-02-06 18:46:34,606 Epoch 492: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.16 
2024-02-06 18:46:34,607 EPOCH 493
2024-02-06 18:46:51,012 Epoch 493: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.17 
2024-02-06 18:46:51,012 EPOCH 494
2024-02-06 18:47:07,220 Epoch 494: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.11 
2024-02-06 18:47:07,220 EPOCH 495
2024-02-06 18:47:23,556 Epoch 495: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.11 
2024-02-06 18:47:23,557 EPOCH 496
2024-02-06 18:47:40,146 Epoch 496: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.43 
2024-02-06 18:47:40,146 EPOCH 497
2024-02-06 18:47:56,486 Epoch 497: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.31 
2024-02-06 18:47:56,487 EPOCH 498
2024-02-06 18:48:12,765 Epoch 498: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.20 
2024-02-06 18:48:12,765 EPOCH 499
2024-02-06 18:48:29,364 Epoch 499: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.27 
2024-02-06 18:48:29,364 EPOCH 500
2024-02-06 18:48:45,473 [Epoch: 500 Step: 00004500] Batch Recognition Loss:   0.001988 => Gls Tokens per Sec:      659 || Batch Translation Loss:   0.119016 => Txt Tokens per Sec:     1824 || Lr: 0.000100
2024-02-06 18:48:45,473 Epoch 500: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.22 
2024-02-06 18:48:45,473 EPOCH 501
2024-02-06 18:49:02,018 Epoch 501: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.23 
2024-02-06 18:49:02,019 EPOCH 502
2024-02-06 18:49:18,661 Epoch 502: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.29 
2024-02-06 18:49:18,662 EPOCH 503
2024-02-06 18:49:34,988 Epoch 503: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.13 
2024-02-06 18:49:34,989 EPOCH 504
2024-02-06 18:49:51,349 Epoch 504: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.08 
2024-02-06 18:49:51,349 EPOCH 505
2024-02-06 18:50:07,769 Epoch 505: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.08 
2024-02-06 18:50:07,770 EPOCH 506
2024-02-06 18:50:23,942 Epoch 506: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.15 
2024-02-06 18:50:23,942 EPOCH 507
2024-02-06 18:50:40,144 Epoch 507: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.99 
2024-02-06 18:50:40,145 EPOCH 508
2024-02-06 18:50:56,489 Epoch 508: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.02 
2024-02-06 18:50:56,490 EPOCH 509
2024-02-06 18:51:12,793 Epoch 509: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.01 
2024-02-06 18:51:12,794 EPOCH 510
2024-02-06 18:51:29,393 Epoch 510: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.15 
2024-02-06 18:51:29,393 EPOCH 511
2024-02-06 18:51:45,838 Epoch 511: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.22 
2024-02-06 18:51:45,839 EPOCH 512
2024-02-06 18:51:46,316 [Epoch: 512 Step: 00004600] Batch Recognition Loss:   0.001154 => Gls Tokens per Sec:     2683 || Batch Translation Loss:   0.129299 => Txt Tokens per Sec:     6757 || Lr: 0.000100
2024-02-06 18:52:02,433 Epoch 512: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.12 
2024-02-06 18:52:02,433 EPOCH 513
2024-02-06 18:52:18,589 Epoch 513: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-06 18:52:18,590 EPOCH 514
2024-02-06 18:52:34,985 Epoch 514: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.20 
2024-02-06 18:52:34,986 EPOCH 515
2024-02-06 18:52:51,120 Epoch 515: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.18 
2024-02-06 18:52:51,121 EPOCH 516
2024-02-06 18:53:07,784 Epoch 516: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.14 
2024-02-06 18:53:07,785 EPOCH 517
2024-02-06 18:53:23,986 Epoch 517: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.01 
2024-02-06 18:53:23,987 EPOCH 518
2024-02-06 18:53:40,528 Epoch 518: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.98 
2024-02-06 18:53:40,529 EPOCH 519
2024-02-06 18:53:57,163 Epoch 519: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.04 
2024-02-06 18:53:57,164 EPOCH 520
2024-02-06 18:54:13,679 Epoch 520: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.96 
2024-02-06 18:54:13,680 EPOCH 521
2024-02-06 18:54:29,939 Epoch 521: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.93 
2024-02-06 18:54:29,940 EPOCH 522
2024-02-06 18:54:46,255 Epoch 522: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.89 
2024-02-06 18:54:46,256 EPOCH 523
2024-02-06 18:54:50,918 [Epoch: 523 Step: 00004700] Batch Recognition Loss:   0.001016 => Gls Tokens per Sec:      356 || Batch Translation Loss:   0.112218 => Txt Tokens per Sec:     1000 || Lr: 0.000100
2024-02-06 18:55:02,458 Epoch 523: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.89 
2024-02-06 18:55:02,458 EPOCH 524
2024-02-06 18:55:19,161 Epoch 524: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.87 
2024-02-06 18:55:19,161 EPOCH 525
2024-02-06 18:55:35,379 Epoch 525: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.82 
2024-02-06 18:55:35,380 EPOCH 526
2024-02-06 18:55:51,693 Epoch 526: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.83 
2024-02-06 18:55:51,693 EPOCH 527
2024-02-06 18:56:07,956 Epoch 527: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.88 
2024-02-06 18:56:07,957 EPOCH 528
2024-02-06 18:56:24,340 Epoch 528: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.91 
2024-02-06 18:56:24,341 EPOCH 529
2024-02-06 18:56:40,670 Epoch 529: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.83 
2024-02-06 18:56:40,671 EPOCH 530
2024-02-06 18:56:56,867 Epoch 530: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.88 
2024-02-06 18:56:56,867 EPOCH 531
2024-02-06 18:57:12,944 Epoch 531: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.85 
2024-02-06 18:57:12,945 EPOCH 532
2024-02-06 18:57:29,454 Epoch 532: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.84 
2024-02-06 18:57:29,455 EPOCH 533
2024-02-06 18:57:45,913 Epoch 533: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.80 
2024-02-06 18:57:45,913 EPOCH 534
2024-02-06 18:57:47,319 [Epoch: 534 Step: 00004800] Batch Recognition Loss:   0.006695 => Gls Tokens per Sec:     2736 || Batch Translation Loss:   0.055763 => Txt Tokens per Sec:     6723 || Lr: 0.000100
2024-02-06 18:58:02,237 Epoch 534: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.98 
2024-02-06 18:58:02,238 EPOCH 535
2024-02-06 18:58:18,649 Epoch 535: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.99 
2024-02-06 18:58:18,649 EPOCH 536
2024-02-06 18:58:34,854 Epoch 536: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.01 
2024-02-06 18:58:34,855 EPOCH 537
2024-02-06 18:58:51,032 Epoch 537: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.94 
2024-02-06 18:58:51,032 EPOCH 538
2024-02-06 18:59:07,402 Epoch 538: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.92 
2024-02-06 18:59:07,403 EPOCH 539
2024-02-06 18:59:23,521 Epoch 539: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.90 
2024-02-06 18:59:23,522 EPOCH 540
2024-02-06 18:59:40,049 Epoch 540: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.97 
2024-02-06 18:59:40,050 EPOCH 541
2024-02-06 18:59:56,351 Epoch 541: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.01 
2024-02-06 18:59:56,351 EPOCH 542
2024-02-06 19:00:12,700 Epoch 542: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.98 
2024-02-06 19:00:12,700 EPOCH 543
2024-02-06 19:00:29,082 Epoch 543: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.98 
2024-02-06 19:00:29,083 EPOCH 544
2024-02-06 19:00:45,569 Epoch 544: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-06 19:00:45,570 EPOCH 545
2024-02-06 19:00:56,958 [Epoch: 545 Step: 00004900] Batch Recognition Loss:   0.001765 => Gls Tokens per Sec:      371 || Batch Translation Loss:   0.180295 => Txt Tokens per Sec:     1118 || Lr: 0.000100
2024-02-06 19:01:01,962 Epoch 545: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.14 
2024-02-06 19:01:01,962 EPOCH 546
2024-02-06 19:01:18,315 Epoch 546: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.08 
2024-02-06 19:01:18,316 EPOCH 547
2024-02-06 19:01:34,246 Epoch 547: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.02 
2024-02-06 19:01:34,246 EPOCH 548
2024-02-06 19:01:50,895 Epoch 548: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.15 
2024-02-06 19:01:50,895 EPOCH 549
2024-02-06 19:02:07,157 Epoch 549: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.18 
2024-02-06 19:02:07,158 EPOCH 550
2024-02-06 19:02:23,596 Epoch 550: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.37 
2024-02-06 19:02:23,597 EPOCH 551
2024-02-06 19:02:39,851 Epoch 551: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.83 
2024-02-06 19:02:39,852 EPOCH 552
2024-02-06 19:02:55,893 Epoch 552: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.23 
2024-02-06 19:02:55,893 EPOCH 553
2024-02-06 19:03:12,391 Epoch 553: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.70 
2024-02-06 19:03:12,392 EPOCH 554
2024-02-06 19:03:28,635 Epoch 554: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.03 
2024-02-06 19:03:28,636 EPOCH 555
2024-02-06 19:03:44,918 Epoch 555: Total Training Recognition Loss 0.05  Total Training Translation Loss 8.31 
2024-02-06 19:03:44,918 EPOCH 556
2024-02-06 19:03:47,123 [Epoch: 556 Step: 00005000] Batch Recognition Loss:   0.006058 => Gls Tokens per Sec:     2906 || Batch Translation Loss:   0.940291 => Txt Tokens per Sec:     7567 || Lr: 0.000100
2024-02-06 19:04:01,052 Epoch 556: Total Training Recognition Loss 0.10  Total Training Translation Loss 10.37 
2024-02-06 19:04:01,052 EPOCH 557
2024-02-06 19:04:17,444 Epoch 557: Total Training Recognition Loss 0.16  Total Training Translation Loss 14.99 
2024-02-06 19:04:17,445 EPOCH 558
2024-02-06 19:04:33,841 Epoch 558: Total Training Recognition Loss 0.63  Total Training Translation Loss 12.92 
2024-02-06 19:04:33,841 EPOCH 559
2024-02-06 19:04:50,256 Epoch 559: Total Training Recognition Loss 2.24  Total Training Translation Loss 9.91 
2024-02-06 19:04:50,257 EPOCH 560
2024-02-06 19:05:06,766 Epoch 560: Total Training Recognition Loss 0.61  Total Training Translation Loss 7.54 
2024-02-06 19:05:06,766 EPOCH 561
2024-02-06 19:05:22,962 Epoch 561: Total Training Recognition Loss 0.15  Total Training Translation Loss 4.69 
2024-02-06 19:05:22,963 EPOCH 562
2024-02-06 19:05:39,226 Epoch 562: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.81 
2024-02-06 19:05:39,226 EPOCH 563
2024-02-06 19:05:55,618 Epoch 563: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.01 
2024-02-06 19:05:55,619 EPOCH 564
2024-02-06 19:06:11,737 Epoch 564: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.52 
2024-02-06 19:06:11,738 EPOCH 565
2024-02-06 19:06:28,019 Epoch 565: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.32 
2024-02-06 19:06:28,019 EPOCH 566
2024-02-06 19:06:44,235 Epoch 566: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.30 
2024-02-06 19:06:44,235 EPOCH 567
2024-02-06 19:06:59,133 [Epoch: 567 Step: 00005100] Batch Recognition Loss:   0.002299 => Gls Tokens per Sec:      455 || Batch Translation Loss:   0.080300 => Txt Tokens per Sec:     1321 || Lr: 0.000100
2024-02-06 19:07:00,560 Epoch 567: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.13 
2024-02-06 19:07:00,561 EPOCH 568
2024-02-06 19:07:16,586 Epoch 568: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.04 
2024-02-06 19:07:16,587 EPOCH 569
2024-02-06 19:07:33,054 Epoch 569: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.06 
2024-02-06 19:07:33,054 EPOCH 570
2024-02-06 19:07:49,190 Epoch 570: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.89 
2024-02-06 19:07:49,191 EPOCH 571
2024-02-06 19:08:05,766 Epoch 571: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.87 
2024-02-06 19:08:05,767 EPOCH 572
2024-02-06 19:08:22,452 Epoch 572: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.83 
2024-02-06 19:08:22,452 EPOCH 573
2024-02-06 19:08:38,590 Epoch 573: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.82 
2024-02-06 19:08:38,591 EPOCH 574
2024-02-06 19:08:54,889 Epoch 574: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.77 
2024-02-06 19:08:54,890 EPOCH 575
2024-02-06 19:09:11,292 Epoch 575: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.72 
2024-02-06 19:09:11,293 EPOCH 576
2024-02-06 19:09:27,684 Epoch 576: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.67 
2024-02-06 19:09:27,684 EPOCH 577
2024-02-06 19:09:44,035 Epoch 577: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.63 
2024-02-06 19:09:44,036 EPOCH 578
2024-02-06 19:09:59,683 [Epoch: 578 Step: 00005200] Batch Recognition Loss:   0.002838 => Gls Tokens per Sec:      515 || Batch Translation Loss:   0.028973 => Txt Tokens per Sec:     1460 || Lr: 0.000100
2024-02-06 19:10:00,599 Epoch 578: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.66 
2024-02-06 19:10:00,600 EPOCH 579
2024-02-06 19:10:16,858 Epoch 579: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.66 
2024-02-06 19:10:16,859 EPOCH 580
2024-02-06 19:10:33,175 Epoch 580: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.61 
2024-02-06 19:10:33,175 EPOCH 581
2024-02-06 19:10:49,527 Epoch 581: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-06 19:10:49,528 EPOCH 582
2024-02-06 19:11:05,757 Epoch 582: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.61 
2024-02-06 19:11:05,757 EPOCH 583
2024-02-06 19:11:22,018 Epoch 583: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-06 19:11:22,019 EPOCH 584
2024-02-06 19:11:38,385 Epoch 584: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.61 
2024-02-06 19:11:38,385 EPOCH 585
2024-02-06 19:11:54,499 Epoch 585: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.58 
2024-02-06 19:11:54,500 EPOCH 586
2024-02-06 19:12:10,872 Epoch 586: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.61 
2024-02-06 19:12:10,873 EPOCH 587
2024-02-06 19:12:27,225 Epoch 587: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-06 19:12:27,226 EPOCH 588
2024-02-06 19:12:43,513 Epoch 588: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.59 
2024-02-06 19:12:43,514 EPOCH 589
2024-02-06 19:12:59,571 [Epoch: 589 Step: 00005300] Batch Recognition Loss:   0.001430 => Gls Tokens per Sec:      582 || Batch Translation Loss:   0.023602 => Txt Tokens per Sec:     1686 || Lr: 0.000100
2024-02-06 19:12:59,842 Epoch 589: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.56 
2024-02-06 19:12:59,842 EPOCH 590
2024-02-06 19:13:16,249 Epoch 590: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-06 19:13:16,249 EPOCH 591
2024-02-06 19:13:32,666 Epoch 591: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-06 19:13:32,666 EPOCH 592
2024-02-06 19:13:49,016 Epoch 592: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-06 19:13:49,016 EPOCH 593
2024-02-06 19:14:05,511 Epoch 593: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.55 
2024-02-06 19:14:05,511 EPOCH 594
2024-02-06 19:14:21,858 Epoch 594: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-06 19:14:21,859 EPOCH 595
2024-02-06 19:14:38,109 Epoch 595: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-06 19:14:38,110 EPOCH 596
2024-02-06 19:14:54,470 Epoch 596: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-06 19:14:54,470 EPOCH 597
2024-02-06 19:15:10,764 Epoch 597: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-06 19:15:10,765 EPOCH 598
2024-02-06 19:15:27,031 Epoch 598: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-06 19:15:27,031 EPOCH 599
2024-02-06 19:15:43,316 Epoch 599: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-06 19:15:43,316 EPOCH 600
2024-02-06 19:15:59,617 [Epoch: 600 Step: 00005400] Batch Recognition Loss:   0.001429 => Gls Tokens per Sec:      652 || Batch Translation Loss:   0.065746 => Txt Tokens per Sec:     1803 || Lr: 0.000100
2024-02-06 19:15:59,617 Epoch 600: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-06 19:15:59,618 EPOCH 601
2024-02-06 19:16:16,068 Epoch 601: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-06 19:16:16,068 EPOCH 602
2024-02-06 19:16:32,312 Epoch 602: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-06 19:16:32,313 EPOCH 603
2024-02-06 19:16:48,893 Epoch 603: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-06 19:16:48,893 EPOCH 604
2024-02-06 19:17:05,199 Epoch 604: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.49 
2024-02-06 19:17:05,199 EPOCH 605
2024-02-06 19:17:21,577 Epoch 605: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.49 
2024-02-06 19:17:21,578 EPOCH 606
2024-02-06 19:17:37,785 Epoch 606: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-06 19:17:37,786 EPOCH 607
2024-02-06 19:17:54,109 Epoch 607: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-06 19:17:54,110 EPOCH 608
2024-02-06 19:18:10,535 Epoch 608: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-06 19:18:10,535 EPOCH 609
2024-02-06 19:18:26,670 Epoch 609: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-06 19:18:26,671 EPOCH 610
2024-02-06 19:18:42,678 Epoch 610: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.51 
2024-02-06 19:18:42,679 EPOCH 611
2024-02-06 19:18:59,122 Epoch 611: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 19:18:59,123 EPOCH 612
2024-02-06 19:19:03,397 [Epoch: 612 Step: 00005500] Batch Recognition Loss:   0.001099 => Gls Tokens per Sec:       89 || Batch Translation Loss:   0.022428 => Txt Tokens per Sec:      318 || Lr: 0.000100
2024-02-06 19:19:15,297 Epoch 612: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-06 19:19:15,297 EPOCH 613
2024-02-06 19:19:31,318 Epoch 613: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-06 19:19:31,318 EPOCH 614
2024-02-06 19:19:47,936 Epoch 614: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.45 
2024-02-06 19:19:47,936 EPOCH 615
2024-02-06 19:20:04,568 Epoch 615: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-06 19:20:04,568 EPOCH 616
2024-02-06 19:20:20,994 Epoch 616: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-06 19:20:20,994 EPOCH 617
2024-02-06 19:20:37,229 Epoch 617: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-06 19:20:37,229 EPOCH 618
2024-02-06 19:20:53,659 Epoch 618: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-06 19:20:53,659 EPOCH 619
2024-02-06 19:21:10,108 Epoch 619: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-06 19:21:10,109 EPOCH 620
2024-02-06 19:21:26,468 Epoch 620: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-06 19:21:26,468 EPOCH 621
2024-02-06 19:21:42,575 Epoch 621: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-06 19:21:42,576 EPOCH 622
2024-02-06 19:21:58,790 Epoch 622: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.51 
2024-02-06 19:21:58,791 EPOCH 623
2024-02-06 19:21:59,548 [Epoch: 623 Step: 00005600] Batch Recognition Loss:   0.005292 => Gls Tokens per Sec:     3386 || Batch Translation Loss:   0.037709 => Txt Tokens per Sec:     6812 || Lr: 0.000100
2024-02-06 19:22:15,306 Epoch 623: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-06 19:22:15,307 EPOCH 624
2024-02-06 19:22:31,947 Epoch 624: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.49 
2024-02-06 19:22:31,947 EPOCH 625
2024-02-06 19:22:48,276 Epoch 625: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 19:22:48,277 EPOCH 626
2024-02-06 19:23:04,746 Epoch 626: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.45 
2024-02-06 19:23:04,747 EPOCH 627
2024-02-06 19:23:20,724 Epoch 627: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.43 
2024-02-06 19:23:20,725 EPOCH 628
2024-02-06 19:23:37,100 Epoch 628: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 19:23:37,100 EPOCH 629
2024-02-06 19:23:53,112 Epoch 629: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 19:23:53,112 EPOCH 630
2024-02-06 19:24:09,818 Epoch 630: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 19:24:09,818 EPOCH 631
2024-02-06 19:24:26,088 Epoch 631: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 19:24:26,089 EPOCH 632
2024-02-06 19:24:42,578 Epoch 632: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.44 
2024-02-06 19:24:42,579 EPOCH 633
2024-02-06 19:24:58,882 Epoch 633: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.43 
2024-02-06 19:24:58,883 EPOCH 634
2024-02-06 19:25:09,475 [Epoch: 634 Step: 00005700] Batch Recognition Loss:   0.002628 => Gls Tokens per Sec:      278 || Batch Translation Loss:   0.065867 => Txt Tokens per Sec:      811 || Lr: 0.000100
2024-02-06 19:25:15,308 Epoch 634: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 19:25:15,309 EPOCH 635
2024-02-06 19:25:31,631 Epoch 635: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 19:25:31,632 EPOCH 636
2024-02-06 19:25:48,014 Epoch 636: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 19:25:48,015 EPOCH 637
2024-02-06 19:26:04,427 Epoch 637: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-06 19:26:04,428 EPOCH 638
2024-02-06 19:26:21,115 Epoch 638: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 19:26:21,116 EPOCH 639
2024-02-06 19:26:36,995 Epoch 639: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 19:26:36,995 EPOCH 640
2024-02-06 19:26:53,243 Epoch 640: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-06 19:26:53,244 EPOCH 641
2024-02-06 19:27:09,439 Epoch 641: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 19:27:09,439 EPOCH 642
2024-02-06 19:27:26,193 Epoch 642: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 19:27:26,194 EPOCH 643
2024-02-06 19:27:42,358 Epoch 643: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 19:27:42,358 EPOCH 644
2024-02-06 19:27:58,709 Epoch 644: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 19:27:58,709 EPOCH 645
2024-02-06 19:28:04,537 [Epoch: 645 Step: 00005800] Batch Recognition Loss:   0.001008 => Gls Tokens per Sec:      724 || Batch Translation Loss:   0.040563 => Txt Tokens per Sec:     1966 || Lr: 0.000100
2024-02-06 19:28:15,219 Epoch 645: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.38 
2024-02-06 19:28:15,219 EPOCH 646
2024-02-06 19:28:31,604 Epoch 646: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.39 
2024-02-06 19:28:31,605 EPOCH 647
2024-02-06 19:28:47,745 Epoch 647: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 19:28:47,746 EPOCH 648
2024-02-06 19:29:04,426 Epoch 648: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 19:29:04,427 EPOCH 649
2024-02-06 19:29:20,575 Epoch 649: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.42 
2024-02-06 19:29:20,576 EPOCH 650
2024-02-06 19:29:37,294 Epoch 650: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.40 
2024-02-06 19:29:37,295 EPOCH 651
2024-02-06 19:29:53,429 Epoch 651: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-06 19:29:53,430 EPOCH 652
2024-02-06 19:30:09,987 Epoch 652: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-06 19:30:09,988 EPOCH 653
2024-02-06 19:30:26,202 Epoch 653: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.80 
2024-02-06 19:30:26,203 EPOCH 654
2024-02-06 19:30:42,690 Epoch 654: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-06 19:30:42,691 EPOCH 655
2024-02-06 19:30:58,985 Epoch 655: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.76 
2024-02-06 19:30:58,986 EPOCH 656
2024-02-06 19:31:13,737 [Epoch: 656 Step: 00005900] Batch Recognition Loss:   0.002515 => Gls Tokens per Sec:      373 || Batch Translation Loss:   0.073978 => Txt Tokens per Sec:     1152 || Lr: 0.000100
2024-02-06 19:31:15,353 Epoch 656: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-06 19:31:15,353 EPOCH 657
2024-02-06 19:31:31,473 Epoch 657: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-06 19:31:31,473 EPOCH 658
2024-02-06 19:31:47,730 Epoch 658: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-06 19:31:47,730 EPOCH 659
2024-02-06 19:32:04,043 Epoch 659: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.60 
2024-02-06 19:32:04,044 EPOCH 660
2024-02-06 19:32:20,372 Epoch 660: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-06 19:32:20,373 EPOCH 661
2024-02-06 19:32:36,923 Epoch 661: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-06 19:32:36,924 EPOCH 662
2024-02-06 19:32:53,101 Epoch 662: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-06 19:32:53,101 EPOCH 663
2024-02-06 19:33:09,352 Epoch 663: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-06 19:33:09,353 EPOCH 664
2024-02-06 19:33:25,549 Epoch 664: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.56 
2024-02-06 19:33:25,549 EPOCH 665
2024-02-06 19:33:41,916 Epoch 665: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 19:33:41,917 EPOCH 666
2024-02-06 19:33:58,333 Epoch 666: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-06 19:33:58,333 EPOCH 667
2024-02-06 19:34:13,558 [Epoch: 667 Step: 00006000] Batch Recognition Loss:   0.001245 => Gls Tokens per Sec:      445 || Batch Translation Loss:   0.063381 => Txt Tokens per Sec:     1350 || Lr: 0.000100
2024-02-06 19:35:21,527 Validation result at epoch 667, step     6000: duration: 67.9689s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.55238	Translation Loss: 88206.43750	PPL: 6701.39502
	Eval Metric: BLEU
	WER 5.79	(DEL: 0.00,	INS: 0.00,	SUB: 5.79)
	BLEU-4 0.41	(BLEU-1: 11.80,	BLEU-2: 3.77,	BLEU-3: 1.26,	BLEU-4: 0.41)
	CHRF 17.22	ROUGE 9.87
2024-02-06 19:35:21,529 Logging Recognition and Translation Outputs
2024-02-06 19:35:21,529 ========================================================================================================================
2024-02-06 19:35:21,530 Logging Sequence: 160_153.00
2024-02-06 19:35:21,530 	Gloss Reference :	A B+C+D+E
2024-02-06 19:35:21,530 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 19:35:21,530 	Gloss Alignment :	         
2024-02-06 19:35:21,530 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 19:35:21,534 	Text Reference  :	i have no hard feelings towards rohit sharma and he   will  always   have my    full support as    he   is my   teammate
2024-02-06 19:35:21,534 	Text Hypothesis :	* **** ** **** ******** ******* ***** ****** *** this group includes the  first time when    kohli made a  t20i captain 
2024-02-06 19:35:21,534 	Text Alignment  :	D D    D  D    D        D       D     D      D   S    S     S        S    S     S    S       S     S    S  S    S       
2024-02-06 19:35:21,534 ========================================================================================================================
2024-02-06 19:35:21,535 Logging Sequence: 103_253.00
2024-02-06 19:35:21,535 	Gloss Reference :	A B+C+D+E
2024-02-06 19:35:21,535 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 19:35:21,535 	Gloss Alignment :	         
2024-02-06 19:35:21,535 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 19:35:21,536 	Text Reference  :	canada is *** *** ***** ***** ** 3rd  with 92 medals
2024-02-06 19:35:21,536 	Text Hypothesis :	this   is not the first count of them in   an games 
2024-02-06 19:35:21,536 	Text Alignment  :	S         I   I   I     I     I  S    S    S  S     
2024-02-06 19:35:21,536 ========================================================================================================================
2024-02-06 19:35:21,536 Logging Sequence: 155_25.00
2024-02-06 19:35:21,536 	Gloss Reference :	A B+C+D+E
2024-02-06 19:35:21,537 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 19:35:21,537 	Gloss Alignment :	         
2024-02-06 19:35:21,537 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 19:35:21,538 	Text Reference  :	this is because taliban overthrew the afghan government and took over   the country
2024-02-06 19:35:21,538 	Text Hypothesis :	**** ** ******* ******* i         am  very   grateful   to  my   family as  well   
2024-02-06 19:35:21,538 	Text Alignment  :	D    D  D       D       S         S   S      S          S   S    S      S   S      
2024-02-06 19:35:21,538 ========================================================================================================================
2024-02-06 19:35:21,538 Logging Sequence: 81_105.00
2024-02-06 19:35:21,539 	Gloss Reference :	A B+C+D+E
2024-02-06 19:35:21,539 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 19:35:21,539 	Gloss Alignment :	         
2024-02-06 19:35:21,539 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 19:35:21,541 	Text Reference  :	** ** *** ***** dhoni was tagged in      multiple such posts as       he    was the      brand   ambassador
2024-02-06 19:35:21,541 	Text Hypothesis :	it is not known if    any court  seeking to       do   so    amrapali group so  amrapali group's sports    
2024-02-06 19:35:21,541 	Text Alignment  :	I  I  I   I     S     S   S      S       S        S    S     S        S     S   S        S       S         
2024-02-06 19:35:21,541 ========================================================================================================================
2024-02-06 19:35:21,541 Logging Sequence: 105_136.00
2024-02-06 19:35:21,541 	Gloss Reference :	A B+C+D+E
2024-02-06 19:35:21,541 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 19:35:21,542 	Gloss Alignment :	         
2024-02-06 19:35:21,542 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 19:35:21,542 	Text Reference  :	beating him once is my   biggest dream    
2024-02-06 19:35:21,542 	Text Hypothesis :	******* *** **** ** what a       wonderful
2024-02-06 19:35:21,542 	Text Alignment  :	D       D   D    D  S    S       S        
2024-02-06 19:35:21,542 ========================================================================================================================
2024-02-06 19:35:22,976 Epoch 667: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-06 19:35:22,976 EPOCH 668
2024-02-06 19:35:39,562 Epoch 668: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-06 19:35:39,563 EPOCH 669
2024-02-06 19:35:55,977 Epoch 669: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-06 19:35:55,977 EPOCH 670
2024-02-06 19:36:12,318 Epoch 670: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-06 19:36:12,318 EPOCH 671
2024-02-06 19:36:28,663 Epoch 671: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-06 19:36:28,664 EPOCH 672
2024-02-06 19:36:44,950 Epoch 672: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-06 19:36:44,951 EPOCH 673
2024-02-06 19:37:01,472 Epoch 673: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-06 19:37:01,473 EPOCH 674
2024-02-06 19:37:17,652 Epoch 674: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 19:37:17,653 EPOCH 675
2024-02-06 19:37:33,936 Epoch 675: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-06 19:37:33,936 EPOCH 676
2024-02-06 19:37:50,044 Epoch 676: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 19:37:50,045 EPOCH 677
2024-02-06 19:38:06,321 Epoch 677: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-06 19:38:06,322 EPOCH 678
2024-02-06 19:38:21,814 [Epoch: 678 Step: 00006100] Batch Recognition Loss:   0.000361 => Gls Tokens per Sec:      520 || Batch Translation Loss:   0.024810 => Txt Tokens per Sec:     1501 || Lr: 0.000100
2024-02-06 19:38:22,521 Epoch 678: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 19:38:22,521 EPOCH 679
2024-02-06 19:38:39,204 Epoch 679: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-06 19:38:39,204 EPOCH 680
2024-02-06 19:38:55,429 Epoch 680: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 19:38:55,429 EPOCH 681
2024-02-06 19:39:12,062 Epoch 681: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.43 
2024-02-06 19:39:12,063 EPOCH 682
2024-02-06 19:39:28,328 Epoch 682: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 19:39:28,328 EPOCH 683
2024-02-06 19:39:44,642 Epoch 683: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 19:39:44,643 EPOCH 684
2024-02-06 19:40:01,068 Epoch 684: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 19:40:01,069 EPOCH 685
2024-02-06 19:40:17,263 Epoch 685: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 19:40:17,263 EPOCH 686
2024-02-06 19:40:33,241 Epoch 686: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 19:40:33,241 EPOCH 687
2024-02-06 19:40:49,622 Epoch 687: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-06 19:40:49,623 EPOCH 688
2024-02-06 19:41:05,803 Epoch 688: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-06 19:41:05,803 EPOCH 689
2024-02-06 19:41:21,690 [Epoch: 689 Step: 00006200] Batch Recognition Loss:   0.002973 => Gls Tokens per Sec:      588 || Batch Translation Loss:   0.107330 => Txt Tokens per Sec:     1644 || Lr: 0.000100
2024-02-06 19:41:22,076 Epoch 689: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-06 19:41:22,076 EPOCH 690
2024-02-06 19:41:38,374 Epoch 690: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-06 19:41:38,375 EPOCH 691
2024-02-06 19:41:54,788 Epoch 691: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-06 19:41:54,789 EPOCH 692
2024-02-06 19:42:11,106 Epoch 692: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 19:42:11,107 EPOCH 693
2024-02-06 19:42:27,638 Epoch 693: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.51 
2024-02-06 19:42:27,639 EPOCH 694
2024-02-06 19:42:43,725 Epoch 694: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-06 19:42:43,725 EPOCH 695
2024-02-06 19:42:59,893 Epoch 695: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 19:42:59,894 EPOCH 696
2024-02-06 19:43:16,182 Epoch 696: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-06 19:43:16,183 EPOCH 697
2024-02-06 19:43:32,726 Epoch 697: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-06 19:43:32,726 EPOCH 698
2024-02-06 19:43:49,017 Epoch 698: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-06 19:43:49,018 EPOCH 699
2024-02-06 19:44:05,535 Epoch 699: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-06 19:44:05,536 EPOCH 700
2024-02-06 19:44:21,845 [Epoch: 700 Step: 00006300] Batch Recognition Loss:   0.001453 => Gls Tokens per Sec:      651 || Batch Translation Loss:   0.078498 => Txt Tokens per Sec:     1802 || Lr: 0.000100
2024-02-06 19:44:21,846 Epoch 700: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-06 19:44:21,846 EPOCH 701
2024-02-06 19:44:38,461 Epoch 701: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.85 
2024-02-06 19:44:38,462 EPOCH 702
2024-02-06 19:44:54,478 Epoch 702: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-06 19:44:54,478 EPOCH 703
2024-02-06 19:45:11,033 Epoch 703: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-06 19:45:11,034 EPOCH 704
2024-02-06 19:45:27,145 Epoch 704: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.93 
2024-02-06 19:45:27,146 EPOCH 705
2024-02-06 19:45:43,921 Epoch 705: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-06 19:45:43,922 EPOCH 706
2024-02-06 19:46:00,526 Epoch 706: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.30 
2024-02-06 19:46:00,526 EPOCH 707
2024-02-06 19:46:16,765 Epoch 707: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.53 
2024-02-06 19:46:16,765 EPOCH 708
2024-02-06 19:46:33,343 Epoch 708: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.49 
2024-02-06 19:46:33,344 EPOCH 709
2024-02-06 19:46:49,591 Epoch 709: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.81 
2024-02-06 19:46:49,592 EPOCH 710
2024-02-06 19:47:06,228 Epoch 710: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.66 
2024-02-06 19:47:06,229 EPOCH 711
2024-02-06 19:47:22,426 Epoch 711: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.65 
2024-02-06 19:47:22,426 EPOCH 712
2024-02-06 19:47:22,652 [Epoch: 712 Step: 00006400] Batch Recognition Loss:   0.006864 => Gls Tokens per Sec:     5681 || Batch Translation Loss:   0.271701 => Txt Tokens per Sec:    10119 || Lr: 0.000100
2024-02-06 19:47:38,920 Epoch 712: Total Training Recognition Loss 0.07  Total Training Translation Loss 13.58 
2024-02-06 19:47:38,921 EPOCH 713
2024-02-06 19:47:55,072 Epoch 713: Total Training Recognition Loss 0.08  Total Training Translation Loss 9.52 
2024-02-06 19:47:55,073 EPOCH 714
2024-02-06 19:48:11,311 Epoch 714: Total Training Recognition Loss 0.17  Total Training Translation Loss 10.76 
2024-02-06 19:48:11,312 EPOCH 715
2024-02-06 19:48:27,856 Epoch 715: Total Training Recognition Loss 0.16  Total Training Translation Loss 9.35 
2024-02-06 19:48:27,857 EPOCH 716
2024-02-06 19:48:43,943 Epoch 716: Total Training Recognition Loss 0.24  Total Training Translation Loss 6.02 
2024-02-06 19:48:43,943 EPOCH 717
2024-02-06 19:49:00,408 Epoch 717: Total Training Recognition Loss 0.16  Total Training Translation Loss 3.73 
2024-02-06 19:49:00,408 EPOCH 718
2024-02-06 19:49:16,631 Epoch 718: Total Training Recognition Loss 0.20  Total Training Translation Loss 2.46 
2024-02-06 19:49:16,631 EPOCH 719
2024-02-06 19:49:32,972 Epoch 719: Total Training Recognition Loss 0.41  Total Training Translation Loss 1.43 
2024-02-06 19:49:32,973 EPOCH 720
2024-02-06 19:49:49,593 Epoch 720: Total Training Recognition Loss 0.62  Total Training Translation Loss 1.09 
2024-02-06 19:49:49,594 EPOCH 721
2024-02-06 19:50:05,835 Epoch 721: Total Training Recognition Loss 1.27  Total Training Translation Loss 1.06 
2024-02-06 19:50:05,836 EPOCH 722
2024-02-06 19:50:22,064 Epoch 722: Total Training Recognition Loss 6.18  Total Training Translation Loss 1.55 
2024-02-06 19:50:22,064 EPOCH 723
2024-02-06 19:50:27,364 [Epoch: 723 Step: 00006500] Batch Recognition Loss:   0.030852 => Gls Tokens per Sec:      313 || Batch Translation Loss:   0.374455 => Txt Tokens per Sec:      985 || Lr: 0.000100
2024-02-06 19:50:38,710 Epoch 723: Total Training Recognition Loss 10.87  Total Training Translation Loss 2.99 
2024-02-06 19:50:38,711 EPOCH 724
2024-02-06 19:50:55,250 Epoch 724: Total Training Recognition Loss 8.52  Total Training Translation Loss 3.17 
2024-02-06 19:50:55,251 EPOCH 725
2024-02-06 19:51:11,259 Epoch 725: Total Training Recognition Loss 2.44  Total Training Translation Loss 2.37 
2024-02-06 19:51:11,259 EPOCH 726
2024-02-06 19:51:27,598 Epoch 726: Total Training Recognition Loss 0.70  Total Training Translation Loss 1.70 
2024-02-06 19:51:27,599 EPOCH 727
2024-02-06 19:51:43,741 Epoch 727: Total Training Recognition Loss 0.39  Total Training Translation Loss 1.22 
2024-02-06 19:51:43,742 EPOCH 728
2024-02-06 19:52:00,107 Epoch 728: Total Training Recognition Loss 0.20  Total Training Translation Loss 0.95 
2024-02-06 19:52:00,108 EPOCH 729
2024-02-06 19:52:16,476 Epoch 729: Total Training Recognition Loss 0.14  Total Training Translation Loss 0.76 
2024-02-06 19:52:16,476 EPOCH 730
2024-02-06 19:52:32,737 Epoch 730: Total Training Recognition Loss 0.11  Total Training Translation Loss 0.70 
2024-02-06 19:52:32,738 EPOCH 731
2024-02-06 19:52:49,078 Epoch 731: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.64 
2024-02-06 19:52:49,079 EPOCH 732
2024-02-06 19:53:05,353 Epoch 732: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.58 
2024-02-06 19:53:05,354 EPOCH 733
2024-02-06 19:53:21,906 Epoch 733: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.54 
2024-02-06 19:53:21,907 EPOCH 734
2024-02-06 19:53:27,150 [Epoch: 734 Step: 00006600] Batch Recognition Loss:   0.010452 => Gls Tokens per Sec:      561 || Batch Translation Loss:   0.024879 => Txt Tokens per Sec:     1552 || Lr: 0.000100
2024-02-06 19:53:38,087 Epoch 734: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.51 
2024-02-06 19:53:38,087 EPOCH 735
2024-02-06 19:53:54,528 Epoch 735: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.49 
2024-02-06 19:53:54,528 EPOCH 736
2024-02-06 19:54:10,744 Epoch 736: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.49 
2024-02-06 19:54:10,745 EPOCH 737
2024-02-06 19:54:26,830 Epoch 737: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.47 
2024-02-06 19:54:26,831 EPOCH 738
2024-02-06 19:54:43,107 Epoch 738: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.45 
2024-02-06 19:54:43,107 EPOCH 739
2024-02-06 19:54:59,377 Epoch 739: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.44 
2024-02-06 19:54:59,377 EPOCH 740
2024-02-06 19:55:15,516 Epoch 740: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.45 
2024-02-06 19:55:15,517 EPOCH 741
2024-02-06 19:55:32,033 Epoch 741: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.43 
2024-02-06 19:55:32,034 EPOCH 742
2024-02-06 19:55:48,340 Epoch 742: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.41 
2024-02-06 19:55:48,341 EPOCH 743
2024-02-06 19:56:04,547 Epoch 743: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.40 
2024-02-06 19:56:04,548 EPOCH 744
2024-02-06 19:56:21,013 Epoch 744: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.41 
2024-02-06 19:56:21,014 EPOCH 745
2024-02-06 19:56:30,972 [Epoch: 745 Step: 00006700] Batch Recognition Loss:   0.003373 => Gls Tokens per Sec:      514 || Batch Translation Loss:   0.026921 => Txt Tokens per Sec:     1443 || Lr: 0.000100
2024-02-06 19:56:37,313 Epoch 745: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.42 
2024-02-06 19:56:37,314 EPOCH 746
2024-02-06 19:56:53,328 Epoch 746: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.40 
2024-02-06 19:56:53,328 EPOCH 747
2024-02-06 19:57:09,618 Epoch 747: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.38 
2024-02-06 19:57:09,618 EPOCH 748
2024-02-06 19:57:26,102 Epoch 748: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.39 
2024-02-06 19:57:26,102 EPOCH 749
2024-02-06 19:57:42,850 Epoch 749: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.38 
2024-02-06 19:57:42,850 EPOCH 750
2024-02-06 19:57:59,024 Epoch 750: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.37 
2024-02-06 19:57:59,025 EPOCH 751
2024-02-06 19:58:15,030 Epoch 751: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.37 
2024-02-06 19:58:15,030 EPOCH 752
2024-02-06 19:58:31,553 Epoch 752: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.38 
2024-02-06 19:58:31,554 EPOCH 753
2024-02-06 19:58:47,587 Epoch 753: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.34 
2024-02-06 19:58:47,588 EPOCH 754
2024-02-06 19:59:04,101 Epoch 754: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.33 
2024-02-06 19:59:04,102 EPOCH 755
2024-02-06 19:59:20,204 Epoch 755: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.36 
2024-02-06 19:59:20,204 EPOCH 756
2024-02-06 19:59:34,942 [Epoch: 756 Step: 00006800] Batch Recognition Loss:   0.002187 => Gls Tokens per Sec:      373 || Batch Translation Loss:   0.044271 => Txt Tokens per Sec:     1123 || Lr: 0.000100
2024-02-06 19:59:36,541 Epoch 756: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.32 
2024-02-06 19:59:36,541 EPOCH 757
2024-02-06 19:59:52,658 Epoch 757: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.33 
2024-02-06 19:59:52,658 EPOCH 758
2024-02-06 20:00:08,915 Epoch 758: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.36 
2024-02-06 20:00:08,916 EPOCH 759
2024-02-06 20:00:25,431 Epoch 759: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.31 
2024-02-06 20:00:25,432 EPOCH 760
2024-02-06 20:00:41,996 Epoch 760: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.32 
2024-02-06 20:00:41,996 EPOCH 761
2024-02-06 20:00:58,325 Epoch 761: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.34 
2024-02-06 20:00:58,326 EPOCH 762
2024-02-06 20:01:14,479 Epoch 762: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.34 
2024-02-06 20:01:14,480 EPOCH 763
2024-02-06 20:01:31,067 Epoch 763: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.33 
2024-02-06 20:01:31,068 EPOCH 764
2024-02-06 20:01:47,118 Epoch 764: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.30 
2024-02-06 20:01:47,119 EPOCH 765
2024-02-06 20:02:03,455 Epoch 765: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.32 
2024-02-06 20:02:03,456 EPOCH 766
2024-02-06 20:02:19,397 Epoch 766: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.33 
2024-02-06 20:02:19,398 EPOCH 767
2024-02-06 20:02:31,776 [Epoch: 767 Step: 00006900] Batch Recognition Loss:   0.000539 => Gls Tokens per Sec:      548 || Batch Translation Loss:   0.035970 => Txt Tokens per Sec:     1556 || Lr: 0.000100
2024-02-06 20:02:35,809 Epoch 767: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 20:02:35,809 EPOCH 768
2024-02-06 20:02:52,203 Epoch 768: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.32 
2024-02-06 20:02:52,203 EPOCH 769
2024-02-06 20:03:08,475 Epoch 769: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 20:03:08,475 EPOCH 770
2024-02-06 20:03:24,594 Epoch 770: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 20:03:24,594 EPOCH 771
2024-02-06 20:03:40,918 Epoch 771: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.29 
2024-02-06 20:03:40,919 EPOCH 772
2024-02-06 20:03:57,425 Epoch 772: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.28 
2024-02-06 20:03:57,426 EPOCH 773
2024-02-06 20:04:14,101 Epoch 773: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.30 
2024-02-06 20:04:14,102 EPOCH 774
2024-02-06 20:04:30,425 Epoch 774: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.30 
2024-02-06 20:04:30,425 EPOCH 775
2024-02-06 20:04:46,597 Epoch 775: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.28 
2024-02-06 20:04:46,598 EPOCH 776
2024-02-06 20:05:03,196 Epoch 776: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.31 
2024-02-06 20:05:03,196 EPOCH 777
2024-02-06 20:05:19,258 Epoch 777: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.28 
2024-02-06 20:05:19,259 EPOCH 778
2024-02-06 20:05:31,045 [Epoch: 778 Step: 00007000] Batch Recognition Loss:   0.001461 => Gls Tokens per Sec:      760 || Batch Translation Loss:   0.025202 => Txt Tokens per Sec:     2049 || Lr: 0.000100
2024-02-06 20:05:36,013 Epoch 778: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 20:05:36,013 EPOCH 779
2024-02-06 20:05:52,321 Epoch 779: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.29 
2024-02-06 20:05:52,322 EPOCH 780
2024-02-06 20:06:08,631 Epoch 780: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.31 
2024-02-06 20:06:08,632 EPOCH 781
2024-02-06 20:06:24,828 Epoch 781: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.29 
2024-02-06 20:06:24,828 EPOCH 782
2024-02-06 20:06:41,352 Epoch 782: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.32 
2024-02-06 20:06:41,353 EPOCH 783
2024-02-06 20:06:57,801 Epoch 783: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 20:06:57,802 EPOCH 784
2024-02-06 20:07:13,880 Epoch 784: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 20:07:13,881 EPOCH 785
2024-02-06 20:07:30,059 Epoch 785: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 20:07:30,060 EPOCH 786
2024-02-06 20:07:46,185 Epoch 786: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 20:07:46,186 EPOCH 787
2024-02-06 20:08:02,651 Epoch 787: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 20:08:02,652 EPOCH 788
2024-02-06 20:08:18,937 Epoch 788: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 20:08:18,938 EPOCH 789
2024-02-06 20:08:35,071 [Epoch: 789 Step: 00007100] Batch Recognition Loss:   0.001639 => Gls Tokens per Sec:      579 || Batch Translation Loss:   0.024745 => Txt Tokens per Sec:     1680 || Lr: 0.000100
2024-02-06 20:08:35,316 Epoch 789: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 20:08:35,317 EPOCH 790
2024-02-06 20:08:51,525 Epoch 790: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 20:08:51,526 EPOCH 791
2024-02-06 20:09:07,912 Epoch 791: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 20:09:07,912 EPOCH 792
2024-02-06 20:09:24,805 Epoch 792: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 20:09:24,806 EPOCH 793
2024-02-06 20:09:41,491 Epoch 793: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 20:09:41,491 EPOCH 794
2024-02-06 20:09:57,915 Epoch 794: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 20:09:57,916 EPOCH 795
2024-02-06 20:10:14,255 Epoch 795: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 20:10:14,255 EPOCH 796
2024-02-06 20:10:30,663 Epoch 796: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 20:10:30,663 EPOCH 797
2024-02-06 20:10:47,071 Epoch 797: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 20:10:47,071 EPOCH 798
2024-02-06 20:11:03,687 Epoch 798: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.30 
2024-02-06 20:11:03,687 EPOCH 799
2024-02-06 20:11:20,223 Epoch 799: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 20:11:20,224 EPOCH 800
2024-02-06 20:11:36,544 [Epoch: 800 Step: 00007200] Batch Recognition Loss:   0.002852 => Gls Tokens per Sec:      651 || Batch Translation Loss:   0.023433 => Txt Tokens per Sec:     1801 || Lr: 0.000100
2024-02-06 20:11:36,545 Epoch 800: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.28 
2024-02-06 20:11:36,545 EPOCH 801
2024-02-06 20:11:53,114 Epoch 801: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 20:11:53,115 EPOCH 802
2024-02-06 20:12:09,288 Epoch 802: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 20:12:09,288 EPOCH 803
2024-02-06 20:12:25,507 Epoch 803: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 20:12:25,508 EPOCH 804
2024-02-06 20:12:41,905 Epoch 804: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 20:12:41,906 EPOCH 805
2024-02-06 20:12:58,329 Epoch 805: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 20:12:58,330 EPOCH 806
2024-02-06 20:13:15,103 Epoch 806: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 20:13:15,103 EPOCH 807
2024-02-06 20:13:31,493 Epoch 807: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 20:13:31,494 EPOCH 808
2024-02-06 20:13:48,099 Epoch 808: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 20:13:48,099 EPOCH 809
2024-02-06 20:14:04,542 Epoch 809: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 20:14:04,542 EPOCH 810
2024-02-06 20:14:20,990 Epoch 810: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 20:14:20,991 EPOCH 811
2024-02-06 20:14:37,667 Epoch 811: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 20:14:37,667 EPOCH 812
2024-02-06 20:14:37,986 [Epoch: 812 Step: 00007300] Batch Recognition Loss:   0.000274 => Gls Tokens per Sec:     4038 || Batch Translation Loss:   0.025714 => Txt Tokens per Sec:    10215 || Lr: 0.000100
2024-02-06 20:14:53,919 Epoch 812: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 20:14:53,920 EPOCH 813
2024-02-06 20:15:10,219 Epoch 813: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 20:15:10,219 EPOCH 814
2024-02-06 20:15:26,773 Epoch 814: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 20:15:26,774 EPOCH 815
2024-02-06 20:15:42,984 Epoch 815: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 20:15:42,985 EPOCH 816
2024-02-06 20:15:59,513 Epoch 816: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 20:15:59,514 EPOCH 817
2024-02-06 20:16:16,056 Epoch 817: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 20:16:16,057 EPOCH 818
2024-02-06 20:16:32,475 Epoch 818: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 20:16:32,475 EPOCH 819
2024-02-06 20:16:48,730 Epoch 819: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 20:16:48,731 EPOCH 820
2024-02-06 20:17:05,130 Epoch 820: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 20:17:05,131 EPOCH 821
2024-02-06 20:17:21,500 Epoch 821: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 20:17:21,500 EPOCH 822
2024-02-06 20:17:37,826 Epoch 822: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 20:17:37,827 EPOCH 823
2024-02-06 20:17:38,443 [Epoch: 823 Step: 00007400] Batch Recognition Loss:   0.000869 => Gls Tokens per Sec:     4158 || Batch Translation Loss:   0.014925 => Txt Tokens per Sec:     9429 || Lr: 0.000100
2024-02-06 20:17:54,015 Epoch 823: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 20:17:54,016 EPOCH 824
2024-02-06 20:18:10,867 Epoch 824: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 20:18:10,867 EPOCH 825
2024-02-06 20:18:27,327 Epoch 825: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 20:18:27,328 EPOCH 826
2024-02-06 20:18:43,559 Epoch 826: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 20:18:43,560 EPOCH 827
2024-02-06 20:18:59,923 Epoch 827: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 20:18:59,924 EPOCH 828
2024-02-06 20:19:16,731 Epoch 828: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 20:19:16,732 EPOCH 829
2024-02-06 20:19:33,164 Epoch 829: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 20:19:33,165 EPOCH 830
2024-02-06 20:19:49,501 Epoch 830: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 20:19:49,501 EPOCH 831
2024-02-06 20:20:05,793 Epoch 831: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 20:20:05,793 EPOCH 832
2024-02-06 20:20:22,898 Epoch 832: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 20:20:22,899 EPOCH 833
2024-02-06 20:20:39,494 Epoch 833: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 20:20:39,495 EPOCH 834
2024-02-06 20:20:46,301 [Epoch: 834 Step: 00007500] Batch Recognition Loss:   0.000648 => Gls Tokens per Sec:      564 || Batch Translation Loss:   0.023216 => Txt Tokens per Sec:     1403 || Lr: 0.000100
2024-02-06 20:20:56,150 Epoch 834: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 20:20:56,151 EPOCH 835
2024-02-06 20:21:12,704 Epoch 835: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 20:21:12,704 EPOCH 836
2024-02-06 20:21:28,884 Epoch 836: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 20:21:28,885 EPOCH 837
2024-02-06 20:21:45,230 Epoch 837: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 20:21:45,230 EPOCH 838
2024-02-06 20:22:01,215 Epoch 838: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 20:22:01,216 EPOCH 839
2024-02-06 20:22:17,746 Epoch 839: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 20:22:17,746 EPOCH 840
2024-02-06 20:22:34,132 Epoch 840: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 20:22:34,133 EPOCH 841
2024-02-06 20:22:50,481 Epoch 841: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 20:22:50,482 EPOCH 842
2024-02-06 20:23:06,979 Epoch 842: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 20:23:06,980 EPOCH 843
2024-02-06 20:23:23,687 Epoch 843: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 20:23:23,687 EPOCH 844
2024-02-06 20:23:40,530 Epoch 844: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 20:23:40,531 EPOCH 845
2024-02-06 20:23:46,134 [Epoch: 845 Step: 00007600] Batch Recognition Loss:   0.000540 => Gls Tokens per Sec:      753 || Batch Translation Loss:   0.037321 => Txt Tokens per Sec:     2021 || Lr: 0.000100
2024-02-06 20:23:56,890 Epoch 845: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 20:23:56,890 EPOCH 846
2024-02-06 20:24:13,641 Epoch 846: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 20:24:13,642 EPOCH 847
2024-02-06 20:24:30,186 Epoch 847: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-06 20:24:30,187 EPOCH 848
2024-02-06 20:24:46,637 Epoch 848: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-06 20:24:46,638 EPOCH 849
2024-02-06 20:25:03,150 Epoch 849: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-06 20:25:03,151 EPOCH 850
2024-02-06 20:25:19,651 Epoch 850: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-06 20:25:19,652 EPOCH 851
2024-02-06 20:25:36,329 Epoch 851: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-06 20:25:36,329 EPOCH 852
2024-02-06 20:25:52,855 Epoch 852: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-06 20:25:52,856 EPOCH 853
2024-02-06 20:26:09,283 Epoch 853: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.77 
2024-02-06 20:26:09,284 EPOCH 854
2024-02-06 20:26:25,571 Epoch 854: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-06 20:26:25,571 EPOCH 855
2024-02-06 20:26:42,149 Epoch 855: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-06 20:26:42,150 EPOCH 856
2024-02-06 20:26:44,006 [Epoch: 856 Step: 00007700] Batch Recognition Loss:   0.000814 => Gls Tokens per Sec:     3451 || Batch Translation Loss:   0.089581 => Txt Tokens per Sec:     8440 || Lr: 0.000100
2024-02-06 20:26:58,294 Epoch 856: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-06 20:26:58,294 EPOCH 857
2024-02-06 20:27:14,608 Epoch 857: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-06 20:27:14,609 EPOCH 858
2024-02-06 20:27:31,196 Epoch 858: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-06 20:27:31,196 EPOCH 859
2024-02-06 20:27:47,431 Epoch 859: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-06 20:27:47,432 EPOCH 860
2024-02-06 20:28:04,188 Epoch 860: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.73 
2024-02-06 20:28:04,188 EPOCH 861
2024-02-06 20:28:20,324 Epoch 861: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.55 
2024-02-06 20:28:20,324 EPOCH 862
2024-02-06 20:28:36,469 Epoch 862: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.89 
2024-02-06 20:28:36,469 EPOCH 863
2024-02-06 20:28:53,030 Epoch 863: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.92 
2024-02-06 20:28:53,030 EPOCH 864
2024-02-06 20:29:09,390 Epoch 864: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.02 
2024-02-06 20:29:09,391 EPOCH 865
2024-02-06 20:29:25,891 Epoch 865: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.98 
2024-02-06 20:29:25,892 EPOCH 866
2024-02-06 20:29:42,372 Epoch 866: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.34 
2024-02-06 20:29:42,373 EPOCH 867
2024-02-06 20:29:57,176 [Epoch: 867 Step: 00007800] Batch Recognition Loss:   0.003326 => Gls Tokens per Sec:      458 || Batch Translation Loss:   0.205168 => Txt Tokens per Sec:     1320 || Lr: 0.000100
2024-02-06 20:29:58,863 Epoch 867: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.35 
2024-02-06 20:29:58,863 EPOCH 868
2024-02-06 20:30:15,227 Epoch 868: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.31 
2024-02-06 20:30:15,228 EPOCH 869
2024-02-06 20:30:31,472 Epoch 869: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.04 
2024-02-06 20:30:31,473 EPOCH 870
2024-02-06 20:30:47,801 Epoch 870: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-06 20:30:47,801 EPOCH 871
2024-02-06 20:31:04,448 Epoch 871: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.65 
2024-02-06 20:31:04,450 EPOCH 872
2024-02-06 20:31:20,871 Epoch 872: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-06 20:31:20,871 EPOCH 873
2024-02-06 20:31:37,289 Epoch 873: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-06 20:31:37,290 EPOCH 874
2024-02-06 20:31:53,638 Epoch 874: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.51 
2024-02-06 20:31:53,639 EPOCH 875
2024-02-06 20:32:09,899 Epoch 875: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.49 
2024-02-06 20:32:09,899 EPOCH 876
2024-02-06 20:32:26,310 Epoch 876: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-06 20:32:26,311 EPOCH 877
2024-02-06 20:32:42,639 Epoch 877: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.49 
2024-02-06 20:32:42,639 EPOCH 878
2024-02-06 20:32:54,340 [Epoch: 878 Step: 00007900] Batch Recognition Loss:   0.001046 => Gls Tokens per Sec:      766 || Batch Translation Loss:   0.045375 => Txt Tokens per Sec:     2092 || Lr: 0.000100
2024-02-06 20:32:59,097 Epoch 878: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 20:32:59,097 EPOCH 879
2024-02-06 20:33:15,538 Epoch 879: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.40 
2024-02-06 20:33:15,538 EPOCH 880
2024-02-06 20:33:32,023 Epoch 880: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 20:33:32,024 EPOCH 881
2024-02-06 20:33:47,976 Epoch 881: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.97 
2024-02-06 20:33:47,977 EPOCH 882
2024-02-06 20:34:04,651 Epoch 882: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-06 20:34:04,652 EPOCH 883
2024-02-06 20:34:20,902 Epoch 883: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.65 
2024-02-06 20:34:20,903 EPOCH 884
2024-02-06 20:34:37,924 Epoch 884: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-06 20:34:37,925 EPOCH 885
2024-02-06 20:34:54,532 Epoch 885: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 20:34:54,532 EPOCH 886
2024-02-06 20:35:10,754 Epoch 886: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 20:35:10,754 EPOCH 887
2024-02-06 20:35:27,283 Epoch 887: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 20:35:27,283 EPOCH 888
2024-02-06 20:35:43,708 Epoch 888: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 20:35:43,708 EPOCH 889
2024-02-06 20:35:59,838 [Epoch: 889 Step: 00008000] Batch Recognition Loss:   0.001074 => Gls Tokens per Sec:      579 || Batch Translation Loss:   0.219799 => Txt Tokens per Sec:     1620 || Lr: 0.000100
2024-02-06 20:37:07,628 Validation result at epoch 889, step     8000: duration: 67.7903s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.45098	Translation Loss: 90285.59375	PPL: 8248.09375
	Eval Metric: BLEU
	WER 5.37	(DEL: 0.00,	INS: 0.00,	SUB: 5.37)
	BLEU-4 0.70	(BLEU-1: 11.73,	BLEU-2: 3.81,	BLEU-3: 1.49,	BLEU-4: 0.70)
	CHRF 17.16	ROUGE 9.75
2024-02-06 20:37:07,630 Logging Recognition and Translation Outputs
2024-02-06 20:37:07,631 ========================================================================================================================
2024-02-06 20:37:07,631 Logging Sequence: 180_236.00
2024-02-06 20:37:07,631 	Gloss Reference :	A B+C+D+E
2024-02-06 20:37:07,631 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 20:37:07,631 	Gloss Alignment :	         
2024-02-06 20:37:07,631 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 20:37:07,633 	Text Reference  :	however the wrestlers returned to      the  protest site at   jantar   mantar  with thier     demands
2024-02-06 20:37:07,633 	Text Hypothesis :	******* *** ********* ******** earlier they were    a    huge argument however 9    wrestlers said   
2024-02-06 20:37:07,633 	Text Alignment  :	D       D   D         D        S       S    S       S    S    S        S       S    S         S      
2024-02-06 20:37:07,633 ========================================================================================================================
2024-02-06 20:37:07,633 Logging Sequence: 111_154.00
2024-02-06 20:37:07,634 	Gloss Reference :	A B+C+D+E  
2024-02-06 20:37:07,634 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-06 20:37:07,634 	Gloss Alignment :	  S        
2024-02-06 20:37:07,634 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 20:37:07,635 	Text Reference  :	***** *** due   to       csk's  slow over   rate dhoni   was fined rs 12 lakh *** **** *********
2024-02-06 20:37:07,635 	Text Hypothesis :	after the first instance during a    season the  captain is  fined rs 12 lakh for slow over-rate
2024-02-06 20:37:07,636 	Text Alignment  :	I     I   S     S        S      S    S      S    S       S                    I   I    I        
2024-02-06 20:37:07,636 ========================================================================================================================
2024-02-06 20:37:07,636 Logging Sequence: 118_314.00
2024-02-06 20:37:07,636 	Gloss Reference :	A B+C+D+E
2024-02-06 20:37:07,636 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 20:37:07,636 	Gloss Alignment :	         
2024-02-06 20:37:07,636 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 20:37:07,637 	Text Reference  :	wow even the president had    come to    watch
2024-02-06 20:37:07,637 	Text Hypothesis :	*** what a   proud     moment for  these match
2024-02-06 20:37:07,637 	Text Alignment  :	D   S    S   S         S      S    S     S    
2024-02-06 20:37:07,637 ========================================================================================================================
2024-02-06 20:37:07,638 Logging Sequence: 156_197.00
2024-02-06 20:37:07,638 	Gloss Reference :	A B+C+D+E
2024-02-06 20:37:07,638 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 20:37:07,638 	Gloss Alignment :	         
2024-02-06 20:37:07,638 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 20:37:07,640 	Text Reference  :	*** ******* ***** **** **** ******** *** seattle orcas sor  is owned by   many      investors including satya nadella microsoft ceo      
2024-02-06 20:37:07,640 	Text Hypothesis :	kkr batters could have many sponsors but at      the   2020 is ***** very different but       they      have  from    their     passports
2024-02-06 20:37:07,640 	Text Alignment  :	I   I       I     I    I    I        I   S       S     S       D     S    S         S         S         S     S       S         S        
2024-02-06 20:37:07,640 ========================================================================================================================
2024-02-06 20:37:07,640 Logging Sequence: 183_159.00
2024-02-06 20:37:07,641 	Gloss Reference :	A B+C+D+E
2024-02-06 20:37:07,641 	Gloss Hypothesis:	A B+C+D  
2024-02-06 20:37:07,641 	Gloss Alignment :	  S      
2024-02-06 20:37:07,641 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 20:37:07,643 	Text Reference  :	*** however an   exception to this     is   virat          kohli    and his wife anushka sharma who refuse to share images of    their daughter
2024-02-06 20:37:07,643 	Text Hypothesis :	the picture went viral     it received many congratulatory messages and his **** ******* ****** *** ****** ** ***** son    orion keech singh   
2024-02-06 20:37:07,643 	Text Alignment  :	I   S       S    S         S  S        S    S              S                D    D       D      D   D      D  D     S      S     S     S       
2024-02-06 20:37:07,643 ========================================================================================================================
2024-02-06 20:37:08,246 Epoch 889: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 20:37:08,246 EPOCH 890
2024-02-06 20:37:25,413 Epoch 890: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 20:37:25,413 EPOCH 891
2024-02-06 20:37:41,839 Epoch 891: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-06 20:37:41,840 EPOCH 892
2024-02-06 20:37:58,321 Epoch 892: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 20:37:58,321 EPOCH 893
2024-02-06 20:38:14,640 Epoch 893: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 20:38:14,641 EPOCH 894
2024-02-06 20:38:31,304 Epoch 894: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 20:38:31,305 EPOCH 895
2024-02-06 20:38:47,702 Epoch 895: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 20:38:47,703 EPOCH 896
2024-02-06 20:39:04,598 Epoch 896: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 20:39:04,599 EPOCH 897
2024-02-06 20:39:21,085 Epoch 897: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 20:39:21,086 EPOCH 898
2024-02-06 20:39:37,511 Epoch 898: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 20:39:37,512 EPOCH 899
2024-02-06 20:39:53,857 Epoch 899: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 20:39:53,858 EPOCH 900
2024-02-06 20:40:10,327 [Epoch: 900 Step: 00008100] Batch Recognition Loss:   0.000964 => Gls Tokens per Sec:      645 || Batch Translation Loss:   0.030999 => Txt Tokens per Sec:     1784 || Lr: 0.000100
2024-02-06 20:40:10,328 Epoch 900: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 20:40:10,328 EPOCH 901
2024-02-06 20:40:26,772 Epoch 901: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 20:40:26,773 EPOCH 902
2024-02-06 20:40:43,675 Epoch 902: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 20:40:43,676 EPOCH 903
2024-02-06 20:41:00,038 Epoch 903: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 20:41:00,038 EPOCH 904
2024-02-06 20:41:16,316 Epoch 904: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 20:41:16,317 EPOCH 905
2024-02-06 20:41:32,722 Epoch 905: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 20:41:32,723 EPOCH 906
2024-02-06 20:41:49,103 Epoch 906: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 20:41:49,103 EPOCH 907
2024-02-06 20:42:05,764 Epoch 907: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 20:42:05,764 EPOCH 908
2024-02-06 20:42:22,226 Epoch 908: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 20:42:22,227 EPOCH 909
2024-02-06 20:42:38,527 Epoch 909: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 20:42:38,527 EPOCH 910
2024-02-06 20:42:55,078 Epoch 910: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 20:42:55,078 EPOCH 911
2024-02-06 20:43:11,479 Epoch 911: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 20:43:11,480 EPOCH 912
2024-02-06 20:43:11,841 [Epoch: 912 Step: 00008200] Batch Recognition Loss:   0.000757 => Gls Tokens per Sec:     3552 || Batch Translation Loss:   0.031619 => Txt Tokens per Sec:     9686 || Lr: 0.000100
2024-02-06 20:43:27,653 Epoch 912: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 20:43:27,654 EPOCH 913
2024-02-06 20:43:44,401 Epoch 913: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 20:43:44,402 EPOCH 914
2024-02-06 20:44:00,615 Epoch 914: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 20:44:00,616 EPOCH 915
2024-02-06 20:44:17,255 Epoch 915: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 20:44:17,255 EPOCH 916
2024-02-06 20:44:33,512 Epoch 916: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 20:44:33,513 EPOCH 917
2024-02-06 20:44:49,979 Epoch 917: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 20:44:49,979 EPOCH 918
2024-02-06 20:45:06,371 Epoch 918: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 20:45:06,372 EPOCH 919
2024-02-06 20:45:22,743 Epoch 919: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 20:45:22,743 EPOCH 920
2024-02-06 20:45:39,400 Epoch 920: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 20:45:39,400 EPOCH 921
2024-02-06 20:45:55,836 Epoch 921: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 20:45:55,837 EPOCH 922
2024-02-06 20:46:12,423 Epoch 922: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 20:46:12,424 EPOCH 923
2024-02-06 20:46:13,630 [Epoch: 923 Step: 00008300] Batch Recognition Loss:   0.000369 => Gls Tokens per Sec:     2124 || Batch Translation Loss:   0.030504 => Txt Tokens per Sec:     6179 || Lr: 0.000100
2024-02-06 20:46:28,957 Epoch 923: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 20:46:28,957 EPOCH 924
2024-02-06 20:46:45,478 Epoch 924: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 20:46:45,479 EPOCH 925
2024-02-06 20:47:02,463 Epoch 925: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 20:47:02,463 EPOCH 926
2024-02-06 20:47:19,228 Epoch 926: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 20:47:19,228 EPOCH 927
2024-02-06 20:47:35,668 Epoch 927: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 20:47:35,669 EPOCH 928
2024-02-06 20:47:52,147 Epoch 928: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 20:47:52,148 EPOCH 929
2024-02-06 20:48:08,684 Epoch 929: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 20:48:08,684 EPOCH 930
2024-02-06 20:48:25,117 Epoch 930: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 20:48:25,118 EPOCH 931
2024-02-06 20:48:41,695 Epoch 931: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 20:48:41,695 EPOCH 932
2024-02-06 20:48:58,115 Epoch 932: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 20:48:58,115 EPOCH 933
2024-02-06 20:49:14,748 Epoch 933: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 20:49:14,749 EPOCH 934
2024-02-06 20:49:24,631 [Epoch: 934 Step: 00008400] Batch Recognition Loss:   0.000558 => Gls Tokens per Sec:      389 || Batch Translation Loss:   0.025036 => Txt Tokens per Sec:     1148 || Lr: 0.000100
2024-02-06 20:49:31,263 Epoch 934: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 20:49:31,263 EPOCH 935
2024-02-06 20:49:47,695 Epoch 935: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 20:49:47,696 EPOCH 936
2024-02-06 20:50:03,931 Epoch 936: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 20:50:03,931 EPOCH 937
2024-02-06 20:50:20,459 Epoch 937: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 20:50:20,460 EPOCH 938
2024-02-06 20:50:36,711 Epoch 938: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 20:50:36,712 EPOCH 939
2024-02-06 20:50:53,074 Epoch 939: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 20:50:53,075 EPOCH 940
2024-02-06 20:51:09,576 Epoch 940: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 20:51:09,577 EPOCH 941
2024-02-06 20:51:26,585 Epoch 941: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-06 20:51:26,586 EPOCH 942
2024-02-06 20:51:42,791 Epoch 942: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 20:51:42,791 EPOCH 943
2024-02-06 20:51:59,416 Epoch 943: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 20:51:59,417 EPOCH 944
2024-02-06 20:52:15,822 Epoch 944: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 20:52:15,823 EPOCH 945
2024-02-06 20:52:27,088 [Epoch: 945 Step: 00008500] Batch Recognition Loss:   0.001538 => Gls Tokens per Sec:      375 || Batch Translation Loss:   0.052642 => Txt Tokens per Sec:     1095 || Lr: 0.000100
2024-02-06 20:52:32,321 Epoch 945: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-06 20:52:32,322 EPOCH 946
2024-02-06 20:52:48,544 Epoch 946: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 20:52:48,545 EPOCH 947
2024-02-06 20:53:05,223 Epoch 947: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-06 20:53:05,224 EPOCH 948
2024-02-06 20:53:21,587 Epoch 948: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-06 20:53:21,588 EPOCH 949
2024-02-06 20:53:38,436 Epoch 949: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 20:53:38,437 EPOCH 950
2024-02-06 20:53:54,881 Epoch 950: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 20:53:54,882 EPOCH 951
2024-02-06 20:54:11,377 Epoch 951: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-06 20:54:11,378 EPOCH 952
2024-02-06 20:54:28,037 Epoch 952: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-06 20:54:28,038 EPOCH 953
2024-02-06 20:54:44,467 Epoch 953: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-06 20:54:44,468 EPOCH 954
2024-02-06 20:55:00,897 Epoch 954: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-06 20:55:00,897 EPOCH 955
2024-02-06 20:55:17,380 Epoch 955: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-06 20:55:17,381 EPOCH 956
2024-02-06 20:55:25,975 [Epoch: 956 Step: 00008600] Batch Recognition Loss:   0.001128 => Gls Tokens per Sec:      640 || Batch Translation Loss:   0.042507 => Txt Tokens per Sec:     1612 || Lr: 0.000100
2024-02-06 20:55:33,809 Epoch 956: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-06 20:55:33,810 EPOCH 957
2024-02-06 20:55:50,649 Epoch 957: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-06 20:55:50,650 EPOCH 958
2024-02-06 20:56:06,929 Epoch 958: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-06 20:56:06,929 EPOCH 959
2024-02-06 20:56:23,678 Epoch 959: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-06 20:56:23,678 EPOCH 960
2024-02-06 20:56:40,005 Epoch 960: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 20:56:40,006 EPOCH 961
2024-02-06 20:56:56,648 Epoch 961: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-06 20:56:56,649 EPOCH 962
2024-02-06 20:57:12,885 Epoch 962: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 20:57:12,886 EPOCH 963
2024-02-06 20:57:29,516 Epoch 963: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 20:57:29,517 EPOCH 964
2024-02-06 20:57:46,057 Epoch 964: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 20:57:46,057 EPOCH 965
2024-02-06 20:58:02,334 Epoch 965: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 20:58:02,334 EPOCH 966
2024-02-06 20:58:18,976 Epoch 966: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 20:58:18,976 EPOCH 967
2024-02-06 20:58:28,341 [Epoch: 967 Step: 00008700] Batch Recognition Loss:   0.003190 => Gls Tokens per Sec:      724 || Batch Translation Loss:   0.066848 => Txt Tokens per Sec:     1967 || Lr: 0.000100
2024-02-06 20:58:35,239 Epoch 967: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 20:58:35,240 EPOCH 968
2024-02-06 20:58:51,874 Epoch 968: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 20:58:51,875 EPOCH 969
2024-02-06 20:59:08,502 Epoch 969: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 20:59:08,503 EPOCH 970
2024-02-06 20:59:24,878 Epoch 970: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 20:59:24,878 EPOCH 971
2024-02-06 20:59:41,264 Epoch 971: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 20:59:41,264 EPOCH 972
2024-02-06 20:59:57,795 Epoch 972: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 20:59:57,796 EPOCH 973
2024-02-06 21:00:14,222 Epoch 973: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 21:00:14,223 EPOCH 974
2024-02-06 21:00:30,731 Epoch 974: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 21:00:30,732 EPOCH 975
2024-02-06 21:00:47,226 Epoch 975: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-06 21:00:47,226 EPOCH 976
2024-02-06 21:01:03,242 Epoch 976: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-06 21:01:03,242 EPOCH 977
2024-02-06 21:01:19,859 Epoch 977: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.59 
2024-02-06 21:01:19,860 EPOCH 978
2024-02-06 21:01:35,659 [Epoch: 978 Step: 00008800] Batch Recognition Loss:   0.009945 => Gls Tokens per Sec:      510 || Batch Translation Loss:   4.887678 => Txt Tokens per Sec:     1430 || Lr: 0.000100
2024-02-06 21:01:36,590 Epoch 978: Total Training Recognition Loss 0.02  Total Training Translation Loss 12.59 
2024-02-06 21:01:36,590 EPOCH 979
2024-02-06 21:01:52,778 Epoch 979: Total Training Recognition Loss 0.07  Total Training Translation Loss 6.86 
2024-02-06 21:01:52,778 EPOCH 980
2024-02-06 21:02:09,129 Epoch 980: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.66 
2024-02-06 21:02:09,130 EPOCH 981
2024-02-06 21:02:25,700 Epoch 981: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.24 
2024-02-06 21:02:25,700 EPOCH 982
2024-02-06 21:02:42,338 Epoch 982: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.49 
2024-02-06 21:02:42,339 EPOCH 983
2024-02-06 21:02:58,981 Epoch 983: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.05 
2024-02-06 21:02:58,982 EPOCH 984
2024-02-06 21:03:15,467 Epoch 984: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.81 
2024-02-06 21:03:15,468 EPOCH 985
2024-02-06 21:03:31,988 Epoch 985: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-06 21:03:31,988 EPOCH 986
2024-02-06 21:03:48,514 Epoch 986: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-06 21:03:48,515 EPOCH 987
2024-02-06 21:04:05,198 Epoch 987: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.42 
2024-02-06 21:04:05,198 EPOCH 988
2024-02-06 21:04:21,426 Epoch 988: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.40 
2024-02-06 21:04:21,427 EPOCH 989
2024-02-06 21:04:33,436 [Epoch: 989 Step: 00008900] Batch Recognition Loss:   0.002391 => Gls Tokens per Sec:      853 || Batch Translation Loss:   0.038241 => Txt Tokens per Sec:     2334 || Lr: 0.000100
2024-02-06 21:04:37,728 Epoch 989: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.35 
2024-02-06 21:04:37,728 EPOCH 990
2024-02-06 21:04:54,243 Epoch 990: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.34 
2024-02-06 21:04:54,243 EPOCH 991
2024-02-06 21:05:10,731 Epoch 991: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 21:05:10,732 EPOCH 992
2024-02-06 21:05:27,099 Epoch 992: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.32 
2024-02-06 21:05:27,100 EPOCH 993
2024-02-06 21:05:43,881 Epoch 993: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 21:05:43,882 EPOCH 994
2024-02-06 21:06:00,435 Epoch 994: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.30 
2024-02-06 21:06:00,435 EPOCH 995
2024-02-06 21:06:16,771 Epoch 995: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 21:06:16,771 EPOCH 996
2024-02-06 21:06:33,247 Epoch 996: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 21:06:33,247 EPOCH 997
2024-02-06 21:06:49,854 Epoch 997: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 21:06:49,854 EPOCH 998
2024-02-06 21:07:05,968 Epoch 998: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 21:07:05,968 EPOCH 999
2024-02-06 21:07:22,682 Epoch 999: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 21:07:22,683 EPOCH 1000
2024-02-06 21:07:38,911 [Epoch: 1000 Step: 00009000] Batch Recognition Loss:   0.000980 => Gls Tokens per Sec:      654 || Batch Translation Loss:   0.041056 => Txt Tokens per Sec:     1811 || Lr: 0.000100
2024-02-06 21:07:38,912 Epoch 1000: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.38 
2024-02-06 21:07:38,913 EPOCH 1001
2024-02-06 21:07:55,584 Epoch 1001: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.26 
2024-02-06 21:07:55,585 EPOCH 1002
2024-02-06 21:08:11,448 Epoch 1002: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 21:08:11,449 EPOCH 1003
2024-02-06 21:08:27,946 Epoch 1003: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 21:08:27,946 EPOCH 1004
2024-02-06 21:08:44,603 Epoch 1004: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 21:08:44,604 EPOCH 1005
2024-02-06 21:09:01,096 Epoch 1005: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 21:09:01,097 EPOCH 1006
2024-02-06 21:09:17,440 Epoch 1006: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 21:09:17,441 EPOCH 1007
2024-02-06 21:09:34,033 Epoch 1007: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 21:09:34,034 EPOCH 1008
2024-02-06 21:09:50,577 Epoch 1008: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 21:09:50,578 EPOCH 1009
2024-02-06 21:10:07,257 Epoch 1009: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 21:10:07,258 EPOCH 1010
2024-02-06 21:10:23,840 Epoch 1010: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 21:10:23,840 EPOCH 1011
2024-02-06 21:10:40,252 Epoch 1011: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 21:10:40,253 EPOCH 1012
2024-02-06 21:10:40,538 [Epoch: 1012 Step: 00009100] Batch Recognition Loss:   0.000571 => Gls Tokens per Sec:     4507 || Batch Translation Loss:   0.031917 => Txt Tokens per Sec:    10032 || Lr: 0.000100
2024-02-06 21:10:56,999 Epoch 1012: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 21:10:57,000 EPOCH 1013
2024-02-06 21:11:13,420 Epoch 1013: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 21:11:13,420 EPOCH 1014
2024-02-06 21:11:30,338 Epoch 1014: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 21:11:30,338 EPOCH 1015
2024-02-06 21:11:46,726 Epoch 1015: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 21:11:46,726 EPOCH 1016
2024-02-06 21:12:03,195 Epoch 1016: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 21:12:03,195 EPOCH 1017
2024-02-06 21:12:19,633 Epoch 1017: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 21:12:19,634 EPOCH 1018
2024-02-06 21:12:35,947 Epoch 1018: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 21:12:35,948 EPOCH 1019
2024-02-06 21:12:52,492 Epoch 1019: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 21:12:52,492 EPOCH 1020
2024-02-06 21:13:08,905 Epoch 1020: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 21:13:08,906 EPOCH 1021
2024-02-06 21:13:25,346 Epoch 1021: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 21:13:25,347 EPOCH 1022
2024-02-06 21:13:41,702 Epoch 1022: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 21:13:41,702 EPOCH 1023
2024-02-06 21:13:42,696 [Epoch: 1023 Step: 00009200] Batch Recognition Loss:   0.000420 => Gls Tokens per Sec:     2578 || Batch Translation Loss:   0.017326 => Txt Tokens per Sec:     6710 || Lr: 0.000100
2024-02-06 21:13:58,035 Epoch 1023: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 21:13:58,036 EPOCH 1024
2024-02-06 21:14:14,663 Epoch 1024: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 21:14:14,664 EPOCH 1025
2024-02-06 21:14:30,829 Epoch 1025: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 21:14:30,830 EPOCH 1026
2024-02-06 21:14:47,547 Epoch 1026: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 21:14:47,547 EPOCH 1027
2024-02-06 21:15:04,140 Epoch 1027: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 21:15:04,140 EPOCH 1028
2024-02-06 21:15:20,496 Epoch 1028: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 21:15:20,496 EPOCH 1029
2024-02-06 21:15:36,778 Epoch 1029: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 21:15:36,779 EPOCH 1030
2024-02-06 21:15:53,012 Epoch 1030: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 21:15:53,012 EPOCH 1031
2024-02-06 21:16:09,485 Epoch 1031: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 21:16:09,485 EPOCH 1032
2024-02-06 21:16:25,997 Epoch 1032: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 21:16:25,998 EPOCH 1033
2024-02-06 21:16:42,438 Epoch 1033: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 21:16:42,439 EPOCH 1034
2024-02-06 21:16:49,621 [Epoch: 1034 Step: 00009300] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:      535 || Batch Translation Loss:   0.030168 => Txt Tokens per Sec:     1585 || Lr: 0.000100
2024-02-06 21:16:58,981 Epoch 1034: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-06 21:16:58,982 EPOCH 1035
2024-02-06 21:17:15,221 Epoch 1035: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 21:17:15,222 EPOCH 1036
2024-02-06 21:17:31,675 Epoch 1036: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 21:17:31,675 EPOCH 1037
2024-02-06 21:17:48,082 Epoch 1037: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 21:17:48,082 EPOCH 1038
2024-02-06 21:18:04,622 Epoch 1038: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 21:18:04,623 EPOCH 1039
2024-02-06 21:18:20,976 Epoch 1039: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 21:18:20,977 EPOCH 1040
2024-02-06 21:18:37,449 Epoch 1040: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 21:18:37,450 EPOCH 1041
2024-02-06 21:18:53,508 Epoch 1041: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 21:18:53,508 EPOCH 1042
2024-02-06 21:19:10,106 Epoch 1042: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 21:19:10,106 EPOCH 1043
2024-02-06 21:19:26,456 Epoch 1043: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 21:19:26,456 EPOCH 1044
2024-02-06 21:19:43,032 Epoch 1044: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 21:19:43,033 EPOCH 1045
2024-02-06 21:19:57,024 [Epoch: 1045 Step: 00009400] Batch Recognition Loss:   0.000548 => Gls Tokens per Sec:      302 || Batch Translation Loss:   0.023785 => Txt Tokens per Sec:      871 || Lr: 0.000100
2024-02-06 21:19:59,566 Epoch 1045: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 21:19:59,566 EPOCH 1046
2024-02-06 21:20:15,803 Epoch 1046: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 21:20:15,804 EPOCH 1047
2024-02-06 21:20:32,248 Epoch 1047: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 21:20:32,249 EPOCH 1048
2024-02-06 21:20:48,714 Epoch 1048: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 21:20:48,714 EPOCH 1049
2024-02-06 21:21:05,203 Epoch 1049: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 21:21:05,204 EPOCH 1050
2024-02-06 21:21:21,534 Epoch 1050: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 21:21:21,535 EPOCH 1051
2024-02-06 21:21:38,179 Epoch 1051: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 21:21:38,179 EPOCH 1052
2024-02-06 21:21:54,470 Epoch 1052: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-06 21:21:54,470 EPOCH 1053
2024-02-06 21:22:10,997 Epoch 1053: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 21:22:10,997 EPOCH 1054
2024-02-06 21:22:27,227 Epoch 1054: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 21:22:27,227 EPOCH 1055
2024-02-06 21:22:43,716 Epoch 1055: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-06 21:22:43,717 EPOCH 1056
2024-02-06 21:22:51,345 [Epoch: 1056 Step: 00009500] Batch Recognition Loss:   0.000953 => Gls Tokens per Sec:      839 || Batch Translation Loss:   0.012926 => Txt Tokens per Sec:     2259 || Lr: 0.000100
2024-02-06 21:23:00,045 Epoch 1056: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 21:23:00,045 EPOCH 1057
2024-02-06 21:23:16,418 Epoch 1057: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 21:23:16,419 EPOCH 1058
2024-02-06 21:23:32,748 Epoch 1058: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-06 21:23:32,749 EPOCH 1059
2024-02-06 21:23:49,253 Epoch 1059: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 21:23:49,253 EPOCH 1060
2024-02-06 21:24:05,502 Epoch 1060: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 21:24:05,502 EPOCH 1061
2024-02-06 21:24:21,862 Epoch 1061: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-06 21:24:21,863 EPOCH 1062
2024-02-06 21:24:38,336 Epoch 1062: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-06 21:24:38,336 EPOCH 1063
2024-02-06 21:24:54,786 Epoch 1063: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 21:24:54,787 EPOCH 1064
2024-02-06 21:25:10,986 Epoch 1064: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 21:25:10,986 EPOCH 1065
2024-02-06 21:25:27,499 Epoch 1065: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-06 21:25:27,500 EPOCH 1066
2024-02-06 21:25:44,163 Epoch 1066: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-06 21:25:44,164 EPOCH 1067
2024-02-06 21:25:56,529 [Epoch: 1067 Step: 00009600] Batch Recognition Loss:   0.001570 => Gls Tokens per Sec:      548 || Batch Translation Loss:   0.298906 => Txt Tokens per Sec:     1445 || Lr: 0.000100
2024-02-06 21:26:00,956 Epoch 1067: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.17 
2024-02-06 21:26:00,956 EPOCH 1068
2024-02-06 21:26:17,483 Epoch 1068: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.59 
2024-02-06 21:26:17,483 EPOCH 1069
2024-02-06 21:26:33,985 Epoch 1069: Total Training Recognition Loss 0.05  Total Training Translation Loss 7.64 
2024-02-06 21:26:33,986 EPOCH 1070
2024-02-06 21:26:50,229 Epoch 1070: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.15 
2024-02-06 21:26:50,229 EPOCH 1071
2024-02-06 21:27:07,086 Epoch 1071: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.20 
2024-02-06 21:27:07,087 EPOCH 1072
2024-02-06 21:27:23,807 Epoch 1072: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.90 
2024-02-06 21:27:23,808 EPOCH 1073
2024-02-06 21:27:40,428 Epoch 1073: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.30 
2024-02-06 21:27:40,429 EPOCH 1074
2024-02-06 21:27:56,629 Epoch 1074: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.91 
2024-02-06 21:27:56,630 EPOCH 1075
2024-02-06 21:28:13,079 Epoch 1075: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.58 
2024-02-06 21:28:13,080 EPOCH 1076
2024-02-06 21:28:29,541 Epoch 1076: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-06 21:28:29,542 EPOCH 1077
2024-02-06 21:28:45,738 Epoch 1077: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.41 
2024-02-06 21:28:45,739 EPOCH 1078
2024-02-06 21:29:01,229 [Epoch: 1078 Step: 00009700] Batch Recognition Loss:   0.001160 => Gls Tokens per Sec:      520 || Batch Translation Loss:   0.015539 => Txt Tokens per Sec:     1430 || Lr: 0.000100
2024-02-06 21:29:02,267 Epoch 1078: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.35 
2024-02-06 21:29:02,267 EPOCH 1079
2024-02-06 21:29:18,791 Epoch 1079: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 21:29:18,792 EPOCH 1080
2024-02-06 21:29:35,253 Epoch 1080: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 21:29:35,253 EPOCH 1081
2024-02-06 21:29:51,573 Epoch 1081: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 21:29:51,574 EPOCH 1082
2024-02-06 21:30:08,068 Epoch 1082: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 21:30:08,068 EPOCH 1083
2024-02-06 21:30:24,365 Epoch 1083: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 21:30:24,366 EPOCH 1084
2024-02-06 21:30:40,345 Epoch 1084: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 21:30:40,345 EPOCH 1085
2024-02-06 21:30:56,712 Epoch 1085: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 21:30:56,712 EPOCH 1086
2024-02-06 21:31:13,212 Epoch 1086: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 21:31:13,213 EPOCH 1087
2024-02-06 21:31:29,726 Epoch 1087: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 21:31:29,726 EPOCH 1088
2024-02-06 21:31:46,343 Epoch 1088: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 21:31:46,343 EPOCH 1089
2024-02-06 21:32:02,513 [Epoch: 1089 Step: 00009800] Batch Recognition Loss:   0.000922 => Gls Tokens per Sec:      578 || Batch Translation Loss:   0.014279 => Txt Tokens per Sec:     1588 || Lr: 0.000100
2024-02-06 21:32:03,073 Epoch 1089: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 21:32:03,074 EPOCH 1090
2024-02-06 21:32:19,490 Epoch 1090: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 21:32:19,490 EPOCH 1091
2024-02-06 21:32:35,996 Epoch 1091: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 21:32:35,996 EPOCH 1092
2024-02-06 21:32:52,453 Epoch 1092: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 21:32:52,454 EPOCH 1093
2024-02-06 21:33:08,887 Epoch 1093: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 21:33:08,887 EPOCH 1094
2024-02-06 21:33:25,619 Epoch 1094: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 21:33:25,620 EPOCH 1095
2024-02-06 21:33:42,169 Epoch 1095: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 21:33:42,171 EPOCH 1096
2024-02-06 21:33:58,809 Epoch 1096: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 21:33:58,810 EPOCH 1097
2024-02-06 21:34:15,291 Epoch 1097: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 21:34:15,292 EPOCH 1098
2024-02-06 21:34:31,879 Epoch 1098: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 21:34:31,881 EPOCH 1099
2024-02-06 21:34:48,671 Epoch 1099: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 21:34:48,672 EPOCH 1100
2024-02-06 21:35:05,790 [Epoch: 1100 Step: 00009900] Batch Recognition Loss:   0.000461 => Gls Tokens per Sec:      620 || Batch Translation Loss:   0.023864 => Txt Tokens per Sec:     1716 || Lr: 0.000100
2024-02-06 21:35:05,791 Epoch 1100: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 21:35:05,791 EPOCH 1101
2024-02-06 21:35:22,724 Epoch 1101: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 21:35:22,724 EPOCH 1102
2024-02-06 21:35:39,630 Epoch 1102: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 21:35:39,631 EPOCH 1103
2024-02-06 21:35:56,216 Epoch 1103: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 21:35:56,218 EPOCH 1104
2024-02-06 21:36:13,838 Epoch 1104: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 21:36:13,839 EPOCH 1105
2024-02-06 21:36:30,878 Epoch 1105: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 21:36:30,879 EPOCH 1106
2024-02-06 21:36:48,211 Epoch 1106: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 21:36:48,211 EPOCH 1107
2024-02-06 21:37:04,482 Epoch 1107: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 21:37:04,484 EPOCH 1108
2024-02-06 21:37:21,652 Epoch 1108: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 21:37:21,653 EPOCH 1109
2024-02-06 21:37:38,178 Epoch 1109: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 21:37:38,179 EPOCH 1110
2024-02-06 21:37:55,645 Epoch 1110: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 21:37:55,646 EPOCH 1111
2024-02-06 21:38:11,833 Epoch 1111: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 21:38:11,835 EPOCH 1112
2024-02-06 21:38:12,321 [Epoch: 1112 Step: 00010000] Batch Recognition Loss:   0.000558 => Gls Tokens per Sec:     2639 || Batch Translation Loss:   0.022343 => Txt Tokens per Sec:     7276 || Lr: 0.000100
2024-02-06 21:39:20,942 Validation result at epoch 1112, step    10000: duration: 68.6201s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.53727	Translation Loss: 90832.82031	PPL: 8711.46582
	Eval Metric: BLEU
	WER 5.44	(DEL: 0.00,	INS: 0.00,	SUB: 5.44)
	BLEU-4 0.45	(BLEU-1: 11.93,	BLEU-2: 3.95,	BLEU-3: 1.44,	BLEU-4: 0.45)
	CHRF 17.65	ROUGE 10.20
2024-02-06 21:39:20,944 Logging Recognition and Translation Outputs
2024-02-06 21:39:20,944 ========================================================================================================================
2024-02-06 21:39:20,944 Logging Sequence: 123_147.00
2024-02-06 21:39:20,944 	Gloss Reference :	A B+C+D+E
2024-02-06 21:39:20,945 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 21:39:20,945 	Gloss Alignment :	         
2024-02-06 21:39:20,945 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 21:39:20,948 	Text Reference  :	the former captain also owns the pontiac firebird trans am  car worth    rs         68 lakh    
2024-02-06 21:39:20,948 	Text Hypothesis :	*** ****** ******* **** **** *** ******* ******** dhoni was a   stunning collection of vehicles
2024-02-06 21:39:20,948 	Text Alignment  :	D   D      D       D    D    D   D       D        S     S   S   S        S          S  S       
2024-02-06 21:39:20,949 ========================================================================================================================
2024-02-06 21:39:20,949 Logging Sequence: 58_196.00
2024-02-06 21:39:20,949 	Gloss Reference :	A B+C+D+E
2024-02-06 21:39:20,949 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 21:39:20,949 	Gloss Alignment :	         
2024-02-06 21:39:20,949 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 21:39:20,950 	Text Reference  :	the ***** *** talents   and   skills of our  athletes knows no    bounds
2024-02-06 21:39:20,950 	Text Hypothesis :	the games are currently being held   in many medals   in    tokyo games 
2024-02-06 21:39:20,951 	Text Alignment  :	    I     I   S         S     S      S  S    S        S     S     S     
2024-02-06 21:39:20,951 ========================================================================================================================
2024-02-06 21:39:20,951 Logging Sequence: 168_184.00
2024-02-06 21:39:20,951 	Gloss Reference :	A B+C+D+E    
2024-02-06 21:39:20,952 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-06 21:39:20,952 	Gloss Alignment :	  S          
2024-02-06 21:39:20,952 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 21:39:20,954 	Text Reference  :	people say  that we    may get     a  true glimpse of    vamika in   february 2022  when    she turns 1  year old 
2024-02-06 21:39:20,954 	Text Hypothesis :	****** they have asked the threats as they have    truly learnt from other    celeb parents and focus on his  team
2024-02-06 21:39:20,954 	Text Alignment  :	D      S    S    S     S   S       S  S    S       S     S      S    S        S     S       S   S     S  S    S   
2024-02-06 21:39:20,954 ========================================================================================================================
2024-02-06 21:39:20,955 Logging Sequence: 87_123.00
2024-02-06 21:39:20,955 	Gloss Reference :	A B+C+D+E
2024-02-06 21:39:20,955 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 21:39:20,955 	Gloss Alignment :	         
2024-02-06 21:39:20,955 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 21:39:20,956 	Text Reference  :	he said that he hoped   kl  rahul would   be fit   for     the upcoming world  cup 
2024-02-06 21:39:20,957 	Text Hypothesis :	** **** **** ** gambhir has been  accused of being jealous of  his      kohli' rise
2024-02-06 21:39:20,957 	Text Alignment  :	D  D    D    D  S       S   S     S       S  S     S       S   S        S      S   
2024-02-06 21:39:20,957 ========================================================================================================================
2024-02-06 21:39:20,957 Logging Sequence: 144_154.00
2024-02-06 21:39:20,957 	Gloss Reference :	A B+C+D+E
2024-02-06 21:39:20,958 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 21:39:20,958 	Gloss Alignment :	         
2024-02-06 21:39:20,958 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 21:39:20,959 	Text Reference  :	*** ****** she   also participated in the rural olympic games organised in   rajasthan a    few months
2024-02-06 21:39:20,959 	Text Hypothesis :	the second match was  held         at the end   of      the   national  team 3         days for help  
2024-02-06 21:39:20,959 	Text Alignment  :	I   I      S     S    S            S      S     S       S     S         S    S         S    S   S     
2024-02-06 21:39:20,959 ========================================================================================================================
2024-02-06 21:39:37,727 Epoch 1112: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 21:39:37,728 EPOCH 1113
2024-02-06 21:39:54,478 Epoch 1113: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 21:39:54,479 EPOCH 1114
2024-02-06 21:40:10,908 Epoch 1114: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 21:40:10,908 EPOCH 1115
2024-02-06 21:40:27,635 Epoch 1115: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 21:40:27,636 EPOCH 1116
2024-02-06 21:40:44,112 Epoch 1116: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 21:40:44,112 EPOCH 1117
2024-02-06 21:41:00,558 Epoch 1117: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 21:41:00,558 EPOCH 1118
2024-02-06 21:41:17,172 Epoch 1118: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 21:41:17,172 EPOCH 1119
2024-02-06 21:41:33,490 Epoch 1119: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-06 21:41:33,490 EPOCH 1120
2024-02-06 21:41:49,781 Epoch 1120: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 21:41:49,781 EPOCH 1121
2024-02-06 21:42:06,062 Epoch 1121: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 21:42:06,063 EPOCH 1122
2024-02-06 21:42:22,650 Epoch 1122: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 21:42:22,650 EPOCH 1123
2024-02-06 21:42:27,317 [Epoch: 1123 Step: 00010100] Batch Recognition Loss:   0.000579 => Gls Tokens per Sec:      356 || Batch Translation Loss:   0.018691 => Txt Tokens per Sec:      998 || Lr: 0.000100
2024-02-06 21:42:38,875 Epoch 1123: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 21:42:38,876 EPOCH 1124
2024-02-06 21:42:55,251 Epoch 1124: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 21:42:55,251 EPOCH 1125
2024-02-06 21:43:11,504 Epoch 1125: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 21:43:11,506 EPOCH 1126
2024-02-06 21:43:27,994 Epoch 1126: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 21:43:27,995 EPOCH 1127
2024-02-06 21:43:44,401 Epoch 1127: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-06 21:43:44,402 EPOCH 1128
2024-02-06 21:44:00,926 Epoch 1128: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-06 21:44:00,926 EPOCH 1129
2024-02-06 21:44:17,050 Epoch 1129: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 21:44:17,051 EPOCH 1130
2024-02-06 21:44:33,781 Epoch 1130: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 21:44:33,781 EPOCH 1131
2024-02-06 21:44:50,227 Epoch 1131: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 21:44:50,228 EPOCH 1132
2024-02-06 21:45:06,503 Epoch 1132: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-06 21:45:06,503 EPOCH 1133
2024-02-06 21:45:23,065 Epoch 1133: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-06 21:45:23,066 EPOCH 1134
2024-02-06 21:45:24,563 [Epoch: 1134 Step: 00010200] Batch Recognition Loss:   0.001014 => Gls Tokens per Sec:     2567 || Batch Translation Loss:   0.024344 => Txt Tokens per Sec:     6585 || Lr: 0.000100
2024-02-06 21:45:39,444 Epoch 1134: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-06 21:45:39,444 EPOCH 1135
2024-02-06 21:45:55,601 Epoch 1135: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 21:45:55,601 EPOCH 1136
2024-02-06 21:46:12,289 Epoch 1136: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-06 21:46:12,290 EPOCH 1137
2024-02-06 21:46:28,768 Epoch 1137: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 21:46:28,769 EPOCH 1138
2024-02-06 21:46:45,409 Epoch 1138: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-06 21:46:45,410 EPOCH 1139
2024-02-06 21:47:01,862 Epoch 1139: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 21:47:01,863 EPOCH 1140
2024-02-06 21:47:18,378 Epoch 1140: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-06 21:47:18,378 EPOCH 1141
2024-02-06 21:47:34,796 Epoch 1141: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 21:47:34,796 EPOCH 1142
2024-02-06 21:47:51,156 Epoch 1142: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 21:47:51,157 EPOCH 1143
2024-02-06 21:48:07,906 Epoch 1143: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 21:48:07,908 EPOCH 1144
2024-02-06 21:48:24,505 Epoch 1144: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-06 21:48:24,506 EPOCH 1145
2024-02-06 21:48:26,422 [Epoch: 1145 Step: 00010300] Batch Recognition Loss:   0.000748 => Gls Tokens per Sec:     2674 || Batch Translation Loss:   0.012314 => Txt Tokens per Sec:     6643 || Lr: 0.000100
2024-02-06 21:48:41,066 Epoch 1145: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 21:48:41,066 EPOCH 1146
2024-02-06 21:48:57,827 Epoch 1146: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 21:48:57,828 EPOCH 1147
2024-02-06 21:49:14,457 Epoch 1147: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-06 21:49:14,458 EPOCH 1148
2024-02-06 21:49:31,154 Epoch 1148: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-06 21:49:31,154 EPOCH 1149
2024-02-06 21:49:47,680 Epoch 1149: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-06 21:49:47,681 EPOCH 1150
2024-02-06 21:50:04,138 Epoch 1150: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 21:50:04,139 EPOCH 1151
2024-02-06 21:50:20,494 Epoch 1151: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-06 21:50:20,495 EPOCH 1152
2024-02-06 21:50:36,872 Epoch 1152: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 21:50:36,873 EPOCH 1153
2024-02-06 21:50:53,461 Epoch 1153: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 21:50:53,461 EPOCH 1154
2024-02-06 21:51:09,791 Epoch 1154: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 21:51:09,791 EPOCH 1155
2024-02-06 21:51:26,913 Epoch 1155: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 21:51:26,913 EPOCH 1156
2024-02-06 21:51:33,209 [Epoch: 1156 Step: 00010400] Batch Recognition Loss:   0.000553 => Gls Tokens per Sec:      874 || Batch Translation Loss:   0.017076 => Txt Tokens per Sec:     2307 || Lr: 0.000100
2024-02-06 21:51:43,986 Epoch 1156: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 21:51:43,987 EPOCH 1157
2024-02-06 21:52:00,660 Epoch 1157: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 21:52:00,660 EPOCH 1158
2024-02-06 21:52:17,437 Epoch 1158: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 21:52:17,438 EPOCH 1159
2024-02-06 21:52:34,279 Epoch 1159: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 21:52:34,280 EPOCH 1160
2024-02-06 21:52:51,529 Epoch 1160: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 21:52:51,530 EPOCH 1161
2024-02-06 21:53:08,221 Epoch 1161: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-06 21:53:08,222 EPOCH 1162
2024-02-06 21:53:25,507 Epoch 1162: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 21:53:25,508 EPOCH 1163
2024-02-06 21:53:42,397 Epoch 1163: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 21:53:42,398 EPOCH 1164
2024-02-06 21:53:59,351 Epoch 1164: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 21:53:59,351 EPOCH 1165
2024-02-06 21:54:16,127 Epoch 1165: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 21:54:16,128 EPOCH 1166
2024-02-06 21:54:33,023 Epoch 1166: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 21:54:33,024 EPOCH 1167
2024-02-06 21:54:42,913 [Epoch: 1167 Step: 00010500] Batch Recognition Loss:   0.000259 => Gls Tokens per Sec:      686 || Batch Translation Loss:   0.011218 => Txt Tokens per Sec:     1939 || Lr: 0.000100
2024-02-06 21:54:50,122 Epoch 1167: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 21:54:50,123 EPOCH 1168
2024-02-06 21:55:07,280 Epoch 1168: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 21:55:07,280 EPOCH 1169
2024-02-06 21:55:24,336 Epoch 1169: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-06 21:55:24,337 EPOCH 1170
2024-02-06 21:55:41,269 Epoch 1170: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 21:55:41,269 EPOCH 1171
2024-02-06 21:55:58,111 Epoch 1171: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 21:55:58,111 EPOCH 1172
2024-02-06 21:56:14,307 Epoch 1172: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 21:56:14,308 EPOCH 1173
2024-02-06 21:56:31,116 Epoch 1173: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 21:56:31,117 EPOCH 1174
2024-02-06 21:56:47,660 Epoch 1174: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 21:56:47,660 EPOCH 1175
2024-02-06 21:57:04,663 Epoch 1175: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 21:57:04,664 EPOCH 1176
2024-02-06 21:57:21,104 Epoch 1176: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-06 21:57:21,105 EPOCH 1177
2024-02-06 21:57:37,760 Epoch 1177: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-06 21:57:37,760 EPOCH 1178
2024-02-06 21:57:53,625 [Epoch: 1178 Step: 00010600] Batch Recognition Loss:   0.000553 => Gls Tokens per Sec:      508 || Batch Translation Loss:   0.069205 => Txt Tokens per Sec:     1527 || Lr: 0.000100
2024-02-06 21:57:54,347 Epoch 1178: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-06 21:57:54,348 EPOCH 1179
2024-02-06 21:58:10,704 Epoch 1179: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-06 21:58:10,705 EPOCH 1180
2024-02-06 21:58:27,025 Epoch 1180: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-06 21:58:27,025 EPOCH 1181
2024-02-06 21:58:43,422 Epoch 1181: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-06 21:58:43,423 EPOCH 1182
2024-02-06 21:58:59,695 Epoch 1182: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 21:58:59,695 EPOCH 1183
2024-02-06 21:59:16,644 Epoch 1183: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 21:59:16,644 EPOCH 1184
2024-02-06 21:59:33,337 Epoch 1184: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 21:59:33,337 EPOCH 1185
2024-02-06 21:59:50,186 Epoch 1185: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 21:59:50,187 EPOCH 1186
2024-02-06 22:00:06,689 Epoch 1186: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 22:00:06,689 EPOCH 1187
2024-02-06 22:00:23,827 Epoch 1187: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 22:00:23,828 EPOCH 1188
2024-02-06 22:00:40,257 Epoch 1188: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 22:00:40,259 EPOCH 1189
2024-02-06 22:00:55,967 [Epoch: 1189 Step: 00010700] Batch Recognition Loss:   0.002359 => Gls Tokens per Sec:      595 || Batch Translation Loss:   0.066778 => Txt Tokens per Sec:     1625 || Lr: 0.000100
2024-02-06 22:00:56,820 Epoch 1189: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 22:00:56,820 EPOCH 1190
2024-02-06 22:01:13,219 Epoch 1190: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 22:01:13,220 EPOCH 1191
2024-02-06 22:01:29,877 Epoch 1191: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-06 22:01:29,878 EPOCH 1192
2024-02-06 22:01:46,511 Epoch 1192: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-06 22:01:46,512 EPOCH 1193
2024-02-06 22:02:02,831 Epoch 1193: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 22:02:02,832 EPOCH 1194
2024-02-06 22:02:19,280 Epoch 1194: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 22:02:19,280 EPOCH 1195
2024-02-06 22:02:35,529 Epoch 1195: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 22:02:35,530 EPOCH 1196
2024-02-06 22:02:51,797 Epoch 1196: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 22:02:51,798 EPOCH 1197
2024-02-06 22:03:08,420 Epoch 1197: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 22:03:08,421 EPOCH 1198
2024-02-06 22:03:25,014 Epoch 1198: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 22:03:25,015 EPOCH 1199
2024-02-06 22:03:41,363 Epoch 1199: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 22:03:41,364 EPOCH 1200
2024-02-06 22:03:58,277 [Epoch: 1200 Step: 00010800] Batch Recognition Loss:   0.000586 => Gls Tokens per Sec:      628 || Batch Translation Loss:   0.066245 => Txt Tokens per Sec:     1737 || Lr: 0.000100
2024-02-06 22:03:58,277 Epoch 1200: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 22:03:58,278 EPOCH 1201
2024-02-06 22:04:14,946 Epoch 1201: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 22:04:14,946 EPOCH 1202
2024-02-06 22:04:31,310 Epoch 1202: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 22:04:31,310 EPOCH 1203
2024-02-06 22:04:47,803 Epoch 1203: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 22:04:47,804 EPOCH 1204
2024-02-06 22:05:04,259 Epoch 1204: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 22:05:04,260 EPOCH 1205
2024-02-06 22:05:21,053 Epoch 1205: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 22:05:21,053 EPOCH 1206
2024-02-06 22:05:37,387 Epoch 1206: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 22:05:37,388 EPOCH 1207
2024-02-06 22:05:53,751 Epoch 1207: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 22:05:53,752 EPOCH 1208
2024-02-06 22:06:10,504 Epoch 1208: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 22:06:10,505 EPOCH 1209
2024-02-06 22:06:27,061 Epoch 1209: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 22:06:27,061 EPOCH 1210
2024-02-06 22:06:43,358 Epoch 1210: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 22:06:43,358 EPOCH 1211
2024-02-06 22:06:59,691 Epoch 1211: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-06 22:06:59,691 EPOCH 1212
2024-02-06 22:07:04,140 [Epoch: 1212 Step: 00010900] Batch Recognition Loss:   0.000674 => Gls Tokens per Sec:       85 || Batch Translation Loss:   0.010560 => Txt Tokens per Sec:      305 || Lr: 0.000100
2024-02-06 22:07:16,587 Epoch 1212: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 22:07:16,588 EPOCH 1213
2024-02-06 22:07:33,012 Epoch 1213: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 22:07:33,012 EPOCH 1214
2024-02-06 22:07:49,619 Epoch 1214: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 22:07:49,619 EPOCH 1215
2024-02-06 22:08:06,321 Epoch 1215: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 22:08:06,322 EPOCH 1216
2024-02-06 22:08:22,547 Epoch 1216: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 22:08:22,547 EPOCH 1217
2024-02-06 22:08:38,973 Epoch 1217: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 22:08:38,974 EPOCH 1218
2024-02-06 22:08:55,241 Epoch 1218: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 22:08:55,241 EPOCH 1219
2024-02-06 22:09:12,021 Epoch 1219: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 22:09:12,022 EPOCH 1220
2024-02-06 22:09:28,529 Epoch 1220: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 22:09:28,530 EPOCH 1221
2024-02-06 22:09:45,409 Epoch 1221: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-06 22:09:45,410 EPOCH 1222
2024-02-06 22:10:01,866 Epoch 1222: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-06 22:10:01,867 EPOCH 1223
2024-02-06 22:10:08,471 [Epoch: 1223 Step: 00011000] Batch Recognition Loss:   0.001162 => Gls Tokens per Sec:      388 || Batch Translation Loss:   0.062990 => Txt Tokens per Sec:     1013 || Lr: 0.000100
2024-02-06 22:10:18,653 Epoch 1223: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 22:10:18,654 EPOCH 1224
2024-02-06 22:10:35,110 Epoch 1224: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 22:10:35,111 EPOCH 1225
2024-02-06 22:10:51,678 Epoch 1225: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 22:10:51,679 EPOCH 1226
2024-02-06 22:11:08,055 Epoch 1226: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 22:11:08,055 EPOCH 1227
2024-02-06 22:11:24,491 Epoch 1227: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 22:11:24,491 EPOCH 1228
2024-02-06 22:11:40,937 Epoch 1228: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 22:11:40,938 EPOCH 1229
2024-02-06 22:11:57,560 Epoch 1229: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 22:11:57,560 EPOCH 1230
2024-02-06 22:12:14,292 Epoch 1230: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 22:12:14,292 EPOCH 1231
2024-02-06 22:12:31,210 Epoch 1231: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 22:12:31,210 EPOCH 1232
2024-02-06 22:12:47,791 Epoch 1232: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-06 22:12:47,792 EPOCH 1233
2024-02-06 22:13:04,033 Epoch 1233: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 22:13:04,034 EPOCH 1234
2024-02-06 22:13:11,217 [Epoch: 1234 Step: 00011100] Batch Recognition Loss:   0.003445 => Gls Tokens per Sec:      535 || Batch Translation Loss:   0.063680 => Txt Tokens per Sec:     1538 || Lr: 0.000100
2024-02-06 22:13:20,763 Epoch 1234: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 22:13:20,764 EPOCH 1235
2024-02-06 22:13:37,256 Epoch 1235: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 22:13:37,256 EPOCH 1236
2024-02-06 22:13:53,466 Epoch 1236: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 22:13:53,466 EPOCH 1237
2024-02-06 22:14:09,971 Epoch 1237: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-06 22:14:09,971 EPOCH 1238
2024-02-06 22:14:26,575 Epoch 1238: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 22:14:26,575 EPOCH 1239
2024-02-06 22:14:43,014 Epoch 1239: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 22:14:43,014 EPOCH 1240
2024-02-06 22:14:59,539 Epoch 1240: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-06 22:14:59,540 EPOCH 1241
2024-02-06 22:15:16,123 Epoch 1241: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-06 22:15:16,124 EPOCH 1242
2024-02-06 22:15:32,633 Epoch 1242: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 22:15:32,633 EPOCH 1243
2024-02-06 22:15:48,913 Epoch 1243: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 22:15:48,914 EPOCH 1244
2024-02-06 22:16:04,945 Epoch 1244: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 22:16:04,945 EPOCH 1245
2024-02-06 22:16:10,566 [Epoch: 1245 Step: 00011200] Batch Recognition Loss:   0.000240 => Gls Tokens per Sec:      751 || Batch Translation Loss:   0.051910 => Txt Tokens per Sec:     1957 || Lr: 0.000100
2024-02-06 22:16:21,387 Epoch 1245: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 22:16:21,388 EPOCH 1246
2024-02-06 22:16:38,048 Epoch 1246: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 22:16:38,049 EPOCH 1247
2024-02-06 22:16:54,492 Epoch 1247: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-06 22:16:54,492 EPOCH 1248
2024-02-06 22:17:10,940 Epoch 1248: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 22:17:10,941 EPOCH 1249
2024-02-06 22:17:27,460 Epoch 1249: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 22:17:27,460 EPOCH 1250
2024-02-06 22:17:43,851 Epoch 1250: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-06 22:17:43,851 EPOCH 1251
2024-02-06 22:18:00,196 Epoch 1251: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 22:18:00,196 EPOCH 1252
2024-02-06 22:18:16,564 Epoch 1252: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-06 22:18:16,564 EPOCH 1253
2024-02-06 22:18:33,458 Epoch 1253: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.71 
2024-02-06 22:18:33,459 EPOCH 1254
2024-02-06 22:18:49,907 Epoch 1254: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-06 22:18:49,907 EPOCH 1255
2024-02-06 22:19:06,176 Epoch 1255: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-06 22:19:06,177 EPOCH 1256
2024-02-06 22:19:14,165 [Epoch: 1256 Step: 00011300] Batch Recognition Loss:   0.000597 => Gls Tokens per Sec:      801 || Batch Translation Loss:   0.115660 => Txt Tokens per Sec:     2202 || Lr: 0.000100
2024-02-06 22:19:22,604 Epoch 1256: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-06 22:19:22,605 EPOCH 1257
2024-02-06 22:19:39,564 Epoch 1257: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.92 
2024-02-06 22:19:39,564 EPOCH 1258
2024-02-06 22:19:55,927 Epoch 1258: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-06 22:19:55,927 EPOCH 1259
2024-02-06 22:20:12,321 Epoch 1259: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-06 22:20:12,322 EPOCH 1260
2024-02-06 22:20:29,281 Epoch 1260: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-06 22:20:29,282 EPOCH 1261
2024-02-06 22:20:45,947 Epoch 1261: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-06 22:20:45,948 EPOCH 1262
2024-02-06 22:21:02,219 Epoch 1262: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-06 22:21:02,220 EPOCH 1263
2024-02-06 22:21:18,501 Epoch 1263: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.27 
2024-02-06 22:21:18,501 EPOCH 1264
2024-02-06 22:21:35,099 Epoch 1264: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.38 
2024-02-06 22:21:35,100 EPOCH 1265
2024-02-06 22:21:51,399 Epoch 1265: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.39 
2024-02-06 22:21:51,400 EPOCH 1266
2024-02-06 22:22:08,089 Epoch 1266: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.71 
2024-02-06 22:22:08,090 EPOCH 1267
2024-02-06 22:22:17,695 [Epoch: 1267 Step: 00011400] Batch Recognition Loss:   0.000966 => Gls Tokens per Sec:      706 || Batch Translation Loss:   0.190577 => Txt Tokens per Sec:     2023 || Lr: 0.000100
2024-02-06 22:22:24,295 Epoch 1267: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.41 
2024-02-06 22:22:24,296 EPOCH 1268
2024-02-06 22:22:40,455 Epoch 1268: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.82 
2024-02-06 22:22:40,456 EPOCH 1269
2024-02-06 22:22:57,115 Epoch 1269: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.63 
2024-02-06 22:22:57,116 EPOCH 1270
2024-02-06 22:23:13,494 Epoch 1270: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.70 
2024-02-06 22:23:13,494 EPOCH 1271
2024-02-06 22:23:30,082 Epoch 1271: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.23 
2024-02-06 22:23:30,083 EPOCH 1272
2024-02-06 22:23:46,363 Epoch 1272: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.62 
2024-02-06 22:23:46,364 EPOCH 1273
2024-02-06 22:24:02,773 Epoch 1273: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.99 
2024-02-06 22:24:02,774 EPOCH 1274
2024-02-06 22:24:19,457 Epoch 1274: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-06 22:24:19,457 EPOCH 1275
2024-02-06 22:24:35,901 Epoch 1275: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 22:24:35,902 EPOCH 1276
2024-02-06 22:24:52,653 Epoch 1276: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 22:24:52,654 EPOCH 1277
2024-02-06 22:25:09,037 Epoch 1277: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 22:25:09,038 EPOCH 1278
2024-02-06 22:25:20,669 [Epoch: 1278 Step: 00011500] Batch Recognition Loss:   0.001435 => Gls Tokens per Sec:      770 || Batch Translation Loss:   0.040339 => Txt Tokens per Sec:     2131 || Lr: 0.000100
2024-02-06 22:25:25,394 Epoch 1278: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-06 22:25:25,394 EPOCH 1279
2024-02-06 22:25:41,658 Epoch 1279: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 22:25:41,658 EPOCH 1280
2024-02-06 22:25:58,491 Epoch 1280: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 22:25:58,492 EPOCH 1281
2024-02-06 22:26:15,243 Epoch 1281: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 22:26:15,245 EPOCH 1282
2024-02-06 22:26:31,851 Epoch 1282: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-06 22:26:31,851 EPOCH 1283
2024-02-06 22:26:48,741 Epoch 1283: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 22:26:48,741 EPOCH 1284
2024-02-06 22:27:05,345 Epoch 1284: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 22:27:05,345 EPOCH 1285
2024-02-06 22:27:21,639 Epoch 1285: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 22:27:21,640 EPOCH 1286
2024-02-06 22:27:38,515 Epoch 1286: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 22:27:38,515 EPOCH 1287
2024-02-06 22:27:54,735 Epoch 1287: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 22:27:54,736 EPOCH 1288
2024-02-06 22:28:11,102 Epoch 1288: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 22:28:11,103 EPOCH 1289
2024-02-06 22:28:27,103 [Epoch: 1289 Step: 00011600] Batch Recognition Loss:   0.000251 => Gls Tokens per Sec:      584 || Batch Translation Loss:   0.017720 => Txt Tokens per Sec:     1606 || Lr: 0.000100
2024-02-06 22:28:27,671 Epoch 1289: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 22:28:27,671 EPOCH 1290
2024-02-06 22:28:44,024 Epoch 1290: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 22:28:44,024 EPOCH 1291
2024-02-06 22:29:00,730 Epoch 1291: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 22:29:00,731 EPOCH 1292
2024-02-06 22:29:17,284 Epoch 1292: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-06 22:29:17,284 EPOCH 1293
2024-02-06 22:29:33,713 Epoch 1293: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-06 22:29:33,713 EPOCH 1294
2024-02-06 22:29:50,178 Epoch 1294: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-06 22:29:50,178 EPOCH 1295
2024-02-06 22:30:06,828 Epoch 1295: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-06 22:30:06,829 EPOCH 1296
2024-02-06 22:30:23,148 Epoch 1296: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-06 22:30:23,148 EPOCH 1297
2024-02-06 22:30:39,479 Epoch 1297: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-06 22:30:39,479 EPOCH 1298
2024-02-06 22:30:55,919 Epoch 1298: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-06 22:30:55,919 EPOCH 1299
2024-02-06 22:31:12,079 Epoch 1299: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-06 22:31:12,080 EPOCH 1300
2024-02-06 22:31:28,351 [Epoch: 1300 Step: 00011700] Batch Recognition Loss:   0.003430 => Gls Tokens per Sec:      653 || Batch Translation Loss:   0.024065 => Txt Tokens per Sec:     1806 || Lr: 0.000100
2024-02-06 22:31:28,352 Epoch 1300: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-06 22:31:28,352 EPOCH 1301
2024-02-06 22:31:44,942 Epoch 1301: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-06 22:31:44,943 EPOCH 1302
2024-02-06 22:32:01,291 Epoch 1302: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-06 22:32:01,292 EPOCH 1303
2024-02-06 22:32:18,033 Epoch 1303: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-06 22:32:18,034 EPOCH 1304
2024-02-06 22:32:34,510 Epoch 1304: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-06 22:32:34,510 EPOCH 1305
2024-02-06 22:32:50,535 Epoch 1305: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-06 22:32:50,536 EPOCH 1306
2024-02-06 22:33:06,975 Epoch 1306: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-06 22:33:06,976 EPOCH 1307
2024-02-06 22:33:23,503 Epoch 1307: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-06 22:33:23,504 EPOCH 1308
2024-02-06 22:33:39,918 Epoch 1308: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-06 22:33:39,919 EPOCH 1309
2024-02-06 22:33:56,621 Epoch 1309: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-06 22:33:56,622 EPOCH 1310
2024-02-06 22:34:12,944 Epoch 1310: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-06 22:34:12,944 EPOCH 1311
2024-02-06 22:34:29,309 Epoch 1311: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-06 22:34:29,310 EPOCH 1312
2024-02-06 22:34:29,578 [Epoch: 1312 Step: 00011800] Batch Recognition Loss:   0.000365 => Gls Tokens per Sec:     4776 || Batch Translation Loss:   0.015769 => Txt Tokens per Sec:    10716 || Lr: 0.000100
2024-02-06 22:34:45,643 Epoch 1312: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-06 22:34:45,644 EPOCH 1313
2024-02-06 22:35:02,026 Epoch 1313: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-06 22:35:02,026 EPOCH 1314
2024-02-06 22:35:18,510 Epoch 1314: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-06 22:35:18,511 EPOCH 1315
2024-02-06 22:35:35,101 Epoch 1315: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 22:35:35,102 EPOCH 1316
2024-02-06 22:35:51,695 Epoch 1316: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-06 22:35:51,696 EPOCH 1317
2024-02-06 22:36:07,944 Epoch 1317: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-06 22:36:07,944 EPOCH 1318
2024-02-06 22:36:24,326 Epoch 1318: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-06 22:36:24,326 EPOCH 1319
2024-02-06 22:36:40,787 Epoch 1319: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-06 22:36:40,788 EPOCH 1320
2024-02-06 22:36:57,127 Epoch 1320: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-06 22:36:57,128 EPOCH 1321
2024-02-06 22:37:13,647 Epoch 1321: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-06 22:37:13,648 EPOCH 1322
2024-02-06 22:37:29,885 Epoch 1322: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-06 22:37:29,885 EPOCH 1323
2024-02-06 22:37:34,854 [Epoch: 1323 Step: 00011900] Batch Recognition Loss:   0.000692 => Gls Tokens per Sec:      334 || Batch Translation Loss:   0.028750 => Txt Tokens per Sec:     1051 || Lr: 0.000100
2024-02-06 22:37:46,125 Epoch 1323: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-06 22:37:46,125 EPOCH 1324
2024-02-06 22:38:02,622 Epoch 1324: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-06 22:38:02,623 EPOCH 1325
2024-02-06 22:38:19,028 Epoch 1325: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-06 22:38:19,028 EPOCH 1326
2024-02-06 22:38:35,539 Epoch 1326: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-06 22:38:35,540 EPOCH 1327
2024-02-06 22:38:52,095 Epoch 1327: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 22:38:52,095 EPOCH 1328
2024-02-06 22:39:08,498 Epoch 1328: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 22:39:08,498 EPOCH 1329
2024-02-06 22:39:25,073 Epoch 1329: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-06 22:39:25,074 EPOCH 1330
2024-02-06 22:39:41,623 Epoch 1330: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 22:39:41,624 EPOCH 1331
2024-02-06 22:39:57,952 Epoch 1331: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-06 22:39:57,953 EPOCH 1332
2024-02-06 22:40:14,395 Epoch 1332: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 22:40:14,396 EPOCH 1333
2024-02-06 22:40:31,096 Epoch 1333: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 22:40:31,097 EPOCH 1334
2024-02-06 22:40:35,765 [Epoch: 1334 Step: 00012000] Batch Recognition Loss:   0.001931 => Gls Tokens per Sec:      823 || Batch Translation Loss:   0.022912 => Txt Tokens per Sec:     2459 || Lr: 0.000100
2024-02-06 22:41:43,908 Validation result at epoch 1334, step    12000: duration: 68.1410s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.58935	Translation Loss: 92596.48438	PPL: 10389.48438
	Eval Metric: BLEU
	WER 5.01	(DEL: 0.00,	INS: 0.00,	SUB: 5.01)
	BLEU-4 0.00	(BLEU-1: 12.06,	BLEU-2: 3.75,	BLEU-3: 1.25,	BLEU-4: 0.00)
	CHRF 17.89	ROUGE 9.72
2024-02-06 22:41:43,910 Logging Recognition and Translation Outputs
2024-02-06 22:41:43,910 ========================================================================================================================
2024-02-06 22:41:43,910 Logging Sequence: 168_56.00
2024-02-06 22:41:43,911 	Gloss Reference :	A B+C+D+E
2024-02-06 22:41:43,911 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 22:41:43,911 	Gloss Alignment :	         
2024-02-06 22:41:43,911 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 22:41:43,913 	Text Reference  :	fans have  been waiting to ***** ** *** ***** *** **** *** *** ******* *** ** see   vamika for  a  long time
2024-02-06 22:41:43,913 	Text Hypothesis :	**** kohli has  refused to focus on the child and they are the parents and it never take   part of the  game
2024-02-06 22:41:43,913 	Text Alignment  :	D    S     S    S          I     I  I   I     I   I    I   I   I       I   I  S     S      S    S  S    S   
2024-02-06 22:41:43,913 ========================================================================================================================
2024-02-06 22:41:43,913 Logging Sequence: 161_74.00
2024-02-06 22:41:43,914 	Gloss Reference :	A B+C+D+E
2024-02-06 22:41:43,914 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 22:41:43,914 	Gloss Alignment :	         
2024-02-06 22:41:43,914 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 22:41:43,915 	Text Reference  :	*** **** *** *** ******* ** i    am      proud of      the indian team's   achievements
2024-02-06 22:41:43,915 	Text Hypothesis :	the bcci can not allowed to wear clothes that  shocked the ****** football team        
2024-02-06 22:41:43,915 	Text Alignment  :	I   I    I   I   I       I  S    S       S     S           D      S        S           
2024-02-06 22:41:43,915 ========================================================================================================================
2024-02-06 22:41:43,916 Logging Sequence: 111_83.00
2024-02-06 22:41:43,916 	Gloss Reference :	A B+C+D+E
2024-02-06 22:41:43,916 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 22:41:43,916 	Gloss Alignment :	         
2024-02-06 22:41:43,916 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 22:41:43,918 	Text Reference  :	and   the other 10 team members are       fined 25   of the  match fee or rs    6  lakh 
2024-02-06 22:41:43,918 	Text Hypothesis :	after the ***** ** bcci is      extremely fit   when i  want to    win a  medal in delhi
2024-02-06 22:41:43,918 	Text Alignment  :	S         D     D  S    S       S         S     S    S  S    S     S   S  S     S  S    
2024-02-06 22:41:43,918 ========================================================================================================================
2024-02-06 22:41:43,918 Logging Sequence: 61_218.00
2024-02-06 22:41:43,919 	Gloss Reference :	A B+C+D+E
2024-02-06 22:41:43,919 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 22:41:43,919 	Gloss Alignment :	         
2024-02-06 22:41:43,919 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 22:41:43,920 	Text Reference  :	***** in 2020  a         woman had said         at the  press conference
2024-02-06 22:41:43,920 	Text Hypothesis :	after 28 years argentina won   the similarities in just world cup       
2024-02-06 22:41:43,920 	Text Alignment  :	I     S  S     S         S     S   S            S  S    S     S         
2024-02-06 22:41:43,920 ========================================================================================================================
2024-02-06 22:41:43,920 Logging Sequence: 94_123.00
2024-02-06 22:41:43,921 	Gloss Reference :	A B+C+D+E
2024-02-06 22:41:43,921 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 22:41:43,921 	Gloss Alignment :	         
2024-02-06 22:41:43,921 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 22:41:43,923 	Text Reference  :	* ****** the     venue narendra modi stadium for the india-pakistan match has    been      kept    the   same people   can book    flights etc  
2024-02-06 22:41:43,923 	Text Hypothesis :	a police officer said  that     the  plans   for the ************** ***** eighth encounter between india and  pakistan had playing the     plans
2024-02-06 22:41:43,923 	Text Alignment  :	I I      S       S     S        S    S               D              D     S      S         S       S     S    S        S   S       S       S    
2024-02-06 22:41:43,923 ========================================================================================================================
2024-02-06 22:41:56,640 Epoch 1334: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-06 22:41:56,641 EPOCH 1335
2024-02-06 22:42:13,465 Epoch 1335: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-06 22:42:13,466 EPOCH 1336
2024-02-06 22:42:30,028 Epoch 1336: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-06 22:42:30,029 EPOCH 1337
2024-02-06 22:42:46,317 Epoch 1337: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 22:42:46,318 EPOCH 1338
2024-02-06 22:43:02,921 Epoch 1338: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-06 22:43:02,921 EPOCH 1339
2024-02-06 22:43:19,626 Epoch 1339: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-06 22:43:19,626 EPOCH 1340
2024-02-06 22:43:36,181 Epoch 1340: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-06 22:43:36,182 EPOCH 1341
2024-02-06 22:43:52,729 Epoch 1341: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-06 22:43:52,730 EPOCH 1342
2024-02-06 22:44:09,214 Epoch 1342: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-06 22:44:09,214 EPOCH 1343
2024-02-06 22:44:25,684 Epoch 1343: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-06 22:44:25,684 EPOCH 1344
2024-02-06 22:44:42,095 Epoch 1344: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-06 22:44:42,096 EPOCH 1345
2024-02-06 22:44:43,615 [Epoch: 1345 Step: 00012100] Batch Recognition Loss:   0.000209 => Gls Tokens per Sec:     3370 || Batch Translation Loss:   0.017138 => Txt Tokens per Sec:     8134 || Lr: 0.000100
2024-02-06 22:44:58,305 Epoch 1345: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-06 22:44:58,305 EPOCH 1346
2024-02-06 22:45:14,840 Epoch 1346: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-06 22:45:14,841 EPOCH 1347
2024-02-06 22:45:31,149 Epoch 1347: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-06 22:45:31,150 EPOCH 1348
2024-02-06 22:45:48,130 Epoch 1348: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-06 22:45:48,130 EPOCH 1349
2024-02-06 22:46:04,380 Epoch 1349: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 22:46:04,381 EPOCH 1350
2024-02-06 22:46:20,826 Epoch 1350: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.09 
2024-02-06 22:46:20,826 EPOCH 1351
2024-02-06 22:46:37,448 Epoch 1351: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-06 22:46:37,448 EPOCH 1352
2024-02-06 22:46:53,766 Epoch 1352: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-06 22:46:53,766 EPOCH 1353
2024-02-06 22:47:10,240 Epoch 1353: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-06 22:47:10,241 EPOCH 1354
2024-02-06 22:47:26,800 Epoch 1354: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-06 22:47:26,801 EPOCH 1355
2024-02-06 22:47:43,083 Epoch 1355: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.43 
2024-02-06 22:47:43,083 EPOCH 1356
2024-02-06 22:47:49,197 [Epoch: 1356 Step: 00012200] Batch Recognition Loss:   0.000872 => Gls Tokens per Sec:      900 || Batch Translation Loss:   0.132436 => Txt Tokens per Sec:     2257 || Lr: 0.000100
2024-02-06 22:47:59,978 Epoch 1356: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.74 
2024-02-06 22:47:59,979 EPOCH 1357
2024-02-06 22:48:16,515 Epoch 1357: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.64 
2024-02-06 22:48:16,516 EPOCH 1358
2024-02-06 22:48:33,175 Epoch 1358: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-06 22:48:33,175 EPOCH 1359
2024-02-06 22:48:49,659 Epoch 1359: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-06 22:48:49,659 EPOCH 1360
2024-02-06 22:49:06,184 Epoch 1360: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-06 22:49:06,184 EPOCH 1361
2024-02-06 22:49:22,508 Epoch 1361: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-06 22:49:22,509 EPOCH 1362
2024-02-06 22:49:39,385 Epoch 1362: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 22:49:39,386 EPOCH 1363
2024-02-06 22:49:55,947 Epoch 1363: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 22:49:55,948 EPOCH 1364
2024-02-06 22:50:12,376 Epoch 1364: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 22:50:12,376 EPOCH 1365
2024-02-06 22:50:28,818 Epoch 1365: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 22:50:28,819 EPOCH 1366
2024-02-06 22:50:45,304 Epoch 1366: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-06 22:50:45,305 EPOCH 1367
2024-02-06 22:50:54,900 [Epoch: 1367 Step: 00012300] Batch Recognition Loss:   0.000849 => Gls Tokens per Sec:      707 || Batch Translation Loss:   0.043730 => Txt Tokens per Sec:     1996 || Lr: 0.000100
2024-02-06 22:51:01,946 Epoch 1367: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 22:51:01,947 EPOCH 1368
2024-02-06 22:51:18,454 Epoch 1368: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 22:51:18,455 EPOCH 1369
2024-02-06 22:51:35,067 Epoch 1369: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 22:51:35,068 EPOCH 1370
2024-02-06 22:51:51,475 Epoch 1370: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 22:51:51,476 EPOCH 1371
2024-02-06 22:52:07,954 Epoch 1371: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 22:52:07,954 EPOCH 1372
2024-02-06 22:52:24,272 Epoch 1372: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 22:52:24,273 EPOCH 1373
2024-02-06 22:52:41,068 Epoch 1373: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-06 22:52:41,068 EPOCH 1374
2024-02-06 22:52:57,271 Epoch 1374: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-06 22:52:57,272 EPOCH 1375
2024-02-06 22:53:13,749 Epoch 1375: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 22:53:13,750 EPOCH 1376
2024-02-06 22:53:30,003 Epoch 1376: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 22:53:30,004 EPOCH 1377
2024-02-06 22:53:46,520 Epoch 1377: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 22:53:46,521 EPOCH 1378
2024-02-06 22:53:59,735 [Epoch: 1378 Step: 00012400] Batch Recognition Loss:   0.001377 => Gls Tokens per Sec:      610 || Batch Translation Loss:   0.034374 => Txt Tokens per Sec:     1741 || Lr: 0.000100
2024-02-06 22:54:03,223 Epoch 1378: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 22:54:03,223 EPOCH 1379
2024-02-06 22:54:19,830 Epoch 1379: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-06 22:54:19,831 EPOCH 1380
2024-02-06 22:54:36,132 Epoch 1380: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 22:54:36,133 EPOCH 1381
2024-02-06 22:54:52,986 Epoch 1381: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 22:54:52,986 EPOCH 1382
2024-02-06 22:55:08,925 Epoch 1382: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 22:55:08,925 EPOCH 1383
2024-02-06 22:55:25,527 Epoch 1383: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-06 22:55:25,527 EPOCH 1384
2024-02-06 22:55:41,822 Epoch 1384: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 22:55:41,822 EPOCH 1385
2024-02-06 22:55:58,248 Epoch 1385: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 22:55:58,248 EPOCH 1386
2024-02-06 22:56:14,949 Epoch 1386: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 22:56:14,950 EPOCH 1387
2024-02-06 22:56:31,425 Epoch 1387: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-06 22:56:31,426 EPOCH 1388
2024-02-06 22:56:47,888 Epoch 1388: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-06 22:56:47,889 EPOCH 1389
2024-02-06 22:56:58,246 [Epoch: 1389 Step: 00012500] Batch Recognition Loss:   0.000515 => Gls Tokens per Sec:      902 || Batch Translation Loss:   0.021577 => Txt Tokens per Sec:     2411 || Lr: 0.000100
2024-02-06 22:57:04,277 Epoch 1389: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-06 22:57:04,277 EPOCH 1390
2024-02-06 22:57:20,421 Epoch 1390: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 22:57:20,422 EPOCH 1391
2024-02-06 22:57:36,987 Epoch 1391: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 22:57:36,988 EPOCH 1392
2024-02-06 22:57:53,141 Epoch 1392: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-06 22:57:53,142 EPOCH 1393
2024-02-06 22:58:09,919 Epoch 1393: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-06 22:58:09,919 EPOCH 1394
2024-02-06 22:58:26,213 Epoch 1394: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-06 22:58:26,213 EPOCH 1395
2024-02-06 22:58:42,782 Epoch 1395: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-06 22:58:42,783 EPOCH 1396
2024-02-06 22:58:59,388 Epoch 1396: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 22:58:59,389 EPOCH 1397
2024-02-06 22:59:15,725 Epoch 1397: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 22:59:15,726 EPOCH 1398
2024-02-06 22:59:32,422 Epoch 1398: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 22:59:32,423 EPOCH 1399
2024-02-06 22:59:48,869 Epoch 1399: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 22:59:48,870 EPOCH 1400
2024-02-06 23:00:04,956 [Epoch: 1400 Step: 00012600] Batch Recognition Loss:   0.000389 => Gls Tokens per Sec:      660 || Batch Translation Loss:   0.060857 => Txt Tokens per Sec:     1827 || Lr: 0.000100
2024-02-06 23:00:04,956 Epoch 1400: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 23:00:04,957 EPOCH 1401
2024-02-06 23:00:21,461 Epoch 1401: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 23:00:21,462 EPOCH 1402
2024-02-06 23:00:38,231 Epoch 1402: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-06 23:00:38,232 EPOCH 1403
2024-02-06 23:00:54,846 Epoch 1403: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-06 23:00:54,847 EPOCH 1404
2024-02-06 23:01:11,044 Epoch 1404: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-06 23:01:11,045 EPOCH 1405
2024-02-06 23:01:27,494 Epoch 1405: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-06 23:01:27,494 EPOCH 1406
2024-02-06 23:01:43,776 Epoch 1406: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-06 23:01:43,776 EPOCH 1407
2024-02-06 23:02:00,194 Epoch 1407: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 23:02:00,194 EPOCH 1408
2024-02-06 23:02:16,834 Epoch 1408: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 23:02:16,835 EPOCH 1409
2024-02-06 23:02:33,289 Epoch 1409: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 23:02:33,289 EPOCH 1410
2024-02-06 23:02:49,758 Epoch 1410: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.20 
2024-02-06 23:02:49,759 EPOCH 1411
2024-02-06 23:03:06,107 Epoch 1411: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-06 23:03:06,107 EPOCH 1412
2024-02-06 23:03:06,371 [Epoch: 1412 Step: 00012700] Batch Recognition Loss:   0.000943 => Gls Tokens per Sec:     4886 || Batch Translation Loss:   0.011925 => Txt Tokens per Sec:     8821 || Lr: 0.000100
2024-02-06 23:03:22,466 Epoch 1412: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 23:03:22,467 EPOCH 1413
2024-02-06 23:03:38,935 Epoch 1413: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-06 23:03:38,935 EPOCH 1414
2024-02-06 23:03:55,246 Epoch 1414: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-06 23:03:55,247 EPOCH 1415
2024-02-06 23:04:11,788 Epoch 1415: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-06 23:04:11,788 EPOCH 1416
2024-02-06 23:04:28,372 Epoch 1416: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-06 23:04:28,373 EPOCH 1417
2024-02-06 23:04:45,014 Epoch 1417: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-06 23:04:45,016 EPOCH 1418
2024-02-06 23:05:01,507 Epoch 1418: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-06 23:05:01,507 EPOCH 1419
2024-02-06 23:05:17,881 Epoch 1419: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 23:05:17,882 EPOCH 1420
2024-02-06 23:05:34,418 Epoch 1420: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-06 23:05:34,419 EPOCH 1421
2024-02-06 23:05:50,627 Epoch 1421: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-06 23:05:50,627 EPOCH 1422
2024-02-06 23:06:07,011 Epoch 1422: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 23:06:07,011 EPOCH 1423
2024-02-06 23:06:13,407 [Epoch: 1423 Step: 00012800] Batch Recognition Loss:   0.001729 => Gls Tokens per Sec:      400 || Batch Translation Loss:   0.030197 => Txt Tokens per Sec:     1048 || Lr: 0.000100
2024-02-06 23:06:23,624 Epoch 1423: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 23:06:23,624 EPOCH 1424
2024-02-06 23:06:40,100 Epoch 1424: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-06 23:06:40,100 EPOCH 1425
2024-02-06 23:06:56,439 Epoch 1425: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 23:06:56,440 EPOCH 1426
2024-02-06 23:07:12,872 Epoch 1426: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 23:07:12,873 EPOCH 1427
2024-02-06 23:07:29,329 Epoch 1427: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-06 23:07:29,330 EPOCH 1428
2024-02-06 23:07:45,887 Epoch 1428: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-06 23:07:45,887 EPOCH 1429
2024-02-06 23:08:02,245 Epoch 1429: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-06 23:08:02,246 EPOCH 1430
2024-02-06 23:08:18,907 Epoch 1430: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 23:08:18,908 EPOCH 1431
2024-02-06 23:08:35,379 Epoch 1431: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-06 23:08:35,380 EPOCH 1432
2024-02-06 23:08:51,749 Epoch 1432: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 23:08:51,750 EPOCH 1433
2024-02-06 23:09:08,390 Epoch 1433: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.49 
2024-02-06 23:09:08,391 EPOCH 1434
2024-02-06 23:09:12,440 [Epoch: 1434 Step: 00012900] Batch Recognition Loss:   0.001585 => Gls Tokens per Sec:      948 || Batch Translation Loss:   0.058787 => Txt Tokens per Sec:     2789 || Lr: 0.000100
2024-02-06 23:09:24,467 Epoch 1434: Total Training Recognition Loss 0.11  Total Training Translation Loss 0.44 
2024-02-06 23:09:24,468 EPOCH 1435
2024-02-06 23:09:41,198 Epoch 1435: Total Training Recognition Loss 2.33  Total Training Translation Loss 0.56 
2024-02-06 23:09:41,199 EPOCH 1436
2024-02-06 23:09:57,476 Epoch 1436: Total Training Recognition Loss 3.68  Total Training Translation Loss 0.97 
2024-02-06 23:09:57,476 EPOCH 1437
2024-02-06 23:10:13,872 Epoch 1437: Total Training Recognition Loss 2.26  Total Training Translation Loss 1.92 
2024-02-06 23:10:13,872 EPOCH 1438
2024-02-06 23:10:30,169 Epoch 1438: Total Training Recognition Loss 0.76  Total Training Translation Loss 2.79 
2024-02-06 23:10:30,170 EPOCH 1439
2024-02-06 23:10:46,359 Epoch 1439: Total Training Recognition Loss 0.11  Total Training Translation Loss 5.23 
2024-02-06 23:10:46,360 EPOCH 1440
2024-02-06 23:11:02,989 Epoch 1440: Total Training Recognition Loss 0.12  Total Training Translation Loss 9.33 
2024-02-06 23:11:02,990 EPOCH 1441
2024-02-06 23:11:19,075 Epoch 1441: Total Training Recognition Loss 0.09  Total Training Translation Loss 4.89 
2024-02-06 23:11:19,076 EPOCH 1442
2024-02-06 23:11:35,437 Epoch 1442: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.08 
2024-02-06 23:11:35,438 EPOCH 1443
2024-02-06 23:11:51,785 Epoch 1443: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.82 
2024-02-06 23:11:51,786 EPOCH 1444
2024-02-06 23:12:08,195 Epoch 1444: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.02 
2024-02-06 23:12:08,196 EPOCH 1445
2024-02-06 23:12:13,895 [Epoch: 1445 Step: 00013000] Batch Recognition Loss:   0.002198 => Gls Tokens per Sec:      741 || Batch Translation Loss:   0.088596 => Txt Tokens per Sec:     1822 || Lr: 0.000100
2024-02-06 23:12:24,859 Epoch 1445: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.73 
2024-02-06 23:12:24,860 EPOCH 1446
2024-02-06 23:12:41,119 Epoch 1446: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.55 
2024-02-06 23:12:41,120 EPOCH 1447
2024-02-06 23:12:57,441 Epoch 1447: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.38 
2024-02-06 23:12:57,442 EPOCH 1448
2024-02-06 23:13:14,106 Epoch 1448: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.34 
2024-02-06 23:13:14,107 EPOCH 1449
2024-02-06 23:13:30,322 Epoch 1449: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.28 
2024-02-06 23:13:30,323 EPOCH 1450
2024-02-06 23:13:46,962 Epoch 1450: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.25 
2024-02-06 23:13:46,962 EPOCH 1451
2024-02-06 23:14:03,451 Epoch 1451: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.25 
2024-02-06 23:14:03,452 EPOCH 1452
2024-02-06 23:14:19,811 Epoch 1452: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.24 
2024-02-06 23:14:19,812 EPOCH 1453
2024-02-06 23:14:36,502 Epoch 1453: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.24 
2024-02-06 23:14:36,503 EPOCH 1454
2024-02-06 23:14:52,935 Epoch 1454: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.21 
2024-02-06 23:14:52,936 EPOCH 1455
2024-02-06 23:15:08,894 Epoch 1455: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 23:15:08,894 EPOCH 1456
2024-02-06 23:15:13,912 [Epoch: 1456 Step: 00013100] Batch Recognition Loss:   0.000695 => Gls Tokens per Sec:     1276 || Batch Translation Loss:   0.027221 => Txt Tokens per Sec:     3262 || Lr: 0.000100
2024-02-06 23:15:25,812 Epoch 1456: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-06 23:15:25,812 EPOCH 1457
2024-02-06 23:15:42,220 Epoch 1457: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.21 
2024-02-06 23:15:42,221 EPOCH 1458
2024-02-06 23:15:59,065 Epoch 1458: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 23:15:59,066 EPOCH 1459
2024-02-06 23:16:15,566 Epoch 1459: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.26 
2024-02-06 23:16:15,567 EPOCH 1460
2024-02-06 23:16:32,105 Epoch 1460: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.24 
2024-02-06 23:16:32,105 EPOCH 1461
2024-02-06 23:16:48,722 Epoch 1461: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 23:16:48,723 EPOCH 1462
2024-02-06 23:17:05,223 Epoch 1462: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 23:17:05,224 EPOCH 1463
2024-02-06 23:17:21,723 Epoch 1463: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.20 
2024-02-06 23:17:21,724 EPOCH 1464
2024-02-06 23:17:37,899 Epoch 1464: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 23:17:37,900 EPOCH 1465
2024-02-06 23:17:54,160 Epoch 1465: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-06 23:17:54,161 EPOCH 1466
2024-02-06 23:18:10,731 Epoch 1466: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-06 23:18:10,731 EPOCH 1467
2024-02-06 23:18:20,264 [Epoch: 1467 Step: 00013200] Batch Recognition Loss:   0.006144 => Gls Tokens per Sec:      711 || Batch Translation Loss:   0.008296 => Txt Tokens per Sec:     2030 || Lr: 0.000100
2024-02-06 23:18:27,082 Epoch 1467: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-06 23:18:27,083 EPOCH 1468
2024-02-06 23:18:43,563 Epoch 1468: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-06 23:18:43,563 EPOCH 1469
2024-02-06 23:19:00,352 Epoch 1469: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-06 23:19:00,353 EPOCH 1470
2024-02-06 23:19:16,650 Epoch 1470: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-06 23:19:16,650 EPOCH 1471
2024-02-06 23:19:33,073 Epoch 1471: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-06 23:19:33,073 EPOCH 1472
2024-02-06 23:19:49,403 Epoch 1472: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-06 23:19:49,403 EPOCH 1473
2024-02-06 23:20:06,126 Epoch 1473: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-06 23:20:06,127 EPOCH 1474
2024-02-06 23:20:22,473 Epoch 1474: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-06 23:20:22,474 EPOCH 1475
2024-02-06 23:20:39,330 Epoch 1475: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-06 23:20:39,331 EPOCH 1476
2024-02-06 23:20:55,671 Epoch 1476: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-06 23:20:55,671 EPOCH 1477
2024-02-06 23:21:12,245 Epoch 1477: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-06 23:21:12,246 EPOCH 1478
2024-02-06 23:21:27,842 [Epoch: 1478 Step: 00013300] Batch Recognition Loss:   0.000855 => Gls Tokens per Sec:      517 || Batch Translation Loss:   0.014617 => Txt Tokens per Sec:     1419 || Lr: 0.000100
2024-02-06 23:21:28,986 Epoch 1478: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-06 23:21:28,986 EPOCH 1479
2024-02-06 23:21:45,511 Epoch 1479: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-06 23:21:45,512 EPOCH 1480
2024-02-06 23:22:01,889 Epoch 1480: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-06 23:22:01,890 EPOCH 1481
2024-02-06 23:22:18,255 Epoch 1481: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-06 23:22:18,256 EPOCH 1482
2024-02-06 23:22:34,396 Epoch 1482: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-06 23:22:34,397 EPOCH 1483
2024-02-06 23:22:51,263 Epoch 1483: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-06 23:22:51,264 EPOCH 1484
2024-02-06 23:23:07,800 Epoch 1484: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-06 23:23:07,800 EPOCH 1485
2024-02-06 23:23:24,105 Epoch 1485: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-06 23:23:24,106 EPOCH 1486
2024-02-06 23:23:40,280 Epoch 1486: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-06 23:23:40,281 EPOCH 1487
2024-02-06 23:23:56,667 Epoch 1487: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-06 23:23:56,668 EPOCH 1488
2024-02-06 23:24:13,671 Epoch 1488: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-06 23:24:13,672 EPOCH 1489
2024-02-06 23:24:23,909 [Epoch: 1489 Step: 00013400] Batch Recognition Loss:   0.000238 => Gls Tokens per Sec:      913 || Batch Translation Loss:   0.017104 => Txt Tokens per Sec:     2439 || Lr: 0.000100
2024-02-06 23:24:30,052 Epoch 1489: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-06 23:24:30,052 EPOCH 1490
2024-02-06 23:24:46,662 Epoch 1490: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-06 23:24:46,662 EPOCH 1491
2024-02-06 23:25:03,097 Epoch 1491: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-06 23:25:03,098 EPOCH 1492
2024-02-06 23:25:19,352 Epoch 1492: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-06 23:25:19,353 EPOCH 1493
2024-02-06 23:25:35,842 Epoch 1493: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-06 23:25:35,842 EPOCH 1494
2024-02-06 23:25:52,353 Epoch 1494: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-06 23:25:52,354 EPOCH 1495
2024-02-06 23:26:08,749 Epoch 1495: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 23:26:08,749 EPOCH 1496
2024-02-06 23:26:25,347 Epoch 1496: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 23:26:25,348 EPOCH 1497
2024-02-06 23:26:41,738 Epoch 1497: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-06 23:26:41,738 EPOCH 1498
2024-02-06 23:26:58,180 Epoch 1498: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-06 23:26:58,181 EPOCH 1499
2024-02-06 23:27:14,727 Epoch 1499: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 23:27:14,728 EPOCH 1500
2024-02-06 23:27:31,061 [Epoch: 1500 Step: 00013500] Batch Recognition Loss:   0.002002 => Gls Tokens per Sec:      650 || Batch Translation Loss:   0.051332 => Txt Tokens per Sec:     1799 || Lr: 0.000100
2024-02-06 23:27:31,062 Epoch 1500: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 23:27:31,062 EPOCH 1501
2024-02-06 23:27:47,267 Epoch 1501: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-06 23:27:47,267 EPOCH 1502
2024-02-06 23:28:03,803 Epoch 1502: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 23:28:03,804 EPOCH 1503
2024-02-06 23:28:20,136 Epoch 1503: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-06 23:28:20,136 EPOCH 1504
2024-02-06 23:28:36,680 Epoch 1504: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 23:28:36,681 EPOCH 1505
2024-02-06 23:28:53,412 Epoch 1505: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 23:28:53,413 EPOCH 1506
2024-02-06 23:29:09,940 Epoch 1506: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-06 23:29:09,940 EPOCH 1507
2024-02-06 23:29:26,459 Epoch 1507: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 23:29:26,459 EPOCH 1508
2024-02-06 23:29:42,433 Epoch 1508: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 23:29:42,434 EPOCH 1509
2024-02-06 23:29:58,845 Epoch 1509: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 23:29:58,845 EPOCH 1510
2024-02-06 23:30:15,439 Epoch 1510: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-06 23:30:15,440 EPOCH 1511
2024-02-06 23:30:31,839 Epoch 1511: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 23:30:31,839 EPOCH 1512
2024-02-06 23:30:38,009 [Epoch: 1512 Step: 00013600] Batch Recognition Loss:   0.000769 => Gls Tokens per Sec:      208 || Batch Translation Loss:   0.036726 => Txt Tokens per Sec:      715 || Lr: 0.000100
2024-02-06 23:30:48,634 Epoch 1512: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 23:30:48,634 EPOCH 1513
2024-02-06 23:31:05,068 Epoch 1513: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 23:31:05,069 EPOCH 1514
2024-02-06 23:31:21,691 Epoch 1514: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-06 23:31:21,692 EPOCH 1515
2024-02-06 23:31:37,884 Epoch 1515: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-06 23:31:37,885 EPOCH 1516
2024-02-06 23:31:54,576 Epoch 1516: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-06 23:31:54,576 EPOCH 1517
2024-02-06 23:32:11,007 Epoch 1517: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-06 23:32:11,007 EPOCH 1518
2024-02-06 23:32:27,716 Epoch 1518: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-06 23:32:27,717 EPOCH 1519
2024-02-06 23:32:44,370 Epoch 1519: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-06 23:32:44,371 EPOCH 1520
2024-02-06 23:33:00,762 Epoch 1520: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-06 23:33:00,763 EPOCH 1521
2024-02-06 23:33:17,276 Epoch 1521: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 23:33:17,277 EPOCH 1522
2024-02-06 23:33:33,651 Epoch 1522: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-06 23:33:33,652 EPOCH 1523
2024-02-06 23:33:34,610 [Epoch: 1523 Step: 00013700] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     2676 || Batch Translation Loss:   0.025928 => Txt Tokens per Sec:     6689 || Lr: 0.000100
2024-02-06 23:33:50,229 Epoch 1523: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-06 23:33:50,230 EPOCH 1524
2024-02-06 23:34:06,400 Epoch 1524: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 23:34:06,400 EPOCH 1525
2024-02-06 23:34:22,736 Epoch 1525: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-06 23:34:22,737 EPOCH 1526
2024-02-06 23:34:39,352 Epoch 1526: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 23:34:39,353 EPOCH 1527
2024-02-06 23:34:55,671 Epoch 1527: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-06 23:34:55,672 EPOCH 1528
2024-02-06 23:35:12,078 Epoch 1528: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-06 23:35:12,079 EPOCH 1529
2024-02-06 23:35:28,510 Epoch 1529: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 23:35:28,511 EPOCH 1530
2024-02-06 23:35:44,960 Epoch 1530: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 23:35:44,961 EPOCH 1531
2024-02-06 23:36:01,589 Epoch 1531: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-06 23:36:01,590 EPOCH 1532
2024-02-06 23:36:18,246 Epoch 1532: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 23:36:18,247 EPOCH 1533
2024-02-06 23:36:34,699 Epoch 1533: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 23:36:34,700 EPOCH 1534
2024-02-06 23:36:39,940 [Epoch: 1534 Step: 00013800] Batch Recognition Loss:   0.000375 => Gls Tokens per Sec:      561 || Batch Translation Loss:   0.009452 => Txt Tokens per Sec:     1242 || Lr: 0.000100
2024-02-06 23:36:51,394 Epoch 1534: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 23:36:51,394 EPOCH 1535
2024-02-06 23:37:07,650 Epoch 1535: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-06 23:37:07,650 EPOCH 1536
2024-02-06 23:37:24,348 Epoch 1536: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-06 23:37:24,349 EPOCH 1537
2024-02-06 23:37:40,564 Epoch 1537: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-06 23:37:40,565 EPOCH 1538
2024-02-06 23:37:57,184 Epoch 1538: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-06 23:37:57,184 EPOCH 1539
2024-02-06 23:38:13,473 Epoch 1539: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-06 23:38:13,474 EPOCH 1540
2024-02-06 23:38:29,928 Epoch 1540: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-06 23:38:29,929 EPOCH 1541
2024-02-06 23:38:46,173 Epoch 1541: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-06 23:38:46,174 EPOCH 1542
2024-02-06 23:39:02,568 Epoch 1542: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-06 23:39:02,569 EPOCH 1543
2024-02-06 23:39:19,301 Epoch 1543: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-06 23:39:19,302 EPOCH 1544
2024-02-06 23:39:35,679 Epoch 1544: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-06 23:39:35,679 EPOCH 1545
2024-02-06 23:39:42,777 [Epoch: 1545 Step: 00013900] Batch Recognition Loss:   0.000596 => Gls Tokens per Sec:      721 || Batch Translation Loss:   0.008957 => Txt Tokens per Sec:     1906 || Lr: 0.000100
2024-02-06 23:39:52,044 Epoch 1545: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-06 23:39:52,044 EPOCH 1546
2024-02-06 23:40:08,267 Epoch 1546: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-06 23:40:08,268 EPOCH 1547
2024-02-06 23:40:24,656 Epoch 1547: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-06 23:40:24,657 EPOCH 1548
2024-02-06 23:40:41,585 Epoch 1548: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-06 23:40:41,586 EPOCH 1549
2024-02-06 23:40:58,176 Epoch 1549: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-06 23:40:58,178 EPOCH 1550
2024-02-06 23:41:14,793 Epoch 1550: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-06 23:41:14,793 EPOCH 1551
2024-02-06 23:41:31,383 Epoch 1551: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-06 23:41:31,384 EPOCH 1552
2024-02-06 23:41:47,655 Epoch 1552: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-06 23:41:47,655 EPOCH 1553
2024-02-06 23:42:04,271 Epoch 1553: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-06 23:42:04,271 EPOCH 1554
2024-02-06 23:42:20,551 Epoch 1554: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-06 23:42:20,552 EPOCH 1555
2024-02-06 23:42:37,048 Epoch 1555: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-06 23:42:37,049 EPOCH 1556
2024-02-06 23:42:43,439 [Epoch: 1556 Step: 00014000] Batch Recognition Loss:   0.002847 => Gls Tokens per Sec:      861 || Batch Translation Loss:   0.040282 => Txt Tokens per Sec:     2466 || Lr: 0.000100
2024-02-06 23:43:51,582 Validation result at epoch 1556, step    14000: duration: 68.1419s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.45651	Translation Loss: 92948.84375	PPL: 10761.64648
	Eval Metric: BLEU
	WER 4.87	(DEL: 0.00,	INS: 0.00,	SUB: 4.87)
	BLEU-4 0.39	(BLEU-1: 11.24,	BLEU-2: 3.47,	BLEU-3: 1.19,	BLEU-4: 0.39)
	CHRF 17.15	ROUGE 9.20
2024-02-06 23:43:51,584 Logging Recognition and Translation Outputs
2024-02-06 23:43:51,584 ========================================================================================================================
2024-02-06 23:43:51,584 Logging Sequence: 177_50.00
2024-02-06 23:43:51,585 	Gloss Reference :	A B+C+D+E
2024-02-06 23:43:51,585 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 23:43:51,585 	Gloss Alignment :	         
2024-02-06 23:43:51,585 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 23:43:51,588 	Text Reference  :	**** ** ** ***** a        similar reward of  rs  50000 was announced for    information against his  associate ajay  kumar
2024-02-06 23:43:51,588 	Text Hypothesis :	will be in tokyo olympics sushil  kumar  who led to    the motive    behind the         brawl   that killed    sagar rana 
2024-02-06 23:43:51,588 	Text Alignment  :	I    I  I  I     S        S       S      S   S   S     S   S         S      S           S       S    S         S     S    
2024-02-06 23:43:51,588 ========================================================================================================================
2024-02-06 23:43:51,589 Logging Sequence: 136_175.00
2024-02-06 23:43:51,589 	Gloss Reference :	A B+C+D+E  
2024-02-06 23:43:51,589 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-06 23:43:51,589 	Gloss Alignment :	  S        
2024-02-06 23:43:51,590 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 23:43:51,591 	Text Reference  :	after 49     years india' hockey team          beat britain and       qualified for the        semi-finals
2024-02-06 23:43:51,591 	Text Hypothesis :	4     anyone found to     be     proselytizing for  other   religions would     be  criminally prosecuted 
2024-02-06 23:43:51,591 	Text Alignment  :	S     S      S     S      S      S             S    S       S         S         S   S          S          
2024-02-06 23:43:51,591 ========================================================================================================================
2024-02-06 23:43:51,591 Logging Sequence: 126_159.00
2024-02-06 23:43:51,592 	Gloss Reference :	A B+C+D+E
2024-02-06 23:43:51,592 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 23:43:51,592 	Gloss Alignment :	         
2024-02-06 23:43:51,592 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 23:43:51,593 	Text Reference  :	despite multiple challenges and injuries you  did   not give up  
2024-02-06 23:43:51,593 	Text Hypothesis :	******* ******** ********** he  played   very grate to  his  fans
2024-02-06 23:43:51,593 	Text Alignment  :	D       D        D          S   S        S    S     S   S    S   
2024-02-06 23:43:51,593 ========================================================================================================================
2024-02-06 23:43:51,593 Logging Sequence: 70_88.00
2024-02-06 23:43:51,593 	Gloss Reference :	A B+C+D+E
2024-02-06 23:43:51,594 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 23:43:51,594 	Gloss Alignment :	         
2024-02-06 23:43:51,594 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 23:43:51,595 	Text Reference  :	two coca-cola bottles were  placed on   the ***** table next    to the ****** mic 
2024-02-06 23:43:51,595 	Text Hypothesis :	*** ********* ******* sadly the    lost the match and   england at the second time
2024-02-06 23:43:51,595 	Text Alignment  :	D   D         D       S     S      S        I     S     S       S      I      S   
2024-02-06 23:43:51,595 ========================================================================================================================
2024-02-06 23:43:51,595 Logging Sequence: 54_201.00
2024-02-06 23:43:51,596 	Gloss Reference :	A B+C+D+E
2024-02-06 23:43:51,596 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 23:43:51,596 	Gloss Alignment :	         
2024-02-06 23:43:51,596 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 23:43:51,599 	Text Reference  :	there is a huge demand       mostly from    non-resident indians nris who  are    excited   to see          the match ******* and they    have booked the       hotel ******* ***** ** rooms
2024-02-06 23:43:51,600 	Text Hypothesis :	***** ** * the  middle-class fans   usually decide       at      the  last moment depending on availability of  match tickets if  tickets are  not    available hotel booking would go waste
2024-02-06 23:43:51,600 	Text Alignment  :	D     D  D S    S            S      S       S            S       S    S    S      S         S  S            S         I       S   S       S    S      S               I       I     I  S    
2024-02-06 23:43:51,600 ========================================================================================================================
2024-02-06 23:44:02,195 Epoch 1556: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-06 23:44:02,196 EPOCH 1557
2024-02-06 23:44:18,976 Epoch 1557: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-06 23:44:18,976 EPOCH 1558
2024-02-06 23:44:35,445 Epoch 1558: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-06 23:44:35,446 EPOCH 1559
2024-02-06 23:44:51,674 Epoch 1559: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-06 23:44:51,675 EPOCH 1560
2024-02-06 23:45:07,775 Epoch 1560: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 23:45:07,775 EPOCH 1561
2024-02-06 23:45:24,226 Epoch 1561: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-06 23:45:24,226 EPOCH 1562
2024-02-06 23:45:40,639 Epoch 1562: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-06 23:45:40,640 EPOCH 1563
2024-02-06 23:45:57,132 Epoch 1563: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-06 23:45:57,133 EPOCH 1564
2024-02-06 23:46:13,593 Epoch 1564: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-06 23:46:13,594 EPOCH 1565
2024-02-06 23:46:29,977 Epoch 1565: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-06 23:46:29,978 EPOCH 1566
2024-02-06 23:46:46,517 Epoch 1566: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-06 23:46:46,517 EPOCH 1567
2024-02-06 23:46:56,074 [Epoch: 1567 Step: 00014100] Batch Recognition Loss:   0.000672 => Gls Tokens per Sec:      710 || Batch Translation Loss:   0.028221 => Txt Tokens per Sec:     1971 || Lr: 0.000100
2024-02-06 23:47:02,856 Epoch 1567: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 23:47:02,857 EPOCH 1568
2024-02-06 23:47:19,449 Epoch 1568: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 23:47:19,449 EPOCH 1569
2024-02-06 23:47:35,667 Epoch 1569: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-06 23:47:35,668 EPOCH 1570
2024-02-06 23:47:52,740 Epoch 1570: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 23:47:52,740 EPOCH 1571
2024-02-06 23:48:08,871 Epoch 1571: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-06 23:48:08,872 EPOCH 1572
2024-02-06 23:48:25,294 Epoch 1572: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-06 23:48:25,295 EPOCH 1573
2024-02-06 23:48:41,529 Epoch 1573: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-06 23:48:41,529 EPOCH 1574
2024-02-06 23:48:58,095 Epoch 1574: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-06 23:48:58,096 EPOCH 1575
2024-02-06 23:49:14,340 Epoch 1575: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-06 23:49:14,341 EPOCH 1576
2024-02-06 23:49:30,823 Epoch 1576: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-06 23:49:30,823 EPOCH 1577
2024-02-06 23:49:47,167 Epoch 1577: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-06 23:49:47,168 EPOCH 1578
2024-02-06 23:49:54,299 [Epoch: 1578 Step: 00014200] Batch Recognition Loss:   0.000333 => Gls Tokens per Sec:     1130 || Batch Translation Loss:   0.038827 => Txt Tokens per Sec:     2930 || Lr: 0.000100
2024-02-06 23:50:03,453 Epoch 1578: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 23:50:03,453 EPOCH 1579
2024-02-06 23:50:20,220 Epoch 1579: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-06 23:50:20,222 EPOCH 1580
2024-02-06 23:50:36,612 Epoch 1580: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-06 23:50:36,612 EPOCH 1581
2024-02-06 23:50:53,022 Epoch 1581: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-06 23:50:53,022 EPOCH 1582
2024-02-06 23:51:09,714 Epoch 1582: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 23:51:09,714 EPOCH 1583
2024-02-06 23:51:26,062 Epoch 1583: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 23:51:26,063 EPOCH 1584
2024-02-06 23:51:42,611 Epoch 1584: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-06 23:51:42,612 EPOCH 1585
2024-02-06 23:51:58,990 Epoch 1585: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 23:51:58,991 EPOCH 1586
2024-02-06 23:52:15,475 Epoch 1586: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-06 23:52:15,476 EPOCH 1587
2024-02-06 23:52:31,896 Epoch 1587: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-06 23:52:31,897 EPOCH 1588
2024-02-06 23:52:48,399 Epoch 1588: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-06 23:52:48,399 EPOCH 1589
2024-02-06 23:53:04,483 [Epoch: 1589 Step: 00014300] Batch Recognition Loss:   0.000328 => Gls Tokens per Sec:      581 || Batch Translation Loss:   0.023837 => Txt Tokens per Sec:     1652 || Lr: 0.000100
2024-02-06 23:53:04,792 Epoch 1589: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-06 23:53:04,792 EPOCH 1590
2024-02-06 23:53:21,479 Epoch 1590: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-06 23:53:21,479 EPOCH 1591
2024-02-06 23:53:37,902 Epoch 1591: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-06 23:53:37,903 EPOCH 1592
2024-02-06 23:53:54,267 Epoch 1592: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-06 23:53:54,268 EPOCH 1593
2024-02-06 23:54:10,918 Epoch 1593: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-06 23:54:10,918 EPOCH 1594
2024-02-06 23:54:27,258 Epoch 1594: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-06 23:54:27,259 EPOCH 1595
2024-02-06 23:54:43,900 Epoch 1595: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-06 23:54:43,901 EPOCH 1596
2024-02-06 23:55:00,382 Epoch 1596: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-06 23:55:00,382 EPOCH 1597
2024-02-06 23:55:17,054 Epoch 1597: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-06 23:55:17,055 EPOCH 1598
2024-02-06 23:55:33,378 Epoch 1598: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-06 23:55:33,379 EPOCH 1599
2024-02-06 23:55:49,809 Epoch 1599: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.11 
2024-02-06 23:55:49,809 EPOCH 1600
2024-02-06 23:56:05,961 [Epoch: 1600 Step: 00014400] Batch Recognition Loss:   0.001932 => Gls Tokens per Sec:      658 || Batch Translation Loss:   0.344339 => Txt Tokens per Sec:     1819 || Lr: 0.000100
2024-02-06 23:56:05,961 Epoch 1600: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.31 
2024-02-06 23:56:05,962 EPOCH 1601
2024-02-06 23:56:22,533 Epoch 1601: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.68 
2024-02-06 23:56:22,534 EPOCH 1602
2024-02-06 23:56:38,745 Epoch 1602: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.12 
2024-02-06 23:56:38,745 EPOCH 1603
2024-02-06 23:56:55,371 Epoch 1603: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.34 
2024-02-06 23:56:55,371 EPOCH 1604
2024-02-06 23:57:11,871 Epoch 1604: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.46 
2024-02-06 23:57:11,871 EPOCH 1605
2024-02-06 23:57:28,226 Epoch 1605: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.22 
2024-02-06 23:57:28,227 EPOCH 1606
2024-02-06 23:57:45,209 Epoch 1606: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-06 23:57:45,209 EPOCH 1607
2024-02-06 23:58:01,783 Epoch 1607: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-06 23:58:01,785 EPOCH 1608
2024-02-06 23:58:18,259 Epoch 1608: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-06 23:58:18,259 EPOCH 1609
2024-02-06 23:58:34,589 Epoch 1609: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-06 23:58:34,590 EPOCH 1610
2024-02-06 23:58:51,166 Epoch 1610: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 23:58:51,167 EPOCH 1611
2024-02-06 23:59:07,665 Epoch 1611: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 23:59:07,665 EPOCH 1612
2024-02-06 23:59:08,072 [Epoch: 1612 Step: 00014500] Batch Recognition Loss:   0.000734 => Gls Tokens per Sec:     3153 || Batch Translation Loss:   0.033096 => Txt Tokens per Sec:     8012 || Lr: 0.000100
2024-02-06 23:59:24,406 Epoch 1612: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 23:59:24,406 EPOCH 1613
2024-02-06 23:59:41,160 Epoch 1613: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 23:59:41,160 EPOCH 1614
2024-02-06 23:59:57,584 Epoch 1614: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-06 23:59:57,585 EPOCH 1615
2024-02-07 00:00:14,205 Epoch 1615: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-07 00:00:14,206 EPOCH 1616
2024-02-07 00:00:30,476 Epoch 1616: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-07 00:00:30,477 EPOCH 1617
2024-02-07 00:00:46,528 Epoch 1617: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-07 00:00:46,528 EPOCH 1618
2024-02-07 00:01:03,118 Epoch 1618: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-07 00:01:03,118 EPOCH 1619
2024-02-07 00:01:19,276 Epoch 1619: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-07 00:01:19,277 EPOCH 1620
2024-02-07 00:01:35,735 Epoch 1620: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-07 00:01:35,736 EPOCH 1621
2024-02-07 00:01:52,380 Epoch 1621: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-07 00:01:52,380 EPOCH 1622
2024-02-07 00:02:08,687 Epoch 1622: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-07 00:02:08,687 EPOCH 1623
2024-02-07 00:02:09,756 [Epoch: 1623 Step: 00014600] Batch Recognition Loss:   0.000625 => Gls Tokens per Sec:     2401 || Batch Translation Loss:   0.029261 => Txt Tokens per Sec:     6129 || Lr: 0.000100
2024-02-07 00:02:25,273 Epoch 1623: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-07 00:02:25,273 EPOCH 1624
2024-02-07 00:02:42,101 Epoch 1624: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-07 00:02:42,101 EPOCH 1625
2024-02-07 00:02:58,608 Epoch 1625: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 00:02:58,609 EPOCH 1626
2024-02-07 00:03:15,192 Epoch 1626: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 00:03:15,192 EPOCH 1627
2024-02-07 00:03:31,623 Epoch 1627: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 00:03:31,623 EPOCH 1628
2024-02-07 00:03:47,937 Epoch 1628: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 00:03:47,938 EPOCH 1629
2024-02-07 00:04:04,377 Epoch 1629: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-07 00:04:04,378 EPOCH 1630
2024-02-07 00:04:20,989 Epoch 1630: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-07 00:04:20,990 EPOCH 1631
2024-02-07 00:04:37,479 Epoch 1631: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-07 00:04:37,480 EPOCH 1632
2024-02-07 00:04:53,728 Epoch 1632: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-07 00:04:53,728 EPOCH 1633
2024-02-07 00:05:10,466 Epoch 1633: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-07 00:05:10,467 EPOCH 1634
2024-02-07 00:05:12,008 [Epoch: 1634 Step: 00014700] Batch Recognition Loss:   0.000297 => Gls Tokens per Sec:     2494 || Batch Translation Loss:   0.048527 => Txt Tokens per Sec:     6546 || Lr: 0.000100
2024-02-07 00:05:27,322 Epoch 1634: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-07 00:05:27,322 EPOCH 1635
2024-02-07 00:05:43,688 Epoch 1635: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-07 00:05:43,689 EPOCH 1636
2024-02-07 00:05:59,953 Epoch 1636: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-07 00:05:59,953 EPOCH 1637
2024-02-07 00:06:16,557 Epoch 1637: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-07 00:06:16,558 EPOCH 1638
2024-02-07 00:06:33,006 Epoch 1638: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 00:06:33,006 EPOCH 1639
2024-02-07 00:06:49,434 Epoch 1639: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 00:06:49,435 EPOCH 1640
2024-02-07 00:07:05,598 Epoch 1640: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 00:07:05,598 EPOCH 1641
2024-02-07 00:07:22,074 Epoch 1641: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 00:07:22,075 EPOCH 1642
2024-02-07 00:07:38,314 Epoch 1642: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 00:07:38,314 EPOCH 1643
2024-02-07 00:07:55,171 Epoch 1643: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 00:07:55,172 EPOCH 1644
2024-02-07 00:08:11,510 Epoch 1644: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 00:08:11,510 EPOCH 1645
2024-02-07 00:08:13,497 [Epoch: 1645 Step: 00014800] Batch Recognition Loss:   0.000230 => Gls Tokens per Sec:     2578 || Batch Translation Loss:   0.016902 => Txt Tokens per Sec:     6752 || Lr: 0.000100
2024-02-07 00:08:28,026 Epoch 1645: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 00:08:28,026 EPOCH 1646
2024-02-07 00:08:44,688 Epoch 1646: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 00:08:44,689 EPOCH 1647
2024-02-07 00:09:01,285 Epoch 1647: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 00:09:01,285 EPOCH 1648
2024-02-07 00:09:17,877 Epoch 1648: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 00:09:17,878 EPOCH 1649
2024-02-07 00:09:34,811 Epoch 1649: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 00:09:34,812 EPOCH 1650
2024-02-07 00:09:51,491 Epoch 1650: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 00:09:51,492 EPOCH 1651
2024-02-07 00:10:07,942 Epoch 1651: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 00:10:07,943 EPOCH 1652
2024-02-07 00:10:24,550 Epoch 1652: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 00:10:24,550 EPOCH 1653
2024-02-07 00:10:41,018 Epoch 1653: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 00:10:41,018 EPOCH 1654
2024-02-07 00:10:57,798 Epoch 1654: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 00:10:57,799 EPOCH 1655
2024-02-07 00:11:14,637 Epoch 1655: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 00:11:14,638 EPOCH 1656
2024-02-07 00:11:26,731 [Epoch: 1656 Step: 00014900] Batch Recognition Loss:   0.000596 => Gls Tokens per Sec:      455 || Batch Translation Loss:   0.020348 => Txt Tokens per Sec:     1359 || Lr: 0.000100
2024-02-07 00:11:31,154 Epoch 1656: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 00:11:31,155 EPOCH 1657
2024-02-07 00:11:47,526 Epoch 1657: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 00:11:47,526 EPOCH 1658
2024-02-07 00:12:03,862 Epoch 1658: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 00:12:03,862 EPOCH 1659
2024-02-07 00:12:20,159 Epoch 1659: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 00:12:20,160 EPOCH 1660
2024-02-07 00:12:36,732 Epoch 1660: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 00:12:36,732 EPOCH 1661
2024-02-07 00:12:52,722 Epoch 1661: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 00:12:52,722 EPOCH 1662
2024-02-07 00:13:09,456 Epoch 1662: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 00:13:09,458 EPOCH 1663
2024-02-07 00:13:25,955 Epoch 1663: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 00:13:25,955 EPOCH 1664
2024-02-07 00:13:42,441 Epoch 1664: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 00:13:42,441 EPOCH 1665
2024-02-07 00:13:59,001 Epoch 1665: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 00:13:59,002 EPOCH 1666
2024-02-07 00:14:15,601 Epoch 1666: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 00:14:15,602 EPOCH 1667
2024-02-07 00:14:26,694 [Epoch: 1667 Step: 00015000] Batch Recognition Loss:   0.000247 => Gls Tokens per Sec:      692 || Batch Translation Loss:   0.016426 => Txt Tokens per Sec:     1917 || Lr: 0.000100
2024-02-07 00:14:32,003 Epoch 1667: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 00:14:32,004 EPOCH 1668
2024-02-07 00:14:48,450 Epoch 1668: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 00:14:48,451 EPOCH 1669
2024-02-07 00:15:04,412 Epoch 1669: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 00:15:04,413 EPOCH 1670
2024-02-07 00:15:21,060 Epoch 1670: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 00:15:21,060 EPOCH 1671
2024-02-07 00:15:37,561 Epoch 1671: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 00:15:37,562 EPOCH 1672
2024-02-07 00:15:54,016 Epoch 1672: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 00:15:54,016 EPOCH 1673
2024-02-07 00:16:10,450 Epoch 1673: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 00:16:10,450 EPOCH 1674
2024-02-07 00:16:26,949 Epoch 1674: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 00:16:26,949 EPOCH 1675
2024-02-07 00:16:43,495 Epoch 1675: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 00:16:43,496 EPOCH 1676
2024-02-07 00:16:59,948 Epoch 1676: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 00:16:59,948 EPOCH 1677
2024-02-07 00:17:16,227 Epoch 1677: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 00:17:16,228 EPOCH 1678
2024-02-07 00:17:31,479 [Epoch: 1678 Step: 00015100] Batch Recognition Loss:   0.000348 => Gls Tokens per Sec:      529 || Batch Translation Loss:   0.023130 => Txt Tokens per Sec:     1485 || Lr: 0.000100
2024-02-07 00:17:32,623 Epoch 1678: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 00:17:32,623 EPOCH 1679
2024-02-07 00:17:49,408 Epoch 1679: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 00:17:49,409 EPOCH 1680
2024-02-07 00:18:05,812 Epoch 1680: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 00:18:05,812 EPOCH 1681
2024-02-07 00:18:22,461 Epoch 1681: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 00:18:22,462 EPOCH 1682
2024-02-07 00:18:38,721 Epoch 1682: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 00:18:38,722 EPOCH 1683
2024-02-07 00:18:55,467 Epoch 1683: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 00:18:55,467 EPOCH 1684
2024-02-07 00:19:11,834 Epoch 1684: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 00:19:11,834 EPOCH 1685
2024-02-07 00:19:28,304 Epoch 1685: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 00:19:28,305 EPOCH 1686
2024-02-07 00:19:45,362 Epoch 1686: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 00:19:45,363 EPOCH 1687
2024-02-07 00:20:01,803 Epoch 1687: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 00:20:01,804 EPOCH 1688
2024-02-07 00:20:18,210 Epoch 1688: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 00:20:18,211 EPOCH 1689
2024-02-07 00:20:30,254 [Epoch: 1689 Step: 00015200] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:      850 || Batch Translation Loss:   0.020326 => Txt Tokens per Sec:     2327 || Lr: 0.000100
2024-02-07 00:20:34,760 Epoch 1689: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 00:20:34,761 EPOCH 1690
2024-02-07 00:20:51,603 Epoch 1690: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 00:20:51,604 EPOCH 1691
2024-02-07 00:21:08,123 Epoch 1691: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 00:21:08,123 EPOCH 1692
2024-02-07 00:21:24,220 Epoch 1692: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 00:21:24,221 EPOCH 1693
2024-02-07 00:21:40,809 Epoch 1693: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 00:21:40,809 EPOCH 1694
2024-02-07 00:21:56,982 Epoch 1694: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 00:21:56,982 EPOCH 1695
2024-02-07 00:22:13,289 Epoch 1695: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 00:22:13,290 EPOCH 1696
2024-02-07 00:22:29,472 Epoch 1696: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 00:22:29,472 EPOCH 1697
2024-02-07 00:22:46,411 Epoch 1697: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 00:22:46,412 EPOCH 1698
2024-02-07 00:23:02,535 Epoch 1698: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 00:23:02,536 EPOCH 1699
2024-02-07 00:23:19,302 Epoch 1699: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 00:23:19,303 EPOCH 1700
2024-02-07 00:23:36,062 [Epoch: 1700 Step: 00015300] Batch Recognition Loss:   0.000238 => Gls Tokens per Sec:      634 || Batch Translation Loss:   0.018750 => Txt Tokens per Sec:     1753 || Lr: 0.000100
2024-02-07 00:23:36,063 Epoch 1700: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-07 00:23:36,063 EPOCH 1701
2024-02-07 00:23:52,357 Epoch 1701: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-07 00:23:52,357 EPOCH 1702
2024-02-07 00:24:08,446 Epoch 1702: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.83 
2024-02-07 00:24:08,447 EPOCH 1703
2024-02-07 00:24:25,054 Epoch 1703: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.05 
2024-02-07 00:24:25,055 EPOCH 1704
2024-02-07 00:24:41,617 Epoch 1704: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.67 
2024-02-07 00:24:41,618 EPOCH 1705
2024-02-07 00:24:57,903 Epoch 1705: Total Training Recognition Loss 0.06  Total Training Translation Loss 7.72 
2024-02-07 00:24:57,904 EPOCH 1706
2024-02-07 00:25:14,384 Epoch 1706: Total Training Recognition Loss 0.10  Total Training Translation Loss 16.53 
2024-02-07 00:25:14,384 EPOCH 1707
2024-02-07 00:25:30,889 Epoch 1707: Total Training Recognition Loss 0.08  Total Training Translation Loss 7.97 
2024-02-07 00:25:30,890 EPOCH 1708
2024-02-07 00:25:47,307 Epoch 1708: Total Training Recognition Loss 0.06  Total Training Translation Loss 5.76 
2024-02-07 00:25:47,307 EPOCH 1709
2024-02-07 00:26:03,815 Epoch 1709: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.46 
2024-02-07 00:26:03,816 EPOCH 1710
2024-02-07 00:26:20,297 Epoch 1710: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.43 
2024-02-07 00:26:20,297 EPOCH 1711
2024-02-07 00:26:37,004 Epoch 1711: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.84 
2024-02-07 00:26:37,006 EPOCH 1712
2024-02-07 00:26:37,870 [Epoch: 1712 Step: 00015400] Batch Recognition Loss:   0.002432 => Gls Tokens per Sec:     1483 || Batch Translation Loss:   0.080506 => Txt Tokens per Sec:     4508 || Lr: 0.000100
2024-02-07 00:26:53,648 Epoch 1712: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-07 00:26:53,649 EPOCH 1713
2024-02-07 00:27:09,977 Epoch 1713: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-07 00:27:09,977 EPOCH 1714
2024-02-07 00:27:26,855 Epoch 1714: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-07 00:27:26,857 EPOCH 1715
2024-02-07 00:27:43,674 Epoch 1715: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-07 00:27:43,675 EPOCH 1716
2024-02-07 00:27:59,928 Epoch 1716: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-07 00:27:59,929 EPOCH 1717
2024-02-07 00:28:16,268 Epoch 1717: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-07 00:28:16,269 EPOCH 1718
2024-02-07 00:28:32,704 Epoch 1718: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-07 00:28:32,705 EPOCH 1719
2024-02-07 00:28:49,157 Epoch 1719: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-07 00:28:49,157 EPOCH 1720
2024-02-07 00:29:05,601 Epoch 1720: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-07 00:29:05,601 EPOCH 1721
2024-02-07 00:29:22,136 Epoch 1721: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-07 00:29:22,137 EPOCH 1722
2024-02-07 00:29:38,514 Epoch 1722: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-07 00:29:38,514 EPOCH 1723
2024-02-07 00:29:41,926 [Epoch: 1723 Step: 00015500] Batch Recognition Loss:   0.000520 => Gls Tokens per Sec:      751 || Batch Translation Loss:   0.017755 => Txt Tokens per Sec:     1882 || Lr: 0.000100
2024-02-07 00:29:54,901 Epoch 1723: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-07 00:29:54,902 EPOCH 1724
2024-02-07 00:30:11,698 Epoch 1724: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-07 00:30:11,699 EPOCH 1725
2024-02-07 00:30:27,954 Epoch 1725: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-07 00:30:27,954 EPOCH 1726
2024-02-07 00:30:44,386 Epoch 1726: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 00:30:44,387 EPOCH 1727
2024-02-07 00:31:01,084 Epoch 1727: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 00:31:01,084 EPOCH 1728
2024-02-07 00:31:17,557 Epoch 1728: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-07 00:31:17,558 EPOCH 1729
2024-02-07 00:31:34,112 Epoch 1729: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 00:31:34,113 EPOCH 1730
2024-02-07 00:31:50,873 Epoch 1730: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 00:31:50,874 EPOCH 1731
2024-02-07 00:32:07,290 Epoch 1731: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 00:32:07,291 EPOCH 1732
2024-02-07 00:32:23,728 Epoch 1732: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 00:32:23,728 EPOCH 1733
2024-02-07 00:32:40,164 Epoch 1733: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 00:32:40,164 EPOCH 1734
2024-02-07 00:32:41,326 [Epoch: 1734 Step: 00015600] Batch Recognition Loss:   0.000464 => Gls Tokens per Sec:     3308 || Batch Translation Loss:   0.017809 => Txt Tokens per Sec:     8331 || Lr: 0.000100
2024-02-07 00:32:56,523 Epoch 1734: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 00:32:56,524 EPOCH 1735
2024-02-07 00:33:13,302 Epoch 1735: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 00:33:13,302 EPOCH 1736
2024-02-07 00:33:29,920 Epoch 1736: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 00:33:29,921 EPOCH 1737
2024-02-07 00:33:46,056 Epoch 1737: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 00:33:46,057 EPOCH 1738
2024-02-07 00:34:02,547 Epoch 1738: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 00:34:02,547 EPOCH 1739
2024-02-07 00:34:19,223 Epoch 1739: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 00:34:19,224 EPOCH 1740
2024-02-07 00:34:35,521 Epoch 1740: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 00:34:35,521 EPOCH 1741
2024-02-07 00:34:51,991 Epoch 1741: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 00:34:51,992 EPOCH 1742
2024-02-07 00:35:08,809 Epoch 1742: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 00:35:08,810 EPOCH 1743
2024-02-07 00:35:25,127 Epoch 1743: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 00:35:25,127 EPOCH 1744
2024-02-07 00:35:41,718 Epoch 1744: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 00:35:41,719 EPOCH 1745
2024-02-07 00:35:50,090 [Epoch: 1745 Step: 00015700] Batch Recognition Loss:   0.000389 => Gls Tokens per Sec:      504 || Batch Translation Loss:   0.016869 => Txt Tokens per Sec:     1317 || Lr: 0.000100
2024-02-07 00:35:58,346 Epoch 1745: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 00:35:58,346 EPOCH 1746
2024-02-07 00:36:14,718 Epoch 1746: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 00:36:14,719 EPOCH 1747
2024-02-07 00:36:31,044 Epoch 1747: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 00:36:31,045 EPOCH 1748
2024-02-07 00:36:47,574 Epoch 1748: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 00:36:47,574 EPOCH 1749
2024-02-07 00:37:04,041 Epoch 1749: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 00:37:04,041 EPOCH 1750
2024-02-07 00:37:20,922 Epoch 1750: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 00:37:20,923 EPOCH 1751
2024-02-07 00:37:37,612 Epoch 1751: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 00:37:37,613 EPOCH 1752
2024-02-07 00:37:54,023 Epoch 1752: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 00:37:54,023 EPOCH 1753
2024-02-07 00:38:10,258 Epoch 1753: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 00:38:10,259 EPOCH 1754
2024-02-07 00:38:26,602 Epoch 1754: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 00:38:26,602 EPOCH 1755
2024-02-07 00:38:43,114 Epoch 1755: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 00:38:43,115 EPOCH 1756
2024-02-07 00:38:50,855 [Epoch: 1756 Step: 00015800] Batch Recognition Loss:   0.000259 => Gls Tokens per Sec:      827 || Batch Translation Loss:   0.017921 => Txt Tokens per Sec:     2222 || Lr: 0.000100
2024-02-07 00:38:59,408 Epoch 1756: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 00:38:59,409 EPOCH 1757
2024-02-07 00:39:16,218 Epoch 1757: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 00:39:16,218 EPOCH 1758
2024-02-07 00:39:32,708 Epoch 1758: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 00:39:32,709 EPOCH 1759
2024-02-07 00:39:49,227 Epoch 1759: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 00:39:49,227 EPOCH 1760
2024-02-07 00:40:05,756 Epoch 1760: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 00:40:05,756 EPOCH 1761
2024-02-07 00:40:22,479 Epoch 1761: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 00:40:22,480 EPOCH 1762
2024-02-07 00:40:38,804 Epoch 1762: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 00:40:38,804 EPOCH 1763
2024-02-07 00:40:55,105 Epoch 1763: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 00:40:55,105 EPOCH 1764
2024-02-07 00:41:11,109 Epoch 1764: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 00:41:11,109 EPOCH 1765
2024-02-07 00:41:27,701 Epoch 1765: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 00:41:27,702 EPOCH 1766
2024-02-07 00:41:44,081 Epoch 1766: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 00:41:44,082 EPOCH 1767
2024-02-07 00:41:59,434 [Epoch: 1767 Step: 00015900] Batch Recognition Loss:   0.000259 => Gls Tokens per Sec:      442 || Batch Translation Loss:   0.015227 => Txt Tokens per Sec:     1321 || Lr: 0.000100
2024-02-07 00:42:00,537 Epoch 1767: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 00:42:00,537 EPOCH 1768
2024-02-07 00:42:16,841 Epoch 1768: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 00:42:16,842 EPOCH 1769
2024-02-07 00:42:33,549 Epoch 1769: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 00:42:33,549 EPOCH 1770
2024-02-07 00:42:49,729 Epoch 1770: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 00:42:49,729 EPOCH 1771
2024-02-07 00:43:06,210 Epoch 1771: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 00:43:06,210 EPOCH 1772
2024-02-07 00:43:22,508 Epoch 1772: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 00:43:22,508 EPOCH 1773
2024-02-07 00:43:38,691 Epoch 1773: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 00:43:38,692 EPOCH 1774
2024-02-07 00:43:55,375 Epoch 1774: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 00:43:55,376 EPOCH 1775
2024-02-07 00:44:11,561 Epoch 1775: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 00:44:11,562 EPOCH 1776
2024-02-07 00:44:27,919 Epoch 1776: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 00:44:27,920 EPOCH 1777
2024-02-07 00:44:44,499 Epoch 1777: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 00:44:44,500 EPOCH 1778
2024-02-07 00:45:00,231 [Epoch: 1778 Step: 00016000] Batch Recognition Loss:   0.001872 => Gls Tokens per Sec:      512 || Batch Translation Loss:   0.008562 => Txt Tokens per Sec:     1436 || Lr: 0.000100
2024-02-07 00:46:08,271 Validation result at epoch 1778, step    16000: duration: 68.0373s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.49468	Translation Loss: 92087.25781	PPL: 9874.27441
	Eval Metric: BLEU
	WER 4.87	(DEL: 0.00,	INS: 0.00,	SUB: 4.87)
	BLEU-4 0.38	(BLEU-1: 11.66,	BLEU-2: 3.58,	BLEU-3: 1.14,	BLEU-4: 0.38)
	CHRF 17.24	ROUGE 9.59
2024-02-07 00:46:08,273 Logging Recognition and Translation Outputs
2024-02-07 00:46:08,273 ========================================================================================================================
2024-02-07 00:46:08,273 Logging Sequence: 163_116.00
2024-02-07 00:46:08,273 	Gloss Reference :	A B+C+D+E
2024-02-07 00:46:08,274 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 00:46:08,274 	Gloss Alignment :	         
2024-02-07 00:46:08,274 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 00:46:08,275 	Text Reference  :	******** **** ***** ***** ******** ***** ** people said   that she   looked similar to       virat
2024-02-07 00:46:08,276 	Text Hypothesis :	whenever they would share pictures taken at his    family and  there was    very    vamika's head 
2024-02-07 00:46:08,276 	Text Alignment  :	I        I    I     I     I        I     I  S      S      S    S     S      S       S        S    
2024-02-07 00:46:08,276 ========================================================================================================================
2024-02-07 00:46:08,276 Logging Sequence: 53_161.00
2024-02-07 00:46:08,277 	Gloss Reference :	A B+C+D+E
2024-02-07 00:46:08,277 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 00:46:08,277 	Gloss Alignment :	         
2024-02-07 00:46:08,277 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 00:46:08,279 	Text Reference  :	rashid has also been urging people      to donate to  his rashid khan foundation and afghanistan cricket association
2024-02-07 00:46:08,279 	Text Hypothesis :	he     has now  been ****** afghanistan to ****** win but will   be   available  for the         trent   rockets    
2024-02-07 00:46:08,279 	Text Alignment  :	S          S         D      S              D      S   S   S      S    S          S   S           S       S          
2024-02-07 00:46:08,279 ========================================================================================================================
2024-02-07 00:46:08,279 Logging Sequence: 67_73.00
2024-02-07 00:46:08,279 	Gloss Reference :	A B+C+D+E
2024-02-07 00:46:08,280 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 00:46:08,280 	Gloss Alignment :	         
2024-02-07 00:46:08,280 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 00:46:08,280 	Text Reference  :	*** ****** in   his        tweet   he  also said
2024-02-07 00:46:08,280 	Text Hypothesis :	the indian team absolutely admires the all  out 
2024-02-07 00:46:08,281 	Text Alignment  :	I   I      S    S          S       S   S    S   
2024-02-07 00:46:08,281 ========================================================================================================================
2024-02-07 00:46:08,281 Logging Sequence: 137_44.00
2024-02-07 00:46:08,281 	Gloss Reference :	A B+C+D+E
2024-02-07 00:46:08,281 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 00:46:08,281 	Gloss Alignment :	         
2024-02-07 00:46:08,281 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 00:46:08,283 	Text Reference  :	let me tell you the rules that qatar has announced for the      fans travelling  for       the world  cup       
2024-02-07 00:46:08,283 	Text Hypothesis :	*** ** **** *** *** ***** **** they  all posed     for pictures amid celebratory fireworks the indian supporters
2024-02-07 00:46:08,283 	Text Alignment  :	D   D  D    D   D   D     D    S     S   S             S        S    S           S             S      S         
2024-02-07 00:46:08,283 ========================================================================================================================
2024-02-07 00:46:08,283 Logging Sequence: 99_158.00
2024-02-07 00:46:08,283 	Gloss Reference :	A B+C+D+E
2024-02-07 00:46:08,283 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 00:46:08,284 	Gloss Alignment :	         
2024-02-07 00:46:08,284 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 00:46:08,284 	Text Reference  :	the incident occured in  dubai and     it  was  extremely shameful
2024-02-07 00:46:08,284 	Text Hypothesis :	*** shocking he      has now   retired but will be        true    
2024-02-07 00:46:08,285 	Text Alignment  :	D   S        S       S   S     S       S   S    S         S       
2024-02-07 00:46:08,285 ========================================================================================================================
2024-02-07 00:46:09,479 Epoch 1778: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 00:46:09,479 EPOCH 1779
2024-02-07 00:46:26,726 Epoch 1779: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 00:46:26,727 EPOCH 1780
2024-02-07 00:46:43,102 Epoch 1780: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 00:46:43,104 EPOCH 1781
2024-02-07 00:46:59,526 Epoch 1781: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 00:46:59,527 EPOCH 1782
2024-02-07 00:47:16,134 Epoch 1782: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 00:47:16,135 EPOCH 1783
2024-02-07 00:47:32,607 Epoch 1783: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 00:47:32,607 EPOCH 1784
2024-02-07 00:47:48,958 Epoch 1784: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 00:47:48,958 EPOCH 1785
2024-02-07 00:48:05,514 Epoch 1785: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 00:48:05,514 EPOCH 1786
2024-02-07 00:48:22,287 Epoch 1786: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 00:48:22,288 EPOCH 1787
2024-02-07 00:48:38,276 Epoch 1787: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 00:48:38,276 EPOCH 1788
2024-02-07 00:48:54,935 Epoch 1788: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 00:48:54,935 EPOCH 1789
2024-02-07 00:49:11,056 [Epoch: 1789 Step: 00016100] Batch Recognition Loss:   0.000421 => Gls Tokens per Sec:      579 || Batch Translation Loss:   0.021153 => Txt Tokens per Sec:     1679 || Lr: 0.000100
2024-02-07 00:49:11,423 Epoch 1789: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 00:49:11,424 EPOCH 1790
2024-02-07 00:49:28,092 Epoch 1790: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 00:49:28,093 EPOCH 1791
2024-02-07 00:49:44,434 Epoch 1791: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 00:49:44,435 EPOCH 1792
2024-02-07 00:50:00,845 Epoch 1792: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 00:50:00,846 EPOCH 1793
2024-02-07 00:50:17,356 Epoch 1793: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 00:50:17,357 EPOCH 1794
2024-02-07 00:50:33,760 Epoch 1794: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 00:50:33,760 EPOCH 1795
2024-02-07 00:50:50,180 Epoch 1795: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 00:50:50,180 EPOCH 1796
2024-02-07 00:51:06,457 Epoch 1796: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 00:51:06,458 EPOCH 1797
2024-02-07 00:51:22,754 Epoch 1797: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 00:51:22,755 EPOCH 1798
2024-02-07 00:51:39,414 Epoch 1798: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 00:51:39,414 EPOCH 1799
2024-02-07 00:51:55,609 Epoch 1799: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 00:51:55,610 EPOCH 1800
2024-02-07 00:52:12,463 [Epoch: 1800 Step: 00016200] Batch Recognition Loss:   0.000256 => Gls Tokens per Sec:      630 || Batch Translation Loss:   0.016595 => Txt Tokens per Sec:     1744 || Lr: 0.000100
2024-02-07 00:52:12,464 Epoch 1800: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 00:52:12,464 EPOCH 1801
2024-02-07 00:52:28,916 Epoch 1801: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 00:52:28,917 EPOCH 1802
2024-02-07 00:52:45,513 Epoch 1802: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 00:52:45,513 EPOCH 1803
2024-02-07 00:53:01,373 Epoch 1803: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 00:53:01,374 EPOCH 1804
2024-02-07 00:53:18,304 Epoch 1804: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 00:53:18,305 EPOCH 1805
2024-02-07 00:53:34,530 Epoch 1805: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 00:53:34,530 EPOCH 1806
2024-02-07 00:53:51,167 Epoch 1806: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 00:53:51,168 EPOCH 1807
2024-02-07 00:54:07,927 Epoch 1807: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 00:54:07,928 EPOCH 1808
2024-02-07 00:54:24,305 Epoch 1808: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 00:54:24,305 EPOCH 1809
2024-02-07 00:54:40,844 Epoch 1809: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 00:54:40,845 EPOCH 1810
2024-02-07 00:54:57,238 Epoch 1810: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 00:54:57,239 EPOCH 1811
2024-02-07 00:55:13,883 Epoch 1811: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 00:55:13,884 EPOCH 1812
2024-02-07 00:55:14,826 [Epoch: 1812 Step: 00016300] Batch Recognition Loss:   0.000604 => Gls Tokens per Sec:     1360 || Batch Translation Loss:   0.019485 => Txt Tokens per Sec:     4105 || Lr: 0.000100
2024-02-07 00:55:30,542 Epoch 1812: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 00:55:30,542 EPOCH 1813
2024-02-07 00:55:46,974 Epoch 1813: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 00:55:46,974 EPOCH 1814
2024-02-07 00:56:03,637 Epoch 1814: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 00:56:03,638 EPOCH 1815
2024-02-07 00:56:20,108 Epoch 1815: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 00:56:20,109 EPOCH 1816
2024-02-07 00:56:36,624 Epoch 1816: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 00:56:36,625 EPOCH 1817
2024-02-07 00:56:53,078 Epoch 1817: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 00:56:53,079 EPOCH 1818
2024-02-07 00:57:09,534 Epoch 1818: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 00:57:09,534 EPOCH 1819
2024-02-07 00:57:25,913 Epoch 1819: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 00:57:25,913 EPOCH 1820
2024-02-07 00:57:42,319 Epoch 1820: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 00:57:42,320 EPOCH 1821
2024-02-07 00:57:58,737 Epoch 1821: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 00:57:58,737 EPOCH 1822
2024-02-07 00:58:15,567 Epoch 1822: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 00:58:15,568 EPOCH 1823
2024-02-07 00:58:24,861 [Epoch: 1823 Step: 00016400] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:      276 || Batch Translation Loss:   0.027965 => Txt Tokens per Sec:      915 || Lr: 0.000100
2024-02-07 00:58:32,127 Epoch 1823: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-07 00:58:32,128 EPOCH 1824
2024-02-07 00:58:48,564 Epoch 1824: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-07 00:58:48,565 EPOCH 1825
2024-02-07 00:59:05,183 Epoch 1825: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-07 00:59:05,183 EPOCH 1826
2024-02-07 00:59:21,800 Epoch 1826: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-07 00:59:21,801 EPOCH 1827
2024-02-07 00:59:38,016 Epoch 1827: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-07 00:59:38,017 EPOCH 1828
2024-02-07 00:59:54,224 Epoch 1828: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-07 00:59:54,225 EPOCH 1829
2024-02-07 01:00:10,551 Epoch 1829: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.27 
2024-02-07 01:00:10,551 EPOCH 1830
2024-02-07 01:00:26,776 Epoch 1830: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-07 01:00:26,776 EPOCH 1831
2024-02-07 01:00:43,293 Epoch 1831: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-07 01:00:43,294 EPOCH 1832
2024-02-07 01:00:59,863 Epoch 1832: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-07 01:00:59,864 EPOCH 1833
2024-02-07 01:01:16,216 Epoch 1833: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-07 01:01:16,217 EPOCH 1834
2024-02-07 01:01:20,217 [Epoch: 1834 Step: 00016500] Batch Recognition Loss:   0.000347 => Gls Tokens per Sec:      960 || Batch Translation Loss:   0.028460 => Txt Tokens per Sec:     2317 || Lr: 0.000100
2024-02-07 01:01:32,752 Epoch 1834: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-07 01:01:32,752 EPOCH 1835
2024-02-07 01:01:49,100 Epoch 1835: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-07 01:01:49,101 EPOCH 1836
2024-02-07 01:02:05,445 Epoch 1836: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-07 01:02:05,445 EPOCH 1837
2024-02-07 01:02:22,188 Epoch 1837: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-07 01:02:22,189 EPOCH 1838
2024-02-07 01:02:38,536 Epoch 1838: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-07 01:02:38,536 EPOCH 1839
2024-02-07 01:02:55,046 Epoch 1839: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-07 01:02:55,047 EPOCH 1840
2024-02-07 01:03:11,398 Epoch 1840: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-07 01:03:11,399 EPOCH 1841
2024-02-07 01:03:27,731 Epoch 1841: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-07 01:03:27,732 EPOCH 1842
2024-02-07 01:03:44,008 Epoch 1842: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-07 01:03:44,009 EPOCH 1843
2024-02-07 01:04:00,681 Epoch 1843: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-07 01:04:00,682 EPOCH 1844
2024-02-07 01:04:16,938 Epoch 1844: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-07 01:04:16,939 EPOCH 1845
2024-02-07 01:04:25,596 [Epoch: 1845 Step: 00016600] Batch Recognition Loss:   0.000325 => Gls Tokens per Sec:      487 || Batch Translation Loss:   0.010173 => Txt Tokens per Sec:     1415 || Lr: 0.000100
2024-02-07 01:04:33,460 Epoch 1845: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 01:04:33,461 EPOCH 1846
2024-02-07 01:04:49,809 Epoch 1846: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-07 01:04:49,810 EPOCH 1847
2024-02-07 01:05:06,235 Epoch 1847: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-07 01:05:06,236 EPOCH 1848
2024-02-07 01:05:22,582 Epoch 1848: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 01:05:22,582 EPOCH 1849
2024-02-07 01:05:38,819 Epoch 1849: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 01:05:38,820 EPOCH 1850
2024-02-07 01:05:55,495 Epoch 1850: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 01:05:55,496 EPOCH 1851
2024-02-07 01:06:11,908 Epoch 1851: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 01:06:11,908 EPOCH 1852
2024-02-07 01:06:28,263 Epoch 1852: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 01:06:28,264 EPOCH 1853
2024-02-07 01:06:44,587 Epoch 1853: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 01:06:44,588 EPOCH 1854
2024-02-07 01:07:01,177 Epoch 1854: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 01:07:01,178 EPOCH 1855
2024-02-07 01:07:17,643 Epoch 1855: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 01:07:17,643 EPOCH 1856
2024-02-07 01:07:28,661 [Epoch: 1856 Step: 00016700] Batch Recognition Loss:   0.000515 => Gls Tokens per Sec:      581 || Batch Translation Loss:   0.022353 => Txt Tokens per Sec:     1626 || Lr: 0.000100
2024-02-07 01:07:34,455 Epoch 1856: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 01:07:34,455 EPOCH 1857
2024-02-07 01:07:50,804 Epoch 1857: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 01:07:50,805 EPOCH 1858
2024-02-07 01:08:07,418 Epoch 1858: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 01:08:07,418 EPOCH 1859
2024-02-07 01:08:23,777 Epoch 1859: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 01:08:23,778 EPOCH 1860
2024-02-07 01:08:40,127 Epoch 1860: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 01:08:40,128 EPOCH 1861
2024-02-07 01:08:56,584 Epoch 1861: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 01:08:56,584 EPOCH 1862
2024-02-07 01:09:13,219 Epoch 1862: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 01:09:13,219 EPOCH 1863
2024-02-07 01:09:29,553 Epoch 1863: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 01:09:29,554 EPOCH 1864
2024-02-07 01:09:46,271 Epoch 1864: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 01:09:46,273 EPOCH 1865
2024-02-07 01:10:03,405 Epoch 1865: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 01:10:03,405 EPOCH 1866
2024-02-07 01:10:19,958 Epoch 1866: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 01:10:19,959 EPOCH 1867
2024-02-07 01:10:35,393 [Epoch: 1867 Step: 00016800] Batch Recognition Loss:   0.000272 => Gls Tokens per Sec:      439 || Batch Translation Loss:   0.019766 => Txt Tokens per Sec:     1358 || Lr: 0.000100
2024-02-07 01:10:36,334 Epoch 1867: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 01:10:36,334 EPOCH 1868
2024-02-07 01:10:52,833 Epoch 1868: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 01:10:52,834 EPOCH 1869
2024-02-07 01:11:09,178 Epoch 1869: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 01:11:09,179 EPOCH 1870
2024-02-07 01:11:25,708 Epoch 1870: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.14 
2024-02-07 01:11:25,709 EPOCH 1871
2024-02-07 01:11:41,865 Epoch 1871: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.16 
2024-02-07 01:11:41,865 EPOCH 1872
2024-02-07 01:11:58,311 Epoch 1872: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 01:11:58,312 EPOCH 1873
2024-02-07 01:12:14,919 Epoch 1873: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 01:12:14,919 EPOCH 1874
2024-02-07 01:12:31,437 Epoch 1874: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 01:12:31,437 EPOCH 1875
2024-02-07 01:12:48,234 Epoch 1875: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-07 01:12:48,234 EPOCH 1876
2024-02-07 01:13:04,369 Epoch 1876: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 01:13:04,370 EPOCH 1877
2024-02-07 01:13:20,963 Epoch 1877: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 01:13:20,963 EPOCH 1878
2024-02-07 01:13:33,749 [Epoch: 1878 Step: 00016900] Batch Recognition Loss:   0.000364 => Gls Tokens per Sec:      630 || Batch Translation Loss:   0.024948 => Txt Tokens per Sec:     1724 || Lr: 0.000100
2024-02-07 01:13:37,269 Epoch 1878: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 01:13:37,269 EPOCH 1879
2024-02-07 01:13:53,467 Epoch 1879: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 01:13:53,467 EPOCH 1880
2024-02-07 01:14:10,032 Epoch 1880: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 01:14:10,032 EPOCH 1881
2024-02-07 01:14:26,424 Epoch 1881: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 01:14:26,425 EPOCH 1882
2024-02-07 01:14:42,787 Epoch 1882: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 01:14:42,787 EPOCH 1883
2024-02-07 01:14:59,481 Epoch 1883: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-07 01:14:59,482 EPOCH 1884
2024-02-07 01:15:16,121 Epoch 1884: Total Training Recognition Loss 0.00  Total Training Translation Loss 2.36 
2024-02-07 01:15:16,122 EPOCH 1885
2024-02-07 01:15:32,606 Epoch 1885: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.29 
2024-02-07 01:15:32,607 EPOCH 1886
2024-02-07 01:15:49,011 Epoch 1886: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.77 
2024-02-07 01:15:49,012 EPOCH 1887
2024-02-07 01:16:05,335 Epoch 1887: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.22 
2024-02-07 01:16:05,336 EPOCH 1888
2024-02-07 01:16:21,968 Epoch 1888: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.51 
2024-02-07 01:16:21,969 EPOCH 1889
2024-02-07 01:16:35,187 [Epoch: 1889 Step: 00017000] Batch Recognition Loss:   0.001277 => Gls Tokens per Sec:      707 || Batch Translation Loss:   0.090328 => Txt Tokens per Sec:     1913 || Lr: 0.000100
2024-02-07 01:16:38,412 Epoch 1889: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.20 
2024-02-07 01:16:38,413 EPOCH 1890
2024-02-07 01:16:55,081 Epoch 1890: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-07 01:16:55,082 EPOCH 1891
2024-02-07 01:17:11,347 Epoch 1891: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-07 01:17:11,347 EPOCH 1892
2024-02-07 01:17:27,986 Epoch 1892: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-07 01:17:27,986 EPOCH 1893
2024-02-07 01:17:44,370 Epoch 1893: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-07 01:17:44,371 EPOCH 1894
2024-02-07 01:18:01,052 Epoch 1894: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-07 01:18:01,053 EPOCH 1895
2024-02-07 01:18:17,226 Epoch 1895: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-07 01:18:17,227 EPOCH 1896
2024-02-07 01:18:33,555 Epoch 1896: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-07 01:18:33,555 EPOCH 1897
2024-02-07 01:18:49,992 Epoch 1897: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-07 01:18:49,992 EPOCH 1898
2024-02-07 01:19:06,371 Epoch 1898: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 01:19:06,371 EPOCH 1899
2024-02-07 01:19:22,912 Epoch 1899: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 01:19:22,912 EPOCH 1900
2024-02-07 01:19:39,323 [Epoch: 1900 Step: 00017100] Batch Recognition Loss:   0.000733 => Gls Tokens per Sec:      647 || Batch Translation Loss:   0.013629 => Txt Tokens per Sec:     1791 || Lr: 0.000100
2024-02-07 01:19:39,323 Epoch 1900: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 01:19:39,323 EPOCH 1901
2024-02-07 01:19:55,513 Epoch 1901: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 01:19:55,514 EPOCH 1902
2024-02-07 01:20:11,909 Epoch 1902: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 01:20:11,909 EPOCH 1903
2024-02-07 01:20:28,463 Epoch 1903: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 01:20:28,464 EPOCH 1904
2024-02-07 01:20:45,043 Epoch 1904: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 01:20:45,044 EPOCH 1905
2024-02-07 01:21:01,543 Epoch 1905: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 01:21:01,544 EPOCH 1906
2024-02-07 01:21:17,716 Epoch 1906: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-07 01:21:17,716 EPOCH 1907
2024-02-07 01:21:34,040 Epoch 1907: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-07 01:21:34,041 EPOCH 1908
2024-02-07 01:21:50,288 Epoch 1908: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-07 01:21:50,289 EPOCH 1909
2024-02-07 01:22:06,663 Epoch 1909: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-07 01:22:06,663 EPOCH 1910
2024-02-07 01:22:22,912 Epoch 1910: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-07 01:22:22,912 EPOCH 1911
2024-02-07 01:22:39,409 Epoch 1911: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-07 01:22:39,409 EPOCH 1912
2024-02-07 01:22:40,141 [Epoch: 1912 Step: 00017200] Batch Recognition Loss:   0.000495 => Gls Tokens per Sec:     1751 || Batch Translation Loss:   0.052756 => Txt Tokens per Sec:     5285 || Lr: 0.000100
2024-02-07 01:22:56,026 Epoch 1912: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-07 01:22:56,027 EPOCH 1913
2024-02-07 01:23:12,509 Epoch 1913: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-07 01:23:12,509 EPOCH 1914
2024-02-07 01:23:28,983 Epoch 1914: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-07 01:23:28,984 EPOCH 1915
2024-02-07 01:23:45,731 Epoch 1915: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-07 01:23:45,731 EPOCH 1916
2024-02-07 01:24:02,283 Epoch 1916: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-07 01:24:02,284 EPOCH 1917
2024-02-07 01:24:18,583 Epoch 1917: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 01:24:18,583 EPOCH 1918
2024-02-07 01:24:35,423 Epoch 1918: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.20 
2024-02-07 01:24:35,423 EPOCH 1919
2024-02-07 01:24:52,034 Epoch 1919: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 01:24:52,035 EPOCH 1920
2024-02-07 01:25:08,249 Epoch 1920: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.20 
2024-02-07 01:25:08,249 EPOCH 1921
2024-02-07 01:25:24,792 Epoch 1921: Total Training Recognition Loss 0.38  Total Training Translation Loss 0.22 
2024-02-07 01:25:24,793 EPOCH 1922
2024-02-07 01:25:41,320 Epoch 1922: Total Training Recognition Loss 0.31  Total Training Translation Loss 0.26 
2024-02-07 01:25:41,321 EPOCH 1923
2024-02-07 01:25:46,220 [Epoch: 1923 Step: 00017300] Batch Recognition Loss:   0.001685 => Gls Tokens per Sec:      339 || Batch Translation Loss:   0.030130 => Txt Tokens per Sec:      990 || Lr: 0.000100
2024-02-07 01:25:57,782 Epoch 1923: Total Training Recognition Loss 0.81  Total Training Translation Loss 0.30 
2024-02-07 01:25:57,783 EPOCH 1924
2024-02-07 01:26:14,084 Epoch 1924: Total Training Recognition Loss 1.07  Total Training Translation Loss 0.32 
2024-02-07 01:26:14,085 EPOCH 1925
2024-02-07 01:26:30,479 Epoch 1925: Total Training Recognition Loss 0.95  Total Training Translation Loss 0.33 
2024-02-07 01:26:30,480 EPOCH 1926
2024-02-07 01:26:47,075 Epoch 1926: Total Training Recognition Loss 3.01  Total Training Translation Loss 0.56 
2024-02-07 01:26:47,076 EPOCH 1927
2024-02-07 01:27:03,291 Epoch 1927: Total Training Recognition Loss 1.41  Total Training Translation Loss 0.78 
2024-02-07 01:27:03,292 EPOCH 1928
2024-02-07 01:27:19,891 Epoch 1928: Total Training Recognition Loss 0.42  Total Training Translation Loss 0.72 
2024-02-07 01:27:19,891 EPOCH 1929
2024-02-07 01:27:36,441 Epoch 1929: Total Training Recognition Loss 0.11  Total Training Translation Loss 0.60 
2024-02-07 01:27:36,441 EPOCH 1930
2024-02-07 01:27:53,103 Epoch 1930: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.65 
2024-02-07 01:27:53,103 EPOCH 1931
2024-02-07 01:28:09,671 Epoch 1931: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.43 
2024-02-07 01:28:09,671 EPOCH 1932
2024-02-07 01:28:26,537 Epoch 1932: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.38 
2024-02-07 01:28:26,537 EPOCH 1933
2024-02-07 01:28:43,231 Epoch 1933: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.33 
2024-02-07 01:28:43,231 EPOCH 1934
2024-02-07 01:28:51,457 [Epoch: 1934 Step: 00017400] Batch Recognition Loss:   0.000773 => Gls Tokens per Sec:      357 || Batch Translation Loss:   0.036475 => Txt Tokens per Sec:     1132 || Lr: 0.000100
2024-02-07 01:28:59,675 Epoch 1934: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-07 01:28:59,676 EPOCH 1935
2024-02-07 01:29:16,177 Epoch 1935: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-07 01:29:16,177 EPOCH 1936
2024-02-07 01:29:32,543 Epoch 1936: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.23 
2024-02-07 01:29:32,544 EPOCH 1937
2024-02-07 01:29:48,947 Epoch 1937: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.23 
2024-02-07 01:29:48,948 EPOCH 1938
2024-02-07 01:30:05,090 Epoch 1938: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 01:30:05,090 EPOCH 1939
2024-02-07 01:30:21,538 Epoch 1939: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 01:30:21,539 EPOCH 1940
2024-02-07 01:30:38,142 Epoch 1940: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 01:30:38,143 EPOCH 1941
2024-02-07 01:30:54,771 Epoch 1941: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 01:30:54,771 EPOCH 1942
2024-02-07 01:31:11,241 Epoch 1942: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 01:31:11,242 EPOCH 1943
2024-02-07 01:31:27,624 Epoch 1943: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 01:31:27,625 EPOCH 1944
2024-02-07 01:31:44,226 Epoch 1944: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 01:31:44,227 EPOCH 1945
2024-02-07 01:31:45,581 [Epoch: 1945 Step: 00017500] Batch Recognition Loss:   0.000240 => Gls Tokens per Sec:     3783 || Batch Translation Loss:   0.022375 => Txt Tokens per Sec:     8936 || Lr: 0.000100
2024-02-07 01:32:00,421 Epoch 1945: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 01:32:00,422 EPOCH 1946
2024-02-07 01:32:16,930 Epoch 1946: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 01:32:16,932 EPOCH 1947
2024-02-07 01:32:33,260 Epoch 1947: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 01:32:33,261 EPOCH 1948
2024-02-07 01:32:49,644 Epoch 1948: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-07 01:32:49,644 EPOCH 1949
2024-02-07 01:33:05,895 Epoch 1949: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 01:33:05,895 EPOCH 1950
2024-02-07 01:33:22,198 Epoch 1950: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 01:33:22,198 EPOCH 1951
2024-02-07 01:33:38,732 Epoch 1951: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 01:33:38,733 EPOCH 1952
2024-02-07 01:33:55,004 Epoch 1952: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 01:33:55,005 EPOCH 1953
2024-02-07 01:34:11,597 Epoch 1953: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 01:34:11,598 EPOCH 1954
2024-02-07 01:34:28,354 Epoch 1954: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 01:34:28,355 EPOCH 1955
2024-02-07 01:34:44,638 Epoch 1955: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 01:34:44,639 EPOCH 1956
2024-02-07 01:34:49,991 [Epoch: 1956 Step: 00017600] Batch Recognition Loss:   0.000381 => Gls Tokens per Sec:     1196 || Batch Translation Loss:   0.013910 => Txt Tokens per Sec:     3448 || Lr: 0.000100
2024-02-07 01:35:01,084 Epoch 1956: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 01:35:01,085 EPOCH 1957
2024-02-07 01:35:17,499 Epoch 1957: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 01:35:17,500 EPOCH 1958
2024-02-07 01:35:33,992 Epoch 1958: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 01:35:33,992 EPOCH 1959
2024-02-07 01:35:50,465 Epoch 1959: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 01:35:50,465 EPOCH 1960
2024-02-07 01:36:06,705 Epoch 1960: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 01:36:06,706 EPOCH 1961
2024-02-07 01:36:23,041 Epoch 1961: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 01:36:23,041 EPOCH 1962
2024-02-07 01:36:39,668 Epoch 1962: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 01:36:39,669 EPOCH 1963
2024-02-07 01:36:56,110 Epoch 1963: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 01:36:56,110 EPOCH 1964
2024-02-07 01:37:12,453 Epoch 1964: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 01:37:12,454 EPOCH 1965
2024-02-07 01:37:29,051 Epoch 1965: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 01:37:29,051 EPOCH 1966
2024-02-07 01:37:45,385 Epoch 1966: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 01:37:45,386 EPOCH 1967
2024-02-07 01:37:52,048 [Epoch: 1967 Step: 00017700] Batch Recognition Loss:   0.000700 => Gls Tokens per Sec:     1018 || Batch Translation Loss:   0.010211 => Txt Tokens per Sec:     2648 || Lr: 0.000100
2024-02-07 01:38:01,778 Epoch 1967: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 01:38:01,779 EPOCH 1968
2024-02-07 01:38:18,585 Epoch 1968: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 01:38:18,586 EPOCH 1969
2024-02-07 01:38:35,092 Epoch 1969: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 01:38:35,093 EPOCH 1970
2024-02-07 01:38:51,399 Epoch 1970: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-07 01:38:51,400 EPOCH 1971
2024-02-07 01:39:07,963 Epoch 1971: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.33 
2024-02-07 01:39:07,964 EPOCH 1972
2024-02-07 01:39:24,312 Epoch 1972: Total Training Recognition Loss 0.01  Total Training Translation Loss 8.95 
2024-02-07 01:39:24,313 EPOCH 1973
2024-02-07 01:39:40,786 Epoch 1973: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.23 
2024-02-07 01:39:40,786 EPOCH 1974
2024-02-07 01:39:57,237 Epoch 1974: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.93 
2024-02-07 01:39:57,238 EPOCH 1975
2024-02-07 01:40:13,615 Epoch 1975: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.17 
2024-02-07 01:40:13,615 EPOCH 1976
2024-02-07 01:40:30,250 Epoch 1976: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-07 01:40:30,250 EPOCH 1977
2024-02-07 01:40:46,706 Epoch 1977: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.02 
2024-02-07 01:40:46,707 EPOCH 1978
2024-02-07 01:40:59,397 [Epoch: 1978 Step: 00017800] Batch Recognition Loss:   0.001559 => Gls Tokens per Sec:      635 || Batch Translation Loss:   0.097689 => Txt Tokens per Sec:     1701 || Lr: 0.000100
2024-02-07 01:41:03,140 Epoch 1978: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-07 01:41:03,140 EPOCH 1979
2024-02-07 01:41:19,596 Epoch 1979: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-07 01:41:19,597 EPOCH 1980
2024-02-07 01:41:36,152 Epoch 1980: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-07 01:41:36,152 EPOCH 1981
2024-02-07 01:41:52,499 Epoch 1981: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-07 01:41:52,500 EPOCH 1982
2024-02-07 01:42:09,257 Epoch 1982: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-07 01:42:09,257 EPOCH 1983
2024-02-07 01:42:25,733 Epoch 1983: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-07 01:42:25,733 EPOCH 1984
2024-02-07 01:42:42,238 Epoch 1984: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-07 01:42:42,239 EPOCH 1985
2024-02-07 01:42:58,324 Epoch 1985: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-07 01:42:58,325 EPOCH 1986
2024-02-07 01:43:15,068 Epoch 1986: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 01:43:15,068 EPOCH 1987
2024-02-07 01:43:31,558 Epoch 1987: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-07 01:43:31,559 EPOCH 1988
2024-02-07 01:43:48,240 Epoch 1988: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 01:43:48,240 EPOCH 1989
2024-02-07 01:44:04,166 [Epoch: 1989 Step: 00017900] Batch Recognition Loss:   0.001115 => Gls Tokens per Sec:      587 || Batch Translation Loss:   0.022242 => Txt Tokens per Sec:     1638 || Lr: 0.000100
2024-02-07 01:44:04,542 Epoch 1989: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 01:44:04,542 EPOCH 1990
2024-02-07 01:44:20,986 Epoch 1990: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 01:44:20,987 EPOCH 1991
2024-02-07 01:44:37,473 Epoch 1991: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 01:44:37,475 EPOCH 1992
2024-02-07 01:44:54,386 Epoch 1992: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 01:44:54,386 EPOCH 1993
2024-02-07 01:45:11,020 Epoch 1993: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 01:45:11,021 EPOCH 1994
2024-02-07 01:45:27,483 Epoch 1994: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 01:45:27,483 EPOCH 1995
2024-02-07 01:45:44,104 Epoch 1995: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 01:45:44,105 EPOCH 1996
2024-02-07 01:46:00,288 Epoch 1996: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 01:46:00,288 EPOCH 1997
2024-02-07 01:46:17,132 Epoch 1997: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 01:46:17,133 EPOCH 1998
2024-02-07 01:46:33,471 Epoch 1998: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 01:46:33,471 EPOCH 1999
2024-02-07 01:46:50,425 Epoch 1999: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 01:46:50,426 EPOCH 2000
2024-02-07 01:47:06,616 [Epoch: 2000 Step: 00018000] Batch Recognition Loss:   0.000781 => Gls Tokens per Sec:      656 || Batch Translation Loss:   0.026637 => Txt Tokens per Sec:     1815 || Lr: 0.000100
2024-02-07 01:48:14,779 Validation result at epoch 2000, step    18000: duration: 68.1611s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.40410	Translation Loss: 93589.39062	PPL: 11472.64844
	Eval Metric: BLEU
	WER 4.38	(DEL: 0.00,	INS: 0.00,	SUB: 4.38)
	BLEU-4 0.67	(BLEU-1: 11.47,	BLEU-2: 3.91,	BLEU-3: 1.43,	BLEU-4: 0.67)
	CHRF 17.48	ROUGE 9.65
2024-02-07 01:48:14,781 Logging Recognition and Translation Outputs
2024-02-07 01:48:14,781 ========================================================================================================================
2024-02-07 01:48:14,781 Logging Sequence: 179_309.00
2024-02-07 01:48:14,781 	Gloss Reference :	A B+C+D+E
2024-02-07 01:48:14,781 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 01:48:14,782 	Gloss Alignment :	         
2024-02-07 01:48:14,782 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 01:48:14,785 	Text Reference  :	** before the ioa could    send the  notice wfi has asked    phogat to   explain her indiscipline
2024-02-07 01:48:14,785 	Text Hypothesis :	we hope   the *** response to   help she    is  so  athletes be     held in      her departure   
2024-02-07 01:48:14,785 	Text Alignment  :	I  S          D   S        S    S    S      S   S   S        S      S    S           S           
2024-02-07 01:48:14,786 ========================================================================================================================
2024-02-07 01:48:14,786 Logging Sequence: 156_35.00
2024-02-07 01:48:14,786 	Gloss Reference :	A B+C+D+E
2024-02-07 01:48:14,786 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 01:48:14,786 	Gloss Alignment :	         
2024-02-07 01:48:14,786 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 01:48:14,788 	Text Reference  :	the first season   of      mlc began  on      13th     july   2023 and  ended on    30th july  2023  with     six  teams
2024-02-07 01:48:14,789 	Text Hypothesis :	*** miny' original captain was kieron pollard nicholas pooran was  held in    place in   place which pakistan were 126  
2024-02-07 01:48:14,789 	Text Alignment  :	D   S     S        S       S   S      S       S        S      S    S    S     S     S    S     S     S        S    S    
2024-02-07 01:48:14,789 ========================================================================================================================
2024-02-07 01:48:14,789 Logging Sequence: 129_45.00
2024-02-07 01:48:14,789 	Gloss Reference :	A B+C+D+E
2024-02-07 01:48:14,789 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 01:48:14,790 	Gloss Alignment :	         
2024-02-07 01:48:14,790 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 01:48:14,792 	Text Reference  :	suga then         announced that from 5        july onwards japan  will be      in  a      state of *** emergency
2024-02-07 01:48:14,792 	Text Hypothesis :	the  postponement of        the  2020 olympics by   15      months has  stalled the income flow  of the ioc      
2024-02-07 01:48:14,792 	Text Alignment  :	S    S            S         S    S    S        S    S       S      S    S       S   S      S        I   S        
2024-02-07 01:48:14,792 ========================================================================================================================
2024-02-07 01:48:14,792 Logging Sequence: 56_17.00
2024-02-07 01:48:14,792 	Gloss Reference :	A B+C+D+E  
2024-02-07 01:48:14,792 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-07 01:48:14,793 	Gloss Alignment :	  S        
2024-02-07 01:48:14,793 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 01:48:14,793 	Text Reference  :	*** it     was *** held at  mumbai's wankhede stadium
2024-02-07 01:48:14,793 	Text Hypothesis :	the reason was not the  ipl despite  their    screens
2024-02-07 01:48:14,794 	Text Alignment  :	I   S          I   S    S   S        S        S      
2024-02-07 01:48:14,794 ========================================================================================================================
2024-02-07 01:48:14,794 Logging Sequence: 152_73.00
2024-02-07 01:48:14,794 	Gloss Reference :	A B+C+D+E
2024-02-07 01:48:14,794 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 01:48:14,794 	Gloss Alignment :	         
2024-02-07 01:48:14,794 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 01:48:14,795 	Text Reference  :	**** ****** *** ********* **** ** eventually he      too      got out by    shaheen afridi  
2024-02-07 01:48:14,795 	Text Hypothesis :	when sharma was pakistan' turn to bat        against pakistan in  any place with    pakistan
2024-02-07 01:48:14,795 	Text Alignment  :	I    I      I   I         I    I  S          S       S        S   S   S     S       S       
2024-02-07 01:48:14,796 ========================================================================================================================
2024-02-07 01:48:14,799 Epoch 2000: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 01:48:14,800 EPOCH 2001
2024-02-07 01:48:31,951 Epoch 2001: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 01:48:31,952 EPOCH 2002
2024-02-07 01:48:48,389 Epoch 2002: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 01:48:48,390 EPOCH 2003
2024-02-07 01:49:04,951 Epoch 2003: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 01:49:04,952 EPOCH 2004
2024-02-07 01:49:21,334 Epoch 2004: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 01:49:21,335 EPOCH 2005
2024-02-07 01:49:37,601 Epoch 2005: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 01:49:37,601 EPOCH 2006
2024-02-07 01:49:53,707 Epoch 2006: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 01:49:53,708 EPOCH 2007
2024-02-07 01:50:10,236 Epoch 2007: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 01:50:10,237 EPOCH 2008
2024-02-07 01:50:26,835 Epoch 2008: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 01:50:26,836 EPOCH 2009
2024-02-07 01:50:43,080 Epoch 2009: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 01:50:43,080 EPOCH 2010
2024-02-07 01:50:59,650 Epoch 2010: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 01:50:59,651 EPOCH 2011
2024-02-07 01:51:16,128 Epoch 2011: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 01:51:16,128 EPOCH 2012
2024-02-07 01:51:16,589 [Epoch: 2012 Step: 00018100] Batch Recognition Loss:   0.000285 => Gls Tokens per Sec:     2780 || Batch Translation Loss:   0.021506 => Txt Tokens per Sec:     7627 || Lr: 0.000100
2024-02-07 01:51:32,524 Epoch 2012: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 01:51:32,524 EPOCH 2013
2024-02-07 01:51:49,184 Epoch 2013: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 01:51:49,185 EPOCH 2014
2024-02-07 01:52:05,613 Epoch 2014: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 01:52:05,614 EPOCH 2015
2024-02-07 01:52:22,314 Epoch 2015: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 01:52:22,314 EPOCH 2016
2024-02-07 01:52:38,512 Epoch 2016: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 01:52:38,512 EPOCH 2017
2024-02-07 01:52:54,999 Epoch 2017: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 01:52:54,999 EPOCH 2018
2024-02-07 01:53:11,445 Epoch 2018: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 01:53:11,445 EPOCH 2019
2024-02-07 01:53:27,942 Epoch 2019: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 01:53:27,942 EPOCH 2020
2024-02-07 01:53:44,481 Epoch 2020: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-07 01:53:44,481 EPOCH 2021
2024-02-07 01:54:00,923 Epoch 2021: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-07 01:54:00,924 EPOCH 2022
2024-02-07 01:54:17,072 Epoch 2022: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-07 01:54:17,073 EPOCH 2023
2024-02-07 01:54:22,132 [Epoch: 2023 Step: 00018200] Batch Recognition Loss:   0.000333 => Gls Tokens per Sec:      328 || Batch Translation Loss:   0.010643 => Txt Tokens per Sec:      998 || Lr: 0.000100
2024-02-07 01:54:33,534 Epoch 2023: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-07 01:54:33,535 EPOCH 2024
2024-02-07 01:54:50,005 Epoch 2024: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-07 01:54:50,006 EPOCH 2025
2024-02-07 01:55:06,710 Epoch 2025: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 01:55:06,711 EPOCH 2026
2024-02-07 01:55:23,121 Epoch 2026: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 01:55:23,122 EPOCH 2027
2024-02-07 01:55:39,420 Epoch 2027: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 01:55:39,420 EPOCH 2028
2024-02-07 01:55:55,817 Epoch 2028: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 01:55:55,818 EPOCH 2029
2024-02-07 01:56:12,471 Epoch 2029: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 01:56:12,471 EPOCH 2030
2024-02-07 01:56:29,021 Epoch 2030: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 01:56:29,022 EPOCH 2031
2024-02-07 01:56:45,581 Epoch 2031: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 01:56:45,582 EPOCH 2032
2024-02-07 01:57:01,875 Epoch 2032: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 01:57:01,875 EPOCH 2033
2024-02-07 01:57:18,335 Epoch 2033: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 01:57:18,335 EPOCH 2034
2024-02-07 01:57:22,274 [Epoch: 2034 Step: 00018300] Batch Recognition Loss:   0.000959 => Gls Tokens per Sec:      975 || Batch Translation Loss:   0.019257 => Txt Tokens per Sec:     2602 || Lr: 0.000100
2024-02-07 01:57:34,730 Epoch 2034: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 01:57:34,731 EPOCH 2035
2024-02-07 01:57:51,224 Epoch 2035: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 01:57:51,224 EPOCH 2036
2024-02-07 01:58:07,573 Epoch 2036: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 01:58:07,573 EPOCH 2037
2024-02-07 01:58:24,329 Epoch 2037: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 01:58:24,329 EPOCH 2038
2024-02-07 01:58:40,884 Epoch 2038: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 01:58:40,884 EPOCH 2039
2024-02-07 01:58:57,204 Epoch 2039: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 01:58:57,205 EPOCH 2040
2024-02-07 01:59:13,439 Epoch 2040: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 01:59:13,439 EPOCH 2041
2024-02-07 01:59:29,795 Epoch 2041: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 01:59:29,796 EPOCH 2042
2024-02-07 01:59:46,209 Epoch 2042: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 01:59:46,210 EPOCH 2043
2024-02-07 02:00:02,280 Epoch 2043: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 02:00:02,281 EPOCH 2044
2024-02-07 02:00:18,678 Epoch 2044: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 02:00:18,679 EPOCH 2045
2024-02-07 02:00:29,089 [Epoch: 2045 Step: 00018400] Batch Recognition Loss:   0.000419 => Gls Tokens per Sec:      492 || Batch Translation Loss:   0.020034 => Txt Tokens per Sec:     1526 || Lr: 0.000100
2024-02-07 02:00:35,065 Epoch 2045: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 02:00:35,066 EPOCH 2046
2024-02-07 02:00:51,461 Epoch 2046: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 02:00:51,462 EPOCH 2047
2024-02-07 02:01:08,035 Epoch 2047: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-07 02:01:08,036 EPOCH 2048
2024-02-07 02:01:24,448 Epoch 2048: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 02:01:24,449 EPOCH 2049
2024-02-07 02:01:41,202 Epoch 2049: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-07 02:01:41,203 EPOCH 2050
2024-02-07 02:01:57,585 Epoch 2050: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-07 02:01:57,586 EPOCH 2051
2024-02-07 02:02:13,934 Epoch 2051: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-07 02:02:13,934 EPOCH 2052
2024-02-07 02:02:30,219 Epoch 2052: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-07 02:02:30,220 EPOCH 2053
2024-02-07 02:02:46,444 Epoch 2053: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.77 
2024-02-07 02:02:46,444 EPOCH 2054
2024-02-07 02:03:03,096 Epoch 2054: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.81 
2024-02-07 02:03:03,097 EPOCH 2055
2024-02-07 02:03:19,588 Epoch 2055: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-07 02:03:19,588 EPOCH 2056
2024-02-07 02:03:31,363 [Epoch: 2056 Step: 00018500] Batch Recognition Loss:   0.000618 => Gls Tokens per Sec:      467 || Batch Translation Loss:   0.038347 => Txt Tokens per Sec:     1311 || Lr: 0.000100
2024-02-07 02:03:36,116 Epoch 2056: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-07 02:03:36,116 EPOCH 2057
2024-02-07 02:03:52,718 Epoch 2057: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-07 02:03:52,718 EPOCH 2058
2024-02-07 02:04:09,470 Epoch 2058: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-07 02:04:09,471 EPOCH 2059
2024-02-07 02:04:25,904 Epoch 2059: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-07 02:04:25,905 EPOCH 2060
2024-02-07 02:04:42,524 Epoch 2060: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-07 02:04:42,524 EPOCH 2061
2024-02-07 02:04:58,725 Epoch 2061: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.86 
2024-02-07 02:04:58,726 EPOCH 2062
2024-02-07 02:05:15,282 Epoch 2062: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-07 02:05:15,283 EPOCH 2063
2024-02-07 02:05:31,688 Epoch 2063: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.21 
2024-02-07 02:05:31,688 EPOCH 2064
2024-02-07 02:05:48,438 Epoch 2064: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.99 
2024-02-07 02:05:48,439 EPOCH 2065
2024-02-07 02:06:04,823 Epoch 2065: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-07 02:06:04,824 EPOCH 2066
2024-02-07 02:06:21,433 Epoch 2066: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-07 02:06:21,434 EPOCH 2067
2024-02-07 02:06:33,888 [Epoch: 2067 Step: 00018600] Batch Recognition Loss:   0.001225 => Gls Tokens per Sec:      544 || Batch Translation Loss:   0.086066 => Txt Tokens per Sec:     1473 || Lr: 0.000100
2024-02-07 02:06:38,098 Epoch 2067: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-07 02:06:38,098 EPOCH 2068
2024-02-07 02:06:54,646 Epoch 2068: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-07 02:06:54,646 EPOCH 2069
2024-02-07 02:07:11,112 Epoch 2069: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-07 02:07:11,113 EPOCH 2070
2024-02-07 02:07:27,962 Epoch 2070: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.70 
2024-02-07 02:07:27,963 EPOCH 2071
2024-02-07 02:07:44,324 Epoch 2071: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.22 
2024-02-07 02:07:44,324 EPOCH 2072
2024-02-07 02:08:00,707 Epoch 2072: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.76 
2024-02-07 02:08:00,707 EPOCH 2073
2024-02-07 02:08:16,948 Epoch 2073: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.22 
2024-02-07 02:08:16,948 EPOCH 2074
2024-02-07 02:08:33,234 Epoch 2074: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-07 02:08:33,235 EPOCH 2075
2024-02-07 02:08:49,600 Epoch 2075: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-07 02:08:49,600 EPOCH 2076
2024-02-07 02:09:06,001 Epoch 2076: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-07 02:09:06,002 EPOCH 2077
2024-02-07 02:09:22,654 Epoch 2077: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-07 02:09:22,654 EPOCH 2078
2024-02-07 02:09:32,184 [Epoch: 2078 Step: 00018700] Batch Recognition Loss:   0.000458 => Gls Tokens per Sec:      846 || Batch Translation Loss:   0.085639 => Txt Tokens per Sec:     2215 || Lr: 0.000100
2024-02-07 02:09:38,871 Epoch 2078: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-07 02:09:38,871 EPOCH 2079
2024-02-07 02:09:55,549 Epoch 2079: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-07 02:09:55,549 EPOCH 2080
2024-02-07 02:10:12,111 Epoch 2080: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-07 02:10:12,112 EPOCH 2081
2024-02-07 02:10:28,572 Epoch 2081: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-07 02:10:28,572 EPOCH 2082
2024-02-07 02:10:44,816 Epoch 2082: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-07 02:10:44,816 EPOCH 2083
2024-02-07 02:11:01,089 Epoch 2083: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-07 02:11:01,090 EPOCH 2084
2024-02-07 02:11:17,598 Epoch 2084: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-07 02:11:17,598 EPOCH 2085
2024-02-07 02:11:33,855 Epoch 2085: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-07 02:11:33,856 EPOCH 2086
2024-02-07 02:11:50,275 Epoch 2086: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-07 02:11:50,276 EPOCH 2087
2024-02-07 02:12:06,657 Epoch 2087: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 02:12:06,657 EPOCH 2088
2024-02-07 02:12:22,929 Epoch 2088: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 02:12:22,929 EPOCH 2089
2024-02-07 02:12:38,979 [Epoch: 2089 Step: 00018800] Batch Recognition Loss:   0.000664 => Gls Tokens per Sec:      582 || Batch Translation Loss:   0.021982 => Txt Tokens per Sec:     1610 || Lr: 0.000100
2024-02-07 02:12:39,434 Epoch 2089: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 02:12:39,434 EPOCH 2090
2024-02-07 02:12:55,875 Epoch 2090: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 02:12:55,876 EPOCH 2091
2024-02-07 02:13:11,968 Epoch 2091: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 02:13:11,969 EPOCH 2092
2024-02-07 02:13:28,717 Epoch 2092: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 02:13:28,718 EPOCH 2093
2024-02-07 02:13:44,911 Epoch 2093: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 02:13:44,912 EPOCH 2094
2024-02-07 02:14:01,360 Epoch 2094: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 02:14:01,361 EPOCH 2095
2024-02-07 02:14:17,785 Epoch 2095: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 02:14:17,786 EPOCH 2096
2024-02-07 02:14:34,385 Epoch 2096: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 02:14:34,386 EPOCH 2097
2024-02-07 02:14:50,769 Epoch 2097: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 02:14:50,770 EPOCH 2098
2024-02-07 02:15:07,181 Epoch 2098: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 02:15:07,181 EPOCH 2099
2024-02-07 02:15:23,483 Epoch 2099: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 02:15:23,483 EPOCH 2100
2024-02-07 02:15:39,676 [Epoch: 2100 Step: 00018900] Batch Recognition Loss:   0.000431 => Gls Tokens per Sec:      656 || Batch Translation Loss:   0.020326 => Txt Tokens per Sec:     1815 || Lr: 0.000100
2024-02-07 02:15:39,677 Epoch 2100: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 02:15:39,677 EPOCH 2101
2024-02-07 02:15:56,256 Epoch 2101: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 02:15:56,257 EPOCH 2102
2024-02-07 02:16:12,757 Epoch 2102: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 02:16:12,757 EPOCH 2103
2024-02-07 02:16:29,465 Epoch 2103: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 02:16:29,466 EPOCH 2104
2024-02-07 02:16:46,178 Epoch 2104: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 02:16:46,179 EPOCH 2105
2024-02-07 02:17:02,308 Epoch 2105: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 02:17:02,308 EPOCH 2106
2024-02-07 02:17:18,940 Epoch 2106: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 02:17:18,940 EPOCH 2107
2024-02-07 02:17:35,388 Epoch 2107: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 02:17:35,389 EPOCH 2108
2024-02-07 02:17:51,821 Epoch 2108: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 02:17:51,821 EPOCH 2109
2024-02-07 02:18:08,213 Epoch 2109: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 02:18:08,213 EPOCH 2110
2024-02-07 02:18:24,456 Epoch 2110: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 02:18:24,456 EPOCH 2111
2024-02-07 02:18:40,532 Epoch 2111: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 02:18:40,532 EPOCH 2112
2024-02-07 02:18:40,866 [Epoch: 2112 Step: 00019000] Batch Recognition Loss:   0.000322 => Gls Tokens per Sec:     3855 || Batch Translation Loss:   0.011406 => Txt Tokens per Sec:     8530 || Lr: 0.000100
2024-02-07 02:18:57,006 Epoch 2112: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 02:18:57,007 EPOCH 2113
2024-02-07 02:19:13,464 Epoch 2113: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 02:19:13,464 EPOCH 2114
2024-02-07 02:19:29,877 Epoch 2114: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 02:19:29,878 EPOCH 2115
2024-02-07 02:19:46,451 Epoch 2115: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 02:19:46,452 EPOCH 2116
2024-02-07 02:20:02,903 Epoch 2116: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 02:20:02,904 EPOCH 2117
2024-02-07 02:20:19,583 Epoch 2117: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-07 02:20:19,584 EPOCH 2118
2024-02-07 02:20:35,891 Epoch 2118: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-07 02:20:35,892 EPOCH 2119
2024-02-07 02:20:52,520 Epoch 2119: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-07 02:20:52,521 EPOCH 2120
2024-02-07 02:21:08,583 Epoch 2120: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.03 
2024-02-07 02:21:08,584 EPOCH 2121
2024-02-07 02:21:25,060 Epoch 2121: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-07 02:21:25,060 EPOCH 2122
2024-02-07 02:21:41,474 Epoch 2122: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-07 02:21:41,474 EPOCH 2123
2024-02-07 02:21:42,008 [Epoch: 2123 Step: 00019100] Batch Recognition Loss:   0.000748 => Gls Tokens per Sec:     4803 || Batch Translation Loss:   0.058418 => Txt Tokens per Sec:    10450 || Lr: 0.000100
2024-02-07 02:21:57,423 Epoch 2123: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-07 02:21:57,423 EPOCH 2124
2024-02-07 02:22:13,780 Epoch 2124: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-07 02:22:13,780 EPOCH 2125
2024-02-07 02:22:30,336 Epoch 2125: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-07 02:22:30,337 EPOCH 2126
2024-02-07 02:22:46,784 Epoch 2126: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-07 02:22:46,785 EPOCH 2127
2024-02-07 02:23:03,080 Epoch 2127: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-07 02:23:03,081 EPOCH 2128
2024-02-07 02:23:19,640 Epoch 2128: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-07 02:23:19,641 EPOCH 2129
2024-02-07 02:23:35,944 Epoch 2129: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 02:23:35,945 EPOCH 2130
2024-02-07 02:23:52,372 Epoch 2130: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 02:23:52,372 EPOCH 2131
2024-02-07 02:24:08,694 Epoch 2131: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 02:24:08,694 EPOCH 2132
2024-02-07 02:24:25,085 Epoch 2132: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 02:24:25,086 EPOCH 2133
2024-02-07 02:24:41,171 Epoch 2133: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 02:24:41,171 EPOCH 2134
2024-02-07 02:24:42,473 [Epoch: 2134 Step: 00019200] Batch Recognition Loss:   0.000332 => Gls Tokens per Sec:     2952 || Batch Translation Loss:   0.017871 => Txt Tokens per Sec:     7767 || Lr: 0.000100
2024-02-07 02:24:57,333 Epoch 2134: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 02:24:57,334 EPOCH 2135
2024-02-07 02:25:14,000 Epoch 2135: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 02:25:14,001 EPOCH 2136
2024-02-07 02:25:30,404 Epoch 2136: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 02:25:30,405 EPOCH 2137
2024-02-07 02:25:46,902 Epoch 2137: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 02:25:46,903 EPOCH 2138
2024-02-07 02:26:03,314 Epoch 2138: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 02:26:03,315 EPOCH 2139
2024-02-07 02:26:19,989 Epoch 2139: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 02:26:19,990 EPOCH 2140
2024-02-07 02:26:36,271 Epoch 2140: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 02:26:36,272 EPOCH 2141
2024-02-07 02:26:52,659 Epoch 2141: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 02:26:52,659 EPOCH 2142
2024-02-07 02:27:09,055 Epoch 2142: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 02:27:09,056 EPOCH 2143
2024-02-07 02:27:25,866 Epoch 2143: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 02:27:25,867 EPOCH 2144
2024-02-07 02:27:42,100 Epoch 2144: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 02:27:42,100 EPOCH 2145
2024-02-07 02:27:49,766 [Epoch: 2145 Step: 00019300] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:      668 || Batch Translation Loss:   0.012078 => Txt Tokens per Sec:     1851 || Lr: 0.000100
2024-02-07 02:27:59,040 Epoch 2145: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 02:27:59,041 EPOCH 2146
2024-02-07 02:28:15,401 Epoch 2146: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 02:28:15,402 EPOCH 2147
2024-02-07 02:28:31,798 Epoch 2147: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 02:28:31,799 EPOCH 2148
2024-02-07 02:28:47,991 Epoch 2148: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 02:28:47,992 EPOCH 2149
2024-02-07 02:29:04,435 Epoch 2149: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 02:29:04,436 EPOCH 2150
2024-02-07 02:29:20,721 Epoch 2150: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 02:29:20,722 EPOCH 2151
2024-02-07 02:29:37,239 Epoch 2151: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 02:29:37,240 EPOCH 2152
2024-02-07 02:29:53,652 Epoch 2152: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 02:29:53,653 EPOCH 2153
2024-02-07 02:30:10,319 Epoch 2153: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 02:30:10,320 EPOCH 2154
2024-02-07 02:30:26,548 Epoch 2154: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 02:30:26,549 EPOCH 2155
2024-02-07 02:30:43,134 Epoch 2155: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 02:30:43,135 EPOCH 2156
2024-02-07 02:30:57,591 [Epoch: 2156 Step: 00019400] Batch Recognition Loss:   0.000359 => Gls Tokens per Sec:      380 || Batch Translation Loss:   0.016701 => Txt Tokens per Sec:     1069 || Lr: 0.000100
2024-02-07 02:30:59,616 Epoch 2156: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 02:30:59,616 EPOCH 2157
2024-02-07 02:31:16,066 Epoch 2157: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 02:31:16,067 EPOCH 2158
2024-02-07 02:31:32,933 Epoch 2158: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 02:31:32,933 EPOCH 2159
2024-02-07 02:31:49,092 Epoch 2159: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 02:31:49,093 EPOCH 2160
2024-02-07 02:32:05,530 Epoch 2160: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 02:32:05,531 EPOCH 2161
2024-02-07 02:32:21,719 Epoch 2161: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 02:32:21,719 EPOCH 2162
2024-02-07 02:32:38,291 Epoch 2162: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 02:32:38,292 EPOCH 2163
2024-02-07 02:32:54,285 Epoch 2163: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 02:32:54,286 EPOCH 2164
2024-02-07 02:33:10,715 Epoch 2164: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 02:33:10,715 EPOCH 2165
2024-02-07 02:33:27,326 Epoch 2165: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 02:33:27,327 EPOCH 2166
2024-02-07 02:33:43,701 Epoch 2166: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 02:33:43,701 EPOCH 2167
2024-02-07 02:33:54,832 [Epoch: 2167 Step: 00019500] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:      690 || Batch Translation Loss:   0.011466 => Txt Tokens per Sec:     1994 || Lr: 0.000100
2024-02-07 02:33:59,771 Epoch 2167: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 02:33:59,772 EPOCH 2168
2024-02-07 02:34:16,128 Epoch 2168: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 02:34:16,129 EPOCH 2169
2024-02-07 02:34:32,763 Epoch 2169: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 02:34:32,764 EPOCH 2170
2024-02-07 02:34:49,217 Epoch 2170: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-07 02:34:49,218 EPOCH 2171
2024-02-07 02:35:05,546 Epoch 2171: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 02:35:05,547 EPOCH 2172
2024-02-07 02:35:22,018 Epoch 2172: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 02:35:22,019 EPOCH 2173
2024-02-07 02:35:38,381 Epoch 2173: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 02:35:38,382 EPOCH 2174
2024-02-07 02:35:54,821 Epoch 2174: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 02:35:54,822 EPOCH 2175
2024-02-07 02:36:11,049 Epoch 2175: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 02:36:11,050 EPOCH 2176
2024-02-07 02:36:27,545 Epoch 2176: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-07 02:36:27,545 EPOCH 2177
2024-02-07 02:36:44,274 Epoch 2177: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-07 02:36:44,275 EPOCH 2178
2024-02-07 02:36:59,937 [Epoch: 2178 Step: 00019600] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:      515 || Batch Translation Loss:   0.077397 => Txt Tokens per Sec:     1484 || Lr: 0.000100
2024-02-07 02:37:00,898 Epoch 2178: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-07 02:37:00,898 EPOCH 2179
2024-02-07 02:37:17,265 Epoch 2179: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-07 02:37:17,265 EPOCH 2180
2024-02-07 02:37:33,805 Epoch 2180: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-07 02:37:33,805 EPOCH 2181
2024-02-07 02:37:50,429 Epoch 2181: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-07 02:37:50,429 EPOCH 2182
2024-02-07 02:38:07,057 Epoch 2182: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-07 02:38:07,057 EPOCH 2183
2024-02-07 02:38:23,556 Epoch 2183: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-07 02:38:23,557 EPOCH 2184
2024-02-07 02:38:39,912 Epoch 2184: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-07 02:38:39,912 EPOCH 2185
2024-02-07 02:38:56,494 Epoch 2185: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-07 02:38:56,495 EPOCH 2186
2024-02-07 02:39:12,993 Epoch 2186: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-07 02:39:12,994 EPOCH 2187
2024-02-07 02:39:29,505 Epoch 2187: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-07 02:39:29,506 EPOCH 2188
2024-02-07 02:39:45,377 Epoch 2188: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-07 02:39:45,377 EPOCH 2189
2024-02-07 02:40:01,497 [Epoch: 2189 Step: 00019700] Batch Recognition Loss:   0.000694 => Gls Tokens per Sec:      579 || Batch Translation Loss:   0.085421 => Txt Tokens per Sec:     1647 || Lr: 0.000100
2024-02-07 02:40:01,830 Epoch 2189: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-07 02:40:01,830 EPOCH 2190
2024-02-07 02:40:18,132 Epoch 2190: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-07 02:40:18,133 EPOCH 2191
2024-02-07 02:40:34,208 Epoch 2191: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-07 02:40:34,208 EPOCH 2192
2024-02-07 02:40:50,848 Epoch 2192: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-07 02:40:50,849 EPOCH 2193
2024-02-07 02:41:06,928 Epoch 2193: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-07 02:41:06,928 EPOCH 2194
2024-02-07 02:41:23,422 Epoch 2194: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-07 02:41:23,422 EPOCH 2195
2024-02-07 02:41:39,735 Epoch 2195: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-07 02:41:39,735 EPOCH 2196
2024-02-07 02:41:56,208 Epoch 2196: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-07 02:41:56,208 EPOCH 2197
2024-02-07 02:42:12,988 Epoch 2197: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-07 02:42:12,989 EPOCH 2198
2024-02-07 02:42:29,321 Epoch 2198: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 02:42:29,322 EPOCH 2199
2024-02-07 02:42:45,659 Epoch 2199: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 02:42:45,659 EPOCH 2200
2024-02-07 02:43:02,273 [Epoch: 2200 Step: 00019800] Batch Recognition Loss:   0.000416 => Gls Tokens per Sec:      639 || Batch Translation Loss:   0.024636 => Txt Tokens per Sec:     1769 || Lr: 0.000100
2024-02-07 02:43:02,273 Epoch 2200: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 02:43:02,273 EPOCH 2201
2024-02-07 02:43:18,716 Epoch 2201: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 02:43:18,717 EPOCH 2202
2024-02-07 02:43:35,145 Epoch 2202: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 02:43:35,146 EPOCH 2203
2024-02-07 02:43:51,349 Epoch 2203: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 02:43:51,350 EPOCH 2204
2024-02-07 02:44:08,109 Epoch 2204: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-07 02:44:08,110 EPOCH 2205
2024-02-07 02:44:24,135 Epoch 2205: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 02:44:24,136 EPOCH 2206
2024-02-07 02:44:40,906 Epoch 2206: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-07 02:44:40,907 EPOCH 2207
2024-02-07 02:44:57,221 Epoch 2207: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-07 02:44:57,222 EPOCH 2208
2024-02-07 02:45:13,711 Epoch 2208: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-07 02:45:13,712 EPOCH 2209
2024-02-07 02:45:29,857 Epoch 2209: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-07 02:45:29,858 EPOCH 2210
2024-02-07 02:45:46,318 Epoch 2210: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-07 02:45:46,319 EPOCH 2211
2024-02-07 02:46:02,633 Epoch 2211: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-07 02:46:02,635 EPOCH 2212
2024-02-07 02:46:03,002 [Epoch: 2212 Step: 00019900] Batch Recognition Loss:   0.000494 => Gls Tokens per Sec:     3497 || Batch Translation Loss:   0.061284 => Txt Tokens per Sec:     8806 || Lr: 0.000100
2024-02-07 02:46:19,179 Epoch 2212: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-07 02:46:19,180 EPOCH 2213
2024-02-07 02:46:35,641 Epoch 2213: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-07 02:46:35,642 EPOCH 2214
2024-02-07 02:46:52,023 Epoch 2214: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-07 02:46:52,024 EPOCH 2215
2024-02-07 02:47:08,510 Epoch 2215: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-07 02:47:08,511 EPOCH 2216
2024-02-07 02:47:25,096 Epoch 2216: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-07 02:47:25,097 EPOCH 2217
2024-02-07 02:47:41,324 Epoch 2217: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-07 02:47:41,324 EPOCH 2218
2024-02-07 02:47:57,933 Epoch 2218: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-07 02:47:57,933 EPOCH 2219
2024-02-07 02:48:14,532 Epoch 2219: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-07 02:48:14,533 EPOCH 2220
2024-02-07 02:48:30,881 Epoch 2220: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-07 02:48:30,882 EPOCH 2221
2024-02-07 02:48:47,148 Epoch 2221: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-07 02:48:47,149 EPOCH 2222
2024-02-07 02:49:03,885 Epoch 2222: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-07 02:49:03,885 EPOCH 2223
2024-02-07 02:49:04,506 [Epoch: 2223 Step: 00020000] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:     4129 || Batch Translation Loss:   0.024072 => Txt Tokens per Sec:     8331 || Lr: 0.000100
2024-02-07 02:50:12,300 Validation result at epoch 2223, step    20000: duration: 67.7939s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.41454	Translation Loss: 94226.20312	PPL: 12226.07715
	Eval Metric: BLEU
	WER 3.88	(DEL: 0.00,	INS: 0.00,	SUB: 3.88)
	BLEU-4 0.63	(BLEU-1: 11.60,	BLEU-2: 3.91,	BLEU-3: 1.40,	BLEU-4: 0.63)
	CHRF 17.48	ROUGE 9.49
2024-02-07 02:50:12,302 Logging Recognition and Translation Outputs
2024-02-07 02:50:12,302 ========================================================================================================================
2024-02-07 02:50:12,302 Logging Sequence: 120_7.00
2024-02-07 02:50:12,302 	Gloss Reference :	A B+C+D+E
2024-02-07 02:50:12,304 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 02:50:12,304 	Gloss Alignment :	         
2024-02-07 02:50:12,305 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 02:50:12,305 	Text Reference  :	he *** had tested positive for  covid-19 on        may     19
2024-02-07 02:50:12,307 	Text Hypothesis :	he was in  the    first    time a        wrestling stadium in
2024-02-07 02:50:12,307 	Text Alignment  :	   I   S   S      S        S    S        S         S       S 
2024-02-07 02:50:12,307 ========================================================================================================================
2024-02-07 02:50:12,307 Logging Sequence: 148_186.00
2024-02-07 02:50:12,307 	Gloss Reference :	A B+C+D+E
2024-02-07 02:50:12,307 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 02:50:12,308 	Gloss Alignment :	         
2024-02-07 02:50:12,308 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 02:50:12,309 	Text Reference  :	siraj also took  four wickets in 1 over  thus becoming the record-holder for    most   wickets      in an  over  in        odis         
2024-02-07 02:50:12,309 	Text Hypothesis :	***** **** india had  won     in a world cup  trophy   in  2022          neeraj chopra participated in the world athletics championships
2024-02-07 02:50:12,309 	Text Alignment  :	D     D    S     S    S          S S     S    S        S   S             S      S      S               S   S     S         S            
2024-02-07 02:50:12,309 ========================================================================================================================
2024-02-07 02:50:12,309 Logging Sequence: 67_73.00
2024-02-07 02:50:12,311 	Gloss Reference :	A B+C+D+E
2024-02-07 02:50:12,311 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 02:50:12,311 	Gloss Alignment :	         
2024-02-07 02:50:12,311 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 02:50:12,312 	Text Reference  :	** in   his tweet he   also said   
2024-02-07 02:50:12,312 	Text Hypothesis :	we have won the   toss and  captain
2024-02-07 02:50:12,312 	Text Alignment  :	I  S    S   S     S    S    S      
2024-02-07 02:50:12,312 ========================================================================================================================
2024-02-07 02:50:12,312 Logging Sequence: 164_526.00
2024-02-07 02:50:12,312 	Gloss Reference :	A B+C+D+E
2024-02-07 02:50:12,312 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 02:50:12,312 	Gloss Alignment :	         
2024-02-07 02:50:12,312 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 02:50:12,312 	Text Reference  :	you are aware that viacom18 bought the broadcast rights of ** ipl   
2024-02-07 02:50:12,314 	Text Hypothesis :	*** *** ***** **** in       t20is  and different types  of in sports
2024-02-07 02:50:12,314 	Text Alignment  :	D   D   D     D    S        S      S   S         S         I  S     
2024-02-07 02:50:12,314 ========================================================================================================================
2024-02-07 02:50:12,314 Logging Sequence: 108_28.00
2024-02-07 02:50:12,314 	Gloss Reference :	A B+C+D+E
2024-02-07 02:50:12,314 	Gloss Hypothesis:	A B+C+D  
2024-02-07 02:50:12,314 	Gloss Alignment :	  S      
2024-02-07 02:50:12,315 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 02:50:12,316 	Text Reference  :	the 10 teams bought 204  players including 67  foreign players after spending      a   total of     rs    55170 crore
2024-02-07 02:50:12,316 	Text Hypothesis :	*** ** ***** ****** many of      you       may believe in      such  superstitions and khan  always share price tag  
2024-02-07 02:50:12,316 	Text Alignment  :	D   D  D     D      S    S       S         S   S       S       S     S             S   S     S      S     S     S    
2024-02-07 02:50:12,317 ========================================================================================================================
2024-02-07 02:50:29,240 Epoch 2223: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 02:50:29,241 EPOCH 2224
2024-02-07 02:50:45,814 Epoch 2224: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-07 02:50:45,814 EPOCH 2225
2024-02-07 02:51:02,446 Epoch 2225: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-07 02:51:02,447 EPOCH 2226
2024-02-07 02:51:18,640 Epoch 2226: Total Training Recognition Loss 0.01  Total Training Translation Loss 6.32 
2024-02-07 02:51:18,641 EPOCH 2227
2024-02-07 02:51:34,994 Epoch 2227: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.13 
2024-02-07 02:51:34,994 EPOCH 2228
2024-02-07 02:51:51,329 Epoch 2228: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.77 
2024-02-07 02:51:51,330 EPOCH 2229
2024-02-07 02:52:07,842 Epoch 2229: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.68 
2024-02-07 02:52:07,842 EPOCH 2230
2024-02-07 02:52:24,319 Epoch 2230: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.48 
2024-02-07 02:52:24,321 EPOCH 2231
2024-02-07 02:52:40,817 Epoch 2231: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.05 
2024-02-07 02:52:40,817 EPOCH 2232
2024-02-07 02:52:57,362 Epoch 2232: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.17 
2024-02-07 02:52:57,362 EPOCH 2233
2024-02-07 02:53:13,886 Epoch 2233: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-07 02:53:13,886 EPOCH 2234
2024-02-07 02:53:19,166 [Epoch: 2234 Step: 00020100] Batch Recognition Loss:   0.001203 => Gls Tokens per Sec:      557 || Batch Translation Loss:   0.054713 => Txt Tokens per Sec:     1533 || Lr: 0.000100
2024-02-07 02:53:30,310 Epoch 2234: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-07 02:53:30,310 EPOCH 2235
2024-02-07 02:53:46,619 Epoch 2235: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-07 02:53:46,619 EPOCH 2236
2024-02-07 02:54:02,817 Epoch 2236: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-07 02:54:02,817 EPOCH 2237
2024-02-07 02:54:19,483 Epoch 2237: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-07 02:54:19,484 EPOCH 2238
2024-02-07 02:54:35,924 Epoch 2238: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-07 02:54:35,925 EPOCH 2239
2024-02-07 02:54:51,875 Epoch 2239: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 02:54:51,875 EPOCH 2240
2024-02-07 02:55:08,230 Epoch 2240: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 02:55:08,230 EPOCH 2241
2024-02-07 02:55:24,673 Epoch 2241: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 02:55:24,673 EPOCH 2242
2024-02-07 02:55:41,111 Epoch 2242: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 02:55:41,112 EPOCH 2243
2024-02-07 02:55:57,630 Epoch 2243: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 02:55:57,631 EPOCH 2244
2024-02-07 02:56:14,231 Epoch 2244: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 02:56:14,232 EPOCH 2245
2024-02-07 02:56:21,725 [Epoch: 2245 Step: 00020200] Batch Recognition Loss:   0.000373 => Gls Tokens per Sec:      683 || Batch Translation Loss:   0.018198 => Txt Tokens per Sec:     1930 || Lr: 0.000100
2024-02-07 02:56:30,623 Epoch 2245: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 02:56:30,624 EPOCH 2246
2024-02-07 02:56:47,213 Epoch 2246: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 02:56:47,214 EPOCH 2247
2024-02-07 02:57:03,880 Epoch 2247: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 02:57:03,881 EPOCH 2248
2024-02-07 02:57:20,494 Epoch 2248: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 02:57:20,495 EPOCH 2249
2024-02-07 02:57:36,900 Epoch 2249: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 02:57:36,900 EPOCH 2250
2024-02-07 02:57:53,412 Epoch 2250: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 02:57:53,413 EPOCH 2251
2024-02-07 02:58:09,834 Epoch 2251: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 02:58:09,835 EPOCH 2252
2024-02-07 02:58:26,220 Epoch 2252: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 02:58:26,221 EPOCH 2253
2024-02-07 02:58:42,638 Epoch 2253: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 02:58:42,639 EPOCH 2254
2024-02-07 02:58:59,255 Epoch 2254: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 02:58:59,255 EPOCH 2255
2024-02-07 02:59:15,806 Epoch 2255: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 02:59:15,806 EPOCH 2256
2024-02-07 02:59:21,714 [Epoch: 2256 Step: 00020300] Batch Recognition Loss:   0.000280 => Gls Tokens per Sec:      931 || Batch Translation Loss:   0.015335 => Txt Tokens per Sec:     2349 || Lr: 0.000100
2024-02-07 02:59:32,116 Epoch 2256: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 02:59:32,116 EPOCH 2257
2024-02-07 02:59:48,546 Epoch 2257: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 02:59:48,547 EPOCH 2258
2024-02-07 03:00:04,865 Epoch 2258: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 03:00:04,865 EPOCH 2259
2024-02-07 03:00:21,341 Epoch 2259: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:00:21,342 EPOCH 2260
2024-02-07 03:00:37,719 Epoch 2260: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 03:00:37,719 EPOCH 2261
2024-02-07 03:00:54,139 Epoch 2261: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 03:00:54,140 EPOCH 2262
2024-02-07 03:01:10,400 Epoch 2262: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 03:01:10,401 EPOCH 2263
2024-02-07 03:01:26,549 Epoch 2263: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 03:01:26,550 EPOCH 2264
2024-02-07 03:01:43,072 Epoch 2264: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 03:01:43,072 EPOCH 2265
2024-02-07 03:01:59,485 Epoch 2265: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 03:01:59,486 EPOCH 2266
2024-02-07 03:02:15,844 Epoch 2266: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 03:02:15,845 EPOCH 2267
2024-02-07 03:02:24,995 [Epoch: 2267 Step: 00020400] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:      741 || Batch Translation Loss:   0.014736 => Txt Tokens per Sec:     2054 || Lr: 0.000100
2024-02-07 03:02:31,941 Epoch 2267: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 03:02:31,942 EPOCH 2268
2024-02-07 03:02:48,579 Epoch 2268: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:02:48,579 EPOCH 2269
2024-02-07 03:03:04,619 Epoch 2269: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 03:03:04,619 EPOCH 2270
2024-02-07 03:03:21,101 Epoch 2270: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 03:03:21,102 EPOCH 2271
2024-02-07 03:03:37,354 Epoch 2271: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:03:37,355 EPOCH 2272
2024-02-07 03:03:53,956 Epoch 2272: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:03:53,957 EPOCH 2273
2024-02-07 03:04:10,166 Epoch 2273: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 03:04:10,167 EPOCH 2274
2024-02-07 03:04:26,764 Epoch 2274: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:04:26,765 EPOCH 2275
2024-02-07 03:04:43,038 Epoch 2275: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 03:04:43,040 EPOCH 2276
2024-02-07 03:04:59,994 Epoch 2276: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 03:04:59,994 EPOCH 2277
2024-02-07 03:05:16,398 Epoch 2277: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 03:05:16,398 EPOCH 2278
2024-02-07 03:05:32,049 [Epoch: 2278 Step: 00020500] Batch Recognition Loss:   0.000293 => Gls Tokens per Sec:      515 || Batch Translation Loss:   0.010088 => Txt Tokens per Sec:     1416 || Lr: 0.000100
2024-02-07 03:05:33,068 Epoch 2278: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 03:05:33,068 EPOCH 2279
2024-02-07 03:05:49,322 Epoch 2279: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:05:49,323 EPOCH 2280
2024-02-07 03:06:05,825 Epoch 2280: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 03:06:05,826 EPOCH 2281
2024-02-07 03:06:22,435 Epoch 2281: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 03:06:22,436 EPOCH 2282
2024-02-07 03:06:39,156 Epoch 2282: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 03:06:39,156 EPOCH 2283
2024-02-07 03:06:55,617 Epoch 2283: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 03:06:55,618 EPOCH 2284
2024-02-07 03:07:11,932 Epoch 2284: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:07:11,933 EPOCH 2285
2024-02-07 03:07:28,273 Epoch 2285: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:07:28,273 EPOCH 2286
2024-02-07 03:07:44,560 Epoch 2286: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 03:07:44,561 EPOCH 2287
2024-02-07 03:08:01,161 Epoch 2287: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 03:08:01,162 EPOCH 2288
2024-02-07 03:08:17,447 Epoch 2288: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 03:08:17,447 EPOCH 2289
2024-02-07 03:08:33,384 [Epoch: 2289 Step: 00020600] Batch Recognition Loss:   0.000210 => Gls Tokens per Sec:      586 || Batch Translation Loss:   0.013814 => Txt Tokens per Sec:     1603 || Lr: 0.000100
2024-02-07 03:08:34,092 Epoch 2289: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 03:08:34,092 EPOCH 2290
2024-02-07 03:08:50,272 Epoch 2290: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 03:08:50,273 EPOCH 2291
2024-02-07 03:09:06,649 Epoch 2291: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 03:09:06,649 EPOCH 2292
2024-02-07 03:09:22,727 Epoch 2292: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 03:09:22,727 EPOCH 2293
2024-02-07 03:09:39,205 Epoch 2293: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:09:39,205 EPOCH 2294
2024-02-07 03:09:55,374 Epoch 2294: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 03:09:55,375 EPOCH 2295
2024-02-07 03:10:12,339 Epoch 2295: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 03:10:12,340 EPOCH 2296
2024-02-07 03:10:28,706 Epoch 2296: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:10:28,707 EPOCH 2297
2024-02-07 03:10:45,036 Epoch 2297: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:10:45,037 EPOCH 2298
2024-02-07 03:11:01,528 Epoch 2298: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:11:01,529 EPOCH 2299
2024-02-07 03:11:17,948 Epoch 2299: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 03:11:17,948 EPOCH 2300
2024-02-07 03:11:34,225 [Epoch: 2300 Step: 00020700] Batch Recognition Loss:   0.000681 => Gls Tokens per Sec:      653 || Batch Translation Loss:   0.019432 => Txt Tokens per Sec:     1805 || Lr: 0.000100
2024-02-07 03:11:34,226 Epoch 2300: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:11:34,226 EPOCH 2301
2024-02-07 03:11:50,908 Epoch 2301: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 03:11:50,909 EPOCH 2302
2024-02-07 03:12:07,307 Epoch 2302: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:12:07,307 EPOCH 2303
2024-02-07 03:12:23,667 Epoch 2303: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 03:12:23,667 EPOCH 2304
2024-02-07 03:12:39,934 Epoch 2304: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 03:12:39,935 EPOCH 2305
2024-02-07 03:12:56,263 Epoch 2305: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 03:12:56,264 EPOCH 2306
2024-02-07 03:13:12,511 Epoch 2306: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:13:12,511 EPOCH 2307
2024-02-07 03:13:28,931 Epoch 2307: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:13:28,931 EPOCH 2308
2024-02-07 03:13:45,273 Epoch 2308: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 03:13:45,273 EPOCH 2309
2024-02-07 03:14:01,591 Epoch 2309: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:14:01,592 EPOCH 2310
2024-02-07 03:14:18,088 Epoch 2310: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 03:14:18,089 EPOCH 2311
2024-02-07 03:14:34,375 Epoch 2311: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 03:14:34,376 EPOCH 2312
2024-02-07 03:14:38,789 [Epoch: 2312 Step: 00020800] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:       86 || Batch Translation Loss:   0.007009 => Txt Tokens per Sec:      308 || Lr: 0.000100
2024-02-07 03:14:50,879 Epoch 2312: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 03:14:50,879 EPOCH 2313
2024-02-07 03:15:07,522 Epoch 2313: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:15:07,523 EPOCH 2314
2024-02-07 03:15:24,204 Epoch 2314: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 03:15:24,204 EPOCH 2315
2024-02-07 03:15:40,697 Epoch 2315: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:15:40,698 EPOCH 2316
2024-02-07 03:15:57,441 Epoch 2316: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 03:15:57,441 EPOCH 2317
2024-02-07 03:16:14,170 Epoch 2317: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:16:14,170 EPOCH 2318
2024-02-07 03:16:30,722 Epoch 2318: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 03:16:30,723 EPOCH 2319
2024-02-07 03:16:47,063 Epoch 2319: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 03:16:47,064 EPOCH 2320
2024-02-07 03:17:03,170 Epoch 2320: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:17:03,171 EPOCH 2321
2024-02-07 03:17:19,475 Epoch 2321: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 03:17:19,475 EPOCH 2322
2024-02-07 03:17:35,729 Epoch 2322: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 03:17:35,730 EPOCH 2323
2024-02-07 03:17:39,278 [Epoch: 2323 Step: 00020900] Batch Recognition Loss:   0.000669 => Gls Tokens per Sec:      722 || Batch Translation Loss:   0.019728 => Txt Tokens per Sec:     2070 || Lr: 0.000100
2024-02-07 03:17:52,034 Epoch 2323: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 03:17:52,035 EPOCH 2324
2024-02-07 03:18:08,757 Epoch 2324: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 03:18:08,757 EPOCH 2325
2024-02-07 03:18:25,052 Epoch 2325: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 03:18:25,053 EPOCH 2326
2024-02-07 03:18:41,586 Epoch 2326: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 03:18:41,586 EPOCH 2327
2024-02-07 03:18:58,310 Epoch 2327: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 03:18:58,310 EPOCH 2328
2024-02-07 03:19:14,843 Epoch 2328: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 03:19:14,844 EPOCH 2329
2024-02-07 03:19:31,188 Epoch 2329: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 03:19:31,188 EPOCH 2330
2024-02-07 03:19:47,370 Epoch 2330: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 03:19:47,371 EPOCH 2331
2024-02-07 03:20:03,909 Epoch 2331: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-07 03:20:03,909 EPOCH 2332
2024-02-07 03:20:20,750 Epoch 2332: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 03:20:20,751 EPOCH 2333
2024-02-07 03:20:37,185 Epoch 2333: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 03:20:37,185 EPOCH 2334
2024-02-07 03:20:47,240 [Epoch: 2334 Step: 00021000] Batch Recognition Loss:   0.000346 => Gls Tokens per Sec:      382 || Batch Translation Loss:   0.022304 => Txt Tokens per Sec:     1216 || Lr: 0.000100
2024-02-07 03:20:54,194 Epoch 2334: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-07 03:20:54,195 EPOCH 2335
2024-02-07 03:21:10,919 Epoch 2335: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-07 03:21:10,920 EPOCH 2336
2024-02-07 03:21:27,355 Epoch 2336: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-07 03:21:27,356 EPOCH 2337
2024-02-07 03:21:44,081 Epoch 2337: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-07 03:21:44,082 EPOCH 2338
2024-02-07 03:22:00,611 Epoch 2338: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-07 03:22:00,612 EPOCH 2339
2024-02-07 03:22:17,269 Epoch 2339: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.92 
2024-02-07 03:22:17,269 EPOCH 2340
2024-02-07 03:22:33,544 Epoch 2340: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.19 
2024-02-07 03:22:33,545 EPOCH 2341
2024-02-07 03:22:49,942 Epoch 2341: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.32 
2024-02-07 03:22:49,944 EPOCH 2342
2024-02-07 03:23:06,826 Epoch 2342: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.89 
2024-02-07 03:23:06,826 EPOCH 2343
2024-02-07 03:23:23,266 Epoch 2343: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.95 
2024-02-07 03:23:23,267 EPOCH 2344
2024-02-07 03:23:39,845 Epoch 2344: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.42 
2024-02-07 03:23:39,846 EPOCH 2345
2024-02-07 03:23:50,054 [Epoch: 2345 Step: 00021100] Batch Recognition Loss:   0.002771 => Gls Tokens per Sec:      502 || Batch Translation Loss:   0.089628 => Txt Tokens per Sec:     1335 || Lr: 0.000100
2024-02-07 03:23:56,470 Epoch 2345: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-07 03:23:56,470 EPOCH 2346
2024-02-07 03:24:12,553 Epoch 2346: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-07 03:24:12,554 EPOCH 2347
2024-02-07 03:24:29,443 Epoch 2347: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-07 03:24:29,444 EPOCH 2348
2024-02-07 03:24:45,670 Epoch 2348: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-07 03:24:45,671 EPOCH 2349
2024-02-07 03:25:02,341 Epoch 2349: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-07 03:25:02,342 EPOCH 2350
2024-02-07 03:25:18,776 Epoch 2350: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-07 03:25:18,777 EPOCH 2351
2024-02-07 03:25:35,035 Epoch 2351: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-07 03:25:35,036 EPOCH 2352
2024-02-07 03:25:51,581 Epoch 2352: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-07 03:25:51,581 EPOCH 2353
2024-02-07 03:26:08,193 Epoch 2353: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-07 03:26:08,194 EPOCH 2354
2024-02-07 03:26:24,425 Epoch 2354: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-07 03:26:24,426 EPOCH 2355
2024-02-07 03:26:40,665 Epoch 2355: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-07 03:26:40,665 EPOCH 2356
2024-02-07 03:26:45,728 [Epoch: 2356 Step: 00021200] Batch Recognition Loss:   0.000427 => Gls Tokens per Sec:     1264 || Batch Translation Loss:   0.014762 => Txt Tokens per Sec:     3458 || Lr: 0.000100
2024-02-07 03:26:57,128 Epoch 2356: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-07 03:26:57,128 EPOCH 2357
2024-02-07 03:27:13,556 Epoch 2357: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-07 03:27:13,557 EPOCH 2358
2024-02-07 03:27:29,740 Epoch 2358: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 03:27:29,741 EPOCH 2359
2024-02-07 03:27:46,140 Epoch 2359: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 03:27:46,140 EPOCH 2360
2024-02-07 03:28:02,475 Epoch 2360: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 03:28:02,476 EPOCH 2361
2024-02-07 03:28:19,426 Epoch 2361: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 03:28:19,426 EPOCH 2362
2024-02-07 03:28:35,891 Epoch 2362: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 03:28:35,891 EPOCH 2363
2024-02-07 03:28:52,126 Epoch 2363: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 03:28:52,127 EPOCH 2364
2024-02-07 03:29:08,713 Epoch 2364: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 03:29:08,714 EPOCH 2365
2024-02-07 03:29:24,844 Epoch 2365: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 03:29:24,845 EPOCH 2366
2024-02-07 03:29:41,000 Epoch 2366: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 03:29:41,001 EPOCH 2367
2024-02-07 03:29:50,432 [Epoch: 2367 Step: 00021300] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:      719 || Batch Translation Loss:   0.015681 => Txt Tokens per Sec:     1940 || Lr: 0.000100
2024-02-07 03:29:57,520 Epoch 2367: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 03:29:57,521 EPOCH 2368
2024-02-07 03:30:14,009 Epoch 2368: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 03:30:14,010 EPOCH 2369
2024-02-07 03:30:30,447 Epoch 2369: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 03:30:30,447 EPOCH 2370
2024-02-07 03:30:46,939 Epoch 2370: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 03:30:46,940 EPOCH 2371
2024-02-07 03:31:03,423 Epoch 2371: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 03:31:03,424 EPOCH 2372
2024-02-07 03:31:20,067 Epoch 2372: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:31:20,067 EPOCH 2373
2024-02-07 03:31:36,462 Epoch 2373: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 03:31:36,463 EPOCH 2374
2024-02-07 03:31:52,796 Epoch 2374: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:31:52,797 EPOCH 2375
2024-02-07 03:32:09,144 Epoch 2375: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 03:32:09,144 EPOCH 2376
2024-02-07 03:32:25,588 Epoch 2376: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:32:25,589 EPOCH 2377
2024-02-07 03:32:42,264 Epoch 2377: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:32:42,265 EPOCH 2378
2024-02-07 03:32:47,877 [Epoch: 2378 Step: 00021400] Batch Recognition Loss:   0.000327 => Gls Tokens per Sec:     1597 || Batch Translation Loss:   0.017928 => Txt Tokens per Sec:     4207 || Lr: 0.000100
2024-02-07 03:32:58,580 Epoch 2378: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 03:32:58,581 EPOCH 2379
2024-02-07 03:33:15,419 Epoch 2379: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 03:33:15,419 EPOCH 2380
2024-02-07 03:33:31,663 Epoch 2380: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 03:33:31,663 EPOCH 2381
2024-02-07 03:33:47,834 Epoch 2381: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 03:33:47,834 EPOCH 2382
2024-02-07 03:34:04,711 Epoch 2382: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 03:34:04,713 EPOCH 2383
2024-02-07 03:34:21,466 Epoch 2383: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 03:34:21,466 EPOCH 2384
2024-02-07 03:34:37,915 Epoch 2384: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 03:34:37,916 EPOCH 2385
2024-02-07 03:34:53,922 Epoch 2385: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 03:34:53,923 EPOCH 2386
2024-02-07 03:35:10,453 Epoch 2386: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 03:35:10,453 EPOCH 2387
2024-02-07 03:35:26,483 Epoch 2387: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 03:35:26,484 EPOCH 2388
2024-02-07 03:35:43,191 Epoch 2388: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:35:43,192 EPOCH 2389
2024-02-07 03:35:59,132 [Epoch: 2389 Step: 00021500] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:      586 || Batch Translation Loss:   0.015578 => Txt Tokens per Sec:     1611 || Lr: 0.000100
2024-02-07 03:35:59,662 Epoch 2389: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 03:35:59,663 EPOCH 2390
2024-02-07 03:36:16,074 Epoch 2390: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:36:16,074 EPOCH 2391
2024-02-07 03:36:32,662 Epoch 2391: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:36:32,662 EPOCH 2392
2024-02-07 03:36:48,950 Epoch 2392: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 03:36:48,951 EPOCH 2393
2024-02-07 03:37:05,441 Epoch 2393: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 03:37:05,442 EPOCH 2394
2024-02-07 03:37:21,801 Epoch 2394: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 03:37:21,801 EPOCH 2395
2024-02-07 03:37:37,859 Epoch 2395: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 03:37:37,860 EPOCH 2396
2024-02-07 03:37:54,473 Epoch 2396: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 03:37:54,473 EPOCH 2397
2024-02-07 03:38:10,929 Epoch 2397: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-07 03:38:10,929 EPOCH 2398
2024-02-07 03:38:27,139 Epoch 2398: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-07 03:38:27,140 EPOCH 2399
2024-02-07 03:38:43,633 Epoch 2399: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-07 03:38:43,633 EPOCH 2400
2024-02-07 03:38:59,881 [Epoch: 2400 Step: 00021600] Batch Recognition Loss:   0.000231 => Gls Tokens per Sec:      654 || Batch Translation Loss:   0.040382 => Txt Tokens per Sec:     1809 || Lr: 0.000100
2024-02-07 03:38:59,881 Epoch 2400: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-07 03:38:59,881 EPOCH 2401
2024-02-07 03:39:16,508 Epoch 2401: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-07 03:39:16,508 EPOCH 2402
2024-02-07 03:39:32,853 Epoch 2402: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 03:39:32,854 EPOCH 2403
2024-02-07 03:39:49,383 Epoch 2403: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 03:39:49,383 EPOCH 2404
2024-02-07 03:40:05,898 Epoch 2404: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 03:40:05,899 EPOCH 2405
2024-02-07 03:40:22,555 Epoch 2405: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-07 03:40:22,556 EPOCH 2406
2024-02-07 03:40:39,144 Epoch 2406: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-07 03:40:39,145 EPOCH 2407
2024-02-07 03:40:55,654 Epoch 2407: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-07 03:40:55,655 EPOCH 2408
2024-02-07 03:41:12,010 Epoch 2408: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-07 03:41:12,010 EPOCH 2409
2024-02-07 03:41:28,584 Epoch 2409: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-07 03:41:28,585 EPOCH 2410
2024-02-07 03:41:44,994 Epoch 2410: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-07 03:41:44,995 EPOCH 2411
2024-02-07 03:42:01,416 Epoch 2411: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-07 03:42:01,417 EPOCH 2412
2024-02-07 03:42:01,914 [Epoch: 2412 Step: 00021700] Batch Recognition Loss:   0.000485 => Gls Tokens per Sec:     2581 || Batch Translation Loss:   0.072205 => Txt Tokens per Sec:     7536 || Lr: 0.000100
2024-02-07 03:42:17,750 Epoch 2412: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.55 
2024-02-07 03:42:17,750 EPOCH 2413
2024-02-07 03:42:33,979 Epoch 2413: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.16 
2024-02-07 03:42:33,980 EPOCH 2414
2024-02-07 03:42:50,535 Epoch 2414: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.05 
2024-02-07 03:42:50,536 EPOCH 2415
2024-02-07 03:43:06,929 Epoch 2415: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.19 
2024-02-07 03:43:06,930 EPOCH 2416
2024-02-07 03:43:23,707 Epoch 2416: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.48 
2024-02-07 03:43:23,707 EPOCH 2417
2024-02-07 03:43:40,274 Epoch 2417: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.57 
2024-02-07 03:43:40,274 EPOCH 2418
2024-02-07 03:43:57,002 Epoch 2418: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.06 
2024-02-07 03:43:57,003 EPOCH 2419
2024-02-07 03:44:13,510 Epoch 2419: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-07 03:44:13,511 EPOCH 2420
2024-02-07 03:44:30,068 Epoch 2420: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-07 03:44:30,069 EPOCH 2421
2024-02-07 03:44:46,494 Epoch 2421: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-07 03:44:46,495 EPOCH 2422
2024-02-07 03:45:02,785 Epoch 2422: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-07 03:45:02,785 EPOCH 2423
2024-02-07 03:45:04,150 [Epoch: 2423 Step: 00021800] Batch Recognition Loss:   0.000889 => Gls Tokens per Sec:     1878 || Batch Translation Loss:   0.037368 => Txt Tokens per Sec:     5562 || Lr: 0.000100
2024-02-07 03:45:19,187 Epoch 2423: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-07 03:45:19,187 EPOCH 2424
2024-02-07 03:45:35,654 Epoch 2424: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-07 03:45:35,654 EPOCH 2425
2024-02-07 03:45:51,914 Epoch 2425: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-07 03:45:51,914 EPOCH 2426
2024-02-07 03:46:08,269 Epoch 2426: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-07 03:46:08,270 EPOCH 2427
2024-02-07 03:46:24,694 Epoch 2427: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 03:46:24,695 EPOCH 2428
2024-02-07 03:46:41,118 Epoch 2428: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 03:46:41,118 EPOCH 2429
2024-02-07 03:46:57,779 Epoch 2429: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 03:46:57,779 EPOCH 2430
2024-02-07 03:47:14,089 Epoch 2430: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 03:47:14,089 EPOCH 2431
2024-02-07 03:47:30,406 Epoch 2431: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 03:47:30,407 EPOCH 2432
2024-02-07 03:47:46,904 Epoch 2432: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 03:47:46,905 EPOCH 2433
2024-02-07 03:48:03,185 Epoch 2433: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 03:48:03,186 EPOCH 2434
2024-02-07 03:48:08,473 [Epoch: 2434 Step: 00021900] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:      556 || Batch Translation Loss:   0.008770 => Txt Tokens per Sec:     1544 || Lr: 0.000100
2024-02-07 03:48:20,013 Epoch 2434: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 03:48:20,014 EPOCH 2435
2024-02-07 03:48:36,499 Epoch 2435: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 03:48:36,500 EPOCH 2436
2024-02-07 03:48:52,904 Epoch 2436: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 03:48:52,904 EPOCH 2437
2024-02-07 03:49:09,407 Epoch 2437: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 03:49:09,408 EPOCH 2438
2024-02-07 03:49:25,854 Epoch 2438: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 03:49:25,855 EPOCH 2439
2024-02-07 03:49:42,154 Epoch 2439: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 03:49:42,155 EPOCH 2440
2024-02-07 03:49:58,579 Epoch 2440: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 03:49:58,579 EPOCH 2441
2024-02-07 03:50:15,263 Epoch 2441: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 03:50:15,264 EPOCH 2442
2024-02-07 03:50:31,650 Epoch 2442: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 03:50:31,650 EPOCH 2443
2024-02-07 03:50:47,657 Epoch 2443: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 03:50:47,658 EPOCH 2444
2024-02-07 03:51:03,961 Epoch 2444: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 03:51:03,961 EPOCH 2445
2024-02-07 03:51:06,023 [Epoch: 2445 Step: 00022000] Batch Recognition Loss:   0.000565 => Gls Tokens per Sec:     2484 || Batch Translation Loss:   0.017860 => Txt Tokens per Sec:     6631 || Lr: 0.000100
2024-02-07 03:52:13,759 Hooray! New best validation result [eval_metric]!
2024-02-07 03:52:13,761 Saving new checkpoint.
2024-02-07 03:52:14,092 Validation result at epoch 2445, step    22000: duration: 68.0696s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.44279	Translation Loss: 93292.56250	PPL: 11137.51270
	Eval Metric: BLEU
	WER 3.88	(DEL: 0.00,	INS: 0.00,	SUB: 3.88)
	BLEU-4 0.86	(BLEU-1: 11.24,	BLEU-2: 3.76,	BLEU-3: 1.63,	BLEU-4: 0.86)
	CHRF 17.26	ROUGE 9.55
2024-02-07 03:52:14,093 Logging Recognition and Translation Outputs
2024-02-07 03:52:14,094 ========================================================================================================================
2024-02-07 03:52:14,094 Logging Sequence: 179_2.00
2024-02-07 03:52:14,094 	Gloss Reference :	A B+C+D+E
2024-02-07 03:52:14,094 	Gloss Hypothesis:	A B+C+D  
2024-02-07 03:52:14,095 	Gloss Alignment :	  S      
2024-02-07 03:52:14,095 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 03:52:14,095 	Text Reference  :	vinesh phogat is  a    well known wrestler
2024-02-07 03:52:14,096 	Text Hypothesis :	****** the    duo were very sorry you     
2024-02-07 03:52:14,096 	Text Alignment  :	D      S      S   S    S    S     S       
2024-02-07 03:52:14,096 ========================================================================================================================
2024-02-07 03:52:14,096 Logging Sequence: 55_124.00
2024-02-07 03:52:14,096 	Gloss Reference :	A B+C+D+E
2024-02-07 03:52:14,097 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 03:52:14,097 	Gloss Alignment :	         
2024-02-07 03:52:14,097 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 03:52:14,098 	Text Reference  :	next to  him with    the        patel jersey was ajaz  patel
2024-02-07 03:52:14,098 	Text Hypothesis :	it   was a   strange experience on    the    4th march 2023 
2024-02-07 03:52:14,098 	Text Alignment  :	S    S   S   S       S          S     S      S   S     S    
2024-02-07 03:52:14,098 ========================================================================================================================
2024-02-07 03:52:14,099 Logging Sequence: 148_105.00
2024-02-07 03:52:14,099 	Gloss Reference :	A B+C+D+E
2024-02-07 03:52:14,099 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 03:52:14,099 	Gloss Alignment :	         
2024-02-07 03:52:14,099 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 03:52:14,101 	Text Reference  :	later with amazing bowling   by     hardik pandya and kuldeep yadav sri lanka were all out  in just 50  runs   
2024-02-07 03:52:14,101 	Text Hypothesis :	***** even india'  legendary scored 63     runs   off 31      balls had also  took her face in **** 321 innings
2024-02-07 03:52:14,101 	Text Alignment  :	D     S    S       S         S      S      S      S   S       S     S   S     S    S   S       D    S   S      
2024-02-07 03:52:14,102 ========================================================================================================================
2024-02-07 03:52:14,102 Logging Sequence: 125_165.00
2024-02-07 03:52:14,102 	Gloss Reference :	A B+C+D+E
2024-02-07 03:52:14,102 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 03:52:14,102 	Gloss Alignment :	         
2024-02-07 03:52:14,102 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 03:52:14,103 	Text Reference  :	please do not target nadeem we speak to  each other and        share a     good bond    
2024-02-07 03:52:14,104 	Text Hypothesis :	****** ** *** ****** ****** ** while one were many  youngsters in    india who  practise
2024-02-07 03:52:14,104 	Text Alignment  :	D      D  D   D      D      D  S     S   S    S     S          S     S     S    S       
2024-02-07 03:52:14,104 ========================================================================================================================
2024-02-07 03:52:14,104 Logging Sequence: 77_52.00
2024-02-07 03:52:14,104 	Gloss Reference :	A B+C+D+E
2024-02-07 03:52:14,104 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 03:52:14,104 	Gloss Alignment :	         
2024-02-07 03:52:14,105 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 03:52:14,106 	Text Reference  :	kane williamson held down the  fort for hyderabad by   scoring   66  runs    and  ended the match  in a tie   
2024-02-07 03:52:14,107 	Text Hypothesis :	**** ********** the  8    runs were an  8         days wonderful but england lost to    an  option of 6 medals
2024-02-07 03:52:14,107 	Text Alignment  :	D    D          S    S    S    S    S   S         S    S         S   S       S    S     S   S      S  S S     
2024-02-07 03:52:14,107 ========================================================================================================================
2024-02-07 03:52:29,286 Epoch 2445: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 03:52:29,286 EPOCH 2446
2024-02-07 03:52:46,104 Epoch 2446: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 03:52:46,105 EPOCH 2447
2024-02-07 03:53:02,395 Epoch 2447: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 03:53:02,396 EPOCH 2448
2024-02-07 03:53:18,616 Epoch 2448: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 03:53:18,617 EPOCH 2449
2024-02-07 03:53:35,119 Epoch 2449: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 03:53:35,119 EPOCH 2450
2024-02-07 03:53:51,565 Epoch 2450: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 03:53:51,565 EPOCH 2451
2024-02-07 03:54:07,826 Epoch 2451: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 03:54:07,827 EPOCH 2452
2024-02-07 03:54:24,183 Epoch 2452: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:54:24,183 EPOCH 2453
2024-02-07 03:54:40,692 Epoch 2453: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 03:54:40,692 EPOCH 2454
2024-02-07 03:54:57,227 Epoch 2454: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 03:54:57,227 EPOCH 2455
2024-02-07 03:55:13,348 Epoch 2455: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 03:55:13,349 EPOCH 2456
2024-02-07 03:55:19,655 [Epoch: 2456 Step: 00022100] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:      872 || Batch Translation Loss:   0.008049 => Txt Tokens per Sec:     2332 || Lr: 0.000100
2024-02-07 03:55:29,798 Epoch 2456: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 03:55:29,798 EPOCH 2457
2024-02-07 03:55:46,249 Epoch 2457: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 03:55:46,250 EPOCH 2458
2024-02-07 03:56:03,125 Epoch 2458: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 03:56:03,125 EPOCH 2459
2024-02-07 03:56:19,668 Epoch 2459: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:56:19,669 EPOCH 2460
2024-02-07 03:56:36,361 Epoch 2460: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 03:56:36,362 EPOCH 2461
2024-02-07 03:56:52,636 Epoch 2461: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:56:52,637 EPOCH 2462
2024-02-07 03:57:08,924 Epoch 2462: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 03:57:08,925 EPOCH 2463
2024-02-07 03:57:25,772 Epoch 2463: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 03:57:25,772 EPOCH 2464
2024-02-07 03:57:42,183 Epoch 2464: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 03:57:42,183 EPOCH 2465
2024-02-07 03:57:58,954 Epoch 2465: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:57:58,954 EPOCH 2466
2024-02-07 03:58:15,709 Epoch 2466: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 03:58:15,710 EPOCH 2467
2024-02-07 03:58:28,233 [Epoch: 2467 Step: 00022200] Batch Recognition Loss:   0.000297 => Gls Tokens per Sec:      541 || Batch Translation Loss:   0.022292 => Txt Tokens per Sec:     1555 || Lr: 0.000100
2024-02-07 03:58:32,245 Epoch 2467: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 03:58:32,245 EPOCH 2468
2024-02-07 03:58:48,763 Epoch 2468: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:58:48,764 EPOCH 2469
2024-02-07 03:59:04,641 Epoch 2469: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 03:59:04,642 EPOCH 2470
2024-02-07 03:59:20,966 Epoch 2470: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 03:59:20,966 EPOCH 2471
2024-02-07 03:59:37,468 Epoch 2471: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 03:59:37,468 EPOCH 2472
2024-02-07 03:59:54,383 Epoch 2472: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 03:59:54,384 EPOCH 2473
2024-02-07 04:00:10,748 Epoch 2473: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 04:00:10,749 EPOCH 2474
2024-02-07 04:00:27,347 Epoch 2474: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 04:00:27,348 EPOCH 2475
2024-02-07 04:00:43,761 Epoch 2475: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 04:00:43,762 EPOCH 2476
2024-02-07 04:01:00,219 Epoch 2476: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 04:01:00,219 EPOCH 2477
2024-02-07 04:01:16,652 Epoch 2477: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 04:01:16,652 EPOCH 2478
2024-02-07 04:01:28,312 [Epoch: 2478 Step: 00022300] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:      769 || Batch Translation Loss:   0.019906 => Txt Tokens per Sec:     2122 || Lr: 0.000100
2024-02-07 04:01:33,015 Epoch 2478: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 04:01:33,016 EPOCH 2479
2024-02-07 04:01:49,306 Epoch 2479: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 04:01:49,307 EPOCH 2480
2024-02-07 04:02:05,920 Epoch 2480: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 04:02:05,921 EPOCH 2481
2024-02-07 04:02:22,688 Epoch 2481: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 04:02:22,689 EPOCH 2482
2024-02-07 04:02:39,033 Epoch 2482: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 04:02:39,034 EPOCH 2483
2024-02-07 04:02:55,294 Epoch 2483: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 04:02:55,295 EPOCH 2484
2024-02-07 04:03:11,962 Epoch 2484: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 04:03:11,963 EPOCH 2485
2024-02-07 04:03:28,412 Epoch 2485: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 04:03:28,412 EPOCH 2486
2024-02-07 04:03:44,663 Epoch 2486: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 04:03:44,664 EPOCH 2487
2024-02-07 04:04:01,590 Epoch 2487: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 04:04:01,590 EPOCH 2488
2024-02-07 04:04:18,305 Epoch 2488: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 04:04:18,306 EPOCH 2489
2024-02-07 04:04:28,544 [Epoch: 2489 Step: 00022400] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:      912 || Batch Translation Loss:   0.012718 => Txt Tokens per Sec:     2438 || Lr: 0.000100
2024-02-07 04:04:34,657 Epoch 2489: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 04:04:34,658 EPOCH 2490
2024-02-07 04:04:51,509 Epoch 2490: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 04:04:51,511 EPOCH 2491
2024-02-07 04:05:08,227 Epoch 2491: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 04:05:08,228 EPOCH 2492
2024-02-07 04:05:24,303 Epoch 2492: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 04:05:24,304 EPOCH 2493
2024-02-07 04:05:40,920 Epoch 2493: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 04:05:40,921 EPOCH 2494
2024-02-07 04:05:57,285 Epoch 2494: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 04:05:57,285 EPOCH 2495
2024-02-07 04:06:14,032 Epoch 2495: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 04:06:14,033 EPOCH 2496
2024-02-07 04:06:30,269 Epoch 2496: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 04:06:30,270 EPOCH 2497
2024-02-07 04:06:46,614 Epoch 2497: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 04:06:46,615 EPOCH 2498
2024-02-07 04:07:02,975 Epoch 2498: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 04:07:02,975 EPOCH 2499
2024-02-07 04:07:19,637 Epoch 2499: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 04:07:19,638 EPOCH 2500
2024-02-07 04:07:36,083 [Epoch: 2500 Step: 00022500] Batch Recognition Loss:   0.000263 => Gls Tokens per Sec:      646 || Batch Translation Loss:   0.016113 => Txt Tokens per Sec:     1787 || Lr: 0.000100
2024-02-07 04:07:36,084 Epoch 2500: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 04:07:36,084 EPOCH 2501
2024-02-07 04:07:52,937 Epoch 2501: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 04:07:52,937 EPOCH 2502
2024-02-07 04:08:09,233 Epoch 2502: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 04:08:09,233 EPOCH 2503
2024-02-07 04:08:25,461 Epoch 2503: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 04:08:25,461 EPOCH 2504
2024-02-07 04:08:42,274 Epoch 2504: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 04:08:42,276 EPOCH 2505
2024-02-07 04:08:58,531 Epoch 2505: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 04:08:58,531 EPOCH 2506
2024-02-07 04:09:15,094 Epoch 2506: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 04:09:15,095 EPOCH 2507
2024-02-07 04:09:31,620 Epoch 2507: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 04:09:31,621 EPOCH 2508
2024-02-07 04:09:47,916 Epoch 2508: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-07 04:09:47,916 EPOCH 2509
2024-02-07 04:10:04,619 Epoch 2509: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-07 04:10:04,619 EPOCH 2510
2024-02-07 04:10:21,123 Epoch 2510: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-07 04:10:21,124 EPOCH 2511
2024-02-07 04:10:37,845 Epoch 2511: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-07 04:10:37,846 EPOCH 2512
2024-02-07 04:10:38,173 [Epoch: 2512 Step: 00022600] Batch Recognition Loss:   0.000414 => Gls Tokens per Sec:     3938 || Batch Translation Loss:   0.038699 => Txt Tokens per Sec:     8766 || Lr: 0.000100
2024-02-07 04:10:54,319 Epoch 2512: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-07 04:10:54,319 EPOCH 2513
2024-02-07 04:11:10,737 Epoch 2513: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-07 04:11:10,737 EPOCH 2514
2024-02-07 04:11:27,098 Epoch 2514: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-07 04:11:27,099 EPOCH 2515
2024-02-07 04:11:43,634 Epoch 2515: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-07 04:11:43,635 EPOCH 2516
2024-02-07 04:11:59,891 Epoch 2516: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-07 04:11:59,891 EPOCH 2517
2024-02-07 04:12:16,452 Epoch 2517: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-07 04:12:16,453 EPOCH 2518
2024-02-07 04:12:32,979 Epoch 2518: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-07 04:12:32,979 EPOCH 2519
2024-02-07 04:12:49,360 Epoch 2519: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-07 04:12:49,361 EPOCH 2520
2024-02-07 04:13:05,873 Epoch 2520: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-07 04:13:05,874 EPOCH 2521
2024-02-07 04:13:22,628 Epoch 2521: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-07 04:13:22,629 EPOCH 2522
2024-02-07 04:13:38,668 Epoch 2522: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-07 04:13:38,669 EPOCH 2523
2024-02-07 04:13:43,734 [Epoch: 2523 Step: 00022700] Batch Recognition Loss:   0.001149 => Gls Tokens per Sec:      328 || Batch Translation Loss:   0.014864 => Txt Tokens per Sec:      960 || Lr: 0.000100
2024-02-07 04:13:55,535 Epoch 2523: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-07 04:13:55,536 EPOCH 2524
2024-02-07 04:14:12,080 Epoch 2524: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-07 04:14:12,081 EPOCH 2525
2024-02-07 04:14:28,341 Epoch 2525: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-07 04:14:28,341 EPOCH 2526
2024-02-07 04:14:45,298 Epoch 2526: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-07 04:14:45,298 EPOCH 2527
2024-02-07 04:15:01,412 Epoch 2527: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-07 04:15:01,413 EPOCH 2528
2024-02-07 04:15:18,039 Epoch 2528: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-07 04:15:18,039 EPOCH 2529
2024-02-07 04:15:34,594 Epoch 2529: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 04:15:34,595 EPOCH 2530
2024-02-07 04:15:50,846 Epoch 2530: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 04:15:50,846 EPOCH 2531
2024-02-07 04:16:07,393 Epoch 2531: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 04:16:07,394 EPOCH 2532
2024-02-07 04:16:23,682 Epoch 2532: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 04:16:23,683 EPOCH 2533
2024-02-07 04:16:40,253 Epoch 2533: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-07 04:16:40,254 EPOCH 2534
2024-02-07 04:16:47,236 [Epoch: 2534 Step: 00022800] Batch Recognition Loss:   0.000273 => Gls Tokens per Sec:      550 || Batch Translation Loss:   0.027599 => Txt Tokens per Sec:     1568 || Lr: 0.000100
2024-02-07 04:16:56,608 Epoch 2534: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-07 04:16:56,609 EPOCH 2535
2024-02-07 04:17:12,899 Epoch 2535: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-07 04:17:12,900 EPOCH 2536
2024-02-07 04:17:29,507 Epoch 2536: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 04:17:29,508 EPOCH 2537
2024-02-07 04:17:46,014 Epoch 2537: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 04:17:46,015 EPOCH 2538
2024-02-07 04:18:02,843 Epoch 2538: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 04:18:02,844 EPOCH 2539
2024-02-07 04:18:19,428 Epoch 2539: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 04:18:19,429 EPOCH 2540
2024-02-07 04:18:35,735 Epoch 2540: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 04:18:35,736 EPOCH 2541
2024-02-07 04:18:52,048 Epoch 2541: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-07 04:18:52,048 EPOCH 2542
2024-02-07 04:19:08,528 Epoch 2542: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 04:19:08,530 EPOCH 2543
2024-02-07 04:19:24,762 Epoch 2543: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 04:19:24,763 EPOCH 2544
2024-02-07 04:19:41,141 Epoch 2544: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-07 04:19:41,142 EPOCH 2545
2024-02-07 04:19:55,310 [Epoch: 2545 Step: 00022900] Batch Recognition Loss:   0.003835 => Gls Tokens per Sec:      298 || Batch Translation Loss:   0.027333 => Txt Tokens per Sec:      923 || Lr: 0.000100
2024-02-07 04:19:57,558 Epoch 2545: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-07 04:19:57,559 EPOCH 2546
2024-02-07 04:20:13,742 Epoch 2546: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-07 04:20:13,743 EPOCH 2547
2024-02-07 04:20:30,381 Epoch 2547: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-07 04:20:30,381 EPOCH 2548
2024-02-07 04:20:46,753 Epoch 2548: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-07 04:20:46,753 EPOCH 2549
2024-02-07 04:21:03,193 Epoch 2549: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-07 04:21:03,194 EPOCH 2550
2024-02-07 04:21:19,437 Epoch 2550: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-07 04:21:19,438 EPOCH 2551
2024-02-07 04:21:36,276 Epoch 2551: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-07 04:21:36,277 EPOCH 2552
2024-02-07 04:21:52,808 Epoch 2552: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-07 04:21:52,808 EPOCH 2553
2024-02-07 04:22:09,109 Epoch 2553: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-07 04:22:09,110 EPOCH 2554
2024-02-07 04:22:25,850 Epoch 2554: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-07 04:22:25,851 EPOCH 2555
2024-02-07 04:22:42,287 Epoch 2555: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-07 04:22:42,287 EPOCH 2556
2024-02-07 04:22:48,578 [Epoch: 2556 Step: 00023000] Batch Recognition Loss:   0.000397 => Gls Tokens per Sec:      874 || Batch Translation Loss:   0.087294 => Txt Tokens per Sec:     2499 || Lr: 0.000100
2024-02-07 04:22:58,703 Epoch 2556: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-07 04:22:58,704 EPOCH 2557
2024-02-07 04:23:15,165 Epoch 2557: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.57 
2024-02-07 04:23:15,166 EPOCH 2558
2024-02-07 04:23:31,358 Epoch 2558: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.84 
2024-02-07 04:23:31,358 EPOCH 2559
2024-02-07 04:23:48,109 Epoch 2559: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.47 
2024-02-07 04:23:48,109 EPOCH 2560
2024-02-07 04:24:04,390 Epoch 2560: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.94 
2024-02-07 04:24:04,390 EPOCH 2561
2024-02-07 04:24:20,862 Epoch 2561: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.36 
2024-02-07 04:24:20,863 EPOCH 2562
2024-02-07 04:24:37,222 Epoch 2562: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.06 
2024-02-07 04:24:37,223 EPOCH 2563
2024-02-07 04:24:53,533 Epoch 2563: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.71 
2024-02-07 04:24:53,533 EPOCH 2564
2024-02-07 04:25:09,880 Epoch 2564: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.96 
2024-02-07 04:25:09,881 EPOCH 2565
2024-02-07 04:25:26,377 Epoch 2565: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.71 
2024-02-07 04:25:26,378 EPOCH 2566
2024-02-07 04:25:42,957 Epoch 2566: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.41 
2024-02-07 04:25:42,958 EPOCH 2567
2024-02-07 04:25:51,517 [Epoch: 2567 Step: 00023100] Batch Recognition Loss:   0.000724 => Gls Tokens per Sec:      897 || Batch Translation Loss:   0.033321 => Txt Tokens per Sec:     2465 || Lr: 0.000100
2024-02-07 04:25:59,430 Epoch 2567: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-07 04:25:59,431 EPOCH 2568
2024-02-07 04:26:16,036 Epoch 2568: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-07 04:26:16,037 EPOCH 2569
2024-02-07 04:26:32,583 Epoch 2569: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-07 04:26:32,584 EPOCH 2570
2024-02-07 04:26:48,915 Epoch 2570: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-07 04:26:48,915 EPOCH 2571
2024-02-07 04:27:05,356 Epoch 2571: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-07 04:27:05,358 EPOCH 2572
2024-02-07 04:27:21,784 Epoch 2572: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-07 04:27:21,785 EPOCH 2573
2024-02-07 04:27:38,837 Epoch 2573: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-07 04:27:38,838 EPOCH 2574
2024-02-07 04:27:55,288 Epoch 2574: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 04:27:55,289 EPOCH 2575
2024-02-07 04:28:11,948 Epoch 2575: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 04:28:11,949 EPOCH 2576
2024-02-07 04:28:28,175 Epoch 2576: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 04:28:28,176 EPOCH 2577
2024-02-07 04:28:45,033 Epoch 2577: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 04:28:45,034 EPOCH 2578
2024-02-07 04:29:00,426 [Epoch: 2578 Step: 00023200] Batch Recognition Loss:   0.000516 => Gls Tokens per Sec:      524 || Batch Translation Loss:   0.024421 => Txt Tokens per Sec:     1507 || Lr: 0.000100
2024-02-07 04:29:01,502 Epoch 2578: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 04:29:01,502 EPOCH 2579
2024-02-07 04:29:17,812 Epoch 2579: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 04:29:17,813 EPOCH 2580
2024-02-07 04:29:34,083 Epoch 2580: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 04:29:34,084 EPOCH 2581
2024-02-07 04:29:50,679 Epoch 2581: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 04:29:50,680 EPOCH 2582
2024-02-07 04:30:06,995 Epoch 2582: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 04:30:06,996 EPOCH 2583
2024-02-07 04:30:23,829 Epoch 2583: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 04:30:23,830 EPOCH 2584
2024-02-07 04:30:40,372 Epoch 2584: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 04:30:40,373 EPOCH 2585
2024-02-07 04:30:56,751 Epoch 2585: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 04:30:56,752 EPOCH 2586
2024-02-07 04:31:13,340 Epoch 2586: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 04:31:13,341 EPOCH 2587
2024-02-07 04:31:29,629 Epoch 2587: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 04:31:29,629 EPOCH 2588
2024-02-07 04:31:46,117 Epoch 2588: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 04:31:46,118 EPOCH 2589
2024-02-07 04:32:02,001 [Epoch: 2589 Step: 00023300] Batch Recognition Loss:   0.000952 => Gls Tokens per Sec:      588 || Batch Translation Loss:   0.018075 => Txt Tokens per Sec:     1628 || Lr: 0.000100
2024-02-07 04:32:02,472 Epoch 2589: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 04:32:02,472 EPOCH 2590
2024-02-07 04:32:18,583 Epoch 2590: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 04:32:18,584 EPOCH 2591
2024-02-07 04:32:35,313 Epoch 2591: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 04:32:35,313 EPOCH 2592
2024-02-07 04:32:51,899 Epoch 2592: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 04:32:51,899 EPOCH 2593
2024-02-07 04:33:08,378 Epoch 2593: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 04:33:08,379 EPOCH 2594
2024-02-07 04:33:25,062 Epoch 2594: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 04:33:25,063 EPOCH 2595
2024-02-07 04:33:41,458 Epoch 2595: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 04:33:41,459 EPOCH 2596
2024-02-07 04:33:57,731 Epoch 2596: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 04:33:57,732 EPOCH 2597
2024-02-07 04:34:14,492 Epoch 2597: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 04:34:14,493 EPOCH 2598
2024-02-07 04:34:30,959 Epoch 2598: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 04:34:30,959 EPOCH 2599
2024-02-07 04:34:47,473 Epoch 2599: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 04:34:47,473 EPOCH 2600
2024-02-07 04:35:03,762 [Epoch: 2600 Step: 00023400] Batch Recognition Loss:   0.000644 => Gls Tokens per Sec:      652 || Batch Translation Loss:   0.008138 => Txt Tokens per Sec:     1804 || Lr: 0.000100
2024-02-07 04:35:03,763 Epoch 2600: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 04:35:03,763 EPOCH 2601
2024-02-07 04:35:19,984 Epoch 2601: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 04:35:19,984 EPOCH 2602
2024-02-07 04:35:36,452 Epoch 2602: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 04:35:36,452 EPOCH 2603
2024-02-07 04:35:52,775 Epoch 2603: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 04:35:52,775 EPOCH 2604
2024-02-07 04:36:09,634 Epoch 2604: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 04:36:09,634 EPOCH 2605
2024-02-07 04:36:25,657 Epoch 2605: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 04:36:25,658 EPOCH 2606
2024-02-07 04:36:42,082 Epoch 2606: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 04:36:42,083 EPOCH 2607
2024-02-07 04:36:58,919 Epoch 2607: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 04:36:58,920 EPOCH 2608
2024-02-07 04:37:15,162 Epoch 2608: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 04:37:15,163 EPOCH 2609
2024-02-07 04:37:31,432 Epoch 2609: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 04:37:31,433 EPOCH 2610
2024-02-07 04:37:48,301 Epoch 2610: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 04:37:48,301 EPOCH 2611
2024-02-07 04:38:04,694 Epoch 2611: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 04:38:04,695 EPOCH 2612
2024-02-07 04:38:10,616 [Epoch: 2612 Step: 00023500] Batch Recognition Loss:   0.003028 => Gls Tokens per Sec:      216 || Batch Translation Loss:   0.029317 => Txt Tokens per Sec:      744 || Lr: 0.000100
2024-02-07 04:38:21,061 Epoch 2612: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 04:38:21,062 EPOCH 2613
2024-02-07 04:38:37,457 Epoch 2613: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 04:38:37,458 EPOCH 2614
2024-02-07 04:38:54,152 Epoch 2614: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 04:38:54,152 EPOCH 2615
2024-02-07 04:39:10,400 Epoch 2615: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 04:39:10,401 EPOCH 2616
2024-02-07 04:39:26,724 Epoch 2616: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 04:39:26,725 EPOCH 2617
2024-02-07 04:39:43,245 Epoch 2617: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 04:39:43,245 EPOCH 2618
2024-02-07 04:39:59,594 Epoch 2618: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 04:39:59,594 EPOCH 2619
2024-02-07 04:40:16,291 Epoch 2619: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 04:40:16,291 EPOCH 2620
2024-02-07 04:40:32,845 Epoch 2620: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 04:40:32,846 EPOCH 2621
2024-02-07 04:40:49,325 Epoch 2621: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 04:40:49,325 EPOCH 2622
2024-02-07 04:41:05,810 Epoch 2622: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 04:41:05,810 EPOCH 2623
2024-02-07 04:41:06,558 [Epoch: 2623 Step: 00023600] Batch Recognition Loss:   0.000382 => Gls Tokens per Sec:     3432 || Batch Translation Loss:   0.011849 => Txt Tokens per Sec:     9124 || Lr: 0.000100
2024-02-07 04:41:22,479 Epoch 2623: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 04:41:22,479 EPOCH 2624
2024-02-07 04:41:38,731 Epoch 2624: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 04:41:38,732 EPOCH 2625
2024-02-07 04:41:54,954 Epoch 2625: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 04:41:54,955 EPOCH 2626
2024-02-07 04:42:11,506 Epoch 2626: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 04:42:11,506 EPOCH 2627
2024-02-07 04:42:28,019 Epoch 2627: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 04:42:28,019 EPOCH 2628
2024-02-07 04:42:44,332 Epoch 2628: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-07 04:42:44,333 EPOCH 2629
2024-02-07 04:43:00,908 Epoch 2629: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.02 
2024-02-07 04:43:00,908 EPOCH 2630
2024-02-07 04:43:17,419 Epoch 2630: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.42 
2024-02-07 04:43:17,420 EPOCH 2631
2024-02-07 04:43:33,849 Epoch 2631: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.27 
2024-02-07 04:43:33,849 EPOCH 2632
2024-02-07 04:43:50,458 Epoch 2632: Total Training Recognition Loss 0.17  Total Training Translation Loss 2.28 
2024-02-07 04:43:50,459 EPOCH 2633
2024-02-07 04:44:07,030 Epoch 2633: Total Training Recognition Loss 0.45  Total Training Translation Loss 1.18 
2024-02-07 04:44:07,031 EPOCH 2634
2024-02-07 04:44:14,918 [Epoch: 2634 Step: 00023700] Batch Recognition Loss:   0.055434 => Gls Tokens per Sec:      373 || Batch Translation Loss:   0.107290 => Txt Tokens per Sec:      979 || Lr: 0.000100
2024-02-07 04:44:23,544 Epoch 2634: Total Training Recognition Loss 1.76  Total Training Translation Loss 0.87 
2024-02-07 04:44:23,545 EPOCH 2635
2024-02-07 04:44:39,896 Epoch 2635: Total Training Recognition Loss 0.48  Total Training Translation Loss 0.66 
2024-02-07 04:44:39,896 EPOCH 2636
2024-02-07 04:44:56,285 Epoch 2636: Total Training Recognition Loss 0.17  Total Training Translation Loss 0.46 
2024-02-07 04:44:56,285 EPOCH 2637
2024-02-07 04:45:12,830 Epoch 2637: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.40 
2024-02-07 04:45:12,831 EPOCH 2638
2024-02-07 04:45:29,783 Epoch 2638: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.35 
2024-02-07 04:45:29,783 EPOCH 2639
2024-02-07 04:45:46,101 Epoch 2639: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.36 
2024-02-07 04:45:46,101 EPOCH 2640
2024-02-07 04:46:03,024 Epoch 2640: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.30 
2024-02-07 04:46:03,025 EPOCH 2641
2024-02-07 04:46:19,766 Epoch 2641: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-07 04:46:19,767 EPOCH 2642
2024-02-07 04:46:36,013 Epoch 2642: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-07 04:46:36,014 EPOCH 2643
2024-02-07 04:46:52,522 Epoch 2643: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 04:46:52,523 EPOCH 2644
2024-02-07 04:47:08,807 Epoch 2644: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 04:47:08,807 EPOCH 2645
2024-02-07 04:47:10,441 [Epoch: 2645 Step: 00023800] Batch Recognition Loss:   0.000326 => Gls Tokens per Sec:     3135 || Batch Translation Loss:   0.015553 => Txt Tokens per Sec:     8160 || Lr: 0.000100
2024-02-07 04:47:25,239 Epoch 2645: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 04:47:25,240 EPOCH 2646
2024-02-07 04:47:41,516 Epoch 2646: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 04:47:41,517 EPOCH 2647
2024-02-07 04:47:57,851 Epoch 2647: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 04:47:57,852 EPOCH 2648
2024-02-07 04:48:14,392 Epoch 2648: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 04:48:14,393 EPOCH 2649
2024-02-07 04:48:30,763 Epoch 2649: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 04:48:30,764 EPOCH 2650
2024-02-07 04:48:47,464 Epoch 2650: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 04:48:47,465 EPOCH 2651
2024-02-07 04:49:03,866 Epoch 2651: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 04:49:03,867 EPOCH 2652
2024-02-07 04:49:20,218 Epoch 2652: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 04:49:20,218 EPOCH 2653
2024-02-07 04:49:36,645 Epoch 2653: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 04:49:36,646 EPOCH 2654
2024-02-07 04:49:53,145 Epoch 2654: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 04:49:53,146 EPOCH 2655
2024-02-07 04:50:09,604 Epoch 2655: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 04:50:09,605 EPOCH 2656
2024-02-07 04:50:20,511 [Epoch: 2656 Step: 00023900] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:      587 || Batch Translation Loss:   0.018763 => Txt Tokens per Sec:     1734 || Lr: 0.000100
2024-02-07 04:50:26,037 Epoch 2656: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 04:50:26,038 EPOCH 2657
2024-02-07 04:50:42,335 Epoch 2657: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 04:50:42,336 EPOCH 2658
2024-02-07 04:50:58,833 Epoch 2658: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 04:50:58,834 EPOCH 2659
2024-02-07 04:51:15,341 Epoch 2659: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 04:51:15,342 EPOCH 2660
2024-02-07 04:51:31,917 Epoch 2660: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 04:51:31,917 EPOCH 2661
2024-02-07 04:51:48,553 Epoch 2661: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 04:51:48,554 EPOCH 2662
2024-02-07 04:52:04,941 Epoch 2662: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 04:52:04,942 EPOCH 2663
2024-02-07 04:52:21,170 Epoch 2663: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 04:52:21,170 EPOCH 2664
2024-02-07 04:52:37,601 Epoch 2664: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 04:52:37,602 EPOCH 2665
2024-02-07 04:52:54,042 Epoch 2665: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 04:52:54,043 EPOCH 2666
2024-02-07 04:53:10,743 Epoch 2666: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 04:53:10,744 EPOCH 2667
2024-02-07 04:53:22,963 [Epoch: 2667 Step: 00024000] Batch Recognition Loss:   0.001381 => Gls Tokens per Sec:      555 || Batch Translation Loss:   0.022721 => Txt Tokens per Sec:     1593 || Lr: 0.000100
2024-02-07 04:54:30,883 Validation result at epoch 2667, step    24000: duration: 67.9199s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.44572	Translation Loss: 94133.10938	PPL: 12112.92188
	Eval Metric: BLEU
	WER 3.60	(DEL: 0.00,	INS: 0.00,	SUB: 3.60)
	BLEU-4 0.75	(BLEU-1: 10.74,	BLEU-2: 3.36,	BLEU-3: 1.42,	BLEU-4: 0.75)
	CHRF 17.23	ROUGE 9.04
2024-02-07 04:54:30,884 Logging Recognition and Translation Outputs
2024-02-07 04:54:30,885 ========================================================================================================================
2024-02-07 04:54:30,885 Logging Sequence: 171_2.00
2024-02-07 04:54:30,886 	Gloss Reference :	A B+C+D+E  
2024-02-07 04:54:30,886 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-07 04:54:30,886 	Gloss Alignment :	  S        
2024-02-07 04:54:30,886 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 04:54:30,888 	Text Reference  :	as you might all know that the ***** ipl is  about to   end      the finals are on  28th  may
2024-02-07 04:54:30,888 	Text Hypothesis :	** *** on    16  july 2021 the aiscd won the first time spending the ****** *** t20 world cup
2024-02-07 04:54:30,888 	Text Alignment  :	D  D   S     S   S    S        I     S   S   S     S    S            D      D   S   S     S  
2024-02-07 04:54:30,888 ========================================================================================================================
2024-02-07 04:54:30,889 Logging Sequence: 119_33.00
2024-02-07 04:54:30,889 	Gloss Reference :	A B+C+D+E
2024-02-07 04:54:30,889 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 04:54:30,889 	Gloss Alignment :	         
2024-02-07 04:54:30,889 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 04:54:30,890 	Text Reference  :	***** he  wanted  to gift *** ****** 35  people wow wonderful
2024-02-07 04:54:30,890 	Text Hypothesis :	messi got married to gift the team's and it     was injured  
2024-02-07 04:54:30,890 	Text Alignment  :	I     S   S               I   I      S   S      S   S        
2024-02-07 04:54:30,891 ========================================================================================================================
2024-02-07 04:54:30,891 Logging Sequence: 158_131.00
2024-02-07 04:54:30,891 	Gloss Reference :	A B+C+D+E
2024-02-07 04:54:30,891 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 04:54:30,891 	Gloss Alignment :	         
2024-02-07 04:54:30,891 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 04:54:30,893 	Text Reference  :	on 10th april 2023 there was   a   match   between rcb and      lsg     in      bengaluru
2024-02-07 04:54:30,893 	Text Hypothesis :	** you  know  that virat kohli and gambhir had     an  argument women's cricket team     
2024-02-07 04:54:30,893 	Text Alignment  :	D  S    S     S    S     S     S   S       S       S   S        S       S       S        
2024-02-07 04:54:30,893 ========================================================================================================================
2024-02-07 04:54:30,894 Logging Sequence: 164_412.00
2024-02-07 04:54:30,894 	Gloss Reference :	A B+C+D+E
2024-02-07 04:54:30,894 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 04:54:30,894 	Gloss Alignment :	         
2024-02-07 04:54:30,894 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 04:54:30,896 	Text Reference  :	if you divide these two figures you will be shocked to know     that       each ball's worth  is rs  50         lakhs  
2024-02-07 04:54:30,896 	Text Hypothesis :	** *** ****** ***** *** ******* *** **** ** ******* ** reliance industries owns 51     shares of the viacom18's company
2024-02-07 04:54:30,896 	Text Alignment  :	D  D   D      D     D   D       D   D    D  D       D  S        S          S    S      S      S  S   S          S      
2024-02-07 04:54:30,896 ========================================================================================================================
2024-02-07 04:54:30,896 Logging Sequence: 159_112.00
2024-02-07 04:54:30,896 	Gloss Reference :	A B+C+D+E
2024-02-07 04:54:30,896 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 04:54:30,896 	Gloss Alignment :	         
2024-02-07 04:54:30,897 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 04:54:30,898 	Text Reference  :	******** kohli had revealed that before the tournament he   did  not    touch his bat for a  month   yes 1   month   
2024-02-07 04:54:30,898 	Text Hypothesis :	mohammed shami has said     that ****** the ********** pani puri seller on    his *** *** 50 million on  his athletes
2024-02-07 04:54:30,899 	Text Alignment  :	I        S     S   S             D          D          S    S    S      S         D   D   S  S       S   S   S       
2024-02-07 04:54:30,899 ========================================================================================================================
2024-02-07 04:54:35,088 Epoch 2667: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 04:54:35,088 EPOCH 2668
2024-02-07 04:54:51,995 Epoch 2668: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 04:54:51,996 EPOCH 2669
2024-02-07 04:55:08,812 Epoch 2669: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 04:55:08,813 EPOCH 2670
2024-02-07 04:55:25,419 Epoch 2670: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 04:55:25,420 EPOCH 2671
2024-02-07 04:55:41,978 Epoch 2671: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 04:55:41,978 EPOCH 2672
2024-02-07 04:55:58,254 Epoch 2672: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 04:55:58,255 EPOCH 2673
2024-02-07 04:56:14,683 Epoch 2673: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 04:56:14,684 EPOCH 2674
2024-02-07 04:56:30,945 Epoch 2674: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 04:56:30,945 EPOCH 2675
2024-02-07 04:56:47,657 Epoch 2675: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 04:56:47,658 EPOCH 2676
2024-02-07 04:57:03,678 Epoch 2676: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 04:57:03,679 EPOCH 2677
2024-02-07 04:57:20,068 Epoch 2677: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 04:57:20,068 EPOCH 2678
2024-02-07 04:57:32,358 [Epoch: 2678 Step: 00024100] Batch Recognition Loss:   0.000207 => Gls Tokens per Sec:      729 || Batch Translation Loss:   0.014056 => Txt Tokens per Sec:     1994 || Lr: 0.000100
2024-02-07 04:57:37,143 Epoch 2678: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 04:57:37,143 EPOCH 2679
2024-02-07 04:57:53,448 Epoch 2679: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 04:57:53,449 EPOCH 2680
2024-02-07 04:58:09,829 Epoch 2680: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 04:58:09,830 EPOCH 2681
2024-02-07 04:58:26,558 Epoch 2681: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 04:58:26,558 EPOCH 2682
2024-02-07 04:58:42,997 Epoch 2682: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 04:58:42,998 EPOCH 2683
2024-02-07 04:58:59,301 Epoch 2683: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 04:58:59,302 EPOCH 2684
2024-02-07 04:59:15,762 Epoch 2684: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 04:59:15,763 EPOCH 2685
2024-02-07 04:59:32,323 Epoch 2685: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 04:59:32,323 EPOCH 2686
2024-02-07 04:59:48,613 Epoch 2686: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 04:59:48,613 EPOCH 2687
2024-02-07 05:00:05,136 Epoch 2687: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:00:05,137 EPOCH 2688
2024-02-07 05:00:21,576 Epoch 2688: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:00:21,576 EPOCH 2689
2024-02-07 05:00:31,728 [Epoch: 2689 Step: 00024200] Batch Recognition Loss:   0.000242 => Gls Tokens per Sec:      920 || Batch Translation Loss:   0.010855 => Txt Tokens per Sec:     2459 || Lr: 0.000100
2024-02-07 05:00:37,804 Epoch 2689: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 05:00:37,805 EPOCH 2690
2024-02-07 05:00:54,713 Epoch 2690: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 05:00:54,714 EPOCH 2691
2024-02-07 05:01:11,050 Epoch 2691: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:01:11,050 EPOCH 2692
2024-02-07 05:01:27,370 Epoch 2692: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:01:27,370 EPOCH 2693
2024-02-07 05:01:44,158 Epoch 2693: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 05:01:44,159 EPOCH 2694
2024-02-07 05:02:00,161 Epoch 2694: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:02:00,162 EPOCH 2695
2024-02-07 05:02:16,539 Epoch 2695: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 05:02:16,539 EPOCH 2696
2024-02-07 05:02:32,828 Epoch 2696: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:02:32,828 EPOCH 2697
2024-02-07 05:02:49,211 Epoch 2697: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 05:02:49,211 EPOCH 2698
2024-02-07 05:03:05,991 Epoch 2698: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:03:05,991 EPOCH 2699
2024-02-07 05:03:22,695 Epoch 2699: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 05:03:22,696 EPOCH 2700
2024-02-07 05:03:39,257 [Epoch: 2700 Step: 00024300] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:      641 || Batch Translation Loss:   0.006417 => Txt Tokens per Sec:     1774 || Lr: 0.000100
2024-02-07 05:03:39,258 Epoch 2700: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:03:39,258 EPOCH 2701
2024-02-07 05:03:55,933 Epoch 2701: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 05:03:55,934 EPOCH 2702
2024-02-07 05:04:12,122 Epoch 2702: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:04:12,122 EPOCH 2703
2024-02-07 05:04:28,640 Epoch 2703: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:04:28,641 EPOCH 2704
2024-02-07 05:04:45,430 Epoch 2704: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:04:45,431 EPOCH 2705
2024-02-07 05:05:02,134 Epoch 2705: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:05:02,134 EPOCH 2706
2024-02-07 05:05:18,429 Epoch 2706: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:05:18,429 EPOCH 2707
2024-02-07 05:05:34,728 Epoch 2707: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:05:34,728 EPOCH 2708
2024-02-07 05:05:51,340 Epoch 2708: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:05:51,341 EPOCH 2709
2024-02-07 05:06:07,888 Epoch 2709: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:06:07,889 EPOCH 2710
2024-02-07 05:06:24,569 Epoch 2710: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:06:24,570 EPOCH 2711
2024-02-07 05:06:41,039 Epoch 2711: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:06:41,040 EPOCH 2712
2024-02-07 05:06:41,494 [Epoch: 2712 Step: 00024400] Batch Recognition Loss:   0.000437 => Gls Tokens per Sec:     2826 || Batch Translation Loss:   0.014037 => Txt Tokens per Sec:     7762 || Lr: 0.000100
2024-02-07 05:06:57,498 Epoch 2712: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:06:57,499 EPOCH 2713
2024-02-07 05:07:13,801 Epoch 2713: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:07:13,802 EPOCH 2714
2024-02-07 05:07:30,244 Epoch 2714: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:07:30,244 EPOCH 2715
2024-02-07 05:07:46,755 Epoch 2715: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:07:46,756 EPOCH 2716
2024-02-07 05:08:03,422 Epoch 2716: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:08:03,422 EPOCH 2717
2024-02-07 05:08:19,976 Epoch 2717: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:08:19,977 EPOCH 2718
2024-02-07 05:08:36,672 Epoch 2718: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:08:36,673 EPOCH 2719
2024-02-07 05:08:53,113 Epoch 2719: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:08:53,114 EPOCH 2720
2024-02-07 05:09:09,503 Epoch 2720: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:09:09,504 EPOCH 2721
2024-02-07 05:09:25,789 Epoch 2721: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:09:25,789 EPOCH 2722
2024-02-07 05:09:42,343 Epoch 2722: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:09:42,344 EPOCH 2723
2024-02-07 05:09:46,099 [Epoch: 2723 Step: 00024500] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:      682 || Batch Translation Loss:   0.016640 => Txt Tokens per Sec:     2042 || Lr: 0.000100
2024-02-07 05:09:58,832 Epoch 2723: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:09:58,833 EPOCH 2724
2024-02-07 05:10:15,379 Epoch 2724: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:10:15,380 EPOCH 2725
2024-02-07 05:10:32,159 Epoch 2725: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:10:32,159 EPOCH 2726
2024-02-07 05:10:48,249 Epoch 2726: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:10:48,249 EPOCH 2727
2024-02-07 05:11:04,892 Epoch 2727: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:11:04,893 EPOCH 2728
2024-02-07 05:11:21,135 Epoch 2728: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:11:21,136 EPOCH 2729
2024-02-07 05:11:37,674 Epoch 2729: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:11:37,675 EPOCH 2730
2024-02-07 05:11:53,987 Epoch 2730: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:11:53,988 EPOCH 2731
2024-02-07 05:12:10,454 Epoch 2731: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 05:12:10,455 EPOCH 2732
2024-02-07 05:12:26,708 Epoch 2732: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 05:12:26,708 EPOCH 2733
2024-02-07 05:12:43,156 Epoch 2733: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 05:12:43,157 EPOCH 2734
2024-02-07 05:12:51,135 [Epoch: 2734 Step: 00024600] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:      369 || Batch Translation Loss:   0.013230 => Txt Tokens per Sec:     1089 || Lr: 0.000100
2024-02-07 05:12:59,424 Epoch 2734: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 05:12:59,424 EPOCH 2735
2024-02-07 05:13:15,962 Epoch 2735: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 05:13:15,963 EPOCH 2736
2024-02-07 05:13:32,430 Epoch 2736: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 05:13:32,430 EPOCH 2737
2024-02-07 05:13:49,195 Epoch 2737: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 05:13:49,196 EPOCH 2738
2024-02-07 05:14:05,660 Epoch 2738: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 05:14:05,660 EPOCH 2739
2024-02-07 05:14:22,001 Epoch 2739: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 05:14:22,002 EPOCH 2740
2024-02-07 05:14:38,430 Epoch 2740: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 05:14:38,431 EPOCH 2741
2024-02-07 05:14:55,032 Epoch 2741: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 05:14:55,033 EPOCH 2742
2024-02-07 05:15:11,586 Epoch 2742: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 05:15:11,586 EPOCH 2743
2024-02-07 05:15:28,125 Epoch 2743: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 05:15:28,125 EPOCH 2744
2024-02-07 05:15:44,629 Epoch 2744: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 05:15:44,630 EPOCH 2745
2024-02-07 05:15:54,673 [Epoch: 2745 Step: 00024700] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:      510 || Batch Translation Loss:   0.017216 => Txt Tokens per Sec:     1458 || Lr: 0.000100
2024-02-07 05:16:00,952 Epoch 2745: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 05:16:00,952 EPOCH 2746
2024-02-07 05:16:17,522 Epoch 2746: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-07 05:16:17,522 EPOCH 2747
2024-02-07 05:16:33,953 Epoch 2747: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-07 05:16:33,953 EPOCH 2748
2024-02-07 05:16:50,294 Epoch 2748: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-07 05:16:50,294 EPOCH 2749
2024-02-07 05:17:06,803 Epoch 2749: Total Training Recognition Loss 0.00  Total Training Translation Loss 5.13 
2024-02-07 05:17:06,804 EPOCH 2750
2024-02-07 05:17:23,326 Epoch 2750: Total Training Recognition Loss 0.09  Total Training Translation Loss 10.15 
2024-02-07 05:17:23,326 EPOCH 2751
2024-02-07 05:17:39,852 Epoch 2751: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.59 
2024-02-07 05:17:39,852 EPOCH 2752
2024-02-07 05:17:56,285 Epoch 2752: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.39 
2024-02-07 05:17:56,286 EPOCH 2753
2024-02-07 05:18:12,636 Epoch 2753: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.58 
2024-02-07 05:18:12,637 EPOCH 2754
2024-02-07 05:18:28,989 Epoch 2754: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.55 
2024-02-07 05:18:28,990 EPOCH 2755
2024-02-07 05:18:45,336 Epoch 2755: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.88 
2024-02-07 05:18:45,337 EPOCH 2756
2024-02-07 05:18:56,900 [Epoch: 2756 Step: 00024800] Batch Recognition Loss:   0.001189 => Gls Tokens per Sec:      476 || Batch Translation Loss:   0.031921 => Txt Tokens per Sec:     1226 || Lr: 0.000100
2024-02-07 05:19:02,075 Epoch 2756: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-07 05:19:02,076 EPOCH 2757
2024-02-07 05:19:18,318 Epoch 2757: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-07 05:19:18,319 EPOCH 2758
2024-02-07 05:19:34,725 Epoch 2758: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-07 05:19:34,726 EPOCH 2759
2024-02-07 05:19:51,029 Epoch 2759: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-07 05:19:51,029 EPOCH 2760
2024-02-07 05:20:07,336 Epoch 2760: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-07 05:20:07,336 EPOCH 2761
2024-02-07 05:20:23,831 Epoch 2761: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-07 05:20:23,832 EPOCH 2762
2024-02-07 05:20:40,475 Epoch 2762: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-07 05:20:40,476 EPOCH 2763
2024-02-07 05:20:57,074 Epoch 2763: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-07 05:20:57,075 EPOCH 2764
2024-02-07 05:21:13,662 Epoch 2764: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-07 05:21:13,663 EPOCH 2765
2024-02-07 05:21:30,240 Epoch 2765: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 05:21:30,241 EPOCH 2766
2024-02-07 05:21:46,700 Epoch 2766: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 05:21:46,701 EPOCH 2767
2024-02-07 05:21:56,639 [Epoch: 2767 Step: 00024900] Batch Recognition Loss:   0.000351 => Gls Tokens per Sec:      682 || Batch Translation Loss:   0.020123 => Txt Tokens per Sec:     1955 || Lr: 0.000100
2024-02-07 05:22:03,353 Epoch 2767: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-07 05:22:03,354 EPOCH 2768
2024-02-07 05:22:19,922 Epoch 2768: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 05:22:19,922 EPOCH 2769
2024-02-07 05:22:36,406 Epoch 2769: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 05:22:36,408 EPOCH 2770
2024-02-07 05:22:52,588 Epoch 2770: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 05:22:52,589 EPOCH 2771
2024-02-07 05:23:09,054 Epoch 2771: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 05:23:09,055 EPOCH 2772
2024-02-07 05:23:25,542 Epoch 2772: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 05:23:25,543 EPOCH 2773
2024-02-07 05:23:42,015 Epoch 2773: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 05:23:42,016 EPOCH 2774
2024-02-07 05:23:58,770 Epoch 2774: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 05:23:58,770 EPOCH 2775
2024-02-07 05:24:15,014 Epoch 2775: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 05:24:15,015 EPOCH 2776
2024-02-07 05:24:31,225 Epoch 2776: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 05:24:31,226 EPOCH 2777
2024-02-07 05:24:47,696 Epoch 2777: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 05:24:47,697 EPOCH 2778
2024-02-07 05:25:00,010 [Epoch: 2778 Step: 00025000] Batch Recognition Loss:   0.000288 => Gls Tokens per Sec:      655 || Batch Translation Loss:   0.017448 => Txt Tokens per Sec:     1739 || Lr: 0.000100
2024-02-07 05:25:03,989 Epoch 2778: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 05:25:03,989 EPOCH 2779
2024-02-07 05:25:20,604 Epoch 2779: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 05:25:20,604 EPOCH 2780
2024-02-07 05:25:37,062 Epoch 2780: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 05:25:37,062 EPOCH 2781
2024-02-07 05:25:53,647 Epoch 2781: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 05:25:53,648 EPOCH 2782
2024-02-07 05:26:10,282 Epoch 2782: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 05:26:10,284 EPOCH 2783
2024-02-07 05:26:27,150 Epoch 2783: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 05:26:27,151 EPOCH 2784
2024-02-07 05:26:43,471 Epoch 2784: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 05:26:43,472 EPOCH 2785
2024-02-07 05:27:00,201 Epoch 2785: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 05:27:00,201 EPOCH 2786
2024-02-07 05:27:17,121 Epoch 2786: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 05:27:17,121 EPOCH 2787
2024-02-07 05:27:33,348 Epoch 2787: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 05:27:33,349 EPOCH 2788
2024-02-07 05:27:49,747 Epoch 2788: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 05:27:49,747 EPOCH 2789
2024-02-07 05:28:06,100 [Epoch: 2789 Step: 00025100] Batch Recognition Loss:   0.000282 => Gls Tokens per Sec:      571 || Batch Translation Loss:   0.014495 => Txt Tokens per Sec:     1621 || Lr: 0.000100
2024-02-07 05:28:06,428 Epoch 2789: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 05:28:06,428 EPOCH 2790
2024-02-07 05:28:22,763 Epoch 2790: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 05:28:22,764 EPOCH 2791
2024-02-07 05:28:39,266 Epoch 2791: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 05:28:39,266 EPOCH 2792
2024-02-07 05:28:55,736 Epoch 2792: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 05:28:55,737 EPOCH 2793
2024-02-07 05:29:12,095 Epoch 2793: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 05:29:12,096 EPOCH 2794
2024-02-07 05:29:28,622 Epoch 2794: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 05:29:28,623 EPOCH 2795
2024-02-07 05:29:44,659 Epoch 2795: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 05:29:44,659 EPOCH 2796
2024-02-07 05:30:01,561 Epoch 2796: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 05:30:01,561 EPOCH 2797
2024-02-07 05:30:17,687 Epoch 2797: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 05:30:17,687 EPOCH 2798
2024-02-07 05:30:34,705 Epoch 2798: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 05:30:34,706 EPOCH 2799
2024-02-07 05:30:51,021 Epoch 2799: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 05:30:51,022 EPOCH 2800
2024-02-07 05:31:07,802 [Epoch: 2800 Step: 00025200] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:      633 || Batch Translation Loss:   0.008946 => Txt Tokens per Sec:     1751 || Lr: 0.000100
2024-02-07 05:31:07,803 Epoch 2800: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 05:31:07,803 EPOCH 2801
2024-02-07 05:31:24,173 Epoch 2801: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:31:24,174 EPOCH 2802
2024-02-07 05:31:40,870 Epoch 2802: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 05:31:40,871 EPOCH 2803
2024-02-07 05:31:57,203 Epoch 2803: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 05:31:57,204 EPOCH 2804
2024-02-07 05:32:13,504 Epoch 2804: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 05:32:13,505 EPOCH 2805
2024-02-07 05:32:29,784 Epoch 2805: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 05:32:29,784 EPOCH 2806
2024-02-07 05:32:46,142 Epoch 2806: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 05:32:46,143 EPOCH 2807
2024-02-07 05:33:02,430 Epoch 2807: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 05:33:02,431 EPOCH 2808
2024-02-07 05:33:18,643 Epoch 2808: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:33:18,644 EPOCH 2809
2024-02-07 05:33:35,290 Epoch 2809: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 05:33:35,291 EPOCH 2810
2024-02-07 05:33:51,578 Epoch 2810: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 05:33:51,578 EPOCH 2811
2024-02-07 05:34:08,376 Epoch 2811: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:34:08,377 EPOCH 2812
2024-02-07 05:34:12,698 [Epoch: 2812 Step: 00025300] Batch Recognition Loss:   0.000820 => Gls Tokens per Sec:       88 || Batch Translation Loss:   0.007111 => Txt Tokens per Sec:      314 || Lr: 0.000100
2024-02-07 05:34:24,765 Epoch 2812: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:34:24,765 EPOCH 2813
2024-02-07 05:34:41,237 Epoch 2813: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:34:41,237 EPOCH 2814
2024-02-07 05:34:57,588 Epoch 2814: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 05:34:57,588 EPOCH 2815
2024-02-07 05:35:14,029 Epoch 2815: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:35:14,030 EPOCH 2816
2024-02-07 05:35:30,478 Epoch 2816: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 05:35:30,479 EPOCH 2817
2024-02-07 05:35:46,783 Epoch 2817: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:35:46,783 EPOCH 2818
2024-02-07 05:36:03,401 Epoch 2818: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:36:03,402 EPOCH 2819
2024-02-07 05:36:19,702 Epoch 2819: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:36:19,704 EPOCH 2820
2024-02-07 05:36:36,317 Epoch 2820: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 05:36:36,318 EPOCH 2821
2024-02-07 05:36:53,164 Epoch 2821: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 05:36:53,164 EPOCH 2822
2024-02-07 05:37:09,627 Epoch 2822: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 05:37:09,628 EPOCH 2823
2024-02-07 05:37:10,804 [Epoch: 2823 Step: 00025400] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     2181 || Batch Translation Loss:   0.012117 => Txt Tokens per Sec:     6050 || Lr: 0.000100
2024-02-07 05:37:26,336 Epoch 2823: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:37:26,337 EPOCH 2824
2024-02-07 05:37:42,829 Epoch 2824: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 05:37:42,830 EPOCH 2825
2024-02-07 05:37:59,278 Epoch 2825: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 05:37:59,279 EPOCH 2826
2024-02-07 05:38:15,779 Epoch 2826: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 05:38:15,780 EPOCH 2827
2024-02-07 05:38:31,833 Epoch 2827: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 05:38:31,835 EPOCH 2828
2024-02-07 05:38:48,566 Epoch 2828: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 05:38:48,567 EPOCH 2829
2024-02-07 05:39:05,309 Epoch 2829: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:39:05,310 EPOCH 2830
2024-02-07 05:39:21,509 Epoch 2830: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:39:21,509 EPOCH 2831
2024-02-07 05:39:37,980 Epoch 2831: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:39:37,982 EPOCH 2832
2024-02-07 05:39:55,172 Epoch 2832: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:39:55,172 EPOCH 2833
2024-02-07 05:40:11,819 Epoch 2833: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:40:11,820 EPOCH 2834
2024-02-07 05:40:19,410 [Epoch: 2834 Step: 00025500] Batch Recognition Loss:   0.000209 => Gls Tokens per Sec:      506 || Batch Translation Loss:   0.022052 => Txt Tokens per Sec:     1552 || Lr: 0.000100
2024-02-07 05:40:28,554 Epoch 2834: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:40:28,555 EPOCH 2835
2024-02-07 05:40:45,324 Epoch 2835: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:40:45,326 EPOCH 2836
2024-02-07 05:41:01,872 Epoch 2836: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:41:01,873 EPOCH 2837
2024-02-07 05:41:18,443 Epoch 2837: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:41:18,443 EPOCH 2838
2024-02-07 05:41:35,266 Epoch 2838: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:41:35,267 EPOCH 2839
2024-02-07 05:41:52,321 Epoch 2839: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:41:52,321 EPOCH 2840
2024-02-07 05:42:08,644 Epoch 2840: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:42:08,646 EPOCH 2841
2024-02-07 05:42:25,261 Epoch 2841: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:42:25,262 EPOCH 2842
2024-02-07 05:42:41,798 Epoch 2842: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 05:42:41,799 EPOCH 2843
2024-02-07 05:42:58,361 Epoch 2843: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:42:58,361 EPOCH 2844
2024-02-07 05:43:14,989 Epoch 2844: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 05:43:14,990 EPOCH 2845
2024-02-07 05:43:23,494 [Epoch: 2845 Step: 00025600] Batch Recognition Loss:   0.000191 => Gls Tokens per Sec:      496 || Batch Translation Loss:   0.018450 => Txt Tokens per Sec:     1438 || Lr: 0.000100
2024-02-07 05:43:31,738 Epoch 2845: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:43:31,738 EPOCH 2846
2024-02-07 05:43:48,588 Epoch 2846: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:43:48,589 EPOCH 2847
2024-02-07 05:44:05,193 Epoch 2847: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:44:05,194 EPOCH 2848
2024-02-07 05:44:21,607 Epoch 2848: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:44:21,609 EPOCH 2849
2024-02-07 05:44:38,472 Epoch 2849: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:44:38,473 EPOCH 2850
2024-02-07 05:44:55,142 Epoch 2850: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:44:55,143 EPOCH 2851
2024-02-07 05:45:11,497 Epoch 2851: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:45:11,497 EPOCH 2852
2024-02-07 05:45:27,820 Epoch 2852: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:45:27,821 EPOCH 2853
2024-02-07 05:45:44,811 Epoch 2853: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:45:44,811 EPOCH 2854
2024-02-07 05:46:01,907 Epoch 2854: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:46:01,907 EPOCH 2855
2024-02-07 05:46:18,092 Epoch 2855: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 05:46:18,093 EPOCH 2856
2024-02-07 05:46:26,850 [Epoch: 2856 Step: 00025700] Batch Recognition Loss:   0.000207 => Gls Tokens per Sec:      628 || Batch Translation Loss:   0.017868 => Txt Tokens per Sec:     1684 || Lr: 0.000100
2024-02-07 05:46:34,590 Epoch 2856: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:46:34,591 EPOCH 2857
2024-02-07 05:46:51,207 Epoch 2857: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:46:51,208 EPOCH 2858
2024-02-07 05:47:07,607 Epoch 2858: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:47:07,607 EPOCH 2859
2024-02-07 05:47:24,198 Epoch 2859: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:47:24,199 EPOCH 2860
2024-02-07 05:47:40,631 Epoch 2860: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 05:47:40,632 EPOCH 2861
2024-02-07 05:47:57,206 Epoch 2861: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 05:47:57,208 EPOCH 2862
2024-02-07 05:48:13,428 Epoch 2862: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 05:48:13,429 EPOCH 2863
2024-02-07 05:48:30,176 Epoch 2863: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 05:48:30,178 EPOCH 2864
2024-02-07 05:48:46,758 Epoch 2864: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 05:48:46,759 EPOCH 2865
2024-02-07 05:49:03,470 Epoch 2865: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 05:49:03,470 EPOCH 2866
2024-02-07 05:49:20,225 Epoch 2866: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 05:49:20,226 EPOCH 2867
2024-02-07 05:49:28,974 [Epoch: 2867 Step: 00025800] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:      878 || Batch Translation Loss:   0.017126 => Txt Tokens per Sec:     2473 || Lr: 0.000100
2024-02-07 05:49:37,079 Epoch 2867: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 05:49:37,080 EPOCH 2868
2024-02-07 05:49:53,762 Epoch 2868: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 05:49:53,763 EPOCH 2869
2024-02-07 05:50:10,748 Epoch 2869: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 05:50:10,748 EPOCH 2870
2024-02-07 05:50:26,980 Epoch 2870: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-07 05:50:26,982 EPOCH 2871
2024-02-07 05:50:44,075 Epoch 2871: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 05:50:44,076 EPOCH 2872
2024-02-07 05:51:00,893 Epoch 2872: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 05:51:00,893 EPOCH 2873
2024-02-07 05:51:17,448 Epoch 2873: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 05:51:17,448 EPOCH 2874
2024-02-07 05:51:33,729 Epoch 2874: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-07 05:51:33,730 EPOCH 2875
2024-02-07 05:51:49,942 Epoch 2875: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-07 05:51:49,942 EPOCH 2876
2024-02-07 05:52:06,925 Epoch 2876: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-07 05:52:06,926 EPOCH 2877
2024-02-07 05:52:23,658 Epoch 2877: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-07 05:52:23,659 EPOCH 2878
2024-02-07 05:52:33,010 [Epoch: 2878 Step: 00025900] Batch Recognition Loss:   0.002454 => Gls Tokens per Sec:      862 || Batch Translation Loss:   0.134756 => Txt Tokens per Sec:     2257 || Lr: 0.000100
2024-02-07 05:52:39,991 Epoch 2878: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.59 
2024-02-07 05:52:39,991 EPOCH 2879
2024-02-07 05:52:56,507 Epoch 2879: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.60 
2024-02-07 05:52:56,507 EPOCH 2880
2024-02-07 05:53:12,733 Epoch 2880: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.33 
2024-02-07 05:53:12,733 EPOCH 2881
2024-02-07 05:53:29,061 Epoch 2881: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.53 
2024-02-07 05:53:29,063 EPOCH 2882
2024-02-07 05:53:46,216 Epoch 2882: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.68 
2024-02-07 05:53:46,216 EPOCH 2883
2024-02-07 05:54:02,696 Epoch 2883: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.87 
2024-02-07 05:54:02,696 EPOCH 2884
2024-02-07 05:54:19,480 Epoch 2884: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.34 
2024-02-07 05:54:19,482 EPOCH 2885
2024-02-07 05:54:36,300 Epoch 2885: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.85 
2024-02-07 05:54:36,301 EPOCH 2886
2024-02-07 05:54:52,963 Epoch 2886: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-07 05:54:52,964 EPOCH 2887
2024-02-07 05:55:09,558 Epoch 2887: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-07 05:55:09,559 EPOCH 2888
2024-02-07 05:55:25,908 Epoch 2888: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-07 05:55:25,910 EPOCH 2889
2024-02-07 05:55:41,870 [Epoch: 2889 Step: 00026000] Batch Recognition Loss:   0.000641 => Gls Tokens per Sec:      585 || Batch Translation Loss:   0.032848 => Txt Tokens per Sec:     1596 || Lr: 0.000100
2024-02-07 05:56:49,987 Validation result at epoch 2889, step    26000: duration: 68.1158s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.33895	Translation Loss: 93180.61719	PPL: 11013.67969
	Eval Metric: BLEU
	WER 3.88	(DEL: 0.00,	INS: 0.00,	SUB: 3.88)
	BLEU-4 0.53	(BLEU-1: 11.02,	BLEU-2: 3.40,	BLEU-3: 1.24,	BLEU-4: 0.53)
	CHRF 16.98	ROUGE 9.17
2024-02-07 05:56:49,989 Logging Recognition and Translation Outputs
2024-02-07 05:56:49,990 ========================================================================================================================
2024-02-07 05:56:49,990 Logging Sequence: 166_243.00
2024-02-07 05:56:49,990 	Gloss Reference :	A B+C+D+E
2024-02-07 05:56:49,990 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 05:56:49,990 	Gloss Alignment :	         
2024-02-07 05:56:49,991 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 05:56:49,992 	Text Reference  :	*** ***** ********* *** ***** *** ** **** icc   worked with  members boards like bcci  pcb     cricket australia etc
2024-02-07 05:56:49,993 	Text Hypothesis :	the board organised the world cup rs 2000 crore the    board of      the    2    teams playing for     the       mlc
2024-02-07 05:56:49,993 	Text Alignment  :	I   I     I         I   I     I   I  I    S     S      S     S       S      S    S     S       S       S         S  
2024-02-07 05:56:49,993 ========================================================================================================================
2024-02-07 05:56:49,993 Logging Sequence: 59_152.00
2024-02-07 05:56:49,993 	Gloss Reference :	A B+C+D+E
2024-02-07 05:56:49,993 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 05:56:49,994 	Gloss Alignment :	         
2024-02-07 05:56:49,994 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 05:56:49,995 	Text Reference  :	** *** *** **** **** ****** ** *** the   organisers encouraged athletes to    use        the  condoms in   their home countries
2024-02-07 05:56:49,996 	Text Hypothesis :	he had not even worn gloves he was using his        bare       hands    these sandwiches were being   sold for   a    condom   
2024-02-07 05:56:49,996 	Text Alignment  :	I  I   I   I    I    I      I  I   S     S          S          S        S     S          S    S       S    S     S    S        
2024-02-07 05:56:49,996 ========================================================================================================================
2024-02-07 05:56:49,996 Logging Sequence: 145_52.00
2024-02-07 05:56:49,996 	Gloss Reference :	A B+C+D+E
2024-02-07 05:56:49,996 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 05:56:49,996 	Gloss Alignment :	         
2024-02-07 05:56:49,997 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 05:56:49,998 	Text Reference  :	her name was dropped despite having qualified as    she   was the only female athlete
2024-02-07 05:56:49,998 	Text Hypothesis :	*** **** *** the     finals  were   asked     their child to  be  held on     it     
2024-02-07 05:56:49,998 	Text Alignment  :	D   D    D   S       S       S      S         S     S     S   S   S    S      S      
2024-02-07 05:56:49,998 ========================================================================================================================
2024-02-07 05:56:49,998 Logging Sequence: 172_163.00
2024-02-07 05:56:49,998 	Gloss Reference :	A B+C+D+E
2024-02-07 05:56:49,999 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 05:56:49,999 	Gloss Alignment :	         
2024-02-07 05:56:50,000 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 05:56:50,001 	Text Reference  :	if the match starts anywhere between 730 pm   to **** 935 pm    a  full 20-over match *** **** can be  played
2024-02-07 05:56:50,001 	Text Hypothesis :	if *** ***** ****** ******** ******* you wish to know the match as per  the     match all know for the game  
2024-02-07 05:56:50,002 	Text Alignment  :	   D   D     D      D        D       S   S       I    S   S     S  S    S             I   I    S   S   S     
2024-02-07 05:56:50,002 ========================================================================================================================
2024-02-07 05:56:50,002 Logging Sequence: 150_20.00
2024-02-07 05:56:50,002 	Gloss Reference :	A B+C+D+E
2024-02-07 05:56:50,002 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 05:56:50,002 	Gloss Alignment :	         
2024-02-07 05:56:50,003 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 05:56:50,004 	Text Reference  :	after a tough match india won the **** *** ***** *** saff   championship 2023        title
2024-02-07 05:56:50,004 	Text Hypothesis :	***** * ***** he    has   won the toss and could not played very         interesting times
2024-02-07 05:56:50,004 	Text Alignment  :	D     D D     S     S             I    I   I     I   S      S            S           S    
2024-02-07 05:56:50,004 ========================================================================================================================
2024-02-07 05:56:50,932 Epoch 2889: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-07 05:56:50,932 EPOCH 2890
2024-02-07 05:57:08,161 Epoch 2890: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-07 05:57:08,162 EPOCH 2891
2024-02-07 05:57:24,868 Epoch 2891: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-07 05:57:24,868 EPOCH 2892
2024-02-07 05:57:41,749 Epoch 2892: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 05:57:41,750 EPOCH 2893
2024-02-07 05:57:58,241 Epoch 2893: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 05:57:58,241 EPOCH 2894
2024-02-07 05:58:14,540 Epoch 2894: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 05:58:14,540 EPOCH 2895
2024-02-07 05:58:30,709 Epoch 2895: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 05:58:30,710 EPOCH 2896
2024-02-07 05:58:47,661 Epoch 2896: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 05:58:47,661 EPOCH 2897
2024-02-07 05:59:04,099 Epoch 2897: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-07 05:59:04,100 EPOCH 2898
2024-02-07 05:59:20,270 Epoch 2898: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 05:59:20,270 EPOCH 2899
2024-02-07 05:59:36,753 Epoch 2899: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 05:59:36,754 EPOCH 2900
2024-02-07 05:59:53,626 [Epoch: 2900 Step: 00026100] Batch Recognition Loss:   0.001304 => Gls Tokens per Sec:      630 || Batch Translation Loss:   0.010487 => Txt Tokens per Sec:     1742 || Lr: 0.000100
2024-02-07 05:59:53,628 Epoch 2900: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 05:59:53,628 EPOCH 2901
2024-02-07 06:00:10,796 Epoch 2901: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 06:00:10,797 EPOCH 2902
2024-02-07 06:00:26,988 Epoch 2902: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 06:00:26,988 EPOCH 2903
2024-02-07 06:00:43,554 Epoch 2903: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 06:00:43,554 EPOCH 2904
2024-02-07 06:01:00,220 Epoch 2904: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 06:01:00,221 EPOCH 2905
2024-02-07 06:01:16,860 Epoch 2905: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 06:01:16,861 EPOCH 2906
2024-02-07 06:01:33,155 Epoch 2906: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 06:01:33,156 EPOCH 2907
2024-02-07 06:01:49,249 Epoch 2907: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 06:01:49,250 EPOCH 2908
2024-02-07 06:02:05,715 Epoch 2908: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 06:02:05,715 EPOCH 2909
2024-02-07 06:02:22,166 Epoch 2909: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 06:02:22,166 EPOCH 2910
2024-02-07 06:02:38,603 Epoch 2910: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 06:02:38,604 EPOCH 2911
2024-02-07 06:02:55,082 Epoch 2911: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 06:02:55,083 EPOCH 2912
2024-02-07 06:02:55,301 [Epoch: 2912 Step: 00026200] Batch Recognition Loss:   0.000445 => Gls Tokens per Sec:     5899 || Batch Translation Loss:   0.008585 => Txt Tokens per Sec:    10691 || Lr: 0.000100
2024-02-07 06:03:11,343 Epoch 2912: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 06:03:11,343 EPOCH 2913
2024-02-07 06:03:28,104 Epoch 2913: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 06:03:28,104 EPOCH 2914
2024-02-07 06:03:44,862 Epoch 2914: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 06:03:44,862 EPOCH 2915
2024-02-07 06:04:01,682 Epoch 2915: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 06:04:01,684 EPOCH 2916
2024-02-07 06:04:18,224 Epoch 2916: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 06:04:18,225 EPOCH 2917
2024-02-07 06:04:34,773 Epoch 2917: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 06:04:34,774 EPOCH 2918
2024-02-07 06:04:51,069 Epoch 2918: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 06:04:51,069 EPOCH 2919
2024-02-07 06:05:07,552 Epoch 2919: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 06:05:07,553 EPOCH 2920
2024-02-07 06:05:24,235 Epoch 2920: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 06:05:24,235 EPOCH 2921
2024-02-07 06:05:40,225 Epoch 2921: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 06:05:40,226 EPOCH 2922
2024-02-07 06:05:56,509 Epoch 2922: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 06:05:56,510 EPOCH 2923
2024-02-07 06:05:57,539 [Epoch: 2923 Step: 00026300] Batch Recognition Loss:   0.000314 => Gls Tokens per Sec:     2492 || Batch Translation Loss:   0.014893 => Txt Tokens per Sec:     7020 || Lr: 0.000100
2024-02-07 06:06:13,267 Epoch 2923: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 06:06:13,268 EPOCH 2924
2024-02-07 06:06:29,787 Epoch 2924: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 06:06:29,788 EPOCH 2925
2024-02-07 06:06:46,164 Epoch 2925: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 06:06:46,165 EPOCH 2926
2024-02-07 06:07:02,706 Epoch 2926: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 06:07:02,707 EPOCH 2927
2024-02-07 06:07:19,190 Epoch 2927: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 06:07:19,190 EPOCH 2928
2024-02-07 06:07:35,785 Epoch 2928: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 06:07:35,785 EPOCH 2929
2024-02-07 06:07:52,466 Epoch 2929: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 06:07:52,467 EPOCH 2930
2024-02-07 06:08:08,838 Epoch 2930: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 06:08:08,838 EPOCH 2931
2024-02-07 06:08:25,357 Epoch 2931: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 06:08:25,358 EPOCH 2932
2024-02-07 06:08:41,888 Epoch 2932: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 06:08:41,888 EPOCH 2933
2024-02-07 06:08:58,119 Epoch 2933: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 06:08:58,120 EPOCH 2934
2024-02-07 06:08:59,270 [Epoch: 2934 Step: 00026400] Batch Recognition Loss:   0.000458 => Gls Tokens per Sec:     3342 || Batch Translation Loss:   0.008938 => Txt Tokens per Sec:     7325 || Lr: 0.000100
2024-02-07 06:09:14,753 Epoch 2934: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 06:09:14,753 EPOCH 2935
2024-02-07 06:09:30,989 Epoch 2935: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 06:09:30,989 EPOCH 2936
2024-02-07 06:09:47,509 Epoch 2936: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 06:09:47,510 EPOCH 2937
2024-02-07 06:10:03,645 Epoch 2937: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 06:10:03,645 EPOCH 2938
2024-02-07 06:10:20,581 Epoch 2938: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 06:10:20,582 EPOCH 2939
2024-02-07 06:10:37,452 Epoch 2939: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 06:10:37,453 EPOCH 2940
2024-02-07 06:10:53,866 Epoch 2940: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 06:10:53,867 EPOCH 2941
2024-02-07 06:11:10,230 Epoch 2941: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 06:11:10,231 EPOCH 2942
2024-02-07 06:11:26,650 Epoch 2942: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 06:11:26,651 EPOCH 2943
2024-02-07 06:11:43,485 Epoch 2943: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 06:11:43,486 EPOCH 2944
2024-02-07 06:11:59,836 Epoch 2944: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 06:11:59,836 EPOCH 2945
2024-02-07 06:12:08,534 [Epoch: 2945 Step: 00026500] Batch Recognition Loss:   0.001049 => Gls Tokens per Sec:      485 || Batch Translation Loss:   0.016550 => Txt Tokens per Sec:     1500 || Lr: 0.000100
2024-02-07 06:12:15,998 Epoch 2945: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 06:12:15,998 EPOCH 2946
2024-02-07 06:12:32,820 Epoch 2946: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 06:12:32,820 EPOCH 2947
2024-02-07 06:12:49,382 Epoch 2947: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 06:12:49,383 EPOCH 2948
2024-02-07 06:13:05,918 Epoch 2948: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 06:13:05,919 EPOCH 2949
2024-02-07 06:13:22,706 Epoch 2949: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 06:13:22,706 EPOCH 2950
2024-02-07 06:13:39,158 Epoch 2950: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 06:13:39,158 EPOCH 2951
2024-02-07 06:13:55,664 Epoch 2951: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 06:13:55,665 EPOCH 2952
2024-02-07 06:14:12,065 Epoch 2952: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 06:14:12,065 EPOCH 2953
2024-02-07 06:14:28,545 Epoch 2953: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 06:14:28,545 EPOCH 2954
2024-02-07 06:14:44,934 Epoch 2954: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 06:14:44,935 EPOCH 2955
2024-02-07 06:15:01,562 Epoch 2955: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 06:15:01,563 EPOCH 2956
2024-02-07 06:15:10,834 [Epoch: 2956 Step: 00026600] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:      593 || Batch Translation Loss:   0.026280 => Txt Tokens per Sec:     1737 || Lr: 0.000100
2024-02-07 06:15:18,029 Epoch 2956: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 06:15:18,029 EPOCH 2957
2024-02-07 06:15:34,735 Epoch 2957: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 06:15:34,736 EPOCH 2958
2024-02-07 06:15:51,252 Epoch 2958: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 06:15:51,252 EPOCH 2959
2024-02-07 06:16:07,544 Epoch 2959: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 06:16:07,545 EPOCH 2960
2024-02-07 06:16:23,805 Epoch 2960: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 06:16:23,806 EPOCH 2961
2024-02-07 06:16:40,258 Epoch 2961: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 06:16:40,259 EPOCH 2962
2024-02-07 06:16:56,501 Epoch 2962: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-07 06:16:56,501 EPOCH 2963
2024-02-07 06:17:13,008 Epoch 2963: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-07 06:17:13,008 EPOCH 2964
2024-02-07 06:17:29,536 Epoch 2964: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-07 06:17:29,537 EPOCH 2965
2024-02-07 06:17:46,073 Epoch 2965: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-07 06:17:46,073 EPOCH 2966
2024-02-07 06:18:02,384 Epoch 2966: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-07 06:18:02,384 EPOCH 2967
2024-02-07 06:18:07,537 [Epoch: 2967 Step: 00026700] Batch Recognition Loss:   0.000385 => Gls Tokens per Sec:     1491 || Batch Translation Loss:   0.055553 => Txt Tokens per Sec:     3835 || Lr: 0.000100
2024-02-07 06:18:18,796 Epoch 2967: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-07 06:18:18,796 EPOCH 2968
2024-02-07 06:18:35,194 Epoch 2968: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-07 06:18:35,195 EPOCH 2969
2024-02-07 06:18:51,716 Epoch 2969: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-07 06:18:51,716 EPOCH 2970
2024-02-07 06:19:07,935 Epoch 2970: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-07 06:19:07,935 EPOCH 2971
2024-02-07 06:19:24,786 Epoch 2971: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-07 06:19:24,786 EPOCH 2972
2024-02-07 06:19:41,244 Epoch 2972: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-07 06:19:41,244 EPOCH 2973
2024-02-07 06:19:57,538 Epoch 2973: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 06:19:57,539 EPOCH 2974
2024-02-07 06:20:13,734 Epoch 2974: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 06:20:13,735 EPOCH 2975
2024-02-07 06:20:30,454 Epoch 2975: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-07 06:20:30,454 EPOCH 2976
2024-02-07 06:20:46,892 Epoch 2976: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 06:20:46,893 EPOCH 2977
2024-02-07 06:21:03,266 Epoch 2977: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 06:21:03,266 EPOCH 2978
2024-02-07 06:21:14,939 [Epoch: 2978 Step: 00026800] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:      768 || Batch Translation Loss:   0.017582 => Txt Tokens per Sec:     2100 || Lr: 0.000100
2024-02-07 06:21:19,735 Epoch 2978: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 06:21:19,735 EPOCH 2979
2024-02-07 06:21:36,160 Epoch 2979: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 06:21:36,161 EPOCH 2980
2024-02-07 06:21:52,533 Epoch 2980: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 06:21:52,533 EPOCH 2981
2024-02-07 06:22:09,268 Epoch 2981: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 06:22:09,268 EPOCH 2982
2024-02-07 06:22:25,841 Epoch 2982: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 06:22:25,842 EPOCH 2983
2024-02-07 06:22:42,245 Epoch 2983: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 06:22:42,245 EPOCH 2984
2024-02-07 06:22:58,628 Epoch 2984: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 06:22:58,628 EPOCH 2985
2024-02-07 06:23:15,319 Epoch 2985: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 06:23:15,320 EPOCH 2986
2024-02-07 06:23:31,870 Epoch 2986: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 06:23:31,870 EPOCH 2987
2024-02-07 06:23:48,244 Epoch 2987: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 06:23:48,244 EPOCH 2988
2024-02-07 06:24:04,520 Epoch 2988: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 06:24:04,520 EPOCH 2989
2024-02-07 06:24:20,220 [Epoch: 2989 Step: 00026900] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:      595 || Batch Translation Loss:   0.031176 => Txt Tokens per Sec:     1647 || Lr: 0.000100
2024-02-07 06:24:20,852 Epoch 2989: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 06:24:20,853 EPOCH 2990
2024-02-07 06:24:37,351 Epoch 2990: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.19 
2024-02-07 06:24:37,352 EPOCH 2991
2024-02-07 06:24:53,996 Epoch 2991: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.21 
2024-02-07 06:24:53,998 EPOCH 2992
2024-02-07 06:25:10,297 Epoch 2992: Total Training Recognition Loss 0.89  Total Training Translation Loss 0.24 
2024-02-07 06:25:10,298 EPOCH 2993
2024-02-07 06:25:26,965 Epoch 2993: Total Training Recognition Loss 2.56  Total Training Translation Loss 0.38 
2024-02-07 06:25:26,966 EPOCH 2994
2024-02-07 06:25:43,401 Epoch 2994: Total Training Recognition Loss 2.41  Total Training Translation Loss 0.59 
2024-02-07 06:25:43,402 EPOCH 2995
2024-02-07 06:25:59,673 Epoch 2995: Total Training Recognition Loss 1.15  Total Training Translation Loss 0.65 
2024-02-07 06:25:59,674 EPOCH 2996
2024-02-07 06:26:16,395 Epoch 2996: Total Training Recognition Loss 0.61  Total Training Translation Loss 0.72 
2024-02-07 06:26:16,396 EPOCH 2997
2024-02-07 06:26:32,883 Epoch 2997: Total Training Recognition Loss 0.45  Total Training Translation Loss 0.70 
2024-02-07 06:26:32,883 EPOCH 2998
2024-02-07 06:26:49,423 Epoch 2998: Total Training Recognition Loss 0.14  Total Training Translation Loss 0.81 
2024-02-07 06:26:49,424 EPOCH 2999
2024-02-07 06:27:05,689 Epoch 2999: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.79 
2024-02-07 06:27:05,690 EPOCH 3000
2024-02-07 06:27:22,318 [Epoch: 3000 Step: 00027000] Batch Recognition Loss:   0.004085 => Gls Tokens per Sec:      639 || Batch Translation Loss:   0.035981 => Txt Tokens per Sec:     1767 || Lr: 0.000100
2024-02-07 06:27:22,320 Epoch 3000: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-07 06:27:22,320 EPOCH 3001
2024-02-07 06:27:38,588 Epoch 3001: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-07 06:27:38,589 EPOCH 3002
2024-02-07 06:27:55,253 Epoch 3002: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-07 06:27:55,253 EPOCH 3003
2024-02-07 06:28:11,547 Epoch 3003: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-07 06:28:11,547 EPOCH 3004
2024-02-07 06:28:28,111 Epoch 3004: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.44 
2024-02-07 06:28:28,112 EPOCH 3005
2024-02-07 06:28:44,657 Epoch 3005: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.41 
2024-02-07 06:28:44,658 EPOCH 3006
2024-02-07 06:29:01,029 Epoch 3006: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.44 
2024-02-07 06:29:01,029 EPOCH 3007
2024-02-07 06:29:17,684 Epoch 3007: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-07 06:29:17,684 EPOCH 3008
2024-02-07 06:29:34,322 Epoch 3008: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-07 06:29:34,323 EPOCH 3009
2024-02-07 06:29:50,918 Epoch 3009: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-07 06:29:50,918 EPOCH 3010
2024-02-07 06:30:07,567 Epoch 3010: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-07 06:30:07,567 EPOCH 3011
2024-02-07 06:30:23,874 Epoch 3011: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-07 06:30:23,875 EPOCH 3012
2024-02-07 06:30:24,386 [Epoch: 3012 Step: 00027100] Batch Recognition Loss:   0.004070 => Gls Tokens per Sec:     2513 || Batch Translation Loss:   0.028085 => Txt Tokens per Sec:     6925 || Lr: 0.000100
2024-02-07 06:30:40,829 Epoch 3012: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 06:30:40,830 EPOCH 3013
2024-02-07 06:30:57,326 Epoch 3013: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 06:30:57,327 EPOCH 3014
2024-02-07 06:31:13,755 Epoch 3014: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 06:31:13,756 EPOCH 3015
2024-02-07 06:31:30,205 Epoch 3015: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 06:31:30,205 EPOCH 3016
2024-02-07 06:31:46,864 Epoch 3016: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-07 06:31:46,864 EPOCH 3017
2024-02-07 06:32:03,268 Epoch 3017: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 06:32:03,269 EPOCH 3018
2024-02-07 06:32:20,028 Epoch 3018: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 06:32:20,028 EPOCH 3019
2024-02-07 06:32:36,287 Epoch 3019: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 06:32:36,288 EPOCH 3020
2024-02-07 06:32:52,534 Epoch 3020: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 06:32:52,534 EPOCH 3021
2024-02-07 06:33:08,773 Epoch 3021: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 06:33:08,773 EPOCH 3022
2024-02-07 06:33:24,944 Epoch 3022: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 06:33:24,944 EPOCH 3023
2024-02-07 06:33:25,728 [Epoch: 3023 Step: 00027200] Batch Recognition Loss:   0.002523 => Gls Tokens per Sec:     3269 || Batch Translation Loss:   0.009233 => Txt Tokens per Sec:     7654 || Lr: 0.000100
2024-02-07 06:33:41,527 Epoch 3023: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 06:33:41,528 EPOCH 3024
2024-02-07 06:33:57,810 Epoch 3024: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 06:33:57,811 EPOCH 3025
2024-02-07 06:34:14,618 Epoch 3025: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 06:34:14,619 EPOCH 3026
2024-02-07 06:34:30,706 Epoch 3026: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 06:34:30,707 EPOCH 3027
2024-02-07 06:34:47,560 Epoch 3027: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 06:34:47,561 EPOCH 3028
2024-02-07 06:35:03,835 Epoch 3028: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 06:35:03,836 EPOCH 3029
2024-02-07 06:35:20,169 Epoch 3029: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 06:35:20,170 EPOCH 3030
2024-02-07 06:35:36,306 Epoch 3030: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 06:35:36,307 EPOCH 3031
2024-02-07 06:35:52,700 Epoch 3031: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 06:35:52,700 EPOCH 3032
2024-02-07 06:36:09,156 Epoch 3032: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 06:36:09,156 EPOCH 3033
2024-02-07 06:36:25,626 Epoch 3033: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 06:36:25,627 EPOCH 3034
2024-02-07 06:36:32,584 [Epoch: 3034 Step: 00027300] Batch Recognition Loss:   0.000966 => Gls Tokens per Sec:      552 || Batch Translation Loss:   0.018822 => Txt Tokens per Sec:     1594 || Lr: 0.000100
2024-02-07 06:36:42,066 Epoch 3034: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 06:36:42,066 EPOCH 3035
2024-02-07 06:36:58,250 Epoch 3035: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 06:36:58,251 EPOCH 3036
2024-02-07 06:37:14,906 Epoch 3036: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 06:37:14,906 EPOCH 3037
2024-02-07 06:37:31,248 Epoch 3037: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 06:37:31,249 EPOCH 3038
2024-02-07 06:37:47,974 Epoch 3038: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 06:37:47,974 EPOCH 3039
2024-02-07 06:38:04,898 Epoch 3039: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-07 06:38:04,898 EPOCH 3040
2024-02-07 06:38:21,440 Epoch 3040: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 06:38:21,440 EPOCH 3041
2024-02-07 06:38:37,717 Epoch 3041: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 06:38:37,718 EPOCH 3042
2024-02-07 06:38:54,148 Epoch 3042: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 06:38:54,149 EPOCH 3043
2024-02-07 06:39:10,718 Epoch 3043: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 06:39:10,718 EPOCH 3044
2024-02-07 06:39:27,152 Epoch 3044: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 06:39:27,153 EPOCH 3045
2024-02-07 06:39:34,872 [Epoch: 3045 Step: 00027400] Batch Recognition Loss:   0.000116 => Gls Tokens per Sec:      663 || Batch Translation Loss:   0.015071 => Txt Tokens per Sec:     1955 || Lr: 0.000100
2024-02-07 06:39:43,472 Epoch 3045: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 06:39:43,472 EPOCH 3046
2024-02-07 06:40:00,168 Epoch 3046: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 06:40:00,169 EPOCH 3047
2024-02-07 06:40:16,361 Epoch 3047: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 06:40:16,362 EPOCH 3048
2024-02-07 06:40:32,593 Epoch 3048: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 06:40:32,594 EPOCH 3049
2024-02-07 06:40:49,291 Epoch 3049: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 06:40:49,292 EPOCH 3050
2024-02-07 06:41:05,922 Epoch 3050: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 06:41:05,922 EPOCH 3051
2024-02-07 06:41:22,411 Epoch 3051: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 06:41:22,411 EPOCH 3052
2024-02-07 06:41:39,400 Epoch 3052: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 06:41:39,400 EPOCH 3053
2024-02-07 06:41:55,839 Epoch 3053: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 06:41:55,840 EPOCH 3054
2024-02-07 06:42:12,085 Epoch 3054: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 06:42:12,085 EPOCH 3055
2024-02-07 06:42:28,777 Epoch 3055: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 06:42:28,778 EPOCH 3056
2024-02-07 06:42:43,792 [Epoch: 3056 Step: 00027500] Batch Recognition Loss:   0.000232 => Gls Tokens per Sec:      366 || Batch Translation Loss:   0.019460 => Txt Tokens per Sec:     1161 || Lr: 0.000100
2024-02-07 06:42:45,233 Epoch 3056: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 06:42:45,234 EPOCH 3057
2024-02-07 06:43:01,607 Epoch 3057: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 06:43:01,608 EPOCH 3058
2024-02-07 06:43:17,921 Epoch 3058: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 06:43:17,922 EPOCH 3059
2024-02-07 06:43:34,136 Epoch 3059: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 06:43:34,137 EPOCH 3060
2024-02-07 06:43:50,874 Epoch 3060: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 06:43:50,875 EPOCH 3061
2024-02-07 06:44:07,106 Epoch 3061: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-07 06:44:07,107 EPOCH 3062
2024-02-07 06:44:23,609 Epoch 3062: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.90 
2024-02-07 06:44:23,609 EPOCH 3063
2024-02-07 06:44:39,984 Epoch 3063: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.55 
2024-02-07 06:44:39,984 EPOCH 3064
2024-02-07 06:44:56,465 Epoch 3064: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.36 
2024-02-07 06:44:56,466 EPOCH 3065
2024-02-07 06:45:12,719 Epoch 3065: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.88 
2024-02-07 06:45:12,719 EPOCH 3066
2024-02-07 06:45:29,089 Epoch 3066: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.84 
2024-02-07 06:45:29,090 EPOCH 3067
2024-02-07 06:45:44,065 [Epoch: 3067 Step: 00027600] Batch Recognition Loss:   0.000840 => Gls Tokens per Sec:      453 || Batch Translation Loss:   0.138486 => Txt Tokens per Sec:     1323 || Lr: 0.000100
2024-02-07 06:45:45,313 Epoch 3067: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-07 06:45:45,313 EPOCH 3068
2024-02-07 06:46:01,607 Epoch 3068: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-07 06:46:01,607 EPOCH 3069
2024-02-07 06:46:17,957 Epoch 3069: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-07 06:46:17,957 EPOCH 3070
2024-02-07 06:46:34,605 Epoch 3070: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-07 06:46:34,606 EPOCH 3071
2024-02-07 06:46:51,068 Epoch 3071: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-07 06:46:51,069 EPOCH 3072
2024-02-07 06:47:07,656 Epoch 3072: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-07 06:47:07,657 EPOCH 3073
2024-02-07 06:47:23,699 Epoch 3073: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-07 06:47:23,699 EPOCH 3074
2024-02-07 06:47:40,294 Epoch 3074: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-07 06:47:40,294 EPOCH 3075
2024-02-07 06:47:56,783 Epoch 3075: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-07 06:47:56,784 EPOCH 3076
2024-02-07 06:48:13,093 Epoch 3076: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-07 06:48:13,094 EPOCH 3077
2024-02-07 06:48:29,820 Epoch 3077: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-07 06:48:29,821 EPOCH 3078
2024-02-07 06:48:44,981 [Epoch: 3078 Step: 00027700] Batch Recognition Loss:   0.000792 => Gls Tokens per Sec:      532 || Batch Translation Loss:   0.037479 => Txt Tokens per Sec:     1494 || Lr: 0.000100
2024-02-07 06:48:46,202 Epoch 3078: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-07 06:48:46,202 EPOCH 3079
2024-02-07 06:49:02,521 Epoch 3079: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-07 06:49:02,522 EPOCH 3080
2024-02-07 06:49:18,972 Epoch 3080: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-07 06:49:18,972 EPOCH 3081
2024-02-07 06:49:35,579 Epoch 3081: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-07 06:49:35,579 EPOCH 3082
2024-02-07 06:49:51,821 Epoch 3082: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-07 06:49:51,822 EPOCH 3083
2024-02-07 06:50:08,309 Epoch 3083: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 06:50:08,309 EPOCH 3084
2024-02-07 06:50:24,578 Epoch 3084: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-07 06:50:24,579 EPOCH 3085
2024-02-07 06:50:40,891 Epoch 3085: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 06:50:40,892 EPOCH 3086
2024-02-07 06:50:57,217 Epoch 3086: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-07 06:50:57,218 EPOCH 3087
2024-02-07 06:51:13,738 Epoch 3087: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-07 06:51:13,738 EPOCH 3088
2024-02-07 06:51:30,086 Epoch 3088: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 06:51:30,087 EPOCH 3089
2024-02-07 06:51:43,500 [Epoch: 3089 Step: 00027800] Batch Recognition Loss:   0.000280 => Gls Tokens per Sec:      696 || Batch Translation Loss:   0.027234 => Txt Tokens per Sec:     1884 || Lr: 0.000100
2024-02-07 06:51:46,826 Epoch 3089: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 06:51:46,826 EPOCH 3090
2024-02-07 06:52:03,540 Epoch 3090: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 06:52:03,541 EPOCH 3091
2024-02-07 06:52:20,105 Epoch 3091: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-07 06:52:20,106 EPOCH 3092
2024-02-07 06:52:36,734 Epoch 3092: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 06:52:36,735 EPOCH 3093
2024-02-07 06:52:53,399 Epoch 3093: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 06:52:53,400 EPOCH 3094
2024-02-07 06:53:09,802 Epoch 3094: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 06:53:09,803 EPOCH 3095
2024-02-07 06:53:26,092 Epoch 3095: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 06:53:26,093 EPOCH 3096
2024-02-07 06:53:42,611 Epoch 3096: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 06:53:42,611 EPOCH 3097
2024-02-07 06:53:58,895 Epoch 3097: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 06:53:58,895 EPOCH 3098
2024-02-07 06:54:15,042 Epoch 3098: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 06:54:15,042 EPOCH 3099
2024-02-07 06:54:31,738 Epoch 3099: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 06:54:31,739 EPOCH 3100
2024-02-07 06:54:47,877 [Epoch: 3100 Step: 00027900] Batch Recognition Loss:   0.000759 => Gls Tokens per Sec:      658 || Batch Translation Loss:   0.018865 => Txt Tokens per Sec:     1821 || Lr: 0.000100
2024-02-07 06:54:47,878 Epoch 3100: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 06:54:47,878 EPOCH 3101
2024-02-07 06:55:04,718 Epoch 3101: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 06:55:04,718 EPOCH 3102
2024-02-07 06:55:21,310 Epoch 3102: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 06:55:21,310 EPOCH 3103
2024-02-07 06:55:37,383 Epoch 3103: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 06:55:37,384 EPOCH 3104
2024-02-07 06:55:54,102 Epoch 3104: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 06:55:54,102 EPOCH 3105
2024-02-07 06:56:10,469 Epoch 3105: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 06:56:10,470 EPOCH 3106
2024-02-07 06:56:27,060 Epoch 3106: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 06:56:27,060 EPOCH 3107
2024-02-07 06:56:43,607 Epoch 3107: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 06:56:43,607 EPOCH 3108
2024-02-07 06:56:59,911 Epoch 3108: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 06:56:59,912 EPOCH 3109
2024-02-07 06:57:16,676 Epoch 3109: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 06:57:16,676 EPOCH 3110
2024-02-07 06:57:33,069 Epoch 3110: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 06:57:33,070 EPOCH 3111
2024-02-07 06:57:49,087 Epoch 3111: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 06:57:49,087 EPOCH 3112
2024-02-07 06:57:49,721 [Epoch: 3112 Step: 00028000] Batch Recognition Loss:   0.000288 => Gls Tokens per Sec:     2025 || Batch Translation Loss:   0.013830 => Txt Tokens per Sec:     5524 || Lr: 0.000100
2024-02-07 06:58:57,976 Validation result at epoch 3112, step    28000: duration: 68.2544s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.29291	Translation Loss: 95479.87500	PPL: 13856.95898
	Eval Metric: BLEU
	WER 2.82	(DEL: 0.00,	INS: 0.00,	SUB: 2.82)
	BLEU-4 0.41	(BLEU-1: 11.30,	BLEU-2: 3.10,	BLEU-3: 0.99,	BLEU-4: 0.41)
	CHRF 17.28	ROUGE 9.21
2024-02-07 06:58:57,978 Logging Recognition and Translation Outputs
2024-02-07 06:58:57,978 ========================================================================================================================
2024-02-07 06:58:57,978 Logging Sequence: 156_288.00
2024-02-07 06:58:57,978 	Gloss Reference :	A B+C+D+E
2024-02-07 06:58:57,978 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 06:58:57,979 	Gloss Alignment :	         
2024-02-07 06:58:57,979 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 06:58:57,981 	Text Reference  :	pooran led  the ***** ******* team  to  victory miny became winners of the 1st        season
2024-02-07 06:58:57,982 	Text Hypothesis :	****** both the match between india and kuwait  at   the    end     of the regulation time  
2024-02-07 06:58:57,982 	Text Alignment  :	D      S        I     I       S     S   S       S    S      S              S          S     
2024-02-07 06:58:57,982 ========================================================================================================================
2024-02-07 06:58:57,982 Logging Sequence: 98_135.00
2024-02-07 06:58:57,983 	Gloss Reference :	A B+C+D+E
2024-02-07 06:58:57,983 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 06:58:57,983 	Gloss Alignment :	         
2024-02-07 06:58:57,983 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 06:58:57,984 	Text Reference  :	however due to the rise in   coronavirus cases the tournament ***** was shifted
2024-02-07 06:58:57,984 	Text Hypothesis :	******* *** ** the **** bcci can         not   the tournament after the series 
2024-02-07 06:58:57,984 	Text Alignment  :	D       D   D      D    S    S           S                    I     S   S      
2024-02-07 06:58:57,985 ========================================================================================================================
2024-02-07 06:58:57,985 Logging Sequence: 161_47.00
2024-02-07 06:58:57,985 	Gloss Reference :	A B+C+D+E
2024-02-07 06:58:57,985 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 06:58:57,985 	Gloss Alignment :	         
2024-02-07 06:58:57,985 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 06:58:57,986 	Text Reference  :	he requested confidentiality as      he was planning to    make  an official announcement
2024-02-07 06:58:57,987 	Text Hypothesis :	** ********* *************** because of the for      rana' death a  court    marriage    
2024-02-07 06:58:57,987 	Text Alignment  :	D  D         D               S       S  S   S        S     S     S  S        S           
2024-02-07 06:58:57,987 ========================================================================================================================
2024-02-07 06:58:57,987 Logging Sequence: 131_159.00
2024-02-07 06:58:57,987 	Gloss Reference :	A B+C+D+E
2024-02-07 06:58:57,987 	Gloss Hypothesis:	A B+C+D  
2024-02-07 06:58:57,988 	Gloss Alignment :	  S      
2024-02-07 06:58:57,988 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 06:58:57,989 	Text Reference  :	chanu also met biren singh following the     meeting  singh   described chanu       as    our nation' pride     
2024-02-07 06:58:57,989 	Text Hypothesis :	***** **** *** ***** the   taliban   swiftly regained control of        afghanistan after us  troops  withdrawal
2024-02-07 06:58:57,989 	Text Alignment  :	D     D    D   D     S     S         S       S        S       S         S           S     S   S       S         
2024-02-07 06:58:57,989 ========================================================================================================================
2024-02-07 06:58:57,990 Logging Sequence: 137_167.00
2024-02-07 06:58:57,990 	Gloss Reference :	A B+C+D+E
2024-02-07 06:58:57,990 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 06:58:57,990 	Gloss Alignment :	         
2024-02-07 06:58:57,990 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 06:58:57,991 	Text Reference  :	however after 630 pm there will be certain fan zones where beer   will be  available and nowhere else   
2024-02-07 06:58:57,991 	Text Hypothesis :	******* ***** *** ** ***** **** ** ******* *** ***** the   indian team was disgusted by  the     threats
2024-02-07 06:58:57,992 	Text Alignment  :	D       D     D   D  D     D    D  D       D   D     S     S      S    S   S         S   S       S      
2024-02-07 06:58:57,992 ========================================================================================================================
2024-02-07 06:59:14,894 Epoch 3112: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 06:59:14,895 EPOCH 3113
2024-02-07 06:59:31,340 Epoch 3113: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 06:59:31,340 EPOCH 3114
2024-02-07 06:59:47,952 Epoch 3114: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 06:59:47,953 EPOCH 3115
2024-02-07 07:00:04,540 Epoch 3115: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:00:04,540 EPOCH 3116
2024-02-07 07:00:20,616 Epoch 3116: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:00:20,617 EPOCH 3117
2024-02-07 07:00:37,621 Epoch 3117: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 07:00:37,621 EPOCH 3118
2024-02-07 07:00:54,089 Epoch 3118: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 07:00:54,089 EPOCH 3119
2024-02-07 07:01:10,516 Epoch 3119: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 07:01:10,516 EPOCH 3120
2024-02-07 07:01:27,341 Epoch 3120: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 07:01:27,342 EPOCH 3121
2024-02-07 07:01:43,645 Epoch 3121: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 07:01:43,645 EPOCH 3122
2024-02-07 07:02:00,312 Epoch 3122: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 07:02:00,313 EPOCH 3123
2024-02-07 07:02:01,401 [Epoch: 3123 Step: 00028100] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:     2353 || Batch Translation Loss:   0.017004 => Txt Tokens per Sec:     6562 || Lr: 0.000100
2024-02-07 07:02:16,832 Epoch 3123: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:02:16,833 EPOCH 3124
2024-02-07 07:02:33,197 Epoch 3124: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 07:02:33,197 EPOCH 3125
2024-02-07 07:02:49,955 Epoch 3125: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 07:02:49,956 EPOCH 3126
2024-02-07 07:03:06,332 Epoch 3126: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 07:03:06,333 EPOCH 3127
2024-02-07 07:03:22,888 Epoch 3127: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 07:03:22,889 EPOCH 3128
2024-02-07 07:03:39,314 Epoch 3128: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 07:03:39,315 EPOCH 3129
2024-02-07 07:03:55,750 Epoch 3129: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:03:55,751 EPOCH 3130
2024-02-07 07:04:12,327 Epoch 3130: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 07:04:12,328 EPOCH 3131
2024-02-07 07:04:28,695 Epoch 3131: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:04:28,696 EPOCH 3132
2024-02-07 07:04:45,484 Epoch 3132: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 07:04:45,484 EPOCH 3133
2024-02-07 07:05:01,858 Epoch 3133: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 07:05:01,858 EPOCH 3134
2024-02-07 07:05:09,045 [Epoch: 3134 Step: 00028200] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:      534 || Batch Translation Loss:   0.013940 => Txt Tokens per Sec:     1470 || Lr: 0.000100
2024-02-07 07:05:18,398 Epoch 3134: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 07:05:18,399 EPOCH 3135
2024-02-07 07:05:34,690 Epoch 3135: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 07:05:34,691 EPOCH 3136
2024-02-07 07:05:50,876 Epoch 3136: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 07:05:50,877 EPOCH 3137
2024-02-07 07:06:07,432 Epoch 3137: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:06:07,433 EPOCH 3138
2024-02-07 07:06:23,876 Epoch 3138: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 07:06:23,877 EPOCH 3139
2024-02-07 07:06:39,864 Epoch 3139: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 07:06:39,864 EPOCH 3140
2024-02-07 07:06:56,256 Epoch 3140: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 07:06:56,257 EPOCH 3141
2024-02-07 07:07:12,670 Epoch 3141: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 07:07:12,670 EPOCH 3142
2024-02-07 07:07:29,217 Epoch 3142: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 07:07:29,218 EPOCH 3143
2024-02-07 07:07:45,625 Epoch 3143: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 07:07:45,626 EPOCH 3144
2024-02-07 07:08:02,098 Epoch 3144: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:08:02,099 EPOCH 3145
2024-02-07 07:08:10,242 [Epoch: 3145 Step: 00028300] Batch Recognition Loss:   0.000818 => Gls Tokens per Sec:      518 || Batch Translation Loss:   0.010886 => Txt Tokens per Sec:     1423 || Lr: 0.000100
2024-02-07 07:08:18,582 Epoch 3145: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:08:18,583 EPOCH 3146
2024-02-07 07:08:34,979 Epoch 3146: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:08:34,979 EPOCH 3147
2024-02-07 07:08:51,404 Epoch 3147: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:08:51,405 EPOCH 3148
2024-02-07 07:09:07,726 Epoch 3148: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:09:07,726 EPOCH 3149
2024-02-07 07:09:24,167 Epoch 3149: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:09:24,167 EPOCH 3150
2024-02-07 07:09:40,565 Epoch 3150: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 07:09:40,566 EPOCH 3151
2024-02-07 07:09:57,052 Epoch 3151: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:09:57,053 EPOCH 3152
2024-02-07 07:10:13,588 Epoch 3152: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:10:13,588 EPOCH 3153
2024-02-07 07:10:30,151 Epoch 3153: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:10:30,152 EPOCH 3154
2024-02-07 07:10:46,555 Epoch 3154: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:10:46,556 EPOCH 3155
2024-02-07 07:11:03,004 Epoch 3155: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:11:03,005 EPOCH 3156
2024-02-07 07:11:12,117 [Epoch: 3156 Step: 00028400] Batch Recognition Loss:   0.000314 => Gls Tokens per Sec:      604 || Batch Translation Loss:   0.009724 => Txt Tokens per Sec:     1684 || Lr: 0.000100
2024-02-07 07:11:19,631 Epoch 3156: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:11:19,631 EPOCH 3157
2024-02-07 07:11:36,058 Epoch 3157: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 07:11:36,059 EPOCH 3158
2024-02-07 07:11:52,611 Epoch 3158: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 07:11:52,612 EPOCH 3159
2024-02-07 07:12:08,980 Epoch 3159: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 07:12:08,981 EPOCH 3160
2024-02-07 07:12:25,416 Epoch 3160: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 07:12:25,417 EPOCH 3161
2024-02-07 07:12:41,763 Epoch 3161: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-07 07:12:41,764 EPOCH 3162
2024-02-07 07:12:58,178 Epoch 3162: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-07 07:12:58,178 EPOCH 3163
2024-02-07 07:13:14,829 Epoch 3163: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-07 07:13:14,831 EPOCH 3164
2024-02-07 07:13:31,232 Epoch 3164: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-07 07:13:31,233 EPOCH 3165
2024-02-07 07:13:47,599 Epoch 3165: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-07 07:13:47,599 EPOCH 3166
2024-02-07 07:14:04,088 Epoch 3166: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.82 
2024-02-07 07:14:04,089 EPOCH 3167
2024-02-07 07:14:16,741 [Epoch: 3167 Step: 00028500] Batch Recognition Loss:   0.002808 => Gls Tokens per Sec:      536 || Batch Translation Loss:   0.706959 => Txt Tokens per Sec:     1562 || Lr: 0.000100
2024-02-07 07:14:20,617 Epoch 3167: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.33 
2024-02-07 07:14:20,618 EPOCH 3168
2024-02-07 07:14:36,927 Epoch 3168: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.59 
2024-02-07 07:14:36,928 EPOCH 3169
2024-02-07 07:14:53,310 Epoch 3169: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.61 
2024-02-07 07:14:53,311 EPOCH 3170
2024-02-07 07:15:09,646 Epoch 3170: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.33 
2024-02-07 07:15:09,646 EPOCH 3171
2024-02-07 07:15:26,139 Epoch 3171: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.22 
2024-02-07 07:15:26,140 EPOCH 3172
2024-02-07 07:15:42,587 Epoch 3172: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.84 
2024-02-07 07:15:42,588 EPOCH 3173
2024-02-07 07:15:58,574 Epoch 3173: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-07 07:15:58,575 EPOCH 3174
2024-02-07 07:16:14,885 Epoch 3174: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-07 07:16:14,886 EPOCH 3175
2024-02-07 07:16:31,291 Epoch 3175: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-07 07:16:31,291 EPOCH 3176
2024-02-07 07:16:47,785 Epoch 3176: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-07 07:16:47,785 EPOCH 3177
2024-02-07 07:17:04,113 Epoch 3177: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-07 07:17:04,113 EPOCH 3178
2024-02-07 07:17:13,741 [Epoch: 3178 Step: 00028600] Batch Recognition Loss:   0.000461 => Gls Tokens per Sec:      837 || Batch Translation Loss:   0.043048 => Txt Tokens per Sec:     2209 || Lr: 0.000100
2024-02-07 07:17:20,532 Epoch 3178: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-07 07:17:20,533 EPOCH 3179
2024-02-07 07:17:37,011 Epoch 3179: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-07 07:17:37,012 EPOCH 3180
2024-02-07 07:17:53,379 Epoch 3180: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 07:17:53,379 EPOCH 3181
2024-02-07 07:18:09,905 Epoch 3181: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-07 07:18:09,906 EPOCH 3182
2024-02-07 07:18:26,157 Epoch 3182: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 07:18:26,157 EPOCH 3183
2024-02-07 07:18:42,602 Epoch 3183: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 07:18:42,603 EPOCH 3184
2024-02-07 07:18:58,888 Epoch 3184: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 07:18:58,889 EPOCH 3185
2024-02-07 07:19:15,623 Epoch 3185: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 07:19:15,624 EPOCH 3186
2024-02-07 07:19:32,010 Epoch 3186: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 07:19:32,010 EPOCH 3187
2024-02-07 07:19:48,307 Epoch 3187: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-07 07:19:48,307 EPOCH 3188
2024-02-07 07:20:04,780 Epoch 3188: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-07 07:20:04,781 EPOCH 3189
2024-02-07 07:20:14,821 [Epoch: 3189 Step: 00028700] Batch Recognition Loss:   0.000248 => Gls Tokens per Sec:      930 || Batch Translation Loss:   0.017927 => Txt Tokens per Sec:     2489 || Lr: 0.000100
2024-02-07 07:20:20,825 Epoch 3189: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-07 07:20:20,826 EPOCH 3190
2024-02-07 07:20:37,441 Epoch 3190: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 07:20:37,442 EPOCH 3191
2024-02-07 07:20:54,257 Epoch 3191: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 07:20:54,258 EPOCH 3192
2024-02-07 07:21:10,832 Epoch 3192: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 07:21:10,833 EPOCH 3193
2024-02-07 07:21:27,488 Epoch 3193: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 07:21:27,489 EPOCH 3194
2024-02-07 07:21:44,195 Epoch 3194: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 07:21:44,195 EPOCH 3195
2024-02-07 07:22:00,597 Epoch 3195: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 07:22:00,598 EPOCH 3196
2024-02-07 07:22:16,760 Epoch 3196: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 07:22:16,761 EPOCH 3197
2024-02-07 07:22:33,366 Epoch 3197: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 07:22:33,367 EPOCH 3198
2024-02-07 07:22:49,929 Epoch 3198: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 07:22:49,929 EPOCH 3199
2024-02-07 07:23:06,185 Epoch 3199: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 07:23:06,186 EPOCH 3200
2024-02-07 07:23:22,527 [Epoch: 3200 Step: 00028800] Batch Recognition Loss:   0.000409 => Gls Tokens per Sec:      650 || Batch Translation Loss:   0.019621 => Txt Tokens per Sec:     1798 || Lr: 0.000100
2024-02-07 07:23:22,527 Epoch 3200: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 07:23:22,527 EPOCH 3201
2024-02-07 07:23:39,099 Epoch 3201: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:23:39,100 EPOCH 3202
2024-02-07 07:23:55,507 Epoch 3202: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:23:55,508 EPOCH 3203
2024-02-07 07:24:11,807 Epoch 3203: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:24:11,807 EPOCH 3204
2024-02-07 07:24:28,570 Epoch 3204: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:24:28,571 EPOCH 3205
2024-02-07 07:24:44,698 Epoch 3205: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:24:44,699 EPOCH 3206
2024-02-07 07:25:01,143 Epoch 3206: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:25:01,143 EPOCH 3207
2024-02-07 07:25:17,908 Epoch 3207: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:25:17,908 EPOCH 3208
2024-02-07 07:25:34,247 Epoch 3208: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:25:34,247 EPOCH 3209
2024-02-07 07:25:50,738 Epoch 3209: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 07:25:50,739 EPOCH 3210
2024-02-07 07:26:07,376 Epoch 3210: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:26:07,377 EPOCH 3211
2024-02-07 07:26:23,732 Epoch 3211: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:26:23,733 EPOCH 3212
2024-02-07 07:26:24,598 [Epoch: 3212 Step: 00028900] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:     1481 || Batch Translation Loss:   0.020647 => Txt Tokens per Sec:     4459 || Lr: 0.000100
2024-02-07 07:26:40,400 Epoch 3212: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:26:40,400 EPOCH 3213
2024-02-07 07:26:56,671 Epoch 3213: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:26:56,672 EPOCH 3214
2024-02-07 07:27:13,084 Epoch 3214: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:27:13,085 EPOCH 3215
2024-02-07 07:27:29,341 Epoch 3215: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:27:29,342 EPOCH 3216
2024-02-07 07:27:46,142 Epoch 3216: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:27:46,142 EPOCH 3217
2024-02-07 07:28:02,647 Epoch 3217: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:28:02,648 EPOCH 3218
2024-02-07 07:28:19,183 Epoch 3218: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:28:19,184 EPOCH 3219
2024-02-07 07:28:36,009 Epoch 3219: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 07:28:36,010 EPOCH 3220
2024-02-07 07:28:52,725 Epoch 3220: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:28:52,725 EPOCH 3221
2024-02-07 07:29:09,024 Epoch 3221: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:29:09,024 EPOCH 3222
2024-02-07 07:29:25,475 Epoch 3222: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:29:25,476 EPOCH 3223
2024-02-07 07:29:30,473 [Epoch: 3223 Step: 00029000] Batch Recognition Loss:   0.000163 => Gls Tokens per Sec:      332 || Batch Translation Loss:   0.022095 => Txt Tokens per Sec:     1054 || Lr: 0.000100
2024-02-07 07:29:41,885 Epoch 3223: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:29:41,886 EPOCH 3224
2024-02-07 07:29:58,029 Epoch 3224: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:29:58,030 EPOCH 3225
2024-02-07 07:30:14,627 Epoch 3225: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:30:14,627 EPOCH 3226
2024-02-07 07:30:31,360 Epoch 3226: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:30:31,361 EPOCH 3227
2024-02-07 07:30:47,839 Epoch 3227: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:30:47,840 EPOCH 3228
2024-02-07 07:31:04,592 Epoch 3228: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 07:31:04,593 EPOCH 3229
2024-02-07 07:31:20,707 Epoch 3229: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:31:20,708 EPOCH 3230
2024-02-07 07:31:37,326 Epoch 3230: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:31:37,326 EPOCH 3231
2024-02-07 07:31:53,503 Epoch 3231: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:31:53,503 EPOCH 3232
2024-02-07 07:32:09,926 Epoch 3232: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:32:09,927 EPOCH 3233
2024-02-07 07:32:26,396 Epoch 3233: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 07:32:26,396 EPOCH 3234
2024-02-07 07:32:30,656 [Epoch: 3234 Step: 00029100] Batch Recognition Loss:   0.000416 => Gls Tokens per Sec:      902 || Batch Translation Loss:   0.024073 => Txt Tokens per Sec:     2642 || Lr: 0.000100
2024-02-07 07:32:42,900 Epoch 3234: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 07:32:42,901 EPOCH 3235
2024-02-07 07:32:59,481 Epoch 3235: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 07:32:59,482 EPOCH 3236
2024-02-07 07:33:15,836 Epoch 3236: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 07:33:15,836 EPOCH 3237
2024-02-07 07:33:32,673 Epoch 3237: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-07 07:33:32,674 EPOCH 3238
2024-02-07 07:33:48,947 Epoch 3238: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-07 07:33:48,948 EPOCH 3239
2024-02-07 07:34:05,552 Epoch 3239: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-07 07:34:05,553 EPOCH 3240
2024-02-07 07:34:21,773 Epoch 3240: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-07 07:34:21,774 EPOCH 3241
2024-02-07 07:34:38,264 Epoch 3241: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-07 07:34:38,264 EPOCH 3242
2024-02-07 07:34:54,743 Epoch 3242: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-07 07:34:54,744 EPOCH 3243
2024-02-07 07:35:11,087 Epoch 3243: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-07 07:35:11,088 EPOCH 3244
2024-02-07 07:35:27,444 Epoch 3244: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-07 07:35:27,444 EPOCH 3245
2024-02-07 07:35:32,231 [Epoch: 3245 Step: 00029200] Batch Recognition Loss:   0.000284 => Gls Tokens per Sec:     1070 || Batch Translation Loss:   0.063920 => Txt Tokens per Sec:     3085 || Lr: 0.000100
2024-02-07 07:35:43,925 Epoch 3245: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-07 07:35:43,925 EPOCH 3246
2024-02-07 07:36:00,442 Epoch 3246: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-07 07:36:00,442 EPOCH 3247
2024-02-07 07:36:17,081 Epoch 3247: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-07 07:36:17,082 EPOCH 3248
2024-02-07 07:36:33,404 Epoch 3248: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-07 07:36:33,405 EPOCH 3249
2024-02-07 07:36:49,643 Epoch 3249: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-07 07:36:49,643 EPOCH 3250
2024-02-07 07:37:06,157 Epoch 3250: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-07 07:37:06,158 EPOCH 3251
2024-02-07 07:37:22,611 Epoch 3251: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-07 07:37:22,611 EPOCH 3252
2024-02-07 07:37:38,961 Epoch 3252: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-07 07:37:38,962 EPOCH 3253
2024-02-07 07:37:55,203 Epoch 3253: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-07 07:37:55,203 EPOCH 3254
2024-02-07 07:38:11,801 Epoch 3254: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-07 07:38:11,802 EPOCH 3255
2024-02-07 07:38:28,239 Epoch 3255: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.79 
2024-02-07 07:38:28,239 EPOCH 3256
2024-02-07 07:38:33,230 [Epoch: 3256 Step: 00029300] Batch Recognition Loss:   0.003087 => Gls Tokens per Sec:     1283 || Batch Translation Loss:   0.882488 => Txt Tokens per Sec:     3419 || Lr: 0.000100
2024-02-07 07:38:44,563 Epoch 3256: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.85 
2024-02-07 07:38:44,564 EPOCH 3257
2024-02-07 07:39:00,994 Epoch 3257: Total Training Recognition Loss 0.06  Total Training Translation Loss 5.87 
2024-02-07 07:39:00,994 EPOCH 3258
2024-02-07 07:39:17,142 Epoch 3258: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.00 
2024-02-07 07:39:17,142 EPOCH 3259
2024-02-07 07:39:33,701 Epoch 3259: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.14 
2024-02-07 07:39:33,702 EPOCH 3260
2024-02-07 07:39:49,978 Epoch 3260: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.24 
2024-02-07 07:39:49,979 EPOCH 3261
2024-02-07 07:40:06,887 Epoch 3261: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.75 
2024-02-07 07:40:06,888 EPOCH 3262
2024-02-07 07:40:23,166 Epoch 3262: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.51 
2024-02-07 07:40:23,167 EPOCH 3263
2024-02-07 07:40:39,533 Epoch 3263: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-07 07:40:39,534 EPOCH 3264
2024-02-07 07:40:56,038 Epoch 3264: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-07 07:40:56,039 EPOCH 3265
2024-02-07 07:41:12,290 Epoch 3265: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-07 07:41:12,291 EPOCH 3266
2024-02-07 07:41:28,699 Epoch 3266: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-07 07:41:28,699 EPOCH 3267
2024-02-07 07:41:34,277 [Epoch: 3267 Step: 00029400] Batch Recognition Loss:   0.000548 => Gls Tokens per Sec:     1377 || Batch Translation Loss:   0.028098 => Txt Tokens per Sec:     3650 || Lr: 0.000100
2024-02-07 07:41:45,062 Epoch 3267: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-07 07:41:45,063 EPOCH 3268
2024-02-07 07:42:01,704 Epoch 3268: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 07:42:01,705 EPOCH 3269
2024-02-07 07:42:18,374 Epoch 3269: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 07:42:18,374 EPOCH 3270
2024-02-07 07:42:34,692 Epoch 3270: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 07:42:34,693 EPOCH 3271
2024-02-07 07:42:51,192 Epoch 3271: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 07:42:51,193 EPOCH 3272
2024-02-07 07:43:07,660 Epoch 3272: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 07:43:07,660 EPOCH 3273
2024-02-07 07:43:24,124 Epoch 3273: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 07:43:24,124 EPOCH 3274
2024-02-07 07:43:40,609 Epoch 3274: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 07:43:40,609 EPOCH 3275
2024-02-07 07:43:57,233 Epoch 3275: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 07:43:57,234 EPOCH 3276
2024-02-07 07:44:13,460 Epoch 3276: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 07:44:13,460 EPOCH 3277
2024-02-07 07:44:29,948 Epoch 3277: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 07:44:29,949 EPOCH 3278
2024-02-07 07:44:36,773 [Epoch: 3278 Step: 00029500] Batch Recognition Loss:   0.000660 => Gls Tokens per Sec:     1181 || Batch Translation Loss:   0.010062 => Txt Tokens per Sec:     3059 || Lr: 0.000100
2024-02-07 07:44:46,290 Epoch 3278: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 07:44:46,290 EPOCH 3279
2024-02-07 07:45:03,014 Epoch 3279: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 07:45:03,015 EPOCH 3280
2024-02-07 07:45:19,614 Epoch 3280: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 07:45:19,615 EPOCH 3281
2024-02-07 07:45:36,068 Epoch 3281: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:45:36,069 EPOCH 3282
2024-02-07 07:45:52,545 Epoch 3282: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:45:52,546 EPOCH 3283
2024-02-07 07:46:09,463 Epoch 3283: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 07:46:09,463 EPOCH 3284
2024-02-07 07:46:25,701 Epoch 3284: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:46:25,702 EPOCH 3285
2024-02-07 07:46:42,121 Epoch 3285: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:46:42,121 EPOCH 3286
2024-02-07 07:46:58,629 Epoch 3286: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:46:58,630 EPOCH 3287
2024-02-07 07:47:14,977 Epoch 3287: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 07:47:14,978 EPOCH 3288
2024-02-07 07:47:31,812 Epoch 3288: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:47:31,813 EPOCH 3289
2024-02-07 07:47:47,497 [Epoch: 3289 Step: 00029600] Batch Recognition Loss:   0.000304 => Gls Tokens per Sec:      596 || Batch Translation Loss:   0.006838 => Txt Tokens per Sec:     1625 || Lr: 0.000100
2024-02-07 07:47:48,172 Epoch 3289: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:47:48,172 EPOCH 3290
2024-02-07 07:48:04,606 Epoch 3290: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:48:04,607 EPOCH 3291
2024-02-07 07:48:20,828 Epoch 3291: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:48:20,829 EPOCH 3292
2024-02-07 07:48:37,315 Epoch 3292: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:48:37,315 EPOCH 3293
2024-02-07 07:48:53,956 Epoch 3293: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:48:53,957 EPOCH 3294
2024-02-07 07:49:10,418 Epoch 3294: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 07:49:10,419 EPOCH 3295
2024-02-07 07:49:26,783 Epoch 3295: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:49:26,783 EPOCH 3296
2024-02-07 07:49:43,289 Epoch 3296: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:49:43,289 EPOCH 3297
2024-02-07 07:49:59,752 Epoch 3297: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:49:59,753 EPOCH 3298
2024-02-07 07:50:16,504 Epoch 3298: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 07:50:16,505 EPOCH 3299
2024-02-07 07:50:33,518 Epoch 3299: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:50:33,519 EPOCH 3300
2024-02-07 07:50:50,231 [Epoch: 3300 Step: 00029700] Batch Recognition Loss:   0.000342 => Gls Tokens per Sec:      636 || Batch Translation Loss:   0.009131 => Txt Tokens per Sec:     1758 || Lr: 0.000100
2024-02-07 07:50:50,231 Epoch 3300: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:50:50,232 EPOCH 3301
2024-02-07 07:51:06,824 Epoch 3301: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:51:06,825 EPOCH 3302
2024-02-07 07:51:23,114 Epoch 3302: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:51:23,115 EPOCH 3303
2024-02-07 07:51:39,459 Epoch 3303: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:51:39,460 EPOCH 3304
2024-02-07 07:51:56,115 Epoch 3304: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:51:56,116 EPOCH 3305
2024-02-07 07:52:12,521 Epoch 3305: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:52:12,522 EPOCH 3306
2024-02-07 07:52:28,995 Epoch 3306: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:52:28,995 EPOCH 3307
2024-02-07 07:52:45,020 Epoch 3307: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:52:45,021 EPOCH 3308
2024-02-07 07:53:01,519 Epoch 3308: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:53:01,520 EPOCH 3309
2024-02-07 07:53:18,088 Epoch 3309: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:53:18,088 EPOCH 3310
2024-02-07 07:53:34,674 Epoch 3310: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:53:34,675 EPOCH 3311
2024-02-07 07:53:51,068 Epoch 3311: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:53:51,068 EPOCH 3312
2024-02-07 07:53:57,309 [Epoch: 3312 Step: 00029800] Batch Recognition Loss:   0.000296 => Gls Tokens per Sec:      205 || Batch Translation Loss:   0.016558 => Txt Tokens per Sec:      708 || Lr: 0.000100
2024-02-07 07:54:07,713 Epoch 3312: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 07:54:07,714 EPOCH 3313
2024-02-07 07:54:23,869 Epoch 3313: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 07:54:23,869 EPOCH 3314
2024-02-07 07:54:40,454 Epoch 3314: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 07:54:40,455 EPOCH 3315
2024-02-07 07:54:56,821 Epoch 3315: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 07:54:56,822 EPOCH 3316
2024-02-07 07:55:13,359 Epoch 3316: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 07:55:13,360 EPOCH 3317
2024-02-07 07:55:29,557 Epoch 3317: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:55:29,558 EPOCH 3318
2024-02-07 07:55:45,544 Epoch 3318: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:55:45,545 EPOCH 3319
2024-02-07 07:56:02,035 Epoch 3319: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:56:02,036 EPOCH 3320
2024-02-07 07:56:18,305 Epoch 3320: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 07:56:18,306 EPOCH 3321
2024-02-07 07:56:35,050 Epoch 3321: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 07:56:35,051 EPOCH 3322
2024-02-07 07:56:51,457 Epoch 3322: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 07:56:51,458 EPOCH 3323
2024-02-07 07:56:55,399 [Epoch: 3323 Step: 00029900] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:      650 || Batch Translation Loss:   0.011828 => Txt Tokens per Sec:     1926 || Lr: 0.000100
2024-02-07 07:57:08,316 Epoch 3323: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 07:57:08,316 EPOCH 3324
2024-02-07 07:57:24,723 Epoch 3324: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 07:57:24,724 EPOCH 3325
2024-02-07 07:57:41,162 Epoch 3325: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:57:41,163 EPOCH 3326
2024-02-07 07:57:57,294 Epoch 3326: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:57:57,295 EPOCH 3327
2024-02-07 07:58:13,682 Epoch 3327: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:58:13,683 EPOCH 3328
2024-02-07 07:58:29,946 Epoch 3328: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:58:29,946 EPOCH 3329
2024-02-07 07:58:46,180 Epoch 3329: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 07:58:46,181 EPOCH 3330
2024-02-07 07:59:02,611 Epoch 3330: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:59:02,612 EPOCH 3331
2024-02-07 07:59:19,017 Epoch 3331: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:59:19,018 EPOCH 3332
2024-02-07 07:59:35,384 Epoch 3332: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 07:59:35,385 EPOCH 3333
2024-02-07 07:59:51,734 Epoch 3333: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 07:59:51,734 EPOCH 3334
2024-02-07 07:59:56,938 [Epoch: 3334 Step: 00030000] Batch Recognition Loss:   0.000377 => Gls Tokens per Sec:      565 || Batch Translation Loss:   0.010539 => Txt Tokens per Sec:     1479 || Lr: 0.000100
2024-02-07 08:01:04,704 Validation result at epoch 3334, step    30000: duration: 67.7649s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.28709	Translation Loss: 96162.85938	PPL: 14835.23242
	Eval Metric: BLEU
	WER 2.68	(DEL: 0.00,	INS: 0.00,	SUB: 2.68)
	BLEU-4 0.40	(BLEU-1: 10.99,	BLEU-2: 3.49,	BLEU-3: 1.23,	BLEU-4: 0.40)
	CHRF 17.19	ROUGE 9.23
2024-02-07 08:01:04,706 Logging Recognition and Translation Outputs
2024-02-07 08:01:04,707 ========================================================================================================================
2024-02-07 08:01:04,707 Logging Sequence: 146_102.00
2024-02-07 08:01:04,707 	Gloss Reference :	A B+C+D+E
2024-02-07 08:01:04,707 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 08:01:04,707 	Gloss Alignment :	         
2024-02-07 08:01:04,707 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 08:01:04,709 	Text Reference  :	famous indian champion players like kidambi  srikanth and ashwini ponappa have tested positive for  coronavirus
2024-02-07 08:01:04,709 	Text Hypothesis :	along  with   them     these   who  messaged kohli    and ******* ******* want to     win      many wickets    
2024-02-07 08:01:04,709 	Text Alignment  :	S      S      S        S       S    S        S            D       D       S    S      S        S    S          
2024-02-07 08:01:04,709 ========================================================================================================================
2024-02-07 08:01:04,710 Logging Sequence: 53_178.00
2024-02-07 08:01:04,710 	Gloss Reference :	A B+C+D+E
2024-02-07 08:01:04,710 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 08:01:04,710 	Gloss Alignment :	         
2024-02-07 08:01:04,710 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 08:01:04,711 	Text Reference  :	the   money would help all    those affected by  the   humanitarian crisis in    afghanistan
2024-02-07 08:01:04,711 	Text Hypothesis :	there are   a     huge number of    people   are still has          the    trent rockets    
2024-02-07 08:01:04,712 	Text Alignment  :	S     S     S     S    S      S     S        S   S     S            S      S     S          
2024-02-07 08:01:04,712 ========================================================================================================================
2024-02-07 08:01:04,712 Logging Sequence: 129_200.00
2024-02-07 08:01:04,712 	Gloss Reference :	A B+C+D+E      
2024-02-07 08:01:04,712 	Gloss Hypothesis:	A B+C+D+E+D+E+D
2024-02-07 08:01:04,712 	Gloss Alignment :	  S            
2024-02-07 08:01:04,712 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 08:01:04,714 	Text Reference  :	*** **** *** ********** ** the ioc      would lose  about  4   billion if  the olympics were to be cancelled
2024-02-07 08:01:04,714 	Text Hypothesis :	now that the organisers of the olympics in    tokyo handed out for     all the olympics **** ** ** *********
2024-02-07 08:01:04,714 	Text Alignment  :	I   I    I   I          I      S        S     S     S      S   S       S                D    D  D  D        
2024-02-07 08:01:04,714 ========================================================================================================================
2024-02-07 08:01:04,715 Logging Sequence: 77_2.00
2024-02-07 08:01:04,715 	Gloss Reference :	A B+C+D+E  
2024-02-07 08:01:04,715 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-07 08:01:04,715 	Gloss Alignment :	  S        
2024-02-07 08:01:04,716 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 08:01:04,717 	Text Reference  :	on 25th april the ipl match between sunrisers hyderabad and  delhi capitals ended in  a   tie  
2024-02-07 08:01:04,717 	Text Hypothesis :	on **** ***** *** *** ***** 24th    october   india     lost the   t20      world cup sri lanka
2024-02-07 08:01:04,717 	Text Alignment  :	   D    D     D   D   D     S       S         S         S    S     S        S     S   S   S    
2024-02-07 08:01:04,717 ========================================================================================================================
2024-02-07 08:01:04,717 Logging Sequence: 119_170.00
2024-02-07 08:01:04,718 	Gloss Reference :	A B+C+D+E
2024-02-07 08:01:04,718 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 08:01:04,718 	Gloss Alignment :	         
2024-02-07 08:01:04,718 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 08:01:04,719 	Text Reference  :	they said it was a proud moment messi  is     a    big  hearted man  
2024-02-07 08:01:04,719 	Text Hypothesis :	**** **** ** *** * after the    defeat indian team lost the     match
2024-02-07 08:01:04,719 	Text Alignment  :	D    D    D  D   D S     S      S      S      S    S    S       S    
2024-02-07 08:01:04,719 ========================================================================================================================
2024-02-07 08:01:16,915 Epoch 3334: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 08:01:16,916 EPOCH 3335
2024-02-07 08:01:33,582 Epoch 3335: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 08:01:33,583 EPOCH 3336
2024-02-07 08:01:49,827 Epoch 3336: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 08:01:49,827 EPOCH 3337
2024-02-07 08:02:06,293 Epoch 3337: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 08:02:06,294 EPOCH 3338
2024-02-07 08:02:22,680 Epoch 3338: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 08:02:22,681 EPOCH 3339
2024-02-07 08:02:39,306 Epoch 3339: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 08:02:39,307 EPOCH 3340
2024-02-07 08:02:55,362 Epoch 3340: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 08:02:55,362 EPOCH 3341
2024-02-07 08:03:12,001 Epoch 3341: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 08:03:12,002 EPOCH 3342
2024-02-07 08:03:28,438 Epoch 3342: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 08:03:28,439 EPOCH 3343
2024-02-07 08:03:44,820 Epoch 3343: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 08:03:44,821 EPOCH 3344
2024-02-07 08:04:01,279 Epoch 3344: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-07 08:04:01,280 EPOCH 3345
2024-02-07 08:04:05,805 [Epoch: 3345 Step: 00030100] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:     1132 || Batch Translation Loss:   0.007236 => Txt Tokens per Sec:     3056 || Lr: 0.000100
2024-02-07 08:04:17,704 Epoch 3345: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 08:04:17,704 EPOCH 3346
2024-02-07 08:04:33,947 Epoch 3346: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 08:04:33,948 EPOCH 3347
2024-02-07 08:04:50,520 Epoch 3347: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 08:04:50,521 EPOCH 3348
2024-02-07 08:05:06,758 Epoch 3348: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 08:05:06,758 EPOCH 3349
2024-02-07 08:05:23,381 Epoch 3349: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 08:05:23,382 EPOCH 3350
2024-02-07 08:05:39,725 Epoch 3350: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 08:05:39,726 EPOCH 3351
2024-02-07 08:05:56,463 Epoch 3351: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 08:05:56,463 EPOCH 3352
2024-02-07 08:06:12,719 Epoch 3352: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 08:06:12,719 EPOCH 3353
2024-02-07 08:06:29,038 Epoch 3353: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 08:06:29,039 EPOCH 3354
2024-02-07 08:06:45,855 Epoch 3354: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 08:06:45,855 EPOCH 3355
2024-02-07 08:07:02,341 Epoch 3355: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 08:07:02,342 EPOCH 3356
2024-02-07 08:07:12,654 [Epoch: 3356 Step: 00030200] Batch Recognition Loss:   0.000318 => Gls Tokens per Sec:      621 || Batch Translation Loss:   0.010309 => Txt Tokens per Sec:     1670 || Lr: 0.000100
2024-02-07 08:07:18,963 Epoch 3356: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 08:07:18,963 EPOCH 3357
2024-02-07 08:07:35,307 Epoch 3357: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 08:07:35,308 EPOCH 3358
2024-02-07 08:07:51,782 Epoch 3358: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 08:07:51,782 EPOCH 3359
2024-02-07 08:08:08,122 Epoch 3359: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 08:08:08,123 EPOCH 3360
2024-02-07 08:08:24,708 Epoch 3360: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 08:08:24,709 EPOCH 3361
2024-02-07 08:08:41,321 Epoch 3361: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 08:08:41,322 EPOCH 3362
2024-02-07 08:08:57,842 Epoch 3362: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 08:08:57,842 EPOCH 3363
2024-02-07 08:09:14,170 Epoch 3363: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 08:09:14,171 EPOCH 3364
2024-02-07 08:09:30,374 Epoch 3364: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 08:09:30,374 EPOCH 3365
2024-02-07 08:09:46,706 Epoch 3365: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 08:09:46,707 EPOCH 3366
2024-02-07 08:10:03,165 Epoch 3366: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 08:10:03,166 EPOCH 3367
2024-02-07 08:10:14,453 [Epoch: 3367 Step: 00030300] Batch Recognition Loss:   0.000373 => Gls Tokens per Sec:      680 || Batch Translation Loss:   0.024909 => Txt Tokens per Sec:     2028 || Lr: 0.000100
2024-02-07 08:10:19,389 Epoch 3367: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 08:10:19,389 EPOCH 3368
2024-02-07 08:10:36,017 Epoch 3368: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 08:10:36,018 EPOCH 3369
2024-02-07 08:10:52,841 Epoch 3369: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 08:10:52,842 EPOCH 3370
2024-02-07 08:11:09,683 Epoch 3370: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 08:11:09,683 EPOCH 3371
2024-02-07 08:11:26,089 Epoch 3371: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 08:11:26,089 EPOCH 3372
2024-02-07 08:11:42,708 Epoch 3372: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 08:11:42,709 EPOCH 3373
2024-02-07 08:11:59,339 Epoch 3373: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 08:11:59,339 EPOCH 3374
2024-02-07 08:12:15,813 Epoch 3374: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 08:12:15,813 EPOCH 3375
2024-02-07 08:12:32,246 Epoch 3375: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 08:12:32,247 EPOCH 3376
2024-02-07 08:12:48,776 Epoch 3376: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 08:12:48,777 EPOCH 3377
2024-02-07 08:13:05,114 Epoch 3377: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 08:13:05,114 EPOCH 3378
2024-02-07 08:13:17,863 [Epoch: 3378 Step: 00030400] Batch Recognition Loss:   0.000343 => Gls Tokens per Sec:      632 || Batch Translation Loss:   0.047525 => Txt Tokens per Sec:     1711 || Lr: 0.000100
2024-02-07 08:13:21,573 Epoch 3378: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 08:13:21,573 EPOCH 3379
2024-02-07 08:13:38,170 Epoch 3379: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-07 08:13:38,170 EPOCH 3380
2024-02-07 08:13:54,650 Epoch 3380: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-07 08:13:54,651 EPOCH 3381
2024-02-07 08:14:11,600 Epoch 3381: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-07 08:14:11,601 EPOCH 3382
2024-02-07 08:14:27,954 Epoch 3382: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-07 08:14:27,955 EPOCH 3383
2024-02-07 08:14:44,635 Epoch 3383: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-07 08:14:44,636 EPOCH 3384
2024-02-07 08:15:01,054 Epoch 3384: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-07 08:15:01,054 EPOCH 3385
2024-02-07 08:15:17,619 Epoch 3385: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.22 
2024-02-07 08:15:17,620 EPOCH 3386
2024-02-07 08:15:33,915 Epoch 3386: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.81 
2024-02-07 08:15:33,916 EPOCH 3387
2024-02-07 08:15:50,449 Epoch 3387: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.25 
2024-02-07 08:15:50,449 EPOCH 3388
2024-02-07 08:16:06,752 Epoch 3388: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.10 
2024-02-07 08:16:06,752 EPOCH 3389
2024-02-07 08:16:22,477 [Epoch: 3389 Step: 00030500] Batch Recognition Loss:   0.002198 => Gls Tokens per Sec:      594 || Batch Translation Loss:   0.155450 => Txt Tokens per Sec:     1622 || Lr: 0.000100
2024-02-07 08:16:23,190 Epoch 3389: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.55 
2024-02-07 08:16:23,190 EPOCH 3390
2024-02-07 08:16:39,450 Epoch 3390: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-07 08:16:39,451 EPOCH 3391
2024-02-07 08:16:55,788 Epoch 3391: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-07 08:16:55,789 EPOCH 3392
2024-02-07 08:17:12,558 Epoch 3392: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-07 08:17:12,558 EPOCH 3393
2024-02-07 08:17:29,110 Epoch 3393: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-07 08:17:29,111 EPOCH 3394
2024-02-07 08:17:45,216 Epoch 3394: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-07 08:17:45,217 EPOCH 3395
2024-02-07 08:18:01,846 Epoch 3395: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-07 08:18:01,847 EPOCH 3396
2024-02-07 08:18:18,500 Epoch 3396: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-07 08:18:18,501 EPOCH 3397
2024-02-07 08:18:34,876 Epoch 3397: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-07 08:18:34,876 EPOCH 3398
2024-02-07 08:18:51,420 Epoch 3398: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 08:18:51,421 EPOCH 3399
2024-02-07 08:19:08,033 Epoch 3399: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 08:19:08,033 EPOCH 3400
2024-02-07 08:19:24,490 [Epoch: 3400 Step: 00030600] Batch Recognition Loss:   0.000427 => Gls Tokens per Sec:      645 || Batch Translation Loss:   0.033280 => Txt Tokens per Sec:     1785 || Lr: 0.000100
2024-02-07 08:19:24,491 Epoch 3400: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 08:19:24,491 EPOCH 3401
2024-02-07 08:19:40,813 Epoch 3401: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 08:19:40,813 EPOCH 3402
2024-02-07 08:19:57,450 Epoch 3402: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 08:19:57,450 EPOCH 3403
2024-02-07 08:20:13,804 Epoch 3403: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 08:20:13,805 EPOCH 3404
2024-02-07 08:20:30,064 Epoch 3404: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 08:20:30,065 EPOCH 3405
2024-02-07 08:20:46,765 Epoch 3405: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 08:20:46,766 EPOCH 3406
2024-02-07 08:21:03,257 Epoch 3406: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 08:21:03,257 EPOCH 3407
2024-02-07 08:21:20,531 Epoch 3407: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 08:21:20,532 EPOCH 3408
2024-02-07 08:21:37,184 Epoch 3408: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 08:21:37,185 EPOCH 3409
2024-02-07 08:21:53,767 Epoch 3409: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 08:21:53,768 EPOCH 3410
2024-02-07 08:22:10,277 Epoch 3410: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 08:22:10,277 EPOCH 3411
2024-02-07 08:22:26,766 Epoch 3411: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 08:22:26,767 EPOCH 3412
2024-02-07 08:22:27,219 [Epoch: 3412 Step: 00030700] Batch Recognition Loss:   0.000401 => Gls Tokens per Sec:     2841 || Batch Translation Loss:   0.017071 => Txt Tokens per Sec:     7772 || Lr: 0.000100
2024-02-07 08:22:43,141 Epoch 3412: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 08:22:43,142 EPOCH 3413
2024-02-07 08:22:59,258 Epoch 3413: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 08:22:59,259 EPOCH 3414
2024-02-07 08:23:16,091 Epoch 3414: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 08:23:16,091 EPOCH 3415
2024-02-07 08:23:32,918 Epoch 3415: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 08:23:32,918 EPOCH 3416
2024-02-07 08:23:49,283 Epoch 3416: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 08:23:49,285 EPOCH 3417
2024-02-07 08:24:05,973 Epoch 3417: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 08:24:05,973 EPOCH 3418
2024-02-07 08:24:22,716 Epoch 3418: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 08:24:22,716 EPOCH 3419
2024-02-07 08:24:39,051 Epoch 3419: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 08:24:39,052 EPOCH 3420
2024-02-07 08:24:55,722 Epoch 3420: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 08:24:55,723 EPOCH 3421
2024-02-07 08:25:12,065 Epoch 3421: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 08:25:12,066 EPOCH 3422
2024-02-07 08:25:28,363 Epoch 3422: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 08:25:28,363 EPOCH 3423
2024-02-07 08:25:33,355 [Epoch: 3423 Step: 00030800] Batch Recognition Loss:   0.000758 => Gls Tokens per Sec:      333 || Batch Translation Loss:   0.007279 => Txt Tokens per Sec:     1014 || Lr: 0.000100
2024-02-07 08:25:45,024 Epoch 3423: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 08:25:45,024 EPOCH 3424
2024-02-07 08:26:01,310 Epoch 3424: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 08:26:01,310 EPOCH 3425
2024-02-07 08:26:17,674 Epoch 3425: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 08:26:17,674 EPOCH 3426
2024-02-07 08:26:34,061 Epoch 3426: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 08:26:34,062 EPOCH 3427
2024-02-07 08:26:50,557 Epoch 3427: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 08:26:50,557 EPOCH 3428
2024-02-07 08:27:07,011 Epoch 3428: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 08:27:07,012 EPOCH 3429
2024-02-07 08:27:23,257 Epoch 3429: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 08:27:23,258 EPOCH 3430
2024-02-07 08:27:39,796 Epoch 3430: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 08:27:39,796 EPOCH 3431
2024-02-07 08:27:56,189 Epoch 3431: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 08:27:56,189 EPOCH 3432
2024-02-07 08:28:12,413 Epoch 3432: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 08:28:12,414 EPOCH 3433
2024-02-07 08:28:29,103 Epoch 3433: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 08:28:29,103 EPOCH 3434
2024-02-07 08:28:34,294 [Epoch: 3434 Step: 00030900] Batch Recognition Loss:   0.000267 => Gls Tokens per Sec:      567 || Batch Translation Loss:   0.006347 => Txt Tokens per Sec:     1606 || Lr: 0.000100
2024-02-07 08:28:45,358 Epoch 3434: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 08:28:45,359 EPOCH 3435
2024-02-07 08:29:01,787 Epoch 3435: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 08:29:01,787 EPOCH 3436
2024-02-07 08:29:18,075 Epoch 3436: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 08:29:18,076 EPOCH 3437
2024-02-07 08:29:34,283 Epoch 3437: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 08:29:34,284 EPOCH 3438
2024-02-07 08:29:50,674 Epoch 3438: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 08:29:50,674 EPOCH 3439
2024-02-07 08:30:07,354 Epoch 3439: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 08:30:07,355 EPOCH 3440
2024-02-07 08:30:23,837 Epoch 3440: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 08:30:23,838 EPOCH 3441
2024-02-07 08:30:40,338 Epoch 3441: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 08:30:40,339 EPOCH 3442
2024-02-07 08:30:56,629 Epoch 3442: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 08:30:56,629 EPOCH 3443
2024-02-07 08:31:12,826 Epoch 3443: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 08:31:12,827 EPOCH 3444
2024-02-07 08:31:29,330 Epoch 3444: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 08:31:29,330 EPOCH 3445
2024-02-07 08:31:37,013 [Epoch: 3445 Step: 00031000] Batch Recognition Loss:   0.000356 => Gls Tokens per Sec:      666 || Batch Translation Loss:   0.019139 => Txt Tokens per Sec:     1962 || Lr: 0.000100
2024-02-07 08:31:45,822 Epoch 3445: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 08:31:45,822 EPOCH 3446
2024-02-07 08:32:02,139 Epoch 3446: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 08:32:02,140 EPOCH 3447
2024-02-07 08:32:18,660 Epoch 3447: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 08:32:18,661 EPOCH 3448
2024-02-07 08:32:35,323 Epoch 3448: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 08:32:35,323 EPOCH 3449
2024-02-07 08:32:51,833 Epoch 3449: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 08:32:51,833 EPOCH 3450
2024-02-07 08:33:08,132 Epoch 3450: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 08:33:08,132 EPOCH 3451
2024-02-07 08:33:24,635 Epoch 3451: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 08:33:24,635 EPOCH 3452
2024-02-07 08:33:41,522 Epoch 3452: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 08:33:41,523 EPOCH 3453
2024-02-07 08:33:57,787 Epoch 3453: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 08:33:57,788 EPOCH 3454
2024-02-07 08:34:14,106 Epoch 3454: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 08:34:14,107 EPOCH 3455
2024-02-07 08:34:30,506 Epoch 3455: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 08:34:30,506 EPOCH 3456
2024-02-07 08:34:38,500 [Epoch: 3456 Step: 00031100] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:      801 || Batch Translation Loss:   0.010235 => Txt Tokens per Sec:     2120 || Lr: 0.000100
2024-02-07 08:34:47,018 Epoch 3456: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.13 
2024-02-07 08:34:47,019 EPOCH 3457
2024-02-07 08:35:03,526 Epoch 3457: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.13 
2024-02-07 08:35:03,526 EPOCH 3458
2024-02-07 08:35:20,215 Epoch 3458: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 08:35:20,215 EPOCH 3459
2024-02-07 08:35:36,409 Epoch 3459: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.12 
2024-02-07 08:35:36,409 EPOCH 3460
2024-02-07 08:35:52,625 Epoch 3460: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.12 
2024-02-07 08:35:52,625 EPOCH 3461
2024-02-07 08:36:09,351 Epoch 3461: Total Training Recognition Loss 0.14  Total Training Translation Loss 0.16 
2024-02-07 08:36:09,352 EPOCH 3462
2024-02-07 08:36:25,976 Epoch 3462: Total Training Recognition Loss 0.20  Total Training Translation Loss 0.14 
2024-02-07 08:36:25,976 EPOCH 3463
2024-02-07 08:36:42,265 Epoch 3463: Total Training Recognition Loss 0.62  Total Training Translation Loss 0.19 
2024-02-07 08:36:42,266 EPOCH 3464
2024-02-07 08:36:58,470 Epoch 3464: Total Training Recognition Loss 0.22  Total Training Translation Loss 0.23 
2024-02-07 08:36:58,471 EPOCH 3465
2024-02-07 08:37:15,148 Epoch 3465: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.25 
2024-02-07 08:37:15,149 EPOCH 3466
2024-02-07 08:37:31,706 Epoch 3466: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.26 
2024-02-07 08:37:31,706 EPOCH 3467
2024-02-07 08:37:40,109 [Epoch: 3467 Step: 00031200] Batch Recognition Loss:   0.011135 => Gls Tokens per Sec:      914 || Batch Translation Loss:   0.041902 => Txt Tokens per Sec:     2458 || Lr: 0.000100
2024-02-07 08:37:48,114 Epoch 3467: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.27 
2024-02-07 08:37:48,114 EPOCH 3468
2024-02-07 08:38:04,722 Epoch 3468: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.30 
2024-02-07 08:38:04,722 EPOCH 3469
2024-02-07 08:38:21,291 Epoch 3469: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.25 
2024-02-07 08:38:21,291 EPOCH 3470
2024-02-07 08:38:37,767 Epoch 3470: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-07 08:38:37,768 EPOCH 3471
2024-02-07 08:38:54,147 Epoch 3471: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.20 
2024-02-07 08:38:54,149 EPOCH 3472
2024-02-07 08:39:10,468 Epoch 3472: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-07 08:39:10,468 EPOCH 3473
2024-02-07 08:39:26,946 Epoch 3473: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-07 08:39:26,946 EPOCH 3474
2024-02-07 08:39:43,264 Epoch 3474: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-07 08:39:43,265 EPOCH 3475
2024-02-07 08:39:59,925 Epoch 3475: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 08:39:59,925 EPOCH 3476
2024-02-07 08:40:16,199 Epoch 3476: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-07 08:40:16,200 EPOCH 3477
2024-02-07 08:40:32,896 Epoch 3477: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 08:40:32,896 EPOCH 3478
2024-02-07 08:40:48,148 [Epoch: 3478 Step: 00031300] Batch Recognition Loss:   0.000309 => Gls Tokens per Sec:      529 || Batch Translation Loss:   0.035445 => Txt Tokens per Sec:     1457 || Lr: 0.000100
2024-02-07 08:40:49,207 Epoch 3478: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 08:40:49,207 EPOCH 3479
2024-02-07 08:41:05,819 Epoch 3479: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 08:41:05,819 EPOCH 3480
2024-02-07 08:41:22,064 Epoch 3480: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 08:41:22,064 EPOCH 3481
2024-02-07 08:41:38,808 Epoch 3481: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 08:41:38,808 EPOCH 3482
2024-02-07 08:41:55,410 Epoch 3482: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-07 08:41:55,411 EPOCH 3483
2024-02-07 08:42:11,953 Epoch 3483: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-07 08:42:11,954 EPOCH 3484
2024-02-07 08:42:28,280 Epoch 3484: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-07 08:42:28,280 EPOCH 3485
2024-02-07 08:42:44,792 Epoch 3485: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-07 08:42:44,792 EPOCH 3486
2024-02-07 08:43:01,345 Epoch 3486: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-07 08:43:01,346 EPOCH 3487
2024-02-07 08:43:17,910 Epoch 3487: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 08:43:17,911 EPOCH 3488
2024-02-07 08:43:34,252 Epoch 3488: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 08:43:34,252 EPOCH 3489
2024-02-07 08:43:46,652 [Epoch: 3489 Step: 00031400] Batch Recognition Loss:   0.000346 => Gls Tokens per Sec:      826 || Batch Translation Loss:   0.031955 => Txt Tokens per Sec:     2260 || Lr: 0.000100
2024-02-07 08:43:50,962 Epoch 3489: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 08:43:50,963 EPOCH 3490
2024-02-07 08:44:07,432 Epoch 3490: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 08:44:07,433 EPOCH 3491
2024-02-07 08:44:24,035 Epoch 3491: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 08:44:24,036 EPOCH 3492
2024-02-07 08:44:40,767 Epoch 3492: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 08:44:40,768 EPOCH 3493
2024-02-07 08:44:56,996 Epoch 3493: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 08:44:56,997 EPOCH 3494
2024-02-07 08:45:13,446 Epoch 3494: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 08:45:13,447 EPOCH 3495
2024-02-07 08:45:29,802 Epoch 3495: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-07 08:45:29,802 EPOCH 3496
2024-02-07 08:45:46,678 Epoch 3496: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-07 08:45:46,679 EPOCH 3497
2024-02-07 08:46:02,847 Epoch 3497: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-07 08:46:02,848 EPOCH 3498
2024-02-07 08:46:19,490 Epoch 3498: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-07 08:46:19,491 EPOCH 3499
2024-02-07 08:46:35,711 Epoch 3499: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-07 08:46:35,712 EPOCH 3500
2024-02-07 08:46:52,548 [Epoch: 3500 Step: 00031500] Batch Recognition Loss:   0.000539 => Gls Tokens per Sec:      631 || Batch Translation Loss:   0.131934 => Txt Tokens per Sec:     1745 || Lr: 0.000100
2024-02-07 08:46:52,549 Epoch 3500: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-07 08:46:52,549 EPOCH 3501
2024-02-07 08:47:08,774 Epoch 3501: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-07 08:47:08,774 EPOCH 3502
2024-02-07 08:47:25,803 Epoch 3502: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-07 08:47:25,803 EPOCH 3503
2024-02-07 08:47:42,067 Epoch 3503: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-07 08:47:42,068 EPOCH 3504
2024-02-07 08:47:58,903 Epoch 3504: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-07 08:47:58,904 EPOCH 3505
2024-02-07 08:48:15,299 Epoch 3505: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-07 08:48:15,299 EPOCH 3506
2024-02-07 08:48:32,072 Epoch 3506: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-07 08:48:32,072 EPOCH 3507
2024-02-07 08:48:48,331 Epoch 3507: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-07 08:48:48,331 EPOCH 3508
2024-02-07 08:49:04,811 Epoch 3508: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-07 08:49:04,812 EPOCH 3509
2024-02-07 08:49:21,410 Epoch 3509: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 08:49:21,411 EPOCH 3510
2024-02-07 08:49:37,683 Epoch 3510: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 08:49:37,684 EPOCH 3511
2024-02-07 08:49:54,069 Epoch 3511: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 08:49:54,069 EPOCH 3512
2024-02-07 08:49:54,406 [Epoch: 3512 Step: 00031600] Batch Recognition Loss:   0.000318 => Gls Tokens per Sec:     3810 || Batch Translation Loss:   0.014346 => Txt Tokens per Sec:     8440 || Lr: 0.000100
2024-02-07 08:50:10,432 Epoch 3512: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 08:50:10,433 EPOCH 3513
2024-02-07 08:50:26,818 Epoch 3513: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 08:50:26,819 EPOCH 3514
2024-02-07 08:50:43,356 Epoch 3514: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 08:50:43,357 EPOCH 3515
2024-02-07 08:50:59,898 Epoch 3515: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 08:50:59,898 EPOCH 3516
2024-02-07 08:51:16,111 Epoch 3516: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 08:51:16,112 EPOCH 3517
2024-02-07 08:51:32,576 Epoch 3517: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 08:51:32,576 EPOCH 3518
2024-02-07 08:51:49,137 Epoch 3518: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 08:51:49,137 EPOCH 3519
2024-02-07 08:52:05,649 Epoch 3519: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 08:52:05,651 EPOCH 3520
2024-02-07 08:52:22,108 Epoch 3520: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 08:52:22,108 EPOCH 3521
2024-02-07 08:52:38,547 Epoch 3521: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 08:52:38,547 EPOCH 3522
2024-02-07 08:52:55,000 Epoch 3522: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 08:52:55,001 EPOCH 3523
2024-02-07 08:52:59,597 [Epoch: 3523 Step: 00031700] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:      361 || Batch Translation Loss:   0.006169 => Txt Tokens per Sec:      793 || Lr: 0.000100
2024-02-07 08:53:11,268 Epoch 3523: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 08:53:11,268 EPOCH 3524
2024-02-07 08:53:27,794 Epoch 3524: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 08:53:27,794 EPOCH 3525
2024-02-07 08:53:44,620 Epoch 3525: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 08:53:44,621 EPOCH 3526
2024-02-07 08:54:00,883 Epoch 3526: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 08:54:00,884 EPOCH 3527
2024-02-07 08:54:17,262 Epoch 3527: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 08:54:17,262 EPOCH 3528
2024-02-07 08:54:33,845 Epoch 3528: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 08:54:33,845 EPOCH 3529
2024-02-07 08:54:50,381 Epoch 3529: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 08:54:50,382 EPOCH 3530
2024-02-07 08:55:06,808 Epoch 3530: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 08:55:06,809 EPOCH 3531
2024-02-07 08:55:23,169 Epoch 3531: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 08:55:23,169 EPOCH 3532
2024-02-07 08:55:39,619 Epoch 3532: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 08:55:39,620 EPOCH 3533
2024-02-07 08:55:56,034 Epoch 3533: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 08:55:56,035 EPOCH 3534
2024-02-07 08:55:56,989 [Epoch: 3534 Step: 00031800] Batch Recognition Loss:   0.000495 => Gls Tokens per Sec:     4028 || Batch Translation Loss:   0.008562 => Txt Tokens per Sec:     8828 || Lr: 0.000100
2024-02-07 08:56:12,550 Epoch 3534: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 08:56:12,551 EPOCH 3535
2024-02-07 08:56:28,650 Epoch 3535: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 08:56:28,651 EPOCH 3536
2024-02-07 08:56:45,149 Epoch 3536: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 08:56:45,150 EPOCH 3537
2024-02-07 08:57:01,502 Epoch 3537: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 08:57:01,503 EPOCH 3538
2024-02-07 08:57:17,823 Epoch 3538: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 08:57:17,823 EPOCH 3539
2024-02-07 08:57:34,319 Epoch 3539: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 08:57:34,320 EPOCH 3540
2024-02-07 08:57:50,778 Epoch 3540: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-07 08:57:50,779 EPOCH 3541
2024-02-07 08:58:07,402 Epoch 3541: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-07 08:58:07,402 EPOCH 3542
2024-02-07 08:58:24,125 Epoch 3542: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-07 08:58:24,125 EPOCH 3543
2024-02-07 08:58:40,366 Epoch 3543: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-07 08:58:40,366 EPOCH 3544
2024-02-07 08:58:56,379 Epoch 3544: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-07 08:58:56,380 EPOCH 3545
2024-02-07 08:59:08,231 [Epoch: 3545 Step: 00031900] Batch Recognition Loss:   0.000415 => Gls Tokens per Sec:      356 || Batch Translation Loss:   0.037965 => Txt Tokens per Sec:     1055 || Lr: 0.000100
2024-02-07 08:59:13,015 Epoch 3545: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-07 08:59:13,016 EPOCH 3546
2024-02-07 08:59:29,788 Epoch 3546: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-07 08:59:29,788 EPOCH 3547
2024-02-07 08:59:46,071 Epoch 3547: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.75 
2024-02-07 08:59:46,073 EPOCH 3548
2024-02-07 09:00:02,612 Epoch 3548: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.21 
2024-02-07 09:00:02,613 EPOCH 3549
2024-02-07 09:00:18,980 Epoch 3549: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.88 
2024-02-07 09:00:18,980 EPOCH 3550
2024-02-07 09:00:34,917 Epoch 3550: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.00 
2024-02-07 09:00:34,918 EPOCH 3551
2024-02-07 09:00:51,328 Epoch 3551: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.72 
2024-02-07 09:00:51,328 EPOCH 3552
2024-02-07 09:01:07,585 Epoch 3552: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.07 
2024-02-07 09:01:07,585 EPOCH 3553
2024-02-07 09:01:24,282 Epoch 3553: Total Training Recognition Loss 0.07  Total Training Translation Loss 6.51 
2024-02-07 09:01:24,282 EPOCH 3554
2024-02-07 09:01:40,553 Epoch 3554: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.38 
2024-02-07 09:01:40,553 EPOCH 3555
2024-02-07 09:01:56,948 Epoch 3555: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.43 
2024-02-07 09:01:56,949 EPOCH 3556
2024-02-07 09:02:03,421 [Epoch: 3556 Step: 00032000] Batch Recognition Loss:   0.001786 => Gls Tokens per Sec:      850 || Batch Translation Loss:   0.082868 => Txt Tokens per Sec:     2430 || Lr: 0.000100
2024-02-07 09:03:11,268 Validation result at epoch 3556, step    32000: duration: 67.8462s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.27637	Translation Loss: 95558.33594	PPL: 13965.98730
	Eval Metric: BLEU
	WER 2.82	(DEL: 0.00,	INS: 0.00,	SUB: 2.82)
	BLEU-4 0.50	(BLEU-1: 11.62,	BLEU-2: 3.68,	BLEU-3: 1.31,	BLEU-4: 0.50)
	CHRF 16.80	ROUGE 9.53
2024-02-07 09:03:11,270 Logging Recognition and Translation Outputs
2024-02-07 09:03:11,270 ========================================================================================================================
2024-02-07 09:03:11,270 Logging Sequence: 162_133.00
2024-02-07 09:03:11,270 	Gloss Reference :	A B+C+D+E
2024-02-07 09:03:11,271 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 09:03:11,271 	Gloss Alignment :	         
2024-02-07 09:03:11,271 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 09:03:11,273 	Text Reference  :	***** *** * **** they          also        sent rape threats to his 9-month old daughter
2024-02-07 09:03:11,273 	Text Hypothesis :	kohli has a such technological advancement in   ipl  which   is the right   pad first   
2024-02-07 09:03:11,274 	Text Alignment  :	I     I   I I    S             S           S    S    S       S  S   S       S   S       
2024-02-07 09:03:11,274 ========================================================================================================================
2024-02-07 09:03:11,274 Logging Sequence: 134_236.00
2024-02-07 09:03:11,274 	Gloss Reference :	A B+C+D+E
2024-02-07 09:03:11,274 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 09:03:11,274 	Gloss Alignment :	         
2024-02-07 09:03:11,274 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 09:03:11,276 	Text Reference  :	after   the  interaction modi  tweeted the ******** ** *** images and captioned it   saying
2024-02-07 09:03:11,276 	Text Hypothesis :	indians were very        happy of      the athletes at his goals  he  played    very well  
2024-02-07 09:03:11,276 	Text Alignment  :	S       S    S           S     S           I        I  I   S      S   S         S    S     
2024-02-07 09:03:11,276 ========================================================================================================================
2024-02-07 09:03:11,276 Logging Sequence: 145_52.00
2024-02-07 09:03:11,277 	Gloss Reference :	A B+C+D+E
2024-02-07 09:03:11,277 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 09:03:11,277 	Gloss Alignment :	         
2024-02-07 09:03:11,277 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 09:03:11,278 	Text Reference  :	her name was dropped despite having qualified as    she  was the      only female athlete
2024-02-07 09:03:11,278 	Text Hypothesis :	*** **** *** the     finals  were   made      india with a   talented with his    famous 
2024-02-07 09:03:11,278 	Text Alignment  :	D   D    D   S       S       S      S         S     S    S   S        S    S      S      
2024-02-07 09:03:11,278 ========================================================================================================================
2024-02-07 09:03:11,279 Logging Sequence: 175_40.00
2024-02-07 09:03:11,279 	Gloss Reference :	A B+C+D+E
2024-02-07 09:03:11,279 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 09:03:11,279 	Gloss Alignment :	         
2024-02-07 09:03:11,279 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 09:03:11,281 	Text Reference  :	*** ** **** ***** soumyadeep and     shreya bagged three medals each including a silver medal ** *** ***** each
2024-02-07 09:03:11,281 	Text Hypothesis :	now we have taken it's       revenge by     this   is    why    he   won       a gold   medal in the world cup 
2024-02-07 09:03:11,281 	Text Alignment  :	I   I  I    I     S          S       S      S      S     S      S    S           S            I  I   I     S   
2024-02-07 09:03:11,281 ========================================================================================================================
2024-02-07 09:03:11,281 Logging Sequence: 156_51.00
2024-02-07 09:03:11,282 	Gloss Reference :	A B+C+D+E
2024-02-07 09:03:11,282 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 09:03:11,282 	Gloss Alignment :	         
2024-02-07 09:03:11,282 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 09:03:11,283 	Text Reference  :	** *** ** the  selection of      the players was similar to that of  ipl 
2024-02-07 09:03:11,283 	Text Hypothesis :	in ipl we have been      friends and i       am  sure    to see  the post
2024-02-07 09:03:11,283 	Text Alignment  :	I  I   I  S    S         S       S   S       S   S          S    S   S   
2024-02-07 09:03:11,283 ========================================================================================================================
2024-02-07 09:03:21,880 Epoch 3556: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-07 09:03:21,880 EPOCH 3557
2024-02-07 09:03:38,863 Epoch 3557: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-07 09:03:38,863 EPOCH 3558
2024-02-07 09:03:55,229 Epoch 3558: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-07 09:03:55,230 EPOCH 3559
2024-02-07 09:04:11,547 Epoch 3559: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-07 09:04:11,548 EPOCH 3560
2024-02-07 09:04:28,015 Epoch 3560: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-07 09:04:28,016 EPOCH 3561
2024-02-07 09:04:44,174 Epoch 3561: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-07 09:04:44,175 EPOCH 3562
2024-02-07 09:05:00,501 Epoch 3562: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 09:05:00,502 EPOCH 3563
2024-02-07 09:05:17,053 Epoch 3563: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 09:05:17,054 EPOCH 3564
2024-02-07 09:05:33,501 Epoch 3564: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 09:05:33,502 EPOCH 3565
2024-02-07 09:05:49,705 Epoch 3565: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 09:05:49,705 EPOCH 3566
2024-02-07 09:06:06,290 Epoch 3566: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 09:06:06,291 EPOCH 3567
2024-02-07 09:06:21,460 [Epoch: 3567 Step: 00032100] Batch Recognition Loss:   0.000562 => Gls Tokens per Sec:      447 || Batch Translation Loss:   0.020102 => Txt Tokens per Sec:     1278 || Lr: 0.000100
2024-02-07 09:06:22,951 Epoch 3567: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 09:06:22,951 EPOCH 3568
2024-02-07 09:06:39,347 Epoch 3568: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 09:06:39,347 EPOCH 3569
2024-02-07 09:06:55,456 Epoch 3569: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 09:06:55,456 EPOCH 3570
2024-02-07 09:07:12,114 Epoch 3570: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 09:07:12,115 EPOCH 3571
2024-02-07 09:07:28,822 Epoch 3571: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 09:07:28,822 EPOCH 3572
2024-02-07 09:07:45,063 Epoch 3572: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 09:07:45,064 EPOCH 3573
2024-02-07 09:08:01,603 Epoch 3573: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 09:08:01,604 EPOCH 3574
2024-02-07 09:08:17,924 Epoch 3574: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 09:08:17,924 EPOCH 3575
2024-02-07 09:08:34,391 Epoch 3575: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 09:08:34,391 EPOCH 3576
2024-02-07 09:08:50,630 Epoch 3576: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 09:08:50,631 EPOCH 3577
2024-02-07 09:09:07,318 Epoch 3577: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 09:09:07,318 EPOCH 3578
2024-02-07 09:09:17,488 [Epoch: 3578 Step: 00032200] Batch Recognition Loss:   0.000311 => Gls Tokens per Sec:      793 || Batch Translation Loss:   0.015719 => Txt Tokens per Sec:     2229 || Lr: 0.000100
2024-02-07 09:09:23,888 Epoch 3578: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 09:09:23,888 EPOCH 3579
2024-02-07 09:09:40,087 Epoch 3579: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 09:09:40,088 EPOCH 3580
2024-02-07 09:09:56,535 Epoch 3580: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 09:09:56,536 EPOCH 3581
2024-02-07 09:10:12,812 Epoch 3581: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 09:10:12,813 EPOCH 3582
2024-02-07 09:10:29,314 Epoch 3582: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 09:10:29,315 EPOCH 3583
2024-02-07 09:10:45,817 Epoch 3583: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 09:10:45,818 EPOCH 3584
2024-02-07 09:11:02,286 Epoch 3584: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 09:11:02,287 EPOCH 3585
2024-02-07 09:11:18,816 Epoch 3585: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 09:11:18,817 EPOCH 3586
2024-02-07 09:11:35,297 Epoch 3586: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:11:35,297 EPOCH 3587
2024-02-07 09:11:51,621 Epoch 3587: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:11:51,621 EPOCH 3588
2024-02-07 09:12:08,134 Epoch 3588: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:12:08,135 EPOCH 3589
2024-02-07 09:12:18,372 [Epoch: 3589 Step: 00032300] Batch Recognition Loss:   0.000249 => Gls Tokens per Sec:      912 || Batch Translation Loss:   0.012563 => Txt Tokens per Sec:     2441 || Lr: 0.000100
2024-02-07 09:12:24,428 Epoch 3589: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:12:24,428 EPOCH 3590
2024-02-07 09:12:40,771 Epoch 3590: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:12:40,771 EPOCH 3591
2024-02-07 09:12:57,120 Epoch 3591: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:12:57,120 EPOCH 3592
2024-02-07 09:13:13,396 Epoch 3592: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:13:13,397 EPOCH 3593
2024-02-07 09:13:29,629 Epoch 3593: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:13:29,630 EPOCH 3594
2024-02-07 09:13:45,848 Epoch 3594: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:13:45,848 EPOCH 3595
2024-02-07 09:14:02,266 Epoch 3595: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:14:02,266 EPOCH 3596
2024-02-07 09:14:18,646 Epoch 3596: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:14:18,647 EPOCH 3597
2024-02-07 09:14:35,249 Epoch 3597: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:14:35,250 EPOCH 3598
2024-02-07 09:14:51,762 Epoch 3598: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:14:51,763 EPOCH 3599
2024-02-07 09:15:08,036 Epoch 3599: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:15:08,037 EPOCH 3600
2024-02-07 09:15:24,618 [Epoch: 3600 Step: 00032400] Batch Recognition Loss:   0.000668 => Gls Tokens per Sec:      641 || Batch Translation Loss:   0.007463 => Txt Tokens per Sec:     1772 || Lr: 0.000100
2024-02-07 09:15:24,619 Epoch 3600: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:15:24,619 EPOCH 3601
2024-02-07 09:15:40,989 Epoch 3601: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:15:40,989 EPOCH 3602
2024-02-07 09:15:57,711 Epoch 3602: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:15:57,712 EPOCH 3603
2024-02-07 09:16:13,926 Epoch 3603: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:16:13,926 EPOCH 3604
2024-02-07 09:16:30,376 Epoch 3604: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:16:30,377 EPOCH 3605
2024-02-07 09:16:46,981 Epoch 3605: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:16:46,982 EPOCH 3606
2024-02-07 09:17:03,144 Epoch 3606: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:17:03,145 EPOCH 3607
2024-02-07 09:17:19,512 Epoch 3607: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:17:19,513 EPOCH 3608
2024-02-07 09:17:35,656 Epoch 3608: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:17:35,657 EPOCH 3609
2024-02-07 09:17:52,007 Epoch 3609: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:17:52,008 EPOCH 3610
2024-02-07 09:18:08,819 Epoch 3610: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:18:08,820 EPOCH 3611
2024-02-07 09:18:25,183 Epoch 3611: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:18:25,184 EPOCH 3612
2024-02-07 09:18:25,712 [Epoch: 3612 Step: 00032500] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:     2430 || Batch Translation Loss:   0.010945 => Txt Tokens per Sec:     6658 || Lr: 0.000100
2024-02-07 09:18:41,716 Epoch 3612: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:18:41,716 EPOCH 3613
2024-02-07 09:18:57,889 Epoch 3613: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:18:57,889 EPOCH 3614
2024-02-07 09:19:14,497 Epoch 3614: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:19:14,497 EPOCH 3615
2024-02-07 09:19:30,744 Epoch 3615: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-07 09:19:30,745 EPOCH 3616
2024-02-07 09:19:46,925 Epoch 3616: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-07 09:19:46,926 EPOCH 3617
2024-02-07 09:20:03,422 Epoch 3617: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 09:20:03,423 EPOCH 3618
2024-02-07 09:20:19,916 Epoch 3618: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-07 09:20:19,917 EPOCH 3619
2024-02-07 09:20:36,297 Epoch 3619: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 09:20:36,297 EPOCH 3620
2024-02-07 09:20:52,797 Epoch 3620: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 09:20:52,798 EPOCH 3621
2024-02-07 09:21:09,080 Epoch 3621: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 09:21:09,081 EPOCH 3622
2024-02-07 09:21:25,580 Epoch 3622: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 09:21:25,581 EPOCH 3623
2024-02-07 09:21:30,500 [Epoch: 3623 Step: 00032600] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:      337 || Batch Translation Loss:   0.006599 => Txt Tokens per Sec:     1066 || Lr: 0.000100
2024-02-07 09:21:41,971 Epoch 3623: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:21:41,971 EPOCH 3624
2024-02-07 09:21:58,274 Epoch 3624: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:21:58,275 EPOCH 3625
2024-02-07 09:22:14,241 Epoch 3625: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:22:14,242 EPOCH 3626
2024-02-07 09:22:31,022 Epoch 3626: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:22:31,023 EPOCH 3627
2024-02-07 09:22:47,365 Epoch 3627: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:22:47,366 EPOCH 3628
2024-02-07 09:23:03,981 Epoch 3628: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:23:03,982 EPOCH 3629
2024-02-07 09:23:20,103 Epoch 3629: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:23:20,104 EPOCH 3630
2024-02-07 09:23:36,518 Epoch 3630: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:23:36,519 EPOCH 3631
2024-02-07 09:23:52,893 Epoch 3631: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:23:52,894 EPOCH 3632
2024-02-07 09:24:09,329 Epoch 3632: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:24:09,329 EPOCH 3633
2024-02-07 09:24:25,831 Epoch 3633: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:24:25,831 EPOCH 3634
2024-02-07 09:24:30,808 [Epoch: 3634 Step: 00032700] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:      591 || Batch Translation Loss:   0.005890 => Txt Tokens per Sec:     1636 || Lr: 0.000100
2024-02-07 09:24:42,202 Epoch 3634: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:24:42,202 EPOCH 3635
2024-02-07 09:24:58,607 Epoch 3635: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:24:58,608 EPOCH 3636
2024-02-07 09:25:15,180 Epoch 3636: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:25:15,181 EPOCH 3637
2024-02-07 09:25:31,709 Epoch 3637: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-07 09:25:31,709 EPOCH 3638
2024-02-07 09:25:48,077 Epoch 3638: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-07 09:25:48,078 EPOCH 3639
2024-02-07 09:26:04,610 Epoch 3639: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-07 09:26:04,611 EPOCH 3640
2024-02-07 09:26:20,904 Epoch 3640: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-07 09:26:20,904 EPOCH 3641
2024-02-07 09:26:37,603 Epoch 3641: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-07 09:26:37,603 EPOCH 3642
2024-02-07 09:26:54,008 Epoch 3642: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-07 09:26:54,009 EPOCH 3643
2024-02-07 09:27:10,852 Epoch 3643: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-07 09:27:10,852 EPOCH 3644
2024-02-07 09:27:27,276 Epoch 3644: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-07 09:27:27,277 EPOCH 3645
2024-02-07 09:27:33,003 [Epoch: 3645 Step: 00032800] Batch Recognition Loss:   0.000522 => Gls Tokens per Sec:      737 || Batch Translation Loss:   0.051031 => Txt Tokens per Sec:     1993 || Lr: 0.000100
2024-02-07 09:27:43,791 Epoch 3645: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-07 09:27:43,791 EPOCH 3646
2024-02-07 09:28:00,343 Epoch 3646: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-07 09:28:00,343 EPOCH 3647
2024-02-07 09:28:16,657 Epoch 3647: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-07 09:28:16,657 EPOCH 3648
2024-02-07 09:28:33,198 Epoch 3648: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 09:28:33,198 EPOCH 3649
2024-02-07 09:28:49,611 Epoch 3649: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 09:28:49,611 EPOCH 3650
2024-02-07 09:29:06,082 Epoch 3650: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 09:29:06,082 EPOCH 3651
2024-02-07 09:29:22,604 Epoch 3651: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 09:29:22,604 EPOCH 3652
2024-02-07 09:29:38,774 Epoch 3652: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 09:29:38,774 EPOCH 3653
2024-02-07 09:29:55,356 Epoch 3653: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 09:29:55,356 EPOCH 3654
2024-02-07 09:30:11,754 Epoch 3654: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 09:30:11,755 EPOCH 3655
2024-02-07 09:30:28,040 Epoch 3655: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 09:30:28,040 EPOCH 3656
2024-02-07 09:30:34,226 [Epoch: 3656 Step: 00032900] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:      890 || Batch Translation Loss:   0.009371 => Txt Tokens per Sec:     2205 || Lr: 0.000100
2024-02-07 09:30:44,676 Epoch 3656: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 09:30:44,678 EPOCH 3657
2024-02-07 09:31:01,105 Epoch 3657: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 09:31:01,106 EPOCH 3658
2024-02-07 09:31:17,575 Epoch 3658: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 09:31:17,575 EPOCH 3659
2024-02-07 09:31:34,300 Epoch 3659: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 09:31:34,301 EPOCH 3660
2024-02-07 09:31:50,735 Epoch 3660: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 09:31:50,736 EPOCH 3661
2024-02-07 09:32:06,976 Epoch 3661: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 09:32:06,977 EPOCH 3662
2024-02-07 09:32:23,206 Epoch 3662: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 09:32:23,207 EPOCH 3663
2024-02-07 09:32:39,757 Epoch 3663: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 09:32:39,758 EPOCH 3664
2024-02-07 09:32:56,358 Epoch 3664: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 09:32:56,359 EPOCH 3665
2024-02-07 09:33:13,432 Epoch 3665: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 09:33:13,433 EPOCH 3666
2024-02-07 09:33:29,969 Epoch 3666: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 09:33:29,971 EPOCH 3667
2024-02-07 09:33:42,034 [Epoch: 3667 Step: 00033000] Batch Recognition Loss:   0.000309 => Gls Tokens per Sec:      637 || Batch Translation Loss:   0.017737 => Txt Tokens per Sec:     1895 || Lr: 0.000100
2024-02-07 09:33:47,233 Epoch 3667: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 09:33:47,234 EPOCH 3668
2024-02-07 09:34:03,758 Epoch 3668: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 09:34:03,758 EPOCH 3669
2024-02-07 09:34:20,901 Epoch 3669: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 09:34:20,902 EPOCH 3670
2024-02-07 09:34:38,080 Epoch 3670: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 09:34:38,081 EPOCH 3671
2024-02-07 09:34:54,836 Epoch 3671: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:34:54,837 EPOCH 3672
2024-02-07 09:35:11,646 Epoch 3672: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 09:35:11,647 EPOCH 3673
2024-02-07 09:35:28,120 Epoch 3673: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 09:35:28,121 EPOCH 3674
2024-02-07 09:35:44,981 Epoch 3674: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-07 09:35:44,983 EPOCH 3675
2024-02-07 09:36:01,538 Epoch 3675: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-07 09:36:01,539 EPOCH 3676
2024-02-07 09:36:17,907 Epoch 3676: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.63 
2024-02-07 09:36:17,908 EPOCH 3677
2024-02-07 09:36:34,452 Epoch 3677: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.33 
2024-02-07 09:36:34,454 EPOCH 3678
2024-02-07 09:36:44,095 [Epoch: 3678 Step: 00033100] Batch Recognition Loss:   0.007266 => Gls Tokens per Sec:      836 || Batch Translation Loss:   0.383648 => Txt Tokens per Sec:     2189 || Lr: 0.000100
2024-02-07 09:36:50,834 Epoch 3678: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.29 
2024-02-07 09:36:50,834 EPOCH 3679
2024-02-07 09:37:07,164 Epoch 3679: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.21 
2024-02-07 09:37:07,166 EPOCH 3680
2024-02-07 09:37:23,841 Epoch 3680: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-07 09:37:23,843 EPOCH 3681
2024-02-07 09:37:40,884 Epoch 3681: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-07 09:37:40,885 EPOCH 3682
2024-02-07 09:37:57,616 Epoch 3682: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-07 09:37:57,617 EPOCH 3683
2024-02-07 09:38:14,498 Epoch 3683: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-07 09:38:14,498 EPOCH 3684
2024-02-07 09:38:30,698 Epoch 3684: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-07 09:38:30,699 EPOCH 3685
2024-02-07 09:38:47,467 Epoch 3685: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-07 09:38:47,468 EPOCH 3686
2024-02-07 09:39:03,929 Epoch 3686: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 09:39:03,930 EPOCH 3687
2024-02-07 09:39:19,860 Epoch 3687: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 09:39:19,861 EPOCH 3688
2024-02-07 09:39:36,947 Epoch 3688: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 09:39:36,949 EPOCH 3689
2024-02-07 09:39:52,749 [Epoch: 3689 Step: 00033200] Batch Recognition Loss:   0.000502 => Gls Tokens per Sec:      591 || Batch Translation Loss:   0.022555 => Txt Tokens per Sec:     1612 || Lr: 0.000100
2024-02-07 09:39:53,458 Epoch 3689: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 09:39:53,458 EPOCH 3690
2024-02-07 09:40:10,116 Epoch 3690: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 09:40:10,116 EPOCH 3691
2024-02-07 09:40:26,257 Epoch 3691: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 09:40:26,258 EPOCH 3692
2024-02-07 09:40:42,858 Epoch 3692: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 09:40:42,859 EPOCH 3693
2024-02-07 09:40:59,718 Epoch 3693: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 09:40:59,719 EPOCH 3694
2024-02-07 09:41:16,039 Epoch 3694: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 09:41:16,040 EPOCH 3695
2024-02-07 09:41:33,155 Epoch 3695: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 09:41:33,156 EPOCH 3696
2024-02-07 09:41:49,414 Epoch 3696: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 09:41:49,416 EPOCH 3697
2024-02-07 09:42:06,200 Epoch 3697: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 09:42:06,202 EPOCH 3698
2024-02-07 09:42:22,970 Epoch 3698: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 09:42:22,972 EPOCH 3699
2024-02-07 09:42:39,402 Epoch 3699: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 09:42:39,403 EPOCH 3700
2024-02-07 09:42:56,509 [Epoch: 3700 Step: 00033300] Batch Recognition Loss:   0.000346 => Gls Tokens per Sec:      621 || Batch Translation Loss:   0.006982 => Txt Tokens per Sec:     1718 || Lr: 0.000100
2024-02-07 09:42:56,510 Epoch 3700: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 09:42:56,510 EPOCH 3701
2024-02-07 09:43:13,237 Epoch 3701: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 09:43:13,238 EPOCH 3702
2024-02-07 09:43:30,184 Epoch 3702: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 09:43:30,185 EPOCH 3703
2024-02-07 09:43:46,840 Epoch 3703: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:43:46,842 EPOCH 3704
2024-02-07 09:44:03,469 Epoch 3704: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:44:03,470 EPOCH 3705
2024-02-07 09:44:19,939 Epoch 3705: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 09:44:19,940 EPOCH 3706
2024-02-07 09:44:36,491 Epoch 3706: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 09:44:36,492 EPOCH 3707
2024-02-07 09:44:53,203 Epoch 3707: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 09:44:53,204 EPOCH 3708
2024-02-07 09:45:09,892 Epoch 3708: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 09:45:09,893 EPOCH 3709
2024-02-07 09:45:26,391 Epoch 3709: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:45:26,391 EPOCH 3710
2024-02-07 09:45:43,067 Epoch 3710: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:45:43,069 EPOCH 3711
2024-02-07 09:45:59,665 Epoch 3711: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:45:59,667 EPOCH 3712
2024-02-07 09:46:00,142 [Epoch: 3712 Step: 00033400] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     2706 || Batch Translation Loss:   0.015199 => Txt Tokens per Sec:     7417 || Lr: 0.000100
2024-02-07 09:46:16,441 Epoch 3712: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:46:16,443 EPOCH 3713
2024-02-07 09:46:33,083 Epoch 3713: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 09:46:33,084 EPOCH 3714
2024-02-07 09:46:49,441 Epoch 3714: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 09:46:49,443 EPOCH 3715
2024-02-07 09:47:06,279 Epoch 3715: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:47:06,280 EPOCH 3716
2024-02-07 09:47:22,907 Epoch 3716: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:47:22,908 EPOCH 3717
2024-02-07 09:47:39,664 Epoch 3717: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:47:39,665 EPOCH 3718
2024-02-07 09:47:55,988 Epoch 3718: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:47:55,989 EPOCH 3719
2024-02-07 09:48:12,882 Epoch 3719: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:48:12,883 EPOCH 3720
2024-02-07 09:48:29,362 Epoch 3720: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:48:29,362 EPOCH 3721
2024-02-07 09:48:45,793 Epoch 3721: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:48:45,794 EPOCH 3722
2024-02-07 09:49:02,786 Epoch 3722: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:49:02,786 EPOCH 3723
2024-02-07 09:49:09,554 [Epoch: 3723 Step: 00033500] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:      378 || Batch Translation Loss:   0.010745 => Txt Tokens per Sec:     1136 || Lr: 0.000100
2024-02-07 09:49:19,742 Epoch 3723: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 09:49:19,743 EPOCH 3724
2024-02-07 09:49:36,082 Epoch 3724: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:49:36,083 EPOCH 3725
2024-02-07 09:49:52,721 Epoch 3725: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:49:52,721 EPOCH 3726
2024-02-07 09:50:09,005 Epoch 3726: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:50:09,007 EPOCH 3727
2024-02-07 09:50:25,940 Epoch 3727: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:50:25,941 EPOCH 3728
2024-02-07 09:50:42,278 Epoch 3728: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:50:42,279 EPOCH 3729
2024-02-07 09:50:59,057 Epoch 3729: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:50:59,058 EPOCH 3730
2024-02-07 09:51:16,004 Epoch 3730: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:51:16,005 EPOCH 3731
2024-02-07 09:51:32,458 Epoch 3731: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:51:32,459 EPOCH 3732
2024-02-07 09:51:49,176 Epoch 3732: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:51:49,178 EPOCH 3733
2024-02-07 09:52:06,708 Epoch 3733: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:52:06,709 EPOCH 3734
2024-02-07 09:52:11,949 [Epoch: 3734 Step: 00033600] Batch Recognition Loss:   0.000257 => Gls Tokens per Sec:      561 || Batch Translation Loss:   0.011217 => Txt Tokens per Sec:     1555 || Lr: 0.000100
2024-02-07 09:52:23,668 Epoch 3734: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:52:23,670 EPOCH 3735
2024-02-07 09:52:40,834 Epoch 3735: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:52:40,835 EPOCH 3736
2024-02-07 09:52:57,621 Epoch 3736: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:52:57,623 EPOCH 3737
2024-02-07 09:53:14,692 Epoch 3737: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:53:14,694 EPOCH 3738
2024-02-07 09:53:32,119 Epoch 3738: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 09:53:32,120 EPOCH 3739
2024-02-07 09:53:49,020 Epoch 3739: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 09:53:49,021 EPOCH 3740
2024-02-07 09:54:05,621 Epoch 3740: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 09:54:05,621 EPOCH 3741
2024-02-07 09:54:22,000 Epoch 3741: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 09:54:22,001 EPOCH 3742
2024-02-07 09:54:38,501 Epoch 3742: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 09:54:38,502 EPOCH 3743
2024-02-07 09:54:55,000 Epoch 3743: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 09:54:55,002 EPOCH 3744
2024-02-07 09:55:11,870 Epoch 3744: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 09:55:11,871 EPOCH 3745
2024-02-07 09:55:16,273 [Epoch: 3745 Step: 00033700] Batch Recognition Loss:   0.001144 => Gls Tokens per Sec:     1164 || Batch Translation Loss:   0.014543 => Txt Tokens per Sec:     2932 || Lr: 0.000100
2024-02-07 09:55:28,923 Epoch 3745: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 09:55:28,924 EPOCH 3746
2024-02-07 09:55:45,380 Epoch 3746: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 09:55:45,380 EPOCH 3747
2024-02-07 09:56:01,533 Epoch 3747: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 09:56:01,534 EPOCH 3748
2024-02-07 09:56:18,323 Epoch 3748: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-07 09:56:18,324 EPOCH 3749
2024-02-07 09:56:34,961 Epoch 3749: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-07 09:56:34,962 EPOCH 3750
2024-02-07 09:56:51,481 Epoch 3750: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-07 09:56:51,482 EPOCH 3751
2024-02-07 09:57:08,516 Epoch 3751: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-07 09:57:08,517 EPOCH 3752
2024-02-07 09:57:25,094 Epoch 3752: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-07 09:57:25,095 EPOCH 3753
2024-02-07 09:57:42,013 Epoch 3753: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-07 09:57:42,014 EPOCH 3754
2024-02-07 09:57:58,545 Epoch 3754: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-07 09:57:58,546 EPOCH 3755
2024-02-07 09:58:14,911 Epoch 3755: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-07 09:58:14,913 EPOCH 3756
2024-02-07 09:58:20,495 [Epoch: 3756 Step: 00033800] Batch Recognition Loss:   0.000647 => Gls Tokens per Sec:     1147 || Batch Translation Loss:   0.075605 => Txt Tokens per Sec:     3140 || Lr: 0.000100
2024-02-07 09:58:31,578 Epoch 3756: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-07 09:58:31,579 EPOCH 3757
2024-02-07 09:58:48,676 Epoch 3757: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-07 09:58:48,677 EPOCH 3758
2024-02-07 09:59:05,541 Epoch 3758: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-07 09:59:05,543 EPOCH 3759
2024-02-07 09:59:23,032 Epoch 3759: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-07 09:59:23,034 EPOCH 3760
2024-02-07 09:59:40,106 Epoch 3760: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-07 09:59:40,108 EPOCH 3761
2024-02-07 09:59:56,872 Epoch 3761: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-07 09:59:56,872 EPOCH 3762
2024-02-07 10:00:13,448 Epoch 3762: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-07 10:00:13,448 EPOCH 3763
2024-02-07 10:00:30,420 Epoch 3763: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-07 10:00:30,422 EPOCH 3764
2024-02-07 10:00:47,363 Epoch 3764: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-07 10:00:47,363 EPOCH 3765
2024-02-07 10:01:03,859 Epoch 3765: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-07 10:01:03,859 EPOCH 3766
2024-02-07 10:01:20,504 Epoch 3766: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-07 10:01:20,504 EPOCH 3767
2024-02-07 10:01:33,148 [Epoch: 3767 Step: 00033900] Batch Recognition Loss:   0.001692 => Gls Tokens per Sec:      536 || Batch Translation Loss:   0.053683 => Txt Tokens per Sec:     1520 || Lr: 0.000100
2024-02-07 10:01:37,604 Epoch 3767: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-07 10:01:37,604 EPOCH 3768
2024-02-07 10:01:54,510 Epoch 3768: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-07 10:01:54,510 EPOCH 3769
2024-02-07 10:02:11,256 Epoch 3769: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-07 10:02:11,257 EPOCH 3770
2024-02-07 10:02:28,135 Epoch 3770: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-07 10:02:28,136 EPOCH 3771
2024-02-07 10:02:44,542 Epoch 3771: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 10:02:44,543 EPOCH 3772
2024-02-07 10:03:01,097 Epoch 3772: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 10:03:01,098 EPOCH 3773
2024-02-07 10:03:17,899 Epoch 3773: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 10:03:17,900 EPOCH 3774
2024-02-07 10:03:35,040 Epoch 3774: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 10:03:35,042 EPOCH 3775
2024-02-07 10:03:51,797 Epoch 3775: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 10:03:51,798 EPOCH 3776
2024-02-07 10:04:08,356 Epoch 3776: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-07 10:04:08,357 EPOCH 3777
2024-02-07 10:04:25,471 Epoch 3777: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 10:04:25,472 EPOCH 3778
2024-02-07 10:04:41,489 [Epoch: 3778 Step: 00034000] Batch Recognition Loss:   0.000606 => Gls Tokens per Sec:      503 || Batch Translation Loss:   0.013680 => Txt Tokens per Sec:     1488 || Lr: 0.000100
2024-02-07 10:05:49,140 Validation result at epoch 3778, step    34000: duration: 67.6499s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.31221	Translation Loss: 97719.20312	PPL: 17330.23828
	Eval Metric: BLEU
	WER 2.97	(DEL: 0.00,	INS: 0.00,	SUB: 2.97)
	BLEU-4 0.39	(BLEU-1: 10.11,	BLEU-2: 3.03,	BLEU-3: 0.94,	BLEU-4: 0.39)
	CHRF 16.44	ROUGE 8.53
2024-02-07 10:05:49,142 Logging Recognition and Translation Outputs
2024-02-07 10:05:49,142 ========================================================================================================================
2024-02-07 10:05:49,142 Logging Sequence: 171_158.00
2024-02-07 10:05:49,143 	Gloss Reference :	A B+C+D+E
2024-02-07 10:05:49,143 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 10:05:49,143 	Gloss Alignment :	         
2024-02-07 10:05:49,144 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 10:05:49,146 	Text Reference  :	with speculations of dhoni being banned are spreading many   say  that   it is unlikely to  happen
2024-02-07 10:05:49,146 	Text Hypothesis :	**** ************ ** ***** ***** ****** *** shocking  people were hoping it ** does     not rain  
2024-02-07 10:05:49,146 	Text Alignment  :	D    D            D  D     D     D      D   S         S      S    S         D  S        S   S     
2024-02-07 10:05:49,146 ========================================================================================================================
2024-02-07 10:05:49,146 Logging Sequence: 108_235.00
2024-02-07 10:05:49,147 	Gloss Reference :	A B+C+D+E
2024-02-07 10:05:49,147 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 10:05:49,147 	Gloss Alignment :	         
2024-02-07 10:05:49,147 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 10:05:49,149 	Text Reference  :	he was taken to the hospital and    it was reported that he   is  not       in any danger
2024-02-07 10:05:49,149 	Text Hypothesis :	** *** ***** ** *** even     though he was ******** a    huge fan following in the 2020  
2024-02-07 10:05:49,149 	Text Alignment  :	D  D   D     D  D   S        S      S      D        S    S    S   S            S   S     
2024-02-07 10:05:49,149 ========================================================================================================================
2024-02-07 10:05:49,149 Logging Sequence: 153_206.00
2024-02-07 10:05:49,149 	Gloss Reference :	A B+C+D+E
2024-02-07 10:05:49,149 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 10:05:49,150 	Gloss Alignment :	         
2024-02-07 10:05:49,150 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 10:05:49,151 	Text Reference  :	*** ****** *** ** now       on   13th november everyone is     hoping pakistan rewrites history
2024-02-07 10:05:49,151 	Text Hypothesis :	the couple met in australia both men  and      south    africa during the      visuals  love   
2024-02-07 10:05:49,151 	Text Alignment  :	I   I      I   I  S         S    S    S        S        S      S      S        S        S      
2024-02-07 10:05:49,151 ========================================================================================================================
2024-02-07 10:05:49,151 Logging Sequence: 87_202.00
2024-02-07 10:05:49,152 	Gloss Reference :	A B+C+D+E
2024-02-07 10:05:49,152 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 10:05:49,152 	Gloss Alignment :	         
2024-02-07 10:05:49,152 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 10:05:49,153 	Text Reference  :	** *** **** **** ********* ******** i   love  our     players and   i    love my country
2024-02-07 10:05:49,153 	Text Hypothesis :	do you know that wikipedia provides was being jealous of      india with some of india  
2024-02-07 10:05:49,153 	Text Alignment  :	I  I   I    I    I         I        S   S     S       S       S     S    S    S  S      
2024-02-07 10:05:49,153 ========================================================================================================================
2024-02-07 10:05:49,154 Logging Sequence: 84_2.00
2024-02-07 10:05:49,154 	Gloss Reference :	A B+C+D+E
2024-02-07 10:05:49,154 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 10:05:49,154 	Gloss Alignment :	         
2024-02-07 10:05:49,154 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 10:05:49,156 	Text Reference  :	the 2022   fifa         football world  cup is    going on    in  qatar from    20th   november 2022    to    18th december 2022   
2024-02-07 10:05:49,157 	Text Hypothesis :	*** police instructions even     subway and viral have  taken for the   highest number of       omicron cases at   the      auction
2024-02-07 10:05:49,157 	Text Alignment  :	D   S      S            S        S      S   S     S     S     S   S     S       S      S        S       S     S    S        S      
2024-02-07 10:05:49,157 ========================================================================================================================
2024-02-07 10:05:50,035 Epoch 3778: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 10:05:50,035 EPOCH 3779
2024-02-07 10:06:07,051 Epoch 3779: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 10:06:07,051 EPOCH 3780
2024-02-07 10:06:23,489 Epoch 3780: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 10:06:23,489 EPOCH 3781
2024-02-07 10:06:39,813 Epoch 3781: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 10:06:39,814 EPOCH 3782
2024-02-07 10:06:56,149 Epoch 3782: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 10:06:56,149 EPOCH 3783
2024-02-07 10:07:12,538 Epoch 3783: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 10:07:12,539 EPOCH 3784
2024-02-07 10:07:29,460 Epoch 3784: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 10:07:29,461 EPOCH 3785
2024-02-07 10:07:45,752 Epoch 3785: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 10:07:45,752 EPOCH 3786
2024-02-07 10:08:02,129 Epoch 3786: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 10:08:02,130 EPOCH 3787
2024-02-07 10:08:18,569 Epoch 3787: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 10:08:18,570 EPOCH 3788
2024-02-07 10:08:34,849 Epoch 3788: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 10:08:34,850 EPOCH 3789
2024-02-07 10:08:47,066 [Epoch: 3789 Step: 00034100] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:      838 || Batch Translation Loss:   0.012240 => Txt Tokens per Sec:     2295 || Lr: 0.000100
2024-02-07 10:08:51,375 Epoch 3789: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 10:08:51,375 EPOCH 3790
2024-02-07 10:09:07,541 Epoch 3790: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:09:07,542 EPOCH 3791
2024-02-07 10:09:24,413 Epoch 3791: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:09:24,413 EPOCH 3792
2024-02-07 10:09:40,932 Epoch 3792: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:09:40,933 EPOCH 3793
2024-02-07 10:09:57,264 Epoch 3793: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:09:57,265 EPOCH 3794
2024-02-07 10:10:13,543 Epoch 3794: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:10:13,543 EPOCH 3795
2024-02-07 10:10:29,962 Epoch 3795: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 10:10:29,962 EPOCH 3796
2024-02-07 10:10:46,576 Epoch 3796: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:10:46,577 EPOCH 3797
2024-02-07 10:11:03,131 Epoch 3797: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:11:03,131 EPOCH 3798
2024-02-07 10:11:19,635 Epoch 3798: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:11:19,635 EPOCH 3799
2024-02-07 10:11:36,002 Epoch 3799: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-07 10:11:36,002 EPOCH 3800
2024-02-07 10:11:52,273 [Epoch: 3800 Step: 00034200] Batch Recognition Loss:   0.000292 => Gls Tokens per Sec:      653 || Batch Translation Loss:   0.007005 => Txt Tokens per Sec:     1806 || Lr: 0.000100
2024-02-07 10:11:52,274 Epoch 3800: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 10:11:52,274 EPOCH 3801
2024-02-07 10:12:08,611 Epoch 3801: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 10:12:08,611 EPOCH 3802
2024-02-07 10:12:24,912 Epoch 3802: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 10:12:24,912 EPOCH 3803
2024-02-07 10:12:41,239 Epoch 3803: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 10:12:41,240 EPOCH 3804
2024-02-07 10:12:57,824 Epoch 3804: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:12:57,825 EPOCH 3805
2024-02-07 10:13:14,558 Epoch 3805: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:13:14,558 EPOCH 3806
2024-02-07 10:13:30,950 Epoch 3806: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:13:30,950 EPOCH 3807
2024-02-07 10:13:47,839 Epoch 3807: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:13:47,841 EPOCH 3808
2024-02-07 10:14:04,481 Epoch 3808: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:14:04,482 EPOCH 3809
2024-02-07 10:14:21,074 Epoch 3809: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-07 10:14:21,074 EPOCH 3810
2024-02-07 10:14:37,361 Epoch 3810: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:14:37,362 EPOCH 3811
2024-02-07 10:14:53,598 Epoch 3811: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:14:53,598 EPOCH 3812
2024-02-07 10:14:59,849 [Epoch: 3812 Step: 00034300] Batch Recognition Loss:   0.000571 => Gls Tokens per Sec:      205 || Batch Translation Loss:   0.017530 => Txt Tokens per Sec:      707 || Lr: 0.000100
2024-02-07 10:15:10,381 Epoch 3812: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:15:10,382 EPOCH 3813
2024-02-07 10:15:26,989 Epoch 3813: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:15:26,989 EPOCH 3814
2024-02-07 10:15:43,143 Epoch 3814: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:15:43,143 EPOCH 3815
2024-02-07 10:15:59,580 Epoch 3815: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:15:59,580 EPOCH 3816
2024-02-07 10:16:16,301 Epoch 3816: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:16:16,302 EPOCH 3817
2024-02-07 10:16:32,850 Epoch 3817: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:16:32,851 EPOCH 3818
2024-02-07 10:16:49,143 Epoch 3818: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:16:49,144 EPOCH 3819
2024-02-07 10:17:05,813 Epoch 3819: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:17:05,814 EPOCH 3820
2024-02-07 10:17:22,303 Epoch 3820: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:17:22,304 EPOCH 3821
2024-02-07 10:17:38,653 Epoch 3821: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:17:38,653 EPOCH 3822
2024-02-07 10:17:55,029 Epoch 3822: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:17:55,030 EPOCH 3823
2024-02-07 10:17:58,647 [Epoch: 3823 Step: 00034400] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:      708 || Batch Translation Loss:   0.010366 => Txt Tokens per Sec:     2047 || Lr: 0.000100
2024-02-07 10:18:11,584 Epoch 3823: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 10:18:11,585 EPOCH 3824
2024-02-07 10:18:28,056 Epoch 3824: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 10:18:28,057 EPOCH 3825
2024-02-07 10:18:44,693 Epoch 3825: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 10:18:44,694 EPOCH 3826
2024-02-07 10:19:01,434 Epoch 3826: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 10:19:01,435 EPOCH 3827
2024-02-07 10:19:17,858 Epoch 3827: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 10:19:17,858 EPOCH 3828
2024-02-07 10:19:34,301 Epoch 3828: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-07 10:19:34,302 EPOCH 3829
2024-02-07 10:19:50,953 Epoch 3829: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-07 10:19:50,954 EPOCH 3830
2024-02-07 10:20:07,403 Epoch 3830: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-07 10:20:07,403 EPOCH 3831
2024-02-07 10:20:23,975 Epoch 3831: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-07 10:20:23,975 EPOCH 3832
2024-02-07 10:20:40,681 Epoch 3832: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-07 10:20:40,682 EPOCH 3833
2024-02-07 10:20:56,999 Epoch 3833: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-07 10:20:57,000 EPOCH 3834
2024-02-07 10:21:00,904 [Epoch: 3834 Step: 00034500] Batch Recognition Loss:   0.000611 => Gls Tokens per Sec:      984 || Batch Translation Loss:   0.126494 => Txt Tokens per Sec:     2377 || Lr: 0.000100
2024-02-07 10:21:13,822 Epoch 3834: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-07 10:21:13,823 EPOCH 3835
2024-02-07 10:21:30,464 Epoch 3835: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.73 
2024-02-07 10:21:30,465 EPOCH 3836
2024-02-07 10:21:47,198 Epoch 3836: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.62 
2024-02-07 10:21:47,198 EPOCH 3837
2024-02-07 10:22:03,477 Epoch 3837: Total Training Recognition Loss 0.05  Total Training Translation Loss 14.33 
2024-02-07 10:22:03,478 EPOCH 3838
2024-02-07 10:22:19,873 Epoch 3838: Total Training Recognition Loss 0.06  Total Training Translation Loss 8.17 
2024-02-07 10:22:19,874 EPOCH 3839
2024-02-07 10:22:36,478 Epoch 3839: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.83 
2024-02-07 10:22:36,478 EPOCH 3840
2024-02-07 10:22:52,735 Epoch 3840: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.41 
2024-02-07 10:22:52,736 EPOCH 3841
2024-02-07 10:23:09,144 Epoch 3841: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.85 
2024-02-07 10:23:09,145 EPOCH 3842
2024-02-07 10:23:25,368 Epoch 3842: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-07 10:23:25,369 EPOCH 3843
2024-02-07 10:23:41,675 Epoch 3843: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-07 10:23:41,676 EPOCH 3844
2024-02-07 10:23:58,350 Epoch 3844: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-07 10:23:58,350 EPOCH 3845
2024-02-07 10:24:00,262 [Epoch: 3845 Step: 00034600] Batch Recognition Loss:   0.000975 => Gls Tokens per Sec:     2679 || Batch Translation Loss:   0.039296 => Txt Tokens per Sec:     7021 || Lr: 0.000100
2024-02-07 10:24:14,645 Epoch 3845: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-07 10:24:14,646 EPOCH 3846
2024-02-07 10:24:31,027 Epoch 3846: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-07 10:24:31,027 EPOCH 3847
2024-02-07 10:24:47,381 Epoch 3847: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 10:24:47,382 EPOCH 3848
2024-02-07 10:25:03,607 Epoch 3848: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 10:25:03,608 EPOCH 3849
2024-02-07 10:25:20,183 Epoch 3849: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 10:25:20,184 EPOCH 3850
2024-02-07 10:25:36,333 Epoch 3850: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 10:25:36,333 EPOCH 3851
2024-02-07 10:25:52,510 Epoch 3851: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 10:25:52,511 EPOCH 3852
2024-02-07 10:26:09,112 Epoch 3852: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 10:26:09,113 EPOCH 3853
2024-02-07 10:26:25,551 Epoch 3853: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 10:26:25,551 EPOCH 3854
2024-02-07 10:26:42,039 Epoch 3854: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 10:26:42,039 EPOCH 3855
2024-02-07 10:26:58,169 Epoch 3855: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 10:26:58,170 EPOCH 3856
2024-02-07 10:27:12,507 [Epoch: 3856 Step: 00034700] Batch Recognition Loss:   0.000378 => Gls Tokens per Sec:      384 || Batch Translation Loss:   0.015286 => Txt Tokens per Sec:     1163 || Lr: 0.000100
2024-02-07 10:27:14,316 Epoch 3856: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 10:27:14,316 EPOCH 3857
2024-02-07 10:27:30,815 Epoch 3857: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 10:27:30,816 EPOCH 3858
2024-02-07 10:27:47,185 Epoch 3858: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 10:27:47,186 EPOCH 3859
2024-02-07 10:28:03,720 Epoch 3859: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 10:28:03,721 EPOCH 3860
2024-02-07 10:28:20,320 Epoch 3860: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 10:28:20,321 EPOCH 3861
2024-02-07 10:28:36,888 Epoch 3861: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 10:28:36,888 EPOCH 3862
2024-02-07 10:28:53,674 Epoch 3862: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 10:28:53,675 EPOCH 3863
2024-02-07 10:29:10,158 Epoch 3863: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 10:29:10,159 EPOCH 3864
2024-02-07 10:29:26,708 Epoch 3864: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 10:29:26,708 EPOCH 3865
2024-02-07 10:29:43,093 Epoch 3865: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 10:29:43,094 EPOCH 3866
2024-02-07 10:29:59,688 Epoch 3866: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 10:29:59,689 EPOCH 3867
2024-02-07 10:30:11,190 [Epoch: 3867 Step: 00034800] Batch Recognition Loss:   0.001493 => Gls Tokens per Sec:      668 || Batch Translation Loss:   0.018784 => Txt Tokens per Sec:     1990 || Lr: 0.000100
2024-02-07 10:30:16,229 Epoch 3867: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 10:30:16,230 EPOCH 3868
2024-02-07 10:30:32,633 Epoch 3868: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 10:30:32,634 EPOCH 3869
2024-02-07 10:30:49,081 Epoch 3869: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 10:30:49,082 EPOCH 3870
2024-02-07 10:31:05,415 Epoch 3870: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 10:31:05,416 EPOCH 3871
2024-02-07 10:31:21,959 Epoch 3871: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 10:31:21,960 EPOCH 3872
2024-02-07 10:31:38,528 Epoch 3872: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 10:31:38,530 EPOCH 3873
2024-02-07 10:31:55,195 Epoch 3873: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 10:31:55,195 EPOCH 3874
2024-02-07 10:32:11,504 Epoch 3874: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 10:32:11,505 EPOCH 3875
2024-02-07 10:32:28,280 Epoch 3875: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 10:32:28,281 EPOCH 3876
2024-02-07 10:32:44,660 Epoch 3876: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 10:32:44,660 EPOCH 3877
2024-02-07 10:33:01,085 Epoch 3877: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 10:33:01,086 EPOCH 3878
2024-02-07 10:33:16,285 [Epoch: 3878 Step: 00034900] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:      530 || Batch Translation Loss:   0.008314 => Txt Tokens per Sec:     1445 || Lr: 0.000100
2024-02-07 10:33:17,570 Epoch 3878: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 10:33:17,570 EPOCH 3879
2024-02-07 10:33:33,698 Epoch 3879: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 10:33:33,699 EPOCH 3880
2024-02-07 10:33:50,179 Epoch 3880: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 10:33:50,179 EPOCH 3881
2024-02-07 10:34:06,537 Epoch 3881: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 10:34:06,538 EPOCH 3882
2024-02-07 10:34:23,251 Epoch 3882: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 10:34:23,252 EPOCH 3883
2024-02-07 10:34:39,442 Epoch 3883: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 10:34:39,443 EPOCH 3884
2024-02-07 10:34:55,827 Epoch 3884: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 10:34:55,828 EPOCH 3885
2024-02-07 10:35:12,284 Epoch 3885: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:35:12,285 EPOCH 3886
2024-02-07 10:35:28,657 Epoch 3886: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 10:35:28,658 EPOCH 3887
2024-02-07 10:35:45,169 Epoch 3887: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 10:35:45,169 EPOCH 3888
2024-02-07 10:36:01,486 Epoch 3888: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:36:01,486 EPOCH 3889
2024-02-07 10:36:17,496 [Epoch: 3889 Step: 00035000] Batch Recognition Loss:   0.002163 => Gls Tokens per Sec:      583 || Batch Translation Loss:   0.017508 => Txt Tokens per Sec:     1613 || Lr: 0.000100
2024-02-07 10:36:18,118 Epoch 3889: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-07 10:36:18,118 EPOCH 3890
2024-02-07 10:36:34,553 Epoch 3890: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:36:34,554 EPOCH 3891
2024-02-07 10:36:51,039 Epoch 3891: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:36:51,039 EPOCH 3892
2024-02-07 10:37:07,690 Epoch 3892: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:37:07,691 EPOCH 3893
2024-02-07 10:37:24,236 Epoch 3893: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:37:24,237 EPOCH 3894
2024-02-07 10:37:40,447 Epoch 3894: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:37:40,449 EPOCH 3895
2024-02-07 10:37:57,093 Epoch 3895: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:37:57,093 EPOCH 3896
2024-02-07 10:38:13,508 Epoch 3896: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:38:13,509 EPOCH 3897
2024-02-07 10:38:30,255 Epoch 3897: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:38:30,255 EPOCH 3898
2024-02-07 10:38:46,607 Epoch 3898: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 10:38:46,608 EPOCH 3899
2024-02-07 10:39:02,974 Epoch 3899: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:39:02,975 EPOCH 3900
2024-02-07 10:39:19,614 [Epoch: 3900 Step: 00035100] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:      638 || Batch Translation Loss:   0.010458 => Txt Tokens per Sec:     1766 || Lr: 0.000100
2024-02-07 10:39:19,615 Epoch 3900: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:39:19,615 EPOCH 3901
2024-02-07 10:39:36,059 Epoch 3901: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:39:36,060 EPOCH 3902
2024-02-07 10:39:52,561 Epoch 3902: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:39:52,562 EPOCH 3903
2024-02-07 10:40:08,849 Epoch 3903: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:40:08,850 EPOCH 3904
2024-02-07 10:40:25,176 Epoch 3904: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:40:25,177 EPOCH 3905
2024-02-07 10:40:41,716 Epoch 3905: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:40:41,716 EPOCH 3906
2024-02-07 10:40:58,055 Epoch 3906: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-07 10:40:58,056 EPOCH 3907
2024-02-07 10:41:15,117 Epoch 3907: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:41:15,117 EPOCH 3908
2024-02-07 10:41:31,411 Epoch 3908: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:41:31,412 EPOCH 3909
2024-02-07 10:41:47,826 Epoch 3909: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:41:47,827 EPOCH 3910
2024-02-07 10:42:04,193 Epoch 3910: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:42:04,194 EPOCH 3911
2024-02-07 10:42:20,592 Epoch 3911: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:42:20,593 EPOCH 3912
2024-02-07 10:42:25,010 [Epoch: 3912 Step: 00035200] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:       86 || Batch Translation Loss:   0.006269 => Txt Tokens per Sec:      306 || Lr: 0.000100
2024-02-07 10:42:37,283 Epoch 3912: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:42:37,283 EPOCH 3913
2024-02-07 10:42:53,829 Epoch 3913: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:42:53,829 EPOCH 3914
2024-02-07 10:43:10,500 Epoch 3914: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 10:43:10,500 EPOCH 3915
2024-02-07 10:43:27,188 Epoch 3915: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-07 10:43:27,189 EPOCH 3916
2024-02-07 10:43:43,333 Epoch 3916: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:43:43,334 EPOCH 3917
2024-02-07 10:44:00,269 Epoch 3917: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:44:00,271 EPOCH 3918
2024-02-07 10:44:16,490 Epoch 3918: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:44:16,490 EPOCH 3919
2024-02-07 10:44:32,938 Epoch 3919: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:44:32,938 EPOCH 3920
2024-02-07 10:44:49,290 Epoch 3920: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 10:44:49,290 EPOCH 3921
2024-02-07 10:45:05,703 Epoch 3921: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 10:45:05,704 EPOCH 3922
2024-02-07 10:45:21,879 Epoch 3922: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 10:45:21,880 EPOCH 3923
2024-02-07 10:45:26,690 [Epoch: 3923 Step: 00035300] Batch Recognition Loss:   0.001255 => Gls Tokens per Sec:      345 || Batch Translation Loss:   0.006612 => Txt Tokens per Sec:      870 || Lr: 0.000100
2024-02-07 10:45:38,674 Epoch 3923: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 10:45:38,675 EPOCH 3924
2024-02-07 10:45:54,864 Epoch 3924: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 10:45:54,865 EPOCH 3925
2024-02-07 10:46:11,089 Epoch 3925: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 10:46:11,089 EPOCH 3926
2024-02-07 10:46:27,664 Epoch 3926: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 10:46:27,665 EPOCH 3927
2024-02-07 10:46:44,053 Epoch 3927: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 10:46:44,054 EPOCH 3928
2024-02-07 10:47:00,742 Epoch 3928: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 10:47:00,743 EPOCH 3929
2024-02-07 10:47:17,024 Epoch 3929: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 10:47:17,025 EPOCH 3930
2024-02-07 10:47:33,492 Epoch 3930: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 10:47:33,493 EPOCH 3931
2024-02-07 10:47:50,365 Epoch 3931: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 10:47:50,366 EPOCH 3932
2024-02-07 10:48:06,599 Epoch 3932: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 10:48:06,600 EPOCH 3933
2024-02-07 10:48:23,166 Epoch 3933: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 10:48:23,167 EPOCH 3934
2024-02-07 10:48:29,721 [Epoch: 3934 Step: 00035400] Batch Recognition Loss:   0.000627 => Gls Tokens per Sec:      586 || Batch Translation Loss:   0.018617 => Txt Tokens per Sec:     1520 || Lr: 0.000100
2024-02-07 10:48:39,418 Epoch 3934: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 10:48:39,418 EPOCH 3935
2024-02-07 10:48:55,793 Epoch 3935: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 10:48:55,794 EPOCH 3936
2024-02-07 10:49:12,112 Epoch 3936: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 10:49:12,113 EPOCH 3937
2024-02-07 10:49:28,659 Epoch 3937: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 10:49:28,661 EPOCH 3938
2024-02-07 10:49:45,136 Epoch 3938: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-07 10:49:45,137 EPOCH 3939
2024-02-07 10:50:01,479 Epoch 3939: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 10:50:01,479 EPOCH 3940
2024-02-07 10:50:17,589 Epoch 3940: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-07 10:50:17,590 EPOCH 3941
2024-02-07 10:50:34,158 Epoch 3941: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-07 10:50:34,158 EPOCH 3942
2024-02-07 10:50:50,657 Epoch 3942: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-07 10:50:50,657 EPOCH 3943
2024-02-07 10:51:07,409 Epoch 3943: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-07 10:51:07,409 EPOCH 3944
2024-02-07 10:51:23,682 Epoch 3944: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-07 10:51:23,683 EPOCH 3945
2024-02-07 10:51:37,855 [Epoch: 3945 Step: 00035500] Batch Recognition Loss:   0.001181 => Gls Tokens per Sec:      298 || Batch Translation Loss:   0.098354 => Txt Tokens per Sec:      945 || Lr: 0.000100
2024-02-07 10:51:40,407 Epoch 3945: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-07 10:51:40,407 EPOCH 3946
2024-02-07 10:51:56,744 Epoch 3946: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-07 10:51:56,744 EPOCH 3947
2024-02-07 10:52:13,331 Epoch 3947: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-07 10:52:13,332 EPOCH 3948
2024-02-07 10:52:29,817 Epoch 3948: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-07 10:52:29,817 EPOCH 3949
2024-02-07 10:52:46,476 Epoch 3949: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-07 10:52:46,476 EPOCH 3950
2024-02-07 10:53:02,856 Epoch 3950: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-07 10:53:02,857 EPOCH 3951
2024-02-07 10:53:19,622 Epoch 3951: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-07 10:53:19,623 EPOCH 3952
2024-02-07 10:53:36,040 Epoch 3952: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 10:53:36,040 EPOCH 3953
2024-02-07 10:53:52,369 Epoch 3953: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-07 10:53:52,369 EPOCH 3954
2024-02-07 10:54:08,857 Epoch 3954: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 10:54:08,858 EPOCH 3955
2024-02-07 10:54:25,235 Epoch 3955: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-07 10:54:25,236 EPOCH 3956
2024-02-07 10:54:33,091 [Epoch: 3956 Step: 00035600] Batch Recognition Loss:   0.000210 => Gls Tokens per Sec:      815 || Batch Translation Loss:   0.016158 => Txt Tokens per Sec:     2139 || Lr: 0.000100
2024-02-07 10:54:41,812 Epoch 3956: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-07 10:54:41,813 EPOCH 3957
2024-02-07 10:54:58,104 Epoch 3957: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-07 10:54:58,104 EPOCH 3958
2024-02-07 10:55:14,655 Epoch 3958: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-07 10:55:14,655 EPOCH 3959
2024-02-07 10:55:30,848 Epoch 3959: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-07 10:55:30,848 EPOCH 3960
2024-02-07 10:55:47,358 Epoch 3960: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-07 10:55:47,359 EPOCH 3961
2024-02-07 10:56:03,612 Epoch 3961: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-07 10:56:03,614 EPOCH 3962
2024-02-07 10:56:20,211 Epoch 3962: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-07 10:56:20,211 EPOCH 3963
2024-02-07 10:56:36,641 Epoch 3963: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-07 10:56:36,641 EPOCH 3964
2024-02-07 10:56:53,085 Epoch 3964: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-07 10:56:53,086 EPOCH 3965
2024-02-07 10:57:09,489 Epoch 3965: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-07 10:57:09,489 EPOCH 3966
2024-02-07 10:57:25,677 Epoch 3966: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-07 10:57:25,678 EPOCH 3967
2024-02-07 10:57:36,767 [Epoch: 3967 Step: 00035700] Batch Recognition Loss:   0.000829 => Gls Tokens per Sec:      693 || Batch Translation Loss:   0.038895 => Txt Tokens per Sec:     1955 || Lr: 0.000100
2024-02-07 10:57:41,994 Epoch 3967: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 10:57:41,995 EPOCH 3968
2024-02-07 10:57:58,327 Epoch 3968: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 10:57:58,328 EPOCH 3969
2024-02-07 10:58:14,608 Epoch 3969: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 10:58:14,608 EPOCH 3970
2024-02-07 10:58:31,333 Epoch 3970: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-07 10:58:31,333 EPOCH 3971
2024-02-07 10:58:47,698 Epoch 3971: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-07 10:58:47,700 EPOCH 3972
2024-02-07 10:59:04,347 Epoch 3972: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 10:59:04,348 EPOCH 3973
2024-02-07 10:59:20,915 Epoch 3973: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 10:59:20,917 EPOCH 3974
2024-02-07 10:59:37,746 Epoch 3974: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 10:59:37,747 EPOCH 3975
2024-02-07 10:59:54,342 Epoch 3975: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 10:59:54,342 EPOCH 3976
2024-02-07 11:00:10,531 Epoch 3976: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 11:00:10,533 EPOCH 3977
2024-02-07 11:00:26,912 Epoch 3977: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 11:00:26,913 EPOCH 3978
2024-02-07 11:00:42,452 [Epoch: 3978 Step: 00035800] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:      519 || Batch Translation Loss:   0.012391 => Txt Tokens per Sec:     1430 || Lr: 0.000100
2024-02-07 11:00:43,550 Epoch 3978: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 11:00:43,551 EPOCH 3979
2024-02-07 11:00:59,836 Epoch 3979: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 11:00:59,837 EPOCH 3980
2024-02-07 11:01:16,316 Epoch 3980: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 11:01:16,317 EPOCH 3981
2024-02-07 11:01:32,697 Epoch 3981: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 11:01:32,698 EPOCH 3982
2024-02-07 11:01:48,937 Epoch 3982: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 11:01:48,938 EPOCH 3983
2024-02-07 11:02:05,469 Epoch 3983: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 11:02:05,469 EPOCH 3984
2024-02-07 11:02:21,996 Epoch 3984: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 11:02:21,997 EPOCH 3985
2024-02-07 11:02:38,426 Epoch 3985: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 11:02:38,426 EPOCH 3986
2024-02-07 11:02:54,802 Epoch 3986: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 11:02:54,803 EPOCH 3987
2024-02-07 11:03:11,315 Epoch 3987: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 11:03:11,316 EPOCH 3988
2024-02-07 11:03:28,405 Epoch 3988: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 11:03:28,406 EPOCH 3989
2024-02-07 11:03:44,230 [Epoch: 3989 Step: 00035900] Batch Recognition Loss:   0.000774 => Gls Tokens per Sec:      590 || Batch Translation Loss:   0.012350 => Txt Tokens per Sec:     1611 || Lr: 0.000100
2024-02-07 11:03:44,918 Epoch 3989: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 11:03:44,918 EPOCH 3990
2024-02-07 11:04:00,929 Epoch 3990: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 11:04:00,930 EPOCH 3991
2024-02-07 11:04:17,693 Epoch 3991: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 11:04:17,694 EPOCH 3992
2024-02-07 11:04:34,116 Epoch 3992: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 11:04:34,116 EPOCH 3993
2024-02-07 11:04:50,699 Epoch 3993: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 11:04:50,700 EPOCH 3994
2024-02-07 11:05:06,947 Epoch 3994: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 11:05:06,948 EPOCH 3995
2024-02-07 11:05:23,264 Epoch 3995: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 11:05:23,265 EPOCH 3996
2024-02-07 11:05:39,576 Epoch 3996: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 11:05:39,578 EPOCH 3997
2024-02-07 11:05:55,704 Epoch 3997: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 11:05:55,705 EPOCH 3998
2024-02-07 11:06:12,010 Epoch 3998: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 11:06:12,012 EPOCH 3999
2024-02-07 11:06:28,714 Epoch 3999: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 11:06:28,715 EPOCH 4000
2024-02-07 11:06:45,303 [Epoch: 4000 Step: 00036000] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:      640 || Batch Translation Loss:   0.014431 => Txt Tokens per Sec:     1771 || Lr: 0.000100
2024-02-07 11:07:53,095 Validation result at epoch 4000, step    36000: duration: 67.7904s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.30003	Translation Loss: 98467.32812	PPL: 18674.80273
	Eval Metric: BLEU
	WER 2.90	(DEL: 0.00,	INS: 0.00,	SUB: 2.90)
	BLEU-4 0.48	(BLEU-1: 10.60,	BLEU-2: 3.35,	BLEU-3: 1.23,	BLEU-4: 0.48)
	CHRF 17.10	ROUGE 8.68
2024-02-07 11:07:53,097 Logging Recognition and Translation Outputs
2024-02-07 11:07:53,097 ========================================================================================================================
2024-02-07 11:07:53,097 Logging Sequence: 153_36.00
2024-02-07 11:07:53,098 	Gloss Reference :	A B+C+D+E
2024-02-07 11:07:53,098 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 11:07:53,098 	Gloss Alignment :	         
2024-02-07 11:07:53,098 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 11:07:53,100 	Text Reference  :	********* ** **** ******* **** ***** ** india made   a   good score of 1686 in  20    overs
2024-02-07 11:07:53,100 	Text Hypothesis :	yesterday on 16th october 2022 kohli is only  scored 356 and  loss  of **** the world cup  
2024-02-07 11:07:53,100 	Text Alignment  :	I         I  I    I       I    I     I  S     S      S   S    S        D    S   S     S    
2024-02-07 11:07:53,101 ========================================================================================================================
2024-02-07 11:07:53,101 Logging Sequence: 163_30.00
2024-02-07 11:07:53,101 	Gloss Reference :	A B+C+D+E
2024-02-07 11:07:53,101 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 11:07:53,101 	Gloss Alignment :	         
2024-02-07 11:07:53,101 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 11:07:53,102 	Text Reference  :	***** *** they  never permitted anyone to ****** reveal her face   
2024-02-07 11:07:53,102 	Text Hypothesis :	after the video of    fans      want   to change their  own matches
2024-02-07 11:07:53,102 	Text Alignment  :	I     I   S     S     S         S         I      S      S   S      
2024-02-07 11:07:53,102 ========================================================================================================================
2024-02-07 11:07:53,103 Logging Sequence: 167_60.00
2024-02-07 11:07:53,103 	Gloss Reference :	A B+C+D+E
2024-02-07 11:07:53,103 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 11:07:53,103 	Gloss Alignment :	         
2024-02-07 11:07:53,103 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 11:07:53,104 	Text Reference  :	***** **** *** camel flu  spreads rapidly when   one comes in          close contact with the infected
2024-02-07 11:07:53,105 	Text Hypothesis :	being made out of    them for     all     thanks to  the   coronavirus for   no      run  was slow    
2024-02-07 11:07:53,105 	Text Alignment  :	I     I    I   S     S    S       S       S      S   S     S           S     S       S    S   S       
2024-02-07 11:07:53,105 ========================================================================================================================
2024-02-07 11:07:53,105 Logging Sequence: 84_35.00
2024-02-07 11:07:53,105 	Gloss Reference :	A B+C+D+E
2024-02-07 11:07:53,105 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 11:07:53,106 	Gloss Alignment :	         
2024-02-07 11:07:53,106 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 11:07:53,106 	Text Reference  :	here is  the reason why     they covered their mouth
2024-02-07 11:07:53,106 	Text Hypothesis :	it   was a   we     decided to   keep    the   match
2024-02-07 11:07:53,107 	Text Alignment  :	S    S   S   S      S       S    S       S     S    
2024-02-07 11:07:53,107 ========================================================================================================================
2024-02-07 11:07:53,107 Logging Sequence: 96_2.00
2024-02-07 11:07:53,107 	Gloss Reference :	A B+C+D+E
2024-02-07 11:07:53,107 	Gloss Hypothesis:	A B+C+D  
2024-02-07 11:07:53,107 	Gloss Alignment :	  S      
2024-02-07 11:07:53,107 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 11:07:53,109 	Text Reference  :	the world is preparing for the      t20     world cup scheduled to start from 16th  october this year
2024-02-07 11:07:53,109 	Text Hypothesis :	the ***** ** ********* icc under-19 cricket world cup ********* ** ***** was  first played  in   1988
2024-02-07 11:07:53,109 	Text Alignment  :	    D     D  D         S   S        S                 D         D  D     S    S     S       S    S   
2024-02-07 11:07:53,109 ========================================================================================================================
2024-02-07 11:07:53,113 Epoch 4000: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 11:07:53,113 EPOCH 4001
2024-02-07 11:08:10,473 Epoch 4001: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 11:08:10,474 EPOCH 4002
2024-02-07 11:08:26,721 Epoch 4002: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 11:08:26,722 EPOCH 4003
2024-02-07 11:08:43,176 Epoch 4003: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-07 11:08:43,176 EPOCH 4004
2024-02-07 11:08:59,581 Epoch 4004: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-07 11:08:59,581 EPOCH 4005
2024-02-07 11:09:16,061 Epoch 4005: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-07 11:09:16,062 EPOCH 4006
2024-02-07 11:09:32,345 Epoch 4006: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-07 11:09:32,345 EPOCH 4007
2024-02-07 11:09:49,044 Epoch 4007: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-07 11:09:49,044 EPOCH 4008
2024-02-07 11:10:05,276 Epoch 4008: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-07 11:10:05,277 EPOCH 4009
2024-02-07 11:10:21,856 Epoch 4009: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 11:10:21,857 EPOCH 4010
2024-02-07 11:10:38,302 Epoch 4010: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 11:10:38,303 EPOCH 4011
2024-02-07 11:10:54,751 Epoch 4011: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.31 
2024-02-07 11:10:54,751 EPOCH 4012
2024-02-07 11:10:59,281 [Epoch: 4012 Step: 00036100] Batch Recognition Loss:   0.000607 => Gls Tokens per Sec:       84 || Batch Translation Loss:   0.149898 => Txt Tokens per Sec:      300 || Lr: 0.000100
2024-02-07 11:11:11,407 Epoch 4012: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.82 
2024-02-07 11:11:11,407 EPOCH 4013
2024-02-07 11:11:27,944 Epoch 4013: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.05 
2024-02-07 11:11:27,945 EPOCH 4014
2024-02-07 11:11:44,002 Epoch 4014: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.88 
2024-02-07 11:11:44,003 EPOCH 4015
2024-02-07 11:12:00,558 Epoch 4015: Total Training Recognition Loss 0.14  Total Training Translation Loss 0.57 
2024-02-07 11:12:00,559 EPOCH 4016
2024-02-07 11:12:17,100 Epoch 4016: Total Training Recognition Loss 0.24  Total Training Translation Loss 0.44 
2024-02-07 11:12:17,100 EPOCH 4017
2024-02-07 11:12:33,470 Epoch 4017: Total Training Recognition Loss 0.35  Total Training Translation Loss 0.43 
2024-02-07 11:12:33,470 EPOCH 4018
2024-02-07 11:12:49,812 Epoch 4018: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.45 
2024-02-07 11:12:49,813 EPOCH 4019
2024-02-07 11:13:06,220 Epoch 4019: Total Training Recognition Loss 0.14  Total Training Translation Loss 0.37 
2024-02-07 11:13:06,221 EPOCH 4020
2024-02-07 11:13:22,433 Epoch 4020: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.36 
2024-02-07 11:13:22,434 EPOCH 4021
2024-02-07 11:13:39,125 Epoch 4021: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.25 
2024-02-07 11:13:39,126 EPOCH 4022
2024-02-07 11:13:55,499 Epoch 4022: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.20 
2024-02-07 11:13:55,500 EPOCH 4023
2024-02-07 11:13:56,134 [Epoch: 4023 Step: 00036200] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:     4048 || Batch Translation Loss:   0.016271 => Txt Tokens per Sec:     9627 || Lr: 0.000100
2024-02-07 11:14:11,776 Epoch 4023: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.22 
2024-02-07 11:14:11,776 EPOCH 4024
2024-02-07 11:14:28,067 Epoch 4024: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.34 
2024-02-07 11:14:28,068 EPOCH 4025
2024-02-07 11:14:44,255 Epoch 4025: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-07 11:14:44,256 EPOCH 4026
2024-02-07 11:15:00,529 Epoch 4026: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-07 11:15:00,530 EPOCH 4027
2024-02-07 11:15:17,204 Epoch 4027: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-07 11:15:17,204 EPOCH 4028
2024-02-07 11:15:32,935 Epoch 4028: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-07 11:15:32,935 EPOCH 4029
2024-02-07 11:15:49,547 Epoch 4029: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 11:15:49,548 EPOCH 4030
2024-02-07 11:16:05,891 Epoch 4030: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 11:16:05,891 EPOCH 4031
2024-02-07 11:16:22,482 Epoch 4031: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 11:16:22,483 EPOCH 4032
2024-02-07 11:16:39,028 Epoch 4032: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 11:16:39,029 EPOCH 4033
2024-02-07 11:16:55,456 Epoch 4033: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 11:16:55,456 EPOCH 4034
2024-02-07 11:17:02,433 [Epoch: 4034 Step: 00036300] Batch Recognition Loss:   0.001352 => Gls Tokens per Sec:      551 || Batch Translation Loss:   0.025501 => Txt Tokens per Sec:     1658 || Lr: 0.000100
2024-02-07 11:17:11,858 Epoch 4034: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 11:17:11,859 EPOCH 4035
2024-02-07 11:17:28,346 Epoch 4035: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 11:17:28,346 EPOCH 4036
2024-02-07 11:17:44,590 Epoch 4036: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 11:17:44,590 EPOCH 4037
2024-02-07 11:18:01,000 Epoch 4037: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-07 11:18:01,001 EPOCH 4038
2024-02-07 11:18:17,267 Epoch 4038: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 11:18:17,268 EPOCH 4039
2024-02-07 11:18:33,677 Epoch 4039: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 11:18:33,677 EPOCH 4040
2024-02-07 11:18:50,216 Epoch 4040: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 11:18:50,216 EPOCH 4041
2024-02-07 11:19:06,327 Epoch 4041: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 11:19:06,327 EPOCH 4042
2024-02-07 11:19:23,026 Epoch 4042: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 11:19:23,026 EPOCH 4043
2024-02-07 11:19:39,967 Epoch 4043: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 11:19:39,968 EPOCH 4044
2024-02-07 11:19:56,524 Epoch 4044: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 11:19:56,524 EPOCH 4045
2024-02-07 11:20:01,294 [Epoch: 4045 Step: 00036400] Batch Recognition Loss:   0.000530 => Gls Tokens per Sec:     1074 || Batch Translation Loss:   0.016908 => Txt Tokens per Sec:     3135 || Lr: 0.000100
2024-02-07 11:20:12,928 Epoch 4045: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 11:20:12,929 EPOCH 4046
2024-02-07 11:20:29,688 Epoch 4046: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 11:20:29,689 EPOCH 4047
2024-02-07 11:20:46,355 Epoch 4047: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 11:20:46,356 EPOCH 4048
2024-02-07 11:21:02,478 Epoch 4048: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 11:21:02,479 EPOCH 4049
2024-02-07 11:21:18,597 Epoch 4049: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 11:21:18,597 EPOCH 4050
2024-02-07 11:21:35,168 Epoch 4050: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 11:21:35,168 EPOCH 4051
2024-02-07 11:21:51,358 Epoch 4051: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 11:21:51,359 EPOCH 4052
2024-02-07 11:22:07,689 Epoch 4052: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 11:22:07,689 EPOCH 4053
2024-02-07 11:22:24,222 Epoch 4053: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 11:22:24,223 EPOCH 4054
2024-02-07 11:22:40,583 Epoch 4054: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 11:22:40,584 EPOCH 4055
2024-02-07 11:22:57,207 Epoch 4055: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 11:22:57,207 EPOCH 4056
2024-02-07 11:23:05,897 [Epoch: 4056 Step: 00036500] Batch Recognition Loss:   0.000462 => Gls Tokens per Sec:      633 || Batch Translation Loss:   0.015618 => Txt Tokens per Sec:     1778 || Lr: 0.000100
2024-02-07 11:23:13,355 Epoch 4056: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 11:23:13,355 EPOCH 4057
2024-02-07 11:23:29,817 Epoch 4057: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 11:23:29,818 EPOCH 4058
2024-02-07 11:23:46,457 Epoch 4058: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 11:23:46,457 EPOCH 4059
2024-02-07 11:24:02,758 Epoch 4059: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-07 11:24:02,759 EPOCH 4060
2024-02-07 11:24:19,729 Epoch 4060: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 11:24:19,730 EPOCH 4061
2024-02-07 11:24:36,152 Epoch 4061: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 11:24:36,152 EPOCH 4062
2024-02-07 11:24:52,420 Epoch 4062: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 11:24:52,421 EPOCH 4063
2024-02-07 11:25:08,812 Epoch 4063: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 11:25:08,812 EPOCH 4064
2024-02-07 11:25:25,091 Epoch 4064: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 11:25:25,092 EPOCH 4065
2024-02-07 11:25:41,461 Epoch 4065: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 11:25:41,462 EPOCH 4066
2024-02-07 11:25:57,755 Epoch 4066: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 11:25:57,756 EPOCH 4067
2024-02-07 11:26:07,119 [Epoch: 4067 Step: 00036600] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:      724 || Batch Translation Loss:   0.012797 => Txt Tokens per Sec:     2011 || Lr: 0.000100
2024-02-07 11:26:14,737 Epoch 4067: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 11:26:14,738 EPOCH 4068
2024-02-07 11:26:31,221 Epoch 4068: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 11:26:31,221 EPOCH 4069
2024-02-07 11:26:47,436 Epoch 4069: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 11:26:47,437 EPOCH 4070
2024-02-07 11:27:03,678 Epoch 4070: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 11:27:03,678 EPOCH 4071
2024-02-07 11:27:20,047 Epoch 4071: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 11:27:20,047 EPOCH 4072
2024-02-07 11:27:36,729 Epoch 4072: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 11:27:36,730 EPOCH 4073
2024-02-07 11:27:53,143 Epoch 4073: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 11:27:53,143 EPOCH 4074
2024-02-07 11:28:09,876 Epoch 4074: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-07 11:28:09,877 EPOCH 4075
2024-02-07 11:28:26,101 Epoch 4075: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-07 11:28:26,102 EPOCH 4076
2024-02-07 11:28:42,603 Epoch 4076: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-07 11:28:42,603 EPOCH 4077
2024-02-07 11:28:59,182 Epoch 4077: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-07 11:28:59,183 EPOCH 4078
2024-02-07 11:29:14,692 [Epoch: 4078 Step: 00036700] Batch Recognition Loss:   0.000461 => Gls Tokens per Sec:      520 || Batch Translation Loss:   0.045353 => Txt Tokens per Sec:     1420 || Lr: 0.000100
2024-02-07 11:29:15,832 Epoch 4078: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-07 11:29:15,832 EPOCH 4079
2024-02-07 11:29:32,085 Epoch 4079: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-07 11:29:32,085 EPOCH 4080
2024-02-07 11:29:48,676 Epoch 4080: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.39 
2024-02-07 11:29:48,677 EPOCH 4081
2024-02-07 11:30:04,992 Epoch 4081: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.23 
2024-02-07 11:30:04,992 EPOCH 4082
2024-02-07 11:30:21,292 Epoch 4082: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.65 
2024-02-07 11:30:21,293 EPOCH 4083
2024-02-07 11:30:37,452 Epoch 4083: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.95 
2024-02-07 11:30:37,452 EPOCH 4084
2024-02-07 11:30:53,913 Epoch 4084: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-07 11:30:53,913 EPOCH 4085
2024-02-07 11:31:10,154 Epoch 4085: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-07 11:31:10,154 EPOCH 4086
2024-02-07 11:31:26,641 Epoch 4086: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-07 11:31:26,642 EPOCH 4087
2024-02-07 11:31:42,953 Epoch 4087: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-07 11:31:42,954 EPOCH 4088
2024-02-07 11:31:59,345 Epoch 4088: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-07 11:31:59,346 EPOCH 4089
2024-02-07 11:32:11,774 [Epoch: 4089 Step: 00036800] Batch Recognition Loss:   0.000990 => Gls Tokens per Sec:      824 || Batch Translation Loss:   0.057685 => Txt Tokens per Sec:     2255 || Lr: 0.000100
2024-02-07 11:32:16,220 Epoch 4089: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-07 11:32:16,221 EPOCH 4090
2024-02-07 11:32:32,627 Epoch 4090: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-07 11:32:32,627 EPOCH 4091
2024-02-07 11:32:48,943 Epoch 4091: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-07 11:32:48,944 EPOCH 4092
2024-02-07 11:33:05,132 Epoch 4092: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-07 11:33:05,132 EPOCH 4093
2024-02-07 11:33:21,579 Epoch 4093: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-07 11:33:21,579 EPOCH 4094
2024-02-07 11:33:37,853 Epoch 4094: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 11:33:37,854 EPOCH 4095
2024-02-07 11:33:54,157 Epoch 4095: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 11:33:54,157 EPOCH 4096
2024-02-07 11:34:10,355 Epoch 4096: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 11:34:10,356 EPOCH 4097
2024-02-07 11:34:26,950 Epoch 4097: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 11:34:26,950 EPOCH 4098
2024-02-07 11:34:43,571 Epoch 4098: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 11:34:43,571 EPOCH 4099
2024-02-07 11:35:00,054 Epoch 4099: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 11:35:00,055 EPOCH 4100
2024-02-07 11:35:16,378 [Epoch: 4100 Step: 00036900] Batch Recognition Loss:   0.000836 => Gls Tokens per Sec:      651 || Batch Translation Loss:   0.018050 => Txt Tokens per Sec:     1800 || Lr: 0.000100
2024-02-07 11:35:16,378 Epoch 4100: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 11:35:16,378 EPOCH 4101
2024-02-07 11:35:32,900 Epoch 4101: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 11:35:32,901 EPOCH 4102
2024-02-07 11:35:49,024 Epoch 4102: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 11:35:49,024 EPOCH 4103
2024-02-07 11:36:05,289 Epoch 4103: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 11:36:05,290 EPOCH 4104
2024-02-07 11:36:21,568 Epoch 4104: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 11:36:21,569 EPOCH 4105
2024-02-07 11:36:38,271 Epoch 4105: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 11:36:38,272 EPOCH 4106
2024-02-07 11:36:54,785 Epoch 4106: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 11:36:54,786 EPOCH 4107
2024-02-07 11:37:11,531 Epoch 4107: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 11:37:11,531 EPOCH 4108
2024-02-07 11:37:27,989 Epoch 4108: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 11:37:27,990 EPOCH 4109
2024-02-07 11:37:44,334 Epoch 4109: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 11:37:44,334 EPOCH 4110
2024-02-07 11:38:00,718 Epoch 4110: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 11:38:00,719 EPOCH 4111
2024-02-07 11:38:16,764 Epoch 4111: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 11:38:16,765 EPOCH 4112
2024-02-07 11:38:17,089 [Epoch: 4112 Step: 00037000] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     3963 || Batch Translation Loss:   0.011609 => Txt Tokens per Sec:    10040 || Lr: 0.000100
2024-02-07 11:38:33,218 Epoch 4112: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 11:38:33,219 EPOCH 4113
2024-02-07 11:38:49,405 Epoch 4113: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 11:38:49,405 EPOCH 4114
2024-02-07 11:39:05,908 Epoch 4114: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 11:39:05,910 EPOCH 4115
2024-02-07 11:39:22,416 Epoch 4115: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 11:39:22,417 EPOCH 4116
2024-02-07 11:39:38,939 Epoch 4116: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-07 11:39:38,939 EPOCH 4117
2024-02-07 11:39:55,597 Epoch 4117: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 11:39:55,597 EPOCH 4118
2024-02-07 11:40:12,105 Epoch 4118: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 11:40:12,106 EPOCH 4119
2024-02-07 11:40:28,478 Epoch 4119: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 11:40:28,479 EPOCH 4120
2024-02-07 11:40:44,985 Epoch 4120: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 11:40:44,985 EPOCH 4121
2024-02-07 11:41:01,396 Epoch 4121: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 11:41:01,396 EPOCH 4122
2024-02-07 11:41:17,999 Epoch 4122: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 11:41:18,000 EPOCH 4123
2024-02-07 11:41:19,134 [Epoch: 4123 Step: 00037100] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     2263 || Batch Translation Loss:   0.014801 => Txt Tokens per Sec:     6544 || Lr: 0.000100
2024-02-07 11:41:34,621 Epoch 4123: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 11:41:34,622 EPOCH 4124
2024-02-07 11:41:51,025 Epoch 4124: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 11:41:51,026 EPOCH 4125
2024-02-07 11:42:07,262 Epoch 4125: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 11:42:07,263 EPOCH 4126
2024-02-07 11:42:23,702 Epoch 4126: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 11:42:23,703 EPOCH 4127
2024-02-07 11:42:40,114 Epoch 4127: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 11:42:40,115 EPOCH 4128
2024-02-07 11:42:56,868 Epoch 4128: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 11:42:56,868 EPOCH 4129
2024-02-07 11:43:13,236 Epoch 4129: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 11:43:13,236 EPOCH 4130
2024-02-07 11:43:30,318 Epoch 4130: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 11:43:30,318 EPOCH 4131
2024-02-07 11:43:46,776 Epoch 4131: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 11:43:46,777 EPOCH 4132
2024-02-07 11:44:03,397 Epoch 4132: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 11:44:03,398 EPOCH 4133
2024-02-07 11:44:19,704 Epoch 4133: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 11:44:19,704 EPOCH 4134
2024-02-07 11:44:29,434 [Epoch: 4134 Step: 00037200] Batch Recognition Loss:   0.000308 => Gls Tokens per Sec:      395 || Batch Translation Loss:   0.007866 => Txt Tokens per Sec:     1111 || Lr: 0.000100
2024-02-07 11:44:36,251 Epoch 4134: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 11:44:36,252 EPOCH 4135
2024-02-07 11:44:52,634 Epoch 4135: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 11:44:52,634 EPOCH 4136
2024-02-07 11:45:09,219 Epoch 4136: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 11:45:09,219 EPOCH 4137
2024-02-07 11:45:25,948 Epoch 4137: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 11:45:25,949 EPOCH 4138
2024-02-07 11:45:42,361 Epoch 4138: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 11:45:42,362 EPOCH 4139
2024-02-07 11:45:58,649 Epoch 4139: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 11:45:58,650 EPOCH 4140
2024-02-07 11:46:15,053 Epoch 4140: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 11:46:15,053 EPOCH 4141
2024-02-07 11:46:31,270 Epoch 4141: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 11:46:31,271 EPOCH 4142
2024-02-07 11:46:47,850 Epoch 4142: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 11:46:47,850 EPOCH 4143
2024-02-07 11:47:04,091 Epoch 4143: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-07 11:47:04,092 EPOCH 4144
2024-02-07 11:47:20,560 Epoch 4144: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 11:47:20,560 EPOCH 4145
2024-02-07 11:47:24,907 [Epoch: 4145 Step: 00037300] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     1178 || Batch Translation Loss:   0.008500 => Txt Tokens per Sec:     3207 || Lr: 0.000100
2024-02-07 11:47:36,914 Epoch 4145: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 11:47:36,915 EPOCH 4146
2024-02-07 11:47:53,790 Epoch 4146: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 11:47:53,791 EPOCH 4147
2024-02-07 11:48:10,168 Epoch 4147: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 11:48:10,169 EPOCH 4148
2024-02-07 11:48:26,894 Epoch 4148: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 11:48:26,894 EPOCH 4149
2024-02-07 11:48:43,218 Epoch 4149: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 11:48:43,218 EPOCH 4150
2024-02-07 11:48:59,434 Epoch 4150: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 11:48:59,435 EPOCH 4151
2024-02-07 11:49:15,964 Epoch 4151: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 11:49:15,964 EPOCH 4152
2024-02-07 11:49:32,215 Epoch 4152: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-07 11:49:32,216 EPOCH 4153
2024-02-07 11:49:48,548 Epoch 4153: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 11:49:48,549 EPOCH 4154
2024-02-07 11:50:04,910 Epoch 4154: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 11:50:04,910 EPOCH 4155
2024-02-07 11:50:21,056 Epoch 4155: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 11:50:21,057 EPOCH 4156
2024-02-07 11:50:30,240 [Epoch: 4156 Step: 00037400] Batch Recognition Loss:   0.000152 => Gls Tokens per Sec:      599 || Batch Translation Loss:   0.017100 => Txt Tokens per Sec:     1730 || Lr: 0.000100
2024-02-07 11:50:37,809 Epoch 4156: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 11:50:37,810 EPOCH 4157
2024-02-07 11:50:54,039 Epoch 4157: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 11:50:54,039 EPOCH 4158
2024-02-07 11:51:10,680 Epoch 4158: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 11:51:10,681 EPOCH 4159
2024-02-07 11:51:26,949 Epoch 4159: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 11:51:26,949 EPOCH 4160
2024-02-07 11:51:43,185 Epoch 4160: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 11:51:43,186 EPOCH 4161
2024-02-07 11:51:59,545 Epoch 4161: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 11:51:59,546 EPOCH 4162
2024-02-07 11:52:16,201 Epoch 4162: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 11:52:16,201 EPOCH 4163
2024-02-07 11:52:32,714 Epoch 4163: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 11:52:32,715 EPOCH 4164
2024-02-07 11:52:48,863 Epoch 4164: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-07 11:52:48,865 EPOCH 4165
2024-02-07 11:53:05,345 Epoch 4165: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.86 
2024-02-07 11:53:05,345 EPOCH 4166
2024-02-07 11:53:22,042 Epoch 4166: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.50 
2024-02-07 11:53:22,043 EPOCH 4167
2024-02-07 11:53:34,185 [Epoch: 4167 Step: 00037500] Batch Recognition Loss:   0.001752 => Gls Tokens per Sec:      559 || Batch Translation Loss:   0.261504 => Txt Tokens per Sec:     1497 || Lr: 0.000100
2024-02-07 11:53:38,427 Epoch 4167: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.39 
2024-02-07 11:53:38,428 EPOCH 4168
2024-02-07 11:53:54,521 Epoch 4168: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.54 
2024-02-07 11:53:54,521 EPOCH 4169
2024-02-07 11:54:11,154 Epoch 4169: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.48 
2024-02-07 11:54:11,154 EPOCH 4170
2024-02-07 11:54:27,628 Epoch 4170: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.61 
2024-02-07 11:54:27,629 EPOCH 4171
2024-02-07 11:54:43,936 Epoch 4171: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.52 
2024-02-07 11:54:43,936 EPOCH 4172
2024-02-07 11:54:59,910 Epoch 4172: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.47 
2024-02-07 11:54:59,911 EPOCH 4173
2024-02-07 11:55:16,375 Epoch 4173: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.37 
2024-02-07 11:55:16,376 EPOCH 4174
2024-02-07 11:55:32,680 Epoch 4174: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-07 11:55:32,681 EPOCH 4175
2024-02-07 11:55:49,096 Epoch 4175: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-07 11:55:49,096 EPOCH 4176
2024-02-07 11:56:05,336 Epoch 4176: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-07 11:56:05,336 EPOCH 4177
2024-02-07 11:56:21,656 Epoch 4177: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-07 11:56:21,657 EPOCH 4178
2024-02-07 11:56:27,696 [Epoch: 4178 Step: 00037600] Batch Recognition Loss:   0.000392 => Gls Tokens per Sec:     1484 || Batch Translation Loss:   0.051885 => Txt Tokens per Sec:     3914 || Lr: 0.000100
2024-02-07 11:56:38,316 Epoch 4178: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-07 11:56:38,317 EPOCH 4179
2024-02-07 11:56:54,687 Epoch 4179: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-07 11:56:54,688 EPOCH 4180
2024-02-07 11:57:11,426 Epoch 4180: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-07 11:57:11,426 EPOCH 4181
2024-02-07 11:57:27,597 Epoch 4181: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-07 11:57:27,598 EPOCH 4182
2024-02-07 11:57:43,584 Epoch 4182: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 11:57:43,585 EPOCH 4183
2024-02-07 11:57:59,970 Epoch 4183: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 11:57:59,970 EPOCH 4184
2024-02-07 11:58:16,393 Epoch 4184: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 11:58:16,393 EPOCH 4185
2024-02-07 11:58:32,477 Epoch 4185: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 11:58:32,477 EPOCH 4186
2024-02-07 11:58:48,629 Epoch 4186: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 11:58:48,630 EPOCH 4187
2024-02-07 11:59:05,453 Epoch 4187: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 11:59:05,454 EPOCH 4188
2024-02-07 11:59:21,675 Epoch 4188: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 11:59:21,676 EPOCH 4189
2024-02-07 11:59:37,715 [Epoch: 4189 Step: 00037700] Batch Recognition Loss:   0.000465 => Gls Tokens per Sec:      582 || Batch Translation Loss:   0.011279 => Txt Tokens per Sec:     1612 || Lr: 0.000100
2024-02-07 11:59:38,168 Epoch 4189: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 11:59:38,168 EPOCH 4190
2024-02-07 11:59:54,402 Epoch 4190: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 11:59:54,402 EPOCH 4191
2024-02-07 12:00:10,552 Epoch 4191: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 12:00:10,553 EPOCH 4192
2024-02-07 12:00:27,390 Epoch 4192: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 12:00:27,392 EPOCH 4193
2024-02-07 12:00:43,804 Epoch 4193: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:00:43,804 EPOCH 4194
2024-02-07 12:01:00,259 Epoch 4194: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 12:01:00,259 EPOCH 4195
2024-02-07 12:01:16,496 Epoch 4195: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 12:01:16,496 EPOCH 4196
2024-02-07 12:01:32,872 Epoch 4196: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 12:01:32,872 EPOCH 4197
2024-02-07 12:01:49,198 Epoch 4197: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:01:49,198 EPOCH 4198
2024-02-07 12:02:05,585 Epoch 4198: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:02:05,585 EPOCH 4199
2024-02-07 12:02:21,992 Epoch 4199: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:02:21,993 EPOCH 4200
2024-02-07 12:02:38,904 [Epoch: 4200 Step: 00037800] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:      628 || Batch Translation Loss:   0.013037 => Txt Tokens per Sec:     1738 || Lr: 0.000100
2024-02-07 12:02:38,905 Epoch 4200: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 12:02:38,905 EPOCH 4201
2024-02-07 12:02:55,243 Epoch 4201: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:02:55,243 EPOCH 4202
2024-02-07 12:03:11,693 Epoch 4202: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:03:11,693 EPOCH 4203
2024-02-07 12:03:28,050 Epoch 4203: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:03:28,051 EPOCH 4204
2024-02-07 12:03:44,440 Epoch 4204: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:03:44,441 EPOCH 4205
2024-02-07 12:04:00,602 Epoch 4205: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:04:00,603 EPOCH 4206
2024-02-07 12:04:17,208 Epoch 4206: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 12:04:17,209 EPOCH 4207
2024-02-07 12:04:33,431 Epoch 4207: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 12:04:33,432 EPOCH 4208
2024-02-07 12:04:50,095 Epoch 4208: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 12:04:50,096 EPOCH 4209
2024-02-07 12:05:06,589 Epoch 4209: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 12:05:06,590 EPOCH 4210
2024-02-07 12:05:22,915 Epoch 4210: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 12:05:22,915 EPOCH 4211
2024-02-07 12:05:38,985 Epoch 4211: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 12:05:38,986 EPOCH 4212
2024-02-07 12:05:39,396 [Epoch: 4212 Step: 00037900] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:     3137 || Batch Translation Loss:   0.012087 => Txt Tokens per Sec:     6939 || Lr: 0.000100
2024-02-07 12:05:55,476 Epoch 4212: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:05:55,477 EPOCH 4213
2024-02-07 12:06:11,934 Epoch 4213: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:06:11,935 EPOCH 4214
2024-02-07 12:06:28,250 Epoch 4214: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:06:28,251 EPOCH 4215
2024-02-07 12:06:44,512 Epoch 4215: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:06:44,513 EPOCH 4216
2024-02-07 12:07:01,039 Epoch 4216: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:07:01,039 EPOCH 4217
2024-02-07 12:07:17,403 Epoch 4217: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-07 12:07:17,403 EPOCH 4218
2024-02-07 12:07:33,825 Epoch 4218: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 12:07:33,826 EPOCH 4219
2024-02-07 12:07:50,028 Epoch 4219: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-07 12:07:50,029 EPOCH 4220
2024-02-07 12:08:06,498 Epoch 4220: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 12:08:06,499 EPOCH 4221
2024-02-07 12:08:22,654 Epoch 4221: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:08:22,655 EPOCH 4222
2024-02-07 12:08:39,262 Epoch 4222: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:08:39,262 EPOCH 4223
2024-02-07 12:08:42,715 [Epoch: 4223 Step: 00038000] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:      742 || Batch Translation Loss:   0.008606 => Txt Tokens per Sec:     2004 || Lr: 0.000100
2024-02-07 12:09:50,628 Validation result at epoch 4223, step    38000: duration: 67.9117s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.30881	Translation Loss: 99128.32031	PPL: 19949.32227
	Eval Metric: BLEU
	WER 2.40	(DEL: 0.00,	INS: 0.00,	SUB: 2.40)
	BLEU-4 0.35	(BLEU-1: 10.39,	BLEU-2: 2.87,	BLEU-3: 1.02,	BLEU-4: 0.35)
	CHRF 16.75	ROUGE 8.62
2024-02-07 12:09:50,630 Logging Recognition and Translation Outputs
2024-02-07 12:09:50,630 ========================================================================================================================
2024-02-07 12:09:50,630 Logging Sequence: 59_152.00
2024-02-07 12:09:50,630 	Gloss Reference :	A B+C+D+E
2024-02-07 12:09:50,631 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 12:09:50,631 	Gloss Alignment :	         
2024-02-07 12:09:50,632 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 12:09:50,634 	Text Reference  :	**** the organisers encouraged athletes to  use    the     condoms in  their  home countries
2024-02-07 12:09:50,634 	Text Hypothesis :	well and all        formats    of       odi series against delhi's ace bowler axar etc      
2024-02-07 12:09:50,635 	Text Alignment  :	I    S   S          S          S        S   S      S       S       S   S      S    S        
2024-02-07 12:09:50,635 ========================================================================================================================
2024-02-07 12:09:50,635 Logging Sequence: 155_78.00
2024-02-07 12:09:50,635 	Gloss Reference :	A B+C+D+E
2024-02-07 12:09:50,635 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 12:09:50,635 	Gloss Alignment :	         
2024-02-07 12:09:50,635 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 12:09:50,638 	Text Reference  :	it was difficult for icc to disqualify the   afghan team at    the last minute so   they included them as    per the schedule     
2024-02-07 12:09:50,638 	Text Hypothesis :	** *** ********* *** *** ** until      there is     a    known as  they will   have been excited  and  watch and are championships
2024-02-07 12:09:50,638 	Text Alignment  :	D  D   D         D   D   D  S          S     S      S    S     S   S    S      S    S    S        S    S     S   S   S            
2024-02-07 12:09:50,638 ========================================================================================================================
2024-02-07 12:09:50,638 Logging Sequence: 102_147.00
2024-02-07 12:09:50,639 	Gloss Reference :	A B+C+D+E
2024-02-07 12:09:50,639 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 12:09:50,639 	Gloss Alignment :	         
2024-02-07 12:09:50,639 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 12:09:50,641 	Text Reference  :	despite the muscle cramps this young boy    lifted such a    huge weight and    made   the country proud by securing a gold    medal   
2024-02-07 12:09:50,641 	Text Hypothesis :	******* the ****** ****** **** match starts at     the  same time medal  sheuli lifted the ******* ***** ** ******** * penalty shootout
2024-02-07 12:09:50,641 	Text Alignment  :	D           D      D      D    S     S      S      S    S    S    S      S      S          D       D     D  D        D S       S       
2024-02-07 12:09:50,641 ========================================================================================================================
2024-02-07 12:09:50,641 Logging Sequence: 105_2.00
2024-02-07 12:09:50,641 	Gloss Reference :	A B+C+D+E  
2024-02-07 12:09:50,642 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-07 12:09:50,642 	Gloss Alignment :	  S        
2024-02-07 12:09:50,642 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 12:09:50,643 	Text Reference  :	***** the ******* **** *** airthings masters tournament is     an       online  chess tournament
2024-02-07 12:09:50,643 	Text Hypothesis :	after the intense game was held      at      the        iconic wankhede stadium in    mumbai    
2024-02-07 12:09:50,643 	Text Alignment  :	I         I       I    I   S         S       S          S      S        S       S     S         
2024-02-07 12:09:50,643 ========================================================================================================================
2024-02-07 12:09:50,643 Logging Sequence: 96_31.00
2024-02-07 12:09:50,644 	Gloss Reference :	A B+C+D+E
2024-02-07 12:09:50,644 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 12:09:50,644 	Gloss Alignment :	         
2024-02-07 12:09:50,644 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 12:09:50,645 	Text Reference  :	******* and     then   2 teams will go on to     play the **** final 
2024-02-07 12:09:50,645 	Text Hypothesis :	however india's scored 2 ***** **** ** ** groups -    the next wicket
2024-02-07 12:09:50,645 	Text Alignment  :	I       S       S        D     D    D  D  S      S        I    S     
2024-02-07 12:09:50,645 ========================================================================================================================
2024-02-07 12:10:04,010 Epoch 4223: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 12:10:04,010 EPOCH 4224
2024-02-07 12:10:20,500 Epoch 4224: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 12:10:20,501 EPOCH 4225
2024-02-07 12:10:36,728 Epoch 4225: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:10:36,729 EPOCH 4226
2024-02-07 12:10:53,288 Epoch 4226: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 12:10:53,289 EPOCH 4227
2024-02-07 12:11:09,824 Epoch 4227: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 12:11:09,825 EPOCH 4228
2024-02-07 12:11:26,305 Epoch 4228: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:11:26,306 EPOCH 4229
2024-02-07 12:11:42,965 Epoch 4229: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:11:42,965 EPOCH 4230
2024-02-07 12:11:59,147 Epoch 4230: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 12:11:59,148 EPOCH 4231
2024-02-07 12:12:15,651 Epoch 4231: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:12:15,652 EPOCH 4232
2024-02-07 12:12:32,041 Epoch 4232: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 12:12:32,042 EPOCH 4233
2024-02-07 12:12:48,217 Epoch 4233: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:12:48,218 EPOCH 4234
2024-02-07 12:12:52,186 [Epoch: 4234 Step: 00038100] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:      968 || Batch Translation Loss:   0.014837 => Txt Tokens per Sec:     2791 || Lr: 0.000100
2024-02-07 12:13:04,425 Epoch 4234: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:13:04,426 EPOCH 4235
2024-02-07 12:13:21,225 Epoch 4235: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:13:21,225 EPOCH 4236
2024-02-07 12:13:37,518 Epoch 4236: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:13:37,519 EPOCH 4237
2024-02-07 12:13:54,290 Epoch 4237: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:13:54,291 EPOCH 4238
2024-02-07 12:14:10,524 Epoch 4238: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:14:10,525 EPOCH 4239
2024-02-07 12:14:27,027 Epoch 4239: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 12:14:27,028 EPOCH 4240
2024-02-07 12:14:43,464 Epoch 4240: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 12:14:43,465 EPOCH 4241
2024-02-07 12:14:59,867 Epoch 4241: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:14:59,868 EPOCH 4242
2024-02-07 12:15:16,393 Epoch 4242: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:15:16,394 EPOCH 4243
2024-02-07 12:15:32,848 Epoch 4243: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:15:32,848 EPOCH 4244
2024-02-07 12:15:49,352 Epoch 4244: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:15:49,352 EPOCH 4245
2024-02-07 12:15:56,537 [Epoch: 4245 Step: 00038200] Batch Recognition Loss:   0.000413 => Gls Tokens per Sec:      713 || Batch Translation Loss:   0.017428 => Txt Tokens per Sec:     2028 || Lr: 0.000100
2024-02-07 12:16:05,864 Epoch 4245: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:16:05,864 EPOCH 4246
2024-02-07 12:16:22,319 Epoch 4246: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:16:22,319 EPOCH 4247
2024-02-07 12:16:38,714 Epoch 4247: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:16:38,715 EPOCH 4248
2024-02-07 12:16:54,820 Epoch 4248: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-07 12:16:54,821 EPOCH 4249
2024-02-07 12:17:11,008 Epoch 4249: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 12:17:11,009 EPOCH 4250
2024-02-07 12:17:27,264 Epoch 4250: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:17:27,265 EPOCH 4251
2024-02-07 12:17:43,804 Epoch 4251: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:17:43,804 EPOCH 4252
2024-02-07 12:18:00,279 Epoch 4252: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:18:00,279 EPOCH 4253
2024-02-07 12:18:16,489 Epoch 4253: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-07 12:18:16,489 EPOCH 4254
2024-02-07 12:18:32,727 Epoch 4254: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:18:32,727 EPOCH 4255
2024-02-07 12:18:48,948 Epoch 4255: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 12:18:48,948 EPOCH 4256
2024-02-07 12:18:54,021 [Epoch: 4256 Step: 00038300] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     1263 || Batch Translation Loss:   0.019041 => Txt Tokens per Sec:     3439 || Lr: 0.000100
2024-02-07 12:19:05,604 Epoch 4256: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 12:19:05,604 EPOCH 4257
2024-02-07 12:19:21,989 Epoch 4257: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:19:21,990 EPOCH 4258
2024-02-07 12:19:38,629 Epoch 4258: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:19:38,630 EPOCH 4259
2024-02-07 12:19:55,338 Epoch 4259: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 12:19:55,338 EPOCH 4260
2024-02-07 12:20:11,880 Epoch 4260: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 12:20:11,880 EPOCH 4261
2024-02-07 12:20:28,716 Epoch 4261: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 12:20:28,718 EPOCH 4262
2024-02-07 12:20:45,058 Epoch 4262: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 12:20:45,058 EPOCH 4263
2024-02-07 12:21:01,585 Epoch 4263: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-07 12:21:01,586 EPOCH 4264
2024-02-07 12:21:18,186 Epoch 4264: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-07 12:21:18,186 EPOCH 4265
2024-02-07 12:21:34,480 Epoch 4265: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-07 12:21:34,481 EPOCH 4266
2024-02-07 12:21:50,828 Epoch 4266: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-07 12:21:50,829 EPOCH 4267
2024-02-07 12:22:02,451 [Epoch: 4267 Step: 00038400] Batch Recognition Loss:   0.000421 => Gls Tokens per Sec:      661 || Batch Translation Loss:   0.054524 => Txt Tokens per Sec:     1935 || Lr: 0.000100
2024-02-07 12:22:07,679 Epoch 4267: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-07 12:22:07,680 EPOCH 4268
2024-02-07 12:22:23,983 Epoch 4268: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-07 12:22:23,984 EPOCH 4269
2024-02-07 12:22:40,310 Epoch 4269: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-07 12:22:40,310 EPOCH 4270
2024-02-07 12:22:56,679 Epoch 4270: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-07 12:22:56,679 EPOCH 4271
2024-02-07 12:23:12,891 Epoch 4271: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-07 12:23:12,892 EPOCH 4272
2024-02-07 12:23:29,439 Epoch 4272: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.35 
2024-02-07 12:23:29,440 EPOCH 4273
2024-02-07 12:23:45,638 Epoch 4273: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.11 
2024-02-07 12:23:45,639 EPOCH 4274
2024-02-07 12:24:01,971 Epoch 4274: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.10 
2024-02-07 12:24:01,972 EPOCH 4275
2024-02-07 12:24:18,156 Epoch 4275: Total Training Recognition Loss 0.05  Total Training Translation Loss 6.18 
2024-02-07 12:24:18,157 EPOCH 4276
2024-02-07 12:24:34,539 Epoch 4276: Total Training Recognition Loss 0.04  Total Training Translation Loss 8.48 
2024-02-07 12:24:34,540 EPOCH 4277
2024-02-07 12:24:50,856 Epoch 4277: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.45 
2024-02-07 12:24:50,856 EPOCH 4278
2024-02-07 12:25:02,476 [Epoch: 4278 Step: 00038500] Batch Recognition Loss:   0.002747 => Gls Tokens per Sec:      771 || Batch Translation Loss:   0.090710 => Txt Tokens per Sec:     2168 || Lr: 0.000100
2024-02-07 12:25:07,111 Epoch 4278: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.64 
2024-02-07 12:25:07,111 EPOCH 4279
2024-02-07 12:25:23,597 Epoch 4279: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.78 
2024-02-07 12:25:23,598 EPOCH 4280
2024-02-07 12:25:39,998 Epoch 4280: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-07 12:25:39,999 EPOCH 4281
2024-02-07 12:25:56,510 Epoch 4281: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-07 12:25:56,510 EPOCH 4282
2024-02-07 12:26:12,935 Epoch 4282: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-07 12:26:12,935 EPOCH 4283
2024-02-07 12:26:29,447 Epoch 4283: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-07 12:26:29,448 EPOCH 4284
2024-02-07 12:26:45,581 Epoch 4284: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 12:26:45,582 EPOCH 4285
2024-02-07 12:27:02,374 Epoch 4285: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 12:27:02,375 EPOCH 4286
2024-02-07 12:27:18,980 Epoch 4286: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 12:27:18,980 EPOCH 4287
2024-02-07 12:27:35,632 Epoch 4287: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 12:27:35,632 EPOCH 4288
2024-02-07 12:27:51,905 Epoch 4288: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 12:27:51,906 EPOCH 4289
2024-02-07 12:28:07,744 [Epoch: 4289 Step: 00038600] Batch Recognition Loss:   0.000286 => Gls Tokens per Sec:      590 || Batch Translation Loss:   0.015681 => Txt Tokens per Sec:     1611 || Lr: 0.000100
2024-02-07 12:28:08,656 Epoch 4289: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 12:28:08,657 EPOCH 4290
2024-02-07 12:28:25,099 Epoch 4290: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 12:28:25,100 EPOCH 4291
2024-02-07 12:28:41,492 Epoch 4291: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 12:28:41,492 EPOCH 4292
2024-02-07 12:28:57,770 Epoch 4292: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 12:28:57,770 EPOCH 4293
2024-02-07 12:29:14,441 Epoch 4293: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 12:29:14,442 EPOCH 4294
2024-02-07 12:29:30,890 Epoch 4294: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 12:29:30,891 EPOCH 4295
2024-02-07 12:29:47,473 Epoch 4295: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 12:29:47,473 EPOCH 4296
2024-02-07 12:30:03,852 Epoch 4296: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 12:30:03,852 EPOCH 4297
2024-02-07 12:30:20,079 Epoch 4297: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 12:30:20,080 EPOCH 4298
2024-02-07 12:30:36,550 Epoch 4298: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 12:30:36,550 EPOCH 4299
2024-02-07 12:30:53,164 Epoch 4299: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 12:30:53,164 EPOCH 4300
2024-02-07 12:31:09,484 [Epoch: 4300 Step: 00038700] Batch Recognition Loss:   0.000250 => Gls Tokens per Sec:      651 || Batch Translation Loss:   0.016908 => Txt Tokens per Sec:     1800 || Lr: 0.000100
2024-02-07 12:31:09,485 Epoch 4300: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:31:09,485 EPOCH 4301
2024-02-07 12:31:25,763 Epoch 4301: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:31:25,763 EPOCH 4302
2024-02-07 12:31:42,238 Epoch 4302: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 12:31:42,239 EPOCH 4303
2024-02-07 12:31:58,505 Epoch 4303: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:31:58,505 EPOCH 4304
2024-02-07 12:32:14,847 Epoch 4304: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:32:14,848 EPOCH 4305
2024-02-07 12:32:31,340 Epoch 4305: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:32:31,340 EPOCH 4306
2024-02-07 12:32:47,819 Epoch 4306: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:32:47,820 EPOCH 4307
2024-02-07 12:33:04,528 Epoch 4307: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:33:04,528 EPOCH 4308
2024-02-07 12:33:20,934 Epoch 4308: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:33:20,935 EPOCH 4309
2024-02-07 12:33:37,304 Epoch 4309: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:33:37,305 EPOCH 4310
2024-02-07 12:33:53,831 Epoch 4310: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:33:53,832 EPOCH 4311
2024-02-07 12:34:10,392 Epoch 4311: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:34:10,393 EPOCH 4312
2024-02-07 12:34:14,721 [Epoch: 4312 Step: 00038800] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:       88 || Batch Translation Loss:   0.005919 => Txt Tokens per Sec:      314 || Lr: 0.000100
2024-02-07 12:34:26,682 Epoch 4312: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:34:26,683 EPOCH 4313
2024-02-07 12:34:42,885 Epoch 4313: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:34:42,886 EPOCH 4314
2024-02-07 12:34:59,365 Epoch 4314: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:34:59,366 EPOCH 4315
2024-02-07 12:35:15,790 Epoch 4315: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:35:15,791 EPOCH 4316
2024-02-07 12:35:32,358 Epoch 4316: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-07 12:35:32,358 EPOCH 4317
2024-02-07 12:35:48,711 Epoch 4317: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:35:48,712 EPOCH 4318
2024-02-07 12:36:05,435 Epoch 4318: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:36:05,435 EPOCH 4319
2024-02-07 12:36:21,893 Epoch 4319: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:36:21,894 EPOCH 4320
2024-02-07 12:36:38,121 Epoch 4320: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:36:38,122 EPOCH 4321
2024-02-07 12:36:54,645 Epoch 4321: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:36:54,645 EPOCH 4322
2024-02-07 12:37:11,403 Epoch 4322: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:37:11,404 EPOCH 4323
2024-02-07 12:37:12,380 [Epoch: 4323 Step: 00038900] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:     2627 || Batch Translation Loss:   0.014657 => Txt Tokens per Sec:     7325 || Lr: 0.000100
2024-02-07 12:37:27,621 Epoch 4323: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 12:37:27,621 EPOCH 4324
2024-02-07 12:37:44,002 Epoch 4324: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:37:44,003 EPOCH 4325
2024-02-07 12:38:00,371 Epoch 4325: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:38:00,372 EPOCH 4326
2024-02-07 12:38:16,650 Epoch 4326: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:38:16,651 EPOCH 4327
2024-02-07 12:38:32,898 Epoch 4327: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-07 12:38:32,898 EPOCH 4328
2024-02-07 12:38:49,468 Epoch 4328: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:38:49,468 EPOCH 4329
2024-02-07 12:39:05,911 Epoch 4329: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:39:05,912 EPOCH 4330
2024-02-07 12:39:22,112 Epoch 4330: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:39:22,113 EPOCH 4331
2024-02-07 12:39:38,491 Epoch 4331: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:39:38,492 EPOCH 4332
2024-02-07 12:39:54,862 Epoch 4332: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:39:54,863 EPOCH 4333
2024-02-07 12:40:11,634 Epoch 4333: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:40:11,634 EPOCH 4334
2024-02-07 12:40:15,825 [Epoch: 4334 Step: 00039000] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:      917 || Batch Translation Loss:   0.039682 => Txt Tokens per Sec:     2742 || Lr: 0.000100
2024-02-07 12:40:28,564 Epoch 4334: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 12:40:28,565 EPOCH 4335
2024-02-07 12:40:44,937 Epoch 4335: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:40:44,938 EPOCH 4336
2024-02-07 12:41:01,395 Epoch 4336: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:41:01,396 EPOCH 4337
2024-02-07 12:41:17,699 Epoch 4337: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:41:17,699 EPOCH 4338
2024-02-07 12:41:33,650 Epoch 4338: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:41:33,650 EPOCH 4339
2024-02-07 12:41:50,288 Epoch 4339: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:41:50,289 EPOCH 4340
2024-02-07 12:42:06,751 Epoch 4340: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:42:06,752 EPOCH 4341
2024-02-07 12:42:23,389 Epoch 4341: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:42:23,389 EPOCH 4342
2024-02-07 12:42:40,012 Epoch 4342: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 12:42:40,013 EPOCH 4343
2024-02-07 12:42:56,540 Epoch 4343: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 12:42:56,541 EPOCH 4344
2024-02-07 12:43:12,899 Epoch 4344: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:43:12,899 EPOCH 4345
2024-02-07 12:43:24,634 [Epoch: 4345 Step: 00039100] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:      360 || Batch Translation Loss:   0.011895 => Txt Tokens per Sec:     1103 || Lr: 0.000100
2024-02-07 12:43:29,411 Epoch 4345: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:43:29,411 EPOCH 4346
2024-02-07 12:43:45,643 Epoch 4346: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:43:45,643 EPOCH 4347
2024-02-07 12:44:02,040 Epoch 4347: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:44:02,040 EPOCH 4348
2024-02-07 12:44:18,718 Epoch 4348: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:44:18,719 EPOCH 4349
2024-02-07 12:44:35,053 Epoch 4349: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:44:35,053 EPOCH 4350
2024-02-07 12:44:51,415 Epoch 4350: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:44:51,416 EPOCH 4351
2024-02-07 12:45:07,989 Epoch 4351: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:45:07,991 EPOCH 4352
2024-02-07 12:45:24,488 Epoch 4352: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:45:24,489 EPOCH 4353
2024-02-07 12:45:40,799 Epoch 4353: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:45:40,800 EPOCH 4354
2024-02-07 12:45:57,424 Epoch 4354: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 12:45:57,424 EPOCH 4355
2024-02-07 12:46:14,014 Epoch 4355: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 12:46:14,014 EPOCH 4356
2024-02-07 12:46:25,789 [Epoch: 4356 Step: 00039200] Batch Recognition Loss:   0.000291 => Gls Tokens per Sec:      467 || Batch Translation Loss:   0.017480 => Txt Tokens per Sec:     1357 || Lr: 0.000100
2024-02-07 12:46:30,431 Epoch 4356: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:46:30,431 EPOCH 4357
2024-02-07 12:46:46,838 Epoch 4357: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 12:46:46,839 EPOCH 4358
2024-02-07 12:47:03,123 Epoch 4358: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:47:03,123 EPOCH 4359
2024-02-07 12:47:19,536 Epoch 4359: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 12:47:19,538 EPOCH 4360
2024-02-07 12:47:36,004 Epoch 4360: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 12:47:36,004 EPOCH 4361
2024-02-07 12:47:52,327 Epoch 4361: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 12:47:52,328 EPOCH 4362
2024-02-07 12:48:08,614 Epoch 4362: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 12:48:08,614 EPOCH 4363
2024-02-07 12:48:25,043 Epoch 4363: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 12:48:25,044 EPOCH 4364
2024-02-07 12:48:41,456 Epoch 4364: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 12:48:41,456 EPOCH 4365
2024-02-07 12:48:57,967 Epoch 4365: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 12:48:57,967 EPOCH 4366
2024-02-07 12:49:14,196 Epoch 4366: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 12:49:14,197 EPOCH 4367
2024-02-07 12:49:23,425 [Epoch: 4367 Step: 00039300] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:      735 || Batch Translation Loss:   0.018610 => Txt Tokens per Sec:     1949 || Lr: 0.000100
2024-02-07 12:49:30,564 Epoch 4367: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 12:49:30,565 EPOCH 4368
2024-02-07 12:49:46,910 Epoch 4368: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 12:49:46,910 EPOCH 4369
2024-02-07 12:50:03,329 Epoch 4369: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 12:50:03,329 EPOCH 4370
2024-02-07 12:50:19,877 Epoch 4370: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 12:50:19,878 EPOCH 4371
2024-02-07 12:50:36,054 Epoch 4371: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 12:50:36,054 EPOCH 4372
2024-02-07 12:50:52,643 Epoch 4372: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 12:50:52,643 EPOCH 4373
2024-02-07 12:51:09,087 Epoch 4373: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 12:51:09,088 EPOCH 4374
2024-02-07 12:51:25,601 Epoch 4374: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 12:51:25,601 EPOCH 4375
2024-02-07 12:51:41,716 Epoch 4375: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:51:41,717 EPOCH 4376
2024-02-07 12:51:58,013 Epoch 4376: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-07 12:51:58,013 EPOCH 4377
2024-02-07 12:52:14,245 Epoch 4377: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:52:14,246 EPOCH 4378
2024-02-07 12:52:30,001 [Epoch: 4378 Step: 00039400] Batch Recognition Loss:   0.000423 => Gls Tokens per Sec:      512 || Batch Translation Loss:   0.017361 => Txt Tokens per Sec:     1411 || Lr: 0.000100
2024-02-07 12:52:31,097 Epoch 4378: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:52:31,098 EPOCH 4379
2024-02-07 12:52:47,611 Epoch 4379: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-07 12:52:47,612 EPOCH 4380
2024-02-07 12:53:03,922 Epoch 4380: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:53:03,923 EPOCH 4381
2024-02-07 12:53:20,155 Epoch 4381: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-07 12:53:20,155 EPOCH 4382
2024-02-07 12:53:36,735 Epoch 4382: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:53:36,735 EPOCH 4383
2024-02-07 12:53:52,843 Epoch 4383: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:53:52,844 EPOCH 4384
2024-02-07 12:54:09,080 Epoch 4384: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:54:09,080 EPOCH 4385
2024-02-07 12:54:25,684 Epoch 4385: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 12:54:25,685 EPOCH 4386
2024-02-07 12:54:42,284 Epoch 4386: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:54:42,285 EPOCH 4387
2024-02-07 12:54:59,025 Epoch 4387: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 12:54:59,026 EPOCH 4388
2024-02-07 12:55:15,532 Epoch 4388: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 12:55:15,532 EPOCH 4389
2024-02-07 12:55:31,342 [Epoch: 4389 Step: 00039500] Batch Recognition Loss:   0.000316 => Gls Tokens per Sec:      591 || Batch Translation Loss:   0.014490 => Txt Tokens per Sec:     1613 || Lr: 0.000100
2024-02-07 12:55:32,045 Epoch 4389: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 12:55:32,045 EPOCH 4390
2024-02-07 12:55:48,387 Epoch 4390: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:55:48,387 EPOCH 4391
2024-02-07 12:56:04,927 Epoch 4391: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:56:04,927 EPOCH 4392
2024-02-07 12:56:21,387 Epoch 4392: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 12:56:21,388 EPOCH 4393
2024-02-07 12:56:37,856 Epoch 4393: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 12:56:37,857 EPOCH 4394
2024-02-07 12:56:54,307 Epoch 4394: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 12:56:54,307 EPOCH 4395
2024-02-07 12:57:10,413 Epoch 4395: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 12:57:10,413 EPOCH 4396
2024-02-07 12:57:26,593 Epoch 4396: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 12:57:26,593 EPOCH 4397
2024-02-07 12:57:42,906 Epoch 4397: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 12:57:42,907 EPOCH 4398
2024-02-07 12:57:59,277 Epoch 4398: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 12:57:59,278 EPOCH 4399
2024-02-07 12:58:15,482 Epoch 4399: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-07 12:58:15,483 EPOCH 4400
2024-02-07 12:58:32,095 [Epoch: 4400 Step: 00039600] Batch Recognition Loss:   0.000241 => Gls Tokens per Sec:      639 || Batch Translation Loss:   0.022238 => Txt Tokens per Sec:     1769 || Lr: 0.000100
2024-02-07 12:58:32,096 Epoch 4400: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 12:58:32,096 EPOCH 4401
2024-02-07 12:58:48,495 Epoch 4401: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 12:58:48,496 EPOCH 4402
2024-02-07 12:59:04,795 Epoch 4402: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-07 12:59:04,797 EPOCH 4403
2024-02-07 12:59:21,229 Epoch 4403: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-07 12:59:21,230 EPOCH 4404
2024-02-07 12:59:37,816 Epoch 4404: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 12:59:37,816 EPOCH 4405
2024-02-07 12:59:54,521 Epoch 4405: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 12:59:54,522 EPOCH 4406
2024-02-07 13:00:10,937 Epoch 4406: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 13:00:10,938 EPOCH 4407
2024-02-07 13:00:27,175 Epoch 4407: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 13:00:27,176 EPOCH 4408
2024-02-07 13:00:43,560 Epoch 4408: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-07 13:00:43,561 EPOCH 4409
2024-02-07 13:01:00,192 Epoch 4409: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-07 13:01:00,193 EPOCH 4410
2024-02-07 13:01:16,735 Epoch 4410: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-07 13:01:16,736 EPOCH 4411
2024-02-07 13:01:32,994 Epoch 4411: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-07 13:01:32,995 EPOCH 4412
2024-02-07 13:01:33,705 [Epoch: 4412 Step: 00039700] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:     1808 || Batch Translation Loss:   0.057701 => Txt Tokens per Sec:     5487 || Lr: 0.000100
2024-02-07 13:01:49,543 Epoch 4412: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-07 13:01:49,544 EPOCH 4413
2024-02-07 13:02:06,189 Epoch 4413: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-07 13:02:06,190 EPOCH 4414
2024-02-07 13:02:22,448 Epoch 4414: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-07 13:02:22,449 EPOCH 4415
2024-02-07 13:02:38,901 Epoch 4415: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 13:02:38,902 EPOCH 4416
2024-02-07 13:02:55,300 Epoch 4416: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 13:02:55,301 EPOCH 4417
2024-02-07 13:03:11,988 Epoch 4417: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 13:03:11,989 EPOCH 4418
2024-02-07 13:03:28,351 Epoch 4418: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 13:03:28,352 EPOCH 4419
2024-02-07 13:03:44,557 Epoch 4419: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 13:03:44,558 EPOCH 4420
2024-02-07 13:04:00,799 Epoch 4420: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 13:04:00,800 EPOCH 4421
2024-02-07 13:04:17,311 Epoch 4421: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 13:04:17,312 EPOCH 4422
2024-02-07 13:04:33,556 Epoch 4422: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 13:04:33,557 EPOCH 4423
2024-02-07 13:04:42,850 [Epoch: 4423 Step: 00039800] Batch Recognition Loss:   0.000328 => Gls Tokens per Sec:      276 || Batch Translation Loss:   0.022248 => Txt Tokens per Sec:      916 || Lr: 0.000100
2024-02-07 13:04:49,786 Epoch 4423: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 13:04:49,787 EPOCH 4424
2024-02-07 13:05:06,308 Epoch 4424: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 13:05:06,309 EPOCH 4425
2024-02-07 13:05:22,609 Epoch 4425: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 13:05:22,609 EPOCH 4426
2024-02-07 13:05:39,170 Epoch 4426: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 13:05:39,171 EPOCH 4427
2024-02-07 13:05:55,648 Epoch 4427: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 13:05:55,648 EPOCH 4428
2024-02-07 13:06:11,736 Epoch 4428: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 13:06:11,737 EPOCH 4429
2024-02-07 13:06:28,078 Epoch 4429: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 13:06:28,079 EPOCH 4430
2024-02-07 13:06:44,343 Epoch 4430: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 13:06:44,343 EPOCH 4431
2024-02-07 13:07:00,678 Epoch 4431: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 13:07:00,679 EPOCH 4432
2024-02-07 13:07:17,352 Epoch 4432: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 13:07:17,353 EPOCH 4433
2024-02-07 13:07:33,718 Epoch 4433: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 13:07:33,718 EPOCH 4434
2024-02-07 13:07:41,705 [Epoch: 4434 Step: 00039900] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:      368 || Batch Translation Loss:   0.015212 => Txt Tokens per Sec:     1041 || Lr: 0.000100
2024-02-07 13:07:50,288 Epoch 4434: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-07 13:07:50,288 EPOCH 4435
2024-02-07 13:08:06,501 Epoch 4435: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-07 13:08:06,502 EPOCH 4436
2024-02-07 13:08:23,165 Epoch 4436: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 13:08:23,165 EPOCH 4437
2024-02-07 13:08:39,437 Epoch 4437: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-07 13:08:39,438 EPOCH 4438
2024-02-07 13:08:56,066 Epoch 4438: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-07 13:08:56,066 EPOCH 4439
2024-02-07 13:09:12,396 Epoch 4439: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-07 13:09:12,396 EPOCH 4440
2024-02-07 13:09:28,589 Epoch 4440: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-07 13:09:28,590 EPOCH 4441
2024-02-07 13:09:45,278 Epoch 4441: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-07 13:09:45,279 EPOCH 4442
2024-02-07 13:10:01,645 Epoch 4442: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-07 13:10:01,645 EPOCH 4443
2024-02-07 13:10:18,200 Epoch 4443: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-07 13:10:18,201 EPOCH 4444
2024-02-07 13:10:34,751 Epoch 4444: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-07 13:10:34,752 EPOCH 4445
2024-02-07 13:10:39,153 [Epoch: 4445 Step: 00040000] Batch Recognition Loss:   0.000271 => Gls Tokens per Sec:     1164 || Batch Translation Loss:   0.050951 => Txt Tokens per Sec:     2909 || Lr: 0.000100
2024-02-07 13:11:46,712 Validation result at epoch 4445, step    40000: duration: 67.5576s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.25351	Translation Loss: 98860.15625	PPL: 19422.09180
	Eval Metric: BLEU
	WER 2.33	(DEL: 0.00,	INS: 0.00,	SUB: 2.33)
	BLEU-4 0.52	(BLEU-1: 9.69,	BLEU-2: 2.73,	BLEU-3: 1.02,	BLEU-4: 0.52)
	CHRF 16.40	ROUGE 8.21
2024-02-07 13:11:46,714 Logging Recognition and Translation Outputs
2024-02-07 13:11:46,714 ========================================================================================================================
2024-02-07 13:11:46,714 Logging Sequence: 86_84.00
2024-02-07 13:11:46,715 	Gloss Reference :	A B+C+D+E
2024-02-07 13:11:46,715 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 13:11:46,715 	Gloss Alignment :	         
2024-02-07 13:11:46,717 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 13:11:46,719 	Text Reference  :	amassing 8933 runs    which included 21 centuries with   a  highest score of  201     not out 
2024-02-07 13:11:46,719 	Text Hypothesis :	******** **** yashpal was   one      of the       heroes in india'  world cup triumph in  1983
2024-02-07 13:11:46,719 	Text Alignment  :	D        D    S       S     S        S  S         S      S  S       S     S   S       S   S   
2024-02-07 13:11:46,719 ========================================================================================================================
2024-02-07 13:11:46,719 Logging Sequence: 179_110.00
2024-02-07 13:11:46,720 	Gloss Reference :	A B+C+D+E
2024-02-07 13:11:46,720 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 13:11:46,720 	Gloss Alignment :	         
2024-02-07 13:11:46,720 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 13:11:46,722 	Text Reference  :	******* **** *** *** ********* ****** *** ******* phogat refused    to  stay  in       the same room with    other indian female wrestlers
2024-02-07 13:11:46,722 	Text Hypothesis :	however soon old old coca-cola advert had respect their  daughter's and sonam received it  at   the  airport on    the    july   2023     
2024-02-07 13:11:46,722 	Text Alignment  :	I       I    I   I   I         I      I   I       S      S          S   S     S        S   S    S    S       S     S      S      S        
2024-02-07 13:11:46,722 ========================================================================================================================
2024-02-07 13:11:46,722 Logging Sequence: 102_2.00
2024-02-07 13:11:46,723 	Gloss Reference :	A B+C+D+E    
2024-02-07 13:11:46,723 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-07 13:11:46,723 	Gloss Alignment :	  S          
2024-02-07 13:11:46,723 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 13:11:46,724 	Text Reference  :	commonwealth games are ********* ***** among the world's most recognised gaming championships after   the olympics
2024-02-07 13:11:46,724 	Text Hypothesis :	the          games are currently being held  in  pune    were supposed   to     play          against his place   
2024-02-07 13:11:46,725 	Text Alignment  :	S                      I         I     S     S   S       S    S          S      S             S       S   S       
2024-02-07 13:11:46,725 ========================================================================================================================
2024-02-07 13:11:46,725 Logging Sequence: 60_195.00
2024-02-07 13:11:46,725 	Gloss Reference :	A B+C+D+E
2024-02-07 13:11:46,725 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 13:11:46,725 	Gloss Alignment :	         
2024-02-07 13:11:46,725 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 13:11:46,726 	Text Reference  :	people loved to  watch his   aggressive expressions and   his   bowling
2024-02-07 13:11:46,726 	Text Hypothesis :	as     bcci  has an    image on         the         insta story may    
2024-02-07 13:11:46,726 	Text Alignment  :	S      S     S   S     S     S          S           S     S     S      
2024-02-07 13:11:46,727 ========================================================================================================================
2024-02-07 13:11:46,727 Logging Sequence: 70_200.00
2024-02-07 13:11:46,727 	Gloss Reference :	A B+C+D+E
2024-02-07 13:11:46,727 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 13:11:46,727 	Gloss Alignment :	         
2024-02-07 13:11:46,727 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 13:11:46,728 	Text Reference  :	******* ***** ***** ** showing ronaldo whole-heartedly endorsing the brand 
2024-02-07 13:11:46,728 	Text Hypothesis :	however being proud of her     beer    bottle          against   a   condom
2024-02-07 13:11:46,728 	Text Alignment  :	I       I     I     I  S       S       S               S         S   S     
2024-02-07 13:11:46,728 ========================================================================================================================
2024-02-07 13:11:59,441 Epoch 4445: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-07 13:11:59,442 EPOCH 4446
2024-02-07 13:12:16,420 Epoch 4446: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-07 13:12:16,421 EPOCH 4447
2024-02-07 13:12:32,447 Epoch 4447: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.78 
2024-02-07 13:12:32,447 EPOCH 4448
2024-02-07 13:12:48,714 Epoch 4448: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.87 
2024-02-07 13:12:48,715 EPOCH 4449
2024-02-07 13:13:05,400 Epoch 4449: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.15 
2024-02-07 13:13:05,401 EPOCH 4450
2024-02-07 13:13:21,821 Epoch 4450: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.62 
2024-02-07 13:13:21,821 EPOCH 4451
2024-02-07 13:13:38,588 Epoch 4451: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.05 
2024-02-07 13:13:38,588 EPOCH 4452
2024-02-07 13:13:54,838 Epoch 4452: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.76 
2024-02-07 13:13:54,839 EPOCH 4453
2024-02-07 13:14:11,221 Epoch 4453: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.55 
2024-02-07 13:14:11,222 EPOCH 4454
2024-02-07 13:14:27,823 Epoch 4454: Total Training Recognition Loss 0.30  Total Training Translation Loss 0.40 
2024-02-07 13:14:27,824 EPOCH 4455
2024-02-07 13:14:44,385 Epoch 4455: Total Training Recognition Loss 0.17  Total Training Translation Loss 0.35 
2024-02-07 13:14:44,386 EPOCH 4456
2024-02-07 13:14:49,227 [Epoch: 4456 Step: 00040100] Batch Recognition Loss:   0.003942 => Gls Tokens per Sec:     1322 || Batch Translation Loss:   0.027617 => Txt Tokens per Sec:     3352 || Lr: 0.000100
2024-02-07 13:15:00,920 Epoch 4456: Total Training Recognition Loss 1.06  Total Training Translation Loss 0.30 
2024-02-07 13:15:00,920 EPOCH 4457
2024-02-07 13:15:17,034 Epoch 4457: Total Training Recognition Loss 0.13  Total Training Translation Loss 0.29 
2024-02-07 13:15:17,035 EPOCH 4458
2024-02-07 13:15:33,283 Epoch 4458: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.28 
2024-02-07 13:15:33,283 EPOCH 4459
2024-02-07 13:15:49,876 Epoch 4459: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.26 
2024-02-07 13:15:49,876 EPOCH 4460
2024-02-07 13:16:06,215 Epoch 4460: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-07 13:16:06,215 EPOCH 4461
2024-02-07 13:16:22,581 Epoch 4461: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.20 
2024-02-07 13:16:22,582 EPOCH 4462
2024-02-07 13:16:39,280 Epoch 4462: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 13:16:39,281 EPOCH 4463
2024-02-07 13:16:55,400 Epoch 4463: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 13:16:55,400 EPOCH 4464
2024-02-07 13:17:11,776 Epoch 4464: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 13:17:11,776 EPOCH 4465
2024-02-07 13:17:28,369 Epoch 4465: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 13:17:28,370 EPOCH 4466
2024-02-07 13:17:44,758 Epoch 4466: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 13:17:44,759 EPOCH 4467
2024-02-07 13:17:57,156 [Epoch: 4467 Step: 00040200] Batch Recognition Loss:   0.001796 => Gls Tokens per Sec:      547 || Batch Translation Loss:   0.020229 => Txt Tokens per Sec:     1511 || Lr: 0.000100
2024-02-07 13:18:01,376 Epoch 4467: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 13:18:01,376 EPOCH 4468
2024-02-07 13:18:17,736 Epoch 4468: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 13:18:17,736 EPOCH 4469
2024-02-07 13:18:33,864 Epoch 4469: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 13:18:33,864 EPOCH 4470
2024-02-07 13:18:50,328 Epoch 4470: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 13:18:50,329 EPOCH 4471
2024-02-07 13:19:06,701 Epoch 4471: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 13:19:06,701 EPOCH 4472
2024-02-07 13:19:22,990 Epoch 4472: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 13:19:22,990 EPOCH 4473
2024-02-07 13:19:39,682 Epoch 4473: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 13:19:39,683 EPOCH 4474
2024-02-07 13:19:56,162 Epoch 4474: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 13:19:56,162 EPOCH 4475
2024-02-07 13:20:12,605 Epoch 4475: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 13:20:12,605 EPOCH 4476
2024-02-07 13:20:29,143 Epoch 4476: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 13:20:29,144 EPOCH 4477
2024-02-07 13:20:45,431 Epoch 4477: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 13:20:45,431 EPOCH 4478
2024-02-07 13:21:00,777 [Epoch: 4478 Step: 00040300] Batch Recognition Loss:   0.000219 => Gls Tokens per Sec:      525 || Batch Translation Loss:   0.008062 => Txt Tokens per Sec:     1444 || Lr: 0.000100
2024-02-07 13:21:01,784 Epoch 4478: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 13:21:01,784 EPOCH 4479
2024-02-07 13:21:18,018 Epoch 4479: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 13:21:18,018 EPOCH 4480
2024-02-07 13:21:34,532 Epoch 4480: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 13:21:34,533 EPOCH 4481
2024-02-07 13:21:51,100 Epoch 4481: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 13:21:51,101 EPOCH 4482
2024-02-07 13:22:07,854 Epoch 4482: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-07 13:22:07,855 EPOCH 4483
2024-02-07 13:22:24,297 Epoch 4483: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 13:22:24,297 EPOCH 4484
2024-02-07 13:22:40,475 Epoch 4484: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 13:22:40,476 EPOCH 4485
2024-02-07 13:22:56,742 Epoch 4485: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 13:22:56,743 EPOCH 4486
2024-02-07 13:23:13,057 Epoch 4486: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-07 13:23:13,057 EPOCH 4487
2024-02-07 13:23:29,383 Epoch 4487: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 13:23:29,384 EPOCH 4488
2024-02-07 13:23:46,028 Epoch 4488: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 13:23:46,029 EPOCH 4489
2024-02-07 13:23:58,089 [Epoch: 4489 Step: 00040400] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:      849 || Batch Translation Loss:   0.016539 => Txt Tokens per Sec:     2324 || Lr: 0.000100
2024-02-07 13:24:02,396 Epoch 4489: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-07 13:24:02,397 EPOCH 4490
2024-02-07 13:24:19,312 Epoch 4490: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-07 13:24:19,313 EPOCH 4491
2024-02-07 13:24:35,613 Epoch 4491: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 13:24:35,613 EPOCH 4492
2024-02-07 13:24:51,891 Epoch 4492: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 13:24:51,892 EPOCH 4493
2024-02-07 13:25:08,315 Epoch 4493: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 13:25:08,316 EPOCH 4494
2024-02-07 13:25:24,557 Epoch 4494: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-07 13:25:24,558 EPOCH 4495
2024-02-07 13:25:40,805 Epoch 4495: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 13:25:40,805 EPOCH 4496
2024-02-07 13:25:57,201 Epoch 4496: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 13:25:57,202 EPOCH 4497
2024-02-07 13:26:13,638 Epoch 4497: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 13:26:13,639 EPOCH 4498
2024-02-07 13:26:30,182 Epoch 4498: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 13:26:30,183 EPOCH 4499
2024-02-07 13:26:46,706 Epoch 4499: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 13:26:46,707 EPOCH 4500
2024-02-07 13:27:03,065 [Epoch: 4500 Step: 00040500] Batch Recognition Loss:   0.000235 => Gls Tokens per Sec:      649 || Batch Translation Loss:   0.018613 => Txt Tokens per Sec:     1796 || Lr: 0.000100
2024-02-07 13:27:03,066 Epoch 4500: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 13:27:03,066 EPOCH 4501
2024-02-07 13:27:19,639 Epoch 4501: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 13:27:19,640 EPOCH 4502
2024-02-07 13:27:35,937 Epoch 4502: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 13:27:35,937 EPOCH 4503
2024-02-07 13:27:52,281 Epoch 4503: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 13:27:52,281 EPOCH 4504
2024-02-07 13:28:08,737 Epoch 4504: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 13:28:08,737 EPOCH 4505
2024-02-07 13:28:25,310 Epoch 4505: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 13:28:25,311 EPOCH 4506
2024-02-07 13:28:41,760 Epoch 4506: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 13:28:41,761 EPOCH 4507
2024-02-07 13:28:58,195 Epoch 4507: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 13:28:58,196 EPOCH 4508
2024-02-07 13:29:14,419 Epoch 4508: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 13:29:14,420 EPOCH 4509
2024-02-07 13:29:31,113 Epoch 4509: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 13:29:31,114 EPOCH 4510
2024-02-07 13:29:47,704 Epoch 4510: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 13:29:47,704 EPOCH 4511
2024-02-07 13:30:04,369 Epoch 4511: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 13:30:04,370 EPOCH 4512
2024-02-07 13:30:04,744 [Epoch: 4512 Step: 00040600] Batch Recognition Loss:   0.000208 => Gls Tokens per Sec:     3432 || Batch Translation Loss:   0.023818 => Txt Tokens per Sec:     8737 || Lr: 0.000100
2024-02-07 13:30:20,590 Epoch 4512: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 13:30:20,591 EPOCH 4513
2024-02-07 13:30:36,918 Epoch 4513: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 13:30:36,919 EPOCH 4514
2024-02-07 13:30:53,467 Epoch 4514: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 13:30:53,468 EPOCH 4515
2024-02-07 13:31:09,705 Epoch 4515: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 13:31:09,705 EPOCH 4516
2024-02-07 13:31:26,597 Epoch 4516: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 13:31:26,597 EPOCH 4517
2024-02-07 13:31:42,856 Epoch 4517: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 13:31:42,856 EPOCH 4518
2024-02-07 13:31:59,501 Epoch 4518: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 13:31:59,501 EPOCH 4519
2024-02-07 13:32:15,877 Epoch 4519: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 13:32:15,878 EPOCH 4520
2024-02-07 13:32:32,260 Epoch 4520: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 13:32:32,260 EPOCH 4521
2024-02-07 13:32:48,707 Epoch 4521: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 13:32:48,708 EPOCH 4522
2024-02-07 13:33:05,074 Epoch 4522: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 13:33:05,075 EPOCH 4523
2024-02-07 13:33:08,534 [Epoch: 4523 Step: 00040700] Batch Recognition Loss:   0.000983 => Gls Tokens per Sec:      741 || Batch Translation Loss:   0.016888 => Txt Tokens per Sec:     2217 || Lr: 0.000100
2024-02-07 13:33:21,362 Epoch 4523: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 13:33:21,363 EPOCH 4524
2024-02-07 13:33:37,981 Epoch 4524: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 13:33:37,982 EPOCH 4525
2024-02-07 13:33:54,621 Epoch 4525: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 13:33:54,622 EPOCH 4526
2024-02-07 13:34:10,953 Epoch 4526: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 13:34:10,954 EPOCH 4527
2024-02-07 13:34:27,180 Epoch 4527: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-07 13:34:27,181 EPOCH 4528
2024-02-07 13:34:43,466 Epoch 4528: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-07 13:34:43,466 EPOCH 4529
2024-02-07 13:34:59,882 Epoch 4529: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-07 13:34:59,882 EPOCH 4530
2024-02-07 13:35:16,587 Epoch 4530: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-07 13:35:16,587 EPOCH 4531
2024-02-07 13:35:33,030 Epoch 4531: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-07 13:35:33,031 EPOCH 4532
2024-02-07 13:35:49,290 Epoch 4532: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-07 13:35:49,291 EPOCH 4533
2024-02-07 13:36:05,416 Epoch 4533: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-07 13:36:05,416 EPOCH 4534
2024-02-07 13:36:09,992 [Epoch: 4534 Step: 00040800] Batch Recognition Loss:   0.000319 => Gls Tokens per Sec:      840 || Batch Translation Loss:   0.028551 => Txt Tokens per Sec:     2557 || Lr: 0.000100
2024-02-07 13:36:22,009 Epoch 4534: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 13:36:22,009 EPOCH 4535
2024-02-07 13:36:38,726 Epoch 4535: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 13:36:38,727 EPOCH 4536
2024-02-07 13:36:55,139 Epoch 4536: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 13:36:55,140 EPOCH 4537
2024-02-07 13:37:11,388 Epoch 4537: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-07 13:37:11,389 EPOCH 4538
2024-02-07 13:37:27,923 Epoch 4538: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.01 
2024-02-07 13:37:27,923 EPOCH 4539
2024-02-07 13:37:44,136 Epoch 4539: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.37 
2024-02-07 13:37:44,137 EPOCH 4540
2024-02-07 13:38:00,421 Epoch 4540: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.53 
2024-02-07 13:38:00,421 EPOCH 4541
2024-02-07 13:38:17,350 Epoch 4541: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.62 
2024-02-07 13:38:17,351 EPOCH 4542
2024-02-07 13:38:33,656 Epoch 4542: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.00 
2024-02-07 13:38:33,657 EPOCH 4543
2024-02-07 13:38:49,911 Epoch 4543: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.68 
2024-02-07 13:38:49,912 EPOCH 4544
2024-02-07 13:39:06,271 Epoch 4544: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-07 13:39:06,271 EPOCH 4545
2024-02-07 13:39:15,213 [Epoch: 4545 Step: 00040900] Batch Recognition Loss:   0.000642 => Gls Tokens per Sec:      472 || Batch Translation Loss:   0.053295 => Txt Tokens per Sec:     1458 || Lr: 0.000100
2024-02-07 13:39:22,692 Epoch 4545: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-07 13:39:22,692 EPOCH 4546
2024-02-07 13:39:39,461 Epoch 4546: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-07 13:39:39,461 EPOCH 4547
2024-02-07 13:39:55,530 Epoch 4547: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-07 13:39:55,530 EPOCH 4548
2024-02-07 13:40:11,935 Epoch 4548: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-07 13:40:11,935 EPOCH 4549
2024-02-07 13:40:28,227 Epoch 4549: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 13:40:28,228 EPOCH 4550
2024-02-07 13:40:44,998 Epoch 4550: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 13:40:44,998 EPOCH 4551
2024-02-07 13:41:01,563 Epoch 4551: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 13:41:01,564 EPOCH 4552
2024-02-07 13:41:18,139 Epoch 4552: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 13:41:18,140 EPOCH 4553
2024-02-07 13:41:34,607 Epoch 4553: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 13:41:34,608 EPOCH 4554
2024-02-07 13:41:50,938 Epoch 4554: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 13:41:50,939 EPOCH 4555
2024-02-07 13:42:07,459 Epoch 4555: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 13:42:07,460 EPOCH 4556
2024-02-07 13:42:17,826 [Epoch: 4556 Step: 00041000] Batch Recognition Loss:   0.000305 => Gls Tokens per Sec:      617 || Batch Translation Loss:   0.009144 => Txt Tokens per Sec:     1659 || Lr: 0.000100
2024-02-07 13:42:23,865 Epoch 4556: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 13:42:23,866 EPOCH 4557
2024-02-07 13:42:40,282 Epoch 4557: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 13:42:40,283 EPOCH 4558
2024-02-07 13:42:56,465 Epoch 4558: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 13:42:56,466 EPOCH 4559
2024-02-07 13:43:12,787 Epoch 4559: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 13:43:12,788 EPOCH 4560
2024-02-07 13:43:28,912 Epoch 4560: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 13:43:28,912 EPOCH 4561
2024-02-07 13:43:45,460 Epoch 4561: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 13:43:45,461 EPOCH 4562
2024-02-07 13:44:01,959 Epoch 4562: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 13:44:01,960 EPOCH 4563
2024-02-07 13:44:18,242 Epoch 4563: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 13:44:18,243 EPOCH 4564
2024-02-07 13:44:34,710 Epoch 4564: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 13:44:34,711 EPOCH 4565
2024-02-07 13:44:51,093 Epoch 4565: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 13:44:51,093 EPOCH 4566
2024-02-07 13:45:07,514 Epoch 4566: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 13:45:07,515 EPOCH 4567
2024-02-07 13:45:19,324 [Epoch: 4567 Step: 00041100] Batch Recognition Loss:   0.000261 => Gls Tokens per Sec:      650 || Batch Translation Loss:   0.015499 => Txt Tokens per Sec:     1900 || Lr: 0.000100
2024-02-07 13:45:24,428 Epoch 4567: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 13:45:24,429 EPOCH 4568
2024-02-07 13:45:40,795 Epoch 4568: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 13:45:40,795 EPOCH 4569
2024-02-07 13:45:57,438 Epoch 4569: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 13:45:57,439 EPOCH 4570
2024-02-07 13:46:13,853 Epoch 4570: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 13:46:13,853 EPOCH 4571
2024-02-07 13:46:30,026 Epoch 4571: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 13:46:30,026 EPOCH 4572
2024-02-07 13:46:46,585 Epoch 4572: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 13:46:46,586 EPOCH 4573
2024-02-07 13:47:02,788 Epoch 4573: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 13:47:02,789 EPOCH 4574
2024-02-07 13:47:19,144 Epoch 4574: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 13:47:19,145 EPOCH 4575
2024-02-07 13:47:35,923 Epoch 4575: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 13:47:35,925 EPOCH 4576
2024-02-07 13:47:52,106 Epoch 4576: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-07 13:47:52,107 EPOCH 4577
2024-02-07 13:48:08,934 Epoch 4577: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 13:48:08,934 EPOCH 4578
2024-02-07 13:48:21,161 [Epoch: 4578 Step: 00041200] Batch Recognition Loss:   0.000661 => Gls Tokens per Sec:      659 || Batch Translation Loss:   0.016845 => Txt Tokens per Sec:     1752 || Lr: 0.000100
2024-02-07 13:48:25,235 Epoch 4578: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 13:48:25,236 EPOCH 4579
2024-02-07 13:48:41,780 Epoch 4579: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 13:48:41,781 EPOCH 4580
2024-02-07 13:48:58,061 Epoch 4580: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 13:48:58,061 EPOCH 4581
2024-02-07 13:49:14,538 Epoch 4581: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 13:49:14,539 EPOCH 4582
2024-02-07 13:49:30,888 Epoch 4582: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 13:49:30,889 EPOCH 4583
2024-02-07 13:49:47,557 Epoch 4583: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 13:49:47,558 EPOCH 4584
2024-02-07 13:50:03,990 Epoch 4584: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 13:50:03,990 EPOCH 4585
2024-02-07 13:50:20,496 Epoch 4585: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 13:50:20,496 EPOCH 4586
2024-02-07 13:50:37,148 Epoch 4586: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 13:50:37,149 EPOCH 4587
2024-02-07 13:50:53,529 Epoch 4587: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 13:50:53,529 EPOCH 4588
2024-02-07 13:51:09,826 Epoch 4588: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 13:51:09,826 EPOCH 4589
2024-02-07 13:51:25,864 [Epoch: 4589 Step: 00041300] Batch Recognition Loss:   0.001262 => Gls Tokens per Sec:      582 || Batch Translation Loss:   0.012154 => Txt Tokens per Sec:     1629 || Lr: 0.000100
2024-02-07 13:51:26,230 Epoch 4589: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-07 13:51:26,230 EPOCH 4590
2024-02-07 13:51:42,276 Epoch 4590: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 13:51:42,277 EPOCH 4591
2024-02-07 13:51:58,955 Epoch 4591: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 13:51:58,955 EPOCH 4592
2024-02-07 13:52:15,222 Epoch 4592: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 13:52:15,223 EPOCH 4593
2024-02-07 13:52:31,723 Epoch 4593: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 13:52:31,724 EPOCH 4594
2024-02-07 13:52:48,051 Epoch 4594: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 13:52:48,052 EPOCH 4595
2024-02-07 13:53:04,287 Epoch 4595: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 13:53:04,287 EPOCH 4596
2024-02-07 13:53:20,745 Epoch 4596: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 13:53:20,745 EPOCH 4597
2024-02-07 13:53:37,042 Epoch 4597: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 13:53:37,042 EPOCH 4598
2024-02-07 13:53:53,240 Epoch 4598: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 13:53:53,240 EPOCH 4599
2024-02-07 13:54:09,694 Epoch 4599: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 13:54:09,695 EPOCH 4600
2024-02-07 13:54:25,936 [Epoch: 4600 Step: 00041400] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:      654 || Batch Translation Loss:   0.017527 => Txt Tokens per Sec:     1809 || Lr: 0.000100
2024-02-07 13:54:25,937 Epoch 4600: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 13:54:25,937 EPOCH 4601
2024-02-07 13:54:42,890 Epoch 4601: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 13:54:42,891 EPOCH 4602
2024-02-07 13:54:59,181 Epoch 4602: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 13:54:59,182 EPOCH 4603
2024-02-07 13:55:15,527 Epoch 4603: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 13:55:15,527 EPOCH 4604
2024-02-07 13:55:32,008 Epoch 4604: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 13:55:32,009 EPOCH 4605
2024-02-07 13:55:48,443 Epoch 4605: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 13:55:48,444 EPOCH 4606
2024-02-07 13:56:04,960 Epoch 4606: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 13:56:04,961 EPOCH 4607
2024-02-07 13:56:21,564 Epoch 4607: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 13:56:21,565 EPOCH 4608
2024-02-07 13:56:37,947 Epoch 4608: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 13:56:37,948 EPOCH 4609
2024-02-07 13:56:54,383 Epoch 4609: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 13:56:54,384 EPOCH 4610
2024-02-07 13:57:10,478 Epoch 4610: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 13:57:10,479 EPOCH 4611
2024-02-07 13:57:26,731 Epoch 4611: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 13:57:26,732 EPOCH 4612
2024-02-07 13:57:27,101 [Epoch: 4612 Step: 00041500] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     3478 || Batch Translation Loss:   0.009939 => Txt Tokens per Sec:     8878 || Lr: 0.000100
2024-02-07 13:57:42,954 Epoch 4612: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 13:57:42,954 EPOCH 4613
2024-02-07 13:57:59,149 Epoch 4613: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 13:57:59,150 EPOCH 4614
2024-02-07 13:58:15,715 Epoch 4614: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 13:58:15,716 EPOCH 4615
2024-02-07 13:58:31,950 Epoch 4615: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 13:58:31,951 EPOCH 4616
2024-02-07 13:58:48,550 Epoch 4616: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 13:58:48,550 EPOCH 4617
2024-02-07 13:59:05,116 Epoch 4617: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 13:59:05,117 EPOCH 4618
2024-02-07 13:59:21,450 Epoch 4618: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 13:59:21,450 EPOCH 4619
2024-02-07 13:59:37,863 Epoch 4619: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 13:59:37,863 EPOCH 4620
2024-02-07 13:59:54,132 Epoch 4620: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 13:59:54,132 EPOCH 4621
2024-02-07 14:00:10,594 Epoch 4621: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:00:10,594 EPOCH 4622
2024-02-07 14:00:26,932 Epoch 4622: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 14:00:26,933 EPOCH 4623
2024-02-07 14:00:27,533 [Epoch: 4623 Step: 00041600] Batch Recognition Loss:   0.000113 => Gls Tokens per Sec:     4274 || Batch Translation Loss:   0.015064 => Txt Tokens per Sec:     9659 || Lr: 0.000100
2024-02-07 14:00:43,337 Epoch 4623: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:00:43,337 EPOCH 4624
2024-02-07 14:00:59,767 Epoch 4624: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 14:00:59,768 EPOCH 4625
2024-02-07 14:01:16,227 Epoch 4625: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 14:01:16,227 EPOCH 4626
2024-02-07 14:01:32,446 Epoch 4626: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:01:32,446 EPOCH 4627
2024-02-07 14:01:48,663 Epoch 4627: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:01:48,664 EPOCH 4628
2024-02-07 14:02:05,221 Epoch 4628: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:02:05,221 EPOCH 4629
2024-02-07 14:02:21,383 Epoch 4629: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:02:21,384 EPOCH 4630
2024-02-07 14:02:37,787 Epoch 4630: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:02:37,787 EPOCH 4631
2024-02-07 14:02:54,547 Epoch 4631: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:02:54,547 EPOCH 4632
2024-02-07 14:03:10,983 Epoch 4632: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:03:10,984 EPOCH 4633
2024-02-07 14:03:27,676 Epoch 4633: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 14:03:27,676 EPOCH 4634
2024-02-07 14:03:31,371 [Epoch: 4634 Step: 00041700] Batch Recognition Loss:   0.000871 => Gls Tokens per Sec:     1040 || Batch Translation Loss:   0.006912 => Txt Tokens per Sec:     2493 || Lr: 0.000100
2024-02-07 14:03:44,044 Epoch 4634: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 14:03:44,045 EPOCH 4635
2024-02-07 14:04:00,405 Epoch 4635: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 14:04:00,405 EPOCH 4636
2024-02-07 14:04:16,753 Epoch 4636: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 14:04:16,753 EPOCH 4637
2024-02-07 14:04:33,354 Epoch 4637: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-07 14:04:33,354 EPOCH 4638
2024-02-07 14:04:49,562 Epoch 4638: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-07 14:04:49,562 EPOCH 4639
2024-02-07 14:05:06,131 Epoch 4639: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-07 14:05:06,132 EPOCH 4640
2024-02-07 14:05:22,496 Epoch 4640: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-07 14:05:22,496 EPOCH 4641
2024-02-07 14:05:38,819 Epoch 4641: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-07 14:05:38,820 EPOCH 4642
2024-02-07 14:05:55,481 Epoch 4642: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-07 14:05:55,482 EPOCH 4643
2024-02-07 14:06:11,971 Epoch 4643: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-07 14:06:11,972 EPOCH 4644
2024-02-07 14:06:28,411 Epoch 4644: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 14:06:28,412 EPOCH 4645
2024-02-07 14:06:29,903 [Epoch: 4645 Step: 00041800] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     3437 || Batch Translation Loss:   0.018758 => Txt Tokens per Sec:     8023 || Lr: 0.000100
2024-02-07 14:06:44,807 Epoch 4645: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 14:06:44,807 EPOCH 4646
2024-02-07 14:07:00,906 Epoch 4646: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 14:07:00,907 EPOCH 4647
2024-02-07 14:07:17,304 Epoch 4647: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-07 14:07:17,304 EPOCH 4648
2024-02-07 14:07:33,508 Epoch 4648: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 14:07:33,509 EPOCH 4649
2024-02-07 14:07:50,041 Epoch 4649: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 14:07:50,042 EPOCH 4650
2024-02-07 14:08:06,784 Epoch 4650: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 14:08:06,785 EPOCH 4651
2024-02-07 14:08:23,036 Epoch 4651: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 14:08:23,037 EPOCH 4652
2024-02-07 14:08:39,626 Epoch 4652: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 14:08:39,626 EPOCH 4653
2024-02-07 14:08:55,829 Epoch 4653: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 14:08:55,830 EPOCH 4654
2024-02-07 14:09:12,218 Epoch 4654: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 14:09:12,219 EPOCH 4655
2024-02-07 14:09:28,839 Epoch 4655: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 14:09:28,840 EPOCH 4656
2024-02-07 14:09:43,355 [Epoch: 4656 Step: 00041900] Batch Recognition Loss:   0.000982 => Gls Tokens per Sec:      379 || Batch Translation Loss:   0.007096 => Txt Tokens per Sec:     1108 || Lr: 0.000100
2024-02-07 14:09:45,271 Epoch 4656: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 14:09:45,272 EPOCH 4657
2024-02-07 14:10:01,315 Epoch 4657: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 14:10:01,316 EPOCH 4658
2024-02-07 14:10:17,904 Epoch 4658: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 14:10:17,904 EPOCH 4659
2024-02-07 14:10:34,174 Epoch 4659: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 14:10:34,175 EPOCH 4660
2024-02-07 14:10:50,523 Epoch 4660: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 14:10:50,523 EPOCH 4661
2024-02-07 14:11:06,994 Epoch 4661: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 14:11:06,995 EPOCH 4662
2024-02-07 14:11:24,007 Epoch 4662: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 14:11:24,007 EPOCH 4663
2024-02-07 14:11:40,412 Epoch 4663: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 14:11:40,412 EPOCH 4664
2024-02-07 14:11:56,704 Epoch 4664: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 14:11:56,705 EPOCH 4665
2024-02-07 14:12:13,035 Epoch 4665: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 14:12:13,036 EPOCH 4666
2024-02-07 14:12:29,352 Epoch 4666: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 14:12:29,353 EPOCH 4667
2024-02-07 14:12:44,435 [Epoch: 4667 Step: 00042000] Batch Recognition Loss:   0.000442 => Gls Tokens per Sec:      450 || Batch Translation Loss:   0.027617 => Txt Tokens per Sec:     1336 || Lr: 0.000100
2024-02-07 14:13:52,079 Validation result at epoch 4667, step    42000: duration: 67.6434s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.29070	Translation Loss: 101333.43750	PPL: 24864.53125
	Eval Metric: BLEU
	WER 2.75	(DEL: 0.00,	INS: 0.00,	SUB: 2.75)
	BLEU-4 0.49	(BLEU-1: 10.64,	BLEU-2: 3.09,	BLEU-3: 1.10,	BLEU-4: 0.49)
	CHRF 17.13	ROUGE 8.86
2024-02-07 14:13:52,081 Logging Recognition and Translation Outputs
2024-02-07 14:13:52,082 ========================================================================================================================
2024-02-07 14:13:52,082 Logging Sequence: 154_94.00
2024-02-07 14:13:52,082 	Gloss Reference :	A B+C+D+E
2024-02-07 14:13:52,082 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 14:13:52,082 	Gloss Alignment :	         
2024-02-07 14:13:52,084 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 14:13:52,085 	Text Reference  :	the ipl will also be  held in     uae    from september 19  to october 15         
2024-02-07 14:13:52,085 	Text Hypothesis :	*** *** **** **** was then deepak chahar have could     not at the     coronavirus
2024-02-07 14:13:52,086 	Text Alignment  :	D   D   D    D    S   S    S      S      S    S         S   S  S       S          
2024-02-07 14:13:52,086 ========================================================================================================================
2024-02-07 14:13:52,086 Logging Sequence: 118_2.00
2024-02-07 14:13:52,086 	Gloss Reference :	A B+C+D+E
2024-02-07 14:13:52,086 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 14:13:52,086 	Gloss Alignment :	         
2024-02-07 14:13:52,086 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 14:13:52,087 	Text Reference  :	yesterday was a very exciting day people across the world were watching
2024-02-07 14:13:52,087 	Text Hypothesis :	********* *** * **** ******** *** ****** ****** the video went viral   
2024-02-07 14:13:52,088 	Text Alignment  :	D         D   D D    D        D   D      D          S     S    S       
2024-02-07 14:13:52,088 ========================================================================================================================
2024-02-07 14:13:52,088 Logging Sequence: 165_453.00
2024-02-07 14:13:52,088 	Gloss Reference :	A B+C+D+E
2024-02-07 14:13:52,088 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 14:13:52,088 	Gloss Alignment :	         
2024-02-07 14:13:52,089 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 14:13:52,090 	Text Reference  :	****** icc  did      not agree to    sehwag' decision of  wearing a  numberless jersey 
2024-02-07 14:13:52,091 	Text Hypothesis :	police have qualifed for the   match on      28th     may now     be all        matches
2024-02-07 14:13:52,091 	Text Alignment  :	I      S    S        S   S     S     S       S        S   S       S  S          S      
2024-02-07 14:13:52,091 ========================================================================================================================
2024-02-07 14:13:52,091 Logging Sequence: 126_163.00
2024-02-07 14:13:52,091 	Gloss Reference :	A B+C+D+E
2024-02-07 14:13:52,091 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 14:13:52,091 	Gloss Alignment :	         
2024-02-07 14:13:52,092 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 14:13:52,093 	Text Reference  :	* **** ****** your hard  work has  helped secure a  medal at    the tokyo olympics
2024-02-07 14:13:52,093 	Text Hypothesis :	i were unable to   train and  this is     i      am very  grate to  my    country 
2024-02-07 14:13:52,093 	Text Alignment  :	I I    I      S    S     S    S    S      S      S  S     S     S   S     S       
2024-02-07 14:13:52,093 ========================================================================================================================
2024-02-07 14:13:52,093 Logging Sequence: 84_2.00
2024-02-07 14:13:52,094 	Gloss Reference :	A B+C+D+E
2024-02-07 14:13:52,094 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 14:13:52,094 	Gloss Alignment :	         
2024-02-07 14:13:52,094 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 14:13:52,096 	Text Reference  :	the 2022 fifa football world cup is going on in qatar  from 20th  november 2022 to  18th december 2022
2024-02-07 14:13:52,096 	Text Hypothesis :	*** **** **** police   said  'i  am going ** to retire from virat kohli    with 211 away 8        days
2024-02-07 14:13:52,096 	Text Alignment  :	D   D    D    S        S     S   S        D  S  S           S     S        S    S   S    S        S   
2024-02-07 14:13:52,096 ========================================================================================================================
2024-02-07 14:13:53,643 Epoch 4667: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 14:13:53,643 EPOCH 4668
2024-02-07 14:14:10,800 Epoch 4668: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 14:14:10,801 EPOCH 4669
2024-02-07 14:14:27,322 Epoch 4669: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 14:14:27,322 EPOCH 4670
2024-02-07 14:14:43,611 Epoch 4670: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 14:14:43,612 EPOCH 4671
2024-02-07 14:15:00,000 Epoch 4671: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 14:15:00,000 EPOCH 4672
2024-02-07 14:15:16,337 Epoch 4672: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 14:15:16,337 EPOCH 4673
2024-02-07 14:15:32,648 Epoch 4673: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 14:15:32,649 EPOCH 4674
2024-02-07 14:15:48,889 Epoch 4674: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 14:15:48,890 EPOCH 4675
2024-02-07 14:16:05,099 Epoch 4675: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-07 14:16:05,099 EPOCH 4676
2024-02-07 14:16:21,499 Epoch 4676: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 14:16:21,500 EPOCH 4677
2024-02-07 14:16:38,062 Epoch 4677: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-07 14:16:38,062 EPOCH 4678
2024-02-07 14:16:51,197 [Epoch: 4678 Step: 00042100] Batch Recognition Loss:   0.000345 => Gls Tokens per Sec:      614 || Batch Translation Loss:   0.039865 => Txt Tokens per Sec:     1657 || Lr: 0.000100
2024-02-07 14:16:54,902 Epoch 4678: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-07 14:16:54,903 EPOCH 4679
2024-02-07 14:17:11,335 Epoch 4679: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-07 14:17:11,336 EPOCH 4680
2024-02-07 14:17:28,116 Epoch 4680: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-07 14:17:28,117 EPOCH 4681
2024-02-07 14:17:44,375 Epoch 4681: Total Training Recognition Loss 0.00  Total Training Translation Loss 2.01 
2024-02-07 14:17:44,375 EPOCH 4682
2024-02-07 14:18:01,008 Epoch 4682: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.49 
2024-02-07 14:18:01,009 EPOCH 4683
2024-02-07 14:18:17,306 Epoch 4683: Total Training Recognition Loss 0.05  Total Training Translation Loss 7.45 
2024-02-07 14:18:17,307 EPOCH 4684
2024-02-07 14:18:33,722 Epoch 4684: Total Training Recognition Loss 0.09  Total Training Translation Loss 7.31 
2024-02-07 14:18:33,722 EPOCH 4685
2024-02-07 14:18:50,048 Epoch 4685: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.92 
2024-02-07 14:18:50,049 EPOCH 4686
2024-02-07 14:19:06,298 Epoch 4686: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.18 
2024-02-07 14:19:06,298 EPOCH 4687
2024-02-07 14:19:22,934 Epoch 4687: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.24 
2024-02-07 14:19:22,935 EPOCH 4688
2024-02-07 14:19:39,475 Epoch 4688: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.91 
2024-02-07 14:19:39,475 EPOCH 4689
2024-02-07 14:19:55,305 [Epoch: 4689 Step: 00042200] Batch Recognition Loss:   0.001497 => Gls Tokens per Sec:      590 || Batch Translation Loss:   0.046246 => Txt Tokens per Sec:     1623 || Lr: 0.000100
2024-02-07 14:19:55,828 Epoch 4689: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-07 14:19:55,828 EPOCH 4690
2024-02-07 14:20:12,332 Epoch 4690: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-07 14:20:12,332 EPOCH 4691
2024-02-07 14:20:28,751 Epoch 4691: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-07 14:20:28,752 EPOCH 4692
2024-02-07 14:20:44,960 Epoch 4692: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-07 14:20:44,961 EPOCH 4693
2024-02-07 14:21:01,601 Epoch 4693: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-07 14:21:01,602 EPOCH 4694
2024-02-07 14:21:17,912 Epoch 4694: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 14:21:17,913 EPOCH 4695
2024-02-07 14:21:34,173 Epoch 4695: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 14:21:34,174 EPOCH 4696
2024-02-07 14:21:50,982 Epoch 4696: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 14:21:50,983 EPOCH 4697
2024-02-07 14:22:07,963 Epoch 4697: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 14:22:07,964 EPOCH 4698
2024-02-07 14:22:24,455 Epoch 4698: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 14:22:24,455 EPOCH 4699
2024-02-07 14:22:41,446 Epoch 4699: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-07 14:22:41,447 EPOCH 4700
2024-02-07 14:22:58,195 [Epoch: 4700 Step: 00042300] Batch Recognition Loss:   0.000719 => Gls Tokens per Sec:      634 || Batch Translation Loss:   0.051660 => Txt Tokens per Sec:     1754 || Lr: 0.000100
2024-02-07 14:22:58,195 Epoch 4700: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 14:22:58,195 EPOCH 4701
2024-02-07 14:23:14,598 Epoch 4701: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 14:23:14,599 EPOCH 4702
2024-02-07 14:23:31,151 Epoch 4702: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 14:23:31,152 EPOCH 4703
2024-02-07 14:23:47,754 Epoch 4703: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 14:23:47,755 EPOCH 4704
2024-02-07 14:24:04,032 Epoch 4704: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 14:24:04,033 EPOCH 4705
2024-02-07 14:24:20,283 Epoch 4705: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 14:24:20,284 EPOCH 4706
2024-02-07 14:24:36,547 Epoch 4706: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 14:24:36,548 EPOCH 4707
2024-02-07 14:24:53,083 Epoch 4707: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 14:24:53,084 EPOCH 4708
2024-02-07 14:25:09,434 Epoch 4708: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 14:25:09,435 EPOCH 4709
2024-02-07 14:25:26,254 Epoch 4709: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 14:25:26,255 EPOCH 4710
2024-02-07 14:25:42,661 Epoch 4710: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 14:25:42,662 EPOCH 4711
2024-02-07 14:25:59,065 Epoch 4711: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 14:25:59,066 EPOCH 4712
2024-02-07 14:25:59,798 [Epoch: 4712 Step: 00042400] Batch Recognition Loss:   0.000496 => Gls Tokens per Sec:     1752 || Batch Translation Loss:   0.019317 => Txt Tokens per Sec:     5279 || Lr: 0.000100
2024-02-07 14:26:15,327 Epoch 4712: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 14:26:15,328 EPOCH 4713
2024-02-07 14:26:32,034 Epoch 4713: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 14:26:32,035 EPOCH 4714
2024-02-07 14:26:48,411 Epoch 4714: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 14:26:48,411 EPOCH 4715
2024-02-07 14:27:04,575 Epoch 4715: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:27:04,576 EPOCH 4716
2024-02-07 14:27:21,023 Epoch 4716: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:27:21,023 EPOCH 4717
2024-02-07 14:27:37,715 Epoch 4717: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:27:37,715 EPOCH 4718
2024-02-07 14:27:54,004 Epoch 4718: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:27:54,004 EPOCH 4719
2024-02-07 14:28:10,631 Epoch 4719: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:28:10,631 EPOCH 4720
2024-02-07 14:28:26,952 Epoch 4720: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:28:26,952 EPOCH 4721
2024-02-07 14:28:42,996 Epoch 4721: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:28:42,997 EPOCH 4722
2024-02-07 14:28:59,400 Epoch 4722: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 14:28:59,401 EPOCH 4723
2024-02-07 14:29:03,183 [Epoch: 4723 Step: 00042500] Batch Recognition Loss:   0.000261 => Gls Tokens per Sec:      677 || Batch Translation Loss:   0.011027 => Txt Tokens per Sec:     2015 || Lr: 0.000100
2024-02-07 14:29:16,034 Epoch 4723: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 14:29:16,035 EPOCH 4724
2024-02-07 14:29:32,776 Epoch 4724: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:29:32,777 EPOCH 4725
2024-02-07 14:29:49,358 Epoch 4725: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 14:29:49,359 EPOCH 4726
2024-02-07 14:30:05,961 Epoch 4726: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 14:30:05,961 EPOCH 4727
2024-02-07 14:30:22,425 Epoch 4727: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 14:30:22,425 EPOCH 4728
2024-02-07 14:30:39,003 Epoch 4728: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 14:30:39,003 EPOCH 4729
2024-02-07 14:30:55,309 Epoch 4729: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 14:30:55,310 EPOCH 4730
2024-02-07 14:31:11,669 Epoch 4730: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 14:31:11,670 EPOCH 4731
2024-02-07 14:31:28,006 Epoch 4731: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 14:31:28,006 EPOCH 4732
2024-02-07 14:31:44,201 Epoch 4732: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 14:31:44,202 EPOCH 4733
2024-02-07 14:32:00,521 Epoch 4733: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 14:32:00,521 EPOCH 4734
2024-02-07 14:32:11,694 [Epoch: 4734 Step: 00042600] Batch Recognition Loss:   0.000352 => Gls Tokens per Sec:      263 || Batch Translation Loss:   0.019627 => Txt Tokens per Sec:      861 || Lr: 0.000100
2024-02-07 14:32:16,743 Epoch 4734: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 14:32:16,743 EPOCH 4735
2024-02-07 14:32:33,097 Epoch 4735: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 14:32:33,098 EPOCH 4736
2024-02-07 14:32:49,705 Epoch 4736: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 14:32:49,706 EPOCH 4737
2024-02-07 14:33:05,896 Epoch 4737: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 14:33:05,896 EPOCH 4738
2024-02-07 14:33:22,275 Epoch 4738: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 14:33:22,275 EPOCH 4739
2024-02-07 14:33:38,493 Epoch 4739: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 14:33:38,494 EPOCH 4740
2024-02-07 14:33:55,120 Epoch 4740: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 14:33:55,121 EPOCH 4741
2024-02-07 14:34:11,289 Epoch 4741: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 14:34:11,290 EPOCH 4742
2024-02-07 14:34:27,980 Epoch 4742: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 14:34:27,981 EPOCH 4743
2024-02-07 14:34:44,332 Epoch 4743: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 14:34:44,332 EPOCH 4744
2024-02-07 14:35:00,761 Epoch 4744: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 14:35:00,762 EPOCH 4745
2024-02-07 14:35:11,692 [Epoch: 4745 Step: 00042700] Batch Recognition Loss:   0.000296 => Gls Tokens per Sec:      386 || Batch Translation Loss:   0.016267 => Txt Tokens per Sec:     1037 || Lr: 0.000100
2024-02-07 14:35:16,973 Epoch 4745: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-07 14:35:16,973 EPOCH 4746
2024-02-07 14:35:33,331 Epoch 4746: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-07 14:35:33,331 EPOCH 4747
2024-02-07 14:35:49,685 Epoch 4747: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-07 14:35:49,686 EPOCH 4748
2024-02-07 14:36:05,983 Epoch 4748: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 14:36:05,984 EPOCH 4749
2024-02-07 14:36:22,183 Epoch 4749: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 14:36:22,183 EPOCH 4750
2024-02-07 14:36:38,669 Epoch 4750: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 14:36:38,670 EPOCH 4751
2024-02-07 14:36:55,327 Epoch 4751: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 14:36:55,327 EPOCH 4752
2024-02-07 14:37:11,758 Epoch 4752: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 14:37:11,758 EPOCH 4753
2024-02-07 14:37:28,091 Epoch 4753: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 14:37:28,092 EPOCH 4754
2024-02-07 14:37:44,353 Epoch 4754: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 14:37:44,354 EPOCH 4755
2024-02-07 14:38:01,152 Epoch 4755: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 14:38:01,152 EPOCH 4756
2024-02-07 14:38:08,714 [Epoch: 4756 Step: 00042800] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:      846 || Batch Translation Loss:   0.015843 => Txt Tokens per Sec:     2277 || Lr: 0.000100
2024-02-07 14:38:17,339 Epoch 4756: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 14:38:17,339 EPOCH 4757
2024-02-07 14:38:33,743 Epoch 4757: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 14:38:33,744 EPOCH 4758
2024-02-07 14:38:49,998 Epoch 4758: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 14:38:49,999 EPOCH 4759
2024-02-07 14:39:06,742 Epoch 4759: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 14:39:06,743 EPOCH 4760
2024-02-07 14:39:23,232 Epoch 4760: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 14:39:23,233 EPOCH 4761
2024-02-07 14:39:39,636 Epoch 4761: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 14:39:39,637 EPOCH 4762
2024-02-07 14:39:56,177 Epoch 4762: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:39:56,178 EPOCH 4763
2024-02-07 14:40:12,963 Epoch 4763: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:40:12,964 EPOCH 4764
2024-02-07 14:40:29,206 Epoch 4764: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 14:40:29,207 EPOCH 4765
2024-02-07 14:40:45,612 Epoch 4765: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 14:40:45,612 EPOCH 4766
2024-02-07 14:41:02,032 Epoch 4766: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 14:41:02,032 EPOCH 4767
2024-02-07 14:41:12,891 [Epoch: 4767 Step: 00042900] Batch Recognition Loss:   0.000328 => Gls Tokens per Sec:      707 || Batch Translation Loss:   0.031539 => Txt Tokens per Sec:     1900 || Lr: 0.000100
2024-02-07 14:41:18,518 Epoch 4767: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 14:41:18,518 EPOCH 4768
2024-02-07 14:41:34,799 Epoch 4768: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 14:41:34,799 EPOCH 4769
2024-02-07 14:41:51,506 Epoch 4769: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 14:41:51,507 EPOCH 4770
2024-02-07 14:42:07,811 Epoch 4770: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 14:42:07,812 EPOCH 4771
2024-02-07 14:42:23,865 Epoch 4771: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 14:42:23,865 EPOCH 4772
2024-02-07 14:42:40,550 Epoch 4772: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 14:42:40,550 EPOCH 4773
2024-02-07 14:42:56,818 Epoch 4773: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:42:56,818 EPOCH 4774
2024-02-07 14:43:13,011 Epoch 4774: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:43:13,012 EPOCH 4775
2024-02-07 14:43:29,384 Epoch 4775: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 14:43:29,384 EPOCH 4776
2024-02-07 14:43:45,979 Epoch 4776: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:43:45,979 EPOCH 4777
2024-02-07 14:44:02,230 Epoch 4777: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-07 14:44:02,230 EPOCH 4778
2024-02-07 14:44:14,834 [Epoch: 4778 Step: 00043000] Batch Recognition Loss:   0.000432 => Gls Tokens per Sec:      640 || Batch Translation Loss:   0.019707 => Txt Tokens per Sec:     1824 || Lr: 0.000100
2024-02-07 14:44:18,404 Epoch 4778: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:44:18,404 EPOCH 4779
2024-02-07 14:44:34,465 Epoch 4779: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 14:44:34,466 EPOCH 4780
2024-02-07 14:44:51,215 Epoch 4780: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:44:51,215 EPOCH 4781
2024-02-07 14:45:07,959 Epoch 4781: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:45:07,960 EPOCH 4782
2024-02-07 14:45:24,490 Epoch 4782: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 14:45:24,491 EPOCH 4783
2024-02-07 14:45:41,012 Epoch 4783: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:45:41,012 EPOCH 4784
2024-02-07 14:45:57,524 Epoch 4784: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:45:57,525 EPOCH 4785
2024-02-07 14:46:13,566 Epoch 4785: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:46:13,567 EPOCH 4786
2024-02-07 14:46:30,047 Epoch 4786: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:46:30,047 EPOCH 4787
2024-02-07 14:46:46,279 Epoch 4787: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:46:46,280 EPOCH 4788
2024-02-07 14:47:02,356 Epoch 4788: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 14:47:02,356 EPOCH 4789
2024-02-07 14:47:14,860 [Epoch: 4789 Step: 00043100] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:      819 || Batch Translation Loss:   0.012457 => Txt Tokens per Sec:     2241 || Lr: 0.000100
2024-02-07 14:47:19,172 Epoch 4789: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:47:19,173 EPOCH 4790
2024-02-07 14:47:35,647 Epoch 4790: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:47:35,648 EPOCH 4791
2024-02-07 14:47:52,189 Epoch 4791: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:47:52,190 EPOCH 4792
2024-02-07 14:48:08,329 Epoch 4792: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:48:08,330 EPOCH 4793
2024-02-07 14:48:24,840 Epoch 4793: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:48:24,841 EPOCH 4794
2024-02-07 14:48:41,473 Epoch 4794: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 14:48:41,474 EPOCH 4795
2024-02-07 14:48:57,485 Epoch 4795: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 14:48:57,486 EPOCH 4796
2024-02-07 14:49:13,959 Epoch 4796: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 14:49:13,960 EPOCH 4797
2024-02-07 14:49:30,444 Epoch 4797: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 14:49:30,444 EPOCH 4798
2024-02-07 14:49:46,926 Epoch 4798: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 14:49:46,926 EPOCH 4799
2024-02-07 14:50:03,314 Epoch 4799: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 14:50:03,315 EPOCH 4800
2024-02-07 14:50:19,455 [Epoch: 4800 Step: 00043200] Batch Recognition Loss:   0.000091 => Gls Tokens per Sec:      658 || Batch Translation Loss:   0.008484 => Txt Tokens per Sec:     1820 || Lr: 0.000100
2024-02-07 14:50:19,456 Epoch 4800: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 14:50:19,456 EPOCH 4801
2024-02-07 14:50:36,089 Epoch 4801: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 14:50:36,089 EPOCH 4802
2024-02-07 14:50:52,555 Epoch 4802: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 14:50:52,556 EPOCH 4803
2024-02-07 14:51:08,934 Epoch 4803: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 14:51:08,934 EPOCH 4804
2024-02-07 14:51:25,451 Epoch 4804: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 14:51:25,452 EPOCH 4805
2024-02-07 14:51:41,648 Epoch 4805: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 14:51:41,649 EPOCH 4806
2024-02-07 14:51:57,972 Epoch 4806: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 14:51:57,972 EPOCH 4807
2024-02-07 14:52:14,610 Epoch 4807: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-07 14:52:14,610 EPOCH 4808
2024-02-07 14:52:31,130 Epoch 4808: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-07 14:52:31,130 EPOCH 4809
2024-02-07 14:52:47,828 Epoch 4809: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-07 14:52:47,829 EPOCH 4810
2024-02-07 14:53:04,038 Epoch 4810: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-07 14:53:04,039 EPOCH 4811
2024-02-07 14:53:20,483 Epoch 4811: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-07 14:53:20,484 EPOCH 4812
2024-02-07 14:53:20,848 [Epoch: 4812 Step: 00043300] Batch Recognition Loss:   0.000657 => Gls Tokens per Sec:     3526 || Batch Translation Loss:   0.064325 => Txt Tokens per Sec:     9636 || Lr: 0.000100
2024-02-07 14:53:37,079 Epoch 4812: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-07 14:53:37,079 EPOCH 4813
2024-02-07 14:53:53,300 Epoch 4813: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-07 14:53:53,300 EPOCH 4814
2024-02-07 14:54:09,787 Epoch 4814: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.21 
2024-02-07 14:54:09,787 EPOCH 4815
2024-02-07 14:54:26,399 Epoch 4815: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.44 
2024-02-07 14:54:26,399 EPOCH 4816
2024-02-07 14:54:42,562 Epoch 4816: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.11 
2024-02-07 14:54:42,562 EPOCH 4817
2024-02-07 14:54:59,004 Epoch 4817: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-07 14:54:59,004 EPOCH 4818
2024-02-07 14:55:15,397 Epoch 4818: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-07 14:55:15,398 EPOCH 4819
2024-02-07 14:55:31,964 Epoch 4819: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-07 14:55:31,964 EPOCH 4820
2024-02-07 14:55:48,480 Epoch 4820: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-07 14:55:48,481 EPOCH 4821
2024-02-07 14:56:04,887 Epoch 4821: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-07 14:56:04,887 EPOCH 4822
2024-02-07 14:56:21,358 Epoch 4822: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-07 14:56:21,359 EPOCH 4823
2024-02-07 14:56:27,900 [Epoch: 4823 Step: 00043400] Batch Recognition Loss:   0.002054 => Gls Tokens per Sec:      391 || Batch Translation Loss:   0.044137 => Txt Tokens per Sec:     1213 || Lr: 0.000100
2024-02-07 14:56:37,739 Epoch 4823: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-07 14:56:37,739 EPOCH 4824
2024-02-07 14:56:54,132 Epoch 4824: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-07 14:56:54,133 EPOCH 4825
2024-02-07 14:57:10,523 Epoch 4825: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-07 14:57:10,524 EPOCH 4826
2024-02-07 14:57:26,913 Epoch 4826: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-07 14:57:26,914 EPOCH 4827
2024-02-07 14:57:43,340 Epoch 4827: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-07 14:57:43,341 EPOCH 4828
2024-02-07 14:57:59,727 Epoch 4828: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-07 14:57:59,728 EPOCH 4829
2024-02-07 14:58:16,105 Epoch 4829: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 14:58:16,105 EPOCH 4830
2024-02-07 14:58:32,508 Epoch 4830: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 14:58:32,509 EPOCH 4831
2024-02-07 14:58:48,743 Epoch 4831: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 14:58:48,744 EPOCH 4832
2024-02-07 14:59:05,460 Epoch 4832: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 14:59:05,461 EPOCH 4833
2024-02-07 14:59:22,042 Epoch 4833: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 14:59:22,043 EPOCH 4834
2024-02-07 14:59:32,960 [Epoch: 4834 Step: 00043500] Batch Recognition Loss:   0.000469 => Gls Tokens per Sec:      269 || Batch Translation Loss:   0.026932 => Txt Tokens per Sec:      865 || Lr: 0.000100
2024-02-07 14:59:38,413 Epoch 4834: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 14:59:38,414 EPOCH 4835
2024-02-07 14:59:54,965 Epoch 4835: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 14:59:54,965 EPOCH 4836
2024-02-07 15:00:11,149 Epoch 4836: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 15:00:11,150 EPOCH 4837
2024-02-07 15:00:27,727 Epoch 4837: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 15:00:27,728 EPOCH 4838
2024-02-07 15:00:44,023 Epoch 4838: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 15:00:44,024 EPOCH 4839
2024-02-07 15:01:00,210 Epoch 4839: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 15:01:00,210 EPOCH 4840
2024-02-07 15:01:16,816 Epoch 4840: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-07 15:01:16,817 EPOCH 4841
2024-02-07 15:01:33,366 Epoch 4841: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 15:01:33,367 EPOCH 4842
2024-02-07 15:01:49,795 Epoch 4842: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 15:01:49,795 EPOCH 4843
2024-02-07 15:02:05,890 Epoch 4843: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 15:02:05,891 EPOCH 4844
2024-02-07 15:02:22,165 Epoch 4844: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 15:02:22,165 EPOCH 4845
2024-02-07 15:02:26,462 [Epoch: 4845 Step: 00043600] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:     1192 || Batch Translation Loss:   0.014995 => Txt Tokens per Sec:     3293 || Lr: 0.000100
2024-02-07 15:02:38,436 Epoch 4845: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 15:02:38,436 EPOCH 4846
2024-02-07 15:02:55,222 Epoch 4846: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 15:02:55,222 EPOCH 4847
2024-02-07 15:03:11,647 Epoch 4847: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 15:03:11,648 EPOCH 4848
2024-02-07 15:03:27,807 Epoch 4848: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 15:03:27,807 EPOCH 4849
2024-02-07 15:03:44,212 Epoch 4849: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.17 
2024-02-07 15:03:44,213 EPOCH 4850
2024-02-07 15:04:00,628 Epoch 4850: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.14 
2024-02-07 15:04:00,629 EPOCH 4851
2024-02-07 15:04:17,313 Epoch 4851: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 15:04:17,314 EPOCH 4852
2024-02-07 15:04:33,851 Epoch 4852: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 15:04:33,852 EPOCH 4853
2024-02-07 15:04:50,055 Epoch 4853: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 15:04:50,055 EPOCH 4854
2024-02-07 15:05:06,212 Epoch 4854: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 15:05:06,213 EPOCH 4855
2024-02-07 15:05:22,622 Epoch 4855: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 15:05:22,623 EPOCH 4856
2024-02-07 15:05:37,130 [Epoch: 4856 Step: 00043700] Batch Recognition Loss:   0.005636 => Gls Tokens per Sec:      379 || Batch Translation Loss:   0.005891 => Txt Tokens per Sec:     1183 || Lr: 0.000100
2024-02-07 15:05:38,841 Epoch 4856: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 15:05:38,841 EPOCH 4857
2024-02-07 15:05:55,002 Epoch 4857: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 15:05:55,002 EPOCH 4858
2024-02-07 15:06:11,811 Epoch 4858: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 15:06:11,812 EPOCH 4859
2024-02-07 15:06:28,222 Epoch 4859: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 15:06:28,222 EPOCH 4860
2024-02-07 15:06:44,712 Epoch 4860: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 15:06:44,712 EPOCH 4861
2024-02-07 15:07:01,479 Epoch 4861: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 15:07:01,479 EPOCH 4862
2024-02-07 15:07:17,714 Epoch 4862: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-07 15:07:17,715 EPOCH 4863
2024-02-07 15:07:34,152 Epoch 4863: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 15:07:34,152 EPOCH 4864
2024-02-07 15:07:50,688 Epoch 4864: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 15:07:50,688 EPOCH 4865
2024-02-07 15:08:07,110 Epoch 4865: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 15:08:07,111 EPOCH 4866
2024-02-07 15:08:23,649 Epoch 4866: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 15:08:23,650 EPOCH 4867
2024-02-07 15:08:35,042 [Epoch: 4867 Step: 00043800] Batch Recognition Loss:   0.000504 => Gls Tokens per Sec:      674 || Batch Translation Loss:   0.031015 => Txt Tokens per Sec:     1931 || Lr: 0.000100
2024-02-07 15:08:40,356 Epoch 4867: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 15:08:40,357 EPOCH 4868
2024-02-07 15:08:56,856 Epoch 4868: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 15:08:56,857 EPOCH 4869
2024-02-07 15:09:13,104 Epoch 4869: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 15:09:13,105 EPOCH 4870
2024-02-07 15:09:29,665 Epoch 4870: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 15:09:29,666 EPOCH 4871
2024-02-07 15:09:46,038 Epoch 4871: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 15:09:46,038 EPOCH 4872
2024-02-07 15:10:02,334 Epoch 4872: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 15:10:02,334 EPOCH 4873
2024-02-07 15:10:18,501 Epoch 4873: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 15:10:18,502 EPOCH 4874
2024-02-07 15:10:34,726 Epoch 4874: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 15:10:34,726 EPOCH 4875
2024-02-07 15:10:51,281 Epoch 4875: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 15:10:51,281 EPOCH 4876
2024-02-07 15:11:07,670 Epoch 4876: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 15:11:07,671 EPOCH 4877
2024-02-07 15:11:24,026 Epoch 4877: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 15:11:24,028 EPOCH 4878
2024-02-07 15:11:35,480 [Epoch: 4878 Step: 00043900] Batch Recognition Loss:   0.000588 => Gls Tokens per Sec:      782 || Batch Translation Loss:   0.019886 => Txt Tokens per Sec:     2160 || Lr: 0.000100
2024-02-07 15:11:40,309 Epoch 4878: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 15:11:40,309 EPOCH 4879
2024-02-07 15:11:56,659 Epoch 4879: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 15:11:56,659 EPOCH 4880
2024-02-07 15:12:12,842 Epoch 4880: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 15:12:12,842 EPOCH 4881
2024-02-07 15:12:29,669 Epoch 4881: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 15:12:29,670 EPOCH 4882
2024-02-07 15:12:46,179 Epoch 4882: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 15:12:46,180 EPOCH 4883
2024-02-07 15:13:02,908 Epoch 4883: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 15:13:02,909 EPOCH 4884
2024-02-07 15:13:19,231 Epoch 4884: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 15:13:19,232 EPOCH 4885
2024-02-07 15:13:35,562 Epoch 4885: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 15:13:35,563 EPOCH 4886
2024-02-07 15:13:52,136 Epoch 4886: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 15:13:52,137 EPOCH 4887
2024-02-07 15:14:08,383 Epoch 4887: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 15:14:08,384 EPOCH 4888
2024-02-07 15:14:24,890 Epoch 4888: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 15:14:24,891 EPOCH 4889
2024-02-07 15:14:34,990 [Epoch: 4889 Step: 00044000] Batch Recognition Loss:   0.000383 => Gls Tokens per Sec:      925 || Batch Translation Loss:   0.006563 => Txt Tokens per Sec:     2472 || Lr: 0.000100
2024-02-07 15:15:42,908 Validation result at epoch 4889, step    44000: duration: 67.9173s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.30501	Translation Loss: 101155.42969	PPL: 24426.36523
	Eval Metric: BLEU
	WER 2.33	(DEL: 0.00,	INS: 0.00,	SUB: 2.33)
	BLEU-4 0.00	(BLEU-1: 9.53,	BLEU-2: 2.63,	BLEU-3: 0.81,	BLEU-4: 0.00)
	CHRF 16.40	ROUGE 8.13
2024-02-07 15:15:42,911 Logging Recognition and Translation Outputs
2024-02-07 15:15:42,911 ========================================================================================================================
2024-02-07 15:15:42,911 Logging Sequence: 57_104.00
2024-02-07 15:15:42,911 	Gloss Reference :	A B+C+D+E
2024-02-07 15:15:42,911 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 15:15:42,911 	Gloss Alignment :	         
2024-02-07 15:15:42,911 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 15:15:42,914 	Text Reference  :	the next day kohli and kl rahul continued from    where    they had left and displayed amazing batting performance without losing their wickets
2024-02-07 15:15:42,914 	Text Hypothesis :	*** **** *** ***** *** ** ***** ********* against pakistan won  the toss and ********* ******* ******* *********** ******* chose  to    bowl   
2024-02-07 15:15:42,914 	Text Alignment  :	D   D    D   D     D   D  D     D         S       S        S    S   S        D         D       D       D           D       S      S     S      
2024-02-07 15:15:42,914 ========================================================================================================================
2024-02-07 15:15:42,914 Logging Sequence: 136_64.00
2024-02-07 15:15:42,915 	Gloss Reference :	A B+C+D+E
2024-02-07 15:15:42,915 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 15:15:42,915 	Gloss Alignment :	         
2024-02-07 15:15:42,915 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 15:15:42,915 	Text Reference  :	in all  she has       won 2    medals
2024-02-07 15:15:42,916 	Text Hypothesis :	** this is  currently i   have proud 
2024-02-07 15:15:42,916 	Text Alignment  :	D  S    S   S         S   S    S     
2024-02-07 15:15:42,916 ========================================================================================================================
2024-02-07 15:15:42,916 Logging Sequence: 54_123.00
2024-02-07 15:15:42,916 	Gloss Reference :	A B+C+D+E
2024-02-07 15:15:42,917 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 15:15:42,917 	Gloss Alignment :	         
2024-02-07 15:15:42,917 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 15:15:42,918 	Text Reference  :	vips sponsors international cricket groups have    already booked their hotel rooms     
2024-02-07 15:15:42,918 	Text Hypothesis :	**** ******** people        are     not    believe in      the    same  my    surprising
2024-02-07 15:15:42,918 	Text Alignment  :	D    D        S             S       S      S       S       S      S     S     S         
2024-02-07 15:15:42,918 ========================================================================================================================
2024-02-07 15:15:42,918 Logging Sequence: 168_115.00
2024-02-07 15:15:42,918 	Gloss Reference :	A B+C+D+E
2024-02-07 15:15:42,919 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 15:15:42,919 	Gloss Alignment :	         
2024-02-07 15:15:42,919 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 15:15:42,919 	Text Reference  :	this has  sparked a   major discussion on  social media
2024-02-07 15:15:42,920 	Text Hypothesis :	**** were shocked the world cup        for the    team 
2024-02-07 15:15:42,920 	Text Alignment  :	D    S    S       S   S     S          S   S      S    
2024-02-07 15:15:42,920 ========================================================================================================================
2024-02-07 15:15:42,920 Logging Sequence: 121_132.00
2024-02-07 15:15:42,920 	Gloss Reference :	A B+C+D+E
2024-02-07 15:15:42,920 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 15:15:42,920 	Gloss Alignment :	         
2024-02-07 15:15:42,921 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 15:15:42,922 	Text Reference  :	which is why they will be  retesting her to   check if she consumed any  stamina enhancing drugs
2024-02-07 15:15:42,922 	Text Hypothesis :	***** ** *** **** **** the team      was very proud by hou has      been found   sushil    kumar
2024-02-07 15:15:42,922 	Text Alignment  :	D     D  D   D    D    S   S         S   S    S     S  S   S        S    S       S         S    
2024-02-07 15:15:42,922 ========================================================================================================================
2024-02-07 15:15:49,210 Epoch 4889: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 15:15:49,210 EPOCH 4890
2024-02-07 15:16:06,113 Epoch 4890: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 15:16:06,114 EPOCH 4891
2024-02-07 15:16:22,738 Epoch 4891: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 15:16:22,739 EPOCH 4892
2024-02-07 15:16:39,204 Epoch 4892: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 15:16:39,204 EPOCH 4893
2024-02-07 15:16:55,588 Epoch 4893: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 15:16:55,589 EPOCH 4894
2024-02-07 15:17:12,135 Epoch 4894: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 15:17:12,136 EPOCH 4895
2024-02-07 15:17:28,285 Epoch 4895: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 15:17:28,286 EPOCH 4896
2024-02-07 15:17:44,553 Epoch 4896: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 15:17:44,553 EPOCH 4897
2024-02-07 15:18:01,081 Epoch 4897: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 15:18:01,082 EPOCH 4898
2024-02-07 15:18:17,615 Epoch 4898: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 15:18:17,616 EPOCH 4899
2024-02-07 15:18:34,101 Epoch 4899: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 15:18:34,101 EPOCH 4900
2024-02-07 15:18:50,530 [Epoch: 4900 Step: 00044100] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:      647 || Batch Translation Loss:   0.070526 => Txt Tokens per Sec:     1789 || Lr: 0.000100
2024-02-07 15:18:50,530 Epoch 4900: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 15:18:50,530 EPOCH 4901
2024-02-07 15:19:07,038 Epoch 4901: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 15:19:07,039 EPOCH 4902
2024-02-07 15:19:23,488 Epoch 4902: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 15:19:23,488 EPOCH 4903
2024-02-07 15:19:39,878 Epoch 4903: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 15:19:39,879 EPOCH 4904
2024-02-07 15:19:56,464 Epoch 4904: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 15:19:56,464 EPOCH 4905
2024-02-07 15:20:12,974 Epoch 4905: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-07 15:20:12,975 EPOCH 4906
2024-02-07 15:20:29,459 Epoch 4906: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 15:20:29,459 EPOCH 4907
2024-02-07 15:20:45,543 Epoch 4907: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-07 15:20:45,544 EPOCH 4908
2024-02-07 15:21:01,871 Epoch 4908: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-07 15:21:01,872 EPOCH 4909
2024-02-07 15:21:18,057 Epoch 4909: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-07 15:21:18,057 EPOCH 4910
2024-02-07 15:21:34,242 Epoch 4910: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.97 
2024-02-07 15:21:34,242 EPOCH 4911
2024-02-07 15:21:50,475 Epoch 4911: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.28 
2024-02-07 15:21:50,475 EPOCH 4912
2024-02-07 15:21:51,134 [Epoch: 4912 Step: 00044200] Batch Recognition Loss:   0.001770 => Gls Tokens per Sec:     1945 || Batch Translation Loss:   0.165894 => Txt Tokens per Sec:     5666 || Lr: 0.000100
2024-02-07 15:22:07,267 Epoch 4912: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.53 
2024-02-07 15:22:07,267 EPOCH 4913
2024-02-07 15:22:23,797 Epoch 4913: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.24 
2024-02-07 15:22:23,798 EPOCH 4914
2024-02-07 15:22:40,152 Epoch 4914: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-07 15:22:40,153 EPOCH 4915
2024-02-07 15:22:56,676 Epoch 4915: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-07 15:22:56,677 EPOCH 4916
2024-02-07 15:23:13,344 Epoch 4916: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-07 15:23:13,345 EPOCH 4917
2024-02-07 15:23:29,459 Epoch 4917: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-07 15:23:29,461 EPOCH 4918
2024-02-07 15:23:45,795 Epoch 4918: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-07 15:23:45,796 EPOCH 4919
2024-02-07 15:24:02,127 Epoch 4919: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 15:24:02,128 EPOCH 4920
2024-02-07 15:24:18,364 Epoch 4920: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 15:24:18,364 EPOCH 4921
2024-02-07 15:24:34,793 Epoch 4921: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 15:24:34,794 EPOCH 4922
2024-02-07 15:24:51,169 Epoch 4922: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 15:24:51,170 EPOCH 4923
2024-02-07 15:24:57,678 [Epoch: 4923 Step: 00044300] Batch Recognition Loss:   0.000817 => Gls Tokens per Sec:      393 || Batch Translation Loss:   0.024053 => Txt Tokens per Sec:     1251 || Lr: 0.000100
2024-02-07 15:25:07,322 Epoch 4923: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 15:25:07,322 EPOCH 4924
2024-02-07 15:25:23,488 Epoch 4924: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 15:25:23,489 EPOCH 4925
2024-02-07 15:25:39,870 Epoch 4925: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 15:25:39,871 EPOCH 4926
2024-02-07 15:25:56,432 Epoch 4926: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 15:25:56,433 EPOCH 4927
2024-02-07 15:26:12,756 Epoch 4927: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 15:26:12,757 EPOCH 4928
2024-02-07 15:26:29,265 Epoch 4928: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 15:26:29,266 EPOCH 4929
2024-02-07 15:26:45,498 Epoch 4929: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 15:26:45,499 EPOCH 4930
2024-02-07 15:27:01,822 Epoch 4930: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 15:27:01,823 EPOCH 4931
2024-02-07 15:27:18,291 Epoch 4931: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 15:27:18,292 EPOCH 4932
2024-02-07 15:27:34,639 Epoch 4932: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 15:27:34,640 EPOCH 4933
2024-02-07 15:27:51,029 Epoch 4933: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 15:27:51,030 EPOCH 4934
2024-02-07 15:27:55,362 [Epoch: 4934 Step: 00044400] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:      887 || Batch Translation Loss:   0.009932 => Txt Tokens per Sec:     2459 || Lr: 0.000100
2024-02-07 15:28:07,677 Epoch 4934: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 15:28:07,678 EPOCH 4935
2024-02-07 15:28:23,949 Epoch 4935: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 15:28:23,950 EPOCH 4936
2024-02-07 15:28:40,044 Epoch 4936: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 15:28:40,045 EPOCH 4937
2024-02-07 15:28:56,073 Epoch 4937: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 15:28:56,073 EPOCH 4938
2024-02-07 15:29:12,667 Epoch 4938: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 15:29:12,667 EPOCH 4939
2024-02-07 15:29:28,893 Epoch 4939: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 15:29:28,894 EPOCH 4940
2024-02-07 15:29:44,921 Epoch 4940: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 15:29:44,922 EPOCH 4941
2024-02-07 15:30:01,718 Epoch 4941: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 15:30:01,718 EPOCH 4942
2024-02-07 15:30:18,028 Epoch 4942: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 15:30:18,029 EPOCH 4943
2024-02-07 15:30:34,635 Epoch 4943: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 15:30:34,636 EPOCH 4944
2024-02-07 15:30:51,631 Epoch 4944: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 15:30:51,632 EPOCH 4945
2024-02-07 15:30:57,345 [Epoch: 4945 Step: 00044500] Batch Recognition Loss:   0.000465 => Gls Tokens per Sec:      739 || Batch Translation Loss:   0.012957 => Txt Tokens per Sec:     2005 || Lr: 0.000100
2024-02-07 15:31:08,191 Epoch 4945: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 15:31:08,192 EPOCH 4946
2024-02-07 15:31:24,667 Epoch 4946: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 15:31:24,667 EPOCH 4947
2024-02-07 15:31:41,533 Epoch 4947: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 15:31:41,533 EPOCH 4948
2024-02-07 15:31:58,079 Epoch 4948: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 15:31:58,080 EPOCH 4949
2024-02-07 15:32:14,371 Epoch 4949: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 15:32:14,372 EPOCH 4950
2024-02-07 15:32:30,459 Epoch 4950: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 15:32:30,460 EPOCH 4951
2024-02-07 15:32:46,735 Epoch 4951: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 15:32:46,736 EPOCH 4952
2024-02-07 15:33:03,479 Epoch 4952: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 15:33:03,480 EPOCH 4953
2024-02-07 15:33:20,092 Epoch 4953: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 15:33:20,092 EPOCH 4954
2024-02-07 15:33:36,470 Epoch 4954: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 15:33:36,471 EPOCH 4955
2024-02-07 15:33:53,070 Epoch 4955: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 15:33:53,071 EPOCH 4956
2024-02-07 15:34:03,718 [Epoch: 4956 Step: 00044600] Batch Recognition Loss:   0.000192 => Gls Tokens per Sec:      601 || Batch Translation Loss:   0.012469 => Txt Tokens per Sec:     1669 || Lr: 0.000100
2024-02-07 15:34:09,532 Epoch 4956: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 15:34:09,532 EPOCH 4957
2024-02-07 15:34:25,628 Epoch 4957: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 15:34:25,628 EPOCH 4958
2024-02-07 15:34:42,259 Epoch 4958: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 15:34:42,260 EPOCH 4959
2024-02-07 15:34:58,454 Epoch 4959: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 15:34:58,455 EPOCH 4960
2024-02-07 15:35:14,789 Epoch 4960: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 15:35:14,789 EPOCH 4961
2024-02-07 15:35:31,110 Epoch 4961: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 15:35:31,111 EPOCH 4962
2024-02-07 15:35:47,251 Epoch 4962: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 15:35:47,252 EPOCH 4963
2024-02-07 15:36:03,868 Epoch 4963: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-07 15:36:03,869 EPOCH 4964
2024-02-07 15:36:20,529 Epoch 4964: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-07 15:36:20,529 EPOCH 4965
2024-02-07 15:36:36,983 Epoch 4965: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 15:36:36,984 EPOCH 4966
2024-02-07 15:36:53,688 Epoch 4966: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 15:36:53,688 EPOCH 4967
2024-02-07 15:37:01,974 [Epoch: 4967 Step: 00044700] Batch Recognition Loss:   0.000351 => Gls Tokens per Sec:      927 || Batch Translation Loss:   0.011829 => Txt Tokens per Sec:     2543 || Lr: 0.000100
2024-02-07 15:37:09,850 Epoch 4967: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 15:37:09,850 EPOCH 4968
2024-02-07 15:37:26,147 Epoch 4968: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 15:37:26,147 EPOCH 4969
2024-02-07 15:37:42,649 Epoch 4969: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 15:37:42,649 EPOCH 4970
2024-02-07 15:37:58,851 Epoch 4970: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 15:37:58,852 EPOCH 4971
2024-02-07 15:38:15,148 Epoch 4971: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 15:38:15,148 EPOCH 4972
2024-02-07 15:38:31,859 Epoch 4972: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 15:38:31,860 EPOCH 4973
2024-02-07 15:38:48,033 Epoch 4973: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 15:38:48,034 EPOCH 4974
2024-02-07 15:39:04,286 Epoch 4974: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 15:39:04,287 EPOCH 4975
2024-02-07 15:39:20,811 Epoch 4975: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 15:39:20,812 EPOCH 4976
2024-02-07 15:39:37,167 Epoch 4976: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 15:39:37,168 EPOCH 4977
2024-02-07 15:39:53,717 Epoch 4977: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 15:39:53,717 EPOCH 4978
2024-02-07 15:40:05,185 [Epoch: 4978 Step: 00044800] Batch Recognition Loss:   0.000180 => Gls Tokens per Sec:      781 || Batch Translation Loss:   0.007790 => Txt Tokens per Sec:     2107 || Lr: 0.000100
2024-02-07 15:40:10,214 Epoch 4978: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 15:40:10,214 EPOCH 4979
2024-02-07 15:40:26,689 Epoch 4979: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 15:40:26,690 EPOCH 4980
2024-02-07 15:40:43,188 Epoch 4980: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 15:40:43,188 EPOCH 4981
2024-02-07 15:40:59,489 Epoch 4981: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 15:40:59,489 EPOCH 4982
2024-02-07 15:41:16,109 Epoch 4982: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 15:41:16,109 EPOCH 4983
2024-02-07 15:41:32,404 Epoch 4983: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 15:41:32,405 EPOCH 4984
2024-02-07 15:41:48,785 Epoch 4984: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 15:41:48,785 EPOCH 4985
2024-02-07 15:42:05,152 Epoch 4985: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 15:42:05,152 EPOCH 4986
2024-02-07 15:42:21,741 Epoch 4986: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 15:42:21,742 EPOCH 4987
2024-02-07 15:42:37,941 Epoch 4987: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 15:42:37,941 EPOCH 4988
2024-02-07 15:42:54,041 Epoch 4988: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 15:42:54,041 EPOCH 4989
2024-02-07 15:43:09,931 [Epoch: 4989 Step: 00044900] Batch Recognition Loss:   0.000320 => Gls Tokens per Sec:      588 || Batch Translation Loss:   0.013771 => Txt Tokens per Sec:     1606 || Lr: 0.000100
2024-02-07 15:43:10,650 Epoch 4989: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 15:43:10,650 EPOCH 4990
2024-02-07 15:43:26,798 Epoch 4990: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 15:43:26,799 EPOCH 4991
2024-02-07 15:43:42,952 Epoch 4991: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 15:43:42,952 EPOCH 4992
2024-02-07 15:43:59,346 Epoch 4992: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 15:43:59,347 EPOCH 4993
2024-02-07 15:44:15,589 Epoch 4993: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 15:44:15,590 EPOCH 4994
2024-02-07 15:44:31,871 Epoch 4994: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 15:44:31,872 EPOCH 4995
2024-02-07 15:44:48,549 Epoch 4995: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 15:44:48,549 EPOCH 4996
2024-02-07 15:45:05,089 Epoch 4996: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 15:45:05,090 EPOCH 4997
2024-02-07 15:45:21,540 Epoch 4997: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 15:45:21,541 EPOCH 4998
2024-02-07 15:45:37,837 Epoch 4998: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-07 15:45:37,837 EPOCH 4999
2024-02-07 15:45:54,083 Epoch 4999: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 15:45:54,084 EPOCH 5000
2024-02-07 15:46:10,794 [Epoch: 5000 Step: 00045000] Batch Recognition Loss:   0.000748 => Gls Tokens per Sec:      636 || Batch Translation Loss:   0.007712 => Txt Tokens per Sec:     1758 || Lr: 0.000100
2024-02-07 15:46:10,795 Epoch 5000: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 15:46:10,795 EPOCH 5001
2024-02-07 15:46:27,377 Epoch 5001: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 15:46:27,378 EPOCH 5002
2024-02-07 15:46:43,860 Epoch 5002: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 15:46:43,861 EPOCH 5003
2024-02-07 15:47:00,311 Epoch 5003: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 15:47:00,311 EPOCH 5004
2024-02-07 15:47:17,030 Epoch 5004: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 15:47:17,031 EPOCH 5005
2024-02-07 15:47:33,791 Epoch 5005: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 15:47:33,791 EPOCH 5006
2024-02-07 15:47:50,480 Epoch 5006: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 15:47:50,480 EPOCH 5007
2024-02-07 15:48:07,082 Epoch 5007: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 15:48:07,082 EPOCH 5008
2024-02-07 15:48:23,272 Epoch 5008: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-07 15:48:23,273 EPOCH 5009
2024-02-07 15:48:39,559 Epoch 5009: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.74 
2024-02-07 15:48:39,560 EPOCH 5010
2024-02-07 15:48:56,405 Epoch 5010: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.91 
2024-02-07 15:48:56,405 EPOCH 5011
2024-02-07 15:49:13,244 Epoch 5011: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.85 
2024-02-07 15:49:13,244 EPOCH 5012
2024-02-07 15:49:13,461 [Epoch: 5012 Step: 00045100] Batch Recognition Loss:   0.001052 => Gls Tokens per Sec:     5953 || Batch Translation Loss:   0.061502 => Txt Tokens per Sec:    10762 || Lr: 0.000100
2024-02-07 15:49:29,388 Epoch 5012: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.71 
2024-02-07 15:49:29,389 EPOCH 5013
2024-02-07 15:49:46,011 Epoch 5013: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.38 
2024-02-07 15:49:46,012 EPOCH 5014
2024-02-07 15:50:02,455 Epoch 5014: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.19 
2024-02-07 15:50:02,456 EPOCH 5015
2024-02-07 15:50:18,724 Epoch 5015: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.06 
2024-02-07 15:50:18,724 EPOCH 5016
2024-02-07 15:50:35,255 Epoch 5016: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.72 
2024-02-07 15:50:35,256 EPOCH 5017
2024-02-07 15:50:52,121 Epoch 5017: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.74 
2024-02-07 15:50:52,122 EPOCH 5018
2024-02-07 15:51:08,997 Epoch 5018: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.14 
2024-02-07 15:51:08,997 EPOCH 5019
2024-02-07 15:51:25,088 Epoch 5019: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.11 
2024-02-07 15:51:25,090 EPOCH 5020
2024-02-07 15:51:41,592 Epoch 5020: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.80 
2024-02-07 15:51:41,592 EPOCH 5021
2024-02-07 15:51:58,086 Epoch 5021: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-07 15:51:58,087 EPOCH 5022
2024-02-07 15:52:14,640 Epoch 5022: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-07 15:52:14,641 EPOCH 5023
2024-02-07 15:52:15,508 [Epoch: 5023 Step: 00045200] Batch Recognition Loss:   0.001075 => Gls Tokens per Sec:     2956 || Batch Translation Loss:   0.032560 => Txt Tokens per Sec:     7518 || Lr: 0.000100
2024-02-07 15:52:30,923 Epoch 5023: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-07 15:52:30,924 EPOCH 5024
2024-02-07 15:52:47,404 Epoch 5024: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 15:52:47,405 EPOCH 5025
2024-02-07 15:53:03,743 Epoch 5025: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 15:53:03,743 EPOCH 5026
2024-02-07 15:53:19,911 Epoch 5026: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 15:53:19,911 EPOCH 5027
2024-02-07 15:53:36,415 Epoch 5027: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 15:53:36,415 EPOCH 5028
2024-02-07 15:53:52,683 Epoch 5028: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 15:53:52,684 EPOCH 5029
2024-02-07 15:54:09,105 Epoch 5029: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 15:54:09,106 EPOCH 5030
2024-02-07 15:54:25,561 Epoch 5030: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 15:54:25,561 EPOCH 5031
2024-02-07 15:54:41,424 Epoch 5031: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 15:54:41,425 EPOCH 5032
2024-02-07 15:54:58,247 Epoch 5032: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 15:54:58,248 EPOCH 5033
2024-02-07 15:55:15,018 Epoch 5033: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 15:55:15,018 EPOCH 5034
2024-02-07 15:55:16,126 [Epoch: 5034 Step: 00045300] Batch Recognition Loss:   0.000238 => Gls Tokens per Sec:     3472 || Batch Translation Loss:   0.016425 => Txt Tokens per Sec:     8435 || Lr: 0.000100
2024-02-07 15:55:31,311 Epoch 5034: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 15:55:31,311 EPOCH 5035
2024-02-07 15:55:47,619 Epoch 5035: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 15:55:47,620 EPOCH 5036
2024-02-07 15:56:03,936 Epoch 5036: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 15:56:03,936 EPOCH 5037
2024-02-07 15:56:20,402 Epoch 5037: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 15:56:20,403 EPOCH 5038
2024-02-07 15:56:36,765 Epoch 5038: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 15:56:36,765 EPOCH 5039
2024-02-07 15:56:53,257 Epoch 5039: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 15:56:53,258 EPOCH 5040
2024-02-07 15:57:09,718 Epoch 5040: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 15:57:09,719 EPOCH 5041
2024-02-07 15:57:25,914 Epoch 5041: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 15:57:25,915 EPOCH 5042
2024-02-07 15:57:42,771 Epoch 5042: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 15:57:42,772 EPOCH 5043
2024-02-07 15:57:59,405 Epoch 5043: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 15:57:59,405 EPOCH 5044
2024-02-07 15:58:15,770 Epoch 5044: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 15:58:15,770 EPOCH 5045
2024-02-07 15:58:26,064 [Epoch: 5045 Step: 00045400] Batch Recognition Loss:   0.000230 => Gls Tokens per Sec:      497 || Batch Translation Loss:   0.009965 => Txt Tokens per Sec:     1464 || Lr: 0.000100
2024-02-07 15:58:32,279 Epoch 5045: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 15:58:32,279 EPOCH 5046
2024-02-07 15:58:48,402 Epoch 5046: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 15:58:48,402 EPOCH 5047
2024-02-07 15:59:04,780 Epoch 5047: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 15:59:04,781 EPOCH 5048
2024-02-07 15:59:21,042 Epoch 5048: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 15:59:21,043 EPOCH 5049
2024-02-07 15:59:37,185 Epoch 5049: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 15:59:37,186 EPOCH 5050
2024-02-07 15:59:53,686 Epoch 5050: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 15:59:53,687 EPOCH 5051
2024-02-07 16:00:10,582 Epoch 5051: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 16:00:10,583 EPOCH 5052
2024-02-07 16:00:27,169 Epoch 5052: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:00:27,170 EPOCH 5053
2024-02-07 16:00:44,273 Epoch 5053: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:00:44,273 EPOCH 5054
2024-02-07 16:01:01,123 Epoch 5054: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:01:01,125 EPOCH 5055
2024-02-07 16:01:18,229 Epoch 5055: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:01:18,230 EPOCH 5056
2024-02-07 16:01:27,481 [Epoch: 5056 Step: 00045500] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:      595 || Batch Translation Loss:   0.013280 => Txt Tokens per Sec:     1743 || Lr: 0.000100
2024-02-07 16:01:35,103 Epoch 5056: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:01:35,104 EPOCH 5057
2024-02-07 16:01:51,785 Epoch 5057: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:01:51,786 EPOCH 5058
2024-02-07 16:02:08,677 Epoch 5058: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:02:08,678 EPOCH 5059
2024-02-07 16:02:25,140 Epoch 5059: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:02:25,140 EPOCH 5060
2024-02-07 16:02:41,750 Epoch 5060: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:02:41,751 EPOCH 5061
2024-02-07 16:02:58,272 Epoch 5061: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:02:58,273 EPOCH 5062
2024-02-07 16:03:14,910 Epoch 5062: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:03:14,912 EPOCH 5063
2024-02-07 16:03:31,476 Epoch 5063: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:03:31,478 EPOCH 5064
2024-02-07 16:03:48,199 Epoch 5064: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:03:48,200 EPOCH 5065
2024-02-07 16:04:04,775 Epoch 5065: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 16:04:04,775 EPOCH 5066
2024-02-07 16:04:21,299 Epoch 5066: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 16:04:21,300 EPOCH 5067
2024-02-07 16:04:32,653 [Epoch: 5067 Step: 00045600] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:      677 || Batch Translation Loss:   0.012780 => Txt Tokens per Sec:     1850 || Lr: 0.000100
2024-02-07 16:04:38,262 Epoch 5067: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:04:38,262 EPOCH 5068
2024-02-07 16:04:54,994 Epoch 5068: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 16:04:54,995 EPOCH 5069
2024-02-07 16:05:11,652 Epoch 5069: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 16:05:11,653 EPOCH 5070
2024-02-07 16:05:28,268 Epoch 5070: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 16:05:28,268 EPOCH 5071
2024-02-07 16:05:44,677 Epoch 5071: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 16:05:44,679 EPOCH 5072
2024-02-07 16:06:01,444 Epoch 5072: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 16:06:01,445 EPOCH 5073
2024-02-07 16:06:17,851 Epoch 5073: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 16:06:17,851 EPOCH 5074
2024-02-07 16:06:34,575 Epoch 5074: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:06:34,577 EPOCH 5075
2024-02-07 16:06:50,903 Epoch 5075: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 16:06:50,905 EPOCH 5076
2024-02-07 16:07:07,934 Epoch 5076: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:07:07,935 EPOCH 5077
2024-02-07 16:07:24,095 Epoch 5077: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:07:24,096 EPOCH 5078
2024-02-07 16:07:35,927 [Epoch: 5078 Step: 00045700] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:      757 || Batch Translation Loss:   0.016796 => Txt Tokens per Sec:     2126 || Lr: 0.000100
2024-02-07 16:07:40,654 Epoch 5078: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:07:40,655 EPOCH 5079
2024-02-07 16:07:56,930 Epoch 5079: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:07:56,930 EPOCH 5080
2024-02-07 16:08:13,328 Epoch 5080: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:08:13,330 EPOCH 5081
2024-02-07 16:08:30,050 Epoch 5081: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:08:30,050 EPOCH 5082
2024-02-07 16:08:46,842 Epoch 5082: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:08:46,843 EPOCH 5083
2024-02-07 16:09:03,342 Epoch 5083: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:09:03,342 EPOCH 5084
2024-02-07 16:09:20,275 Epoch 5084: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:09:20,277 EPOCH 5085
2024-02-07 16:09:36,785 Epoch 5085: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 16:09:36,787 EPOCH 5086
2024-02-07 16:09:53,717 Epoch 5086: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:09:53,719 EPOCH 5087
2024-02-07 16:10:10,144 Epoch 5087: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 16:10:10,145 EPOCH 5088
2024-02-07 16:10:26,482 Epoch 5088: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:10:26,483 EPOCH 5089
2024-02-07 16:10:42,966 [Epoch: 5089 Step: 00045800] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:      567 || Batch Translation Loss:   0.007874 => Txt Tokens per Sec:     1568 || Lr: 0.000100
2024-02-07 16:10:43,513 Epoch 5089: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 16:10:43,513 EPOCH 5090
2024-02-07 16:11:00,016 Epoch 5090: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:11:00,018 EPOCH 5091
2024-02-07 16:11:16,789 Epoch 5091: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:11:16,790 EPOCH 5092
2024-02-07 16:11:33,357 Epoch 5092: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:11:33,357 EPOCH 5093
2024-02-07 16:11:49,805 Epoch 5093: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 16:11:49,806 EPOCH 5094
2024-02-07 16:12:06,401 Epoch 5094: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 16:12:06,402 EPOCH 5095
2024-02-07 16:12:22,808 Epoch 5095: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 16:12:22,810 EPOCH 5096
2024-02-07 16:12:39,761 Epoch 5096: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 16:12:39,763 EPOCH 5097
2024-02-07 16:12:55,985 Epoch 5097: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-07 16:12:55,987 EPOCH 5098
2024-02-07 16:13:12,842 Epoch 5098: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-07 16:13:12,842 EPOCH 5099
2024-02-07 16:13:29,447 Epoch 5099: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 16:13:29,448 EPOCH 5100
2024-02-07 16:13:45,713 [Epoch: 5100 Step: 00045900] Batch Recognition Loss:   0.000573 => Gls Tokens per Sec:      653 || Batch Translation Loss:   0.026981 => Txt Tokens per Sec:     1807 || Lr: 0.000100
2024-02-07 16:13:45,715 Epoch 5100: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 16:13:45,715 EPOCH 5101
2024-02-07 16:14:02,349 Epoch 5101: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 16:14:02,349 EPOCH 5102
2024-02-07 16:14:18,757 Epoch 5102: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 16:14:18,758 EPOCH 5103
2024-02-07 16:14:35,555 Epoch 5103: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-07 16:14:35,557 EPOCH 5104
2024-02-07 16:14:52,496 Epoch 5104: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-07 16:14:52,497 EPOCH 5105
2024-02-07 16:15:09,125 Epoch 5105: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.07 
2024-02-07 16:15:09,126 EPOCH 5106
2024-02-07 16:15:26,074 Epoch 5106: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-07 16:15:26,075 EPOCH 5107
2024-02-07 16:15:42,579 Epoch 5107: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-07 16:15:42,581 EPOCH 5108
2024-02-07 16:15:59,021 Epoch 5108: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-07 16:15:59,021 EPOCH 5109
2024-02-07 16:16:15,745 Epoch 5109: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-07 16:16:15,746 EPOCH 5110
2024-02-07 16:16:32,210 Epoch 5110: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-07 16:16:32,211 EPOCH 5111
2024-02-07 16:16:48,561 Epoch 5111: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-07 16:16:48,563 EPOCH 5112
2024-02-07 16:16:49,187 [Epoch: 5112 Step: 00046000] Batch Recognition Loss:   0.000793 => Gls Tokens per Sec:     2055 || Batch Translation Loss:   0.025205 => Txt Tokens per Sec:     5973 || Lr: 0.000100
2024-02-07 16:17:57,306 Validation result at epoch 5112, step    46000: duration: 68.1184s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.25114	Translation Loss: 101445.79688	PPL: 25145.15625
	Eval Metric: BLEU
	WER 2.47	(DEL: 0.00,	INS: 0.00,	SUB: 2.47)
	BLEU-4 0.28	(BLEU-1: 10.20,	BLEU-2: 2.58,	BLEU-3: 0.79,	BLEU-4: 0.28)
	CHRF 17.20	ROUGE 8.61
2024-02-07 16:17:57,308 Logging Recognition and Translation Outputs
2024-02-07 16:17:57,308 ========================================================================================================================
2024-02-07 16:17:57,308 Logging Sequence: 87_207.00
2024-02-07 16:17:57,308 	Gloss Reference :	A B+C+D+E
2024-02-07 16:17:57,308 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 16:17:57,309 	Gloss Alignment :	         
2024-02-07 16:17:57,309 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 16:17:57,311 	Text Reference  :	**** ******** ***** **** ** **** ***** *** there     were 2-3 pakistanis who were speaking anti-india things and things on  kashmir
2024-02-07 16:17:57,311 	Text Hypothesis :	this amrapali group paid rs 3570 crore the remaining rs   652 crore      was paid a        close      to     do  with   the video  
2024-02-07 16:17:57,312 	Text Alignment  :	I    I        I     I    I  I    I     I   S         S    S   S          S   S    S        S          S      S   S      S   S      
2024-02-07 16:17:57,312 ========================================================================================================================
2024-02-07 16:17:57,312 Logging Sequence: 67_73.00
2024-02-07 16:17:57,312 	Gloss Reference :	A B+C+D+E
2024-02-07 16:17:57,312 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 16:17:57,312 	Gloss Alignment :	         
2024-02-07 16:17:57,313 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 16:17:57,313 	Text Reference  :	**** ******* ******* in    his     tweet he  also  said       
2024-02-07 16:17:57,313 	Text Hypothesis :	with india's amazing moves carlsen won   the first tie-breaker
2024-02-07 16:17:57,314 	Text Alignment  :	I    I       I       S     S       S     S   S     S          
2024-02-07 16:17:57,314 ========================================================================================================================
2024-02-07 16:17:57,314 Logging Sequence: 172_267.00
2024-02-07 16:17:57,314 	Gloss Reference :	A B+C+D+E
2024-02-07 16:17:57,314 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 16:17:57,315 	Gloss Alignment :	         
2024-02-07 16:17:57,315 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 16:17:57,315 	Text Reference  :	*** such provisions have been made
2024-02-07 16:17:57,315 	Text Hypothesis :	let me   tell       you  all  it  
2024-02-07 16:17:57,316 	Text Alignment  :	I   S    S          S    S    S   
2024-02-07 16:17:57,316 ========================================================================================================================
2024-02-07 16:17:57,316 Logging Sequence: 144_23.00
2024-02-07 16:17:57,316 	Gloss Reference :	A B+C+D+E
2024-02-07 16:17:57,316 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 16:17:57,317 	Gloss Alignment :	         
2024-02-07 16:17:57,317 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 16:17:57,318 	Text Reference  :	the   girl  is   14-year-old mumal mehar and    she   is      from    kanasar village of barmer in rajasthan  
2024-02-07 16:17:57,318 	Text Hypothesis :	since women this was         known as    sushil kumar urvashi rautela the     target  of ****** ** recognition
2024-02-07 16:17:57,319 	Text Alignment  :	S     S     S    S           S     S     S      S     S       S       S       S          D      D  S          
2024-02-07 16:17:57,319 ========================================================================================================================
2024-02-07 16:17:57,319 Logging Sequence: 133_202.00
2024-02-07 16:17:57,319 	Gloss Reference :	A B+C+D+E
2024-02-07 16:17:57,319 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 16:17:57,319 	Gloss Alignment :	         
2024-02-07 16:17:57,319 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 16:17:57,320 	Text Reference  :	********* australia has  already qualified for the final if india wins  it  will face australia
2024-02-07 16:17:57,321 	Text Hypothesis :	pakistani messi     kept scoring run       for *** ***** ** ***** angry out they felt oppressed
2024-02-07 16:17:57,321 	Text Alignment  :	I         S         S    S       S             D   D     D  D     S     S   S    S    S        
2024-02-07 16:17:57,321 ========================================================================================================================
2024-02-07 16:18:13,698 Epoch 5112: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-07 16:18:13,699 EPOCH 5113
2024-02-07 16:18:29,483 Epoch 5113: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-07 16:18:29,483 EPOCH 5114
2024-02-07 16:18:46,017 Epoch 5114: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-07 16:18:46,018 EPOCH 5115
2024-02-07 16:19:02,570 Epoch 5115: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 16:19:02,571 EPOCH 5116
2024-02-07 16:19:18,873 Epoch 5116: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 16:19:18,874 EPOCH 5117
2024-02-07 16:19:35,575 Epoch 5117: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 16:19:35,576 EPOCH 5118
2024-02-07 16:19:52,066 Epoch 5118: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 16:19:52,067 EPOCH 5119
2024-02-07 16:20:08,222 Epoch 5119: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 16:20:08,222 EPOCH 5120
2024-02-07 16:20:24,660 Epoch 5120: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 16:20:24,660 EPOCH 5121
2024-02-07 16:20:41,196 Epoch 5121: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 16:20:41,196 EPOCH 5122
2024-02-07 16:20:57,420 Epoch 5122: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 16:20:57,421 EPOCH 5123
2024-02-07 16:21:00,968 [Epoch: 5123 Step: 00046100] Batch Recognition Loss:   0.000312 => Gls Tokens per Sec:      722 || Batch Translation Loss:   0.016822 => Txt Tokens per Sec:     2152 || Lr: 0.000100
2024-02-07 16:21:13,983 Epoch 5123: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 16:21:13,983 EPOCH 5124
2024-02-07 16:21:30,405 Epoch 5124: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 16:21:30,405 EPOCH 5125
2024-02-07 16:21:46,700 Epoch 5125: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 16:21:46,700 EPOCH 5126
2024-02-07 16:22:03,515 Epoch 5126: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 16:22:03,516 EPOCH 5127
2024-02-07 16:22:20,790 Epoch 5127: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 16:22:20,792 EPOCH 5128
2024-02-07 16:22:37,103 Epoch 5128: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 16:22:37,104 EPOCH 5129
2024-02-07 16:22:53,794 Epoch 5129: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 16:22:53,795 EPOCH 5130
2024-02-07 16:23:10,333 Epoch 5130: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 16:23:10,334 EPOCH 5131
2024-02-07 16:23:26,606 Epoch 5131: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 16:23:26,606 EPOCH 5132
2024-02-07 16:23:43,027 Epoch 5132: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 16:23:43,027 EPOCH 5133
2024-02-07 16:23:59,417 Epoch 5133: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 16:23:59,418 EPOCH 5134
2024-02-07 16:24:04,759 [Epoch: 5134 Step: 00046200] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:      551 || Batch Translation Loss:   0.017224 => Txt Tokens per Sec:     1550 || Lr: 0.000100
2024-02-07 16:24:15,671 Epoch 5134: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 16:24:15,671 EPOCH 5135
2024-02-07 16:24:32,210 Epoch 5135: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 16:24:32,210 EPOCH 5136
2024-02-07 16:24:48,221 Epoch 5136: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 16:24:48,222 EPOCH 5137
2024-02-07 16:25:04,923 Epoch 5137: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 16:25:04,924 EPOCH 5138
2024-02-07 16:25:21,244 Epoch 5138: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:25:21,245 EPOCH 5139
2024-02-07 16:25:37,748 Epoch 5139: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 16:25:37,748 EPOCH 5140
2024-02-07 16:25:54,201 Epoch 5140: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 16:25:54,201 EPOCH 5141
2024-02-07 16:26:10,652 Epoch 5141: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 16:26:10,652 EPOCH 5142
2024-02-07 16:26:27,051 Epoch 5142: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 16:26:27,052 EPOCH 5143
2024-02-07 16:26:43,608 Epoch 5143: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 16:26:43,609 EPOCH 5144
2024-02-07 16:26:59,860 Epoch 5144: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:26:59,860 EPOCH 5145
2024-02-07 16:27:05,229 [Epoch: 5145 Step: 00046300] Batch Recognition Loss:   0.000125 => Gls Tokens per Sec:      786 || Batch Translation Loss:   0.007682 => Txt Tokens per Sec:     1822 || Lr: 0.000100
2024-02-07 16:27:16,555 Epoch 5145: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:27:16,555 EPOCH 5146
2024-02-07 16:27:32,948 Epoch 5146: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 16:27:32,949 EPOCH 5147
2024-02-07 16:27:49,100 Epoch 5147: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 16:27:49,101 EPOCH 5148
2024-02-07 16:28:05,370 Epoch 5148: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 16:28:05,370 EPOCH 5149
2024-02-07 16:28:21,908 Epoch 5149: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 16:28:21,909 EPOCH 5150
2024-02-07 16:28:38,282 Epoch 5150: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 16:28:38,283 EPOCH 5151
2024-02-07 16:28:54,597 Epoch 5151: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 16:28:54,598 EPOCH 5152
2024-02-07 16:29:11,100 Epoch 5152: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-07 16:29:11,100 EPOCH 5153
2024-02-07 16:29:27,226 Epoch 5153: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 16:29:27,227 EPOCH 5154
2024-02-07 16:29:43,511 Epoch 5154: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 16:29:43,512 EPOCH 5155
2024-02-07 16:30:00,010 Epoch 5155: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 16:30:00,011 EPOCH 5156
2024-02-07 16:30:11,019 [Epoch: 5156 Step: 00046400] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:      581 || Batch Translation Loss:   0.015633 => Txt Tokens per Sec:     1722 || Lr: 0.000100
2024-02-07 16:30:16,619 Epoch 5156: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 16:30:16,619 EPOCH 5157
2024-02-07 16:30:32,667 Epoch 5157: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 16:30:32,668 EPOCH 5158
2024-02-07 16:30:49,411 Epoch 5158: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 16:30:49,413 EPOCH 5159
2024-02-07 16:31:06,117 Epoch 5159: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 16:31:06,117 EPOCH 5160
2024-02-07 16:31:22,393 Epoch 5160: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 16:31:22,394 EPOCH 5161
2024-02-07 16:31:38,784 Epoch 5161: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 16:31:38,785 EPOCH 5162
2024-02-07 16:31:54,943 Epoch 5162: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 16:31:54,944 EPOCH 5163
2024-02-07 16:32:11,101 Epoch 5163: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-07 16:32:11,102 EPOCH 5164
2024-02-07 16:32:27,235 Epoch 5164: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 16:32:27,235 EPOCH 5165
2024-02-07 16:32:43,678 Epoch 5165: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-07 16:32:43,679 EPOCH 5166
2024-02-07 16:33:00,074 Epoch 5166: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-07 16:33:00,075 EPOCH 5167
2024-02-07 16:33:15,306 [Epoch: 5167 Step: 00046500] Batch Recognition Loss:   0.000514 => Gls Tokens per Sec:      445 || Batch Translation Loss:   0.054580 => Txt Tokens per Sec:     1361 || Lr: 0.000100
2024-02-07 16:33:16,421 Epoch 5167: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-07 16:33:16,421 EPOCH 5168
2024-02-07 16:33:32,812 Epoch 5168: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-07 16:33:32,812 EPOCH 5169
2024-02-07 16:33:49,405 Epoch 5169: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-07 16:33:49,405 EPOCH 5170
2024-02-07 16:34:05,409 Epoch 5170: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-07 16:34:05,410 EPOCH 5171
2024-02-07 16:34:22,324 Epoch 5171: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-07 16:34:22,324 EPOCH 5172
2024-02-07 16:34:39,190 Epoch 5172: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-07 16:34:39,191 EPOCH 5173
2024-02-07 16:34:55,557 Epoch 5173: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-07 16:34:55,558 EPOCH 5174
2024-02-07 16:35:11,839 Epoch 5174: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-07 16:35:11,839 EPOCH 5175
2024-02-07 16:35:28,448 Epoch 5175: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-07 16:35:28,449 EPOCH 5176
2024-02-07 16:35:44,749 Epoch 5176: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-07 16:35:44,749 EPOCH 5177
2024-02-07 16:36:01,050 Epoch 5177: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-07 16:36:01,051 EPOCH 5178
2024-02-07 16:36:13,065 [Epoch: 5178 Step: 00046600] Batch Recognition Loss:   0.000486 => Gls Tokens per Sec:      746 || Batch Translation Loss:   0.073188 => Txt Tokens per Sec:     2142 || Lr: 0.000100
2024-02-07 16:36:17,763 Epoch 5178: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-07 16:36:17,764 EPOCH 5179
2024-02-07 16:36:34,362 Epoch 5179: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-07 16:36:34,362 EPOCH 5180
2024-02-07 16:36:50,805 Epoch 5180: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-07 16:36:50,806 EPOCH 5181
2024-02-07 16:37:07,680 Epoch 5181: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-07 16:37:07,681 EPOCH 5182
2024-02-07 16:37:23,915 Epoch 5182: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-07 16:37:23,915 EPOCH 5183
2024-02-07 16:37:40,498 Epoch 5183: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 16:37:40,499 EPOCH 5184
2024-02-07 16:37:56,752 Epoch 5184: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 16:37:56,752 EPOCH 5185
2024-02-07 16:38:13,505 Epoch 5185: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 16:38:13,507 EPOCH 5186
2024-02-07 16:38:29,994 Epoch 5186: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 16:38:29,995 EPOCH 5187
2024-02-07 16:38:46,427 Epoch 5187: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 16:38:46,428 EPOCH 5188
2024-02-07 16:39:02,974 Epoch 5188: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 16:39:02,974 EPOCH 5189
2024-02-07 16:39:16,037 [Epoch: 5189 Step: 00046700] Batch Recognition Loss:   0.000578 => Gls Tokens per Sec:      715 || Batch Translation Loss:   0.006829 => Txt Tokens per Sec:     1935 || Lr: 0.000100
2024-02-07 16:39:19,332 Epoch 5189: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 16:39:19,332 EPOCH 5190
2024-02-07 16:39:35,635 Epoch 5190: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 16:39:35,635 EPOCH 5191
2024-02-07 16:39:51,769 Epoch 5191: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 16:39:51,770 EPOCH 5192
2024-02-07 16:40:08,261 Epoch 5192: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 16:40:08,262 EPOCH 5193
2024-02-07 16:40:24,687 Epoch 5193: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 16:40:24,688 EPOCH 5194
2024-02-07 16:40:40,898 Epoch 5194: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 16:40:40,899 EPOCH 5195
2024-02-07 16:40:57,492 Epoch 5195: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 16:40:57,493 EPOCH 5196
2024-02-07 16:41:13,838 Epoch 5196: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 16:41:13,838 EPOCH 5197
2024-02-07 16:41:30,478 Epoch 5197: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 16:41:30,478 EPOCH 5198
2024-02-07 16:41:46,901 Epoch 5198: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:41:46,901 EPOCH 5199
2024-02-07 16:42:03,231 Epoch 5199: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 16:42:03,231 EPOCH 5200
2024-02-07 16:42:19,445 [Epoch: 5200 Step: 00046800] Batch Recognition Loss:   0.000437 => Gls Tokens per Sec:      655 || Batch Translation Loss:   0.010468 => Txt Tokens per Sec:     1812 || Lr: 0.000100
2024-02-07 16:42:19,446 Epoch 5200: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 16:42:19,446 EPOCH 5201
2024-02-07 16:42:35,813 Epoch 5201: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 16:42:35,813 EPOCH 5202
2024-02-07 16:42:52,041 Epoch 5202: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:42:52,042 EPOCH 5203
2024-02-07 16:43:08,384 Epoch 5203: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:43:08,385 EPOCH 5204
2024-02-07 16:43:24,501 Epoch 5204: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:43:24,501 EPOCH 5205
2024-02-07 16:43:41,091 Epoch 5205: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 16:43:41,091 EPOCH 5206
2024-02-07 16:43:57,752 Epoch 5206: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:43:57,753 EPOCH 5207
2024-02-07 16:44:14,041 Epoch 5207: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-07 16:44:14,042 EPOCH 5208
2024-02-07 16:44:30,565 Epoch 5208: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:44:30,566 EPOCH 5209
2024-02-07 16:44:46,681 Epoch 5209: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:44:46,682 EPOCH 5210
2024-02-07 16:45:03,273 Epoch 5210: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:45:03,273 EPOCH 5211
2024-02-07 16:45:19,851 Epoch 5211: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:45:19,852 EPOCH 5212
2024-02-07 16:45:20,085 [Epoch: 5212 Step: 00046900] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     5486 || Batch Translation Loss:   0.006124 => Txt Tokens per Sec:     9785 || Lr: 0.000100
2024-02-07 16:45:36,320 Epoch 5212: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 16:45:36,320 EPOCH 5213
2024-02-07 16:45:52,837 Epoch 5213: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.09 
2024-02-07 16:45:52,837 EPOCH 5214
2024-02-07 16:46:09,120 Epoch 5214: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 16:46:09,121 EPOCH 5215
2024-02-07 16:46:25,401 Epoch 5215: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:46:25,402 EPOCH 5216
2024-02-07 16:46:41,659 Epoch 5216: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 16:46:41,659 EPOCH 5217
2024-02-07 16:46:58,092 Epoch 5217: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:46:58,093 EPOCH 5218
2024-02-07 16:47:14,583 Epoch 5218: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:47:14,584 EPOCH 5219
2024-02-07 16:47:31,092 Epoch 5219: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:47:31,093 EPOCH 5220
2024-02-07 16:47:47,392 Epoch 5220: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:47:47,393 EPOCH 5221
2024-02-07 16:48:03,665 Epoch 5221: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:48:03,666 EPOCH 5222
2024-02-07 16:48:20,072 Epoch 5222: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:48:20,073 EPOCH 5223
2024-02-07 16:48:23,443 [Epoch: 5223 Step: 00047000] Batch Recognition Loss:   0.000262 => Gls Tokens per Sec:      760 || Batch Translation Loss:   0.006042 => Txt Tokens per Sec:     1901 || Lr: 0.000100
2024-02-07 16:48:36,250 Epoch 5223: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 16:48:36,251 EPOCH 5224
2024-02-07 16:48:52,713 Epoch 5224: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:48:52,714 EPOCH 5225
2024-02-07 16:49:09,116 Epoch 5225: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:49:09,116 EPOCH 5226
2024-02-07 16:49:25,452 Epoch 5226: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 16:49:25,452 EPOCH 5227
2024-02-07 16:49:41,846 Epoch 5227: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:49:41,847 EPOCH 5228
2024-02-07 16:49:58,184 Epoch 5228: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 16:49:58,184 EPOCH 5229
2024-02-07 16:50:14,631 Epoch 5229: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:50:14,631 EPOCH 5230
2024-02-07 16:50:30,852 Epoch 5230: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:50:30,852 EPOCH 5231
2024-02-07 16:50:47,183 Epoch 5231: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:50:47,184 EPOCH 5232
2024-02-07 16:51:03,831 Epoch 5232: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:51:03,832 EPOCH 5233
2024-02-07 16:51:20,374 Epoch 5233: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 16:51:20,374 EPOCH 5234
2024-02-07 16:51:21,424 [Epoch: 5234 Step: 00047100] Batch Recognition Loss:   0.000367 => Gls Tokens per Sec:     3661 || Batch Translation Loss:   0.009457 => Txt Tokens per Sec:     7999 || Lr: 0.000100
2024-02-07 16:51:37,098 Epoch 5234: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:51:37,099 EPOCH 5235
2024-02-07 16:51:53,463 Epoch 5235: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:51:53,463 EPOCH 5236
2024-02-07 16:52:10,072 Epoch 5236: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:52:10,073 EPOCH 5237
2024-02-07 16:52:26,627 Epoch 5237: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:52:26,628 EPOCH 5238
2024-02-07 16:52:42,930 Epoch 5238: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 16:52:42,931 EPOCH 5239
2024-02-07 16:52:59,522 Epoch 5239: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 16:52:59,522 EPOCH 5240
2024-02-07 16:53:15,965 Epoch 5240: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 16:53:15,966 EPOCH 5241
2024-02-07 16:53:32,019 Epoch 5241: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-07 16:53:32,020 EPOCH 5242
2024-02-07 16:53:48,527 Epoch 5242: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 16:53:48,528 EPOCH 5243
2024-02-07 16:54:05,346 Epoch 5243: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.10 
2024-02-07 16:54:05,347 EPOCH 5244
2024-02-07 16:54:21,886 Epoch 5244: Total Training Recognition Loss 0.22  Total Training Translation Loss 0.10 
2024-02-07 16:54:21,887 EPOCH 5245
2024-02-07 16:54:31,927 [Epoch: 5245 Step: 00047200] Batch Recognition Loss:   0.020506 => Gls Tokens per Sec:      510 || Batch Translation Loss:   0.018971 => Txt Tokens per Sec:     1459 || Lr: 0.000100
2024-02-07 16:54:38,402 Epoch 5245: Total Training Recognition Loss 0.60  Total Training Translation Loss 0.11 
2024-02-07 16:54:38,403 EPOCH 5246
2024-02-07 16:54:54,954 Epoch 5246: Total Training Recognition Loss 0.39  Total Training Translation Loss 0.23 
2024-02-07 16:54:54,955 EPOCH 5247
2024-02-07 16:55:11,507 Epoch 5247: Total Training Recognition Loss 0.17  Total Training Translation Loss 0.22 
2024-02-07 16:55:11,508 EPOCH 5248
2024-02-07 16:55:27,985 Epoch 5248: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.22 
2024-02-07 16:55:27,985 EPOCH 5249
2024-02-07 16:55:44,412 Epoch 5249: Total Training Recognition Loss 0.12  Total Training Translation Loss 0.24 
2024-02-07 16:55:44,412 EPOCH 5250
2024-02-07 16:56:01,090 Epoch 5250: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.55 
2024-02-07 16:56:01,092 EPOCH 5251
2024-02-07 16:56:17,357 Epoch 5251: Total Training Recognition Loss 0.10  Total Training Translation Loss 0.58 
2024-02-07 16:56:17,358 EPOCH 5252
2024-02-07 16:56:33,804 Epoch 5252: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.99 
2024-02-07 16:56:33,805 EPOCH 5253
2024-02-07 16:56:50,033 Epoch 5253: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.58 
2024-02-07 16:56:50,034 EPOCH 5254
2024-02-07 16:57:06,546 Epoch 5254: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.52 
2024-02-07 16:57:06,547 EPOCH 5255
2024-02-07 16:57:23,076 Epoch 5255: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.04 
2024-02-07 16:57:23,076 EPOCH 5256
2024-02-07 16:57:29,329 [Epoch: 5256 Step: 00047300] Batch Recognition Loss:   0.000948 => Gls Tokens per Sec:      880 || Batch Translation Loss:   0.068701 => Txt Tokens per Sec:     2411 || Lr: 0.000100
2024-02-07 16:57:39,278 Epoch 5256: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-07 16:57:39,278 EPOCH 5257
2024-02-07 16:57:55,787 Epoch 5257: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-07 16:57:55,788 EPOCH 5258
2024-02-07 16:58:11,944 Epoch 5258: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-07 16:58:11,945 EPOCH 5259
2024-02-07 16:58:28,060 Epoch 5259: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-07 16:58:28,061 EPOCH 5260
2024-02-07 16:58:44,346 Epoch 5260: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-07 16:58:44,346 EPOCH 5261
2024-02-07 16:59:00,518 Epoch 5261: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-07 16:59:00,518 EPOCH 5262
2024-02-07 16:59:17,026 Epoch 5262: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-07 16:59:17,027 EPOCH 5263
2024-02-07 16:59:33,442 Epoch 5263: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-07 16:59:33,442 EPOCH 5264
2024-02-07 16:59:49,821 Epoch 5264: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-07 16:59:49,822 EPOCH 5265
2024-02-07 17:00:06,318 Epoch 5265: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 17:00:06,319 EPOCH 5266
2024-02-07 17:00:23,040 Epoch 5266: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 17:00:23,041 EPOCH 5267
2024-02-07 17:00:38,006 [Epoch: 5267 Step: 00047400] Batch Recognition Loss:   0.000667 => Gls Tokens per Sec:      453 || Batch Translation Loss:   0.028489 => Txt Tokens per Sec:     1262 || Lr: 0.000100
2024-02-07 17:00:39,557 Epoch 5267: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 17:00:39,558 EPOCH 5268
2024-02-07 17:00:55,838 Epoch 5268: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 17:00:55,838 EPOCH 5269
2024-02-07 17:01:12,078 Epoch 5269: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 17:01:12,078 EPOCH 5270
2024-02-07 17:01:28,448 Epoch 5270: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 17:01:28,449 EPOCH 5271
2024-02-07 17:01:45,175 Epoch 5271: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 17:01:45,176 EPOCH 5272
2024-02-07 17:02:01,374 Epoch 5272: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 17:02:01,374 EPOCH 5273
2024-02-07 17:02:18,088 Epoch 5273: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 17:02:18,088 EPOCH 5274
2024-02-07 17:02:34,467 Epoch 5274: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 17:02:34,468 EPOCH 5275
2024-02-07 17:02:50,887 Epoch 5275: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 17:02:50,887 EPOCH 5276
2024-02-07 17:03:07,100 Epoch 5276: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:03:07,101 EPOCH 5277
2024-02-07 17:03:23,730 Epoch 5277: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:03:23,731 EPOCH 5278
2024-02-07 17:03:33,452 [Epoch: 5278 Step: 00047500] Batch Recognition Loss:   0.000737 => Gls Tokens per Sec:      829 || Batch Translation Loss:   0.008908 => Txt Tokens per Sec:     2187 || Lr: 0.000100
2024-02-07 17:03:40,225 Epoch 5278: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 17:03:40,225 EPOCH 5279
2024-02-07 17:03:57,256 Epoch 5279: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:03:57,257 EPOCH 5280
2024-02-07 17:04:13,984 Epoch 5280: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:04:13,985 EPOCH 5281
2024-02-07 17:04:30,198 Epoch 5281: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:04:30,199 EPOCH 5282
2024-02-07 17:04:46,633 Epoch 5282: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:04:46,634 EPOCH 5283
2024-02-07 17:05:03,074 Epoch 5283: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 17:05:03,075 EPOCH 5284
2024-02-07 17:05:19,320 Epoch 5284: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 17:05:19,321 EPOCH 5285
2024-02-07 17:05:35,846 Epoch 5285: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 17:05:35,847 EPOCH 5286
2024-02-07 17:05:51,976 Epoch 5286: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 17:05:51,976 EPOCH 5287
2024-02-07 17:06:08,386 Epoch 5287: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 17:06:08,386 EPOCH 5288
2024-02-07 17:06:24,886 Epoch 5288: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 17:06:24,886 EPOCH 5289
2024-02-07 17:06:40,460 [Epoch: 5289 Step: 00047600] Batch Recognition Loss:   0.000342 => Gls Tokens per Sec:      600 || Batch Translation Loss:   0.017120 => Txt Tokens per Sec:     1637 || Lr: 0.000100
2024-02-07 17:06:41,212 Epoch 5289: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:06:41,212 EPOCH 5290
2024-02-07 17:06:58,174 Epoch 5290: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:06:58,175 EPOCH 5291
2024-02-07 17:07:14,792 Epoch 5291: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:07:14,793 EPOCH 5292
2024-02-07 17:07:31,044 Epoch 5292: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:07:31,044 EPOCH 5293
2024-02-07 17:07:47,646 Epoch 5293: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-07 17:07:47,646 EPOCH 5294
2024-02-07 17:08:04,046 Epoch 5294: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:08:04,047 EPOCH 5295
2024-02-07 17:08:20,357 Epoch 5295: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:08:20,358 EPOCH 5296
2024-02-07 17:08:36,833 Epoch 5296: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 17:08:36,833 EPOCH 5297
2024-02-07 17:08:53,196 Epoch 5297: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:08:53,196 EPOCH 5298
2024-02-07 17:09:09,662 Epoch 5298: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:09:09,662 EPOCH 5299
2024-02-07 17:09:25,965 Epoch 5299: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:09:25,966 EPOCH 5300
2024-02-07 17:09:42,661 [Epoch: 5300 Step: 00047700] Batch Recognition Loss:   0.000213 => Gls Tokens per Sec:      636 || Batch Translation Loss:   0.013817 => Txt Tokens per Sec:     1760 || Lr: 0.000100
2024-02-07 17:09:42,661 Epoch 5300: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:09:42,662 EPOCH 5301
2024-02-07 17:09:59,230 Epoch 5301: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:09:59,231 EPOCH 5302
2024-02-07 17:10:15,462 Epoch 5302: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:10:15,463 EPOCH 5303
2024-02-07 17:10:31,813 Epoch 5303: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 17:10:31,813 EPOCH 5304
2024-02-07 17:10:48,199 Epoch 5304: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 17:10:48,201 EPOCH 5305
2024-02-07 17:11:04,489 Epoch 5305: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 17:11:04,489 EPOCH 5306
2024-02-07 17:11:20,692 Epoch 5306: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 17:11:20,692 EPOCH 5307
2024-02-07 17:11:38,044 Epoch 5307: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 17:11:38,044 EPOCH 5308
2024-02-07 17:11:54,365 Epoch 5308: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:11:54,366 EPOCH 5309
2024-02-07 17:12:10,739 Epoch 5309: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:12:10,739 EPOCH 5310
2024-02-07 17:12:27,531 Epoch 5310: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:12:27,532 EPOCH 5311
2024-02-07 17:12:43,827 Epoch 5311: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:12:43,827 EPOCH 5312
2024-02-07 17:12:44,135 [Epoch: 5312 Step: 00047800] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:     4169 || Batch Translation Loss:   0.008251 => Txt Tokens per Sec:     9264 || Lr: 0.000100
2024-02-07 17:12:59,959 Epoch 5312: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:12:59,960 EPOCH 5313
2024-02-07 17:13:16,724 Epoch 5313: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:13:16,724 EPOCH 5314
2024-02-07 17:13:33,273 Epoch 5314: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:13:33,273 EPOCH 5315
2024-02-07 17:13:49,442 Epoch 5315: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:13:49,443 EPOCH 5316
2024-02-07 17:14:06,185 Epoch 5316: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:14:06,185 EPOCH 5317
2024-02-07 17:14:22,467 Epoch 5317: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 17:14:22,467 EPOCH 5318
2024-02-07 17:14:39,205 Epoch 5318: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 17:14:39,205 EPOCH 5319
2024-02-07 17:14:55,580 Epoch 5319: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 17:14:55,580 EPOCH 5320
2024-02-07 17:15:12,426 Epoch 5320: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 17:15:12,427 EPOCH 5321
2024-02-07 17:15:28,911 Epoch 5321: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 17:15:28,912 EPOCH 5322
2024-02-07 17:15:45,156 Epoch 5322: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 17:15:45,158 EPOCH 5323
2024-02-07 17:15:52,695 [Epoch: 5323 Step: 00047900] Batch Recognition Loss:   0.000378 => Gls Tokens per Sec:      220 || Batch Translation Loss:   0.005911 => Txt Tokens per Sec:      722 || Lr: 0.000100
2024-02-07 17:16:01,588 Epoch 5323: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 17:16:01,588 EPOCH 5324
2024-02-07 17:16:17,947 Epoch 5324: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 17:16:17,948 EPOCH 5325
2024-02-07 17:16:34,392 Epoch 5325: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 17:16:34,393 EPOCH 5326
2024-02-07 17:16:50,894 Epoch 5326: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 17:16:50,894 EPOCH 5327
2024-02-07 17:17:07,266 Epoch 5327: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 17:17:07,266 EPOCH 5328
2024-02-07 17:17:23,456 Epoch 5328: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 17:17:23,457 EPOCH 5329
2024-02-07 17:17:40,109 Epoch 5329: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 17:17:40,110 EPOCH 5330
2024-02-07 17:17:56,422 Epoch 5330: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:17:56,423 EPOCH 5331
2024-02-07 17:18:12,429 Epoch 5331: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:18:12,430 EPOCH 5332
2024-02-07 17:18:29,136 Epoch 5332: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 17:18:29,136 EPOCH 5333
2024-02-07 17:18:45,452 Epoch 5333: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:18:45,452 EPOCH 5334
2024-02-07 17:18:53,482 [Epoch: 5334 Step: 00048000] Batch Recognition Loss:   0.000242 => Gls Tokens per Sec:      366 || Batch Translation Loss:   0.014467 => Txt Tokens per Sec:     1137 || Lr: 0.000100
2024-02-07 17:20:01,203 Validation result at epoch 5334, step    48000: duration: 67.7188s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.27862	Translation Loss: 103640.67188	PPL: 31308.48828
	Eval Metric: BLEU
	WER 2.12	(DEL: 0.00,	INS: 0.00,	SUB: 2.12)
	BLEU-4 0.44	(BLEU-1: 10.75,	BLEU-2: 3.32,	BLEU-3: 1.12,	BLEU-4: 0.44)
	CHRF 16.98	ROUGE 8.95
2024-02-07 17:20:01,205 Logging Recognition and Translation Outputs
2024-02-07 17:20:01,205 ========================================================================================================================
2024-02-07 17:20:01,205 Logging Sequence: 96_93.00
2024-02-07 17:20:01,205 	Gloss Reference :	A B+C+D+E
2024-02-07 17:20:01,205 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 17:20:01,205 	Gloss Alignment :	         
2024-02-07 17:20:01,206 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 17:20:01,209 	Text Reference  :	*** **** **** ** bhuvneshwar kumar took 4  wickets and  hardik pandya took 3   wickets wonderful
2024-02-07 17:20:01,209 	Text Hypothesis :	the 19th over in the         dhoni on   10 teams   have a      rest   of   win was     win      
2024-02-07 17:20:01,209 	Text Alignment  :	I   I    I    I  S           S     S    S  S       S    S      S      S    S   S       S        
2024-02-07 17:20:01,209 ========================================================================================================================
2024-02-07 17:20:01,209 Logging Sequence: 144_2.00
2024-02-07 17:20:01,210 	Gloss Reference :	A B+C+D+E  
2024-02-07 17:20:01,210 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-07 17:20:01,210 	Gloss Alignment :	  S        
2024-02-07 17:20:01,210 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 17:20:01,213 	Text Reference  :	a      girl     posted a ***** video  of herself  playing cricket on   a  village farm   on  social media   the video has gone viral   
2024-02-07 17:20:01,213 	Text Hypothesis :	police detained over   a dozen people in brussels and     eight   more at the     bottom now people started the ***** *** **** district
2024-02-07 17:20:01,213 	Text Alignment  :	S      S        S        I     S      S  S        S       S       S    S  S       S      S   S      S           D     D   D    S       
2024-02-07 17:20:01,213 ========================================================================================================================
2024-02-07 17:20:01,213 Logging Sequence: 178_83.00
2024-02-07 17:20:01,214 	Gloss Reference :	A B+C+D+E
2024-02-07 17:20:01,214 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 17:20:01,214 	Gloss Alignment :	         
2024-02-07 17:20:01,214 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 17:20:01,215 	Text Reference  :	and the     police still haven't apprehended the wrestler
2024-02-07 17:20:01,215 	Text Hypothesis :	*** against sushil kumar was     involved    in  home    
2024-02-07 17:20:01,215 	Text Alignment  :	D   S       S      S     S       S           S   S       
2024-02-07 17:20:01,215 ========================================================================================================================
2024-02-07 17:20:01,215 Logging Sequence: 169_214.00
2024-02-07 17:20:01,216 	Gloss Reference :	A B+C+D+E
2024-02-07 17:20:01,216 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 17:20:01,216 	Gloss Alignment :	         
2024-02-07 17:20:01,216 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 17:20:01,217 	Text Reference  :	virat kohli said that though arshdeep dropped    the   catch he           is   still a    strong part       of     the indian team       
2024-02-07 17:20:01,218 	Text Hypothesis :	***** ***** **** **** ****** the      government found this  unacceptable this could have put    arshdeep's family in  danger harrassment
2024-02-07 17:20:01,218 	Text Alignment  :	D     D     D    D    D      S        S          S     S     S            S    S     S    S      S          S      S   S      S          
2024-02-07 17:20:01,218 ========================================================================================================================
2024-02-07 17:20:01,218 Logging Sequence: 147_202.00
2024-02-07 17:20:01,218 	Gloss Reference :	A B+C+D+E
2024-02-07 17:20:01,218 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 17:20:01,218 	Gloss Alignment :	         
2024-02-07 17:20:01,218 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 17:20:01,221 	Text Reference  :	were impressed that she took the   difficult decision to   withdraw from the olympics and focus  on *** her    mental health
2024-02-07 17:20:01,221 	Text Hypothesis :	**** this      is   why he   loves playing   games    like pubg     call of  duty     and others on his mobile or     ipad  
2024-02-07 17:20:01,221 	Text Alignment  :	D    S         S    S   S    S     S         S        S    S        S    S   S            S         I   S      S      S     
2024-02-07 17:20:01,221 ========================================================================================================================
2024-02-07 17:20:09,999 Epoch 5334: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:20:09,999 EPOCH 5335
2024-02-07 17:20:26,686 Epoch 5335: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:20:26,686 EPOCH 5336
2024-02-07 17:20:43,022 Epoch 5336: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:20:43,023 EPOCH 5337
2024-02-07 17:20:59,523 Epoch 5337: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:20:59,523 EPOCH 5338
2024-02-07 17:21:15,937 Epoch 5338: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:21:15,938 EPOCH 5339
2024-02-07 17:21:32,427 Epoch 5339: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:21:32,428 EPOCH 5340
2024-02-07 17:21:48,624 Epoch 5340: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:21:48,625 EPOCH 5341
2024-02-07 17:22:05,207 Epoch 5341: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:22:05,207 EPOCH 5342
2024-02-07 17:22:21,768 Epoch 5342: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:22:21,769 EPOCH 5343
2024-02-07 17:22:38,962 Epoch 5343: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:22:38,962 EPOCH 5344
2024-02-07 17:22:55,481 Epoch 5344: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:22:55,482 EPOCH 5345
2024-02-07 17:23:04,370 [Epoch: 5345 Step: 00048100] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:      475 || Batch Translation Loss:   0.012805 => Txt Tokens per Sec:     1469 || Lr: 0.000050
2024-02-07 17:23:11,947 Epoch 5345: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:23:11,948 EPOCH 5346
2024-02-07 17:23:28,290 Epoch 5346: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:23:28,291 EPOCH 5347
2024-02-07 17:23:44,799 Epoch 5347: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:23:44,800 EPOCH 5348
2024-02-07 17:24:00,957 Epoch 5348: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:24:00,957 EPOCH 5349
2024-02-07 17:24:17,103 Epoch 5349: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:24:17,104 EPOCH 5350
2024-02-07 17:24:33,203 Epoch 5350: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:24:33,204 EPOCH 5351
2024-02-07 17:24:49,812 Epoch 5351: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:24:49,813 EPOCH 5352
2024-02-07 17:25:06,062 Epoch 5352: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:25:06,063 EPOCH 5353
2024-02-07 17:25:22,340 Epoch 5353: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:25:22,341 EPOCH 5354
2024-02-07 17:25:38,563 Epoch 5354: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:25:38,564 EPOCH 5355
2024-02-07 17:25:54,795 Epoch 5355: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:25:54,795 EPOCH 5356
2024-02-07 17:26:05,613 [Epoch: 5356 Step: 00048200] Batch Recognition Loss:   0.000210 => Gls Tokens per Sec:      592 || Batch Translation Loss:   0.014947 => Txt Tokens per Sec:     1749 || Lr: 0.000050
2024-02-07 17:26:11,151 Epoch 5356: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:26:11,151 EPOCH 5357
2024-02-07 17:26:27,488 Epoch 5357: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:26:27,488 EPOCH 5358
2024-02-07 17:26:43,749 Epoch 5358: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:26:43,750 EPOCH 5359
2024-02-07 17:27:00,066 Epoch 5359: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:27:00,067 EPOCH 5360
2024-02-07 17:27:16,048 Epoch 5360: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:27:16,049 EPOCH 5361
2024-02-07 17:27:32,374 Epoch 5361: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:27:32,375 EPOCH 5362
2024-02-07 17:27:48,460 Epoch 5362: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:27:48,460 EPOCH 5363
2024-02-07 17:28:04,742 Epoch 5363: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:28:04,743 EPOCH 5364
2024-02-07 17:28:21,048 Epoch 5364: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:28:21,049 EPOCH 5365
2024-02-07 17:28:37,163 Epoch 5365: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:28:37,163 EPOCH 5366
2024-02-07 17:28:52,976 Epoch 5366: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:28:52,977 EPOCH 5367
2024-02-07 17:29:04,339 [Epoch: 5367 Step: 00048300] Batch Recognition Loss:   0.000125 => Gls Tokens per Sec:      676 || Batch Translation Loss:   0.015745 => Txt Tokens per Sec:     1978 || Lr: 0.000050
2024-02-07 17:29:09,380 Epoch 5367: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:29:09,380 EPOCH 5368
2024-02-07 17:29:25,487 Epoch 5368: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:29:25,487 EPOCH 5369
2024-02-07 17:29:41,440 Epoch 5369: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:29:41,441 EPOCH 5370
2024-02-07 17:29:57,791 Epoch 5370: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:29:57,792 EPOCH 5371
2024-02-07 17:30:14,015 Epoch 5371: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:30:14,016 EPOCH 5372
2024-02-07 17:30:30,084 Epoch 5372: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:30:30,085 EPOCH 5373
2024-02-07 17:30:46,439 Epoch 5373: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:30:46,440 EPOCH 5374
2024-02-07 17:31:02,857 Epoch 5374: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:31:02,858 EPOCH 5375
2024-02-07 17:31:18,969 Epoch 5375: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:31:18,969 EPOCH 5376
2024-02-07 17:31:35,353 Epoch 5376: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:31:35,354 EPOCH 5377
2024-02-07 17:31:51,625 Epoch 5377: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:31:51,626 EPOCH 5378
2024-02-07 17:32:06,641 [Epoch: 5378 Step: 00048400] Batch Recognition Loss:   0.000270 => Gls Tokens per Sec:      537 || Batch Translation Loss:   0.008185 => Txt Tokens per Sec:     1461 || Lr: 0.000050
2024-02-07 17:32:07,781 Epoch 5378: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:32:07,781 EPOCH 5379
2024-02-07 17:32:23,970 Epoch 5379: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:32:23,970 EPOCH 5380
2024-02-07 17:32:40,009 Epoch 5380: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:32:40,009 EPOCH 5381
2024-02-07 17:32:56,128 Epoch 5381: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:32:56,128 EPOCH 5382
2024-02-07 17:33:12,239 Epoch 5382: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:33:12,240 EPOCH 5383
2024-02-07 17:33:28,439 Epoch 5383: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:33:28,439 EPOCH 5384
2024-02-07 17:33:44,495 Epoch 5384: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:33:44,496 EPOCH 5385
2024-02-07 17:34:00,779 Epoch 5385: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:34:00,780 EPOCH 5386
2024-02-07 17:34:16,873 Epoch 5386: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:34:16,873 EPOCH 5387
2024-02-07 17:34:33,032 Epoch 5387: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:34:33,032 EPOCH 5388
2024-02-07 17:34:49,278 Epoch 5388: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:34:49,279 EPOCH 5389
2024-02-07 17:35:01,137 [Epoch: 5389 Step: 00048500] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:      864 || Batch Translation Loss:   0.014906 => Txt Tokens per Sec:     2363 || Lr: 0.000050
2024-02-07 17:35:05,439 Epoch 5389: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:35:05,440 EPOCH 5390
2024-02-07 17:35:21,915 Epoch 5390: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:35:21,916 EPOCH 5391
2024-02-07 17:35:38,129 Epoch 5391: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:35:38,130 EPOCH 5392
2024-02-07 17:35:54,450 Epoch 5392: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:35:54,450 EPOCH 5393
2024-02-07 17:36:10,931 Epoch 5393: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:36:10,931 EPOCH 5394
2024-02-07 17:36:27,088 Epoch 5394: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:36:27,089 EPOCH 5395
2024-02-07 17:36:43,473 Epoch 5395: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:36:43,474 EPOCH 5396
2024-02-07 17:36:59,684 Epoch 5396: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:36:59,684 EPOCH 5397
2024-02-07 17:37:15,606 Epoch 5397: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:37:15,606 EPOCH 5398
2024-02-07 17:37:32,113 Epoch 5398: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:37:32,113 EPOCH 5399
2024-02-07 17:37:47,973 Epoch 5399: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:37:47,974 EPOCH 5400
2024-02-07 17:38:04,413 [Epoch: 5400 Step: 00048600] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:      646 || Batch Translation Loss:   0.009527 => Txt Tokens per Sec:     1787 || Lr: 0.000050
2024-02-07 17:38:04,414 Epoch 5400: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:38:04,414 EPOCH 5401
2024-02-07 17:38:20,488 Epoch 5401: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:38:20,489 EPOCH 5402
2024-02-07 17:38:36,829 Epoch 5402: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:38:36,829 EPOCH 5403
2024-02-07 17:38:52,943 Epoch 5403: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:38:52,944 EPOCH 5404
2024-02-07 17:39:09,183 Epoch 5404: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:39:09,184 EPOCH 5405
2024-02-07 17:39:25,567 Epoch 5405: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-07 17:39:25,567 EPOCH 5406
2024-02-07 17:39:41,738 Epoch 5406: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:39:41,738 EPOCH 5407
2024-02-07 17:39:57,987 Epoch 5407: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:39:57,988 EPOCH 5408
2024-02-07 17:40:14,120 Epoch 5408: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:40:14,120 EPOCH 5409
2024-02-07 17:40:30,439 Epoch 5409: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:40:30,439 EPOCH 5410
2024-02-07 17:40:46,798 Epoch 5410: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:40:46,799 EPOCH 5411
2024-02-07 17:41:02,956 Epoch 5411: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 17:41:02,957 EPOCH 5412
2024-02-07 17:41:03,820 [Epoch: 5412 Step: 00048700] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:     1485 || Batch Translation Loss:   0.014356 => Txt Tokens per Sec:     4486 || Lr: 0.000050
2024-02-07 17:41:19,396 Epoch 5412: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:41:19,396 EPOCH 5413
2024-02-07 17:41:35,762 Epoch 5413: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:41:35,762 EPOCH 5414
2024-02-07 17:41:51,902 Epoch 5414: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:41:51,902 EPOCH 5415
2024-02-07 17:42:08,119 Epoch 5415: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:42:08,119 EPOCH 5416
2024-02-07 17:42:24,202 Epoch 5416: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 17:42:24,203 EPOCH 5417
2024-02-07 17:42:40,686 Epoch 5417: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 17:42:40,687 EPOCH 5418
2024-02-07 17:42:57,138 Epoch 5418: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 17:42:57,138 EPOCH 5419
2024-02-07 17:43:13,599 Epoch 5419: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 17:43:13,600 EPOCH 5420
2024-02-07 17:43:30,059 Epoch 5420: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 17:43:30,060 EPOCH 5421
2024-02-07 17:43:46,498 Epoch 5421: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 17:43:46,499 EPOCH 5422
2024-02-07 17:44:03,157 Epoch 5422: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 17:44:03,158 EPOCH 5423
2024-02-07 17:44:04,057 [Epoch: 5423 Step: 00048800] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:     2851 || Batch Translation Loss:   0.021840 => Txt Tokens per Sec:     8006 || Lr: 0.000050
2024-02-07 17:44:18,991 Epoch 5423: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 17:44:18,992 EPOCH 5424
2024-02-07 17:44:35,116 Epoch 5424: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 17:44:35,116 EPOCH 5425
2024-02-07 17:44:51,560 Epoch 5425: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 17:44:51,561 EPOCH 5426
2024-02-07 17:45:07,624 Epoch 5426: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-07 17:45:07,625 EPOCH 5427
2024-02-07 17:45:23,889 Epoch 5427: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.76 
2024-02-07 17:45:23,889 EPOCH 5428
2024-02-07 17:45:40,017 Epoch 5428: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-07 17:45:40,018 EPOCH 5429
2024-02-07 17:45:56,161 Epoch 5429: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-07 17:45:56,162 EPOCH 5430
2024-02-07 17:46:12,202 Epoch 5430: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 17:46:12,202 EPOCH 5431
2024-02-07 17:46:28,620 Epoch 5431: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 17:46:28,621 EPOCH 5432
2024-02-07 17:46:44,722 Epoch 5432: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 17:46:44,723 EPOCH 5433
2024-02-07 17:47:00,887 Epoch 5433: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 17:47:00,887 EPOCH 5434
2024-02-07 17:47:02,053 [Epoch: 5434 Step: 00048900] Batch Recognition Loss:   0.000391 => Gls Tokens per Sec:     3298 || Batch Translation Loss:   0.007493 => Txt Tokens per Sec:     7949 || Lr: 0.000050
2024-02-07 17:47:17,019 Epoch 5434: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 17:47:17,020 EPOCH 5435
2024-02-07 17:47:33,062 Epoch 5435: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 17:47:33,062 EPOCH 5436
2024-02-07 17:47:49,329 Epoch 5436: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 17:47:49,329 EPOCH 5437
2024-02-07 17:48:05,592 Epoch 5437: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 17:48:05,593 EPOCH 5438
2024-02-07 17:48:21,957 Epoch 5438: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:48:21,957 EPOCH 5439
2024-02-07 17:48:38,206 Epoch 5439: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:48:38,207 EPOCH 5440
2024-02-07 17:48:54,034 Epoch 5440: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:48:54,034 EPOCH 5441
2024-02-07 17:49:10,243 Epoch 5441: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:49:10,244 EPOCH 5442
2024-02-07 17:49:26,611 Epoch 5442: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:49:26,612 EPOCH 5443
2024-02-07 17:49:42,766 Epoch 5443: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:49:42,767 EPOCH 5444
2024-02-07 17:49:59,214 Epoch 5444: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 17:49:59,215 EPOCH 5445
2024-02-07 17:50:03,812 [Epoch: 5445 Step: 00049000] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:     1114 || Batch Translation Loss:   0.012863 => Txt Tokens per Sec:     2962 || Lr: 0.000050
2024-02-07 17:50:15,547 Epoch 5445: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:50:15,548 EPOCH 5446
2024-02-07 17:50:31,619 Epoch 5446: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 17:50:31,620 EPOCH 5447
2024-02-07 17:50:47,647 Epoch 5447: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 17:50:47,648 EPOCH 5448
2024-02-07 17:51:04,073 Epoch 5448: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 17:51:04,073 EPOCH 5449
2024-02-07 17:51:20,195 Epoch 5449: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 17:51:20,196 EPOCH 5450
2024-02-07 17:51:36,288 Epoch 5450: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 17:51:36,288 EPOCH 5451
2024-02-07 17:51:52,649 Epoch 5451: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:51:52,649 EPOCH 5452
2024-02-07 17:52:08,763 Epoch 5452: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:52:08,764 EPOCH 5453
2024-02-07 17:52:25,252 Epoch 5453: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:52:25,253 EPOCH 5454
2024-02-07 17:52:41,693 Epoch 5454: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 17:52:41,694 EPOCH 5455
2024-02-07 17:52:58,157 Epoch 5455: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:52:58,158 EPOCH 5456
2024-02-07 17:53:00,612 [Epoch: 5456 Step: 00049100] Batch Recognition Loss:   0.000118 => Gls Tokens per Sec:     2608 || Batch Translation Loss:   0.010411 => Txt Tokens per Sec:     7021 || Lr: 0.000050
2024-02-07 17:53:14,359 Epoch 5456: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 17:53:14,360 EPOCH 5457
2024-02-07 17:53:30,809 Epoch 5457: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:53:30,810 EPOCH 5458
2024-02-07 17:53:46,680 Epoch 5458: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 17:53:46,681 EPOCH 5459
2024-02-07 17:54:03,210 Epoch 5459: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 17:54:03,211 EPOCH 5460
2024-02-07 17:54:19,517 Epoch 5460: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 17:54:19,517 EPOCH 5461
2024-02-07 17:54:36,004 Epoch 5461: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 17:54:36,005 EPOCH 5462
2024-02-07 17:54:52,224 Epoch 5462: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:54:52,225 EPOCH 5463
2024-02-07 17:55:08,205 Epoch 5463: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 17:55:08,205 EPOCH 5464
2024-02-07 17:55:24,775 Epoch 5464: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 17:55:24,776 EPOCH 5465
2024-02-07 17:55:40,982 Epoch 5465: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:55:40,983 EPOCH 5466
2024-02-07 17:55:57,108 Epoch 5466: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 17:55:57,109 EPOCH 5467
2024-02-07 17:56:12,390 [Epoch: 5467 Step: 00049200] Batch Recognition Loss:   0.000113 => Gls Tokens per Sec:      444 || Batch Translation Loss:   0.011277 => Txt Tokens per Sec:     1342 || Lr: 0.000050
2024-02-07 17:56:13,527 Epoch 5467: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 17:56:13,527 EPOCH 5468
2024-02-07 17:56:29,735 Epoch 5468: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 17:56:29,736 EPOCH 5469
2024-02-07 17:56:45,827 Epoch 5469: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 17:56:45,829 EPOCH 5470
2024-02-07 17:57:02,080 Epoch 5470: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:57:02,080 EPOCH 5471
2024-02-07 17:57:18,393 Epoch 5471: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:57:18,394 EPOCH 5472
2024-02-07 17:57:35,051 Epoch 5472: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 17:57:35,052 EPOCH 5473
2024-02-07 17:57:51,232 Epoch 5473: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 17:57:51,233 EPOCH 5474
2024-02-07 17:58:07,529 Epoch 5474: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:58:07,529 EPOCH 5475
2024-02-07 17:58:23,859 Epoch 5475: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 17:58:23,859 EPOCH 5476
2024-02-07 17:58:40,197 Epoch 5476: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 17:58:40,199 EPOCH 5477
2024-02-07 17:58:56,778 Epoch 5477: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 17:58:56,779 EPOCH 5478
2024-02-07 17:59:06,599 [Epoch: 5478 Step: 00049300] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:      821 || Batch Translation Loss:   0.006786 => Txt Tokens per Sec:     2308 || Lr: 0.000050
2024-02-07 17:59:12,860 Epoch 5478: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 17:59:12,860 EPOCH 5479
2024-02-07 17:59:29,120 Epoch 5479: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 17:59:29,120 EPOCH 5480
2024-02-07 17:59:45,199 Epoch 5480: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 17:59:45,199 EPOCH 5481
2024-02-07 18:00:01,403 Epoch 5481: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 18:00:01,404 EPOCH 5482
2024-02-07 18:00:17,610 Epoch 5482: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 18:00:17,610 EPOCH 5483
2024-02-07 18:00:33,977 Epoch 5483: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 18:00:33,977 EPOCH 5484
2024-02-07 18:00:50,138 Epoch 5484: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 18:00:50,139 EPOCH 5485
2024-02-07 18:01:06,319 Epoch 5485: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 18:01:06,319 EPOCH 5486
2024-02-07 18:01:22,548 Epoch 5486: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 18:01:22,549 EPOCH 5487
2024-02-07 18:01:38,603 Epoch 5487: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 18:01:38,604 EPOCH 5488
2024-02-07 18:01:54,852 Epoch 5488: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 18:01:54,852 EPOCH 5489
2024-02-07 18:02:06,693 [Epoch: 5489 Step: 00049400] Batch Recognition Loss:   0.000330 => Gls Tokens per Sec:      865 || Batch Translation Loss:   0.008214 => Txt Tokens per Sec:     2367 || Lr: 0.000050
2024-02-07 18:02:11,044 Epoch 5489: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 18:02:11,044 EPOCH 5490
2024-02-07 18:02:27,438 Epoch 5490: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 18:02:27,439 EPOCH 5491
2024-02-07 18:02:43,640 Epoch 5491: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 18:02:43,641 EPOCH 5492
2024-02-07 18:02:59,868 Epoch 5492: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 18:02:59,869 EPOCH 5493
2024-02-07 18:03:16,158 Epoch 5493: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 18:03:16,159 EPOCH 5494
2024-02-07 18:03:32,208 Epoch 5494: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 18:03:32,208 EPOCH 5495
2024-02-07 18:03:48,863 Epoch 5495: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 18:03:48,863 EPOCH 5496
2024-02-07 18:04:04,855 Epoch 5496: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 18:04:04,856 EPOCH 5497
2024-02-07 18:04:21,078 Epoch 5497: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 18:04:21,078 EPOCH 5498
2024-02-07 18:04:37,095 Epoch 5498: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 18:04:37,095 EPOCH 5499
2024-02-07 18:04:53,604 Epoch 5499: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 18:04:53,604 EPOCH 5500
2024-02-07 18:05:09,825 [Epoch: 5500 Step: 00049500] Batch Recognition Loss:   0.000259 => Gls Tokens per Sec:      655 || Batch Translation Loss:   0.020997 => Txt Tokens per Sec:     1812 || Lr: 0.000050
2024-02-07 18:05:09,826 Epoch 5500: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 18:05:09,826 EPOCH 5501
2024-02-07 18:05:26,100 Epoch 5501: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 18:05:26,100 EPOCH 5502
2024-02-07 18:05:42,685 Epoch 5502: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 18:05:42,686 EPOCH 5503
2024-02-07 18:05:59,062 Epoch 5503: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 18:05:59,064 EPOCH 5504
2024-02-07 18:06:15,289 Epoch 5504: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 18:06:15,290 EPOCH 5505
2024-02-07 18:06:31,618 Epoch 5505: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:06:31,619 EPOCH 5506
2024-02-07 18:06:47,763 Epoch 5506: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 18:06:47,763 EPOCH 5507
2024-02-07 18:07:04,399 Epoch 5507: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 18:07:04,399 EPOCH 5508
2024-02-07 18:07:20,663 Epoch 5508: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 18:07:20,663 EPOCH 5509
2024-02-07 18:07:36,998 Epoch 5509: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 18:07:36,998 EPOCH 5510
2024-02-07 18:07:53,495 Epoch 5510: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-07 18:07:53,495 EPOCH 5511
2024-02-07 18:08:09,663 Epoch 5511: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-07 18:08:09,664 EPOCH 5512
2024-02-07 18:08:09,879 [Epoch: 5512 Step: 00049600] Batch Recognition Loss:   0.000334 => Gls Tokens per Sec:     5981 || Batch Translation Loss:   0.011353 => Txt Tokens per Sec:    10795 || Lr: 0.000050
2024-02-07 18:08:25,681 Epoch 5512: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-07 18:08:25,681 EPOCH 5513
2024-02-07 18:08:41,954 Epoch 5513: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-07 18:08:41,955 EPOCH 5514
2024-02-07 18:08:58,257 Epoch 5514: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-07 18:08:58,258 EPOCH 5515
2024-02-07 18:09:14,202 Epoch 5515: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 18:09:14,202 EPOCH 5516
2024-02-07 18:09:30,490 Epoch 5516: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 18:09:30,490 EPOCH 5517
2024-02-07 18:09:46,523 Epoch 5517: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 18:09:46,523 EPOCH 5518
2024-02-07 18:10:03,213 Epoch 5518: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 18:10:03,214 EPOCH 5519
2024-02-07 18:10:19,587 Epoch 5519: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 18:10:19,588 EPOCH 5520
2024-02-07 18:10:35,696 Epoch 5520: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-07 18:10:35,696 EPOCH 5521
2024-02-07 18:10:52,315 Epoch 5521: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 18:10:52,317 EPOCH 5522
2024-02-07 18:11:08,545 Epoch 5522: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 18:11:08,546 EPOCH 5523
2024-02-07 18:11:12,125 [Epoch: 5523 Step: 00049700] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:      715 || Batch Translation Loss:   0.017534 => Txt Tokens per Sec:     2179 || Lr: 0.000050
2024-02-07 18:11:24,655 Epoch 5523: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 18:11:24,656 EPOCH 5524
2024-02-07 18:11:41,397 Epoch 5524: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 18:11:41,397 EPOCH 5525
2024-02-07 18:11:58,077 Epoch 5525: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 18:11:58,077 EPOCH 5526
2024-02-07 18:12:14,755 Epoch 5526: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 18:12:14,756 EPOCH 5527
2024-02-07 18:12:31,150 Epoch 5527: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 18:12:31,150 EPOCH 5528
2024-02-07 18:12:47,434 Epoch 5528: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 18:12:47,435 EPOCH 5529
2024-02-07 18:13:04,001 Epoch 5529: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 18:13:04,002 EPOCH 5530
2024-02-07 18:13:20,254 Epoch 5530: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 18:13:20,254 EPOCH 5531
2024-02-07 18:13:36,371 Epoch 5531: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 18:13:36,371 EPOCH 5532
2024-02-07 18:13:52,565 Epoch 5532: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:13:52,565 EPOCH 5533
2024-02-07 18:14:09,021 Epoch 5533: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:14:09,021 EPOCH 5534
2024-02-07 18:14:12,857 [Epoch: 5534 Step: 00049800] Batch Recognition Loss:   0.000292 => Gls Tokens per Sec:     1002 || Batch Translation Loss:   0.016686 => Txt Tokens per Sec:     2651 || Lr: 0.000050
2024-02-07 18:14:25,329 Epoch 5534: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 18:14:25,329 EPOCH 5535
2024-02-07 18:14:41,652 Epoch 5535: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:14:41,652 EPOCH 5536
2024-02-07 18:14:57,517 Epoch 5536: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 18:14:57,519 EPOCH 5537
2024-02-07 18:15:13,646 Epoch 5537: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 18:15:13,647 EPOCH 5538
2024-02-07 18:15:29,813 Epoch 5538: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:15:29,813 EPOCH 5539
2024-02-07 18:15:46,085 Epoch 5539: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:15:46,086 EPOCH 5540
2024-02-07 18:16:02,397 Epoch 5540: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:16:02,398 EPOCH 5541
2024-02-07 18:16:18,569 Epoch 5541: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:16:18,569 EPOCH 5542
2024-02-07 18:16:35,071 Epoch 5542: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:16:35,072 EPOCH 5543
2024-02-07 18:16:51,592 Epoch 5543: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:16:51,593 EPOCH 5544
2024-02-07 18:17:08,483 Epoch 5544: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:17:08,483 EPOCH 5545
2024-02-07 18:17:09,919 [Epoch: 5545 Step: 00049900] Batch Recognition Loss:   0.000169 => Gls Tokens per Sec:     3569 || Batch Translation Loss:   0.016332 => Txt Tokens per Sec:     8633 || Lr: 0.000050
2024-02-07 18:17:24,624 Epoch 5545: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:17:24,625 EPOCH 5546
2024-02-07 18:17:40,715 Epoch 5546: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:17:40,716 EPOCH 5547
2024-02-07 18:17:56,859 Epoch 5547: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:17:56,861 EPOCH 5548
2024-02-07 18:18:13,449 Epoch 5548: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:18:13,450 EPOCH 5549
2024-02-07 18:18:29,514 Epoch 5549: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:18:29,515 EPOCH 5550
2024-02-07 18:18:45,461 Epoch 5550: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 18:18:45,462 EPOCH 5551
2024-02-07 18:19:01,576 Epoch 5551: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 18:19:01,577 EPOCH 5552
2024-02-07 18:19:18,362 Epoch 5552: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:19:18,363 EPOCH 5553
2024-02-07 18:19:34,360 Epoch 5553: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:19:34,361 EPOCH 5554
2024-02-07 18:19:50,952 Epoch 5554: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:19:50,954 EPOCH 5555
2024-02-07 18:20:07,284 Epoch 5555: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:20:07,285 EPOCH 5556
2024-02-07 18:20:18,798 [Epoch: 5556 Step: 00050000] Batch Recognition Loss:   0.000151 => Gls Tokens per Sec:      478 || Batch Translation Loss:   0.009683 => Txt Tokens per Sec:     1291 || Lr: 0.000050
2024-02-07 18:21:26,802 Validation result at epoch 5556, step    50000: duration: 68.0027s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.26571	Translation Loss: 104185.42188	PPL: 33059.17969
	Eval Metric: BLEU
	WER 2.05	(DEL: 0.00,	INS: 0.00,	SUB: 2.05)
	BLEU-4 0.59	(BLEU-1: 10.19,	BLEU-2: 2.84,	BLEU-3: 1.13,	BLEU-4: 0.59)
	CHRF 16.84	ROUGE 8.50
2024-02-07 18:21:26,804 Logging Recognition and Translation Outputs
2024-02-07 18:21:26,804 ========================================================================================================================
2024-02-07 18:21:26,804 Logging Sequence: 178_157.00
2024-02-07 18:21:26,805 	Gloss Reference :	A B+C+D+E
2024-02-07 18:21:26,805 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 18:21:26,805 	Gloss Alignment :	         
2024-02-07 18:21:26,806 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 18:21:26,807 	Text Reference  :	this is why sushil kumar will   have  to  be      arrested
2024-02-07 18:21:26,807 	Text Hypothesis :	**** ** *** ****** the   couple could not believe that    
2024-02-07 18:21:26,807 	Text Alignment  :	D    D  D   D      S     S      S     S   S       S       
2024-02-07 18:21:26,807 ========================================================================================================================
2024-02-07 18:21:26,808 Logging Sequence: 118_111.00
2024-02-07 18:21:26,808 	Gloss Reference :	A B+C+D+E
2024-02-07 18:21:26,808 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 18:21:26,808 	Gloss Alignment :	         
2024-02-07 18:21:26,808 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 18:21:26,809 	Text Reference  :	and people encourage him have      hope      for         the   next    world cup    
2024-02-07 18:21:26,809 	Text Hypothesis :	*** ****** but       the pakistani batsmen's belligerant shots secured their victory
2024-02-07 18:21:26,809 	Text Alignment  :	D   D      S         S   S         S         S           S     S       S     S      
2024-02-07 18:21:26,809 ========================================================================================================================
2024-02-07 18:21:26,810 Logging Sequence: 148_2.00
2024-02-07 18:21:26,810 	Gloss Reference :	A B+C+D+E
2024-02-07 18:21:26,810 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 18:21:26,810 	Gloss Alignment :	         
2024-02-07 18:21:26,810 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 18:21:26,812 	Text Reference  :	the final of the asia cup 2023   cricket tournament was     played between india and sri lanka *** on     17th september 2023
2024-02-07 18:21:26,812 	Text Hypothesis :	*** ***** ** *** **** *** scored 1       day        earlier played between india and sri lanka had scored 6    balls     it  
2024-02-07 18:21:26,812 	Text Alignment  :	D   D     D  D   D    D   S      S       S          S                                          I   S      S    S         S   
2024-02-07 18:21:26,812 ========================================================================================================================
2024-02-07 18:21:26,812 Logging Sequence: 83_129.00
2024-02-07 18:21:26,813 	Gloss Reference :	A B+C+D+E
2024-02-07 18:21:26,813 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 18:21:26,813 	Gloss Alignment :	         
2024-02-07 18:21:26,813 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 18:21:26,814 	Text Reference  :	later the *** **** ***** **** ** ** * denmark football association tweeted
2024-02-07 18:21:26,814 	Text Hypothesis :	and   the too were being held in no 2 more    than     its         head   
2024-02-07 18:21:26,814 	Text Alignment  :	S         I   I    I     I    I  I  I S       S        S           S      
2024-02-07 18:21:26,814 ========================================================================================================================
2024-02-07 18:21:26,814 Logging Sequence: 99_158.00
2024-02-07 18:21:26,815 	Gloss Reference :	A B+C+D+E
2024-02-07 18:21:26,815 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 18:21:26,815 	Gloss Alignment :	         
2024-02-07 18:21:26,815 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 18:21:26,816 	Text Reference  :	**** ***** *** the     incident occured in  dubai   and it was extremely shameful
2024-02-07 18:21:26,816 	Text Hypothesis :	then kohli and gambhir do       with    the protest and ** did not       athletes
2024-02-07 18:21:26,816 	Text Alignment  :	I    I     I   S       S        S       S   S           D  S   S         S       
2024-02-07 18:21:26,817 ========================================================================================================================
2024-02-07 18:21:32,085 Epoch 5556: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:21:32,085 EPOCH 5557
2024-02-07 18:21:48,816 Epoch 5557: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:21:48,817 EPOCH 5558
2024-02-07 18:22:05,215 Epoch 5558: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:22:05,216 EPOCH 5559
2024-02-07 18:22:21,490 Epoch 5559: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:22:21,491 EPOCH 5560
2024-02-07 18:22:37,502 Epoch 5560: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:22:37,503 EPOCH 5561
2024-02-07 18:22:53,830 Epoch 5561: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:22:53,830 EPOCH 5562
2024-02-07 18:23:10,450 Epoch 5562: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:23:10,451 EPOCH 5563
2024-02-07 18:23:26,420 Epoch 5563: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:23:26,421 EPOCH 5564
2024-02-07 18:23:43,023 Epoch 5564: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:23:43,024 EPOCH 5565
2024-02-07 18:23:59,065 Epoch 5565: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:23:59,066 EPOCH 5566
2024-02-07 18:24:15,212 Epoch 5566: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:24:15,213 EPOCH 5567
2024-02-07 18:24:20,782 [Epoch: 5567 Step: 00050100] Batch Recognition Loss:   0.000121 => Gls Tokens per Sec:     1379 || Batch Translation Loss:   0.011094 => Txt Tokens per Sec:     3653 || Lr: 0.000050
2024-02-07 18:24:31,419 Epoch 5567: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:24:31,420 EPOCH 5568
2024-02-07 18:24:47,720 Epoch 5568: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:24:47,721 EPOCH 5569
2024-02-07 18:25:03,772 Epoch 5569: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:25:03,773 EPOCH 5570
2024-02-07 18:25:20,305 Epoch 5570: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:25:20,306 EPOCH 5571
2024-02-07 18:25:36,432 Epoch 5571: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:25:36,433 EPOCH 5572
2024-02-07 18:25:52,587 Epoch 5572: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 18:25:52,587 EPOCH 5573
2024-02-07 18:26:09,147 Epoch 5573: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 18:26:09,148 EPOCH 5574
2024-02-07 18:26:25,283 Epoch 5574: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 18:26:25,284 EPOCH 5575
2024-02-07 18:26:41,543 Epoch 5575: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 18:26:41,544 EPOCH 5576
2024-02-07 18:26:57,750 Epoch 5576: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 18:26:57,751 EPOCH 5577
2024-02-07 18:27:13,903 Epoch 5577: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 18:27:13,904 EPOCH 5578
2024-02-07 18:27:29,207 [Epoch: 5578 Step: 00050200] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:      527 || Batch Translation Loss:   0.016857 => Txt Tokens per Sec:     1481 || Lr: 0.000050
2024-02-07 18:27:30,219 Epoch 5578: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 18:27:30,220 EPOCH 5579
2024-02-07 18:27:46,151 Epoch 5579: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 18:27:46,152 EPOCH 5580
2024-02-07 18:28:02,302 Epoch 5580: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 18:28:02,302 EPOCH 5581
2024-02-07 18:28:18,591 Epoch 5581: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 18:28:18,591 EPOCH 5582
2024-02-07 18:28:34,742 Epoch 5582: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 18:28:34,743 EPOCH 5583
2024-02-07 18:28:50,949 Epoch 5583: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 18:28:50,950 EPOCH 5584
2024-02-07 18:29:07,225 Epoch 5584: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 18:29:07,226 EPOCH 5585
2024-02-07 18:29:22,906 Epoch 5585: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 18:29:22,906 EPOCH 5586
2024-02-07 18:29:38,942 Epoch 5586: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 18:29:38,942 EPOCH 5587
2024-02-07 18:29:55,181 Epoch 5587: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 18:29:55,181 EPOCH 5588
2024-02-07 18:30:11,734 Epoch 5588: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 18:30:11,734 EPOCH 5589
2024-02-07 18:30:23,424 [Epoch: 5589 Step: 00050300] Batch Recognition Loss:   0.000376 => Gls Tokens per Sec:      876 || Batch Translation Loss:   0.039975 => Txt Tokens per Sec:     2397 || Lr: 0.000050
2024-02-07 18:30:27,731 Epoch 5589: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 18:30:27,731 EPOCH 5590
2024-02-07 18:30:44,335 Epoch 5590: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 18:30:44,335 EPOCH 5591
2024-02-07 18:31:00,559 Epoch 5591: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 18:31:00,560 EPOCH 5592
2024-02-07 18:31:16,631 Epoch 5592: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 18:31:16,631 EPOCH 5593
2024-02-07 18:31:32,834 Epoch 5593: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 18:31:32,835 EPOCH 5594
2024-02-07 18:31:48,968 Epoch 5594: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 18:31:48,969 EPOCH 5595
2024-02-07 18:32:04,850 Epoch 5595: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 18:32:04,852 EPOCH 5596
2024-02-07 18:32:21,052 Epoch 5596: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 18:32:21,053 EPOCH 5597
2024-02-07 18:32:37,479 Epoch 5597: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 18:32:37,480 EPOCH 5598
2024-02-07 18:32:53,884 Epoch 5598: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 18:32:53,885 EPOCH 5599
2024-02-07 18:33:10,549 Epoch 5599: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 18:33:10,550 EPOCH 5600
2024-02-07 18:33:26,644 [Epoch: 5600 Step: 00050400] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:      660 || Batch Translation Loss:   0.017080 => Txt Tokens per Sec:     1826 || Lr: 0.000050
2024-02-07 18:33:26,645 Epoch 5600: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:33:26,645 EPOCH 5601
2024-02-07 18:33:42,686 Epoch 5601: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:33:42,686 EPOCH 5602
2024-02-07 18:33:59,222 Epoch 5602: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:33:59,223 EPOCH 5603
2024-02-07 18:34:15,302 Epoch 5603: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:34:15,302 EPOCH 5604
2024-02-07 18:34:31,388 Epoch 5604: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:34:31,389 EPOCH 5605
2024-02-07 18:34:47,611 Epoch 5605: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:34:47,612 EPOCH 5606
2024-02-07 18:35:03,777 Epoch 5606: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:35:03,778 EPOCH 5607
2024-02-07 18:35:20,250 Epoch 5607: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:35:20,251 EPOCH 5608
2024-02-07 18:35:36,757 Epoch 5608: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:35:36,757 EPOCH 5609
2024-02-07 18:35:52,807 Epoch 5609: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 18:35:52,808 EPOCH 5610
2024-02-07 18:36:09,421 Epoch 5610: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-07 18:36:09,423 EPOCH 5611
2024-02-07 18:36:25,880 Epoch 5611: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-07 18:36:25,881 EPOCH 5612
2024-02-07 18:36:26,324 [Epoch: 5612 Step: 00050500] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:     2902 || Batch Translation Loss:   0.093468 => Txt Tokens per Sec:     8054 || Lr: 0.000050
2024-02-07 18:36:42,058 Epoch 5612: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-07 18:36:42,059 EPOCH 5613
2024-02-07 18:36:58,445 Epoch 5613: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-07 18:36:58,446 EPOCH 5614
2024-02-07 18:37:14,579 Epoch 5614: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-07 18:37:14,580 EPOCH 5615
2024-02-07 18:37:30,749 Epoch 5615: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-07 18:37:30,750 EPOCH 5616
2024-02-07 18:37:47,049 Epoch 5616: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-07 18:37:47,050 EPOCH 5617
2024-02-07 18:38:03,254 Epoch 5617: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-07 18:38:03,255 EPOCH 5618
2024-02-07 18:38:19,427 Epoch 5618: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-07 18:38:19,428 EPOCH 5619
2024-02-07 18:38:35,552 Epoch 5619: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-07 18:38:35,553 EPOCH 5620
2024-02-07 18:38:51,684 Epoch 5620: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-07 18:38:51,685 EPOCH 5621
2024-02-07 18:39:07,821 Epoch 5621: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-07 18:39:07,822 EPOCH 5622
2024-02-07 18:39:23,919 Epoch 5622: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-07 18:39:23,920 EPOCH 5623
2024-02-07 18:39:24,941 [Epoch: 5623 Step: 00050600] Batch Recognition Loss:   0.000350 => Gls Tokens per Sec:     2511 || Batch Translation Loss:   0.038463 => Txt Tokens per Sec:     6848 || Lr: 0.000050
2024-02-07 18:39:40,193 Epoch 5623: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-07 18:39:40,194 EPOCH 5624
2024-02-07 18:39:56,470 Epoch 5624: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-07 18:39:56,470 EPOCH 5625
2024-02-07 18:40:12,546 Epoch 5625: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 18:40:12,547 EPOCH 5626
2024-02-07 18:40:28,735 Epoch 5626: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 18:40:28,735 EPOCH 5627
2024-02-07 18:40:44,934 Epoch 5627: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 18:40:44,935 EPOCH 5628
2024-02-07 18:41:01,131 Epoch 5628: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 18:41:01,132 EPOCH 5629
2024-02-07 18:41:17,158 Epoch 5629: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 18:41:17,159 EPOCH 5630
2024-02-07 18:41:33,383 Epoch 5630: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 18:41:33,384 EPOCH 5631
2024-02-07 18:41:49,829 Epoch 5631: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 18:41:49,830 EPOCH 5632
2024-02-07 18:42:06,271 Epoch 5632: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 18:42:06,272 EPOCH 5633
2024-02-07 18:42:22,519 Epoch 5633: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 18:42:22,520 EPOCH 5634
2024-02-07 18:42:28,126 [Epoch: 5634 Step: 00050700] Batch Recognition Loss:   0.000208 => Gls Tokens per Sec:      525 || Batch Translation Loss:   0.022828 => Txt Tokens per Sec:     1566 || Lr: 0.000050
2024-02-07 18:42:38,730 Epoch 5634: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 18:42:38,731 EPOCH 5635
2024-02-07 18:42:55,117 Epoch 5635: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 18:42:55,117 EPOCH 5636
2024-02-07 18:43:11,219 Epoch 5636: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:43:11,220 EPOCH 5637
2024-02-07 18:43:27,442 Epoch 5637: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:43:27,444 EPOCH 5638
2024-02-07 18:43:43,814 Epoch 5638: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:43:43,815 EPOCH 5639
2024-02-07 18:43:59,832 Epoch 5639: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:43:59,833 EPOCH 5640
2024-02-07 18:44:16,013 Epoch 5640: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:44:16,014 EPOCH 5641
2024-02-07 18:44:32,281 Epoch 5641: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:44:32,281 EPOCH 5642
2024-02-07 18:44:48,444 Epoch 5642: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:44:48,445 EPOCH 5643
2024-02-07 18:45:04,631 Epoch 5643: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:45:04,632 EPOCH 5644
2024-02-07 18:45:20,798 Epoch 5644: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:45:20,799 EPOCH 5645
2024-02-07 18:45:26,974 [Epoch: 5645 Step: 00050800] Batch Recognition Loss:   0.000330 => Gls Tokens per Sec:      683 || Batch Translation Loss:   0.006248 => Txt Tokens per Sec:     2014 || Lr: 0.000050
2024-02-07 18:45:37,150 Epoch 5645: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:45:37,150 EPOCH 5646
2024-02-07 18:45:53,323 Epoch 5646: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:45:53,323 EPOCH 5647
2024-02-07 18:46:09,415 Epoch 5647: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:46:09,416 EPOCH 5648
2024-02-07 18:46:25,860 Epoch 5648: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:46:25,861 EPOCH 5649
2024-02-07 18:46:42,074 Epoch 5649: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:46:42,075 EPOCH 5650
2024-02-07 18:46:58,191 Epoch 5650: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:46:58,192 EPOCH 5651
2024-02-07 18:47:14,237 Epoch 5651: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 18:47:14,237 EPOCH 5652
2024-02-07 18:47:30,316 Epoch 5652: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 18:47:30,317 EPOCH 5653
2024-02-07 18:47:46,625 Epoch 5653: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 18:47:46,626 EPOCH 5654
2024-02-07 18:48:03,174 Epoch 5654: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:48:03,174 EPOCH 5655
2024-02-07 18:48:19,409 Epoch 5655: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 18:48:19,409 EPOCH 5656
2024-02-07 18:48:27,077 [Epoch: 5656 Step: 00050900] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:      835 || Batch Translation Loss:   0.009938 => Txt Tokens per Sec:     2159 || Lr: 0.000050
2024-02-07 18:48:35,972 Epoch 5656: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:48:35,972 EPOCH 5657
2024-02-07 18:48:52,380 Epoch 5657: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:48:52,381 EPOCH 5658
2024-02-07 18:49:08,765 Epoch 5658: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:49:08,766 EPOCH 5659
2024-02-07 18:49:24,999 Epoch 5659: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:49:24,999 EPOCH 5660
2024-02-07 18:49:41,223 Epoch 5660: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:49:41,223 EPOCH 5661
2024-02-07 18:49:57,476 Epoch 5661: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:49:57,477 EPOCH 5662
2024-02-07 18:50:14,049 Epoch 5662: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 18:50:14,049 EPOCH 5663
2024-02-07 18:50:30,465 Epoch 5663: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 18:50:30,465 EPOCH 5664
2024-02-07 18:50:46,692 Epoch 5664: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:50:46,692 EPOCH 5665
2024-02-07 18:51:02,839 Epoch 5665: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:51:02,839 EPOCH 5666
2024-02-07 18:51:19,138 Epoch 5666: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:51:19,139 EPOCH 5667
2024-02-07 18:51:29,651 [Epoch: 5667 Step: 00051000] Batch Recognition Loss:   0.000484 => Gls Tokens per Sec:      731 || Batch Translation Loss:   0.014601 => Txt Tokens per Sec:     1961 || Lr: 0.000050
2024-02-07 18:51:35,256 Epoch 5667: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:51:35,257 EPOCH 5668
2024-02-07 18:51:51,267 Epoch 5668: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:51:51,267 EPOCH 5669
2024-02-07 18:52:07,676 Epoch 5669: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 18:52:07,676 EPOCH 5670
2024-02-07 18:52:24,063 Epoch 5670: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 18:52:24,063 EPOCH 5671
2024-02-07 18:52:40,296 Epoch 5671: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 18:52:40,297 EPOCH 5672
2024-02-07 18:52:56,477 Epoch 5672: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 18:52:56,478 EPOCH 5673
2024-02-07 18:53:12,655 Epoch 5673: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 18:53:12,655 EPOCH 5674
2024-02-07 18:53:28,839 Epoch 5674: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:53:28,839 EPOCH 5675
2024-02-07 18:53:44,999 Epoch 5675: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:53:44,999 EPOCH 5676
2024-02-07 18:54:01,521 Epoch 5676: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:54:01,522 EPOCH 5677
2024-02-07 18:54:17,938 Epoch 5677: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:54:17,939 EPOCH 5678
2024-02-07 18:54:29,602 [Epoch: 5678 Step: 00051100] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:      768 || Batch Translation Loss:   0.011549 => Txt Tokens per Sec:     2123 || Lr: 0.000050
2024-02-07 18:54:34,346 Epoch 5678: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:54:34,347 EPOCH 5679
2024-02-07 18:54:50,566 Epoch 5679: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:54:50,566 EPOCH 5680
2024-02-07 18:55:06,889 Epoch 5680: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:55:06,889 EPOCH 5681
2024-02-07 18:55:23,028 Epoch 5681: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:55:23,029 EPOCH 5682
2024-02-07 18:55:39,106 Epoch 5682: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:55:39,107 EPOCH 5683
2024-02-07 18:55:55,124 Epoch 5683: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:55:55,125 EPOCH 5684
2024-02-07 18:56:11,412 Epoch 5684: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:56:11,413 EPOCH 5685
2024-02-07 18:56:27,438 Epoch 5685: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:56:27,439 EPOCH 5686
2024-02-07 18:56:43,577 Epoch 5686: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:56:43,577 EPOCH 5687
2024-02-07 18:57:00,154 Epoch 5687: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:57:00,155 EPOCH 5688
2024-02-07 18:57:16,654 Epoch 5688: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 18:57:16,655 EPOCH 5689
2024-02-07 18:57:28,422 [Epoch: 5689 Step: 00051200] Batch Recognition Loss:   0.000627 => Gls Tokens per Sec:      870 || Batch Translation Loss:   0.017549 => Txt Tokens per Sec:     2382 || Lr: 0.000050
2024-02-07 18:57:32,753 Epoch 5689: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:57:32,754 EPOCH 5690
2024-02-07 18:57:48,909 Epoch 5690: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:57:48,909 EPOCH 5691
2024-02-07 18:58:05,334 Epoch 5691: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 18:58:05,334 EPOCH 5692
2024-02-07 18:58:21,464 Epoch 5692: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 18:58:21,465 EPOCH 5693
2024-02-07 18:58:37,877 Epoch 5693: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 18:58:37,878 EPOCH 5694
2024-02-07 18:58:53,952 Epoch 5694: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 18:58:53,952 EPOCH 5695
2024-02-07 18:59:10,146 Epoch 5695: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 18:59:10,147 EPOCH 5696
2024-02-07 18:59:26,540 Epoch 5696: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 18:59:26,541 EPOCH 5697
2024-02-07 18:59:42,978 Epoch 5697: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:59:42,979 EPOCH 5698
2024-02-07 18:59:59,301 Epoch 5698: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 18:59:59,302 EPOCH 5699
2024-02-07 19:00:15,469 Epoch 5699: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:00:15,469 EPOCH 5700
2024-02-07 19:00:31,701 [Epoch: 5700 Step: 00051300] Batch Recognition Loss:   0.001603 => Gls Tokens per Sec:      654 || Batch Translation Loss:   0.011686 => Txt Tokens per Sec:     1810 || Lr: 0.000050
2024-02-07 19:00:31,701 Epoch 5700: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 19:00:31,701 EPOCH 5701
2024-02-07 19:00:47,910 Epoch 5701: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 19:00:47,910 EPOCH 5702
2024-02-07 19:01:04,092 Epoch 5702: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 19:01:04,093 EPOCH 5703
2024-02-07 19:01:20,147 Epoch 5703: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 19:01:20,148 EPOCH 5704
2024-02-07 19:01:36,723 Epoch 5704: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 19:01:36,723 EPOCH 5705
2024-02-07 19:01:52,795 Epoch 5705: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 19:01:52,795 EPOCH 5706
2024-02-07 19:02:09,044 Epoch 5706: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 19:02:09,045 EPOCH 5707
2024-02-07 19:02:25,168 Epoch 5707: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-07 19:02:25,169 EPOCH 5708
2024-02-07 19:02:41,643 Epoch 5708: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-07 19:02:41,644 EPOCH 5709
2024-02-07 19:02:58,261 Epoch 5709: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-07 19:02:58,261 EPOCH 5710
2024-02-07 19:03:14,727 Epoch 5710: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 19:03:14,727 EPOCH 5711
2024-02-07 19:03:31,150 Epoch 5711: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 19:03:31,150 EPOCH 5712
2024-02-07 19:03:35,712 [Epoch: 5712 Step: 00051400] Batch Recognition Loss:   0.000370 => Gls Tokens per Sec:       83 || Batch Translation Loss:   0.008985 => Txt Tokens per Sec:      296 || Lr: 0.000050
2024-02-07 19:03:47,846 Epoch 5712: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-07 19:03:47,847 EPOCH 5713
2024-02-07 19:04:04,242 Epoch 5713: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 19:04:04,242 EPOCH 5714
2024-02-07 19:04:20,556 Epoch 5714: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 19:04:20,556 EPOCH 5715
2024-02-07 19:04:36,652 Epoch 5715: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 19:04:36,652 EPOCH 5716
2024-02-07 19:04:53,171 Epoch 5716: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 19:04:53,171 EPOCH 5717
2024-02-07 19:05:09,375 Epoch 5717: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 19:05:09,375 EPOCH 5718
2024-02-07 19:05:25,501 Epoch 5718: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 19:05:25,501 EPOCH 5719
2024-02-07 19:05:41,935 Epoch 5719: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 19:05:41,936 EPOCH 5720
2024-02-07 19:05:58,205 Epoch 5720: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 19:05:58,206 EPOCH 5721
2024-02-07 19:06:14,178 Epoch 5721: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 19:06:14,178 EPOCH 5722
2024-02-07 19:06:30,633 Epoch 5722: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 19:06:30,634 EPOCH 5723
2024-02-07 19:06:38,028 [Epoch: 5723 Step: 00051500] Batch Recognition Loss:   0.000262 => Gls Tokens per Sec:      225 || Batch Translation Loss:   0.018039 => Txt Tokens per Sec:      735 || Lr: 0.000050
2024-02-07 19:06:47,047 Epoch 5723: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 19:06:47,048 EPOCH 5724
2024-02-07 19:07:03,090 Epoch 5724: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 19:07:03,090 EPOCH 5725
2024-02-07 19:07:19,613 Epoch 5725: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 19:07:19,613 EPOCH 5726
2024-02-07 19:07:35,872 Epoch 5726: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 19:07:35,873 EPOCH 5727
2024-02-07 19:07:52,013 Epoch 5727: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 19:07:52,013 EPOCH 5728
2024-02-07 19:08:08,650 Epoch 5728: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 19:08:08,651 EPOCH 5729
2024-02-07 19:08:24,676 Epoch 5729: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 19:08:24,677 EPOCH 5730
2024-02-07 19:08:40,880 Epoch 5730: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 19:08:40,881 EPOCH 5731
2024-02-07 19:08:57,449 Epoch 5731: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 19:08:57,449 EPOCH 5732
2024-02-07 19:09:13,780 Epoch 5732: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 19:09:13,781 EPOCH 5733
2024-02-07 19:09:29,957 Epoch 5733: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:09:29,958 EPOCH 5734
2024-02-07 19:09:40,704 [Epoch: 5734 Step: 00051600] Batch Recognition Loss:   0.000534 => Gls Tokens per Sec:      274 || Batch Translation Loss:   0.018890 => Txt Tokens per Sec:      860 || Lr: 0.000050
2024-02-07 19:09:46,233 Epoch 5734: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 19:09:46,234 EPOCH 5735
2024-02-07 19:10:02,033 Epoch 5735: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:10:02,034 EPOCH 5736
2024-02-07 19:10:18,111 Epoch 5736: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:10:18,111 EPOCH 5737
2024-02-07 19:10:34,529 Epoch 5737: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 19:10:34,530 EPOCH 5738
2024-02-07 19:10:50,691 Epoch 5738: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 19:10:50,692 EPOCH 5739
2024-02-07 19:11:06,687 Epoch 5739: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 19:11:06,687 EPOCH 5740
2024-02-07 19:11:23,075 Epoch 5740: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:11:23,076 EPOCH 5741
2024-02-07 19:11:39,242 Epoch 5741: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:11:39,243 EPOCH 5742
2024-02-07 19:11:56,493 Epoch 5742: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:11:56,494 EPOCH 5743
2024-02-07 19:12:12,612 Epoch 5743: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:12:12,613 EPOCH 5744
2024-02-07 19:12:28,674 Epoch 5744: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 19:12:28,675 EPOCH 5745
2024-02-07 19:12:36,920 [Epoch: 5745 Step: 00051700] Batch Recognition Loss:   0.000276 => Gls Tokens per Sec:      512 || Batch Translation Loss:   0.008828 => Txt Tokens per Sec:     1399 || Lr: 0.000050
2024-02-07 19:12:45,014 Epoch 5745: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-07 19:12:45,014 EPOCH 5746
2024-02-07 19:13:01,521 Epoch 5746: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 19:13:01,522 EPOCH 5747
2024-02-07 19:13:17,532 Epoch 5747: Total Training Recognition Loss 0.00  Total Training Translation Loss 2.43 
2024-02-07 19:13:17,533 EPOCH 5748
2024-02-07 19:13:33,788 Epoch 5748: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.52 
2024-02-07 19:13:33,789 EPOCH 5749
2024-02-07 19:13:50,280 Epoch 5749: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.01 
2024-02-07 19:13:50,283 EPOCH 5750
2024-02-07 19:14:06,249 Epoch 5750: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.55 
2024-02-07 19:14:06,249 EPOCH 5751
2024-02-07 19:14:22,520 Epoch 5751: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-07 19:14:22,521 EPOCH 5752
2024-02-07 19:14:38,666 Epoch 5752: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-07 19:14:38,668 EPOCH 5753
2024-02-07 19:14:54,569 Epoch 5753: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 19:14:54,569 EPOCH 5754
2024-02-07 19:15:10,816 Epoch 5754: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 19:15:10,817 EPOCH 5755
2024-02-07 19:15:26,889 Epoch 5755: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 19:15:26,890 EPOCH 5756
2024-02-07 19:15:41,357 [Epoch: 5756 Step: 00051800] Batch Recognition Loss:   0.000504 => Gls Tokens per Sec:      380 || Batch Translation Loss:   0.021036 => Txt Tokens per Sec:     1164 || Lr: 0.000050
2024-02-07 19:15:43,115 Epoch 5756: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 19:15:43,115 EPOCH 5757
2024-02-07 19:15:59,222 Epoch 5757: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 19:15:59,223 EPOCH 5758
2024-02-07 19:16:15,693 Epoch 5758: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 19:16:15,694 EPOCH 5759
2024-02-07 19:16:32,239 Epoch 5759: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 19:16:32,240 EPOCH 5760
2024-02-07 19:16:48,187 Epoch 5760: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 19:16:48,188 EPOCH 5761
2024-02-07 19:17:04,841 Epoch 5761: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 19:17:04,842 EPOCH 5762
2024-02-07 19:17:20,955 Epoch 5762: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 19:17:20,956 EPOCH 5763
2024-02-07 19:17:37,232 Epoch 5763: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 19:17:37,232 EPOCH 5764
2024-02-07 19:17:53,342 Epoch 5764: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 19:17:53,343 EPOCH 5765
2024-02-07 19:18:09,573 Epoch 5765: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 19:18:09,574 EPOCH 5766
2024-02-07 19:18:25,804 Epoch 5766: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:18:25,805 EPOCH 5767
2024-02-07 19:18:34,944 [Epoch: 5767 Step: 00051900] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:      742 || Batch Translation Loss:   0.008759 => Txt Tokens per Sec:     1945 || Lr: 0.000050
2024-02-07 19:18:41,760 Epoch 5767: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 19:18:41,760 EPOCH 5768
2024-02-07 19:18:57,784 Epoch 5768: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 19:18:57,785 EPOCH 5769
2024-02-07 19:19:14,221 Epoch 5769: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 19:19:14,222 EPOCH 5770
2024-02-07 19:19:30,447 Epoch 5770: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 19:19:30,448 EPOCH 5771
2024-02-07 19:19:46,426 Epoch 5771: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 19:19:46,427 EPOCH 5772
2024-02-07 19:20:02,715 Epoch 5772: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 19:20:02,716 EPOCH 5773
2024-02-07 19:20:18,834 Epoch 5773: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 19:20:18,835 EPOCH 5774
2024-02-07 19:20:34,782 Epoch 5774: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 19:20:34,782 EPOCH 5775
2024-02-07 19:20:51,083 Epoch 5775: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 19:20:51,084 EPOCH 5776
2024-02-07 19:21:07,328 Epoch 5776: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:21:07,328 EPOCH 5777
2024-02-07 19:21:23,456 Epoch 5777: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:21:23,457 EPOCH 5778
2024-02-07 19:21:38,606 [Epoch: 5778 Step: 00052000] Batch Recognition Loss:   0.000422 => Gls Tokens per Sec:      532 || Batch Translation Loss:   0.017792 => Txt Tokens per Sec:     1535 || Lr: 0.000050
2024-02-07 19:22:46,551 Validation result at epoch 5778, step    52000: duration: 67.9445s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.25643	Translation Loss: 105872.86719	PPL: 39128.10938
	Eval Metric: BLEU
	WER 2.12	(DEL: 0.00,	INS: 0.00,	SUB: 2.12)
	BLEU-4 0.33	(BLEU-1: 10.07,	BLEU-2: 2.83,	BLEU-3: 0.93,	BLEU-4: 0.33)
	CHRF 16.40	ROUGE 8.54
2024-02-07 19:22:46,553 Logging Recognition and Translation Outputs
2024-02-07 19:22:46,553 ========================================================================================================================
2024-02-07 19:22:46,554 Logging Sequence: 59_101.00
2024-02-07 19:22:46,554 	Gloss Reference :	A B+C+D+E
2024-02-07 19:22:46,554 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 19:22:46,554 	Gloss Alignment :	         
2024-02-07 19:22:46,554 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 19:22:46,555 	Text Reference  :	did you see the video fox said she won her medals    because    of a       condom  and   is     very happy
2024-02-07 19:22:46,556 	Text Hypothesis :	*** *** *** *** ***** *** **** *** *** a   wrestling federation of india's skipper rohit sharma 14   overs
2024-02-07 19:22:46,556 	Text Alignment  :	D   D   D   D   D     D   D    D   D   S   S         S             S       S       S     S      S    S    
2024-02-07 19:22:46,556 ========================================================================================================================
2024-02-07 19:22:46,556 Logging Sequence: 103_112.00
2024-02-07 19:22:46,556 	Gloss Reference :	A B+C+D+E
2024-02-07 19:22:46,556 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 19:22:46,557 	Gloss Alignment :	         
2024-02-07 19:22:46,557 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 19:22:46,558 	Text Reference  :	you  are aware  that earlier the      britishers had  colonized a   lot   of countries in the world
2024-02-07 19:22:46,558 	Text Hypothesis :	only 50  guests were called  arshdeep singh      from around    the video of ********* ** *** *****
2024-02-07 19:22:46,558 	Text Alignment  :	S    S   S      S    S       S        S          S    S         S   S        D         D  D   D    
2024-02-07 19:22:46,558 ========================================================================================================================
2024-02-07 19:22:46,558 Logging Sequence: 143_11.00
2024-02-07 19:22:46,559 	Gloss Reference :	A B+C+D+E
2024-02-07 19:22:46,559 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 19:22:46,559 	Gloss Alignment :	         
2024-02-07 19:22:46,559 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 19:22:46,560 	Text Reference  :	ronaldo has also become the first person to have 500 million followers on    instagram he is   the most loved footballer
2024-02-07 19:22:46,560 	Text Hypothesis :	******* *** **** ****** *** ***** ****** ** **** *** in      ipl       there is        a  such a   huge fan   following 
2024-02-07 19:22:46,561 	Text Alignment  :	D       D   D    D      D   D     D      D  D    D   S       S         S     S         S  S    S   S    S     S         
2024-02-07 19:22:46,561 ========================================================================================================================
2024-02-07 19:22:46,561 Logging Sequence: 183_23.00
2024-02-07 19:22:46,561 	Gloss Reference :	A B+C+D+E
2024-02-07 19:22:46,562 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 19:22:46,562 	Gloss Alignment :	         
2024-02-07 19:22:46,562 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 19:22:46,563 	Text Reference  :	however everybody has          been waiting for them   to announce the *********** ******* ***** *** name of        the child
2024-02-07 19:22:46,563 	Text Hypothesis :	singh   also      participated in   the     all thanks to ******** the partnership between india has bcci secretary jay shah 
2024-02-07 19:22:46,564 	Text Alignment  :	S       S         S            S    S       S   S         D            I           I       I     I   S    S         S   S    
2024-02-07 19:22:46,564 ========================================================================================================================
2024-02-07 19:22:46,564 Logging Sequence: 169_165.00
2024-02-07 19:22:46,564 	Gloss Reference :	A B+C+D+E
2024-02-07 19:22:46,564 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 19:22:46,564 	Gloss Alignment :	         
2024-02-07 19:22:46,565 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 19:22:46,566 	Text Reference  :	** the indian government was   outraged  by the  incident and     these changes were      undone by     wikipedia  
2024-02-07 19:22:46,566 	Text Hypothesis :	is the ****** ********** first cricketer in such a        violent brawl that    cricketer in     danger harrassment
2024-02-07 19:22:46,566 	Text Alignment  :	I      D      D          S     S         S  S    S        S       S     S       S         S      S      S          
2024-02-07 19:22:46,566 ========================================================================================================================
2024-02-07 19:22:47,551 Epoch 5778: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:22:47,551 EPOCH 5779
2024-02-07 19:23:04,307 Epoch 5779: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:23:04,308 EPOCH 5780
2024-02-07 19:23:20,657 Epoch 5780: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:23:20,658 EPOCH 5781
2024-02-07 19:23:36,686 Epoch 5781: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:23:36,686 EPOCH 5782
2024-02-07 19:23:52,978 Epoch 5782: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 19:23:52,978 EPOCH 5783
2024-02-07 19:24:09,681 Epoch 5783: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 19:24:09,682 EPOCH 5784
2024-02-07 19:24:25,807 Epoch 5784: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:24:25,807 EPOCH 5785
2024-02-07 19:24:41,846 Epoch 5785: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:24:41,847 EPOCH 5786
2024-02-07 19:24:58,096 Epoch 5786: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:24:58,096 EPOCH 5787
2024-02-07 19:25:14,222 Epoch 5787: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:25:14,223 EPOCH 5788
2024-02-07 19:25:30,445 Epoch 5788: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:25:30,446 EPOCH 5789
2024-02-07 19:25:40,881 [Epoch: 5789 Step: 00052100] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:      895 || Batch Translation Loss:   0.010810 => Txt Tokens per Sec:     2391 || Lr: 0.000050
2024-02-07 19:25:46,665 Epoch 5789: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:25:46,665 EPOCH 5790
2024-02-07 19:26:02,699 Epoch 5790: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-07 19:26:02,700 EPOCH 5791
2024-02-07 19:26:18,742 Epoch 5791: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:26:18,743 EPOCH 5792
2024-02-07 19:26:34,815 Epoch 5792: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:26:34,816 EPOCH 5793
2024-02-07 19:26:51,195 Epoch 5793: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:26:51,195 EPOCH 5794
2024-02-07 19:27:07,368 Epoch 5794: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:27:07,369 EPOCH 5795
2024-02-07 19:27:23,543 Epoch 5795: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:27:23,544 EPOCH 5796
2024-02-07 19:27:39,681 Epoch 5796: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:27:39,682 EPOCH 5797
2024-02-07 19:27:56,111 Epoch 5797: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:27:56,112 EPOCH 5798
2024-02-07 19:28:12,175 Epoch 5798: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:28:12,176 EPOCH 5799
2024-02-07 19:28:28,600 Epoch 5799: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:28:28,601 EPOCH 5800
2024-02-07 19:28:44,759 [Epoch: 5800 Step: 00052200] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:      657 || Batch Translation Loss:   0.021105 => Txt Tokens per Sec:     1819 || Lr: 0.000050
2024-02-07 19:28:44,759 Epoch 5800: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:28:44,759 EPOCH 5801
2024-02-07 19:29:00,955 Epoch 5801: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:29:00,957 EPOCH 5802
2024-02-07 19:29:17,173 Epoch 5802: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:29:17,174 EPOCH 5803
2024-02-07 19:29:33,438 Epoch 5803: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:29:33,439 EPOCH 5804
2024-02-07 19:29:49,475 Epoch 5804: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:29:49,476 EPOCH 5805
2024-02-07 19:30:05,350 Epoch 5805: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 19:30:05,350 EPOCH 5806
2024-02-07 19:30:21,896 Epoch 5806: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:30:21,897 EPOCH 5807
2024-02-07 19:30:38,214 Epoch 5807: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 19:30:38,215 EPOCH 5808
2024-02-07 19:30:54,531 Epoch 5808: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:30:54,532 EPOCH 5809
2024-02-07 19:31:10,850 Epoch 5809: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:31:10,851 EPOCH 5810
2024-02-07 19:31:27,112 Epoch 5810: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:31:27,112 EPOCH 5811
2024-02-07 19:31:43,368 Epoch 5811: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:31:43,369 EPOCH 5812
2024-02-07 19:31:49,119 [Epoch: 5812 Step: 00052300] Batch Recognition Loss:   0.000399 => Gls Tokens per Sec:      223 || Batch Translation Loss:   0.015817 => Txt Tokens per Sec:      766 || Lr: 0.000050
2024-02-07 19:31:59,562 Epoch 5812: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:31:59,563 EPOCH 5813
2024-02-07 19:32:15,681 Epoch 5813: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:32:15,682 EPOCH 5814
2024-02-07 19:32:31,714 Epoch 5814: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:32:31,714 EPOCH 5815
2024-02-07 19:32:47,942 Epoch 5815: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:32:47,943 EPOCH 5816
2024-02-07 19:33:03,849 Epoch 5816: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:33:03,850 EPOCH 5817
2024-02-07 19:33:20,275 Epoch 5817: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:33:20,276 EPOCH 5818
2024-02-07 19:33:36,453 Epoch 5818: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:33:36,454 EPOCH 5819
2024-02-07 19:33:52,680 Epoch 5819: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:33:52,680 EPOCH 5820
2024-02-07 19:34:08,992 Epoch 5820: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:34:08,993 EPOCH 5821
2024-02-07 19:34:25,433 Epoch 5821: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:34:25,433 EPOCH 5822
2024-02-07 19:34:41,537 Epoch 5822: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 19:34:41,537 EPOCH 5823
2024-02-07 19:34:42,547 [Epoch: 5823 Step: 00052400] Batch Recognition Loss:   0.000193 => Gls Tokens per Sec:     2538 || Batch Translation Loss:   0.013332 => Txt Tokens per Sec:     6655 || Lr: 0.000050
2024-02-07 19:34:57,938 Epoch 5823: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-07 19:34:57,939 EPOCH 5824
2024-02-07 19:35:13,907 Epoch 5824: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-07 19:35:13,907 EPOCH 5825
2024-02-07 19:35:29,935 Epoch 5825: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 19:35:29,936 EPOCH 5826
2024-02-07 19:35:46,203 Epoch 5826: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 19:35:46,203 EPOCH 5827
2024-02-07 19:36:02,432 Epoch 5827: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 19:36:02,432 EPOCH 5828
2024-02-07 19:36:18,727 Epoch 5828: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 19:36:18,727 EPOCH 5829
2024-02-07 19:36:35,017 Epoch 5829: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 19:36:35,017 EPOCH 5830
2024-02-07 19:36:51,261 Epoch 5830: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 19:36:51,262 EPOCH 5831
2024-02-07 19:37:07,307 Epoch 5831: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 19:37:07,308 EPOCH 5832
2024-02-07 19:37:23,480 Epoch 5832: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-07 19:37:23,481 EPOCH 5833
2024-02-07 19:37:39,340 Epoch 5833: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:37:39,341 EPOCH 5834
2024-02-07 19:37:44,821 [Epoch: 5834 Step: 00052500] Batch Recognition Loss:   0.000240 => Gls Tokens per Sec:      537 || Batch Translation Loss:   0.016294 => Txt Tokens per Sec:     1554 || Lr: 0.000050
2024-02-07 19:37:55,711 Epoch 5834: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:37:55,711 EPOCH 5835
2024-02-07 19:38:11,822 Epoch 5835: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:38:11,822 EPOCH 5836
2024-02-07 19:38:28,111 Epoch 5836: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:38:28,112 EPOCH 5837
2024-02-07 19:38:44,210 Epoch 5837: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:38:44,211 EPOCH 5838
2024-02-07 19:39:00,465 Epoch 5838: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:39:00,466 EPOCH 5839
2024-02-07 19:39:16,649 Epoch 5839: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:39:16,650 EPOCH 5840
2024-02-07 19:39:32,883 Epoch 5840: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:39:32,883 EPOCH 5841
2024-02-07 19:39:48,890 Epoch 5841: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:39:48,891 EPOCH 5842
2024-02-07 19:40:05,112 Epoch 5842: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:40:05,112 EPOCH 5843
2024-02-07 19:40:21,369 Epoch 5843: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:40:21,370 EPOCH 5844
2024-02-07 19:40:37,493 Epoch 5844: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:40:37,494 EPOCH 5845
2024-02-07 19:40:44,717 [Epoch: 5845 Step: 00052600] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:      709 || Batch Translation Loss:   0.016980 => Txt Tokens per Sec:     1842 || Lr: 0.000050
2024-02-07 19:40:53,756 Epoch 5845: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:40:53,756 EPOCH 5846
2024-02-07 19:41:09,907 Epoch 5846: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:41:09,908 EPOCH 5847
2024-02-07 19:41:25,977 Epoch 5847: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:41:25,978 EPOCH 5848
2024-02-07 19:41:42,396 Epoch 5848: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:41:42,397 EPOCH 5849
2024-02-07 19:41:58,430 Epoch 5849: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:41:58,431 EPOCH 5850
2024-02-07 19:42:14,469 Epoch 5850: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:42:14,470 EPOCH 5851
2024-02-07 19:42:30,468 Epoch 5851: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:42:30,469 EPOCH 5852
2024-02-07 19:42:46,796 Epoch 5852: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:42:46,797 EPOCH 5853
2024-02-07 19:43:02,824 Epoch 5853: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:43:02,824 EPOCH 5854
2024-02-07 19:43:19,108 Epoch 5854: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:43:19,109 EPOCH 5855
2024-02-07 19:43:35,419 Epoch 5855: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:43:35,420 EPOCH 5856
2024-02-07 19:43:40,253 [Epoch: 5856 Step: 00052700] Batch Recognition Loss:   0.000207 => Gls Tokens per Sec:     1324 || Batch Translation Loss:   0.014776 => Txt Tokens per Sec:     3530 || Lr: 0.000050
2024-02-07 19:43:51,284 Epoch 5856: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:43:51,285 EPOCH 5857
2024-02-07 19:44:07,467 Epoch 5857: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:44:07,468 EPOCH 5858
2024-02-07 19:44:23,569 Epoch 5858: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:44:23,570 EPOCH 5859
2024-02-07 19:44:39,651 Epoch 5859: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:44:39,652 EPOCH 5860
2024-02-07 19:44:55,837 Epoch 5860: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:44:55,838 EPOCH 5861
2024-02-07 19:45:12,004 Epoch 5861: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:45:12,004 EPOCH 5862
2024-02-07 19:45:28,386 Epoch 5862: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:45:28,387 EPOCH 5863
2024-02-07 19:45:44,377 Epoch 5863: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:45:44,378 EPOCH 5864
2024-02-07 19:46:00,627 Epoch 5864: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:46:00,628 EPOCH 5865
2024-02-07 19:46:16,671 Epoch 5865: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:46:16,673 EPOCH 5866
2024-02-07 19:46:33,158 Epoch 5866: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:46:33,159 EPOCH 5867
2024-02-07 19:46:42,485 [Epoch: 5867 Step: 00052800] Batch Recognition Loss:   0.000432 => Gls Tokens per Sec:      727 || Batch Translation Loss:   0.014826 => Txt Tokens per Sec:     2040 || Lr: 0.000050
2024-02-07 19:46:49,267 Epoch 5867: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:46:49,268 EPOCH 5868
2024-02-07 19:47:05,367 Epoch 5868: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:47:05,368 EPOCH 5869
2024-02-07 19:47:21,651 Epoch 5869: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 19:47:21,652 EPOCH 5870
2024-02-07 19:47:37,901 Epoch 5870: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 19:47:37,902 EPOCH 5871
2024-02-07 19:47:53,954 Epoch 5871: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 19:47:53,955 EPOCH 5872
2024-02-07 19:48:09,771 Epoch 5872: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:48:09,771 EPOCH 5873
2024-02-07 19:48:25,967 Epoch 5873: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 19:48:25,968 EPOCH 5874
2024-02-07 19:48:41,937 Epoch 5874: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:48:41,938 EPOCH 5875
2024-02-07 19:48:58,142 Epoch 5875: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 19:48:58,143 EPOCH 5876
2024-02-07 19:49:14,205 Epoch 5876: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:49:14,205 EPOCH 5877
2024-02-07 19:49:30,368 Epoch 5877: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:49:30,369 EPOCH 5878
2024-02-07 19:49:41,845 [Epoch: 5878 Step: 00052900] Batch Recognition Loss:   0.000098 => Gls Tokens per Sec:      781 || Batch Translation Loss:   0.010142 => Txt Tokens per Sec:     2193 || Lr: 0.000050
2024-02-07 19:49:46,475 Epoch 5878: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:49:46,475 EPOCH 5879
2024-02-07 19:50:03,263 Epoch 5879: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:50:03,263 EPOCH 5880
2024-02-07 19:50:19,314 Epoch 5880: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:50:19,314 EPOCH 5881
2024-02-07 19:50:35,110 Epoch 5881: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:50:35,111 EPOCH 5882
2024-02-07 19:50:51,099 Epoch 5882: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:50:51,100 EPOCH 5883
2024-02-07 19:51:07,190 Epoch 5883: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:51:07,191 EPOCH 5884
2024-02-07 19:51:23,181 Epoch 5884: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:51:23,182 EPOCH 5885
2024-02-07 19:51:39,298 Epoch 5885: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 19:51:39,299 EPOCH 5886
2024-02-07 19:51:55,542 Epoch 5886: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-07 19:51:55,543 EPOCH 5887
2024-02-07 19:52:11,710 Epoch 5887: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 19:52:11,710 EPOCH 5888
2024-02-07 19:52:27,872 Epoch 5888: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.14 
2024-02-07 19:52:27,873 EPOCH 5889
2024-02-07 19:52:43,301 [Epoch: 5889 Step: 00053000] Batch Recognition Loss:   0.001022 => Gls Tokens per Sec:      605 || Batch Translation Loss:   0.010916 => Txt Tokens per Sec:     1655 || Lr: 0.000050
2024-02-07 19:52:43,996 Epoch 5889: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 19:52:43,996 EPOCH 5890
2024-02-07 19:53:00,071 Epoch 5890: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.14 
2024-02-07 19:53:00,071 EPOCH 5891
2024-02-07 19:53:16,081 Epoch 5891: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 19:53:16,081 EPOCH 5892
2024-02-07 19:53:32,228 Epoch 5892: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 19:53:32,229 EPOCH 5893
2024-02-07 19:53:48,367 Epoch 5893: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 19:53:48,367 EPOCH 5894
2024-02-07 19:54:04,696 Epoch 5894: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 19:54:04,697 EPOCH 5895
2024-02-07 19:54:20,947 Epoch 5895: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 19:54:20,948 EPOCH 5896
2024-02-07 19:54:37,108 Epoch 5896: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 19:54:37,108 EPOCH 5897
2024-02-07 19:54:53,155 Epoch 5897: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:54:53,156 EPOCH 5898
2024-02-07 19:55:09,054 Epoch 5898: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 19:55:09,055 EPOCH 5899
2024-02-07 19:55:25,713 Epoch 5899: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 19:55:25,714 EPOCH 5900
2024-02-07 19:55:41,770 [Epoch: 5900 Step: 00053100] Batch Recognition Loss:   0.000316 => Gls Tokens per Sec:      661 || Batch Translation Loss:   0.014919 => Txt Tokens per Sec:     1830 || Lr: 0.000050
2024-02-07 19:55:41,771 Epoch 5900: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:55:41,771 EPOCH 5901
2024-02-07 19:55:58,024 Epoch 5901: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 19:55:58,024 EPOCH 5902
2024-02-07 19:56:14,564 Epoch 5902: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 19:56:14,565 EPOCH 5903
2024-02-07 19:56:30,925 Epoch 5903: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 19:56:30,926 EPOCH 5904
2024-02-07 19:56:47,066 Epoch 5904: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 19:56:47,067 EPOCH 5905
2024-02-07 19:57:02,892 Epoch 5905: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 19:57:02,892 EPOCH 5906
2024-02-07 19:57:18,899 Epoch 5906: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 19:57:18,899 EPOCH 5907
2024-02-07 19:57:34,880 Epoch 5907: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 19:57:34,880 EPOCH 5908
2024-02-07 19:57:51,129 Epoch 5908: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 19:57:51,130 EPOCH 5909
2024-02-07 19:58:07,271 Epoch 5909: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 19:58:07,272 EPOCH 5910
2024-02-07 19:58:23,488 Epoch 5910: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 19:58:23,489 EPOCH 5911
2024-02-07 19:58:39,313 Epoch 5911: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 19:58:39,314 EPOCH 5912
2024-02-07 19:58:43,524 [Epoch: 5912 Step: 00053200] Batch Recognition Loss:   0.000666 => Gls Tokens per Sec:       90 || Batch Translation Loss:   0.008616 => Txt Tokens per Sec:      322 || Lr: 0.000050
2024-02-07 19:58:55,628 Epoch 5912: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-07 19:58:55,629 EPOCH 5913
2024-02-07 19:59:11,637 Epoch 5913: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 19:59:11,639 EPOCH 5914
2024-02-07 19:59:27,720 Epoch 5914: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-07 19:59:27,720 EPOCH 5915
2024-02-07 19:59:44,315 Epoch 5915: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 19:59:44,316 EPOCH 5916
2024-02-07 20:00:00,454 Epoch 5916: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 20:00:00,455 EPOCH 5917
2024-02-07 20:00:16,300 Epoch 5917: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 20:00:16,301 EPOCH 5918
2024-02-07 20:00:32,473 Epoch 5918: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 20:00:32,474 EPOCH 5919
2024-02-07 20:00:48,840 Epoch 5919: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 20:00:48,841 EPOCH 5920
2024-02-07 20:01:04,961 Epoch 5920: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 20:01:04,961 EPOCH 5921
2024-02-07 20:01:20,860 Epoch 5921: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 20:01:20,860 EPOCH 5922
2024-02-07 20:01:36,904 Epoch 5922: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 20:01:36,904 EPOCH 5923
2024-02-07 20:01:37,563 [Epoch: 5923 Step: 00053300] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:     3891 || Batch Translation Loss:   0.016696 => Txt Tokens per Sec:     9713 || Lr: 0.000050
2024-02-07 20:01:53,068 Epoch 5923: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 20:01:53,069 EPOCH 5924
2024-02-07 20:02:09,092 Epoch 5924: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 20:02:09,093 EPOCH 5925
2024-02-07 20:02:25,035 Epoch 5925: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-07 20:02:25,036 EPOCH 5926
2024-02-07 20:02:41,147 Epoch 5926: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 20:02:41,148 EPOCH 5927
2024-02-07 20:02:56,894 Epoch 5927: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 20:02:56,895 EPOCH 5928
2024-02-07 20:03:13,108 Epoch 5928: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 20:03:13,109 EPOCH 5929
2024-02-07 20:03:29,357 Epoch 5929: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 20:03:29,358 EPOCH 5930
2024-02-07 20:03:45,329 Epoch 5930: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 20:03:45,330 EPOCH 5931
2024-02-07 20:04:01,385 Epoch 5931: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 20:04:01,386 EPOCH 5932
2024-02-07 20:04:17,816 Epoch 5932: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 20:04:17,817 EPOCH 5933
2024-02-07 20:04:33,900 Epoch 5933: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 20:04:33,901 EPOCH 5934
2024-02-07 20:04:38,058 [Epoch: 5934 Step: 00053400] Batch Recognition Loss:   0.000486 => Gls Tokens per Sec:      924 || Batch Translation Loss:   0.013567 => Txt Tokens per Sec:     2729 || Lr: 0.000050
2024-02-07 20:04:49,993 Epoch 5934: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 20:04:49,994 EPOCH 5935
2024-02-07 20:05:06,429 Epoch 5935: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 20:05:06,430 EPOCH 5936
2024-02-07 20:05:22,429 Epoch 5936: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 20:05:22,430 EPOCH 5937
2024-02-07 20:05:38,523 Epoch 5937: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 20:05:38,523 EPOCH 5938
2024-02-07 20:05:54,631 Epoch 5938: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 20:05:54,632 EPOCH 5939
2024-02-07 20:06:11,413 Epoch 5939: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 20:06:11,414 EPOCH 5940
2024-02-07 20:06:27,637 Epoch 5940: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 20:06:27,638 EPOCH 5941
2024-02-07 20:06:44,078 Epoch 5941: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 20:06:44,079 EPOCH 5942
2024-02-07 20:07:00,050 Epoch 5942: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 20:07:00,051 EPOCH 5943
2024-02-07 20:07:15,850 Epoch 5943: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 20:07:15,851 EPOCH 5944
2024-02-07 20:07:32,027 Epoch 5944: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 20:07:32,027 EPOCH 5945
2024-02-07 20:07:33,512 [Epoch: 5945 Step: 00053500] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:     3452 || Batch Translation Loss:   0.011258 => Txt Tokens per Sec:     8050 || Lr: 0.000050
2024-02-07 20:07:48,319 Epoch 5945: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 20:07:48,320 EPOCH 5946
2024-02-07 20:08:04,480 Epoch 5946: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 20:08:04,481 EPOCH 5947
2024-02-07 20:08:20,520 Epoch 5947: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 20:08:20,521 EPOCH 5948
2024-02-07 20:08:36,889 Epoch 5948: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 20:08:36,890 EPOCH 5949
2024-02-07 20:08:53,111 Epoch 5949: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 20:08:53,112 EPOCH 5950
2024-02-07 20:09:09,090 Epoch 5950: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 20:09:09,091 EPOCH 5951
2024-02-07 20:09:25,192 Epoch 5951: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 20:09:25,193 EPOCH 5952
2024-02-07 20:09:41,440 Epoch 5952: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 20:09:41,441 EPOCH 5953
2024-02-07 20:09:57,483 Epoch 5953: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 20:09:57,484 EPOCH 5954
2024-02-07 20:10:13,732 Epoch 5954: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 20:10:13,733 EPOCH 5955
2024-02-07 20:10:29,827 Epoch 5955: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 20:10:29,828 EPOCH 5956
2024-02-07 20:10:40,474 [Epoch: 5956 Step: 00053600] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:      601 || Batch Translation Loss:   0.015029 => Txt Tokens per Sec:     1758 || Lr: 0.000050
2024-02-07 20:10:46,228 Epoch 5956: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 20:10:46,229 EPOCH 5957
2024-02-07 20:11:02,355 Epoch 5957: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 20:11:02,355 EPOCH 5958
2024-02-07 20:11:18,555 Epoch 5958: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 20:11:18,555 EPOCH 5959
2024-02-07 20:11:34,475 Epoch 5959: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 20:11:34,476 EPOCH 5960
2024-02-07 20:11:50,805 Epoch 5960: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 20:11:50,806 EPOCH 5961
2024-02-07 20:12:07,123 Epoch 5961: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 20:12:07,123 EPOCH 5962
2024-02-07 20:12:23,100 Epoch 5962: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 20:12:23,102 EPOCH 5963
2024-02-07 20:12:39,077 Epoch 5963: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 20:12:39,077 EPOCH 5964
2024-02-07 20:12:55,481 Epoch 5964: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 20:12:55,481 EPOCH 5965
2024-02-07 20:13:11,743 Epoch 5965: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 20:13:11,744 EPOCH 5966
2024-02-07 20:13:28,402 Epoch 5966: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 20:13:28,403 EPOCH 5967
2024-02-07 20:13:37,767 [Epoch: 5967 Step: 00053700] Batch Recognition Loss:   0.000281 => Gls Tokens per Sec:      724 || Batch Translation Loss:   0.033849 => Txt Tokens per Sec:     1928 || Lr: 0.000050
2024-02-07 20:13:44,561 Epoch 5967: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 20:13:44,561 EPOCH 5968
2024-02-07 20:14:01,298 Epoch 5968: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 20:14:01,298 EPOCH 5969
2024-02-07 20:14:17,447 Epoch 5969: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 20:14:17,448 EPOCH 5970
2024-02-07 20:14:33,617 Epoch 5970: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 20:14:33,617 EPOCH 5971
2024-02-07 20:14:49,838 Epoch 5971: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 20:14:49,839 EPOCH 5972
2024-02-07 20:15:05,914 Epoch 5972: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 20:15:05,915 EPOCH 5973
2024-02-07 20:15:22,181 Epoch 5973: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 20:15:22,182 EPOCH 5974
2024-02-07 20:15:38,299 Epoch 5974: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 20:15:38,300 EPOCH 5975
2024-02-07 20:15:54,359 Epoch 5975: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 20:15:54,359 EPOCH 5976
2024-02-07 20:16:10,990 Epoch 5976: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 20:16:10,991 EPOCH 5977
2024-02-07 20:16:27,010 Epoch 5977: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 20:16:27,010 EPOCH 5978
2024-02-07 20:16:36,688 [Epoch: 5978 Step: 00053800] Batch Recognition Loss:   0.000445 => Gls Tokens per Sec:      833 || Batch Translation Loss:   0.019672 => Txt Tokens per Sec:     2194 || Lr: 0.000050
2024-02-07 20:16:43,201 Epoch 5978: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 20:16:43,202 EPOCH 5979
2024-02-07 20:16:59,191 Epoch 5979: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 20:16:59,192 EPOCH 5980
2024-02-07 20:17:15,332 Epoch 5980: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 20:17:15,333 EPOCH 5981
2024-02-07 20:17:31,629 Epoch 5981: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 20:17:31,629 EPOCH 5982
2024-02-07 20:17:47,546 Epoch 5982: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 20:17:47,547 EPOCH 5983
2024-02-07 20:18:03,558 Epoch 5983: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 20:18:03,559 EPOCH 5984
2024-02-07 20:18:19,767 Epoch 5984: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 20:18:19,767 EPOCH 5985
2024-02-07 20:18:35,972 Epoch 5985: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 20:18:35,973 EPOCH 5986
2024-02-07 20:18:52,217 Epoch 5986: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-07 20:18:52,217 EPOCH 5987
2024-02-07 20:19:08,420 Epoch 5987: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-07 20:19:08,421 EPOCH 5988
2024-02-07 20:19:24,150 Epoch 5988: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-07 20:19:24,151 EPOCH 5989
2024-02-07 20:19:39,620 [Epoch: 5989 Step: 00053900] Batch Recognition Loss:   0.001124 => Gls Tokens per Sec:      604 || Batch Translation Loss:   0.040754 => Txt Tokens per Sec:     1653 || Lr: 0.000050
2024-02-07 20:19:40,353 Epoch 5989: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-07 20:19:40,354 EPOCH 5990
2024-02-07 20:19:56,432 Epoch 5990: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-07 20:19:56,432 EPOCH 5991
2024-02-07 20:20:12,854 Epoch 5991: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 20:20:12,854 EPOCH 5992
2024-02-07 20:20:29,035 Epoch 5992: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 20:20:29,036 EPOCH 5993
2024-02-07 20:20:45,651 Epoch 5993: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 20:20:45,651 EPOCH 5994
2024-02-07 20:21:01,821 Epoch 5994: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 20:21:01,821 EPOCH 5995
2024-02-07 20:21:18,178 Epoch 5995: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 20:21:18,178 EPOCH 5996
2024-02-07 20:21:34,261 Epoch 5996: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 20:21:34,262 EPOCH 5997
2024-02-07 20:21:50,565 Epoch 5997: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 20:21:50,566 EPOCH 5998
2024-02-07 20:22:06,951 Epoch 5998: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 20:22:06,951 EPOCH 5999
2024-02-07 20:22:23,149 Epoch 5999: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 20:22:23,150 EPOCH 6000
2024-02-07 20:22:39,443 [Epoch: 6000 Step: 00054000] Batch Recognition Loss:   0.001143 => Gls Tokens per Sec:      652 || Batch Translation Loss:   0.019885 => Txt Tokens per Sec:     1803 || Lr: 0.000050
2024-02-07 20:23:47,522 Validation result at epoch 6000, step    54000: duration: 68.0767s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.22280	Translation Loss: 107011.29688	PPL: 43840.04688
	Eval Metric: BLEU
	WER 1.98	(DEL: 0.00,	INS: 0.00,	SUB: 1.98)
	BLEU-4 0.29	(BLEU-1: 9.58,	BLEU-2: 2.50,	BLEU-3: 0.78,	BLEU-4: 0.29)
	CHRF 16.50	ROUGE 7.96
2024-02-07 20:23:47,524 Logging Recognition and Translation Outputs
2024-02-07 20:23:47,524 ========================================================================================================================
2024-02-07 20:23:47,524 Logging Sequence: 166_243.00
2024-02-07 20:23:47,525 	Gloss Reference :	A B+C+D+E
2024-02-07 20:23:47,525 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 20:23:47,525 	Gloss Alignment :	         
2024-02-07 20:23:47,525 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 20:23:47,526 	Text Reference  :	*** ***** icc       worked with  members boards  like bcci   pcb  cricket australia etc 
2024-02-07 20:23:47,527 	Text Hypothesis :	the board organised the    world cup     matches in   mumbai navi mumbai  and       pune
2024-02-07 20:23:47,527 	Text Alignment  :	I   I     S         S      S     S       S       S    S      S    S       S         S   
2024-02-07 20:23:47,527 ========================================================================================================================
2024-02-07 20:23:47,527 Logging Sequence: 179_409.00
2024-02-07 20:23:47,527 	Gloss Reference :	A B+C+D+E
2024-02-07 20:23:47,528 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 20:23:47,528 	Gloss Alignment :	         
2024-02-07 20:23:47,528 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 20:23:47,529 	Text Reference  :	*********** the passport was at the ****** **** wfi   office  in delhi
2024-02-07 20:23:47,529 	Text Hypothesis :	afghanistan and then     or  up the finals this would against 23 years
2024-02-07 20:23:47,529 	Text Alignment  :	I           S   S        S   S      I      I    S     S       S  S    
2024-02-07 20:23:47,529 ========================================================================================================================
2024-02-07 20:23:47,529 Logging Sequence: 81_407.00
2024-02-07 20:23:47,529 	Gloss Reference :	A B+C+D+E
2024-02-07 20:23:47,530 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 20:23:47,530 	Gloss Alignment :	         
2024-02-07 20:23:47,530 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 20:23:47,532 	Text Reference  :	******* the ***** ******* government company   -  national buildings construction corporation and they  will complete them in    a   time-bound manner
2024-02-07 20:23:47,532 	Text Hypothesis :	however the start getting deducted   depending on the      fine      of           rs          150 crore so   we       to   dhoni for rs         250   
2024-02-07 20:23:47,532 	Text Alignment  :	I           I     I       S          S         S  S        S         S            S           S   S     S    S        S    S     S   S          S     
2024-02-07 20:23:47,532 ========================================================================================================================
2024-02-07 20:23:47,533 Logging Sequence: 96_31.00
2024-02-07 20:23:47,533 	Gloss Reference :	A B+C+D+E
2024-02-07 20:23:47,533 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 20:23:47,533 	Gloss Alignment :	         
2024-02-07 20:23:47,533 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 20:23:47,534 	Text Reference  :	and then 2 teams will go on to    play the  final  
2024-02-07 20:23:47,534 	Text Hypothesis :	*** **** * ***** **** ** ** kohli also felt relaxed
2024-02-07 20:23:47,534 	Text Alignment  :	D   D    D D     D    D  D  S     S    S    S      
2024-02-07 20:23:47,534 ========================================================================================================================
2024-02-07 20:23:47,534 Logging Sequence: 160_87.00
2024-02-07 20:23:47,534 	Gloss Reference :	A B+C+D+E
2024-02-07 20:23:47,534 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 20:23:47,535 	Gloss Alignment :	         
2024-02-07 20:23:47,535 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 20:23:47,535 	Text Reference  :	*** **** kohli held a    press conference and  said
2024-02-07 20:23:47,536 	Text Hypothesis :	and they have  been done in    the        next day 
2024-02-07 20:23:47,536 	Text Alignment  :	I   I    S     S    S    S     S          S    S   
2024-02-07 20:23:47,536 ========================================================================================================================
2024-02-07 20:23:47,541 Epoch 6000: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 20:23:47,541 EPOCH 6001
2024-02-07 20:24:04,517 Epoch 6001: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 20:24:04,518 EPOCH 6002
2024-02-07 20:24:20,463 Epoch 6002: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 20:24:20,463 EPOCH 6003
2024-02-07 20:24:36,638 Epoch 6003: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 20:24:36,638 EPOCH 6004
2024-02-07 20:24:52,823 Epoch 6004: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 20:24:52,824 EPOCH 6005
2024-02-07 20:25:09,012 Epoch 6005: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 20:25:09,012 EPOCH 6006
2024-02-07 20:25:25,343 Epoch 6006: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 20:25:25,344 EPOCH 6007
2024-02-07 20:25:41,362 Epoch 6007: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 20:25:41,362 EPOCH 6008
2024-02-07 20:25:57,420 Epoch 6008: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 20:25:57,420 EPOCH 6009
2024-02-07 20:26:13,982 Epoch 6009: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 20:26:13,983 EPOCH 6010
2024-02-07 20:26:30,248 Epoch 6010: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 20:26:30,249 EPOCH 6011
2024-02-07 20:26:46,352 Epoch 6011: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:26:46,353 EPOCH 6012
2024-02-07 20:26:50,662 [Epoch: 6012 Step: 00054100] Batch Recognition Loss:   0.000246 => Gls Tokens per Sec:       88 || Batch Translation Loss:   0.005404 => Txt Tokens per Sec:      315 || Lr: 0.000050
2024-02-07 20:27:02,690 Epoch 6012: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 20:27:02,690 EPOCH 6013
2024-02-07 20:27:19,118 Epoch 6013: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 20:27:19,118 EPOCH 6014
2024-02-07 20:27:35,273 Epoch 6014: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 20:27:35,274 EPOCH 6015
2024-02-07 20:27:51,419 Epoch 6015: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-07 20:27:51,420 EPOCH 6016
2024-02-07 20:28:07,538 Epoch 6016: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-07 20:28:07,538 EPOCH 6017
2024-02-07 20:28:23,547 Epoch 6017: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 20:28:23,548 EPOCH 6018
2024-02-07 20:28:39,886 Epoch 6018: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 20:28:39,886 EPOCH 6019
2024-02-07 20:28:55,894 Epoch 6019: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 20:28:55,895 EPOCH 6020
2024-02-07 20:29:11,776 Epoch 6020: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 20:29:11,777 EPOCH 6021
2024-02-07 20:29:28,176 Epoch 6021: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 20:29:28,176 EPOCH 6022
2024-02-07 20:29:44,004 Epoch 6022: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 20:29:44,004 EPOCH 6023
2024-02-07 20:29:50,150 [Epoch: 6023 Step: 00054200] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:      417 || Batch Translation Loss:   0.012477 => Txt Tokens per Sec:     1290 || Lr: 0.000050
2024-02-07 20:29:59,807 Epoch 6023: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 20:29:59,807 EPOCH 6024
2024-02-07 20:30:15,947 Epoch 6024: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 20:30:15,948 EPOCH 6025
2024-02-07 20:30:32,226 Epoch 6025: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 20:30:32,227 EPOCH 6026
2024-02-07 20:30:48,180 Epoch 6026: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 20:30:48,180 EPOCH 6027
2024-02-07 20:31:04,363 Epoch 6027: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 20:31:04,364 EPOCH 6028
2024-02-07 20:31:20,648 Epoch 6028: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-07 20:31:20,649 EPOCH 6029
2024-02-07 20:31:36,889 Epoch 6029: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-07 20:31:36,889 EPOCH 6030
2024-02-07 20:31:53,063 Epoch 6030: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-07 20:31:53,064 EPOCH 6031
2024-02-07 20:32:08,953 Epoch 6031: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-07 20:32:08,953 EPOCH 6032
2024-02-07 20:32:25,058 Epoch 6032: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-07 20:32:25,059 EPOCH 6033
2024-02-07 20:32:41,312 Epoch 6033: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-07 20:32:41,313 EPOCH 6034
2024-02-07 20:32:42,434 [Epoch: 6034 Step: 00054300] Batch Recognition Loss:   0.000435 => Gls Tokens per Sec:     3429 || Batch Translation Loss:   0.043204 => Txt Tokens per Sec:     8762 || Lr: 0.000050
2024-02-07 20:32:57,054 Epoch 6034: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-07 20:32:57,055 EPOCH 6035
2024-02-07 20:33:13,675 Epoch 6035: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 20:33:13,675 EPOCH 6036
2024-02-07 20:33:29,812 Epoch 6036: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 20:33:29,813 EPOCH 6037
2024-02-07 20:33:46,279 Epoch 6037: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 20:33:46,280 EPOCH 6038
2024-02-07 20:34:02,644 Epoch 6038: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 20:34:02,645 EPOCH 6039
2024-02-07 20:34:18,429 Epoch 6039: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 20:34:18,429 EPOCH 6040
2024-02-07 20:34:34,421 Epoch 6040: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 20:34:34,421 EPOCH 6041
2024-02-07 20:34:50,878 Epoch 6041: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 20:34:50,878 EPOCH 6042
2024-02-07 20:35:06,861 Epoch 6042: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 20:35:06,862 EPOCH 6043
2024-02-07 20:35:23,122 Epoch 6043: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 20:35:23,123 EPOCH 6044
2024-02-07 20:35:38,985 Epoch 6044: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 20:35:38,986 EPOCH 6045
2024-02-07 20:35:43,430 [Epoch: 6045 Step: 00054400] Batch Recognition Loss:   0.000317 => Gls Tokens per Sec:     1153 || Batch Translation Loss:   0.017359 => Txt Tokens per Sec:     3131 || Lr: 0.000050
2024-02-07 20:35:55,157 Epoch 6045: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 20:35:55,157 EPOCH 6046
2024-02-07 20:36:11,241 Epoch 6046: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 20:36:11,241 EPOCH 6047
2024-02-07 20:36:27,199 Epoch 6047: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 20:36:27,200 EPOCH 6048
2024-02-07 20:36:43,545 Epoch 6048: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 20:36:43,545 EPOCH 6049
2024-02-07 20:36:59,541 Epoch 6049: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 20:36:59,541 EPOCH 6050
2024-02-07 20:37:16,116 Epoch 6050: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 20:37:16,117 EPOCH 6051
2024-02-07 20:37:32,227 Epoch 6051: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:37:32,227 EPOCH 6052
2024-02-07 20:37:48,154 Epoch 6052: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:37:48,155 EPOCH 6053
2024-02-07 20:38:04,458 Epoch 6053: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:38:04,458 EPOCH 6054
2024-02-07 20:38:20,421 Epoch 6054: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:38:20,421 EPOCH 6055
2024-02-07 20:38:36,662 Epoch 6055: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 20:38:36,663 EPOCH 6056
2024-02-07 20:38:50,673 [Epoch: 6056 Step: 00054500] Batch Recognition Loss:   0.000224 => Gls Tokens per Sec:      393 || Batch Translation Loss:   0.013014 => Txt Tokens per Sec:     1071 || Lr: 0.000050
2024-02-07 20:38:52,960 Epoch 6056: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:38:52,961 EPOCH 6057
2024-02-07 20:39:09,079 Epoch 6057: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 20:39:09,080 EPOCH 6058
2024-02-07 20:39:25,169 Epoch 6058: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 20:39:25,169 EPOCH 6059
2024-02-07 20:39:41,138 Epoch 6059: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 20:39:41,138 EPOCH 6060
2024-02-07 20:39:57,513 Epoch 6060: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 20:39:57,514 EPOCH 6061
2024-02-07 20:40:13,743 Epoch 6061: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 20:40:13,743 EPOCH 6062
2024-02-07 20:40:29,993 Epoch 6062: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:40:29,994 EPOCH 6063
2024-02-07 20:40:46,285 Epoch 6063: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 20:40:46,286 EPOCH 6064
2024-02-07 20:41:02,268 Epoch 6064: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 20:41:02,269 EPOCH 6065
2024-02-07 20:41:18,361 Epoch 6065: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:41:18,362 EPOCH 6066
2024-02-07 20:41:34,700 Epoch 6066: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 20:41:34,701 EPOCH 6067
2024-02-07 20:41:40,049 [Epoch: 6067 Step: 00054600] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:     1436 || Batch Translation Loss:   0.006128 => Txt Tokens per Sec:     3800 || Lr: 0.000050
2024-02-07 20:41:50,753 Epoch 6067: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:41:50,754 EPOCH 6068
2024-02-07 20:42:06,894 Epoch 6068: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:42:06,895 EPOCH 6069
2024-02-07 20:42:23,062 Epoch 6069: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:42:23,062 EPOCH 6070
2024-02-07 20:42:39,036 Epoch 6070: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:42:39,037 EPOCH 6071
2024-02-07 20:42:55,449 Epoch 6071: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 20:42:55,449 EPOCH 6072
2024-02-07 20:43:11,562 Epoch 6072: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:43:11,563 EPOCH 6073
2024-02-07 20:43:27,563 Epoch 6073: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:43:27,563 EPOCH 6074
2024-02-07 20:43:43,945 Epoch 6074: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:43:43,946 EPOCH 6075
2024-02-07 20:44:00,092 Epoch 6075: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:44:00,092 EPOCH 6076
2024-02-07 20:44:16,568 Epoch 6076: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:44:16,569 EPOCH 6077
2024-02-07 20:44:32,667 Epoch 6077: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:44:32,668 EPOCH 6078
2024-02-07 20:44:42,642 [Epoch: 6078 Step: 00054700] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:      808 || Batch Translation Loss:   0.009190 => Txt Tokens per Sec:     2272 || Lr: 0.000050
2024-02-07 20:44:48,627 Epoch 6078: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 20:44:48,628 EPOCH 6079
2024-02-07 20:45:04,843 Epoch 6079: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 20:45:04,844 EPOCH 6080
2024-02-07 20:45:20,966 Epoch 6080: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:45:20,966 EPOCH 6081
2024-02-07 20:45:37,050 Epoch 6081: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 20:45:37,051 EPOCH 6082
2024-02-07 20:45:53,366 Epoch 6082: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 20:45:53,367 EPOCH 6083
2024-02-07 20:46:09,298 Epoch 6083: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:46:09,299 EPOCH 6084
2024-02-07 20:46:25,535 Epoch 6084: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:46:25,536 EPOCH 6085
2024-02-07 20:46:41,301 Epoch 6085: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 20:46:41,302 EPOCH 6086
2024-02-07 20:46:57,497 Epoch 6086: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:46:57,498 EPOCH 6087
2024-02-07 20:47:13,817 Epoch 6087: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 20:47:13,818 EPOCH 6088
2024-02-07 20:47:29,914 Epoch 6088: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:47:29,915 EPOCH 6089
2024-02-07 20:47:41,847 [Epoch: 6089 Step: 00054800] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:      858 || Batch Translation Loss:   0.016660 => Txt Tokens per Sec:     2349 || Lr: 0.000050
2024-02-07 20:47:46,157 Epoch 6089: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:47:46,158 EPOCH 6090
2024-02-07 20:48:02,027 Epoch 6090: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:48:02,027 EPOCH 6091
2024-02-07 20:48:18,354 Epoch 6091: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:48:18,355 EPOCH 6092
2024-02-07 20:48:34,486 Epoch 6092: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:48:34,487 EPOCH 6093
2024-02-07 20:48:50,394 Epoch 6093: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:48:50,395 EPOCH 6094
2024-02-07 20:49:06,584 Epoch 6094: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:49:06,585 EPOCH 6095
2024-02-07 20:49:22,606 Epoch 6095: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 20:49:22,606 EPOCH 6096
2024-02-07 20:49:38,674 Epoch 6096: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:49:38,674 EPOCH 6097
2024-02-07 20:49:54,729 Epoch 6097: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:49:54,729 EPOCH 6098
2024-02-07 20:50:10,698 Epoch 6098: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 20:50:10,698 EPOCH 6099
2024-02-07 20:50:26,693 Epoch 6099: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 20:50:26,693 EPOCH 6100
2024-02-07 20:50:42,916 [Epoch: 6100 Step: 00054900] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:      655 || Batch Translation Loss:   0.018795 => Txt Tokens per Sec:     1811 || Lr: 0.000050
2024-02-07 20:50:42,916 Epoch 6100: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:50:42,917 EPOCH 6101
2024-02-07 20:50:59,201 Epoch 6101: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:50:59,202 EPOCH 6102
2024-02-07 20:51:15,289 Epoch 6102: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.09 
2024-02-07 20:51:15,290 EPOCH 6103
2024-02-07 20:51:31,473 Epoch 6103: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:51:31,473 EPOCH 6104
2024-02-07 20:51:47,701 Epoch 6104: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:51:47,702 EPOCH 6105
2024-02-07 20:52:03,555 Epoch 6105: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:52:03,555 EPOCH 6106
2024-02-07 20:52:19,705 Epoch 6106: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:52:19,706 EPOCH 6107
2024-02-07 20:52:36,070 Epoch 6107: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:52:36,071 EPOCH 6108
2024-02-07 20:52:52,294 Epoch 6108: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 20:52:52,295 EPOCH 6109
2024-02-07 20:53:08,459 Epoch 6109: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:53:08,459 EPOCH 6110
2024-02-07 20:53:24,596 Epoch 6110: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 20:53:24,597 EPOCH 6111
2024-02-07 20:53:41,033 Epoch 6111: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 20:53:41,034 EPOCH 6112
2024-02-07 20:53:41,667 [Epoch: 6112 Step: 00055000] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     2025 || Batch Translation Loss:   0.018001 => Txt Tokens per Sec:     6168 || Lr: 0.000050
2024-02-07 20:53:56,913 Epoch 6112: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 20:53:56,914 EPOCH 6113
2024-02-07 20:54:12,867 Epoch 6113: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 20:54:12,868 EPOCH 6114
2024-02-07 20:54:29,246 Epoch 6114: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 20:54:29,247 EPOCH 6115
2024-02-07 20:54:45,326 Epoch 6115: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 20:54:45,327 EPOCH 6116
2024-02-07 20:55:01,843 Epoch 6116: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 20:55:01,844 EPOCH 6117
2024-02-07 20:55:17,931 Epoch 6117: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 20:55:17,932 EPOCH 6118
2024-02-07 20:55:34,076 Epoch 6118: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 20:55:34,077 EPOCH 6119
2024-02-07 20:55:50,016 Epoch 6119: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 20:55:50,017 EPOCH 6120
2024-02-07 20:56:06,254 Epoch 6120: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 20:56:06,255 EPOCH 6121
2024-02-07 20:56:22,621 Epoch 6121: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 20:56:22,622 EPOCH 6122
2024-02-07 20:56:38,725 Epoch 6122: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 20:56:38,725 EPOCH 6123
2024-02-07 20:56:47,645 [Epoch: 6123 Step: 00055100] Batch Recognition Loss:   0.000327 => Gls Tokens per Sec:      287 || Batch Translation Loss:   0.015114 => Txt Tokens per Sec:      955 || Lr: 0.000050
2024-02-07 20:56:54,756 Epoch 6123: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 20:56:54,756 EPOCH 6124
2024-02-07 20:57:11,176 Epoch 6124: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 20:57:11,176 EPOCH 6125
2024-02-07 20:57:27,563 Epoch 6125: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 20:57:27,563 EPOCH 6126
2024-02-07 20:57:43,755 Epoch 6126: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 20:57:43,755 EPOCH 6127
2024-02-07 20:57:59,853 Epoch 6127: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 20:57:59,854 EPOCH 6128
2024-02-07 20:58:16,112 Epoch 6128: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 20:58:16,112 EPOCH 6129
2024-02-07 20:58:32,257 Epoch 6129: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 20:58:32,258 EPOCH 6130
2024-02-07 20:58:48,400 Epoch 6130: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 20:58:48,400 EPOCH 6131
2024-02-07 20:59:04,766 Epoch 6131: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 20:59:04,766 EPOCH 6132
2024-02-07 20:59:20,881 Epoch 6132: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 20:59:20,881 EPOCH 6133
2024-02-07 20:59:36,854 Epoch 6133: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 20:59:36,854 EPOCH 6134
2024-02-07 20:59:42,366 [Epoch: 6134 Step: 00055200] Batch Recognition Loss:   0.000313 => Gls Tokens per Sec:      534 || Batch Translation Loss:   0.028489 => Txt Tokens per Sec:     1441 || Lr: 0.000050
2024-02-07 20:59:53,060 Epoch 6134: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 20:59:53,061 EPOCH 6135
2024-02-07 21:00:09,447 Epoch 6135: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 21:00:09,447 EPOCH 6136
2024-02-07 21:00:25,360 Epoch 6136: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 21:00:25,360 EPOCH 6137
2024-02-07 21:00:41,625 Epoch 6137: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-07 21:00:41,626 EPOCH 6138
2024-02-07 21:00:57,643 Epoch 6138: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 21:00:57,644 EPOCH 6139
2024-02-07 21:01:13,949 Epoch 6139: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 21:01:13,950 EPOCH 6140
2024-02-07 21:01:30,096 Epoch 6140: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 21:01:30,097 EPOCH 6141
2024-02-07 21:01:46,062 Epoch 6141: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 21:01:46,062 EPOCH 6142
2024-02-07 21:02:02,305 Epoch 6142: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 21:02:02,306 EPOCH 6143
2024-02-07 21:02:18,520 Epoch 6143: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 21:02:18,521 EPOCH 6144
2024-02-07 21:02:34,599 Epoch 6144: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 21:02:34,600 EPOCH 6145
2024-02-07 21:02:42,961 [Epoch: 6145 Step: 00055300] Batch Recognition Loss:   0.000339 => Gls Tokens per Sec:      505 || Batch Translation Loss:   0.005809 => Txt Tokens per Sec:     1376 || Lr: 0.000050
2024-02-07 21:02:50,647 Epoch 6145: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 21:02:50,647 EPOCH 6146
2024-02-07 21:03:06,622 Epoch 6146: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 21:03:06,622 EPOCH 6147
2024-02-07 21:03:22,834 Epoch 6147: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 21:03:22,834 EPOCH 6148
2024-02-07 21:03:38,675 Epoch 6148: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:03:38,676 EPOCH 6149
2024-02-07 21:03:54,878 Epoch 6149: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 21:03:54,879 EPOCH 6150
2024-02-07 21:04:11,138 Epoch 6150: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 21:04:11,138 EPOCH 6151
2024-02-07 21:04:27,690 Epoch 6151: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 21:04:27,690 EPOCH 6152
2024-02-07 21:04:43,865 Epoch 6152: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 21:04:43,866 EPOCH 6153
2024-02-07 21:04:59,928 Epoch 6153: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 21:04:59,929 EPOCH 6154
2024-02-07 21:05:16,276 Epoch 6154: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 21:05:16,277 EPOCH 6155
2024-02-07 21:05:32,545 Epoch 6155: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 21:05:32,546 EPOCH 6156
2024-02-07 21:05:44,317 [Epoch: 6156 Step: 00055400] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:      467 || Batch Translation Loss:   0.012589 => Txt Tokens per Sec:     1338 || Lr: 0.000050
2024-02-07 21:05:48,809 Epoch 6156: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-07 21:05:48,809 EPOCH 6157
2024-02-07 21:06:05,256 Epoch 6157: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-07 21:06:05,257 EPOCH 6158
2024-02-07 21:06:21,300 Epoch 6158: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-07 21:06:21,301 EPOCH 6159
2024-02-07 21:06:37,401 Epoch 6159: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-07 21:06:37,402 EPOCH 6160
2024-02-07 21:06:53,412 Epoch 6160: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-07 21:06:53,412 EPOCH 6161
2024-02-07 21:07:09,757 Epoch 6161: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-07 21:07:09,758 EPOCH 6162
2024-02-07 21:07:26,090 Epoch 6162: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-07 21:07:26,091 EPOCH 6163
2024-02-07 21:07:42,544 Epoch 6163: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-07 21:07:42,545 EPOCH 6164
2024-02-07 21:07:58,985 Epoch 6164: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-07 21:07:58,986 EPOCH 6165
2024-02-07 21:08:14,941 Epoch 6165: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-07 21:08:14,941 EPOCH 6166
2024-02-07 21:08:31,249 Epoch 6166: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-07 21:08:31,250 EPOCH 6167
2024-02-07 21:08:43,248 [Epoch: 6167 Step: 00055500] Batch Recognition Loss:   0.000741 => Gls Tokens per Sec:      565 || Batch Translation Loss:   0.043220 => Txt Tokens per Sec:     1543 || Lr: 0.000050
2024-02-07 21:08:47,294 Epoch 6167: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-07 21:08:47,294 EPOCH 6168
2024-02-07 21:09:03,654 Epoch 6168: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 21:09:03,655 EPOCH 6169
2024-02-07 21:09:19,916 Epoch 6169: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 21:09:19,917 EPOCH 6170
2024-02-07 21:09:36,053 Epoch 6170: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 21:09:36,054 EPOCH 6171
2024-02-07 21:09:52,367 Epoch 6171: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 21:09:52,368 EPOCH 6172
2024-02-07 21:10:08,348 Epoch 6172: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 21:10:08,349 EPOCH 6173
2024-02-07 21:10:24,414 Epoch 6173: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 21:10:24,415 EPOCH 6174
2024-02-07 21:10:40,681 Epoch 6174: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 21:10:40,681 EPOCH 6175
2024-02-07 21:10:56,501 Epoch 6175: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 21:10:56,501 EPOCH 6176
2024-02-07 21:11:12,382 Epoch 6176: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 21:11:12,383 EPOCH 6177
2024-02-07 21:11:28,603 Epoch 6177: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 21:11:28,603 EPOCH 6178
2024-02-07 21:11:38,774 [Epoch: 6178 Step: 00055600] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:      793 || Batch Translation Loss:   0.011340 => Txt Tokens per Sec:     2230 || Lr: 0.000050
2024-02-07 21:11:44,850 Epoch 6178: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 21:11:44,850 EPOCH 6179
2024-02-07 21:12:00,884 Epoch 6179: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 21:12:00,884 EPOCH 6180
2024-02-07 21:12:17,407 Epoch 6180: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 21:12:17,407 EPOCH 6181
2024-02-07 21:12:33,688 Epoch 6181: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 21:12:33,688 EPOCH 6182
2024-02-07 21:12:49,878 Epoch 6182: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 21:12:49,879 EPOCH 6183
2024-02-07 21:13:05,961 Epoch 6183: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 21:13:05,962 EPOCH 6184
2024-02-07 21:13:22,486 Epoch 6184: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 21:13:22,487 EPOCH 6185
2024-02-07 21:13:38,788 Epoch 6185: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:13:38,789 EPOCH 6186
2024-02-07 21:13:54,856 Epoch 6186: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:13:54,856 EPOCH 6187
2024-02-07 21:14:11,140 Epoch 6187: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:14:11,141 EPOCH 6188
2024-02-07 21:14:27,118 Epoch 6188: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:14:27,118 EPOCH 6189
2024-02-07 21:14:42,920 [Epoch: 6189 Step: 00055700] Batch Recognition Loss:   0.000412 => Gls Tokens per Sec:      591 || Batch Translation Loss:   0.016416 => Txt Tokens per Sec:     1714 || Lr: 0.000050
2024-02-07 21:14:43,330 Epoch 6189: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:14:43,330 EPOCH 6190
2024-02-07 21:14:59,719 Epoch 6190: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:14:59,719 EPOCH 6191
2024-02-07 21:15:15,764 Epoch 6191: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:15:15,764 EPOCH 6192
2024-02-07 21:15:31,554 Epoch 6192: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:15:31,554 EPOCH 6193
2024-02-07 21:15:47,967 Epoch 6193: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:15:47,968 EPOCH 6194
2024-02-07 21:16:03,934 Epoch 6194: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 21:16:03,934 EPOCH 6195
2024-02-07 21:16:20,066 Epoch 6195: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:16:20,066 EPOCH 6196
2024-02-07 21:16:36,239 Epoch 6196: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:16:36,239 EPOCH 6197
2024-02-07 21:16:52,211 Epoch 6197: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:16:52,212 EPOCH 6198
2024-02-07 21:17:08,302 Epoch 6198: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:17:08,302 EPOCH 6199
2024-02-07 21:17:24,423 Epoch 6199: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:17:24,424 EPOCH 6200
2024-02-07 21:17:40,649 [Epoch: 6200 Step: 00055800] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:      655 || Batch Translation Loss:   0.012284 => Txt Tokens per Sec:     1811 || Lr: 0.000050
2024-02-07 21:17:40,650 Epoch 6200: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:17:40,650 EPOCH 6201
2024-02-07 21:17:56,894 Epoch 6201: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:17:56,894 EPOCH 6202
2024-02-07 21:18:13,061 Epoch 6202: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:18:13,061 EPOCH 6203
2024-02-07 21:18:29,187 Epoch 6203: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:18:29,187 EPOCH 6204
2024-02-07 21:18:44,844 Epoch 6204: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:18:44,844 EPOCH 6205
2024-02-07 21:19:00,754 Epoch 6205: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:19:00,755 EPOCH 6206
2024-02-07 21:19:16,888 Epoch 6206: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:19:16,888 EPOCH 6207
2024-02-07 21:19:33,016 Epoch 6207: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:19:33,017 EPOCH 6208
2024-02-07 21:19:48,700 Epoch 6208: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 21:19:48,701 EPOCH 6209
2024-02-07 21:20:04,694 Epoch 6209: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 21:20:04,695 EPOCH 6210
2024-02-07 21:20:20,959 Epoch 6210: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 21:20:20,960 EPOCH 6211
2024-02-07 21:20:37,162 Epoch 6211: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 21:20:37,162 EPOCH 6212
2024-02-07 21:20:37,622 [Epoch: 6212 Step: 00055900] Batch Recognition Loss:   0.000505 => Gls Tokens per Sec:     2793 || Batch Translation Loss:   0.014194 => Txt Tokens per Sec:     7765 || Lr: 0.000050
2024-02-07 21:20:53,265 Epoch 6212: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 21:20:53,266 EPOCH 6213
2024-02-07 21:21:09,538 Epoch 6213: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 21:21:09,539 EPOCH 6214
2024-02-07 21:21:25,672 Epoch 6214: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 21:21:25,672 EPOCH 6215
2024-02-07 21:21:41,817 Epoch 6215: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 21:21:41,817 EPOCH 6216
2024-02-07 21:21:57,764 Epoch 6216: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 21:21:57,765 EPOCH 6217
2024-02-07 21:22:14,020 Epoch 6217: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 21:22:14,020 EPOCH 6218
2024-02-07 21:22:29,792 Epoch 6218: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 21:22:29,793 EPOCH 6219
2024-02-07 21:22:46,027 Epoch 6219: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:22:46,028 EPOCH 6220
2024-02-07 21:23:02,181 Epoch 6220: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:23:02,182 EPOCH 6221
2024-02-07 21:23:18,670 Epoch 6221: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:23:18,670 EPOCH 6222
2024-02-07 21:23:34,914 Epoch 6222: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:23:34,915 EPOCH 6223
2024-02-07 21:23:38,481 [Epoch: 6223 Step: 00056000] Batch Recognition Loss:   0.000395 => Gls Tokens per Sec:      718 || Batch Translation Loss:   0.014239 => Txt Tokens per Sec:     2148 || Lr: 0.000050
2024-02-07 21:24:46,150 Validation result at epoch 6223, step    56000: duration: 67.6682s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.24148	Translation Loss: 107654.69531	PPL: 46749.85156
	Eval Metric: BLEU
	WER 2.33	(DEL: 0.00,	INS: 0.00,	SUB: 2.33)
	BLEU-4 0.40	(BLEU-1: 9.46,	BLEU-2: 2.64,	BLEU-3: 0.96,	BLEU-4: 0.40)
	CHRF 16.64	ROUGE 7.97
2024-02-07 21:24:46,152 Logging Recognition and Translation Outputs
2024-02-07 21:24:46,152 ========================================================================================================================
2024-02-07 21:24:46,152 Logging Sequence: 177_167.00
2024-02-07 21:24:46,153 	Gloss Reference :	A B+C+D+E
2024-02-07 21:24:46,153 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 21:24:46,153 	Gloss Alignment :	         
2024-02-07 21:24:46,154 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 21:24:46,157 	Text Reference  :	this is because sushil wanted   to   establish his   fear   to ensure   no  one    would oppose him        
2024-02-07 21:24:46,157 	Text Hypothesis :	**** ** and     police detained over a         dozen people in brussels and others have  been   quarantined
2024-02-07 21:24:46,157 	Text Alignment  :	D    D  S       S      S        S    S         S     S      S  S        S   S      S     S      S          
2024-02-07 21:24:46,158 ========================================================================================================================
2024-02-07 21:24:46,158 Logging Sequence: 127_140.00
2024-02-07 21:24:46,158 	Gloss Reference :	A B+C+D+E
2024-02-07 21:24:46,158 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 21:24:46,158 	Gloss Alignment :	         
2024-02-07 21:24:46,158 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 21:24:46,160 	Text Reference  :	this is india' 3rd   medal in  the world athletics championships he is very  talented and his     performance is   highly impressive 
2024-02-07 21:24:46,160 	Text Hypothesis :	**** ** ****** after india won the ***** ********* ************* ** ** match by       kkr batters could       have been   quarantined
2024-02-07 21:24:46,160 	Text Alignment  :	D    D  D      S     S     S       D     D         D             D  D  S     S        S   S       S           S    S      S          
2024-02-07 21:24:46,160 ========================================================================================================================
2024-02-07 21:24:46,160 Logging Sequence: 126_200.00
2024-02-07 21:24:46,161 	Gloss Reference :	A B+C+D+E
2024-02-07 21:24:46,161 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 21:24:46,161 	Gloss Alignment :	         
2024-02-07 21:24:46,161 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 21:24:46,161 	Text Reference  :	** let me  tell  you   about them        
2024-02-07 21:24:46,162 	Text Hypothesis :	he was her first count of    indiscipline
2024-02-07 21:24:46,162 	Text Alignment  :	I  S   S   S     S     S     S           
2024-02-07 21:24:46,162 ========================================================================================================================
2024-02-07 21:24:46,162 Logging Sequence: 104_119.00
2024-02-07 21:24:46,162 	Gloss Reference :	A B+C+D+E
2024-02-07 21:24:46,163 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 21:24:46,163 	Gloss Alignment :	         
2024-02-07 21:24:46,163 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 21:24:46,165 	Text Reference  :	famous chess players like   viswanathan anand   and   praggnanandhaa's coach  r    b     ramesh congratulated him    for        his   impressive performance
2024-02-07 21:24:46,165 	Text Hypothesis :	****** the   cameras caught indian      skipper rohit sharma           wiping away tears as     he            looked devastated after the        loss       
2024-02-07 21:24:46,165 	Text Alignment  :	D      S     S       S      S           S       S     S                S      S    S     S      S             S      S          S     S          S          
2024-02-07 21:24:46,165 ========================================================================================================================
2024-02-07 21:24:46,166 Logging Sequence: 172_267.00
2024-02-07 21:24:46,166 	Gloss Reference :	A B+C+D+E
2024-02-07 21:24:46,166 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 21:24:46,166 	Gloss Alignment :	         
2024-02-07 21:24:46,166 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 21:24:46,167 	Text Reference  :	*** ** **** such provisions have been made
2024-02-07 21:24:46,167 	Text Hypothesis :	let me tell you  about      it   be   yet 
2024-02-07 21:24:46,167 	Text Alignment  :	I   I  I    S    S          S    S    S   
2024-02-07 21:24:46,167 ========================================================================================================================
2024-02-07 21:24:59,022 Epoch 6223: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:24:59,023 EPOCH 6224
2024-02-07 21:25:15,139 Epoch 6224: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:25:15,140 EPOCH 6225
2024-02-07 21:25:31,085 Epoch 6225: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:25:31,085 EPOCH 6226
2024-02-07 21:25:46,770 Epoch 6226: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:25:46,771 EPOCH 6227
2024-02-07 21:26:03,358 Epoch 6227: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:26:03,359 EPOCH 6228
2024-02-07 21:26:19,384 Epoch 6228: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:26:19,384 EPOCH 6229
2024-02-07 21:26:35,676 Epoch 6229: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:26:35,677 EPOCH 6230
2024-02-07 21:26:51,754 Epoch 6230: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:26:51,754 EPOCH 6231
2024-02-07 21:27:07,818 Epoch 6231: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:27:07,819 EPOCH 6232
2024-02-07 21:27:23,881 Epoch 6232: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:27:23,881 EPOCH 6233
2024-02-07 21:27:40,186 Epoch 6233: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:27:40,187 EPOCH 6234
2024-02-07 21:27:46,576 [Epoch: 6234 Step: 00056100] Batch Recognition Loss:   0.000398 => Gls Tokens per Sec:      601 || Batch Translation Loss:   0.015349 => Txt Tokens per Sec:     1645 || Lr: 0.000050
2024-02-07 21:27:56,092 Epoch 6234: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:27:56,093 EPOCH 6235
2024-02-07 21:28:12,327 Epoch 6235: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:28:12,328 EPOCH 6236
2024-02-07 21:28:28,329 Epoch 6236: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:28:28,329 EPOCH 6237
2024-02-07 21:28:44,617 Epoch 6237: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:28:44,617 EPOCH 6238
2024-02-07 21:29:00,838 Epoch 6238: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 21:29:00,839 EPOCH 6239
2024-02-07 21:29:17,105 Epoch 6239: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 21:29:17,106 EPOCH 6240
2024-02-07 21:29:33,161 Epoch 6240: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 21:29:33,161 EPOCH 6241
2024-02-07 21:29:49,593 Epoch 6241: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:29:49,593 EPOCH 6242
2024-02-07 21:30:05,674 Epoch 6242: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:30:05,675 EPOCH 6243
2024-02-07 21:30:22,037 Epoch 6243: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:30:22,037 EPOCH 6244
2024-02-07 21:30:38,147 Epoch 6244: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:30:38,148 EPOCH 6245
2024-02-07 21:30:49,314 [Epoch: 6245 Step: 00056200] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:      378 || Batch Translation Loss:   0.015199 => Txt Tokens per Sec:     1159 || Lr: 0.000050
2024-02-07 21:30:54,131 Epoch 6245: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:30:54,131 EPOCH 6246
2024-02-07 21:31:10,443 Epoch 6246: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:31:10,443 EPOCH 6247
2024-02-07 21:31:26,949 Epoch 6247: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:31:26,950 EPOCH 6248
2024-02-07 21:31:42,809 Epoch 6248: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:31:42,810 EPOCH 6249
2024-02-07 21:31:58,905 Epoch 6249: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 21:31:58,905 EPOCH 6250
2024-02-07 21:32:15,120 Epoch 6250: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:32:15,121 EPOCH 6251
2024-02-07 21:32:31,204 Epoch 6251: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:32:31,205 EPOCH 6252
2024-02-07 21:32:47,338 Epoch 6252: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:32:47,339 EPOCH 6253
2024-02-07 21:33:03,669 Epoch 6253: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 21:33:03,670 EPOCH 6254
2024-02-07 21:33:19,615 Epoch 6254: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 21:33:19,616 EPOCH 6255
2024-02-07 21:33:35,825 Epoch 6255: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 21:33:35,826 EPOCH 6256
2024-02-07 21:33:37,927 [Epoch: 6256 Step: 00056300] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:     3048 || Batch Translation Loss:   0.025865 => Txt Tokens per Sec:     7611 || Lr: 0.000050
2024-02-07 21:33:51,775 Epoch 6256: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 21:33:51,775 EPOCH 6257
2024-02-07 21:34:08,016 Epoch 6257: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 21:34:08,017 EPOCH 6258
2024-02-07 21:34:24,204 Epoch 6258: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 21:34:24,204 EPOCH 6259
2024-02-07 21:34:40,473 Epoch 6259: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 21:34:40,473 EPOCH 6260
2024-02-07 21:34:56,608 Epoch 6260: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 21:34:56,609 EPOCH 6261
2024-02-07 21:35:12,519 Epoch 6261: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 21:35:12,520 EPOCH 6262
2024-02-07 21:35:28,836 Epoch 6262: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 21:35:28,837 EPOCH 6263
2024-02-07 21:35:44,930 Epoch 6263: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:35:44,931 EPOCH 6264
2024-02-07 21:36:00,831 Epoch 6264: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:36:00,831 EPOCH 6265
2024-02-07 21:36:16,803 Epoch 6265: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 21:36:16,803 EPOCH 6266
2024-02-07 21:36:32,984 Epoch 6266: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 21:36:32,985 EPOCH 6267
2024-02-07 21:36:38,314 [Epoch: 6267 Step: 00056400] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:     1441 || Batch Translation Loss:   0.016904 => Txt Tokens per Sec:     3701 || Lr: 0.000050
2024-02-07 21:36:49,267 Epoch 6267: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 21:36:49,268 EPOCH 6268
2024-02-07 21:37:05,358 Epoch 6268: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-07 21:37:05,358 EPOCH 6269
2024-02-07 21:37:21,283 Epoch 6269: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-07 21:37:21,284 EPOCH 6270
2024-02-07 21:37:37,554 Epoch 6270: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-07 21:37:37,554 EPOCH 6271
2024-02-07 21:37:53,687 Epoch 6271: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-07 21:37:53,687 EPOCH 6272
2024-02-07 21:38:09,543 Epoch 6272: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-07 21:38:09,544 EPOCH 6273
2024-02-07 21:38:26,091 Epoch 6273: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-07 21:38:26,092 EPOCH 6274
2024-02-07 21:38:42,504 Epoch 6274: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-07 21:38:42,504 EPOCH 6275
2024-02-07 21:38:58,406 Epoch 6275: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-07 21:38:58,407 EPOCH 6276
2024-02-07 21:39:14,796 Epoch 6276: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-07 21:39:14,796 EPOCH 6277
2024-02-07 21:39:31,072 Epoch 6277: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-07 21:39:31,073 EPOCH 6278
2024-02-07 21:39:42,832 [Epoch: 6278 Step: 00056500] Batch Recognition Loss:   0.000575 => Gls Tokens per Sec:      762 || Batch Translation Loss:   0.024943 => Txt Tokens per Sec:     2188 || Lr: 0.000050
2024-02-07 21:39:47,478 Epoch 6278: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 21:39:47,478 EPOCH 6279
2024-02-07 21:40:03,745 Epoch 6279: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 21:40:03,746 EPOCH 6280
2024-02-07 21:40:19,948 Epoch 6280: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 21:40:19,948 EPOCH 6281
2024-02-07 21:40:36,365 Epoch 6281: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 21:40:36,365 EPOCH 6282
2024-02-07 21:40:52,250 Epoch 6282: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 21:40:52,251 EPOCH 6283
2024-02-07 21:41:08,197 Epoch 6283: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 21:41:08,198 EPOCH 6284
2024-02-07 21:41:24,286 Epoch 6284: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 21:41:24,287 EPOCH 6285
2024-02-07 21:41:40,597 Epoch 6285: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 21:41:40,598 EPOCH 6286
2024-02-07 21:41:56,660 Epoch 6286: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 21:41:56,661 EPOCH 6287
2024-02-07 21:42:13,018 Epoch 6287: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 21:42:13,018 EPOCH 6288
2024-02-07 21:42:29,136 Epoch 6288: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 21:42:29,137 EPOCH 6289
2024-02-07 21:42:39,078 [Epoch: 6289 Step: 00056600] Batch Recognition Loss:   0.000514 => Gls Tokens per Sec:      940 || Batch Translation Loss:   0.006524 => Txt Tokens per Sec:     2513 || Lr: 0.000050
2024-02-07 21:42:45,141 Epoch 6289: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:42:45,142 EPOCH 6290
2024-02-07 21:43:01,762 Epoch 6290: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:43:01,763 EPOCH 6291
2024-02-07 21:43:18,107 Epoch 6291: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:43:18,108 EPOCH 6292
2024-02-07 21:43:34,199 Epoch 6292: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:43:34,200 EPOCH 6293
2024-02-07 21:43:50,210 Epoch 6293: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:43:50,210 EPOCH 6294
2024-02-07 21:44:06,091 Epoch 6294: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:44:06,092 EPOCH 6295
2024-02-07 21:44:22,151 Epoch 6295: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:44:22,152 EPOCH 6296
2024-02-07 21:44:38,368 Epoch 6296: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:44:38,369 EPOCH 6297
2024-02-07 21:44:54,448 Epoch 6297: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:44:54,449 EPOCH 6298
2024-02-07 21:45:10,870 Epoch 6298: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:45:10,870 EPOCH 6299
2024-02-07 21:45:26,881 Epoch 6299: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:45:26,881 EPOCH 6300
2024-02-07 21:45:43,168 [Epoch: 6300 Step: 00056700] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:      652 || Batch Translation Loss:   0.010588 => Txt Tokens per Sec:     1804 || Lr: 0.000050
2024-02-07 21:45:43,169 Epoch 6300: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:45:43,169 EPOCH 6301
2024-02-07 21:45:59,226 Epoch 6301: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:45:59,227 EPOCH 6302
2024-02-07 21:46:15,332 Epoch 6302: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:46:15,333 EPOCH 6303
2024-02-07 21:46:31,406 Epoch 6303: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:46:31,407 EPOCH 6304
2024-02-07 21:46:47,443 Epoch 6304: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:46:47,444 EPOCH 6305
2024-02-07 21:47:03,630 Epoch 6305: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:47:03,630 EPOCH 6306
2024-02-07 21:47:19,628 Epoch 6306: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:47:19,629 EPOCH 6307
2024-02-07 21:47:35,555 Epoch 6307: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:47:35,555 EPOCH 6308
2024-02-07 21:47:51,952 Epoch 6308: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:47:51,952 EPOCH 6309
2024-02-07 21:48:07,888 Epoch 6309: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:48:07,888 EPOCH 6310
2024-02-07 21:48:24,012 Epoch 6310: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:48:24,014 EPOCH 6311
2024-02-07 21:48:39,934 Epoch 6311: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:48:39,934 EPOCH 6312
2024-02-07 21:48:44,242 [Epoch: 6312 Step: 00056800] Batch Recognition Loss:   0.000466 => Gls Tokens per Sec:       88 || Batch Translation Loss:   0.005242 => Txt Tokens per Sec:      314 || Lr: 0.000050
2024-02-07 21:48:56,169 Epoch 6312: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:48:56,170 EPOCH 6313
2024-02-07 21:49:11,907 Epoch 6313: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:49:11,907 EPOCH 6314
2024-02-07 21:49:27,873 Epoch 6314: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:49:27,874 EPOCH 6315
2024-02-07 21:49:44,065 Epoch 6315: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 21:49:44,066 EPOCH 6316
2024-02-07 21:50:00,104 Epoch 6316: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:50:00,105 EPOCH 6317
2024-02-07 21:50:16,255 Epoch 6317: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:50:16,256 EPOCH 6318
2024-02-07 21:50:32,451 Epoch 6318: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 21:50:32,452 EPOCH 6319
2024-02-07 21:50:48,557 Epoch 6319: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:50:48,558 EPOCH 6320
2024-02-07 21:51:04,884 Epoch 6320: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:51:04,885 EPOCH 6321
2024-02-07 21:51:21,188 Epoch 6321: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:51:21,189 EPOCH 6322
2024-02-07 21:51:37,229 Epoch 6322: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 21:51:37,229 EPOCH 6323
2024-02-07 21:51:42,059 [Epoch: 6323 Step: 00056900] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:      344 || Batch Translation Loss:   0.013087 => Txt Tokens per Sec:     1044 || Lr: 0.000050
2024-02-07 21:51:53,441 Epoch 6323: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:51:53,441 EPOCH 6324
2024-02-07 21:52:09,786 Epoch 6324: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 21:52:09,787 EPOCH 6325
2024-02-07 21:52:25,903 Epoch 6325: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 21:52:25,904 EPOCH 6326
2024-02-07 21:52:42,032 Epoch 6326: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 21:52:42,033 EPOCH 6327
2024-02-07 21:52:57,937 Epoch 6327: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:52:57,937 EPOCH 6328
2024-02-07 21:53:14,299 Epoch 6328: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:53:14,300 EPOCH 6329
2024-02-07 21:53:30,700 Epoch 6329: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:53:30,701 EPOCH 6330
2024-02-07 21:53:46,933 Epoch 6330: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:53:46,934 EPOCH 6331
2024-02-07 21:54:02,810 Epoch 6331: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:54:02,811 EPOCH 6332
2024-02-07 21:54:19,070 Epoch 6332: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:54:19,070 EPOCH 6333
2024-02-07 21:54:35,266 Epoch 6333: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:54:35,267 EPOCH 6334
2024-02-07 21:54:39,356 [Epoch: 6334 Step: 00057000] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:      939 || Batch Translation Loss:   0.010473 => Txt Tokens per Sec:     2775 || Lr: 0.000050
2024-02-07 21:54:51,311 Epoch 6334: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 21:54:51,311 EPOCH 6335
2024-02-07 21:55:07,586 Epoch 6335: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:55:07,587 EPOCH 6336
2024-02-07 21:55:23,810 Epoch 6336: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:55:23,811 EPOCH 6337
2024-02-07 21:55:39,870 Epoch 6337: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:55:39,871 EPOCH 6338
2024-02-07 21:55:56,218 Epoch 6338: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:55:56,219 EPOCH 6339
2024-02-07 21:56:12,509 Epoch 6339: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:56:12,509 EPOCH 6340
2024-02-07 21:56:28,863 Epoch 6340: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:56:28,864 EPOCH 6341
2024-02-07 21:56:45,153 Epoch 6341: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 21:56:45,154 EPOCH 6342
2024-02-07 21:57:01,201 Epoch 6342: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:57:01,202 EPOCH 6343
2024-02-07 21:57:17,279 Epoch 6343: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 21:57:17,280 EPOCH 6344
2024-02-07 21:57:33,401 Epoch 6344: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 21:57:33,401 EPOCH 6345
2024-02-07 21:57:43,250 [Epoch: 6345 Step: 00057100] Batch Recognition Loss:   0.000470 => Gls Tokens per Sec:      520 || Batch Translation Loss:   0.007022 => Txt Tokens per Sec:     1389 || Lr: 0.000050
2024-02-07 21:57:49,662 Epoch 6345: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 21:57:49,662 EPOCH 6346
2024-02-07 21:58:05,497 Epoch 6346: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 21:58:05,498 EPOCH 6347
2024-02-07 21:58:21,874 Epoch 6347: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 21:58:21,875 EPOCH 6348
2024-02-07 21:58:38,197 Epoch 6348: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:58:38,198 EPOCH 6349
2024-02-07 21:58:54,224 Epoch 6349: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 21:58:54,225 EPOCH 6350
2024-02-07 21:59:10,675 Epoch 6350: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 21:59:10,676 EPOCH 6351
2024-02-07 21:59:26,765 Epoch 6351: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 21:59:26,765 EPOCH 6352
2024-02-07 21:59:42,687 Epoch 6352: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 21:59:42,687 EPOCH 6353
2024-02-07 21:59:58,876 Epoch 6353: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 21:59:58,876 EPOCH 6354
2024-02-07 22:00:15,114 Epoch 6354: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 22:00:15,115 EPOCH 6355
2024-02-07 22:00:31,263 Epoch 6355: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 22:00:31,264 EPOCH 6356
2024-02-07 22:00:37,576 [Epoch: 6356 Step: 00057200] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:      872 || Batch Translation Loss:   0.012842 => Txt Tokens per Sec:     2419 || Lr: 0.000050
2024-02-07 22:00:47,411 Epoch 6356: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 22:00:47,412 EPOCH 6357
2024-02-07 22:01:03,362 Epoch 6357: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 22:01:03,363 EPOCH 6358
2024-02-07 22:01:19,651 Epoch 6358: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 22:01:19,651 EPOCH 6359
2024-02-07 22:01:35,828 Epoch 6359: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 22:01:35,829 EPOCH 6360
2024-02-07 22:01:51,907 Epoch 6360: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-07 22:01:51,907 EPOCH 6361
2024-02-07 22:02:08,061 Epoch 6361: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-07 22:02:08,062 EPOCH 6362
2024-02-07 22:02:24,011 Epoch 6362: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.78 
2024-02-07 22:02:24,011 EPOCH 6363
2024-02-07 22:02:40,050 Epoch 6363: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.04 
2024-02-07 22:02:40,050 EPOCH 6364
2024-02-07 22:02:56,042 Epoch 6364: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-07 22:02:56,043 EPOCH 6365
2024-02-07 22:03:12,138 Epoch 6365: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-07 22:03:12,138 EPOCH 6366
2024-02-07 22:03:28,479 Epoch 6366: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.20 
2024-02-07 22:03:28,480 EPOCH 6367
2024-02-07 22:03:37,983 [Epoch: 6367 Step: 00057300] Batch Recognition Loss:   0.001107 => Gls Tokens per Sec:      713 || Batch Translation Loss:   0.101593 => Txt Tokens per Sec:     1936 || Lr: 0.000050
2024-02-07 22:03:44,583 Epoch 6367: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.65 
2024-02-07 22:03:44,583 EPOCH 6368
2024-02-07 22:04:00,530 Epoch 6368: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.26 
2024-02-07 22:04:00,531 EPOCH 6369
2024-02-07 22:04:16,989 Epoch 6369: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.72 
2024-02-07 22:04:16,990 EPOCH 6370
2024-02-07 22:04:33,106 Epoch 6370: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.73 
2024-02-07 22:04:33,106 EPOCH 6371
2024-02-07 22:04:49,332 Epoch 6371: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-07 22:04:49,333 EPOCH 6372
2024-02-07 22:05:05,445 Epoch 6372: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-07 22:05:05,446 EPOCH 6373
2024-02-07 22:05:21,572 Epoch 6373: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 22:05:21,573 EPOCH 6374
2024-02-07 22:05:37,859 Epoch 6374: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 22:05:37,860 EPOCH 6375
2024-02-07 22:05:53,953 Epoch 6375: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 22:05:53,954 EPOCH 6376
2024-02-07 22:06:09,908 Epoch 6376: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 22:06:09,909 EPOCH 6377
2024-02-07 22:06:25,807 Epoch 6377: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 22:06:25,808 EPOCH 6378
2024-02-07 22:06:38,159 [Epoch: 6378 Step: 00057400] Batch Recognition Loss:   0.001004 => Gls Tokens per Sec:      653 || Batch Translation Loss:   0.019518 => Txt Tokens per Sec:     1784 || Lr: 0.000050
2024-02-07 22:06:41,823 Epoch 6378: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 22:06:41,823 EPOCH 6379
2024-02-07 22:06:58,194 Epoch 6379: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 22:06:58,194 EPOCH 6380
2024-02-07 22:07:14,271 Epoch 6380: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 22:07:14,272 EPOCH 6381
2024-02-07 22:07:30,599 Epoch 6381: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 22:07:30,599 EPOCH 6382
2024-02-07 22:07:46,723 Epoch 6382: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 22:07:46,723 EPOCH 6383
2024-02-07 22:08:02,918 Epoch 6383: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 22:08:02,919 EPOCH 6384
2024-02-07 22:08:19,075 Epoch 6384: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 22:08:19,076 EPOCH 6385
2024-02-07 22:08:35,155 Epoch 6385: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 22:08:35,156 EPOCH 6386
2024-02-07 22:08:51,394 Epoch 6386: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 22:08:51,394 EPOCH 6387
2024-02-07 22:09:07,303 Epoch 6387: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 22:09:07,304 EPOCH 6388
2024-02-07 22:09:23,563 Epoch 6388: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:09:23,563 EPOCH 6389
2024-02-07 22:09:39,441 [Epoch: 6389 Step: 00057500] Batch Recognition Loss:   0.000288 => Gls Tokens per Sec:      588 || Batch Translation Loss:   0.019367 => Txt Tokens per Sec:     1705 || Lr: 0.000050
2024-02-07 22:09:39,680 Epoch 6389: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 22:09:39,680 EPOCH 6390
2024-02-07 22:09:55,644 Epoch 6390: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 22:09:55,645 EPOCH 6391
2024-02-07 22:10:11,914 Epoch 6391: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-07 22:10:11,914 EPOCH 6392
2024-02-07 22:10:27,944 Epoch 6392: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:10:27,945 EPOCH 6393
2024-02-07 22:10:43,805 Epoch 6393: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:10:43,806 EPOCH 6394
2024-02-07 22:11:00,453 Epoch 6394: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:11:00,454 EPOCH 6395
2024-02-07 22:11:16,603 Epoch 6395: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:11:16,604 EPOCH 6396
2024-02-07 22:11:32,945 Epoch 6396: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:11:32,946 EPOCH 6397
2024-02-07 22:11:49,033 Epoch 6397: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 22:11:49,033 EPOCH 6398
2024-02-07 22:12:05,095 Epoch 6398: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:12:05,095 EPOCH 6399
2024-02-07 22:12:21,749 Epoch 6399: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:12:21,749 EPOCH 6400
2024-02-07 22:12:37,910 [Epoch: 6400 Step: 00057600] Batch Recognition Loss:   0.000373 => Gls Tokens per Sec:      657 || Batch Translation Loss:   0.011757 => Txt Tokens per Sec:     1818 || Lr: 0.000050
2024-02-07 22:12:37,911 Epoch 6400: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:12:37,911 EPOCH 6401
2024-02-07 22:12:54,205 Epoch 6401: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 22:12:54,205 EPOCH 6402
2024-02-07 22:13:10,542 Epoch 6402: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:13:10,543 EPOCH 6403
2024-02-07 22:13:26,738 Epoch 6403: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:13:26,738 EPOCH 6404
2024-02-07 22:13:42,819 Epoch 6404: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:13:42,819 EPOCH 6405
2024-02-07 22:13:59,663 Epoch 6405: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:13:59,663 EPOCH 6406
2024-02-07 22:14:15,512 Epoch 6406: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:14:15,513 EPOCH 6407
2024-02-07 22:14:31,644 Epoch 6407: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:14:31,644 EPOCH 6408
2024-02-07 22:14:47,827 Epoch 6408: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:14:47,828 EPOCH 6409
2024-02-07 22:15:03,815 Epoch 6409: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:15:03,816 EPOCH 6410
2024-02-07 22:15:19,862 Epoch 6410: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:15:19,863 EPOCH 6411
2024-02-07 22:15:35,957 Epoch 6411: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:15:35,957 EPOCH 6412
2024-02-07 22:15:36,705 [Epoch: 6412 Step: 00057700] Batch Recognition Loss:   0.000298 => Gls Tokens per Sec:     1713 || Batch Translation Loss:   0.014719 => Txt Tokens per Sec:     4976 || Lr: 0.000050
2024-02-07 22:15:52,355 Epoch 6412: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:15:52,356 EPOCH 6413
2024-02-07 22:16:08,686 Epoch 6413: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:16:08,688 EPOCH 6414
2024-02-07 22:16:24,739 Epoch 6414: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:16:24,740 EPOCH 6415
2024-02-07 22:16:40,960 Epoch 6415: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:16:40,961 EPOCH 6416
2024-02-07 22:16:57,026 Epoch 6416: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:16:57,026 EPOCH 6417
2024-02-07 22:17:13,258 Epoch 6417: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:17:13,258 EPOCH 6418
2024-02-07 22:17:29,619 Epoch 6418: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:17:29,620 EPOCH 6419
2024-02-07 22:17:45,805 Epoch 6419: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:17:45,805 EPOCH 6420
2024-02-07 22:18:01,802 Epoch 6420: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:18:01,803 EPOCH 6421
2024-02-07 22:18:17,995 Epoch 6421: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:18:17,996 EPOCH 6422
2024-02-07 22:18:34,011 Epoch 6422: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:18:34,011 EPOCH 6423
2024-02-07 22:18:37,599 [Epoch: 6423 Step: 00057800] Batch Recognition Loss:   0.000319 => Gls Tokens per Sec:      714 || Batch Translation Loss:   0.012324 => Txt Tokens per Sec:     1785 || Lr: 0.000050
2024-02-07 22:18:50,271 Epoch 6423: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:18:50,272 EPOCH 6424
2024-02-07 22:19:06,261 Epoch 6424: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:19:06,261 EPOCH 6425
2024-02-07 22:19:22,377 Epoch 6425: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:19:22,377 EPOCH 6426
2024-02-07 22:19:38,492 Epoch 6426: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:19:38,492 EPOCH 6427
2024-02-07 22:19:54,485 Epoch 6427: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:19:54,485 EPOCH 6428
2024-02-07 22:20:10,625 Epoch 6428: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:20:10,626 EPOCH 6429
2024-02-07 22:20:26,717 Epoch 6429: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:20:26,718 EPOCH 6430
2024-02-07 22:20:42,963 Epoch 6430: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:20:42,964 EPOCH 6431
2024-02-07 22:20:59,070 Epoch 6431: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:20:59,071 EPOCH 6432
2024-02-07 22:21:15,281 Epoch 6432: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:21:15,281 EPOCH 6433
2024-02-07 22:21:31,450 Epoch 6433: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:21:31,450 EPOCH 6434
2024-02-07 22:21:32,375 [Epoch: 6434 Step: 00057900] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:     4155 || Batch Translation Loss:   0.010361 => Txt Tokens per Sec:     9406 || Lr: 0.000050
2024-02-07 22:21:47,505 Epoch 6434: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:21:47,506 EPOCH 6435
2024-02-07 22:22:03,561 Epoch 6435: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:22:03,562 EPOCH 6436
2024-02-07 22:22:19,784 Epoch 6436: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:22:19,785 EPOCH 6437
2024-02-07 22:22:35,920 Epoch 6437: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:22:35,920 EPOCH 6438
2024-02-07 22:22:52,281 Epoch 6438: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:22:52,281 EPOCH 6439
2024-02-07 22:23:08,256 Epoch 6439: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:23:08,257 EPOCH 6440
2024-02-07 22:23:24,446 Epoch 6440: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:23:24,446 EPOCH 6441
2024-02-07 22:23:40,835 Epoch 6441: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:23:40,835 EPOCH 6442
2024-02-07 22:23:57,088 Epoch 6442: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:23:57,088 EPOCH 6443
2024-02-07 22:24:13,507 Epoch 6443: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:24:13,507 EPOCH 6444
2024-02-07 22:24:29,627 Epoch 6444: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:24:29,627 EPOCH 6445
2024-02-07 22:24:35,711 [Epoch: 6445 Step: 00058000] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:      694 || Batch Translation Loss:   0.014358 => Txt Tokens per Sec:     1998 || Lr: 0.000050
2024-02-07 22:25:42,944 Validation result at epoch 6445, step    58000: duration: 67.2316s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.24362	Translation Loss: 107972.07812	PPL: 48255.56641
	Eval Metric: BLEU
	WER 2.33	(DEL: 0.00,	INS: 0.00,	SUB: 2.33)
	BLEU-4 0.00	(BLEU-1: 10.46,	BLEU-2: 2.81,	BLEU-3: 1.00,	BLEU-4: 0.00)
	CHRF 16.70	ROUGE 8.48
2024-02-07 22:25:42,946 Logging Recognition and Translation Outputs
2024-02-07 22:25:42,947 ========================================================================================================================
2024-02-07 22:25:42,947 Logging Sequence: 60_264.00
2024-02-07 22:25:42,947 	Gloss Reference :	A B+C+D+E
2024-02-07 22:25:42,947 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 22:25:42,947 	Gloss Alignment :	         
2024-02-07 22:25:42,947 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 22:25:42,950 	Text Reference  :	plus    do you      know  that a  sex tape of his with two   women had gone viral
2024-02-07 22:25:42,950 	Text Hypothesis :	however at amrapali group paid in the end  of you the  world cup   and talk more 
2024-02-07 22:25:42,950 	Text Alignment  :	S       S  S        S     S    S  S   S       S   S    S     S     S   S    S    
2024-02-07 22:25:42,950 ========================================================================================================================
2024-02-07 22:25:42,950 Logging Sequence: 100_50.00
2024-02-07 22:25:42,951 	Gloss Reference :	A B+C+D+E
2024-02-07 22:25:42,951 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 22:25:42,951 	Gloss Alignment :	         
2024-02-07 22:25:42,951 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 22:25:42,952 	Text Reference  :	with virat kohli as  the   captain
2024-02-07 22:25:42,952 	Text Hypothesis :	let  me    tell  you about it     
2024-02-07 22:25:42,952 	Text Alignment  :	S    S     S     S   S     S      
2024-02-07 22:25:42,952 ========================================================================================================================
2024-02-07 22:25:42,952 Logging Sequence: 137_44.00
2024-02-07 22:25:42,952 	Gloss Reference :	A B+C+D+E
2024-02-07 22:25:42,952 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 22:25:42,953 	Gloss Alignment :	         
2024-02-07 22:25:42,953 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 22:25:42,954 	Text Reference  :	let me tell you the rules that qatar has announced for the       fans  travelling for the   world cup  
2024-02-07 22:25:42,954 	Text Hypothesis :	*** ** **** *** *** ***** **** ***** *** ********* *** meanwhile there was        sad faced each  other
2024-02-07 22:25:42,954 	Text Alignment  :	D   D  D    D   D   D     D    D     D   D         D   S         S     S          S   S     S     S    
2024-02-07 22:25:42,954 ========================================================================================================================
2024-02-07 22:25:42,954 Logging Sequence: 58_27.00
2024-02-07 22:25:42,954 	Gloss Reference :	A B+C+D+E
2024-02-07 22:25:42,955 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 22:25:42,955 	Gloss Alignment :	         
2024-02-07 22:25:42,955 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 22:25:42,956 	Text Reference  :	***** the 19th asian games 2022 were to     be held in **** ** *** ** ** *** hangzhou china 
2024-02-07 22:25:42,956 	Text Hypothesis :	india has won  the   first time in   should be held in them by due to on his 50       medals
2024-02-07 22:25:42,957 	Text Alignment  :	I     S   S    S     S     S    S    S                 I    I  I   I  I  I   S        S     
2024-02-07 22:25:42,957 ========================================================================================================================
2024-02-07 22:25:42,957 Logging Sequence: 75_255.00
2024-02-07 22:25:42,957 	Gloss Reference :	A B+C+D+E  
2024-02-07 22:25:42,957 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-07 22:25:42,957 	Gloss Alignment :	  S        
2024-02-07 22:25:42,957 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 22:25:42,959 	Text Reference  :	we miss our baby boy with this ronaldo' total baby   count has     reached 5 with  2  boys 3   girls
2024-02-07 22:25:42,959 	Text Hypothesis :	** **** *** **** *** **** **** ******** later rooney then  invited the     3 girls to his  vip booth
2024-02-07 22:25:42,959 	Text Alignment  :	D  D    D   D    D   D    D    D        S     S      S     S       S       S S     S  S    S   S    
2024-02-07 22:25:42,959 ========================================================================================================================
2024-02-07 22:25:53,506 Epoch 6445: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:25:53,507 EPOCH 6446
2024-02-07 22:26:10,080 Epoch 6446: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:26:10,081 EPOCH 6447
2024-02-07 22:26:26,407 Epoch 6447: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:26:26,408 EPOCH 6448
2024-02-07 22:26:42,310 Epoch 6448: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:26:42,311 EPOCH 6449
2024-02-07 22:26:58,640 Epoch 6449: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:26:58,641 EPOCH 6450
2024-02-07 22:27:14,892 Epoch 6450: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:27:14,893 EPOCH 6451
2024-02-07 22:27:30,875 Epoch 6451: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:27:30,876 EPOCH 6452
2024-02-07 22:27:46,661 Epoch 6452: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:27:46,662 EPOCH 6453
2024-02-07 22:28:02,885 Epoch 6453: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:28:02,886 EPOCH 6454
2024-02-07 22:28:18,903 Epoch 6454: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:28:18,904 EPOCH 6455
2024-02-07 22:28:34,986 Epoch 6455: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:28:34,986 EPOCH 6456
2024-02-07 22:28:46,653 [Epoch: 6456 Step: 00058100] Batch Recognition Loss:   0.000446 => Gls Tokens per Sec:      471 || Batch Translation Loss:   0.013390 => Txt Tokens per Sec:     1342 || Lr: 0.000050
2024-02-07 22:28:51,274 Epoch 6456: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:28:51,275 EPOCH 6457
2024-02-07 22:29:07,437 Epoch 6457: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:29:07,438 EPOCH 6458
2024-02-07 22:29:23,444 Epoch 6458: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:29:23,445 EPOCH 6459
2024-02-07 22:29:39,735 Epoch 6459: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:29:39,735 EPOCH 6460
2024-02-07 22:29:55,682 Epoch 6460: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:29:55,683 EPOCH 6461
2024-02-07 22:30:11,787 Epoch 6461: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:30:11,788 EPOCH 6462
2024-02-07 22:30:28,002 Epoch 6462: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:30:28,002 EPOCH 6463
2024-02-07 22:30:44,358 Epoch 6463: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:30:44,358 EPOCH 6464
2024-02-07 22:31:00,461 Epoch 6464: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:31:00,462 EPOCH 6465
2024-02-07 22:31:16,470 Epoch 6465: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 22:31:16,470 EPOCH 6466
2024-02-07 22:31:33,036 Epoch 6466: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:31:33,036 EPOCH 6467
2024-02-07 22:31:43,683 [Epoch: 6467 Step: 00058200] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:      721 || Batch Translation Loss:   0.009550 => Txt Tokens per Sec:     2054 || Lr: 0.000050
2024-02-07 22:31:49,068 Epoch 6467: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:31:49,068 EPOCH 6468
2024-02-07 22:32:05,162 Epoch 6468: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 22:32:05,163 EPOCH 6469
2024-02-07 22:32:21,203 Epoch 6469: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:32:21,204 EPOCH 6470
2024-02-07 22:32:37,460 Epoch 6470: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:32:37,461 EPOCH 6471
2024-02-07 22:32:53,420 Epoch 6471: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:32:53,420 EPOCH 6472
2024-02-07 22:33:09,420 Epoch 6472: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:33:09,420 EPOCH 6473
2024-02-07 22:33:25,614 Epoch 6473: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:33:25,615 EPOCH 6474
2024-02-07 22:33:41,909 Epoch 6474: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:33:41,909 EPOCH 6475
2024-02-07 22:33:57,702 Epoch 6475: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:33:57,702 EPOCH 6476
2024-02-07 22:34:14,133 Epoch 6476: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:34:14,133 EPOCH 6477
2024-02-07 22:34:30,197 Epoch 6477: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:34:30,198 EPOCH 6478
2024-02-07 22:34:41,119 [Epoch: 6478 Step: 00058300] Batch Recognition Loss:   0.000163 => Gls Tokens per Sec:      821 || Batch Translation Loss:   0.005644 => Txt Tokens per Sec:     2213 || Lr: 0.000050
2024-02-07 22:34:46,118 Epoch 6478: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:34:46,118 EPOCH 6479
2024-02-07 22:35:02,127 Epoch 6479: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 22:35:02,127 EPOCH 6480
2024-02-07 22:35:18,509 Epoch 6480: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 22:35:18,510 EPOCH 6481
2024-02-07 22:35:34,561 Epoch 6481: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:35:34,562 EPOCH 6482
2024-02-07 22:35:50,791 Epoch 6482: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:35:50,791 EPOCH 6483
2024-02-07 22:36:07,124 Epoch 6483: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:36:07,124 EPOCH 6484
2024-02-07 22:36:23,151 Epoch 6484: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:36:23,152 EPOCH 6485
2024-02-07 22:36:39,225 Epoch 6485: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:36:39,226 EPOCH 6486
2024-02-07 22:36:55,838 Epoch 6486: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:36:55,839 EPOCH 6487
2024-02-07 22:37:12,074 Epoch 6487: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:37:12,074 EPOCH 6488
2024-02-07 22:37:28,101 Epoch 6488: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:37:28,102 EPOCH 6489
2024-02-07 22:37:43,628 [Epoch: 6489 Step: 00058400] Batch Recognition Loss:   0.000355 => Gls Tokens per Sec:      602 || Batch Translation Loss:   0.005582 => Txt Tokens per Sec:     1644 || Lr: 0.000050
2024-02-07 22:37:44,420 Epoch 6489: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:37:44,420 EPOCH 6490
2024-02-07 22:38:00,572 Epoch 6490: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:38:00,573 EPOCH 6491
2024-02-07 22:38:16,807 Epoch 6491: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:38:16,807 EPOCH 6492
2024-02-07 22:38:32,935 Epoch 6492: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:38:32,936 EPOCH 6493
2024-02-07 22:38:49,053 Epoch 6493: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:38:49,054 EPOCH 6494
2024-02-07 22:39:05,101 Epoch 6494: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 22:39:05,102 EPOCH 6495
2024-02-07 22:39:21,422 Epoch 6495: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 22:39:21,423 EPOCH 6496
2024-02-07 22:39:37,568 Epoch 6496: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 22:39:37,568 EPOCH 6497
2024-02-07 22:39:53,536 Epoch 6497: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 22:39:53,536 EPOCH 6498
2024-02-07 22:40:09,623 Epoch 6498: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 22:40:09,624 EPOCH 6499
2024-02-07 22:40:25,495 Epoch 6499: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 22:40:25,496 EPOCH 6500
2024-02-07 22:40:41,444 [Epoch: 6500 Step: 00058500] Batch Recognition Loss:   0.001081 => Gls Tokens per Sec:      666 || Batch Translation Loss:   0.016626 => Txt Tokens per Sec:     1842 || Lr: 0.000050
2024-02-07 22:40:41,444 Epoch 6500: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 22:40:41,444 EPOCH 6501
2024-02-07 22:40:57,771 Epoch 6501: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 22:40:57,772 EPOCH 6502
2024-02-07 22:41:13,759 Epoch 6502: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:41:13,760 EPOCH 6503
2024-02-07 22:41:30,018 Epoch 6503: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:41:30,019 EPOCH 6504
2024-02-07 22:41:46,104 Epoch 6504: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:41:46,105 EPOCH 6505
2024-02-07 22:42:02,196 Epoch 6505: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:42:02,197 EPOCH 6506
2024-02-07 22:42:18,426 Epoch 6506: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:42:18,427 EPOCH 6507
2024-02-07 22:42:34,852 Epoch 6507: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:42:34,852 EPOCH 6508
2024-02-07 22:42:51,031 Epoch 6508: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:42:51,031 EPOCH 6509
2024-02-07 22:43:07,101 Epoch 6509: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:43:07,102 EPOCH 6510
2024-02-07 22:43:23,282 Epoch 6510: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:43:23,283 EPOCH 6511
2024-02-07 22:43:39,426 Epoch 6511: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:43:39,427 EPOCH 6512
2024-02-07 22:43:40,300 [Epoch: 6512 Step: 00058600] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:     1470 || Batch Translation Loss:   0.014850 => Txt Tokens per Sec:     4398 || Lr: 0.000050
2024-02-07 22:43:55,564 Epoch 6512: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:43:55,564 EPOCH 6513
2024-02-07 22:44:11,669 Epoch 6513: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 22:44:11,670 EPOCH 6514
2024-02-07 22:44:27,622 Epoch 6514: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:44:27,622 EPOCH 6515
2024-02-07 22:44:43,723 Epoch 6515: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 22:44:43,724 EPOCH 6516
2024-02-07 22:44:59,738 Epoch 6516: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 22:44:59,739 EPOCH 6517
2024-02-07 22:45:16,190 Epoch 6517: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:45:16,191 EPOCH 6518
2024-02-07 22:45:32,208 Epoch 6518: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 22:45:32,208 EPOCH 6519
2024-02-07 22:45:48,197 Epoch 6519: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 22:45:48,197 EPOCH 6520
2024-02-07 22:46:04,173 Epoch 6520: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 22:46:04,174 EPOCH 6521
2024-02-07 22:46:20,551 Epoch 6521: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 22:46:20,552 EPOCH 6522
2024-02-07 22:46:36,781 Epoch 6522: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 22:46:36,782 EPOCH 6523
2024-02-07 22:46:43,191 [Epoch: 6523 Step: 00058700] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:      400 || Batch Translation Loss:   0.013984 => Txt Tokens per Sec:     1240 || Lr: 0.000050
2024-02-07 22:46:53,010 Epoch 6523: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 22:46:53,010 EPOCH 6524
2024-02-07 22:47:09,004 Epoch 6524: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 22:47:09,005 EPOCH 6525
2024-02-07 22:47:25,083 Epoch 6525: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 22:47:25,084 EPOCH 6526
2024-02-07 22:47:41,335 Epoch 6526: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 22:47:41,336 EPOCH 6527
2024-02-07 22:47:57,312 Epoch 6527: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-07 22:47:57,313 EPOCH 6528
2024-02-07 22:48:13,570 Epoch 6528: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-07 22:48:13,571 EPOCH 6529
2024-02-07 22:48:29,694 Epoch 6529: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-07 22:48:29,695 EPOCH 6530
2024-02-07 22:48:45,662 Epoch 6530: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-07 22:48:45,662 EPOCH 6531
2024-02-07 22:49:02,007 Epoch 6531: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-07 22:49:02,008 EPOCH 6532
2024-02-07 22:49:18,009 Epoch 6532: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-07 22:49:18,010 EPOCH 6533
2024-02-07 22:49:34,379 Epoch 6533: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-07 22:49:34,380 EPOCH 6534
2024-02-07 22:49:37,975 [Epoch: 6534 Step: 00058800] Batch Recognition Loss:   0.000609 => Gls Tokens per Sec:     1069 || Batch Translation Loss:   0.026888 => Txt Tokens per Sec:     2584 || Lr: 0.000050
2024-02-07 22:49:50,457 Epoch 6534: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 22:49:50,458 EPOCH 6535
2024-02-07 22:50:06,730 Epoch 6535: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 22:50:06,731 EPOCH 6536
2024-02-07 22:50:22,652 Epoch 6536: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 22:50:22,653 EPOCH 6537
2024-02-07 22:50:39,136 Epoch 6537: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 22:50:39,136 EPOCH 6538
2024-02-07 22:50:55,531 Epoch 6538: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 22:50:55,531 EPOCH 6539
2024-02-07 22:51:11,621 Epoch 6539: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 22:51:11,622 EPOCH 6540
2024-02-07 22:51:27,911 Epoch 6540: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 22:51:27,912 EPOCH 6541
2024-02-07 22:51:44,125 Epoch 6541: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 22:51:44,126 EPOCH 6542
2024-02-07 22:52:00,114 Epoch 6542: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 22:52:00,115 EPOCH 6543
2024-02-07 22:52:16,457 Epoch 6543: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 22:52:16,458 EPOCH 6544
2024-02-07 22:52:32,389 Epoch 6544: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 22:52:32,390 EPOCH 6545
2024-02-07 22:52:38,058 [Epoch: 6545 Step: 00058900] Batch Recognition Loss:   0.000335 => Gls Tokens per Sec:      745 || Batch Translation Loss:   0.027856 => Txt Tokens per Sec:     1904 || Lr: 0.000050
2024-02-07 22:52:48,671 Epoch 6545: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 22:52:48,672 EPOCH 6546
2024-02-07 22:53:05,093 Epoch 6546: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 22:53:05,095 EPOCH 6547
2024-02-07 22:53:21,253 Epoch 6547: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 22:53:21,254 EPOCH 6548
2024-02-07 22:53:37,358 Epoch 6548: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 22:53:37,359 EPOCH 6549
2024-02-07 22:53:53,616 Epoch 6549: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 22:53:53,617 EPOCH 6550
2024-02-07 22:54:09,790 Epoch 6550: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 22:54:09,790 EPOCH 6551
2024-02-07 22:54:25,898 Epoch 6551: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 22:54:25,898 EPOCH 6552
2024-02-07 22:54:42,210 Epoch 6552: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 22:54:42,211 EPOCH 6553
2024-02-07 22:54:58,311 Epoch 6553: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:54:58,312 EPOCH 6554
2024-02-07 22:55:14,514 Epoch 6554: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:55:14,514 EPOCH 6555
2024-02-07 22:55:30,670 Epoch 6555: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 22:55:30,670 EPOCH 6556
2024-02-07 22:55:35,399 [Epoch: 6556 Step: 00059000] Batch Recognition Loss:   0.000553 => Gls Tokens per Sec:     1353 || Batch Translation Loss:   0.012934 => Txt Tokens per Sec:     3726 || Lr: 0.000050
2024-02-07 22:55:46,734 Epoch 6556: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:55:46,734 EPOCH 6557
2024-02-07 22:56:03,162 Epoch 6557: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:56:03,162 EPOCH 6558
2024-02-07 22:56:19,141 Epoch 6558: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 22:56:19,141 EPOCH 6559
2024-02-07 22:56:35,075 Epoch 6559: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 22:56:35,075 EPOCH 6560
2024-02-07 22:56:50,963 Epoch 6560: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:56:50,963 EPOCH 6561
2024-02-07 22:57:07,233 Epoch 6561: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 22:57:07,234 EPOCH 6562
2024-02-07 22:57:23,539 Epoch 6562: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 22:57:23,539 EPOCH 6563
2024-02-07 22:57:39,634 Epoch 6563: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:57:39,634 EPOCH 6564
2024-02-07 22:57:55,856 Epoch 6564: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:57:55,857 EPOCH 6565
2024-02-07 22:58:11,785 Epoch 6565: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:58:11,786 EPOCH 6566
2024-02-07 22:58:27,862 Epoch 6566: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:58:27,863 EPOCH 6567
2024-02-07 22:58:38,324 [Epoch: 6567 Step: 00059100] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:      734 || Batch Translation Loss:   0.006935 => Txt Tokens per Sec:     1976 || Lr: 0.000050
2024-02-07 22:58:43,758 Epoch 6567: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 22:58:43,758 EPOCH 6568
2024-02-07 22:58:59,936 Epoch 6568: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 22:58:59,937 EPOCH 6569
2024-02-07 22:59:15,981 Epoch 6569: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 22:59:15,982 EPOCH 6570
2024-02-07 22:59:32,573 Epoch 6570: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 22:59:32,573 EPOCH 6571
2024-02-07 22:59:48,910 Epoch 6571: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 22:59:48,911 EPOCH 6572
2024-02-07 23:00:05,240 Epoch 6572: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 23:00:05,240 EPOCH 6573
2024-02-07 23:00:21,351 Epoch 6573: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 23:00:21,352 EPOCH 6574
2024-02-07 23:00:37,862 Epoch 6574: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 23:00:37,862 EPOCH 6575
2024-02-07 23:00:54,034 Epoch 6575: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:00:54,035 EPOCH 6576
2024-02-07 23:01:10,221 Epoch 6576: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:01:10,221 EPOCH 6577
2024-02-07 23:01:26,398 Epoch 6577: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:01:26,398 EPOCH 6578
2024-02-07 23:01:41,480 [Epoch: 6578 Step: 00059200] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:      534 || Batch Translation Loss:   0.016584 => Txt Tokens per Sec:     1478 || Lr: 0.000050
2024-02-07 23:01:42,584 Epoch 6578: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 23:01:42,585 EPOCH 6579
2024-02-07 23:01:58,720 Epoch 6579: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:01:58,721 EPOCH 6580
2024-02-07 23:02:14,693 Epoch 6580: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 23:02:14,693 EPOCH 6581
2024-02-07 23:02:30,878 Epoch 6581: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 23:02:30,879 EPOCH 6582
2024-02-07 23:02:46,947 Epoch 6582: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:02:46,948 EPOCH 6583
2024-02-07 23:03:03,374 Epoch 6583: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:03:03,374 EPOCH 6584
2024-02-07 23:03:19,453 Epoch 6584: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:03:19,454 EPOCH 6585
2024-02-07 23:03:35,866 Epoch 6585: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:03:35,866 EPOCH 6586
2024-02-07 23:03:51,850 Epoch 6586: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:03:51,851 EPOCH 6587
2024-02-07 23:04:08,008 Epoch 6587: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:04:08,009 EPOCH 6588
2024-02-07 23:04:24,124 Epoch 6588: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:04:24,125 EPOCH 6589
2024-02-07 23:04:39,692 [Epoch: 6589 Step: 00059300] Batch Recognition Loss:   0.000615 => Gls Tokens per Sec:      600 || Batch Translation Loss:   0.005581 => Txt Tokens per Sec:     1740 || Lr: 0.000050
2024-02-07 23:04:39,979 Epoch 6589: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:04:39,979 EPOCH 6590
2024-02-07 23:04:55,991 Epoch 6590: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:04:55,992 EPOCH 6591
2024-02-07 23:05:12,249 Epoch 6591: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:05:12,250 EPOCH 6592
2024-02-07 23:05:28,452 Epoch 6592: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-07 23:05:28,453 EPOCH 6593
2024-02-07 23:05:44,694 Epoch 6593: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:05:44,694 EPOCH 6594
2024-02-07 23:06:00,813 Epoch 6594: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:06:00,814 EPOCH 6595
2024-02-07 23:06:16,852 Epoch 6595: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:06:16,852 EPOCH 6596
2024-02-07 23:06:32,695 Epoch 6596: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:06:32,696 EPOCH 6597
2024-02-07 23:06:48,764 Epoch 6597: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 23:06:48,764 EPOCH 6598
2024-02-07 23:07:04,986 Epoch 6598: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 23:07:04,987 EPOCH 6599
2024-02-07 23:07:21,056 Epoch 6599: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 23:07:21,057 EPOCH 6600
2024-02-07 23:07:37,197 [Epoch: 6600 Step: 00059400] Batch Recognition Loss:   0.001174 => Gls Tokens per Sec:      658 || Batch Translation Loss:   0.005836 => Txt Tokens per Sec:     1821 || Lr: 0.000050
2024-02-07 23:07:37,197 Epoch 6600: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 23:07:37,198 EPOCH 6601
2024-02-07 23:07:53,462 Epoch 6601: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 23:07:53,463 EPOCH 6602
2024-02-07 23:08:09,583 Epoch 6602: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 23:08:09,584 EPOCH 6603
2024-02-07 23:08:25,868 Epoch 6603: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 23:08:25,869 EPOCH 6604
2024-02-07 23:08:42,139 Epoch 6604: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 23:08:42,140 EPOCH 6605
2024-02-07 23:08:58,318 Epoch 6605: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 23:08:58,319 EPOCH 6606
2024-02-07 23:09:14,249 Epoch 6606: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 23:09:14,249 EPOCH 6607
2024-02-07 23:09:30,558 Epoch 6607: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:09:30,558 EPOCH 6608
2024-02-07 23:09:46,601 Epoch 6608: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:09:46,601 EPOCH 6609
2024-02-07 23:10:03,149 Epoch 6609: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:10:03,149 EPOCH 6610
2024-02-07 23:10:19,202 Epoch 6610: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:10:19,202 EPOCH 6611
2024-02-07 23:10:35,236 Epoch 6611: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:10:35,237 EPOCH 6612
2024-02-07 23:10:35,616 [Epoch: 6612 Step: 00059500] Batch Recognition Loss:   0.000121 => Gls Tokens per Sec:     3386 || Batch Translation Loss:   0.009290 => Txt Tokens per Sec:     8701 || Lr: 0.000050
2024-02-07 23:10:51,544 Epoch 6612: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:10:51,545 EPOCH 6613
2024-02-07 23:11:07,727 Epoch 6613: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:11:07,728 EPOCH 6614
2024-02-07 23:11:24,288 Epoch 6614: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:11:24,289 EPOCH 6615
2024-02-07 23:11:40,546 Epoch 6615: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:11:40,546 EPOCH 6616
2024-02-07 23:11:56,375 Epoch 6616: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 23:11:56,375 EPOCH 6617
2024-02-07 23:12:12,779 Epoch 6617: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 23:12:12,780 EPOCH 6618
2024-02-07 23:12:29,495 Epoch 6618: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 23:12:29,496 EPOCH 6619
2024-02-07 23:12:45,431 Epoch 6619: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 23:12:45,432 EPOCH 6620
2024-02-07 23:13:01,418 Epoch 6620: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 23:13:01,419 EPOCH 6621
2024-02-07 23:13:17,568 Epoch 6621: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 23:13:17,568 EPOCH 6622
2024-02-07 23:13:33,874 Epoch 6622: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 23:13:33,875 EPOCH 6623
2024-02-07 23:13:41,560 [Epoch: 6623 Step: 00059600] Batch Recognition Loss:   0.000378 => Gls Tokens per Sec:      216 || Batch Translation Loss:   0.009322 => Txt Tokens per Sec:      710 || Lr: 0.000050
2024-02-07 23:13:50,416 Epoch 6623: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-07 23:13:50,417 EPOCH 6624
2024-02-07 23:14:06,724 Epoch 6624: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 23:14:06,724 EPOCH 6625
2024-02-07 23:14:22,695 Epoch 6625: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 23:14:22,696 EPOCH 6626
2024-02-07 23:14:38,751 Epoch 6626: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 23:14:38,752 EPOCH 6627
2024-02-07 23:14:55,487 Epoch 6627: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-07 23:14:55,488 EPOCH 6628
2024-02-07 23:15:11,640 Epoch 6628: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 23:15:11,641 EPOCH 6629
2024-02-07 23:15:28,179 Epoch 6629: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-07 23:15:28,180 EPOCH 6630
2024-02-07 23:15:44,188 Epoch 6630: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-07 23:15:44,188 EPOCH 6631
2024-02-07 23:16:00,253 Epoch 6631: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 23:16:00,254 EPOCH 6632
2024-02-07 23:16:16,339 Epoch 6632: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 23:16:16,339 EPOCH 6633
2024-02-07 23:16:32,276 Epoch 6633: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 23:16:32,277 EPOCH 6634
2024-02-07 23:16:39,001 [Epoch: 6634 Step: 00059700] Batch Recognition Loss:   0.001221 => Gls Tokens per Sec:      571 || Batch Translation Loss:   0.023353 => Txt Tokens per Sec:     1553 || Lr: 0.000050
2024-02-07 23:16:48,542 Epoch 6634: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 23:16:48,542 EPOCH 6635
2024-02-07 23:17:04,752 Epoch 6635: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 23:17:04,753 EPOCH 6636
2024-02-07 23:17:20,853 Epoch 6636: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-07 23:17:20,854 EPOCH 6637
2024-02-07 23:17:36,935 Epoch 6637: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-07 23:17:36,935 EPOCH 6638
2024-02-07 23:17:53,080 Epoch 6638: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-07 23:17:53,081 EPOCH 6639
2024-02-07 23:18:09,206 Epoch 6639: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-07 23:18:09,207 EPOCH 6640
2024-02-07 23:18:25,360 Epoch 6640: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.52 
2024-02-07 23:18:25,361 EPOCH 6641
2024-02-07 23:18:41,102 Epoch 6641: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-07 23:18:41,102 EPOCH 6642
2024-02-07 23:18:57,140 Epoch 6642: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-07 23:18:57,140 EPOCH 6643
2024-02-07 23:19:13,289 Epoch 6643: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.33 
2024-02-07 23:19:13,290 EPOCH 6644
2024-02-07 23:19:29,259 Epoch 6644: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-07 23:19:29,260 EPOCH 6645
2024-02-07 23:19:36,594 [Epoch: 6645 Step: 00059800] Batch Recognition Loss:   0.002077 => Gls Tokens per Sec:      698 || Batch Translation Loss:   0.061749 => Txt Tokens per Sec:     2116 || Lr: 0.000050
2024-02-07 23:19:45,277 Epoch 6645: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-07 23:19:45,278 EPOCH 6646
2024-02-07 23:20:01,400 Epoch 6646: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-07 23:20:01,401 EPOCH 6647
2024-02-07 23:20:17,548 Epoch 6647: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-07 23:20:17,549 EPOCH 6648
2024-02-07 23:20:33,692 Epoch 6648: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-07 23:20:33,692 EPOCH 6649
2024-02-07 23:20:49,992 Epoch 6649: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-07 23:20:49,993 EPOCH 6650
2024-02-07 23:21:06,084 Epoch 6650: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-07 23:21:06,085 EPOCH 6651
2024-02-07 23:21:21,931 Epoch 6651: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 23:21:21,932 EPOCH 6652
2024-02-07 23:21:38,114 Epoch 6652: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 23:21:38,116 EPOCH 6653
2024-02-07 23:21:54,514 Epoch 6653: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 23:21:54,515 EPOCH 6654
2024-02-07 23:22:10,866 Epoch 6654: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 23:22:10,866 EPOCH 6655
2024-02-07 23:22:27,235 Epoch 6655: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 23:22:27,236 EPOCH 6656
2024-02-07 23:22:41,668 [Epoch: 6656 Step: 00059900] Batch Recognition Loss:   0.000715 => Gls Tokens per Sec:      381 || Batch Translation Loss:   0.014322 => Txt Tokens per Sec:     1087 || Lr: 0.000050
2024-02-07 23:22:43,649 Epoch 6656: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 23:22:43,649 EPOCH 6657
2024-02-07 23:22:59,898 Epoch 6657: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-07 23:22:59,899 EPOCH 6658
2024-02-07 23:23:16,196 Epoch 6658: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:23:16,196 EPOCH 6659
2024-02-07 23:23:32,745 Epoch 6659: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 23:23:32,746 EPOCH 6660
2024-02-07 23:23:49,081 Epoch 6660: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-07 23:23:49,082 EPOCH 6661
2024-02-07 23:24:05,206 Epoch 6661: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:24:05,207 EPOCH 6662
2024-02-07 23:24:21,348 Epoch 6662: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:24:21,349 EPOCH 6663
2024-02-07 23:24:37,433 Epoch 6663: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:24:37,433 EPOCH 6664
2024-02-07 23:24:53,862 Epoch 6664: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:24:53,863 EPOCH 6665
2024-02-07 23:25:10,591 Epoch 6665: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:25:10,592 EPOCH 6666
2024-02-07 23:25:26,876 Epoch 6666: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:25:26,877 EPOCH 6667
2024-02-07 23:25:34,841 [Epoch: 6667 Step: 00060000] Batch Recognition Loss:   0.000245 => Gls Tokens per Sec:      965 || Batch Translation Loss:   0.018150 => Txt Tokens per Sec:     2520 || Lr: 0.000050
2024-02-07 23:26:42,306 Validation result at epoch 6667, step    60000: duration: 67.4649s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.24266	Translation Loss: 108957.35938	PPL: 53245.90234
	Eval Metric: BLEU
	WER 2.26	(DEL: 0.00,	INS: 0.00,	SUB: 2.26)
	BLEU-4 0.44	(BLEU-1: 9.40,	BLEU-2: 2.73,	BLEU-3: 0.95,	BLEU-4: 0.44)
	CHRF 16.82	ROUGE 8.29
2024-02-07 23:26:42,309 Logging Recognition and Translation Outputs
2024-02-07 23:26:42,309 ========================================================================================================================
2024-02-07 23:26:42,309 Logging Sequence: 75_58.00
2024-02-07 23:26:42,309 	Gloss Reference :	A B+C+D+E
2024-02-07 23:26:42,309 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 23:26:42,309 	Gloss Alignment :	         
2024-02-07 23:26:42,309 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 23:26:42,312 	Text Reference  :	it seems like he like to date women and does not want to get married people seem     to      respect his   choices      
2024-02-07 23:26:42,313 	Text Hypothesis :	it ***** **** ** **** ** **** ***** *** **** *** **** ** was a       sudden decision without any     prior communication
2024-02-07 23:26:42,313 	Text Alignment  :	   D     D    D  D    D  D    D     D   D    D   D    D  S   S       S      S        S       S       S     S            
2024-02-07 23:26:42,313 ========================================================================================================================
2024-02-07 23:26:42,313 Logging Sequence: 152_113.00
2024-02-07 23:26:42,313 	Gloss Reference :	A B+C+D+E
2024-02-07 23:26:42,313 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 23:26:42,314 	Gloss Alignment :	         
2024-02-07 23:26:42,314 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 23:26:42,315 	Text Reference  :	******* ****** indians hoping  for   a  victory were distraught at  the defeat 
2024-02-07 23:26:42,315 	Text Hypothesis :	another indian deaf    cricket match kl rahul   was  bowled     for the england
2024-02-07 23:26:42,315 	Text Alignment  :	I       I      S       S       S     S  S       S    S          S       S      
2024-02-07 23:26:42,315 ========================================================================================================================
2024-02-07 23:26:42,315 Logging Sequence: 176_41.00
2024-02-07 23:26:42,316 	Gloss Reference :	A B+C+D+E
2024-02-07 23:26:42,316 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 23:26:42,316 	Gloss Alignment :	         
2024-02-07 23:26:42,316 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 23:26:42,317 	Text Reference  :	****** **** dahiya did not  loose   hope   and put up  a    strong fight
2024-02-07 23:26:42,317 	Text Hypothesis :	people were then   on  this morning attack no  one was very sad    news 
2024-02-07 23:26:42,317 	Text Alignment  :	I      I    S      S   S    S       S      S   S   S   S    S      S    
2024-02-07 23:26:42,317 ========================================================================================================================
2024-02-07 23:26:42,317 Logging Sequence: 77_190.00
2024-02-07 23:26:42,318 	Gloss Reference :	A B+C+D+E
2024-02-07 23:26:42,318 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 23:26:42,318 	Gloss Alignment :	         
2024-02-07 23:26:42,318 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 23:26:42,319 	Text Reference  :	*** *** ***** *** **** there are   many batsmen who   have scrored 36     runs     in  6   balls      
2024-02-07 23:26:42,319 	Text Hypothesis :	she now taken the 2020 was   taken and  sri     lanka has  been    tested positive for the coronavirus
2024-02-07 23:26:42,320 	Text Alignment  :	I   I   I     I   I    S     S     S    S       S     S    S       S      S        S   S   S          
2024-02-07 23:26:42,320 ========================================================================================================================
2024-02-07 23:26:42,320 Logging Sequence: 155_170.00
2024-02-07 23:26:42,320 	Gloss Reference :	A B+C+D+E
2024-02-07 23:26:42,320 	Gloss Hypothesis:	A B+C+D+E
2024-02-07 23:26:42,320 	Gloss Alignment :	         
2024-02-07 23:26:42,320 	--------------------------------------------------------------------------------------------------------------------
2024-02-07 23:26:42,321 	Text Reference  :	india lost the matches   and could not secure a  place in the semi final     
2024-02-07 23:26:42,322 	Text Hypothesis :	one   day  is  cancelled and ***** *** wanted to lead  to my  t20  tournament
2024-02-07 23:26:42,322 	Text Alignment  :	S     S    S   S             D     D   S      S  S     S  S   S    S         
2024-02-07 23:26:42,322 ========================================================================================================================
2024-02-07 23:26:50,719 Epoch 6667: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:26:50,719 EPOCH 6668
2024-02-07 23:27:07,218 Epoch 6668: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:27:07,219 EPOCH 6669
2024-02-07 23:27:23,374 Epoch 6669: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:27:23,374 EPOCH 6670
2024-02-07 23:27:39,288 Epoch 6670: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.09 
2024-02-07 23:27:39,289 EPOCH 6671
2024-02-07 23:27:55,487 Epoch 6671: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:27:55,488 EPOCH 6672
2024-02-07 23:28:11,348 Epoch 6672: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:28:11,349 EPOCH 6673
2024-02-07 23:28:27,753 Epoch 6673: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:28:27,753 EPOCH 6674
2024-02-07 23:28:43,850 Epoch 6674: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:28:43,851 EPOCH 6675
2024-02-07 23:29:00,154 Epoch 6675: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:29:00,154 EPOCH 6676
2024-02-07 23:29:16,325 Epoch 6676: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:29:16,326 EPOCH 6677
2024-02-07 23:29:32,212 Epoch 6677: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:29:32,212 EPOCH 6678
2024-02-07 23:29:41,654 [Epoch: 6678 Step: 00060100] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:      854 || Batch Translation Loss:   0.005710 => Txt Tokens per Sec:     2240 || Lr: 0.000050
2024-02-07 23:29:48,131 Epoch 6678: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:29:48,132 EPOCH 6679
2024-02-07 23:30:04,615 Epoch 6679: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:30:04,616 EPOCH 6680
2024-02-07 23:30:20,752 Epoch 6680: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:30:20,753 EPOCH 6681
2024-02-07 23:30:36,645 Epoch 6681: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:30:36,646 EPOCH 6682
2024-02-07 23:30:53,204 Epoch 6682: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:30:53,204 EPOCH 6683
2024-02-07 23:31:09,657 Epoch 6683: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:31:09,657 EPOCH 6684
2024-02-07 23:31:25,447 Epoch 6684: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:31:25,448 EPOCH 6685
2024-02-07 23:31:41,601 Epoch 6685: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:31:41,602 EPOCH 6686
2024-02-07 23:31:57,787 Epoch 6686: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:31:57,787 EPOCH 6687
2024-02-07 23:32:14,114 Epoch 6687: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:32:14,115 EPOCH 6688
2024-02-07 23:32:30,323 Epoch 6688: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:32:30,324 EPOCH 6689
2024-02-07 23:32:46,319 [Epoch: 6689 Step: 00060200] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:      584 || Batch Translation Loss:   0.013675 => Txt Tokens per Sec:     1694 || Lr: 0.000050
2024-02-07 23:32:46,569 Epoch 6689: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:32:46,569 EPOCH 6690
2024-02-07 23:33:02,262 Epoch 6690: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:33:02,262 EPOCH 6691
2024-02-07 23:33:18,696 Epoch 6691: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:33:18,697 EPOCH 6692
2024-02-07 23:33:35,421 Epoch 6692: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:33:35,422 EPOCH 6693
2024-02-07 23:33:51,677 Epoch 6693: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:33:51,678 EPOCH 6694
2024-02-07 23:34:07,788 Epoch 6694: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:34:07,789 EPOCH 6695
2024-02-07 23:34:23,810 Epoch 6695: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:34:23,810 EPOCH 6696
2024-02-07 23:34:39,858 Epoch 6696: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:34:39,860 EPOCH 6697
2024-02-07 23:34:56,300 Epoch 6697: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:34:56,300 EPOCH 6698
2024-02-07 23:35:12,442 Epoch 6698: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:35:12,442 EPOCH 6699
2024-02-07 23:35:28,583 Epoch 6699: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:35:28,584 EPOCH 6700
2024-02-07 23:35:44,636 [Epoch: 6700 Step: 00060300] Batch Recognition Loss:   0.000097 => Gls Tokens per Sec:      662 || Batch Translation Loss:   0.007979 => Txt Tokens per Sec:     1831 || Lr: 0.000050
2024-02-07 23:35:44,636 Epoch 6700: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:35:44,636 EPOCH 6701
2024-02-07 23:36:00,844 Epoch 6701: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:36:00,845 EPOCH 6702
2024-02-07 23:36:17,215 Epoch 6702: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:36:17,216 EPOCH 6703
2024-02-07 23:36:33,306 Epoch 6703: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:36:33,306 EPOCH 6704
2024-02-07 23:36:49,526 Epoch 6704: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:36:49,526 EPOCH 6705
2024-02-07 23:37:05,641 Epoch 6705: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:37:05,642 EPOCH 6706
2024-02-07 23:37:21,481 Epoch 6706: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:37:21,481 EPOCH 6707
2024-02-07 23:37:37,803 Epoch 6707: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:37:37,803 EPOCH 6708
2024-02-07 23:37:53,755 Epoch 6708: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:37:53,755 EPOCH 6709
2024-02-07 23:38:10,284 Epoch 6709: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:38:10,285 EPOCH 6710
2024-02-07 23:38:26,938 Epoch 6710: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:38:26,939 EPOCH 6711
2024-02-07 23:38:43,175 Epoch 6711: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:38:43,176 EPOCH 6712
2024-02-07 23:38:43,403 [Epoch: 6712 Step: 00060400] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     5664 || Batch Translation Loss:   0.006169 => Txt Tokens per Sec:    10102 || Lr: 0.000050
2024-02-07 23:38:59,239 Epoch 6712: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:38:59,239 EPOCH 6713
2024-02-07 23:39:15,483 Epoch 6713: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:39:15,483 EPOCH 6714
2024-02-07 23:39:31,603 Epoch 6714: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:39:31,603 EPOCH 6715
2024-02-07 23:39:47,164 Epoch 6715: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:39:47,164 EPOCH 6716
2024-02-07 23:40:03,222 Epoch 6716: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:40:03,222 EPOCH 6717
2024-02-07 23:40:19,599 Epoch 6717: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:40:19,599 EPOCH 6718
2024-02-07 23:40:35,566 Epoch 6718: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:40:35,567 EPOCH 6719
2024-02-07 23:40:51,589 Epoch 6719: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:40:51,591 EPOCH 6720
2024-02-07 23:41:07,844 Epoch 6720: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:41:07,845 EPOCH 6721
2024-02-07 23:41:23,745 Epoch 6721: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:41:23,746 EPOCH 6722
2024-02-07 23:41:39,969 Epoch 6722: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:41:39,969 EPOCH 6723
2024-02-07 23:41:46,478 [Epoch: 6723 Step: 00060500] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:      393 || Batch Translation Loss:   0.013414 => Txt Tokens per Sec:     1240 || Lr: 0.000050
2024-02-07 23:41:56,215 Epoch 6723: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:41:56,215 EPOCH 6724
2024-02-07 23:42:12,307 Epoch 6724: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:42:12,307 EPOCH 6725
2024-02-07 23:42:28,439 Epoch 6725: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:42:28,440 EPOCH 6726
2024-02-07 23:42:44,603 Epoch 6726: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:42:44,604 EPOCH 6727
2024-02-07 23:43:00,854 Epoch 6727: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:43:00,854 EPOCH 6728
2024-02-07 23:43:17,045 Epoch 6728: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:43:17,045 EPOCH 6729
2024-02-07 23:43:33,377 Epoch 6729: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:43:33,377 EPOCH 6730
2024-02-07 23:43:49,462 Epoch 6730: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:43:49,463 EPOCH 6731
2024-02-07 23:44:05,463 Epoch 6731: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:44:05,464 EPOCH 6732
2024-02-07 23:44:21,811 Epoch 6732: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:44:21,812 EPOCH 6733
2024-02-07 23:44:37,890 Epoch 6733: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 23:44:37,891 EPOCH 6734
2024-02-07 23:44:47,833 [Epoch: 6734 Step: 00060600] Batch Recognition Loss:   0.000330 => Gls Tokens per Sec:      386 || Batch Translation Loss:   0.018937 => Txt Tokens per Sec:     1245 || Lr: 0.000050
2024-02-07 23:44:54,058 Epoch 6734: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 23:44:54,058 EPOCH 6735
2024-02-07 23:45:09,898 Epoch 6735: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:45:09,898 EPOCH 6736
2024-02-07 23:45:26,128 Epoch 6736: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:45:26,129 EPOCH 6737
2024-02-07 23:45:42,265 Epoch 6737: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:45:42,266 EPOCH 6738
2024-02-07 23:45:58,197 Epoch 6738: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:45:58,198 EPOCH 6739
2024-02-07 23:46:14,380 Epoch 6739: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:46:14,381 EPOCH 6740
2024-02-07 23:46:30,571 Epoch 6740: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:46:30,571 EPOCH 6741
2024-02-07 23:46:46,936 Epoch 6741: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:46:46,936 EPOCH 6742
2024-02-07 23:47:03,086 Epoch 6742: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:47:03,087 EPOCH 6743
2024-02-07 23:47:19,438 Epoch 6743: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-07 23:47:19,439 EPOCH 6744
2024-02-07 23:47:35,246 Epoch 6744: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:47:35,246 EPOCH 6745
2024-02-07 23:47:40,991 [Epoch: 6745 Step: 00060700] Batch Recognition Loss:   0.000530 => Gls Tokens per Sec:      735 || Batch Translation Loss:   0.027115 => Txt Tokens per Sec:     1951 || Lr: 0.000050
2024-02-07 23:47:51,505 Epoch 6745: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 23:47:51,505 EPOCH 6746
2024-02-07 23:48:07,766 Epoch 6746: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 23:48:07,766 EPOCH 6747
2024-02-07 23:48:24,130 Epoch 6747: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 23:48:24,130 EPOCH 6748
2024-02-07 23:48:40,132 Epoch 6748: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 23:48:40,132 EPOCH 6749
2024-02-07 23:48:56,492 Epoch 6749: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 23:48:56,492 EPOCH 6750
2024-02-07 23:49:12,456 Epoch 6750: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 23:49:12,457 EPOCH 6751
2024-02-07 23:49:28,505 Epoch 6751: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 23:49:28,505 EPOCH 6752
2024-02-07 23:49:44,497 Epoch 6752: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-07 23:49:44,497 EPOCH 6753
2024-02-07 23:50:00,930 Epoch 6753: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-07 23:50:00,930 EPOCH 6754
2024-02-07 23:50:16,918 Epoch 6754: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 23:50:16,919 EPOCH 6755
2024-02-07 23:50:33,144 Epoch 6755: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 23:50:33,145 EPOCH 6756
2024-02-07 23:50:43,295 [Epoch: 6756 Step: 00060800] Batch Recognition Loss:   0.000467 => Gls Tokens per Sec:      631 || Batch Translation Loss:   0.024902 => Txt Tokens per Sec:     1809 || Lr: 0.000050
2024-02-07 23:50:49,233 Epoch 6756: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 23:50:49,234 EPOCH 6757
2024-02-07 23:51:05,396 Epoch 6757: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 23:51:05,396 EPOCH 6758
2024-02-07 23:51:21,474 Epoch 6758: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 23:51:21,474 EPOCH 6759
2024-02-07 23:51:37,894 Epoch 6759: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 23:51:37,896 EPOCH 6760
2024-02-07 23:51:53,932 Epoch 6760: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 23:51:53,933 EPOCH 6761
2024-02-07 23:52:10,332 Epoch 6761: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 23:52:10,333 EPOCH 6762
2024-02-07 23:52:26,425 Epoch 6762: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-07 23:52:26,426 EPOCH 6763
2024-02-07 23:52:42,540 Epoch 6763: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 23:52:42,540 EPOCH 6764
2024-02-07 23:52:58,343 Epoch 6764: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 23:52:58,343 EPOCH 6765
2024-02-07 23:53:14,701 Epoch 6765: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-07 23:53:14,702 EPOCH 6766
2024-02-07 23:53:31,285 Epoch 6766: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-07 23:53:31,286 EPOCH 6767
2024-02-07 23:53:39,271 [Epoch: 6767 Step: 00060900] Batch Recognition Loss:   0.000330 => Gls Tokens per Sec:      962 || Batch Translation Loss:   0.018730 => Txt Tokens per Sec:     2557 || Lr: 0.000050
2024-02-07 23:53:47,272 Epoch 6767: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 23:53:47,273 EPOCH 6768
2024-02-07 23:54:03,256 Epoch 6768: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 23:54:03,257 EPOCH 6769
2024-02-07 23:54:19,193 Epoch 6769: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 23:54:19,194 EPOCH 6770
2024-02-07 23:54:35,515 Epoch 6770: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 23:54:35,516 EPOCH 6771
2024-02-07 23:54:51,565 Epoch 6771: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:54:51,566 EPOCH 6772
2024-02-07 23:55:07,642 Epoch 6772: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 23:55:07,643 EPOCH 6773
2024-02-07 23:55:23,298 Epoch 6773: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 23:55:23,299 EPOCH 6774
2024-02-07 23:55:39,392 Epoch 6774: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-07 23:55:39,393 EPOCH 6775
2024-02-07 23:55:55,141 Epoch 6775: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 23:55:55,142 EPOCH 6776
2024-02-07 23:56:11,496 Epoch 6776: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 23:56:11,498 EPOCH 6777
2024-02-07 23:56:27,640 Epoch 6777: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-07 23:56:27,640 EPOCH 6778
2024-02-07 23:56:43,363 [Epoch: 6778 Step: 00061000] Batch Recognition Loss:   0.000323 => Gls Tokens per Sec:      513 || Batch Translation Loss:   0.025920 => Txt Tokens per Sec:     1514 || Lr: 0.000050
2024-02-07 23:56:44,087 Epoch 6778: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-07 23:56:44,087 EPOCH 6779
2024-02-07 23:57:00,172 Epoch 6779: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-07 23:57:00,173 EPOCH 6780
2024-02-07 23:57:16,269 Epoch 6780: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 23:57:16,270 EPOCH 6781
2024-02-07 23:57:32,400 Epoch 6781: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-07 23:57:32,400 EPOCH 6782
2024-02-07 23:57:48,797 Epoch 6782: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:57:48,799 EPOCH 6783
2024-02-07 23:58:04,934 Epoch 6783: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:58:04,934 EPOCH 6784
2024-02-07 23:58:21,272 Epoch 6784: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:58:21,273 EPOCH 6785
2024-02-07 23:58:37,560 Epoch 6785: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:58:37,560 EPOCH 6786
2024-02-07 23:58:53,636 Epoch 6786: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:58:53,637 EPOCH 6787
2024-02-07 23:59:09,742 Epoch 6787: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-07 23:59:09,743 EPOCH 6788
2024-02-07 23:59:25,798 Epoch 6788: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:59:25,798 EPOCH 6789
2024-02-07 23:59:38,755 [Epoch: 6789 Step: 00061100] Batch Recognition Loss:   0.000381 => Gls Tokens per Sec:      721 || Batch Translation Loss:   0.005127 => Txt Tokens per Sec:     1952 || Lr: 0.000050
2024-02-07 23:59:41,833 Epoch 6789: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:59:41,833 EPOCH 6790
2024-02-07 23:59:57,829 Epoch 6790: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-07 23:59:57,830 EPOCH 6791
2024-02-08 00:00:13,904 Epoch 6791: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:00:13,904 EPOCH 6792
2024-02-08 00:00:30,234 Epoch 6792: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:00:30,235 EPOCH 6793
2024-02-08 00:00:46,520 Epoch 6793: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:00:46,521 EPOCH 6794
2024-02-08 00:01:02,859 Epoch 6794: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 00:01:02,860 EPOCH 6795
2024-02-08 00:01:18,839 Epoch 6795: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 00:01:18,840 EPOCH 6796
2024-02-08 00:01:34,911 Epoch 6796: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 00:01:34,911 EPOCH 6797
2024-02-08 00:01:50,555 Epoch 6797: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 00:01:50,555 EPOCH 6798
2024-02-08 00:02:06,942 Epoch 6798: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-08 00:02:06,943 EPOCH 6799
2024-02-08 00:02:23,123 Epoch 6799: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 00:02:23,124 EPOCH 6800
2024-02-08 00:02:39,218 [Epoch: 6800 Step: 00061200] Batch Recognition Loss:   0.000297 => Gls Tokens per Sec:      660 || Batch Translation Loss:   0.012735 => Txt Tokens per Sec:     1826 || Lr: 0.000050
2024-02-08 00:02:39,219 Epoch 6800: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 00:02:39,219 EPOCH 6801
2024-02-08 00:02:55,175 Epoch 6801: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 00:02:55,176 EPOCH 6802
2024-02-08 00:03:11,161 Epoch 6802: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 00:03:11,162 EPOCH 6803
2024-02-08 00:03:27,372 Epoch 6803: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-08 00:03:27,372 EPOCH 6804
2024-02-08 00:03:43,518 Epoch 6804: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-08 00:03:43,519 EPOCH 6805
2024-02-08 00:03:59,584 Epoch 6805: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-08 00:03:59,584 EPOCH 6806
2024-02-08 00:04:15,582 Epoch 6806: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-08 00:04:15,582 EPOCH 6807
2024-02-08 00:04:31,564 Epoch 6807: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-08 00:04:31,564 EPOCH 6808
2024-02-08 00:04:47,628 Epoch 6808: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-08 00:04:47,629 EPOCH 6809
2024-02-08 00:05:03,624 Epoch 6809: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-08 00:05:03,625 EPOCH 6810
2024-02-08 00:05:19,677 Epoch 6810: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-08 00:05:19,678 EPOCH 6811
2024-02-08 00:05:35,780 Epoch 6811: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-08 00:05:35,781 EPOCH 6812
2024-02-08 00:05:38,849 [Epoch: 6812 Step: 00061300] Batch Recognition Loss:   0.000422 => Gls Tokens per Sec:      417 || Batch Translation Loss:   0.017979 => Txt Tokens per Sec:     1333 || Lr: 0.000050
2024-02-08 00:05:51,805 Epoch 6812: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-08 00:05:51,805 EPOCH 6813
2024-02-08 00:06:08,178 Epoch 6813: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-08 00:06:08,178 EPOCH 6814
2024-02-08 00:06:24,338 Epoch 6814: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-08 00:06:24,340 EPOCH 6815
2024-02-08 00:06:40,163 Epoch 6815: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-08 00:06:40,163 EPOCH 6816
2024-02-08 00:06:56,307 Epoch 6816: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-08 00:06:56,308 EPOCH 6817
2024-02-08 00:07:11,842 Epoch 6817: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-08 00:07:11,844 EPOCH 6818
2024-02-08 00:07:28,257 Epoch 6818: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-08 00:07:28,258 EPOCH 6819
2024-02-08 00:07:44,564 Epoch 6819: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 00:07:44,565 EPOCH 6820
2024-02-08 00:08:00,782 Epoch 6820: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 00:08:00,783 EPOCH 6821
2024-02-08 00:08:16,855 Epoch 6821: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 00:08:16,856 EPOCH 6822
2024-02-08 00:08:33,070 Epoch 6822: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 00:08:33,070 EPOCH 6823
2024-02-08 00:08:33,920 [Epoch: 6823 Step: 00061400] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     3018 || Batch Translation Loss:   0.011057 => Txt Tokens per Sec:     8011 || Lr: 0.000050
2024-02-08 00:08:49,166 Epoch 6823: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 00:08:49,166 EPOCH 6824
2024-02-08 00:09:05,711 Epoch 6824: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 00:09:05,712 EPOCH 6825
2024-02-08 00:09:22,127 Epoch 6825: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 00:09:22,128 EPOCH 6826
2024-02-08 00:09:38,397 Epoch 6826: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 00:09:38,398 EPOCH 6827
2024-02-08 00:09:54,708 Epoch 6827: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:09:54,709 EPOCH 6828
2024-02-08 00:10:10,773 Epoch 6828: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-08 00:10:10,774 EPOCH 6829
2024-02-08 00:10:26,731 Epoch 6829: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:10:26,732 EPOCH 6830
2024-02-08 00:10:42,790 Epoch 6830: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-08 00:10:42,791 EPOCH 6831
2024-02-08 00:10:59,121 Epoch 6831: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-08 00:10:59,121 EPOCH 6832
2024-02-08 00:11:15,254 Epoch 6832: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 00:11:15,254 EPOCH 6833
2024-02-08 00:11:31,559 Epoch 6833: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-08 00:11:31,559 EPOCH 6834
2024-02-08 00:11:42,147 [Epoch: 6834 Step: 00061500] Batch Recognition Loss:   0.000323 => Gls Tokens per Sec:      278 || Batch Translation Loss:   0.016020 => Txt Tokens per Sec:      852 || Lr: 0.000050
2024-02-08 00:11:47,560 Epoch 6834: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 00:11:47,560 EPOCH 6835
2024-02-08 00:12:03,009 Epoch 6835: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 00:12:03,009 EPOCH 6836
2024-02-08 00:12:19,520 Epoch 6836: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 00:12:19,520 EPOCH 6837
2024-02-08 00:12:35,837 Epoch 6837: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-08 00:12:35,838 EPOCH 6838
2024-02-08 00:12:52,155 Epoch 6838: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-08 00:12:52,155 EPOCH 6839
2024-02-08 00:13:08,472 Epoch 6839: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-08 00:13:08,472 EPOCH 6840
2024-02-08 00:13:24,400 Epoch 6840: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-08 00:13:24,401 EPOCH 6841
2024-02-08 00:13:40,350 Epoch 6841: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-08 00:13:40,351 EPOCH 6842
2024-02-08 00:13:56,269 Epoch 6842: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-08 00:13:56,270 EPOCH 6843
2024-02-08 00:14:12,822 Epoch 6843: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-08 00:14:12,823 EPOCH 6844
2024-02-08 00:14:28,832 Epoch 6844: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-08 00:14:28,833 EPOCH 6845
2024-02-08 00:14:30,795 [Epoch: 6845 Step: 00061600] Batch Recognition Loss:   0.000558 => Gls Tokens per Sec:     2612 || Batch Translation Loss:   0.023686 => Txt Tokens per Sec:     7319 || Lr: 0.000050
2024-02-08 00:14:44,817 Epoch 6845: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-08 00:14:44,818 EPOCH 6846
2024-02-08 00:15:01,275 Epoch 6846: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-08 00:15:01,276 EPOCH 6847
2024-02-08 00:15:17,453 Epoch 6847: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-08 00:15:17,454 EPOCH 6848
2024-02-08 00:15:33,668 Epoch 6848: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 00:15:33,669 EPOCH 6849
2024-02-08 00:15:49,669 Epoch 6849: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 00:15:49,670 EPOCH 6850
2024-02-08 00:16:06,118 Epoch 6850: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:16:06,118 EPOCH 6851
2024-02-08 00:16:22,177 Epoch 6851: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:16:22,177 EPOCH 6852
2024-02-08 00:16:38,140 Epoch 6852: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:16:38,141 EPOCH 6853
2024-02-08 00:16:54,464 Epoch 6853: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:16:54,464 EPOCH 6854
2024-02-08 00:17:10,390 Epoch 6854: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:17:10,391 EPOCH 6855
2024-02-08 00:17:26,533 Epoch 6855: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:17:26,534 EPOCH 6856
2024-02-08 00:17:31,536 [Epoch: 6856 Step: 00061700] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:     1280 || Batch Translation Loss:   0.008679 => Txt Tokens per Sec:     3384 || Lr: 0.000050
2024-02-08 00:17:42,770 Epoch 6856: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:17:42,771 EPOCH 6857
2024-02-08 00:17:59,202 Epoch 6857: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:17:59,202 EPOCH 6858
2024-02-08 00:18:14,747 Epoch 6858: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:18:14,747 EPOCH 6859
2024-02-08 00:18:31,032 Epoch 6859: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:18:31,033 EPOCH 6860
2024-02-08 00:18:47,217 Epoch 6860: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:18:47,218 EPOCH 6861
2024-02-08 00:19:03,624 Epoch 6861: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:19:03,626 EPOCH 6862
2024-02-08 00:19:19,570 Epoch 6862: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:19:19,571 EPOCH 6863
2024-02-08 00:19:36,105 Epoch 6863: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:19:36,106 EPOCH 6864
2024-02-08 00:19:52,156 Epoch 6864: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:19:52,156 EPOCH 6865
2024-02-08 00:20:07,911 Epoch 6865: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-08 00:20:07,912 EPOCH 6866
2024-02-08 00:20:24,290 Epoch 6866: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:20:24,290 EPOCH 6867
2024-02-08 00:20:33,377 [Epoch: 6867 Step: 00061800] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:      746 || Batch Translation Loss:   0.009377 => Txt Tokens per Sec:     1955 || Lr: 0.000050
2024-02-08 00:20:40,243 Epoch 6867: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:20:40,244 EPOCH 6868
2024-02-08 00:20:56,457 Epoch 6868: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-08 00:20:56,458 EPOCH 6869
2024-02-08 00:21:12,812 Epoch 6869: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:21:12,812 EPOCH 6870
2024-02-08 00:21:28,915 Epoch 6870: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:21:28,915 EPOCH 6871
2024-02-08 00:21:45,110 Epoch 6871: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:21:45,110 EPOCH 6872
2024-02-08 00:22:01,101 Epoch 6872: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:22:01,102 EPOCH 6873
2024-02-08 00:22:17,485 Epoch 6873: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 00:22:17,486 EPOCH 6874
2024-02-08 00:22:33,710 Epoch 6874: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 00:22:33,710 EPOCH 6875
2024-02-08 00:22:49,976 Epoch 6875: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:22:49,977 EPOCH 6876
2024-02-08 00:23:06,186 Epoch 6876: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:23:06,186 EPOCH 6877
2024-02-08 00:23:22,398 Epoch 6877: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:23:22,399 EPOCH 6878
2024-02-08 00:23:31,519 [Epoch: 6878 Step: 00061900] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:      884 || Batch Translation Loss:   0.012133 => Txt Tokens per Sec:     2316 || Lr: 0.000050
2024-02-08 00:23:38,259 Epoch 6878: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:23:38,260 EPOCH 6879
2024-02-08 00:23:54,327 Epoch 6879: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:23:54,328 EPOCH 6880
2024-02-08 00:24:10,752 Epoch 6880: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:24:10,752 EPOCH 6881
2024-02-08 00:24:26,894 Epoch 6881: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:24:26,895 EPOCH 6882
2024-02-08 00:24:42,839 Epoch 6882: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:24:42,840 EPOCH 6883
2024-02-08 00:24:59,055 Epoch 6883: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 00:24:59,056 EPOCH 6884
2024-02-08 00:25:15,284 Epoch 6884: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-08 00:25:15,284 EPOCH 6885
2024-02-08 00:25:31,341 Epoch 6885: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 00:25:31,342 EPOCH 6886
2024-02-08 00:25:47,487 Epoch 6886: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 00:25:47,487 EPOCH 6887
2024-02-08 00:26:03,724 Epoch 6887: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 00:26:03,725 EPOCH 6888
2024-02-08 00:26:19,824 Epoch 6888: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:26:19,825 EPOCH 6889
2024-02-08 00:26:35,666 [Epoch: 6889 Step: 00062000] Batch Recognition Loss:   0.001888 => Gls Tokens per Sec:      590 || Batch Translation Loss:   0.005515 => Txt Tokens per Sec:     1619 || Lr: 0.000050
2024-02-08 00:27:43,472 Validation result at epoch 6889, step    62000: duration: 67.8055s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.24138	Translation Loss: 109490.98438	PPL: 56160.84375
	Eval Metric: BLEU
	WER 2.47	(DEL: 0.00,	INS: 0.00,	SUB: 2.47)
	BLEU-4 0.39	(BLEU-1: 10.02,	BLEU-2: 2.78,	BLEU-3: 0.95,	BLEU-4: 0.39)
	CHRF 16.95	ROUGE 8.09
2024-02-08 00:27:43,475 Logging Recognition and Translation Outputs
2024-02-08 00:27:43,475 ========================================================================================================================
2024-02-08 00:27:43,475 Logging Sequence: 165_523.00
2024-02-08 00:27:43,475 	Gloss Reference :	A B+C+D+E
2024-02-08 00:27:43,475 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 00:27:43,475 	Gloss Alignment :	         
2024-02-08 00:27:43,476 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 00:27:43,477 	Text Reference  :	as he believed that his team might lose if     he   takes off     his batting pads
2024-02-08 00:27:43,477 	Text Hypothesis :	** ** ******** **** *** **** ***** **** suhana khan was   shocked to  see     this
2024-02-08 00:27:43,477 	Text Alignment  :	D  D  D        D    D   D    D     D    S      S    S     S       S   S       S   
2024-02-08 00:27:43,477 ========================================================================================================================
2024-02-08 00:27:43,477 Logging Sequence: 165_233.00
2024-02-08 00:27:43,478 	Gloss Reference :	A B+C+D+E
2024-02-08 00:27:43,478 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 00:27:43,478 	Gloss Alignment :	         
2024-02-08 00:27:43,478 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 00:27:43,480 	Text Reference  :	irrespective of whether he was playing the match or not he always sat  with his bag  he  was   happy   when   the  team won     
2024-02-08 00:27:43,480 	Text Hypothesis :	************ ** ******* ** *** ******* *** ***** ** *** ** ****** many had  may tell you about cricket nobody knew what happened
2024-02-08 00:27:43,480 	Text Alignment  :	D            D  D       D  D   D       D   D     D  D   D  D      S    S    S   S    S   S     S       S      S    S    S       
2024-02-08 00:27:43,480 ========================================================================================================================
2024-02-08 00:27:43,480 Logging Sequence: 169_214.00
2024-02-08 00:27:43,480 	Gloss Reference :	A B+C+D+E
2024-02-08 00:27:43,480 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 00:27:43,481 	Gloss Alignment :	         
2024-02-08 00:27:43,481 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 00:27:43,482 	Text Reference  :	virat kohli said that though arshdeep dropped the   catch he       is    still  a   strong part      of the ***** ********** indian  team  
2024-02-08 00:27:43,483 	Text Hypothesis :	***** ***** the  that ****** ******** ******* group c     included south africa and 46     countries of the state government against amount
2024-02-08 00:27:43,483 	Text Alignment  :	D     D     S         D      D        D       S     S     S        S     S      S   S      S                I     I          S       S     
2024-02-08 00:27:43,483 ========================================================================================================================
2024-02-08 00:27:43,483 Logging Sequence: 88_67.00
2024-02-08 00:27:43,483 	Gloss Reference :	A B+C+D+E
2024-02-08 00:27:43,483 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 00:27:43,483 	Gloss Alignment :	         
2024-02-08 00:27:43,483 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 00:27:43,485 	Text Reference  :	pablo javkin the **** mayor  of  rosario is        also      a   drug trafficker so    he  won't take care of       you     
2024-02-08 00:27:43,485 	Text Hypothesis :	***** ****** the cops traced him to      hyderabad ramnagesh was only 23         years old and   was  a    software engineer
2024-02-08 00:27:43,486 	Text Alignment  :	D     D          I    S      S   S       S         S         S   S    S          S     S   S     S    S    S        S       
2024-02-08 00:27:43,486 ========================================================================================================================
2024-02-08 00:27:43,486 Logging Sequence: 69_95.00
2024-02-08 00:27:43,486 	Gloss Reference :	A B+C+D+E
2024-02-08 00:27:43,486 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 00:27:43,486 	Gloss Alignment :	         
2024-02-08 00:27:43,486 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 00:27:43,488 	Text Reference  :	**** *** a     six and a     four sealed csk's victory  and the team   won     the     match   
2024-02-08 00:27:43,488 	Text Hypothesis :	when the final was the world cup  trophy was   unvieled by  an  indian actress deepika padukone
2024-02-08 00:27:43,488 	Text Alignment  :	I    I   S     S   S   S     S    S      S     S        S   S   S      S       S       S       
2024-02-08 00:27:43,488 ========================================================================================================================
2024-02-08 00:27:44,243 Epoch 6889: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:27:44,243 EPOCH 6890
2024-02-08 00:28:00,913 Epoch 6890: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:28:00,914 EPOCH 6891
2024-02-08 00:28:16,951 Epoch 6891: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:28:16,952 EPOCH 6892
2024-02-08 00:28:33,089 Epoch 6892: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:28:33,089 EPOCH 6893
2024-02-08 00:28:49,220 Epoch 6893: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:28:49,220 EPOCH 6894
2024-02-08 00:29:05,292 Epoch 6894: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:29:05,293 EPOCH 6895
2024-02-08 00:29:21,299 Epoch 6895: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:29:21,299 EPOCH 6896
2024-02-08 00:29:37,475 Epoch 6896: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:29:37,475 EPOCH 6897
2024-02-08 00:29:53,540 Epoch 6897: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:29:53,540 EPOCH 6898
2024-02-08 00:30:09,626 Epoch 6898: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:30:09,627 EPOCH 6899
2024-02-08 00:30:26,043 Epoch 6899: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:30:26,044 EPOCH 6900
2024-02-08 00:30:42,131 [Epoch: 6900 Step: 00062100] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:      660 || Batch Translation Loss:   0.011088 => Txt Tokens per Sec:     1826 || Lr: 0.000050
2024-02-08 00:30:42,132 Epoch 6900: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 00:30:42,132 EPOCH 6901
2024-02-08 00:30:58,171 Epoch 6901: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:30:58,172 EPOCH 6902
2024-02-08 00:31:14,294 Epoch 6902: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:31:14,295 EPOCH 6903
2024-02-08 00:31:30,676 Epoch 6903: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:31:30,677 EPOCH 6904
2024-02-08 00:31:46,991 Epoch 6904: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 00:31:46,992 EPOCH 6905
2024-02-08 00:32:03,313 Epoch 6905: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:32:03,314 EPOCH 6906
2024-02-08 00:32:19,410 Epoch 6906: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:32:19,410 EPOCH 6907
2024-02-08 00:32:35,666 Epoch 6907: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:32:35,666 EPOCH 6908
2024-02-08 00:32:51,662 Epoch 6908: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 00:32:51,662 EPOCH 6909
2024-02-08 00:33:08,038 Epoch 6909: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-08 00:33:08,039 EPOCH 6910
2024-02-08 00:33:23,807 Epoch 6910: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-08 00:33:23,807 EPOCH 6911
2024-02-08 00:33:40,103 Epoch 6911: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-08 00:33:40,103 EPOCH 6912
2024-02-08 00:33:44,444 [Epoch: 6912 Step: 00062200] Batch Recognition Loss:   0.000539 => Gls Tokens per Sec:       88 || Batch Translation Loss:   0.009021 => Txt Tokens per Sec:      313 || Lr: 0.000050
2024-02-08 00:33:56,090 Epoch 6912: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-08 00:33:56,090 EPOCH 6913
2024-02-08 00:34:12,379 Epoch 6913: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-08 00:34:12,380 EPOCH 6914
2024-02-08 00:34:28,452 Epoch 6914: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-08 00:34:28,452 EPOCH 6915
2024-02-08 00:34:44,616 Epoch 6915: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-08 00:34:44,617 EPOCH 6916
2024-02-08 00:35:00,705 Epoch 6916: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 00:35:00,706 EPOCH 6917
2024-02-08 00:35:16,800 Epoch 6917: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 00:35:16,800 EPOCH 6918
2024-02-08 00:35:33,048 Epoch 6918: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 00:35:33,048 EPOCH 6919
2024-02-08 00:35:49,567 Epoch 6919: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-08 00:35:49,568 EPOCH 6920
2024-02-08 00:36:05,396 Epoch 6920: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-08 00:36:05,397 EPOCH 6921
2024-02-08 00:36:21,588 Epoch 6921: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.77 
2024-02-08 00:36:21,589 EPOCH 6922
2024-02-08 00:36:37,969 Epoch 6922: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.79 
2024-02-08 00:36:37,969 EPOCH 6923
2024-02-08 00:36:41,543 [Epoch: 6923 Step: 00062300] Batch Recognition Loss:   0.000576 => Gls Tokens per Sec:      717 || Batch Translation Loss:   0.477586 => Txt Tokens per Sec:     1954 || Lr: 0.000050
2024-02-08 00:36:54,292 Epoch 6923: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.76 
2024-02-08 00:36:54,292 EPOCH 6924
2024-02-08 00:37:10,185 Epoch 6924: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.27 
2024-02-08 00:37:10,186 EPOCH 6925
2024-02-08 00:37:25,850 Epoch 6925: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.70 
2024-02-08 00:37:25,850 EPOCH 6926
2024-02-08 00:37:42,112 Epoch 6926: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-08 00:37:42,113 EPOCH 6927
2024-02-08 00:37:58,227 Epoch 6927: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-08 00:37:58,228 EPOCH 6928
2024-02-08 00:38:13,825 Epoch 6928: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-08 00:38:13,826 EPOCH 6929
2024-02-08 00:38:30,011 Epoch 6929: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-08 00:38:30,011 EPOCH 6930
2024-02-08 00:38:46,283 Epoch 6930: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-08 00:38:46,284 EPOCH 6931
2024-02-08 00:39:02,164 Epoch 6931: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-08 00:39:02,165 EPOCH 6932
2024-02-08 00:39:18,621 Epoch 6932: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-08 00:39:18,622 EPOCH 6933
2024-02-08 00:39:34,813 Epoch 6933: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-08 00:39:34,813 EPOCH 6934
2024-02-08 00:39:40,120 [Epoch: 6934 Step: 00062400] Batch Recognition Loss:   0.000340 => Gls Tokens per Sec:      554 || Batch Translation Loss:   0.011727 => Txt Tokens per Sec:     1460 || Lr: 0.000050
2024-02-08 00:39:51,257 Epoch 6934: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 00:39:51,258 EPOCH 6935
2024-02-08 00:40:07,287 Epoch 6935: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-08 00:40:07,288 EPOCH 6936
2024-02-08 00:40:23,366 Epoch 6936: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-08 00:40:23,366 EPOCH 6937
2024-02-08 00:40:39,801 Epoch 6937: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 00:40:39,802 EPOCH 6938
2024-02-08 00:40:55,866 Epoch 6938: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-08 00:40:55,867 EPOCH 6939
2024-02-08 00:41:12,074 Epoch 6939: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 00:41:12,074 EPOCH 6940
2024-02-08 00:41:28,156 Epoch 6940: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 00:41:28,157 EPOCH 6941
2024-02-08 00:41:44,212 Epoch 6941: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-08 00:41:44,213 EPOCH 6942
2024-02-08 00:42:00,546 Epoch 6942: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 00:42:00,547 EPOCH 6943
2024-02-08 00:42:16,579 Epoch 6943: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 00:42:16,579 EPOCH 6944
2024-02-08 00:42:32,585 Epoch 6944: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 00:42:32,585 EPOCH 6945
2024-02-08 00:42:34,451 [Epoch: 6945 Step: 00062500] Batch Recognition Loss:   0.000295 => Gls Tokens per Sec:     2746 || Batch Translation Loss:   0.017636 => Txt Tokens per Sec:     7703 || Lr: 0.000050
2024-02-08 00:42:48,500 Epoch 6945: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 00:42:48,500 EPOCH 6946
2024-02-08 00:43:04,858 Epoch 6946: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 00:43:04,859 EPOCH 6947
2024-02-08 00:43:20,868 Epoch 6947: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 00:43:20,868 EPOCH 6948
2024-02-08 00:43:36,816 Epoch 6948: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:43:36,816 EPOCH 6949
2024-02-08 00:43:53,015 Epoch 6949: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:43:53,016 EPOCH 6950
2024-02-08 00:44:08,708 Epoch 6950: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 00:44:08,708 EPOCH 6951
2024-02-08 00:44:24,583 Epoch 6951: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:44:24,583 EPOCH 6952
2024-02-08 00:44:41,033 Epoch 6952: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-08 00:44:41,034 EPOCH 6953
2024-02-08 00:44:57,032 Epoch 6953: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:44:57,033 EPOCH 6954
2024-02-08 00:45:13,217 Epoch 6954: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:45:13,217 EPOCH 6955
2024-02-08 00:45:29,230 Epoch 6955: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:45:29,231 EPOCH 6956
2024-02-08 00:45:31,667 [Epoch: 6956 Step: 00062600] Batch Recognition Loss:   0.000332 => Gls Tokens per Sec:     2629 || Batch Translation Loss:   0.024100 => Txt Tokens per Sec:     6659 || Lr: 0.000050
2024-02-08 00:45:45,375 Epoch 6956: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 00:45:45,376 EPOCH 6957
2024-02-08 00:46:01,686 Epoch 6957: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:46:01,687 EPOCH 6958
2024-02-08 00:46:17,723 Epoch 6958: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:46:17,723 EPOCH 6959
2024-02-08 00:46:33,651 Epoch 6959: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:46:33,652 EPOCH 6960
2024-02-08 00:46:49,909 Epoch 6960: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:46:49,910 EPOCH 6961
2024-02-08 00:47:06,033 Epoch 6961: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:47:06,034 EPOCH 6962
2024-02-08 00:47:22,045 Epoch 6962: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:47:22,045 EPOCH 6963
2024-02-08 00:47:38,163 Epoch 6963: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:47:38,164 EPOCH 6964
2024-02-08 00:47:54,014 Epoch 6964: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:47:54,015 EPOCH 6965
2024-02-08 00:48:10,193 Epoch 6965: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:48:10,194 EPOCH 6966
2024-02-08 00:48:26,337 Epoch 6966: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.09 
2024-02-08 00:48:26,338 EPOCH 6967
2024-02-08 00:48:33,027 [Epoch: 6967 Step: 00062700] Batch Recognition Loss:   0.000270 => Gls Tokens per Sec:     1014 || Batch Translation Loss:   0.005300 => Txt Tokens per Sec:     2636 || Lr: 0.000050
2024-02-08 00:48:42,288 Epoch 6967: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:48:42,288 EPOCH 6968
2024-02-08 00:48:58,698 Epoch 6968: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:48:58,698 EPOCH 6969
2024-02-08 00:49:14,824 Epoch 6969: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:49:14,825 EPOCH 6970
2024-02-08 00:49:31,049 Epoch 6970: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 00:49:31,050 EPOCH 6971
2024-02-08 00:49:47,703 Epoch 6971: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 00:49:47,703 EPOCH 6972
2024-02-08 00:50:03,857 Epoch 6972: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:50:03,858 EPOCH 6973
2024-02-08 00:50:19,893 Epoch 6973: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 00:50:19,894 EPOCH 6974
2024-02-08 00:50:36,268 Epoch 6974: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:50:36,268 EPOCH 6975
2024-02-08 00:50:52,361 Epoch 6975: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:50:52,362 EPOCH 6976
2024-02-08 00:51:08,389 Epoch 6976: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:51:08,389 EPOCH 6977
2024-02-08 00:51:24,477 Epoch 6977: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 00:51:24,478 EPOCH 6978
2024-02-08 00:51:39,690 [Epoch: 6978 Step: 00062800] Batch Recognition Loss:   0.000907 => Gls Tokens per Sec:      530 || Batch Translation Loss:   0.010558 => Txt Tokens per Sec:     1461 || Lr: 0.000050
2024-02-08 00:51:40,778 Epoch 6978: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 00:51:40,778 EPOCH 6979
2024-02-08 00:51:56,839 Epoch 6979: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:51:56,839 EPOCH 6980
2024-02-08 00:52:13,713 Epoch 6980: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 00:52:13,714 EPOCH 6981
2024-02-08 00:52:29,915 Epoch 6981: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:52:29,915 EPOCH 6982
2024-02-08 00:52:46,116 Epoch 6982: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:52:46,116 EPOCH 6983
2024-02-08 00:53:02,295 Epoch 6983: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:53:02,295 EPOCH 6984
2024-02-08 00:53:18,445 Epoch 6984: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 00:53:18,445 EPOCH 6985
2024-02-08 00:53:34,702 Epoch 6985: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 00:53:34,702 EPOCH 6986
2024-02-08 00:53:50,770 Epoch 6986: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 00:53:50,770 EPOCH 6987
2024-02-08 00:54:06,760 Epoch 6987: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 00:54:06,760 EPOCH 6988
2024-02-08 00:54:22,871 Epoch 6988: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 00:54:22,872 EPOCH 6989
2024-02-08 00:54:38,703 [Epoch: 6989 Step: 00062900] Batch Recognition Loss:   0.000479 => Gls Tokens per Sec:      590 || Batch Translation Loss:   0.005390 => Txt Tokens per Sec:     1711 || Lr: 0.000050
2024-02-08 00:54:38,994 Epoch 6989: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:54:38,995 EPOCH 6990
2024-02-08 00:54:55,328 Epoch 6990: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 00:54:55,329 EPOCH 6991
2024-02-08 00:55:11,447 Epoch 6991: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 00:55:11,447 EPOCH 6992
2024-02-08 00:55:27,604 Epoch 6992: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 00:55:27,605 EPOCH 6993
2024-02-08 00:55:43,935 Epoch 6993: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:55:43,935 EPOCH 6994
2024-02-08 00:56:00,140 Epoch 6994: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 00:56:00,141 EPOCH 6995
2024-02-08 00:56:16,139 Epoch 6995: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:56:16,139 EPOCH 6996
2024-02-08 00:56:32,041 Epoch 6996: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 00:56:32,041 EPOCH 6997
2024-02-08 00:56:47,958 Epoch 6997: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 00:56:47,959 EPOCH 6998
2024-02-08 00:57:04,380 Epoch 6998: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 00:57:04,381 EPOCH 6999
2024-02-08 00:57:20,550 Epoch 6999: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 00:57:20,551 EPOCH 7000
2024-02-08 00:57:36,436 [Epoch: 7000 Step: 00063000] Batch Recognition Loss:   0.000479 => Gls Tokens per Sec:      669 || Batch Translation Loss:   0.004962 => Txt Tokens per Sec:     1850 || Lr: 0.000050
2024-02-08 00:57:36,436 Epoch 7000: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 00:57:36,436 EPOCH 7001
2024-02-08 00:57:52,209 Epoch 7001: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 00:57:52,210 EPOCH 7002
2024-02-08 00:58:08,439 Epoch 7002: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 00:58:08,439 EPOCH 7003
2024-02-08 00:58:24,506 Epoch 7003: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 00:58:24,507 EPOCH 7004
2024-02-08 00:58:40,758 Epoch 7004: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 00:58:40,759 EPOCH 7005
2024-02-08 00:58:56,941 Epoch 7005: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 00:58:56,941 EPOCH 7006
2024-02-08 00:59:12,489 Epoch 7006: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 00:59:12,490 EPOCH 7007
2024-02-08 00:59:28,510 Epoch 7007: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 00:59:28,510 EPOCH 7008
2024-02-08 00:59:44,865 Epoch 7008: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 00:59:44,866 EPOCH 7009
2024-02-08 01:00:01,063 Epoch 7009: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:00:01,063 EPOCH 7010
2024-02-08 01:00:17,074 Epoch 7010: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:00:17,074 EPOCH 7011
2024-02-08 01:00:33,064 Epoch 7011: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:00:33,065 EPOCH 7012
2024-02-08 01:00:33,939 [Epoch: 7012 Step: 00063100] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:     1465 || Batch Translation Loss:   0.012155 => Txt Tokens per Sec:     4443 || Lr: 0.000050
2024-02-08 01:00:49,426 Epoch 7012: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:00:49,427 EPOCH 7013
2024-02-08 01:01:05,295 Epoch 7013: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.09 
2024-02-08 01:01:05,295 EPOCH 7014
2024-02-08 01:01:21,760 Epoch 7014: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:01:21,760 EPOCH 7015
2024-02-08 01:01:37,932 Epoch 7015: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:01:37,933 EPOCH 7016
2024-02-08 01:01:53,778 Epoch 7016: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:01:53,779 EPOCH 7017
2024-02-08 01:02:09,944 Epoch 7017: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:02:09,945 EPOCH 7018
2024-02-08 01:02:25,975 Epoch 7018: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:02:25,976 EPOCH 7019
2024-02-08 01:02:42,277 Epoch 7019: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:02:42,277 EPOCH 7020
2024-02-08 01:02:58,391 Epoch 7020: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:02:58,392 EPOCH 7021
2024-02-08 01:03:14,441 Epoch 7021: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:03:14,442 EPOCH 7022
2024-02-08 01:03:30,905 Epoch 7022: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:03:30,905 EPOCH 7023
2024-02-08 01:03:37,117 [Epoch: 7023 Step: 00063200] Batch Recognition Loss:   0.000484 => Gls Tokens per Sec:      412 || Batch Translation Loss:   0.014716 => Txt Tokens per Sec:     1280 || Lr: 0.000050
2024-02-08 01:03:46,992 Epoch 7023: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:03:46,993 EPOCH 7024
2024-02-08 01:04:03,154 Epoch 7024: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:04:03,154 EPOCH 7025
2024-02-08 01:04:19,413 Epoch 7025: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:04:19,413 EPOCH 7026
2024-02-08 01:04:35,570 Epoch 7026: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:04:35,571 EPOCH 7027
2024-02-08 01:04:51,998 Epoch 7027: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 01:04:51,998 EPOCH 7028
2024-02-08 01:05:08,123 Epoch 7028: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:05:08,123 EPOCH 7029
2024-02-08 01:05:24,020 Epoch 7029: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:05:24,021 EPOCH 7030
2024-02-08 01:05:40,309 Epoch 7030: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:05:40,310 EPOCH 7031
2024-02-08 01:05:56,880 Epoch 7031: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:05:56,880 EPOCH 7032
2024-02-08 01:06:12,940 Epoch 7032: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 01:06:12,940 EPOCH 7033
2024-02-08 01:06:28,856 Epoch 7033: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:06:28,856 EPOCH 7034
2024-02-08 01:06:42,250 [Epoch: 7034 Step: 00063300] Batch Recognition Loss:   0.000423 => Gls Tokens per Sec:      220 || Batch Translation Loss:   0.013427 => Txt Tokens per Sec:      736 || Lr: 0.000050
2024-02-08 01:06:45,126 Epoch 7034: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.09 
2024-02-08 01:06:45,127 EPOCH 7035
2024-02-08 01:07:01,336 Epoch 7035: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:07:01,337 EPOCH 7036
2024-02-08 01:07:17,370 Epoch 7036: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:07:17,371 EPOCH 7037
2024-02-08 01:07:33,339 Epoch 7037: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:07:33,339 EPOCH 7038
2024-02-08 01:07:49,654 Epoch 7038: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:07:49,654 EPOCH 7039
2024-02-08 01:08:05,809 Epoch 7039: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:08:05,809 EPOCH 7040
2024-02-08 01:08:21,782 Epoch 7040: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:08:21,782 EPOCH 7041
2024-02-08 01:08:37,673 Epoch 7041: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 01:08:37,674 EPOCH 7042
2024-02-08 01:08:54,086 Epoch 7042: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:08:54,087 EPOCH 7043
2024-02-08 01:09:10,221 Epoch 7043: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:09:10,221 EPOCH 7044
2024-02-08 01:09:26,557 Epoch 7044: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:09:26,557 EPOCH 7045
2024-02-08 01:09:33,814 [Epoch: 7045 Step: 00063400] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:      706 || Batch Translation Loss:   0.014273 => Txt Tokens per Sec:     2107 || Lr: 0.000050
2024-02-08 01:09:42,470 Epoch 7045: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:09:42,471 EPOCH 7046
2024-02-08 01:09:58,752 Epoch 7046: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:09:58,753 EPOCH 7047
2024-02-08 01:10:14,656 Epoch 7047: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:10:14,657 EPOCH 7048
2024-02-08 01:10:30,952 Epoch 7048: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:10:30,953 EPOCH 7049
2024-02-08 01:10:46,903 Epoch 7049: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:10:46,904 EPOCH 7050
2024-02-08 01:11:02,934 Epoch 7050: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:11:02,935 EPOCH 7051
2024-02-08 01:11:19,567 Epoch 7051: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:11:19,568 EPOCH 7052
2024-02-08 01:11:35,631 Epoch 7052: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:11:35,632 EPOCH 7053
2024-02-08 01:11:51,508 Epoch 7053: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:11:51,509 EPOCH 7054
2024-02-08 01:12:07,844 Epoch 7054: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:12:07,844 EPOCH 7055
2024-02-08 01:12:23,977 Epoch 7055: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:12:23,977 EPOCH 7056
2024-02-08 01:12:38,508 [Epoch: 7056 Step: 00063500] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:      379 || Batch Translation Loss:   0.010296 => Txt Tokens per Sec:     1189 || Lr: 0.000050
2024-02-08 01:12:40,197 Epoch 7056: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:12:40,197 EPOCH 7057
2024-02-08 01:12:56,715 Epoch 7057: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:12:56,716 EPOCH 7058
2024-02-08 01:13:12,740 Epoch 7058: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:13:12,741 EPOCH 7059
2024-02-08 01:13:28,997 Epoch 7059: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 01:13:28,998 EPOCH 7060
2024-02-08 01:13:45,244 Epoch 7060: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 01:13:45,245 EPOCH 7061
2024-02-08 01:14:01,354 Epoch 7061: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 01:14:01,355 EPOCH 7062
2024-02-08 01:14:17,512 Epoch 7062: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 01:14:17,512 EPOCH 7063
2024-02-08 01:14:33,766 Epoch 7063: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 01:14:33,767 EPOCH 7064
2024-02-08 01:14:50,020 Epoch 7064: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 01:14:50,021 EPOCH 7065
2024-02-08 01:15:05,958 Epoch 7065: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 01:15:05,959 EPOCH 7066
2024-02-08 01:15:22,273 Epoch 7066: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 01:15:22,273 EPOCH 7067
2024-02-08 01:15:33,057 [Epoch: 7067 Step: 00063600] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:      712 || Batch Translation Loss:   0.006373 => Txt Tokens per Sec:     1928 || Lr: 0.000050
2024-02-08 01:15:38,476 Epoch 7067: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 01:15:38,477 EPOCH 7068
2024-02-08 01:15:54,681 Epoch 7068: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:15:54,681 EPOCH 7069
2024-02-08 01:16:10,307 Epoch 7069: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 01:16:10,307 EPOCH 7070
2024-02-08 01:16:26,309 Epoch 7070: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 01:16:26,309 EPOCH 7071
2024-02-08 01:16:42,561 Epoch 7071: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:16:42,562 EPOCH 7072
2024-02-08 01:16:58,903 Epoch 7072: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:16:58,904 EPOCH 7073
2024-02-08 01:17:14,929 Epoch 7073: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:17:14,929 EPOCH 7074
2024-02-08 01:17:31,140 Epoch 7074: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 01:17:31,140 EPOCH 7075
2024-02-08 01:17:47,211 Epoch 7075: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:17:47,212 EPOCH 7076
2024-02-08 01:18:03,074 Epoch 7076: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 01:18:03,075 EPOCH 7077
2024-02-08 01:18:19,426 Epoch 7077: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 01:18:19,427 EPOCH 7078
2024-02-08 01:18:30,380 [Epoch: 7078 Step: 00063700] Batch Recognition Loss:   0.000288 => Gls Tokens per Sec:      818 || Batch Translation Loss:   0.013115 => Txt Tokens per Sec:     2238 || Lr: 0.000050
2024-02-08 01:18:35,123 Epoch 7078: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:18:35,123 EPOCH 7079
2024-02-08 01:18:51,465 Epoch 7079: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:18:51,465 EPOCH 7080
2024-02-08 01:19:07,610 Epoch 7080: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:19:07,611 EPOCH 7081
2024-02-08 01:19:23,624 Epoch 7081: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:19:23,625 EPOCH 7082
2024-02-08 01:19:40,087 Epoch 7082: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-08 01:19:40,088 EPOCH 7083
2024-02-08 01:19:56,224 Epoch 7083: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:19:56,224 EPOCH 7084
2024-02-08 01:20:12,416 Epoch 7084: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:20:12,416 EPOCH 7085
2024-02-08 01:20:28,769 Epoch 7085: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:20:28,770 EPOCH 7086
2024-02-08 01:20:44,739 Epoch 7086: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 01:20:44,740 EPOCH 7087
2024-02-08 01:21:00,835 Epoch 7087: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 01:21:00,835 EPOCH 7088
2024-02-08 01:21:16,874 Epoch 7088: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 01:21:16,874 EPOCH 7089
2024-02-08 01:21:32,502 [Epoch: 7089 Step: 00063800] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:      598 || Batch Translation Loss:   0.010691 => Txt Tokens per Sec:     1634 || Lr: 0.000050
2024-02-08 01:21:33,200 Epoch 7089: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 01:21:33,200 EPOCH 7090
2024-02-08 01:21:49,188 Epoch 7090: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 01:21:49,189 EPOCH 7091
2024-02-08 01:22:05,289 Epoch 7091: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 01:22:05,289 EPOCH 7092
2024-02-08 01:22:21,343 Epoch 7092: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 01:22:21,344 EPOCH 7093
2024-02-08 01:22:37,430 Epoch 7093: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 01:22:37,431 EPOCH 7094
2024-02-08 01:22:53,704 Epoch 7094: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 01:22:53,704 EPOCH 7095
2024-02-08 01:23:10,334 Epoch 7095: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 01:23:10,335 EPOCH 7096
2024-02-08 01:23:26,473 Epoch 7096: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 01:23:26,473 EPOCH 7097
2024-02-08 01:23:42,535 Epoch 7097: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 01:23:42,535 EPOCH 7098
2024-02-08 01:23:58,712 Epoch 7098: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-08 01:23:58,713 EPOCH 7099
2024-02-08 01:24:15,141 Epoch 7099: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 01:24:15,142 EPOCH 7100
2024-02-08 01:24:31,344 [Epoch: 7100 Step: 00063900] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:      656 || Batch Translation Loss:   0.021495 => Txt Tokens per Sec:     1814 || Lr: 0.000050
2024-02-08 01:24:31,345 Epoch 7100: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 01:24:31,345 EPOCH 7101
2024-02-08 01:24:47,252 Epoch 7101: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 01:24:47,253 EPOCH 7102
2024-02-08 01:25:03,706 Epoch 7102: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 01:25:03,707 EPOCH 7103
2024-02-08 01:25:19,801 Epoch 7103: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-08 01:25:19,802 EPOCH 7104
2024-02-08 01:25:36,026 Epoch 7104: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-08 01:25:36,026 EPOCH 7105
2024-02-08 01:25:51,961 Epoch 7105: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-08 01:25:51,961 EPOCH 7106
2024-02-08 01:26:07,997 Epoch 7106: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-08 01:26:07,998 EPOCH 7107
2024-02-08 01:26:24,273 Epoch 7107: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-08 01:26:24,273 EPOCH 7108
2024-02-08 01:26:40,164 Epoch 7108: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-08 01:26:40,165 EPOCH 7109
2024-02-08 01:26:56,331 Epoch 7109: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-08 01:26:56,331 EPOCH 7110
2024-02-08 01:27:12,336 Epoch 7110: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-08 01:27:12,337 EPOCH 7111
2024-02-08 01:27:28,636 Epoch 7111: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-08 01:27:28,637 EPOCH 7112
2024-02-08 01:27:32,986 [Epoch: 7112 Step: 00064000] Batch Recognition Loss:   0.000333 => Gls Tokens per Sec:       87 || Batch Translation Loss:   0.005656 => Txt Tokens per Sec:      311 || Lr: 0.000050
2024-02-08 01:28:40,719 Validation result at epoch 7112, step    64000: duration: 67.7329s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.24701	Translation Loss: 110488.64062	PPL: 62045.34375
	Eval Metric: BLEU
	WER 2.47	(DEL: 0.00,	INS: 0.00,	SUB: 2.47)
	BLEU-4 0.41	(BLEU-1: 9.60,	BLEU-2: 2.55,	BLEU-3: 0.87,	BLEU-4: 0.41)
	CHRF 16.33	ROUGE 8.15
2024-02-08 01:28:40,722 Logging Recognition and Translation Outputs
2024-02-08 01:28:40,722 ========================================================================================================================
2024-02-08 01:28:40,722 Logging Sequence: 122_86.00
2024-02-08 01:28:40,723 	Gloss Reference :	A B+C+D+E
2024-02-08 01:28:40,724 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 01:28:40,725 	Gloss Alignment :	         
2024-02-08 01:28:40,725 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 01:28:40,726 	Text Reference  :	after winning chanu spoke to    the media  and said   
2024-02-08 01:28:40,726 	Text Hypothesis :	***** he      won   the   world cup before her innings
2024-02-08 01:28:40,727 	Text Alignment  :	D     S       S     S     S     S   S      S   S      
2024-02-08 01:28:40,727 ========================================================================================================================
2024-02-08 01:28:40,727 Logging Sequence: 82_81.00
2024-02-08 01:28:40,727 	Gloss Reference :	A B+C+D+E
2024-02-08 01:28:40,727 	Gloss Hypothesis:	A B+C+D  
2024-02-08 01:28:40,727 	Gloss Alignment :	  S      
2024-02-08 01:28:40,728 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 01:28:40,729 	Text Reference  :	since the couple were residents of       mumbai the mumbai police  cyber cell began investigating the matter
2024-02-08 01:28:40,729 	Text Hypothesis :	***** *** ****** on   13th      february 2023   the ****** auction was   tv   and   kozhikode     in  2019  
2024-02-08 01:28:40,729 	Text Alignment  :	D     D   D      S    S         S        S          D      S       S     S    S     S             S   S     
2024-02-08 01:28:40,729 ========================================================================================================================
2024-02-08 01:28:40,729 Logging Sequence: 61_65.00
2024-02-08 01:28:40,730 	Gloss Reference :	A B+C+D+E
2024-02-08 01:28:40,730 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 01:28:40,730 	Gloss Alignment :	         
2024-02-08 01:28:40,730 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 01:28:40,731 	Text Reference  :	the name seems indian but whether it has been     made    by     an  indian
2024-02-08 01:28:40,731 	Text Hypothesis :	*** **** ***** ****** *** gambhir is an  argument between season and kohli 
2024-02-08 01:28:40,731 	Text Alignment  :	D   D    D     D      D   S       S  S   S        S       S      S   S     
2024-02-08 01:28:40,731 ========================================================================================================================
2024-02-08 01:28:40,731 Logging Sequence: 179_126.00
2024-02-08 01:28:40,732 	Gloss Reference :	A B+C+D+E
2024-02-08 01:28:40,732 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 01:28:40,733 	Gloss Alignment :	         
2024-02-08 01:28:40,733 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 01:28:40,734 	Text Reference  :	vinesh argued that she might contract coronavirus since these wrestlers travelled    from india where there are      many infections
2024-02-08 01:28:40,735 	Text Hypothesis :	****** ****** **** *** ***** ******** *********** ***** he    also      participated in   the   2020  tokyo olympics very 2021      
2024-02-08 01:28:40,735 	Text Alignment  :	D      D      D    D   D     D        D           D     S     S         S            S    S     S     S     S        S    S         
2024-02-08 01:28:40,735 ========================================================================================================================
2024-02-08 01:28:40,735 Logging Sequence: 62_24.00
2024-02-08 01:28:40,735 	Gloss Reference :	A B+C+D+E
2024-02-08 01:28:40,735 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 01:28:40,736 	Gloss Alignment :	         
2024-02-08 01:28:40,736 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 01:28:40,738 	Text Reference  :	now the women's cricket team  too  is  giving splendid performances which are at  par   with the    men's team 
2024-02-08 01:28:40,738 	Text Hypothesis :	we  is  have    to      thank bcci was you    about    me           tell  you the woman an   number of    india
2024-02-08 01:28:40,738 	Text Alignment  :	S   S   S       S       S     S    S   S      S        S            S     S   S   S     S    S      S     S    
2024-02-08 01:28:40,738 ========================================================================================================================
2024-02-08 01:28:53,087 Epoch 7112: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-08 01:28:53,088 EPOCH 7113
2024-02-08 01:29:09,370 Epoch 7113: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-08 01:29:09,371 EPOCH 7114
2024-02-08 01:29:25,671 Epoch 7114: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-08 01:29:25,671 EPOCH 7115
2024-02-08 01:29:41,625 Epoch 7115: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-08 01:29:41,625 EPOCH 7116
2024-02-08 01:29:57,721 Epoch 7116: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-08 01:29:57,722 EPOCH 7117
2024-02-08 01:30:13,757 Epoch 7117: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-08 01:30:13,757 EPOCH 7118
2024-02-08 01:30:29,965 Epoch 7118: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-08 01:30:29,966 EPOCH 7119
2024-02-08 01:30:46,095 Epoch 7119: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-08 01:30:46,095 EPOCH 7120
2024-02-08 01:31:02,507 Epoch 7120: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-08 01:31:02,508 EPOCH 7121
2024-02-08 01:31:19,042 Epoch 7121: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-08 01:31:19,043 EPOCH 7122
2024-02-08 01:31:35,415 Epoch 7122: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-08 01:31:35,416 EPOCH 7123
2024-02-08 01:31:40,087 [Epoch: 7123 Step: 00064100] Batch Recognition Loss:   0.000303 => Gls Tokens per Sec:      355 || Batch Translation Loss:   0.046800 => Txt Tokens per Sec:     1041 || Lr: 0.000050
2024-02-08 01:31:51,193 Epoch 7123: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-08 01:31:51,194 EPOCH 7124
2024-02-08 01:32:07,581 Epoch 7124: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-08 01:32:07,581 EPOCH 7125
2024-02-08 01:32:23,601 Epoch 7125: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-08 01:32:23,602 EPOCH 7126
2024-02-08 01:32:39,728 Epoch 7126: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-08 01:32:39,728 EPOCH 7127
2024-02-08 01:32:56,006 Epoch 7127: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-08 01:32:56,006 EPOCH 7128
2024-02-08 01:33:11,867 Epoch 7128: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-08 01:33:11,868 EPOCH 7129
2024-02-08 01:33:28,101 Epoch 7129: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 01:33:28,102 EPOCH 7130
2024-02-08 01:33:44,477 Epoch 7130: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 01:33:44,478 EPOCH 7131
2024-02-08 01:34:00,716 Epoch 7131: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 01:34:00,716 EPOCH 7132
2024-02-08 01:34:16,718 Epoch 7132: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-08 01:34:16,718 EPOCH 7133
2024-02-08 01:34:32,927 Epoch 7133: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-08 01:34:32,928 EPOCH 7134
2024-02-08 01:34:36,691 [Epoch: 7134 Step: 00064200] Batch Recognition Loss:   0.000351 => Gls Tokens per Sec:     1021 || Batch Translation Loss:   0.006254 => Txt Tokens per Sec:     2468 || Lr: 0.000050
2024-02-08 01:34:48,987 Epoch 7134: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-08 01:34:48,988 EPOCH 7135
2024-02-08 01:35:05,066 Epoch 7135: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:35:05,067 EPOCH 7136
2024-02-08 01:35:21,221 Epoch 7136: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-08 01:35:21,222 EPOCH 7137
2024-02-08 01:35:37,413 Epoch 7137: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:35:37,414 EPOCH 7138
2024-02-08 01:35:53,675 Epoch 7138: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:35:53,676 EPOCH 7139
2024-02-08 01:36:09,773 Epoch 7139: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:36:09,774 EPOCH 7140
2024-02-08 01:36:25,861 Epoch 7140: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:36:25,861 EPOCH 7141
2024-02-08 01:36:41,776 Epoch 7141: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:36:41,777 EPOCH 7142
2024-02-08 01:36:58,333 Epoch 7142: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:36:58,333 EPOCH 7143
2024-02-08 01:37:14,478 Epoch 7143: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:37:14,479 EPOCH 7144
2024-02-08 01:37:30,699 Epoch 7144: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:37:30,700 EPOCH 7145
2024-02-08 01:37:35,096 [Epoch: 7145 Step: 00064300] Batch Recognition Loss:   0.000274 => Gls Tokens per Sec:     1165 || Batch Translation Loss:   0.011830 => Txt Tokens per Sec:     3086 || Lr: 0.000050
2024-02-08 01:37:46,795 Epoch 7145: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:37:46,795 EPOCH 7146
2024-02-08 01:38:02,923 Epoch 7146: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:38:02,923 EPOCH 7147
2024-02-08 01:38:18,957 Epoch 7147: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:38:18,958 EPOCH 7148
2024-02-08 01:38:35,102 Epoch 7148: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:38:35,102 EPOCH 7149
2024-02-08 01:38:50,897 Epoch 7149: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:38:50,898 EPOCH 7150
2024-02-08 01:39:06,750 Epoch 7150: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:39:06,750 EPOCH 7151
2024-02-08 01:39:23,143 Epoch 7151: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:39:23,144 EPOCH 7152
2024-02-08 01:39:39,311 Epoch 7152: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:39:39,312 EPOCH 7153
2024-02-08 01:39:55,299 Epoch 7153: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:39:55,300 EPOCH 7154
2024-02-08 01:40:11,785 Epoch 7154: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:40:11,786 EPOCH 7155
2024-02-08 01:40:27,864 Epoch 7155: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:40:27,864 EPOCH 7156
2024-02-08 01:40:33,741 [Epoch: 7156 Step: 00064400] Batch Recognition Loss:   0.000288 => Gls Tokens per Sec:      936 || Batch Translation Loss:   0.007314 => Txt Tokens per Sec:     2318 || Lr: 0.000050
2024-02-08 01:40:44,047 Epoch 7156: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:40:44,048 EPOCH 7157
2024-02-08 01:41:00,359 Epoch 7157: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:41:00,361 EPOCH 7158
2024-02-08 01:41:16,494 Epoch 7158: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:41:16,495 EPOCH 7159
2024-02-08 01:41:32,817 Epoch 7159: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:41:32,818 EPOCH 7160
2024-02-08 01:41:48,832 Epoch 7160: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 01:41:48,832 EPOCH 7161
2024-02-08 01:42:05,180 Epoch 7161: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:42:05,181 EPOCH 7162
2024-02-08 01:42:21,280 Epoch 7162: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:42:21,281 EPOCH 7163
2024-02-08 01:42:37,475 Epoch 7163: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 01:42:37,476 EPOCH 7164
2024-02-08 01:42:53,704 Epoch 7164: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 01:42:53,704 EPOCH 7165
2024-02-08 01:43:09,532 Epoch 7165: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 01:43:09,533 EPOCH 7166
2024-02-08 01:43:25,800 Epoch 7166: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:43:25,801 EPOCH 7167
2024-02-08 01:43:38,021 [Epoch: 7167 Step: 00064500] Batch Recognition Loss:   0.000319 => Gls Tokens per Sec:      555 || Batch Translation Loss:   0.013898 => Txt Tokens per Sec:     1593 || Lr: 0.000050
2024-02-08 01:43:41,968 Epoch 7167: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 01:43:41,969 EPOCH 7168
2024-02-08 01:43:58,349 Epoch 7168: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 01:43:58,349 EPOCH 7169
2024-02-08 01:44:14,480 Epoch 7169: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 01:44:14,480 EPOCH 7170
2024-02-08 01:44:30,544 Epoch 7170: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 01:44:30,545 EPOCH 7171
2024-02-08 01:44:46,643 Epoch 7171: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:44:46,643 EPOCH 7172
2024-02-08 01:45:02,909 Epoch 7172: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:45:02,909 EPOCH 7173
2024-02-08 01:45:18,855 Epoch 7173: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 01:45:18,855 EPOCH 7174
2024-02-08 01:45:34,956 Epoch 7174: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 01:45:34,957 EPOCH 7175
2024-02-08 01:45:51,133 Epoch 7175: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 01:45:51,134 EPOCH 7176
2024-02-08 01:46:07,573 Epoch 7176: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 01:46:07,574 EPOCH 7177
2024-02-08 01:46:24,185 Epoch 7177: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 01:46:24,186 EPOCH 7178
2024-02-08 01:46:35,833 [Epoch: 7178 Step: 00064600] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:      769 || Batch Translation Loss:   0.018130 => Txt Tokens per Sec:     2160 || Lr: 0.000050
2024-02-08 01:46:40,553 Epoch 7178: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 01:46:40,554 EPOCH 7179
2024-02-08 01:46:56,813 Epoch 7179: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 01:46:56,814 EPOCH 7180
2024-02-08 01:47:13,309 Epoch 7180: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:47:13,309 EPOCH 7181
2024-02-08 01:47:29,721 Epoch 7181: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:47:29,723 EPOCH 7182
2024-02-08 01:47:46,367 Epoch 7182: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:47:46,367 EPOCH 7183
2024-02-08 01:48:02,457 Epoch 7183: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:48:02,458 EPOCH 7184
2024-02-08 01:48:18,998 Epoch 7184: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:48:18,999 EPOCH 7185
2024-02-08 01:48:35,312 Epoch 7185: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:48:35,312 EPOCH 7186
2024-02-08 01:48:51,183 Epoch 7186: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:48:51,183 EPOCH 7187
2024-02-08 01:49:07,632 Epoch 7187: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:49:07,633 EPOCH 7188
2024-02-08 01:49:23,659 Epoch 7188: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:49:23,660 EPOCH 7189
2024-02-08 01:49:39,587 [Epoch: 7189 Step: 00064700] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:      586 || Batch Translation Loss:   0.009891 => Txt Tokens per Sec:     1666 || Lr: 0.000050
2024-02-08 01:49:39,916 Epoch 7189: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:49:39,916 EPOCH 7190
2024-02-08 01:49:56,112 Epoch 7190: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:49:56,113 EPOCH 7191
2024-02-08 01:50:12,309 Epoch 7191: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 01:50:12,310 EPOCH 7192
2024-02-08 01:50:28,513 Epoch 7192: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 01:50:28,514 EPOCH 7193
2024-02-08 01:50:44,591 Epoch 7193: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-08 01:50:44,592 EPOCH 7194
2024-02-08 01:51:00,673 Epoch 7194: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-08 01:51:00,674 EPOCH 7195
2024-02-08 01:51:16,567 Epoch 7195: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-08 01:51:16,568 EPOCH 7196
2024-02-08 01:51:32,591 Epoch 7196: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-08 01:51:32,592 EPOCH 7197
2024-02-08 01:51:48,575 Epoch 7197: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-08 01:51:48,575 EPOCH 7198
2024-02-08 01:52:04,720 Epoch 7198: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-08 01:52:04,721 EPOCH 7199
2024-02-08 01:52:20,786 Epoch 7199: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-08 01:52:20,786 EPOCH 7200
2024-02-08 01:52:36,627 [Epoch: 7200 Step: 00064800] Batch Recognition Loss:   0.000662 => Gls Tokens per Sec:      671 || Batch Translation Loss:   0.024583 => Txt Tokens per Sec:     1855 || Lr: 0.000050
2024-02-08 01:52:36,627 Epoch 7200: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-08 01:52:36,628 EPOCH 7201
2024-02-08 01:52:52,753 Epoch 7201: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-08 01:52:52,754 EPOCH 7202
2024-02-08 01:53:09,354 Epoch 7202: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-08 01:53:09,354 EPOCH 7203
2024-02-08 01:53:25,708 Epoch 7203: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-08 01:53:25,709 EPOCH 7204
2024-02-08 01:53:41,523 Epoch 7204: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-08 01:53:41,524 EPOCH 7205
2024-02-08 01:53:57,604 Epoch 7205: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 01:53:57,605 EPOCH 7206
2024-02-08 01:54:13,821 Epoch 7206: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 01:54:13,822 EPOCH 7207
2024-02-08 01:54:29,892 Epoch 7207: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 01:54:29,893 EPOCH 7208
2024-02-08 01:54:46,096 Epoch 7208: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-08 01:54:46,097 EPOCH 7209
2024-02-08 01:55:02,033 Epoch 7209: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 01:55:02,034 EPOCH 7210
2024-02-08 01:55:17,993 Epoch 7210: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 01:55:17,993 EPOCH 7211
2024-02-08 01:55:33,983 Epoch 7211: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-08 01:55:33,984 EPOCH 7212
2024-02-08 01:55:34,381 [Epoch: 7212 Step: 00064900] Batch Recognition Loss:   0.000192 => Gls Tokens per Sec:     3249 || Batch Translation Loss:   0.010974 => Txt Tokens per Sec:     7302 || Lr: 0.000050
2024-02-08 01:55:50,380 Epoch 7212: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-08 01:55:50,380 EPOCH 7213
2024-02-08 01:56:06,414 Epoch 7213: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-08 01:56:06,414 EPOCH 7214
2024-02-08 01:56:22,353 Epoch 7214: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 01:56:22,354 EPOCH 7215
2024-02-08 01:56:38,619 Epoch 7215: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 01:56:38,620 EPOCH 7216
2024-02-08 01:56:54,565 Epoch 7216: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 01:56:54,565 EPOCH 7217
2024-02-08 01:57:10,498 Epoch 7217: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 01:57:10,499 EPOCH 7218
2024-02-08 01:57:26,446 Epoch 7218: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 01:57:26,447 EPOCH 7219
2024-02-08 01:57:42,701 Epoch 7219: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 01:57:42,702 EPOCH 7220
2024-02-08 01:57:58,621 Epoch 7220: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 01:57:58,622 EPOCH 7221
2024-02-08 01:58:14,844 Epoch 7221: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 01:58:14,845 EPOCH 7222
2024-02-08 01:58:30,790 Epoch 7222: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:58:30,791 EPOCH 7223
2024-02-08 01:58:31,909 [Epoch: 7223 Step: 00065000] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:     2294 || Batch Translation Loss:   0.022171 => Txt Tokens per Sec:     6051 || Lr: 0.000050
2024-02-08 01:58:47,177 Epoch 7223: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 01:58:47,178 EPOCH 7224
2024-02-08 01:59:03,220 Epoch 7224: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:59:03,220 EPOCH 7225
2024-02-08 01:59:19,288 Epoch 7225: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:59:19,289 EPOCH 7226
2024-02-08 01:59:35,198 Epoch 7226: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:59:35,198 EPOCH 7227
2024-02-08 01:59:51,368 Epoch 7227: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 01:59:51,369 EPOCH 7228
2024-02-08 02:00:07,388 Epoch 7228: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:00:07,389 EPOCH 7229
2024-02-08 02:00:23,607 Epoch 7229: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:00:23,608 EPOCH 7230
2024-02-08 02:00:39,905 Epoch 7230: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 02:00:39,906 EPOCH 7231
2024-02-08 02:00:55,885 Epoch 7231: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 02:00:55,886 EPOCH 7232
2024-02-08 02:01:11,958 Epoch 7232: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 02:01:11,959 EPOCH 7233
2024-02-08 02:01:28,176 Epoch 7233: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-08 02:01:28,176 EPOCH 7234
2024-02-08 02:01:38,757 [Epoch: 7234 Step: 00065100] Batch Recognition Loss:   0.000114 => Gls Tokens per Sec:      278 || Batch Translation Loss:   0.009862 => Txt Tokens per Sec:      856 || Lr: 0.000050
2024-02-08 02:01:44,300 Epoch 7234: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 02:01:44,301 EPOCH 7235
2024-02-08 02:02:00,401 Epoch 7235: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 02:02:00,401 EPOCH 7236
2024-02-08 02:02:16,329 Epoch 7236: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 02:02:16,330 EPOCH 7237
2024-02-08 02:02:32,712 Epoch 7237: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 02:02:32,713 EPOCH 7238
2024-02-08 02:02:48,579 Epoch 7238: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 02:02:48,581 EPOCH 7239
2024-02-08 02:03:04,932 Epoch 7239: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 02:03:04,932 EPOCH 7240
2024-02-08 02:03:20,990 Epoch 7240: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 02:03:20,991 EPOCH 7241
2024-02-08 02:03:37,190 Epoch 7241: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 02:03:37,192 EPOCH 7242
2024-02-08 02:03:53,184 Epoch 7242: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 02:03:53,185 EPOCH 7243
2024-02-08 02:04:09,268 Epoch 7243: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 02:04:09,269 EPOCH 7244
2024-02-08 02:04:25,619 Epoch 7244: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 02:04:25,620 EPOCH 7245
2024-02-08 02:04:34,111 [Epoch: 7245 Step: 00065200] Batch Recognition Loss:   0.000488 => Gls Tokens per Sec:      497 || Batch Translation Loss:   0.004962 => Txt Tokens per Sec:     1506 || Lr: 0.000050
2024-02-08 02:04:41,529 Epoch 7245: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 02:04:41,530 EPOCH 7246
2024-02-08 02:04:57,547 Epoch 7246: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 02:04:57,548 EPOCH 7247
2024-02-08 02:05:13,631 Epoch 7247: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 02:05:13,632 EPOCH 7248
2024-02-08 02:05:29,622 Epoch 7248: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 02:05:29,623 EPOCH 7249
2024-02-08 02:05:45,981 Epoch 7249: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 02:05:45,982 EPOCH 7250
2024-02-08 02:06:02,193 Epoch 7250: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 02:06:02,194 EPOCH 7251
2024-02-08 02:06:18,488 Epoch 7251: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 02:06:18,489 EPOCH 7252
2024-02-08 02:06:34,633 Epoch 7252: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 02:06:34,633 EPOCH 7253
2024-02-08 02:06:50,913 Epoch 7253: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-08 02:06:50,913 EPOCH 7254
2024-02-08 02:07:07,094 Epoch 7254: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 02:07:07,095 EPOCH 7255
2024-02-08 02:07:23,295 Epoch 7255: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 02:07:23,295 EPOCH 7256
2024-02-08 02:07:32,040 [Epoch: 7256 Step: 00065300] Batch Recognition Loss:   0.000352 => Gls Tokens per Sec:      629 || Batch Translation Loss:   0.024238 => Txt Tokens per Sec:     1637 || Lr: 0.000050
2024-02-08 02:07:39,680 Epoch 7256: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 02:07:39,680 EPOCH 7257
2024-02-08 02:07:56,077 Epoch 7257: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 02:07:56,078 EPOCH 7258
2024-02-08 02:08:12,024 Epoch 7258: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-08 02:08:12,024 EPOCH 7259
2024-02-08 02:08:28,351 Epoch 7259: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-08 02:08:28,351 EPOCH 7260
2024-02-08 02:08:44,527 Epoch 7260: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-08 02:08:44,529 EPOCH 7261
2024-02-08 02:09:00,743 Epoch 7261: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-08 02:09:00,744 EPOCH 7262
2024-02-08 02:09:16,906 Epoch 7262: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-08 02:09:16,907 EPOCH 7263
2024-02-08 02:09:32,796 Epoch 7263: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-08 02:09:32,796 EPOCH 7264
2024-02-08 02:09:48,867 Epoch 7264: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-08 02:09:48,868 EPOCH 7265
2024-02-08 02:10:05,089 Epoch 7265: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-08 02:10:05,090 EPOCH 7266
2024-02-08 02:10:20,975 Epoch 7266: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-08 02:10:20,975 EPOCH 7267
2024-02-08 02:10:27,775 [Epoch: 7267 Step: 00065400] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:      997 || Batch Translation Loss:   0.014098 => Txt Tokens per Sec:     2584 || Lr: 0.000050
2024-02-08 02:10:37,322 Epoch 7267: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-08 02:10:37,323 EPOCH 7268
2024-02-08 02:10:53,526 Epoch 7268: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-08 02:10:53,527 EPOCH 7269
2024-02-08 02:11:09,742 Epoch 7269: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-08 02:11:09,743 EPOCH 7270
2024-02-08 02:11:25,779 Epoch 7270: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-08 02:11:25,779 EPOCH 7271
2024-02-08 02:11:42,214 Epoch 7271: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-08 02:11:42,214 EPOCH 7272
2024-02-08 02:11:58,579 Epoch 7272: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 02:11:58,579 EPOCH 7273
2024-02-08 02:12:14,729 Epoch 7273: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-08 02:12:14,729 EPOCH 7274
2024-02-08 02:12:30,619 Epoch 7274: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 02:12:30,621 EPOCH 7275
2024-02-08 02:12:47,195 Epoch 7275: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-08 02:12:47,195 EPOCH 7276
2024-02-08 02:13:03,793 Epoch 7276: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-08 02:13:03,794 EPOCH 7277
2024-02-08 02:13:19,810 Epoch 7277: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 02:13:19,810 EPOCH 7278
2024-02-08 02:13:34,951 [Epoch: 7278 Step: 00065500] Batch Recognition Loss:   0.000373 => Gls Tokens per Sec:      532 || Batch Translation Loss:   0.008071 => Txt Tokens per Sec:     1493 || Lr: 0.000050
2024-02-08 02:13:35,949 Epoch 7278: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 02:13:35,950 EPOCH 7279
2024-02-08 02:13:52,273 Epoch 7279: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 02:13:52,274 EPOCH 7280
2024-02-08 02:14:08,326 Epoch 7280: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 02:14:08,327 EPOCH 7281
2024-02-08 02:14:24,432 Epoch 7281: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 02:14:24,433 EPOCH 7282
2024-02-08 02:14:40,761 Epoch 7282: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 02:14:40,762 EPOCH 7283
2024-02-08 02:14:56,897 Epoch 7283: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 02:14:56,898 EPOCH 7284
2024-02-08 02:15:13,370 Epoch 7284: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 02:15:13,371 EPOCH 7285
2024-02-08 02:15:29,393 Epoch 7285: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:15:29,393 EPOCH 7286
2024-02-08 02:15:45,461 Epoch 7286: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:15:45,462 EPOCH 7287
2024-02-08 02:16:01,256 Epoch 7287: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:16:01,257 EPOCH 7288
2024-02-08 02:16:17,632 Epoch 7288: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 02:16:17,632 EPOCH 7289
2024-02-08 02:16:30,494 [Epoch: 7289 Step: 00065600] Batch Recognition Loss:   0.000534 => Gls Tokens per Sec:      726 || Batch Translation Loss:   0.013132 => Txt Tokens per Sec:     1969 || Lr: 0.000050
2024-02-08 02:16:33,865 Epoch 7289: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:16:33,865 EPOCH 7290
2024-02-08 02:16:50,074 Epoch 7290: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:16:50,074 EPOCH 7291
2024-02-08 02:17:06,326 Epoch 7291: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:17:06,327 EPOCH 7292
2024-02-08 02:17:22,944 Epoch 7292: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:17:22,944 EPOCH 7293
2024-02-08 02:17:39,489 Epoch 7293: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 02:17:39,489 EPOCH 7294
2024-02-08 02:17:55,459 Epoch 7294: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:17:55,459 EPOCH 7295
2024-02-08 02:18:11,499 Epoch 7295: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 02:18:11,500 EPOCH 7296
2024-02-08 02:18:27,658 Epoch 7296: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 02:18:27,658 EPOCH 7297
2024-02-08 02:18:43,646 Epoch 7297: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 02:18:43,648 EPOCH 7298
2024-02-08 02:19:00,088 Epoch 7298: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 02:19:00,088 EPOCH 7299
2024-02-08 02:19:16,289 Epoch 7299: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 02:19:16,290 EPOCH 7300
2024-02-08 02:19:32,647 [Epoch: 7300 Step: 00065700] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:      649 || Batch Translation Loss:   0.009957 => Txt Tokens per Sec:     1796 || Lr: 0.000050
2024-02-08 02:19:32,648 Epoch 7300: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:19:32,648 EPOCH 7301
2024-02-08 02:19:49,012 Epoch 7301: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 02:19:49,013 EPOCH 7302
2024-02-08 02:20:05,574 Epoch 7302: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:20:05,574 EPOCH 7303
2024-02-08 02:20:21,773 Epoch 7303: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 02:20:21,773 EPOCH 7304
2024-02-08 02:20:37,907 Epoch 7304: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 02:20:37,908 EPOCH 7305
2024-02-08 02:20:54,307 Epoch 7305: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 02:20:54,308 EPOCH 7306
2024-02-08 02:21:10,600 Epoch 7306: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 02:21:10,600 EPOCH 7307
2024-02-08 02:21:26,641 Epoch 7307: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 02:21:26,642 EPOCH 7308
2024-02-08 02:21:42,834 Epoch 7308: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-08 02:21:42,835 EPOCH 7309
2024-02-08 02:21:58,864 Epoch 7309: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 02:21:58,864 EPOCH 7310
2024-02-08 02:22:15,729 Epoch 7310: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-08 02:22:15,731 EPOCH 7311
2024-02-08 02:22:32,045 Epoch 7311: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-08 02:22:32,046 EPOCH 7312
2024-02-08 02:22:32,535 [Epoch: 7312 Step: 00065800] Batch Recognition Loss:   0.000263 => Gls Tokens per Sec:     2634 || Batch Translation Loss:   0.096678 => Txt Tokens per Sec:     7584 || Lr: 0.000050
2024-02-08 02:22:48,142 Epoch 7312: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-08 02:22:48,142 EPOCH 7313
2024-02-08 02:23:04,296 Epoch 7313: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-08 02:23:04,296 EPOCH 7314
2024-02-08 02:23:20,410 Epoch 7314: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-08 02:23:20,411 EPOCH 7315
2024-02-08 02:23:36,515 Epoch 7315: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-08 02:23:36,516 EPOCH 7316
2024-02-08 02:23:53,077 Epoch 7316: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-08 02:23:53,079 EPOCH 7317
2024-02-08 02:24:09,023 Epoch 7317: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-08 02:24:09,024 EPOCH 7318
2024-02-08 02:24:25,814 Epoch 7318: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-08 02:24:25,815 EPOCH 7319
2024-02-08 02:24:41,870 Epoch 7319: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-08 02:24:41,871 EPOCH 7320
2024-02-08 02:24:57,773 Epoch 7320: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-08 02:24:57,774 EPOCH 7321
2024-02-08 02:25:14,188 Epoch 7321: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-08 02:25:14,189 EPOCH 7322
2024-02-08 02:25:30,338 Epoch 7322: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 02:25:30,339 EPOCH 7323
2024-02-08 02:25:36,427 [Epoch: 7323 Step: 00065900] Batch Recognition Loss:   0.000436 => Gls Tokens per Sec:      421 || Batch Translation Loss:   0.019100 => Txt Tokens per Sec:     1312 || Lr: 0.000050
2024-02-08 02:25:46,354 Epoch 7323: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 02:25:46,355 EPOCH 7324
2024-02-08 02:26:02,064 Epoch 7324: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 02:26:02,065 EPOCH 7325
2024-02-08 02:26:18,421 Epoch 7325: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 02:26:18,422 EPOCH 7326
2024-02-08 02:26:34,785 Epoch 7326: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 02:26:34,786 EPOCH 7327
2024-02-08 02:26:51,084 Epoch 7327: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 02:26:51,084 EPOCH 7328
2024-02-08 02:27:07,165 Epoch 7328: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 02:27:07,166 EPOCH 7329
2024-02-08 02:27:23,425 Epoch 7329: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 02:27:23,426 EPOCH 7330
2024-02-08 02:27:39,571 Epoch 7330: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 02:27:39,571 EPOCH 7331
2024-02-08 02:27:55,645 Epoch 7331: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 02:27:55,647 EPOCH 7332
2024-02-08 02:28:11,982 Epoch 7332: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 02:28:11,982 EPOCH 7333
2024-02-08 02:28:28,126 Epoch 7333: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 02:28:28,126 EPOCH 7334
2024-02-08 02:28:32,212 [Epoch: 7334 Step: 00066000] Batch Recognition Loss:   0.000286 => Gls Tokens per Sec:      940 || Batch Translation Loss:   0.006849 => Txt Tokens per Sec:     2522 || Lr: 0.000050
2024-02-08 02:29:40,511 Validation result at epoch 7334, step    66000: duration: 68.2966s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.25011	Translation Loss: 112036.29688	PPL: 72417.32031
	Eval Metric: BLEU
	WER 2.33	(DEL: 0.00,	INS: 0.00,	SUB: 2.33)
	BLEU-4 0.40	(BLEU-1: 9.68,	BLEU-2: 2.69,	BLEU-3: 0.86,	BLEU-4: 0.40)
	CHRF 16.53	ROUGE 8.18
2024-02-08 02:29:40,513 Logging Recognition and Translation Outputs
2024-02-08 02:29:40,513 ========================================================================================================================
2024-02-08 02:29:40,513 Logging Sequence: 85_97.00
2024-02-08 02:29:40,514 	Gloss Reference :	A B+C+D+E
2024-02-08 02:29:40,514 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 02:29:40,514 	Gloss Alignment :	         
2024-02-08 02:29:40,514 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 02:29:40,517 	Text Reference  :	** *** like india's bcci australia has *** ******* *** *** ********* *** ** cricket australia
2024-02-08 02:29:40,517 	Text Hypothesis :	it was very sad     by   he        has now retired for his centuries and he has     scored   
2024-02-08 02:29:40,518 	Text Alignment  :	I  I   S    S       S    S             I   I       I   I   I         I   I  S       S        
2024-02-08 02:29:40,518 ========================================================================================================================
2024-02-08 02:29:40,518 Logging Sequence: 53_161.00
2024-02-08 02:29:40,518 	Gloss Reference :	A B+C+D+E
2024-02-08 02:29:40,518 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 02:29:40,518 	Gloss Alignment :	         
2024-02-08 02:29:40,519 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 02:29:40,520 	Text Reference  :	rashid has also been urging people to   donate to  his      rashid khan  foundation and afghanistan cricket association
2024-02-08 02:29:40,520 	Text Hypothesis :	****** *** **** well let    me     tell you    the olympics in     tokyo olympics   in  currently   his     2021       
2024-02-08 02:29:40,520 	Text Alignment  :	D      D   D    S    S      S      S    S      S   S        S      S     S          S   S           S       S          
2024-02-08 02:29:40,520 ========================================================================================================================
2024-02-08 02:29:40,521 Logging Sequence: 101_97.00
2024-02-08 02:29:40,521 	Gloss Reference :	A B+C+D+E
2024-02-08 02:29:40,521 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 02:29:40,521 	Gloss Alignment :	         
2024-02-08 02:29:40,521 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 02:29:40,524 	Text Reference  :	2 of the best indian bowlers who bowled very well    were    raj bawa who took    5  wickets and ******* ravi kumar who   took 4       wickets
2024-02-08 02:29:40,524 	Text Hypothesis :	* ** *** **** ****** ******* *** ****** in   india's amazing to  see  the auction in england and england lost the   score of   india's 0      
2024-02-08 02:29:40,524 	Text Alignment  :	D D  D   D    D      D       D   D      S    S       S       S   S    S   S       S  S           I       S    S     S     S    S       S      
2024-02-08 02:29:40,524 ========================================================================================================================
2024-02-08 02:29:40,524 Logging Sequence: 118_130.00
2024-02-08 02:29:40,525 	Gloss Reference :	A B+C+D+E
2024-02-08 02:29:40,525 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 02:29:40,525 	Gloss Alignment :	         
2024-02-08 02:29:40,525 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 02:29:40,526 	Text Reference  :	**** messi's fans   the     entire team       were      in  tears everyone was   overwhelmed by the victory
2024-02-08 02:29:40,526 	Text Hypothesis :	many former  indian captain dilip  vengsarkar expressed her 2023  to       their way         to 4   teams  
2024-02-08 02:29:40,526 	Text Alignment  :	I    S       S      S       S      S          S         S   S     S        S     S           S  S   S      
2024-02-08 02:29:40,527 ========================================================================================================================
2024-02-08 02:29:40,527 Logging Sequence: 170_195.00
2024-02-08 02:29:40,527 	Gloss Reference :	A B+C+D+E
2024-02-08 02:29:40,527 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 02:29:40,527 	Gloss Alignment :	         
2024-02-08 02:29:40,527 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 02:29:40,528 	Text Reference  :	********* ** i    moved    to   rajasthan royals team  as      they    paid me   8       crores
2024-02-08 02:29:40,528 	Text Hypothesis :	yesterday on 23rd november 2022 there     was    match between germany of   14th october 2023  
2024-02-08 02:29:40,529 	Text Alignment  :	I         I  S    S        S    S         S      S     S       S       S    S    S       S     
2024-02-08 02:29:40,529 ========================================================================================================================
2024-02-08 02:29:53,175 Epoch 7334: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:29:53,176 EPOCH 7335
2024-02-08 02:30:09,537 Epoch 7335: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:30:09,538 EPOCH 7336
2024-02-08 02:30:25,781 Epoch 7336: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:30:25,782 EPOCH 7337
2024-02-08 02:30:41,921 Epoch 7337: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:30:41,921 EPOCH 7338
2024-02-08 02:30:58,080 Epoch 7338: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 02:30:58,081 EPOCH 7339
2024-02-08 02:31:14,281 Epoch 7339: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:31:14,282 EPOCH 7340
2024-02-08 02:31:30,333 Epoch 7340: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 02:31:30,333 EPOCH 7341
2024-02-08 02:31:46,150 Epoch 7341: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:31:46,151 EPOCH 7342
2024-02-08 02:32:02,546 Epoch 7342: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:32:02,546 EPOCH 7343
2024-02-08 02:32:18,812 Epoch 7343: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 02:32:18,812 EPOCH 7344
2024-02-08 02:32:35,078 Epoch 7344: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 02:32:35,078 EPOCH 7345
2024-02-08 02:32:42,031 [Epoch: 7345 Step: 00066100] Batch Recognition Loss:   0.000087 => Gls Tokens per Sec:      737 || Batch Translation Loss:   0.009501 => Txt Tokens per Sec:     2044 || Lr: 0.000050
2024-02-08 02:32:51,137 Epoch 7345: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 02:32:51,137 EPOCH 7346
2024-02-08 02:33:07,096 Epoch 7346: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 02:33:07,096 EPOCH 7347
2024-02-08 02:33:23,594 Epoch 7347: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 02:33:23,595 EPOCH 7348
2024-02-08 02:33:39,705 Epoch 7348: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-08 02:33:39,705 EPOCH 7349
2024-02-08 02:33:55,801 Epoch 7349: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-08 02:33:55,802 EPOCH 7350
2024-02-08 02:34:11,735 Epoch 7350: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-08 02:34:11,735 EPOCH 7351
2024-02-08 02:34:27,928 Epoch 7351: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-08 02:34:27,929 EPOCH 7352
2024-02-08 02:34:44,043 Epoch 7352: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-08 02:34:44,044 EPOCH 7353
2024-02-08 02:35:00,098 Epoch 7353: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 02:35:00,098 EPOCH 7354
2024-02-08 02:35:16,242 Epoch 7354: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 02:35:16,243 EPOCH 7355
2024-02-08 02:35:32,309 Epoch 7355: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 02:35:32,309 EPOCH 7356
2024-02-08 02:35:36,665 [Epoch: 7356 Step: 00066200] Batch Recognition Loss:   0.000475 => Gls Tokens per Sec:     1470 || Batch Translation Loss:   0.012984 => Txt Tokens per Sec:     3681 || Lr: 0.000050
2024-02-08 02:35:48,041 Epoch 7356: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:35:48,041 EPOCH 7357
2024-02-08 02:36:04,464 Epoch 7357: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 02:36:04,465 EPOCH 7358
2024-02-08 02:36:20,704 Epoch 7358: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 02:36:20,705 EPOCH 7359
2024-02-08 02:36:38,232 Epoch 7359: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 02:36:38,232 EPOCH 7360
2024-02-08 02:36:54,635 Epoch 7360: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 02:36:54,637 EPOCH 7361
2024-02-08 02:37:11,493 Epoch 7361: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 02:37:11,494 EPOCH 7362
2024-02-08 02:37:27,887 Epoch 7362: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 02:37:27,889 EPOCH 7363
2024-02-08 02:37:44,223 Epoch 7363: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-08 02:37:44,224 EPOCH 7364
2024-02-08 02:38:00,278 Epoch 7364: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 02:38:00,278 EPOCH 7365
2024-02-08 02:38:16,110 Epoch 7365: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 02:38:16,111 EPOCH 7366
2024-02-08 02:38:32,171 Epoch 7366: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 02:38:32,171 EPOCH 7367
2024-02-08 02:38:44,258 [Epoch: 7367 Step: 00066300] Batch Recognition Loss:   0.001704 => Gls Tokens per Sec:      561 || Batch Translation Loss:   0.012579 => Txt Tokens per Sec:     1533 || Lr: 0.000050
2024-02-08 02:38:48,437 Epoch 7367: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 02:38:48,437 EPOCH 7368
2024-02-08 02:39:04,471 Epoch 7368: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 02:39:04,472 EPOCH 7369
2024-02-08 02:39:20,411 Epoch 7369: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 02:39:20,412 EPOCH 7370
2024-02-08 02:39:36,746 Epoch 7370: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 02:39:36,746 EPOCH 7371
2024-02-08 02:39:52,812 Epoch 7371: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 02:39:52,813 EPOCH 7372
2024-02-08 02:40:08,927 Epoch 7372: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 02:40:08,929 EPOCH 7373
2024-02-08 02:40:25,550 Epoch 7373: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 02:40:25,551 EPOCH 7374
2024-02-08 02:40:41,595 Epoch 7374: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-08 02:40:41,596 EPOCH 7375
2024-02-08 02:40:57,968 Epoch 7375: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 02:40:57,969 EPOCH 7376
2024-02-08 02:41:14,132 Epoch 7376: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 02:41:14,134 EPOCH 7377
2024-02-08 02:41:30,178 Epoch 7377: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 02:41:30,178 EPOCH 7378
2024-02-08 02:41:45,605 [Epoch: 7378 Step: 00066400] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:      523 || Batch Translation Loss:   0.011534 => Txt Tokens per Sec:     1452 || Lr: 0.000050
2024-02-08 02:41:46,559 Epoch 7378: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:41:46,559 EPOCH 7379
2024-02-08 02:42:02,508 Epoch 7379: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:42:02,509 EPOCH 7380
2024-02-08 02:42:18,577 Epoch 7380: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 02:42:18,578 EPOCH 7381
2024-02-08 02:42:34,639 Epoch 7381: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:42:34,640 EPOCH 7382
2024-02-08 02:42:50,870 Epoch 7382: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 02:42:50,871 EPOCH 7383
2024-02-08 02:43:06,782 Epoch 7383: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 02:43:06,783 EPOCH 7384
2024-02-08 02:43:22,942 Epoch 7384: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:43:22,943 EPOCH 7385
2024-02-08 02:43:39,085 Epoch 7385: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:43:39,085 EPOCH 7386
2024-02-08 02:43:55,368 Epoch 7386: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 02:43:55,369 EPOCH 7387
2024-02-08 02:44:11,730 Epoch 7387: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 02:44:11,731 EPOCH 7388
2024-02-08 02:44:28,137 Epoch 7388: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 02:44:28,137 EPOCH 7389
2024-02-08 02:44:43,903 [Epoch: 7389 Step: 00066500] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:      592 || Batch Translation Loss:   0.023067 => Txt Tokens per Sec:     1655 || Lr: 0.000050
2024-02-08 02:44:44,305 Epoch 7389: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:44:44,306 EPOCH 7390
2024-02-08 02:45:00,847 Epoch 7390: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:45:00,849 EPOCH 7391
2024-02-08 02:45:17,199 Epoch 7391: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:45:17,201 EPOCH 7392
2024-02-08 02:45:33,419 Epoch 7392: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:45:33,419 EPOCH 7393
2024-02-08 02:45:49,743 Epoch 7393: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 02:45:49,743 EPOCH 7394
2024-02-08 02:46:05,595 Epoch 7394: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 02:46:05,596 EPOCH 7395
2024-02-08 02:46:21,681 Epoch 7395: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 02:46:21,681 EPOCH 7396
2024-02-08 02:46:38,174 Epoch 7396: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:46:38,174 EPOCH 7397
2024-02-08 02:46:54,254 Epoch 7397: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-08 02:46:54,255 EPOCH 7398
2024-02-08 02:47:10,585 Epoch 7398: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 02:47:10,585 EPOCH 7399
2024-02-08 02:47:27,049 Epoch 7399: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 02:47:27,051 EPOCH 7400
2024-02-08 02:47:43,012 [Epoch: 7400 Step: 00066600] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:      665 || Batch Translation Loss:   0.006330 => Txt Tokens per Sec:     1841 || Lr: 0.000050
2024-02-08 02:47:43,012 Epoch 7400: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 02:47:43,013 EPOCH 7401
2024-02-08 02:47:59,012 Epoch 7401: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 02:47:59,012 EPOCH 7402
2024-02-08 02:48:14,982 Epoch 7402: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 02:48:14,984 EPOCH 7403
2024-02-08 02:48:31,012 Epoch 7403: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:48:31,013 EPOCH 7404
2024-02-08 02:48:47,381 Epoch 7404: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:48:47,383 EPOCH 7405
2024-02-08 02:49:04,283 Epoch 7405: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:49:04,285 EPOCH 7406
2024-02-08 02:49:20,559 Epoch 7406: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 02:49:20,560 EPOCH 7407
2024-02-08 02:49:36,991 Epoch 7407: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:49:36,991 EPOCH 7408
2024-02-08 02:49:53,123 Epoch 7408: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:49:53,123 EPOCH 7409
2024-02-08 02:50:09,554 Epoch 7409: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 02:50:09,555 EPOCH 7410
2024-02-08 02:50:25,582 Epoch 7410: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 02:50:25,583 EPOCH 7411
2024-02-08 02:50:42,104 Epoch 7411: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-08 02:50:42,105 EPOCH 7412
2024-02-08 02:50:45,387 [Epoch: 7412 Step: 00066700] Batch Recognition Loss:   0.000350 => Gls Tokens per Sec:      390 || Batch Translation Loss:   0.018583 => Txt Tokens per Sec:     1249 || Lr: 0.000050
2024-02-08 02:50:58,440 Epoch 7412: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-08 02:50:58,441 EPOCH 7413
2024-02-08 02:51:14,561 Epoch 7413: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-08 02:51:14,563 EPOCH 7414
2024-02-08 02:51:30,754 Epoch 7414: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-08 02:51:30,755 EPOCH 7415
2024-02-08 02:51:46,885 Epoch 7415: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-08 02:51:46,887 EPOCH 7416
2024-02-08 02:52:03,294 Epoch 7416: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-08 02:52:03,296 EPOCH 7417
2024-02-08 02:52:19,760 Epoch 7417: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-08 02:52:19,762 EPOCH 7418
2024-02-08 02:52:35,942 Epoch 7418: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.53 
2024-02-08 02:52:35,943 EPOCH 7419
2024-02-08 02:52:52,032 Epoch 7419: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.90 
2024-02-08 02:52:52,032 EPOCH 7420
2024-02-08 02:53:08,176 Epoch 7420: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.60 
2024-02-08 02:53:08,177 EPOCH 7421
2024-02-08 02:53:24,729 Epoch 7421: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.06 
2024-02-08 02:53:24,730 EPOCH 7422
2024-02-08 02:53:40,718 Epoch 7422: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-08 02:53:40,718 EPOCH 7423
2024-02-08 02:53:41,383 [Epoch: 7423 Step: 00066800] Batch Recognition Loss:   0.000668 => Gls Tokens per Sec:     3861 || Batch Translation Loss:   0.045996 => Txt Tokens per Sec:     9544 || Lr: 0.000050
2024-02-08 02:53:56,995 Epoch 7423: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-08 02:53:56,996 EPOCH 7424
2024-02-08 02:54:12,678 Epoch 7424: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-08 02:54:12,678 EPOCH 7425
2024-02-08 02:54:28,828 Epoch 7425: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-08 02:54:28,829 EPOCH 7426
2024-02-08 02:54:45,083 Epoch 7426: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-08 02:54:45,084 EPOCH 7427
2024-02-08 02:55:01,145 Epoch 7427: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-08 02:55:01,147 EPOCH 7428
2024-02-08 02:55:17,242 Epoch 7428: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-08 02:55:17,243 EPOCH 7429
2024-02-08 02:55:33,687 Epoch 7429: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 02:55:33,688 EPOCH 7430
2024-02-08 02:55:49,950 Epoch 7430: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 02:55:49,951 EPOCH 7431
2024-02-08 02:56:06,127 Epoch 7431: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 02:56:06,127 EPOCH 7432
2024-02-08 02:56:22,237 Epoch 7432: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-08 02:56:22,237 EPOCH 7433
2024-02-08 02:56:38,382 Epoch 7433: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 02:56:38,384 EPOCH 7434
2024-02-08 02:56:43,258 [Epoch: 7434 Step: 00066900] Batch Recognition Loss:   0.000491 => Gls Tokens per Sec:      603 || Batch Translation Loss:   0.006337 => Txt Tokens per Sec:     1340 || Lr: 0.000050
2024-02-08 02:56:54,581 Epoch 7434: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-08 02:56:54,582 EPOCH 7435
2024-02-08 02:57:10,610 Epoch 7435: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-08 02:57:10,611 EPOCH 7436
2024-02-08 02:57:26,977 Epoch 7436: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 02:57:26,978 EPOCH 7437
2024-02-08 02:57:43,174 Epoch 7437: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 02:57:43,174 EPOCH 7438
2024-02-08 02:57:59,374 Epoch 7438: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 02:57:59,375 EPOCH 7439
2024-02-08 02:58:15,471 Epoch 7439: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 02:58:15,472 EPOCH 7440
2024-02-08 02:58:31,360 Epoch 7440: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 02:58:31,362 EPOCH 7441
2024-02-08 02:58:47,728 Epoch 7441: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:58:47,729 EPOCH 7442
2024-02-08 02:59:03,963 Epoch 7442: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:59:03,964 EPOCH 7443
2024-02-08 02:59:19,820 Epoch 7443: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 02:59:19,820 EPOCH 7444
2024-02-08 02:59:35,948 Epoch 7444: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:59:35,949 EPOCH 7445
2024-02-08 02:59:50,130 [Epoch: 7445 Step: 00067000] Batch Recognition Loss:   0.000562 => Gls Tokens per Sec:      298 || Batch Translation Loss:   0.006570 => Txt Tokens per Sec:      966 || Lr: 0.000050
2024-02-08 02:59:52,313 Epoch 7445: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 02:59:52,313 EPOCH 7446
2024-02-08 03:00:08,433 Epoch 7446: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:00:08,433 EPOCH 7447
2024-02-08 03:00:24,342 Epoch 7447: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:00:24,343 EPOCH 7448
2024-02-08 03:00:40,438 Epoch 7448: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:00:40,438 EPOCH 7449
2024-02-08 03:00:56,544 Epoch 7449: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:00:56,545 EPOCH 7450
2024-02-08 03:01:12,997 Epoch 7450: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:01:12,997 EPOCH 7451
2024-02-08 03:01:28,987 Epoch 7451: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:01:28,988 EPOCH 7452
2024-02-08 03:01:45,266 Epoch 7452: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:01:45,267 EPOCH 7453
2024-02-08 03:02:01,980 Epoch 7453: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:02:01,982 EPOCH 7454
2024-02-08 03:02:18,498 Epoch 7454: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:02:18,498 EPOCH 7455
2024-02-08 03:02:34,784 Epoch 7455: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:02:34,785 EPOCH 7456
2024-02-08 03:02:44,069 [Epoch: 7456 Step: 00067100] Batch Recognition Loss:   0.000294 => Gls Tokens per Sec:      592 || Batch Translation Loss:   0.005239 => Txt Tokens per Sec:     1734 || Lr: 0.000050
2024-02-08 03:02:51,318 Epoch 7456: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:02:51,318 EPOCH 7457
2024-02-08 03:03:07,452 Epoch 7457: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:03:07,453 EPOCH 7458
2024-02-08 03:03:23,716 Epoch 7458: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:03:23,717 EPOCH 7459
2024-02-08 03:03:39,757 Epoch 7459: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:03:39,758 EPOCH 7460
2024-02-08 03:03:56,209 Epoch 7460: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 03:03:56,210 EPOCH 7461
2024-02-08 03:04:12,160 Epoch 7461: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-08 03:04:12,160 EPOCH 7462
2024-02-08 03:04:28,759 Epoch 7462: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-08 03:04:28,761 EPOCH 7463
2024-02-08 03:04:44,801 Epoch 7463: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-08 03:04:44,802 EPOCH 7464
2024-02-08 03:05:00,940 Epoch 7464: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 03:05:00,941 EPOCH 7465
2024-02-08 03:05:17,553 Epoch 7465: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:05:17,555 EPOCH 7466
2024-02-08 03:05:33,671 Epoch 7466: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:05:33,672 EPOCH 7467
2024-02-08 03:05:44,453 [Epoch: 7467 Step: 00067200] Batch Recognition Loss:   0.000429 => Gls Tokens per Sec:      712 || Batch Translation Loss:   0.011402 => Txt Tokens per Sec:     2060 || Lr: 0.000050
2024-02-08 03:05:49,636 Epoch 7467: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 03:05:49,636 EPOCH 7468
2024-02-08 03:06:05,680 Epoch 7468: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:06:05,681 EPOCH 7469
2024-02-08 03:06:22,060 Epoch 7469: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 03:06:22,060 EPOCH 7470
2024-02-08 03:06:38,010 Epoch 7470: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:06:38,011 EPOCH 7471
2024-02-08 03:06:54,180 Epoch 7471: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:06:54,180 EPOCH 7472
2024-02-08 03:07:10,296 Epoch 7472: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:07:10,298 EPOCH 7473
2024-02-08 03:07:25,590 Epoch 7473: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:07:25,591 EPOCH 7474
2024-02-08 03:07:42,080 Epoch 7474: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:07:42,081 EPOCH 7475
2024-02-08 03:07:58,518 Epoch 7475: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:07:58,518 EPOCH 7476
2024-02-08 03:08:14,642 Epoch 7476: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:08:14,643 EPOCH 7477
2024-02-08 03:08:30,632 Epoch 7477: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:08:30,633 EPOCH 7478
2024-02-08 03:08:40,452 [Epoch: 7478 Step: 00067300] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:      821 || Batch Translation Loss:   0.010398 => Txt Tokens per Sec:     2212 || Lr: 0.000050
2024-02-08 03:08:46,588 Epoch 7478: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 03:08:46,588 EPOCH 7479
2024-02-08 03:09:02,808 Epoch 7479: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-08 03:09:02,808 EPOCH 7480
2024-02-08 03:09:19,255 Epoch 7480: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-08 03:09:19,256 EPOCH 7481
2024-02-08 03:09:35,426 Epoch 7481: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 03:09:35,427 EPOCH 7482
2024-02-08 03:09:51,892 Epoch 7482: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 03:09:51,892 EPOCH 7483
2024-02-08 03:10:08,112 Epoch 7483: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 03:10:08,113 EPOCH 7484
2024-02-08 03:10:24,327 Epoch 7484: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 03:10:24,328 EPOCH 7485
2024-02-08 03:10:40,504 Epoch 7485: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:10:40,504 EPOCH 7486
2024-02-08 03:10:56,982 Epoch 7486: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:10:56,982 EPOCH 7487
2024-02-08 03:11:13,273 Epoch 7487: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:11:13,273 EPOCH 7488
2024-02-08 03:11:29,260 Epoch 7488: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:11:29,261 EPOCH 7489
2024-02-08 03:11:45,341 [Epoch: 7489 Step: 00067400] Batch Recognition Loss:   0.000207 => Gls Tokens per Sec:      581 || Batch Translation Loss:   0.015772 => Txt Tokens per Sec:     1624 || Lr: 0.000050
2024-02-08 03:11:45,739 Epoch 7489: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:11:45,739 EPOCH 7490
2024-02-08 03:12:01,748 Epoch 7490: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:12:01,749 EPOCH 7491
2024-02-08 03:12:17,995 Epoch 7491: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:12:17,996 EPOCH 7492
2024-02-08 03:12:34,275 Epoch 7492: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:12:34,275 EPOCH 7493
2024-02-08 03:12:50,254 Epoch 7493: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:12:50,255 EPOCH 7494
2024-02-08 03:13:06,538 Epoch 7494: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:13:06,538 EPOCH 7495
2024-02-08 03:13:23,233 Epoch 7495: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:13:23,234 EPOCH 7496
2024-02-08 03:13:39,405 Epoch 7496: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 03:13:39,406 EPOCH 7497
2024-02-08 03:13:55,105 Epoch 7497: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:13:55,106 EPOCH 7498
2024-02-08 03:14:11,637 Epoch 7498: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:14:11,638 EPOCH 7499
2024-02-08 03:14:28,151 Epoch 7499: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:14:28,152 EPOCH 7500
2024-02-08 03:14:44,274 [Epoch: 7500 Step: 00067500] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:      659 || Batch Translation Loss:   0.007528 => Txt Tokens per Sec:     1823 || Lr: 0.000050
2024-02-08 03:14:44,274 Epoch 7500: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:14:44,274 EPOCH 7501
2024-02-08 03:15:00,519 Epoch 7501: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:15:00,521 EPOCH 7502
2024-02-08 03:15:16,845 Epoch 7502: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:15:16,846 EPOCH 7503
2024-02-08 03:15:33,438 Epoch 7503: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 03:15:33,439 EPOCH 7504
2024-02-08 03:15:49,620 Epoch 7504: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:15:49,620 EPOCH 7505
2024-02-08 03:16:05,669 Epoch 7505: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 03:16:05,669 EPOCH 7506
2024-02-08 03:16:21,804 Epoch 7506: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 03:16:21,806 EPOCH 7507
2024-02-08 03:16:38,403 Epoch 7507: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:16:38,404 EPOCH 7508
2024-02-08 03:16:54,650 Epoch 7508: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:16:54,651 EPOCH 7509
2024-02-08 03:17:11,072 Epoch 7509: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:17:11,074 EPOCH 7510
2024-02-08 03:17:27,483 Epoch 7510: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:17:27,485 EPOCH 7511
2024-02-08 03:17:44,104 Epoch 7511: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 03:17:44,105 EPOCH 7512
2024-02-08 03:17:47,391 [Epoch: 7512 Step: 00067600] Batch Recognition Loss:   0.000299 => Gls Tokens per Sec:      390 || Batch Translation Loss:   0.013078 => Txt Tokens per Sec:     1248 || Lr: 0.000050
2024-02-08 03:18:00,341 Epoch 7512: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:18:00,343 EPOCH 7513
2024-02-08 03:18:17,017 Epoch 7513: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:18:17,017 EPOCH 7514
2024-02-08 03:18:33,025 Epoch 7514: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:18:33,025 EPOCH 7515
2024-02-08 03:18:49,200 Epoch 7515: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:18:49,202 EPOCH 7516
2024-02-08 03:19:05,678 Epoch 7516: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:19:05,679 EPOCH 7517
2024-02-08 03:19:22,211 Epoch 7517: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:19:22,212 EPOCH 7518
2024-02-08 03:19:38,208 Epoch 7518: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:19:38,209 EPOCH 7519
2024-02-08 03:19:53,890 Epoch 7519: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:19:53,891 EPOCH 7520
2024-02-08 03:20:10,120 Epoch 7520: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:20:10,122 EPOCH 7521
2024-02-08 03:20:26,570 Epoch 7521: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:20:26,571 EPOCH 7522
2024-02-08 03:20:43,005 Epoch 7522: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:20:43,007 EPOCH 7523
2024-02-08 03:20:49,434 [Epoch: 7523 Step: 00067700] Batch Recognition Loss:   0.000378 => Gls Tokens per Sec:      398 || Batch Translation Loss:   0.029278 => Txt Tokens per Sec:     1291 || Lr: 0.000050
2024-02-08 03:20:59,037 Epoch 7523: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 03:20:59,038 EPOCH 7524
2024-02-08 03:21:15,338 Epoch 7524: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 03:21:15,339 EPOCH 7525
2024-02-08 03:21:31,717 Epoch 7525: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.09 
2024-02-08 03:21:31,718 EPOCH 7526
2024-02-08 03:21:47,881 Epoch 7526: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:21:47,882 EPOCH 7527
2024-02-08 03:22:04,545 Epoch 7527: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:22:04,545 EPOCH 7528
2024-02-08 03:22:20,720 Epoch 7528: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:22:20,720 EPOCH 7529
2024-02-08 03:22:36,641 Epoch 7529: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:22:36,643 EPOCH 7530
2024-02-08 03:22:53,027 Epoch 7530: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:22:53,029 EPOCH 7531
2024-02-08 03:23:09,387 Epoch 7531: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:23:09,388 EPOCH 7532
2024-02-08 03:23:25,751 Epoch 7532: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 03:23:25,752 EPOCH 7533
2024-02-08 03:23:42,141 Epoch 7533: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:23:42,142 EPOCH 7534
2024-02-08 03:23:46,279 [Epoch: 7534 Step: 00067800] Batch Recognition Loss:   0.000335 => Gls Tokens per Sec:      929 || Batch Translation Loss:   0.010080 => Txt Tokens per Sec:     2640 || Lr: 0.000050
2024-02-08 03:23:58,816 Epoch 7534: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:23:58,817 EPOCH 7535
2024-02-08 03:24:15,490 Epoch 7535: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:24:15,492 EPOCH 7536
2024-02-08 03:24:31,953 Epoch 7536: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:24:31,954 EPOCH 7537
2024-02-08 03:24:48,260 Epoch 7537: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:24:48,262 EPOCH 7538
2024-02-08 03:25:04,663 Epoch 7538: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 03:25:04,665 EPOCH 7539
2024-02-08 03:25:21,529 Epoch 7539: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 03:25:21,530 EPOCH 7540
2024-02-08 03:25:37,903 Epoch 7540: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 03:25:37,904 EPOCH 7541
2024-02-08 03:25:53,948 Epoch 7541: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 03:25:53,948 EPOCH 7542
2024-02-08 03:26:10,365 Epoch 7542: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 03:26:10,365 EPOCH 7543
2024-02-08 03:26:26,364 Epoch 7543: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 03:26:26,364 EPOCH 7544
2024-02-08 03:26:42,740 Epoch 7544: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:26:42,741 EPOCH 7545
2024-02-08 03:26:46,950 [Epoch: 7545 Step: 00067900] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     1217 || Batch Translation Loss:   0.008098 => Txt Tokens per Sec:     3078 || Lr: 0.000050
2024-02-08 03:26:58,779 Epoch 7545: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 03:26:58,780 EPOCH 7546
2024-02-08 03:27:15,660 Epoch 7546: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:27:15,661 EPOCH 7547
2024-02-08 03:27:32,300 Epoch 7547: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:27:32,301 EPOCH 7548
2024-02-08 03:27:48,368 Epoch 7548: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:27:48,368 EPOCH 7549
2024-02-08 03:28:04,874 Epoch 7549: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:28:04,875 EPOCH 7550
2024-02-08 03:28:21,169 Epoch 7550: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 03:28:21,170 EPOCH 7551
2024-02-08 03:28:37,389 Epoch 7551: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 03:28:37,391 EPOCH 7552
2024-02-08 03:28:53,883 Epoch 7552: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-08 03:28:53,883 EPOCH 7553
2024-02-08 03:29:09,869 Epoch 7553: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 03:29:09,870 EPOCH 7554
2024-02-08 03:29:26,348 Epoch 7554: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 03:29:26,349 EPOCH 7555
2024-02-08 03:29:43,009 Epoch 7555: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 03:29:43,011 EPOCH 7556
2024-02-08 03:29:52,156 [Epoch: 7556 Step: 00068000] Batch Recognition Loss:   0.000110 => Gls Tokens per Sec:      601 || Batch Translation Loss:   0.012349 => Txt Tokens per Sec:     1766 || Lr: 0.000050
2024-02-08 03:31:00,114 Validation result at epoch 7556, step    68000: duration: 67.9580s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.24992	Translation Loss: 111715.64062	PPL: 70134.74219
	Eval Metric: BLEU
	WER 2.33	(DEL: 0.00,	INS: 0.00,	SUB: 2.33)
	BLEU-4 0.38	(BLEU-1: 9.90,	BLEU-2: 2.68,	BLEU-3: 0.90,	BLEU-4: 0.38)
	CHRF 16.17	ROUGE 8.31
2024-02-08 03:31:00,117 Logging Recognition and Translation Outputs
2024-02-08 03:31:00,117 ========================================================================================================================
2024-02-08 03:31:00,117 Logging Sequence: 148_186.00
2024-02-08 03:31:00,117 	Gloss Reference :	A B+C+D+E
2024-02-08 03:31:00,117 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 03:31:00,117 	Gloss Alignment :	         
2024-02-08 03:31:00,118 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 03:31:00,120 	Text Reference  :	*** siraj also    took four wickets in      1     over thus         becoming the record-holder for   most wickets   in an over in  odis   
2024-02-08 03:31:00,120 	Text Hypothesis :	the 2022  captain was  4    times   india's youth team participated in       the ************* world deaf badminton in ** **** 267 innings
2024-02-08 03:31:00,121 	Text Alignment  :	I   S     S       S    S    S       S       S     S    S            S            D             S     S    S            D  D    S   S      
2024-02-08 03:31:00,121 ========================================================================================================================
2024-02-08 03:31:00,121 Logging Sequence: 61_181.00
2024-02-08 03:31:00,121 	Gloss Reference :	A B+C+D+E
2024-02-08 03:31:00,121 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 03:31:00,121 	Gloss Alignment :	         
2024-02-08 03:31:00,121 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 03:31:00,123 	Text Reference  :	one other fan said it is babar's personal chat     we   should focuc on  this   cricketing career     and not  his personal life 
2024-02-08 03:31:00,123 	Text Hypothesis :	*** ***** *** **** ** ** on      15       february 2021 hotel  by    itc hotels through    bookingcom and when i   would    viral
2024-02-08 03:31:00,124 	Text Alignment  :	D   D     D   D    D  D  S       S        S        S    S      S     S   S      S          S              S    S   S        S    
2024-02-08 03:31:00,124 ========================================================================================================================
2024-02-08 03:31:00,124 Logging Sequence: 123_24.00
2024-02-08 03:31:00,124 	Gloss Reference :	A B+C+D+E
2024-02-08 03:31:00,124 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 03:31:00,124 	Gloss Alignment :	         
2024-02-08 03:31:00,124 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 03:31:00,125 	Text Reference  :	did you know that other than cricket dhoni       has another  passion
2024-02-08 03:31:00,125 	Text Hypothesis :	*** *** **** he   was   a    second  performance by  mohammed siraj  
2024-02-08 03:31:00,125 	Text Alignment  :	D   D   D    S    S     S    S       S           S   S        S      
2024-02-08 03:31:00,126 ========================================================================================================================
2024-02-08 03:31:00,126 Logging Sequence: 84_76.00
2024-02-08 03:31:00,126 	Gloss Reference :	A B+C+D+E
2024-02-08 03:31:00,126 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 03:31:00,126 	Gloss Alignment :	         
2024-02-08 03:31:00,127 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 03:31:00,127 	Text Reference  :	** the teams wanted to support   but were      refused 
2024-02-08 03:31:00,127 	Text Hypothesis :	if the ***** ****** ** situation is  extremely critical
2024-02-08 03:31:00,127 	Text Alignment  :	I      D     D      D  S         S   S         S       
2024-02-08 03:31:00,127 ========================================================================================================================
2024-02-08 03:31:00,128 Logging Sequence: 126_188.00
2024-02-08 03:31:00,128 	Gloss Reference :	A B+C+D+E
2024-02-08 03:31:00,128 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 03:31:00,128 	Gloss Alignment :	         
2024-02-08 03:31:00,128 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 03:31:00,129 	Text Reference  :	now he has  become a gold medalist at   the          2020 tokyo olympics
2024-02-08 03:31:00,129 	Text Hypothesis :	*** he also been   a **** ******** very disappointed by   these games   
2024-02-08 03:31:00,129 	Text Alignment  :	D      S    S        D    D        S    S            S    S     S       
2024-02-08 03:31:00,129 ========================================================================================================================
2024-02-08 03:31:07,584 Epoch 7556: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 03:31:07,584 EPOCH 7557
2024-02-08 03:31:23,891 Epoch 7557: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:31:23,892 EPOCH 7558
2024-02-08 03:31:39,962 Epoch 7558: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:31:39,962 EPOCH 7559
2024-02-08 03:31:56,300 Epoch 7559: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:31:56,301 EPOCH 7560
2024-02-08 03:32:12,692 Epoch 7560: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:32:12,692 EPOCH 7561
2024-02-08 03:32:28,744 Epoch 7561: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:32:28,745 EPOCH 7562
2024-02-08 03:32:44,919 Epoch 7562: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:32:44,920 EPOCH 7563
2024-02-08 03:33:01,116 Epoch 7563: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:33:01,117 EPOCH 7564
2024-02-08 03:33:17,220 Epoch 7564: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:33:17,221 EPOCH 7565
2024-02-08 03:33:33,394 Epoch 7565: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:33:33,395 EPOCH 7566
2024-02-08 03:33:49,628 Epoch 7566: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:33:49,629 EPOCH 7567
2024-02-08 03:33:59,024 [Epoch: 7567 Step: 00068100] Batch Recognition Loss:   0.000428 => Gls Tokens per Sec:      722 || Batch Translation Loss:   0.011468 => Txt Tokens per Sec:     2011 || Lr: 0.000050
2024-02-08 03:34:05,682 Epoch 7567: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:34:05,683 EPOCH 7568
2024-02-08 03:34:21,623 Epoch 7568: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:34:21,624 EPOCH 7569
2024-02-08 03:34:37,683 Epoch 7569: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:34:37,683 EPOCH 7570
2024-02-08 03:34:53,811 Epoch 7570: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:34:53,811 EPOCH 7571
2024-02-08 03:35:10,113 Epoch 7571: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:35:10,114 EPOCH 7572
2024-02-08 03:35:26,319 Epoch 7572: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:35:26,319 EPOCH 7573
2024-02-08 03:35:42,636 Epoch 7573: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:35:42,636 EPOCH 7574
2024-02-08 03:35:58,699 Epoch 7574: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-08 03:35:58,701 EPOCH 7575
2024-02-08 03:36:14,925 Epoch 7575: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:36:14,926 EPOCH 7576
2024-02-08 03:36:30,849 Epoch 7576: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 03:36:30,850 EPOCH 7577
2024-02-08 03:36:46,970 Epoch 7577: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:36:46,971 EPOCH 7578
2024-02-08 03:37:02,352 [Epoch: 7578 Step: 00068200] Batch Recognition Loss:   0.000330 => Gls Tokens per Sec:      524 || Batch Translation Loss:   0.015624 => Txt Tokens per Sec:     1442 || Lr: 0.000050
2024-02-08 03:37:03,354 Epoch 7578: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:37:03,355 EPOCH 7579
2024-02-08 03:37:19,297 Epoch 7579: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-08 03:37:19,297 EPOCH 7580
2024-02-08 03:37:35,581 Epoch 7580: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:37:35,581 EPOCH 7581
2024-02-08 03:37:51,869 Epoch 7581: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:37:51,869 EPOCH 7582
2024-02-08 03:38:08,206 Epoch 7582: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:38:08,207 EPOCH 7583
2024-02-08 03:38:24,242 Epoch 7583: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:38:24,242 EPOCH 7584
2024-02-08 03:38:40,446 Epoch 7584: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:38:40,446 EPOCH 7585
2024-02-08 03:38:56,998 Epoch 7585: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 03:38:56,999 EPOCH 7586
2024-02-08 03:39:12,974 Epoch 7586: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 03:39:12,975 EPOCH 7587
2024-02-08 03:39:29,311 Epoch 7587: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 03:39:29,312 EPOCH 7588
2024-02-08 03:39:45,517 Epoch 7588: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-08 03:39:45,517 EPOCH 7589
2024-02-08 03:40:01,569 [Epoch: 7589 Step: 00068300] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:      582 || Batch Translation Loss:   0.014991 => Txt Tokens per Sec:     1654 || Lr: 0.000050
2024-02-08 03:40:01,877 Epoch 7589: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 03:40:01,877 EPOCH 7590
2024-02-08 03:40:18,101 Epoch 7590: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 03:40:18,101 EPOCH 7591
2024-02-08 03:40:34,264 Epoch 7591: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 03:40:34,265 EPOCH 7592
2024-02-08 03:40:50,366 Epoch 7592: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 03:40:50,366 EPOCH 7593
2024-02-08 03:41:06,652 Epoch 7593: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 03:41:06,653 EPOCH 7594
2024-02-08 03:41:22,541 Epoch 7594: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-08 03:41:22,541 EPOCH 7595
2024-02-08 03:41:38,892 Epoch 7595: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-08 03:41:38,893 EPOCH 7596
2024-02-08 03:41:54,914 Epoch 7596: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-08 03:41:54,915 EPOCH 7597
2024-02-08 03:42:11,136 Epoch 7597: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-08 03:42:11,137 EPOCH 7598
2024-02-08 03:42:27,355 Epoch 7598: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-08 03:42:27,355 EPOCH 7599
2024-02-08 03:42:43,312 Epoch 7599: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-08 03:42:43,312 EPOCH 7600
2024-02-08 03:42:59,863 [Epoch: 7600 Step: 00068400] Batch Recognition Loss:   0.000891 => Gls Tokens per Sec:      642 || Batch Translation Loss:   0.117543 => Txt Tokens per Sec:     1775 || Lr: 0.000050
2024-02-08 03:42:59,863 Epoch 7600: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-08 03:42:59,863 EPOCH 7601
2024-02-08 03:43:15,972 Epoch 7601: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-08 03:43:15,972 EPOCH 7602
2024-02-08 03:43:32,200 Epoch 7602: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-08 03:43:32,200 EPOCH 7603
2024-02-08 03:43:48,340 Epoch 7603: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-08 03:43:48,340 EPOCH 7604
2024-02-08 03:44:04,254 Epoch 7604: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-08 03:44:04,254 EPOCH 7605
2024-02-08 03:44:20,474 Epoch 7605: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 03:44:20,475 EPOCH 7606
2024-02-08 03:44:36,249 Epoch 7606: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-08 03:44:36,250 EPOCH 7607
2024-02-08 03:44:52,589 Epoch 7607: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 03:44:52,590 EPOCH 7608
2024-02-08 03:45:08,820 Epoch 7608: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-08 03:45:08,821 EPOCH 7609
2024-02-08 03:45:24,947 Epoch 7609: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 03:45:24,948 EPOCH 7610
2024-02-08 03:45:41,120 Epoch 7610: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 03:45:41,121 EPOCH 7611
2024-02-08 03:45:57,249 Epoch 7611: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 03:45:57,249 EPOCH 7612
2024-02-08 03:46:01,665 [Epoch: 7612 Step: 00068500] Batch Recognition Loss:   0.000442 => Gls Tokens per Sec:       86 || Batch Translation Loss:   0.006912 => Txt Tokens per Sec:      307 || Lr: 0.000050
2024-02-08 03:46:13,323 Epoch 7612: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 03:46:13,323 EPOCH 7613
2024-02-08 03:46:29,719 Epoch 7613: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 03:46:29,720 EPOCH 7614
2024-02-08 03:46:45,775 Epoch 7614: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:46:45,776 EPOCH 7615
2024-02-08 03:47:01,931 Epoch 7615: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 03:47:01,931 EPOCH 7616
2024-02-08 03:47:18,366 Epoch 7616: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-08 03:47:18,366 EPOCH 7617
2024-02-08 03:47:34,696 Epoch 7617: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 03:47:34,696 EPOCH 7618
2024-02-08 03:47:50,648 Epoch 7618: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 03:47:50,649 EPOCH 7619
2024-02-08 03:48:06,887 Epoch 7619: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 03:48:06,888 EPOCH 7620
2024-02-08 03:48:23,090 Epoch 7620: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 03:48:23,090 EPOCH 7621
2024-02-08 03:48:39,323 Epoch 7621: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-08 03:48:39,323 EPOCH 7622
2024-02-08 03:48:55,531 Epoch 7622: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-08 03:48:55,531 EPOCH 7623
2024-02-08 03:48:59,497 [Epoch: 7623 Step: 00068600] Batch Recognition Loss:   0.000363 => Gls Tokens per Sec:      646 || Batch Translation Loss:   0.024736 => Txt Tokens per Sec:     2011 || Lr: 0.000050
2024-02-08 03:49:11,817 Epoch 7623: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-08 03:49:11,818 EPOCH 7624
2024-02-08 03:49:28,457 Epoch 7624: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-08 03:49:28,458 EPOCH 7625
2024-02-08 03:49:44,573 Epoch 7625: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-08 03:49:44,574 EPOCH 7626
2024-02-08 03:50:00,915 Epoch 7626: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-08 03:50:00,915 EPOCH 7627
2024-02-08 03:50:17,220 Epoch 7627: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-08 03:50:17,221 EPOCH 7628
2024-02-08 03:50:33,490 Epoch 7628: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-08 03:50:33,491 EPOCH 7629
2024-02-08 03:50:49,733 Epoch 7629: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-08 03:50:49,733 EPOCH 7630
2024-02-08 03:51:06,152 Epoch 7630: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-08 03:51:06,153 EPOCH 7631
2024-02-08 03:51:22,333 Epoch 7631: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 03:51:22,334 EPOCH 7632
2024-02-08 03:51:38,368 Epoch 7632: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 03:51:38,368 EPOCH 7633
2024-02-08 03:51:54,687 Epoch 7633: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 03:51:54,688 EPOCH 7634
2024-02-08 03:52:01,317 [Epoch: 7634 Step: 00068700] Batch Recognition Loss:   0.000750 => Gls Tokens per Sec:      579 || Batch Translation Loss:   0.017762 => Txt Tokens per Sec:     1650 || Lr: 0.000050
2024-02-08 03:52:10,967 Epoch 7634: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 03:52:10,968 EPOCH 7635
2024-02-08 03:52:27,160 Epoch 7635: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 03:52:27,161 EPOCH 7636
2024-02-08 03:52:43,069 Epoch 7636: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:52:43,070 EPOCH 7637
2024-02-08 03:52:59,217 Epoch 7637: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:52:59,218 EPOCH 7638
2024-02-08 03:53:15,470 Epoch 7638: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:53:15,471 EPOCH 7639
2024-02-08 03:53:31,792 Epoch 7639: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:53:31,792 EPOCH 7640
2024-02-08 03:53:47,846 Epoch 7640: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:53:47,848 EPOCH 7641
2024-02-08 03:54:03,995 Epoch 7641: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:54:03,995 EPOCH 7642
2024-02-08 03:54:20,056 Epoch 7642: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 03:54:20,057 EPOCH 7643
2024-02-08 03:54:36,240 Epoch 7643: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:54:36,241 EPOCH 7644
2024-02-08 03:54:52,581 Epoch 7644: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 03:54:52,582 EPOCH 7645
2024-02-08 03:54:57,492 [Epoch: 7645 Step: 00068800] Batch Recognition Loss:   0.000343 => Gls Tokens per Sec:     1043 || Batch Translation Loss:   0.015710 => Txt Tokens per Sec:     3096 || Lr: 0.000050
2024-02-08 03:55:08,642 Epoch 7645: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:55:08,643 EPOCH 7646
2024-02-08 03:55:24,720 Epoch 7646: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 03:55:24,720 EPOCH 7647
2024-02-08 03:55:40,911 Epoch 7647: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:55:40,912 EPOCH 7648
2024-02-08 03:55:57,242 Epoch 7648: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:55:57,243 EPOCH 7649
2024-02-08 03:56:13,285 Epoch 7649: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:56:13,286 EPOCH 7650
2024-02-08 03:56:29,425 Epoch 7650: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:56:29,426 EPOCH 7651
2024-02-08 03:56:45,701 Epoch 7651: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:56:45,702 EPOCH 7652
2024-02-08 03:57:01,880 Epoch 7652: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:57:01,881 EPOCH 7653
2024-02-08 03:57:18,329 Epoch 7653: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:57:18,329 EPOCH 7654
2024-02-08 03:57:34,499 Epoch 7654: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:57:34,500 EPOCH 7655
2024-02-08 03:57:50,727 Epoch 7655: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:57:50,727 EPOCH 7656
2024-02-08 03:57:59,182 [Epoch: 7656 Step: 00068900] Batch Recognition Loss:   0.000606 => Gls Tokens per Sec:      651 || Batch Translation Loss:   0.004843 => Txt Tokens per Sec:     1639 || Lr: 0.000050
2024-02-08 03:58:06,914 Epoch 7656: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:58:06,914 EPOCH 7657
2024-02-08 03:58:23,447 Epoch 7657: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-08 03:58:23,448 EPOCH 7658
2024-02-08 03:58:39,732 Epoch 7658: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:58:39,733 EPOCH 7659
2024-02-08 03:58:56,055 Epoch 7659: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:58:56,056 EPOCH 7660
2024-02-08 03:59:12,202 Epoch 7660: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:59:12,203 EPOCH 7661
2024-02-08 03:59:28,347 Epoch 7661: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 03:59:28,348 EPOCH 7662
2024-02-08 03:59:44,473 Epoch 7662: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 03:59:44,474 EPOCH 7663
2024-02-08 04:00:00,965 Epoch 7663: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:00:00,965 EPOCH 7664
2024-02-08 04:00:17,029 Epoch 7664: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:00:17,030 EPOCH 7665
2024-02-08 04:00:33,761 Epoch 7665: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:00:33,762 EPOCH 7666
2024-02-08 04:00:49,910 Epoch 7666: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:00:49,911 EPOCH 7667
2024-02-08 04:00:55,010 [Epoch: 7667 Step: 00069000] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:     1506 || Batch Translation Loss:   0.013479 => Txt Tokens per Sec:     3990 || Lr: 0.000050
2024-02-08 04:01:05,798 Epoch 7667: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:01:05,800 EPOCH 7668
2024-02-08 04:01:22,097 Epoch 7668: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:01:22,097 EPOCH 7669
2024-02-08 04:01:38,097 Epoch 7669: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:01:38,098 EPOCH 7670
2024-02-08 04:01:54,125 Epoch 7670: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:01:54,125 EPOCH 7671
2024-02-08 04:02:10,499 Epoch 7671: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:02:10,499 EPOCH 7672
2024-02-08 04:02:26,506 Epoch 7672: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.09 
2024-02-08 04:02:26,506 EPOCH 7673
2024-02-08 04:02:42,767 Epoch 7673: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:02:42,768 EPOCH 7674
2024-02-08 04:02:58,980 Epoch 7674: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:02:58,981 EPOCH 7675
2024-02-08 04:03:15,017 Epoch 7675: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:03:15,018 EPOCH 7676
2024-02-08 04:03:31,358 Epoch 7676: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:03:31,358 EPOCH 7677
2024-02-08 04:03:47,422 Epoch 7677: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:03:47,422 EPOCH 7678
2024-02-08 04:03:59,830 [Epoch: 7678 Step: 00069100] Batch Recognition Loss:   0.000606 => Gls Tokens per Sec:      650 || Batch Translation Loss:   0.004696 => Txt Tokens per Sec:     1725 || Lr: 0.000050
2024-02-08 04:04:03,590 Epoch 7678: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:04:03,590 EPOCH 7679
2024-02-08 04:04:19,840 Epoch 7679: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:04:19,841 EPOCH 7680
2024-02-08 04:04:35,896 Epoch 7680: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:04:35,897 EPOCH 7681
2024-02-08 04:04:52,227 Epoch 7681: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:04:52,228 EPOCH 7682
2024-02-08 04:05:08,359 Epoch 7682: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:05:08,359 EPOCH 7683
2024-02-08 04:05:24,640 Epoch 7683: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:05:24,641 EPOCH 7684
2024-02-08 04:05:40,544 Epoch 7684: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:05:40,545 EPOCH 7685
2024-02-08 04:05:56,857 Epoch 7685: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 04:05:56,857 EPOCH 7686
2024-02-08 04:06:12,960 Epoch 7686: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 04:06:12,961 EPOCH 7687
2024-02-08 04:06:29,065 Epoch 7687: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 04:06:29,065 EPOCH 7688
2024-02-08 04:06:45,024 Epoch 7688: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 04:06:45,026 EPOCH 7689
2024-02-08 04:07:00,577 [Epoch: 7689 Step: 00069200] Batch Recognition Loss:   0.000191 => Gls Tokens per Sec:      601 || Batch Translation Loss:   0.018255 => Txt Tokens per Sec:     1642 || Lr: 0.000050
2024-02-08 04:07:01,381 Epoch 7689: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-08 04:07:01,382 EPOCH 7690
2024-02-08 04:07:17,966 Epoch 7690: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 04:07:17,967 EPOCH 7691
2024-02-08 04:07:33,891 Epoch 7691: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 04:07:33,891 EPOCH 7692
2024-02-08 04:07:50,311 Epoch 7692: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 04:07:50,311 EPOCH 7693
2024-02-08 04:08:06,726 Epoch 7693: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 04:08:06,727 EPOCH 7694
2024-02-08 04:08:23,173 Epoch 7694: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 04:08:23,173 EPOCH 7695
2024-02-08 04:08:39,469 Epoch 7695: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 04:08:39,470 EPOCH 7696
2024-02-08 04:08:55,805 Epoch 7696: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 04:08:55,805 EPOCH 7697
2024-02-08 04:09:12,110 Epoch 7697: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 04:09:12,111 EPOCH 7698
2024-02-08 04:09:28,591 Epoch 7698: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 04:09:28,591 EPOCH 7699
2024-02-08 04:09:44,887 Epoch 7699: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-08 04:09:44,888 EPOCH 7700
2024-02-08 04:10:01,056 [Epoch: 7700 Step: 00069300] Batch Recognition Loss:   0.000166 => Gls Tokens per Sec:      657 || Batch Translation Loss:   0.017869 => Txt Tokens per Sec:     1817 || Lr: 0.000050
2024-02-08 04:10:01,056 Epoch 7700: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 04:10:01,057 EPOCH 7701
2024-02-08 04:10:17,372 Epoch 7701: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 04:10:17,372 EPOCH 7702
2024-02-08 04:10:33,290 Epoch 7702: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-08 04:10:33,291 EPOCH 7703
2024-02-08 04:10:49,491 Epoch 7703: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-08 04:10:49,492 EPOCH 7704
2024-02-08 04:11:05,484 Epoch 7704: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-08 04:11:05,484 EPOCH 7705
2024-02-08 04:11:21,695 Epoch 7705: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-08 04:11:21,696 EPOCH 7706
2024-02-08 04:11:37,518 Epoch 7706: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-08 04:11:37,519 EPOCH 7707
2024-02-08 04:11:53,889 Epoch 7707: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-08 04:11:53,890 EPOCH 7708
2024-02-08 04:12:09,832 Epoch 7708: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-08 04:12:09,833 EPOCH 7709
2024-02-08 04:12:26,118 Epoch 7709: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-08 04:12:26,118 EPOCH 7710
2024-02-08 04:12:42,246 Epoch 7710: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-08 04:12:42,246 EPOCH 7711
2024-02-08 04:12:58,244 Epoch 7711: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 04:12:58,246 EPOCH 7712
2024-02-08 04:12:59,108 [Epoch: 7712 Step: 00069400] Batch Recognition Loss:   0.000279 => Gls Tokens per Sec:     1486 || Batch Translation Loss:   0.019247 => Txt Tokens per Sec:     4547 || Lr: 0.000050
2024-02-08 04:13:15,016 Epoch 7712: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 04:13:15,017 EPOCH 7713
2024-02-08 04:13:31,423 Epoch 7713: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 04:13:31,424 EPOCH 7714
2024-02-08 04:13:47,641 Epoch 7714: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 04:13:47,642 EPOCH 7715
2024-02-08 04:14:03,830 Epoch 7715: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 04:14:03,831 EPOCH 7716
2024-02-08 04:14:19,893 Epoch 7716: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 04:14:19,893 EPOCH 7717
2024-02-08 04:14:36,016 Epoch 7717: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 04:14:36,017 EPOCH 7718
2024-02-08 04:14:52,104 Epoch 7718: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 04:14:52,104 EPOCH 7719
2024-02-08 04:15:08,453 Epoch 7719: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-08 04:15:08,454 EPOCH 7720
2024-02-08 04:15:24,447 Epoch 7720: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-08 04:15:24,447 EPOCH 7721
2024-02-08 04:15:40,726 Epoch 7721: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-08 04:15:40,726 EPOCH 7722
2024-02-08 04:15:56,775 Epoch 7722: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-08 04:15:56,775 EPOCH 7723
2024-02-08 04:15:57,722 [Epoch: 7723 Step: 00069500] Batch Recognition Loss:   0.000350 => Gls Tokens per Sec:     2709 || Batch Translation Loss:   0.032604 => Txt Tokens per Sec:     7612 || Lr: 0.000050
2024-02-08 04:16:13,062 Epoch 7723: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-08 04:16:13,062 EPOCH 7724
2024-02-08 04:16:29,157 Epoch 7724: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-08 04:16:29,158 EPOCH 7725
2024-02-08 04:16:45,472 Epoch 7725: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-08 04:16:45,474 EPOCH 7726
2024-02-08 04:17:01,506 Epoch 7726: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-08 04:17:01,507 EPOCH 7727
2024-02-08 04:17:17,907 Epoch 7727: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-08 04:17:17,908 EPOCH 7728
2024-02-08 04:17:33,847 Epoch 7728: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-08 04:17:33,848 EPOCH 7729
2024-02-08 04:17:50,404 Epoch 7729: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-08 04:17:50,405 EPOCH 7730
2024-02-08 04:18:07,115 Epoch 7730: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-08 04:18:07,115 EPOCH 7731
2024-02-08 04:18:22,975 Epoch 7731: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-08 04:18:22,976 EPOCH 7732
2024-02-08 04:18:39,634 Epoch 7732: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-08 04:18:39,635 EPOCH 7733
2024-02-08 04:18:55,945 Epoch 7733: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 04:18:55,946 EPOCH 7734
2024-02-08 04:19:01,044 [Epoch: 7734 Step: 00069600] Batch Recognition Loss:   0.000289 => Gls Tokens per Sec:      577 || Batch Translation Loss:   0.009199 => Txt Tokens per Sec:     1410 || Lr: 0.000050
2024-02-08 04:19:12,343 Epoch 7734: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 04:19:12,344 EPOCH 7735
2024-02-08 04:19:28,382 Epoch 7735: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 04:19:28,383 EPOCH 7736
2024-02-08 04:19:44,273 Epoch 7736: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 04:19:44,273 EPOCH 7737
2024-02-08 04:20:00,813 Epoch 7737: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 04:20:00,814 EPOCH 7738
2024-02-08 04:20:16,869 Epoch 7738: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 04:20:16,870 EPOCH 7739
2024-02-08 04:20:33,145 Epoch 7739: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 04:20:33,145 EPOCH 7740
2024-02-08 04:20:49,562 Epoch 7740: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 04:20:49,563 EPOCH 7741
2024-02-08 04:21:05,726 Epoch 7741: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 04:21:05,727 EPOCH 7742
2024-02-08 04:21:21,760 Epoch 7742: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 04:21:21,761 EPOCH 7743
2024-02-08 04:21:37,961 Epoch 7743: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:21:37,961 EPOCH 7744
2024-02-08 04:21:53,940 Epoch 7744: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:21:53,941 EPOCH 7745
2024-02-08 04:22:04,022 [Epoch: 7745 Step: 00069700] Batch Recognition Loss:   0.000100 => Gls Tokens per Sec:      508 || Batch Translation Loss:   0.007133 => Txt Tokens per Sec:     1492 || Lr: 0.000050
2024-02-08 04:22:10,258 Epoch 7745: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:22:10,258 EPOCH 7746
2024-02-08 04:22:26,256 Epoch 7746: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:22:26,258 EPOCH 7747
2024-02-08 04:22:42,453 Epoch 7747: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 04:22:42,454 EPOCH 7748
2024-02-08 04:22:58,556 Epoch 7748: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 04:22:58,557 EPOCH 7749
2024-02-08 04:23:14,775 Epoch 7749: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 04:23:14,775 EPOCH 7750
2024-02-08 04:23:30,999 Epoch 7750: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 04:23:31,000 EPOCH 7751
2024-02-08 04:23:47,856 Epoch 7751: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 04:23:47,858 EPOCH 7752
2024-02-08 04:24:03,983 Epoch 7752: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 04:24:03,983 EPOCH 7753
2024-02-08 04:24:20,524 Epoch 7753: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 04:24:20,524 EPOCH 7754
2024-02-08 04:24:36,718 Epoch 7754: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:24:36,718 EPOCH 7755
2024-02-08 04:24:53,005 Epoch 7755: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:24:53,006 EPOCH 7756
2024-02-08 04:24:57,746 [Epoch: 7756 Step: 00069800] Batch Recognition Loss:   0.000528 => Gls Tokens per Sec:     1350 || Batch Translation Loss:   0.011069 => Txt Tokens per Sec:     3471 || Lr: 0.000050
2024-02-08 04:25:09,205 Epoch 7756: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-08 04:25:09,207 EPOCH 7757
2024-02-08 04:25:25,497 Epoch 7757: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:25:25,497 EPOCH 7758
2024-02-08 04:25:41,505 Epoch 7758: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:25:41,506 EPOCH 7759
2024-02-08 04:25:57,428 Epoch 7759: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:25:57,429 EPOCH 7760
2024-02-08 04:26:13,769 Epoch 7760: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:26:13,769 EPOCH 7761
2024-02-08 04:26:29,716 Epoch 7761: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:26:29,717 EPOCH 7762
2024-02-08 04:26:45,809 Epoch 7762: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:26:45,809 EPOCH 7763
2024-02-08 04:27:02,058 Epoch 7763: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:27:02,059 EPOCH 7764
2024-02-08 04:27:18,313 Epoch 7764: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:27:18,314 EPOCH 7765
2024-02-08 04:27:34,810 Epoch 7765: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:27:34,811 EPOCH 7766
2024-02-08 04:27:50,878 Epoch 7766: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:27:50,879 EPOCH 7767
2024-02-08 04:28:03,208 [Epoch: 7767 Step: 00069900] Batch Recognition Loss:   0.006316 => Gls Tokens per Sec:      550 || Batch Translation Loss:   0.006897 => Txt Tokens per Sec:     1551 || Lr: 0.000050
2024-02-08 04:28:07,272 Epoch 7767: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-08 04:28:07,272 EPOCH 7768
2024-02-08 04:28:23,412 Epoch 7768: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 04:28:23,412 EPOCH 7769
2024-02-08 04:28:39,811 Epoch 7769: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-08 04:28:39,812 EPOCH 7770
2024-02-08 04:28:55,959 Epoch 7770: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 04:28:55,960 EPOCH 7771
2024-02-08 04:29:11,933 Epoch 7771: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 04:29:11,934 EPOCH 7772
2024-02-08 04:29:28,175 Epoch 7772: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 04:29:28,175 EPOCH 7773
2024-02-08 04:29:44,894 Epoch 7773: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 04:29:44,895 EPOCH 7774
2024-02-08 04:30:00,892 Epoch 7774: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 04:30:00,893 EPOCH 7775
2024-02-08 04:30:17,006 Epoch 7775: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 04:30:17,006 EPOCH 7776
2024-02-08 04:30:33,132 Epoch 7776: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-08 04:30:33,133 EPOCH 7777
2024-02-08 04:30:49,237 Epoch 7777: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.77 
2024-02-08 04:30:49,238 EPOCH 7778
2024-02-08 04:31:04,628 [Epoch: 7778 Step: 00070000] Batch Recognition Loss:   0.001161 => Gls Tokens per Sec:      524 || Batch Translation Loss:   0.214743 => Txt Tokens per Sec:     1550 || Lr: 0.000050
2024-02-08 04:32:12,635 Validation result at epoch 7778, step    70000: duration: 68.0064s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.31660	Translation Loss: 111226.12500	PPL: 66788.12500
	Eval Metric: BLEU
	WER 2.33	(DEL: 0.00,	INS: 0.00,	SUB: 2.33)
	BLEU-4 0.32	(BLEU-1: 9.06,	BLEU-2: 2.39,	BLEU-3: 0.90,	BLEU-4: 0.32)
	CHRF 16.49	ROUGE 7.28
2024-02-08 04:32:12,637 Logging Recognition and Translation Outputs
2024-02-08 04:32:12,637 ========================================================================================================================
2024-02-08 04:32:12,637 Logging Sequence: 129_90.00
2024-02-08 04:32:12,637 	Gloss Reference :	A B+C+D+E
2024-02-08 04:32:12,637 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 04:32:12,638 	Gloss Alignment :	         
2024-02-08 04:32:12,638 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 04:32:12,640 	Text Reference  :	however because    of      the     emergency games will  now      be held    without any ******* *** *** ** ** *** spectators
2024-02-08 04:32:12,640 	Text Hypothesis :	******* australian cricket council her       paul  pogba attended a  matches in      any thrower can use it at the 2020      
2024-02-08 04:32:12,641 	Text Alignment  :	D       S          S       S       S         S     S     S        S  S       S           I       I   I   I  I  I   S         
2024-02-08 04:32:12,641 ========================================================================================================================
2024-02-08 04:32:12,641 Logging Sequence: 179_378.00
2024-02-08 04:32:12,641 	Gloss Reference :	A B+C+D+E
2024-02-08 04:32:12,641 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 04:32:12,641 	Gloss Alignment :	         
2024-02-08 04:32:12,641 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 04:32:12,643 	Text Reference  :	these kids think that they are going to      the         olympics so           they've become some kind of    stars
2024-02-08 04:32:12,643 	Text Hypothesis :	***** **** ***** **** **** the deaf  cricket association idca     participated in      1st    day  her  match fee  
2024-02-08 04:32:12,643 	Text Alignment  :	D     D    D     D    D    S   S     S       S           S        S            S       S      S    S    S     S    
2024-02-08 04:32:12,644 ========================================================================================================================
2024-02-08 04:32:12,644 Logging Sequence: 162_20.00
2024-02-08 04:32:12,644 	Gloss Reference :	A B+C+D+E
2024-02-08 04:32:12,644 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 04:32:12,644 	Gloss Alignment :	         
2024-02-08 04:32:12,645 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 04:32:12,647 	Text Reference  :	**** not   only this    but they *** ****** ****** *** ** ** blamed mohammed shami's religion as   the reason for    india's loss    
2024-02-08 04:32:12,647 	Text Hypothesis :	they issue such threats as  they are lonely ignore all of it at     the      focus   on       keep the team   strong its     comments
2024-02-08 04:32:12,647 	Text Alignment  :	I    S     S    S       S        I   I      I      I   I  I  S      S        S       S        S        S      S      S       S       
2024-02-08 04:32:12,647 ========================================================================================================================
2024-02-08 04:32:12,647 Logging Sequence: 106_169.00
2024-02-08 04:32:12,648 	Gloss Reference :	A B+C+D+E
2024-02-08 04:32:12,648 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 04:32:12,648 	Gloss Alignment :	         
2024-02-08 04:32:12,648 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 04:32:12,649 	Text Reference  :	prime minister narendra modi also expressed his    happiness while congratulating the team on    twitter
2024-02-08 04:32:12,649 	Text Hypothesis :	***** ******** do       you  know amrapali  really amazed    by    people         of  by   third time   
2024-02-08 04:32:12,649 	Text Alignment  :	D     D        S        S    S    S         S      S         S     S              S   S    S     S      
2024-02-08 04:32:12,650 ========================================================================================================================
2024-02-08 04:32:12,650 Logging Sequence: 65_77.00
2024-02-08 04:32:12,650 	Gloss Reference :	A B+C+D+E
2024-02-08 04:32:12,650 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 04:32:12,650 	Gloss Alignment :	         
2024-02-08 04:32:12,651 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 04:32:12,651 	Text Reference  :	*** ******* **** indian team    travelling included 16        players
2024-02-08 04:32:12,651 	Text Hypothesis :	now tickets into a      tickets on         12th     september 2022   
2024-02-08 04:32:12,651 	Text Alignment  :	I   I       I    S      S       S          S        S         S      
2024-02-08 04:32:12,651 ========================================================================================================================
2024-02-08 04:32:13,572 Epoch 7778: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-08 04:32:13,572 EPOCH 7779
2024-02-08 04:32:30,576 Epoch 7779: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-08 04:32:30,576 EPOCH 7780
2024-02-08 04:32:47,055 Epoch 7780: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-08 04:32:47,056 EPOCH 7781
2024-02-08 04:33:03,543 Epoch 7781: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-08 04:33:03,544 EPOCH 7782
2024-02-08 04:33:19,906 Epoch 7782: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-08 04:33:19,907 EPOCH 7783
2024-02-08 04:33:36,082 Epoch 7783: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-08 04:33:36,083 EPOCH 7784
2024-02-08 04:33:52,202 Epoch 7784: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-08 04:33:52,203 EPOCH 7785
2024-02-08 04:34:08,642 Epoch 7785: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-08 04:34:08,643 EPOCH 7786
2024-02-08 04:34:24,703 Epoch 7786: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-08 04:34:24,703 EPOCH 7787
2024-02-08 04:34:40,843 Epoch 7787: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-08 04:34:40,843 EPOCH 7788
2024-02-08 04:34:57,020 Epoch 7788: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 04:34:57,020 EPOCH 7789
2024-02-08 04:35:12,871 [Epoch: 7789 Step: 00070100] Batch Recognition Loss:   0.000488 => Gls Tokens per Sec:      589 || Batch Translation Loss:   0.009243 => Txt Tokens per Sec:     1632 || Lr: 0.000050
2024-02-08 04:35:13,324 Epoch 7789: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 04:35:13,324 EPOCH 7790
2024-02-08 04:35:29,367 Epoch 7790: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 04:35:29,367 EPOCH 7791
2024-02-08 04:35:45,731 Epoch 7791: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 04:35:45,731 EPOCH 7792
2024-02-08 04:36:02,082 Epoch 7792: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-08 04:36:02,082 EPOCH 7793
2024-02-08 04:36:18,234 Epoch 7793: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 04:36:18,235 EPOCH 7794
2024-02-08 04:36:34,256 Epoch 7794: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-08 04:36:34,256 EPOCH 7795
2024-02-08 04:36:50,517 Epoch 7795: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 04:36:50,518 EPOCH 7796
2024-02-08 04:37:06,563 Epoch 7796: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 04:37:06,563 EPOCH 7797
2024-02-08 04:37:22,995 Epoch 7797: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:37:22,995 EPOCH 7798
2024-02-08 04:37:39,105 Epoch 7798: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:37:39,106 EPOCH 7799
2024-02-08 04:37:55,317 Epoch 7799: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:37:55,318 EPOCH 7800
2024-02-08 04:38:10,973 [Epoch: 7800 Step: 00070200] Batch Recognition Loss:   0.000645 => Gls Tokens per Sec:      678 || Batch Translation Loss:   0.016692 => Txt Tokens per Sec:     1877 || Lr: 0.000050
2024-02-08 04:38:10,973 Epoch 7800: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:38:10,974 EPOCH 7801
2024-02-08 04:38:27,144 Epoch 7801: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:38:27,145 EPOCH 7802
2024-02-08 04:38:43,206 Epoch 7802: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:38:43,206 EPOCH 7803
2024-02-08 04:38:59,445 Epoch 7803: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:38:59,445 EPOCH 7804
2024-02-08 04:39:15,947 Epoch 7804: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:39:15,947 EPOCH 7805
2024-02-08 04:39:31,988 Epoch 7805: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:39:31,989 EPOCH 7806
2024-02-08 04:39:48,366 Epoch 7806: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:39:48,366 EPOCH 7807
2024-02-08 04:40:04,396 Epoch 7807: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:40:04,397 EPOCH 7808
2024-02-08 04:40:20,639 Epoch 7808: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:40:20,639 EPOCH 7809
2024-02-08 04:40:36,878 Epoch 7809: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:40:36,878 EPOCH 7810
2024-02-08 04:40:52,938 Epoch 7810: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:40:52,939 EPOCH 7811
2024-02-08 04:41:09,303 Epoch 7811: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:41:09,304 EPOCH 7812
2024-02-08 04:41:12,485 [Epoch: 7812 Step: 00070300] Batch Recognition Loss:   0.003202 => Gls Tokens per Sec:      403 || Batch Translation Loss:   0.011540 => Txt Tokens per Sec:     1281 || Lr: 0.000050
2024-02-08 04:41:25,420 Epoch 7812: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-08 04:41:25,421 EPOCH 7813
2024-02-08 04:41:41,518 Epoch 7813: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:41:41,518 EPOCH 7814
2024-02-08 04:41:57,766 Epoch 7814: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:41:57,767 EPOCH 7815
2024-02-08 04:42:14,035 Epoch 7815: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:42:14,035 EPOCH 7816
2024-02-08 04:42:30,383 Epoch 7816: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:42:30,383 EPOCH 7817
2024-02-08 04:42:46,592 Epoch 7817: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 04:42:46,593 EPOCH 7818
2024-02-08 04:43:03,149 Epoch 7818: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:43:03,150 EPOCH 7819
2024-02-08 04:43:19,344 Epoch 7819: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 04:43:19,345 EPOCH 7820
2024-02-08 04:43:35,414 Epoch 7820: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 04:43:35,414 EPOCH 7821
2024-02-08 04:43:51,937 Epoch 7821: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 04:43:51,938 EPOCH 7822
2024-02-08 04:44:08,014 Epoch 7822: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 04:44:08,015 EPOCH 7823
2024-02-08 04:44:14,344 [Epoch: 7823 Step: 00070400] Batch Recognition Loss:   0.000374 => Gls Tokens per Sec:      405 || Batch Translation Loss:   0.007713 => Txt Tokens per Sec:     1214 || Lr: 0.000050
2024-02-08 04:44:24,535 Epoch 7823: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:44:24,536 EPOCH 7824
2024-02-08 04:44:40,757 Epoch 7824: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:44:40,758 EPOCH 7825
2024-02-08 04:44:56,763 Epoch 7825: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 04:44:56,764 EPOCH 7826
2024-02-08 04:45:12,989 Epoch 7826: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 04:45:12,989 EPOCH 7827
2024-02-08 04:45:28,984 Epoch 7827: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:45:28,985 EPOCH 7828
2024-02-08 04:45:45,507 Epoch 7828: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:45:45,508 EPOCH 7829
2024-02-08 04:46:01,959 Epoch 7829: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:46:01,959 EPOCH 7830
2024-02-08 04:46:18,357 Epoch 7830: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:46:18,357 EPOCH 7831
2024-02-08 04:46:34,580 Epoch 7831: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 04:46:34,581 EPOCH 7832
2024-02-08 04:46:50,536 Epoch 7832: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 04:46:50,537 EPOCH 7833
2024-02-08 04:47:06,816 Epoch 7833: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:47:06,817 EPOCH 7834
2024-02-08 04:47:08,269 [Epoch: 7834 Step: 00070500] Batch Recognition Loss:   0.000125 => Gls Tokens per Sec:     2646 || Batch Translation Loss:   0.012608 => Txt Tokens per Sec:     6777 || Lr: 0.000050
2024-02-08 04:47:22,825 Epoch 7834: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 04:47:22,825 EPOCH 7835
2024-02-08 04:47:39,111 Epoch 7835: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-08 04:47:39,111 EPOCH 7836
2024-02-08 04:47:55,356 Epoch 7836: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-08 04:47:55,356 EPOCH 7837
2024-02-08 04:48:11,389 Epoch 7837: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-08 04:48:11,390 EPOCH 7838
2024-02-08 04:48:27,740 Epoch 7838: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.11 
2024-02-08 04:48:27,740 EPOCH 7839
2024-02-08 04:48:44,195 Epoch 7839: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-08 04:48:44,196 EPOCH 7840
2024-02-08 04:49:00,199 Epoch 7840: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.49 
2024-02-08 04:49:00,199 EPOCH 7841
2024-02-08 04:49:16,602 Epoch 7841: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.24 
2024-02-08 04:49:16,602 EPOCH 7842
2024-02-08 04:49:32,578 Epoch 7842: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.64 
2024-02-08 04:49:32,579 EPOCH 7843
2024-02-08 04:49:48,871 Epoch 7843: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-08 04:49:48,872 EPOCH 7844
2024-02-08 04:50:05,190 Epoch 7844: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-08 04:50:05,191 EPOCH 7845
2024-02-08 04:50:10,749 [Epoch: 7845 Step: 00070600] Batch Recognition Loss:   0.001313 => Gls Tokens per Sec:      759 || Batch Translation Loss:   0.014737 => Txt Tokens per Sec:     1860 || Lr: 0.000050
2024-02-08 04:50:21,484 Epoch 7845: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-08 04:50:21,485 EPOCH 7846
2024-02-08 04:50:37,566 Epoch 7846: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-08 04:50:37,567 EPOCH 7847
2024-02-08 04:50:53,706 Epoch 7847: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-08 04:50:53,706 EPOCH 7848
2024-02-08 04:51:09,948 Epoch 7848: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-08 04:51:09,948 EPOCH 7849
2024-02-08 04:51:26,115 Epoch 7849: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-08 04:51:26,117 EPOCH 7850
2024-02-08 04:51:42,620 Epoch 7850: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-08 04:51:42,621 EPOCH 7851
2024-02-08 04:51:58,964 Epoch 7851: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-08 04:51:58,965 EPOCH 7852
2024-02-08 04:52:15,133 Epoch 7852: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-08 04:52:15,134 EPOCH 7853
2024-02-08 04:52:31,908 Epoch 7853: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-08 04:52:31,908 EPOCH 7854
2024-02-08 04:52:48,031 Epoch 7854: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 04:52:48,032 EPOCH 7855
2024-02-08 04:53:04,248 Epoch 7855: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-08 04:53:04,249 EPOCH 7856
2024-02-08 04:53:09,621 [Epoch: 7856 Step: 00070700] Batch Recognition Loss:   0.000391 => Gls Tokens per Sec:     1192 || Batch Translation Loss:   0.019970 => Txt Tokens per Sec:     3361 || Lr: 0.000050
2024-02-08 04:53:20,259 Epoch 7856: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 04:53:20,260 EPOCH 7857
2024-02-08 04:53:36,787 Epoch 7857: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 04:53:36,788 EPOCH 7858
2024-02-08 04:53:53,145 Epoch 7858: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 04:53:53,145 EPOCH 7859
2024-02-08 04:54:09,135 Epoch 7859: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-08 04:54:09,135 EPOCH 7860
2024-02-08 04:54:25,279 Epoch 7860: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 04:54:25,280 EPOCH 7861
2024-02-08 04:54:41,295 Epoch 7861: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 04:54:41,295 EPOCH 7862
2024-02-08 04:54:57,678 Epoch 7862: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 04:54:57,679 EPOCH 7863
2024-02-08 04:55:13,846 Epoch 7863: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:55:13,847 EPOCH 7864
2024-02-08 04:55:30,226 Epoch 7864: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:55:30,227 EPOCH 7865
2024-02-08 04:55:46,313 Epoch 7865: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:55:46,314 EPOCH 7866
2024-02-08 04:56:02,459 Epoch 7866: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:56:02,459 EPOCH 7867
2024-02-08 04:56:14,546 [Epoch: 7867 Step: 00070800] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:      561 || Batch Translation Loss:   0.007515 => Txt Tokens per Sec:     1512 || Lr: 0.000050
2024-02-08 04:56:18,770 Epoch 7867: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:56:18,770 EPOCH 7868
2024-02-08 04:56:34,878 Epoch 7868: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:56:34,879 EPOCH 7869
2024-02-08 04:56:50,973 Epoch 7869: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:56:50,973 EPOCH 7870
2024-02-08 04:57:07,113 Epoch 7870: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-08 04:57:07,114 EPOCH 7871
2024-02-08 04:57:23,488 Epoch 7871: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:57:23,489 EPOCH 7872
2024-02-08 04:57:39,672 Epoch 7872: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:57:39,673 EPOCH 7873
2024-02-08 04:57:55,924 Epoch 7873: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:57:55,924 EPOCH 7874
2024-02-08 04:58:12,211 Epoch 7874: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 04:58:12,211 EPOCH 7875
2024-02-08 04:58:28,323 Epoch 7875: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:58:28,324 EPOCH 7876
2024-02-08 04:58:44,370 Epoch 7876: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:58:44,371 EPOCH 7877
2024-02-08 04:59:00,324 Epoch 7877: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:59:00,325 EPOCH 7878
2024-02-08 04:59:15,813 [Epoch: 7878 Step: 00070900] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:      520 || Batch Translation Loss:   0.017060 => Txt Tokens per Sec:     1522 || Lr: 0.000050
2024-02-08 04:59:16,538 Epoch 7878: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-08 04:59:16,539 EPOCH 7879
2024-02-08 04:59:32,651 Epoch 7879: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 04:59:32,652 EPOCH 7880
2024-02-08 04:59:48,580 Epoch 7880: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 04:59:48,580 EPOCH 7881
2024-02-08 05:00:04,841 Epoch 7881: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:00:04,842 EPOCH 7882
2024-02-08 05:00:20,967 Epoch 7882: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.09 
2024-02-08 05:00:20,967 EPOCH 7883
2024-02-08 05:00:37,410 Epoch 7883: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-08 05:00:37,410 EPOCH 7884
2024-02-08 05:00:53,531 Epoch 7884: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 05:00:53,532 EPOCH 7885
2024-02-08 05:01:09,787 Epoch 7885: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 05:01:09,788 EPOCH 7886
2024-02-08 05:01:25,995 Epoch 7886: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-08 05:01:25,996 EPOCH 7887
2024-02-08 05:01:42,455 Epoch 7887: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 05:01:42,455 EPOCH 7888
2024-02-08 05:01:58,470 Epoch 7888: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:01:58,470 EPOCH 7889
2024-02-08 05:02:10,478 [Epoch: 7889 Step: 00071000] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:      853 || Batch Translation Loss:   0.010991 => Txt Tokens per Sec:     2335 || Lr: 0.000050
2024-02-08 05:02:14,845 Epoch 7889: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:02:14,845 EPOCH 7890
2024-02-08 05:02:31,020 Epoch 7890: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 05:02:31,021 EPOCH 7891
2024-02-08 05:02:47,430 Epoch 7891: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:02:47,431 EPOCH 7892
2024-02-08 05:03:03,766 Epoch 7892: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:03:03,767 EPOCH 7893
2024-02-08 05:03:19,739 Epoch 7893: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:03:19,740 EPOCH 7894
2024-02-08 05:03:36,087 Epoch 7894: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:03:36,088 EPOCH 7895
2024-02-08 05:03:52,188 Epoch 7895: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:03:52,189 EPOCH 7896
2024-02-08 05:04:08,191 Epoch 7896: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:04:08,191 EPOCH 7897
2024-02-08 05:04:24,367 Epoch 7897: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:04:24,367 EPOCH 7898
2024-02-08 05:04:40,409 Epoch 7898: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:04:40,409 EPOCH 7899
2024-02-08 05:04:56,436 Epoch 7899: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:04:56,437 EPOCH 7900
2024-02-08 05:05:12,869 [Epoch: 7900 Step: 00071100] Batch Recognition Loss:   0.000309 => Gls Tokens per Sec:      646 || Batch Translation Loss:   0.004449 => Txt Tokens per Sec:     1788 || Lr: 0.000050
2024-02-08 05:05:12,870 Epoch 7900: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:05:12,870 EPOCH 7901
2024-02-08 05:05:29,182 Epoch 7901: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:05:29,183 EPOCH 7902
2024-02-08 05:05:45,350 Epoch 7902: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:05:45,351 EPOCH 7903
2024-02-08 05:06:01,737 Epoch 7903: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:06:01,738 EPOCH 7904
2024-02-08 05:06:18,186 Epoch 7904: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:06:18,187 EPOCH 7905
2024-02-08 05:06:34,179 Epoch 7905: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:06:34,181 EPOCH 7906
2024-02-08 05:06:50,326 Epoch 7906: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:06:50,326 EPOCH 7907
2024-02-08 05:07:06,655 Epoch 7907: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:07:06,656 EPOCH 7908
2024-02-08 05:07:22,811 Epoch 7908: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:07:22,812 EPOCH 7909
2024-02-08 05:07:39,220 Epoch 7909: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:07:39,222 EPOCH 7910
2024-02-08 05:07:55,689 Epoch 7910: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:07:55,690 EPOCH 7911
2024-02-08 05:08:11,639 Epoch 7911: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:08:11,641 EPOCH 7912
2024-02-08 05:08:17,211 [Epoch: 7912 Step: 00071200] Batch Recognition Loss:   0.000462 => Gls Tokens per Sec:      230 || Batch Translation Loss:   0.013285 => Txt Tokens per Sec:      792 || Lr: 0.000050
2024-02-08 05:08:28,170 Epoch 7912: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:08:28,170 EPOCH 7913
2024-02-08 05:08:44,171 Epoch 7913: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:08:44,173 EPOCH 7914
2024-02-08 05:09:00,632 Epoch 7914: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:09:00,633 EPOCH 7915
2024-02-08 05:09:16,837 Epoch 7915: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:09:16,838 EPOCH 7916
2024-02-08 05:09:33,402 Epoch 7916: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:09:33,403 EPOCH 7917
2024-02-08 05:09:49,427 Epoch 7917: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:09:49,428 EPOCH 7918
2024-02-08 05:10:05,797 Epoch 7918: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:10:05,797 EPOCH 7919
2024-02-08 05:10:21,928 Epoch 7919: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:10:21,928 EPOCH 7920
2024-02-08 05:10:38,355 Epoch 7920: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:10:38,356 EPOCH 7921
2024-02-08 05:10:55,049 Epoch 7921: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:10:55,051 EPOCH 7922
2024-02-08 05:11:11,300 Epoch 7922: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:11:11,301 EPOCH 7923
2024-02-08 05:11:15,810 [Epoch: 7923 Step: 00071300] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:      368 || Batch Translation Loss:   0.004245 => Txt Tokens per Sec:      811 || Lr: 0.000050
2024-02-08 05:11:27,180 Epoch 7923: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:11:27,181 EPOCH 7924
2024-02-08 05:11:43,296 Epoch 7924: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:11:43,297 EPOCH 7925
2024-02-08 05:11:59,748 Epoch 7925: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:11:59,749 EPOCH 7926
2024-02-08 05:12:16,293 Epoch 7926: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:12:16,293 EPOCH 7927
2024-02-08 05:12:32,384 Epoch 7927: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:12:32,386 EPOCH 7928
2024-02-08 05:12:48,692 Epoch 7928: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:12:48,694 EPOCH 7929
2024-02-08 05:13:05,338 Epoch 7929: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:13:05,339 EPOCH 7930
2024-02-08 05:13:21,490 Epoch 7930: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:13:21,490 EPOCH 7931
2024-02-08 05:13:38,234 Epoch 7931: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:13:38,235 EPOCH 7932
2024-02-08 05:13:54,398 Epoch 7932: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:13:54,400 EPOCH 7933
2024-02-08 05:14:10,515 Epoch 7933: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 05:14:10,515 EPOCH 7934
2024-02-08 05:14:12,064 [Epoch: 7934 Step: 00071400] Batch Recognition Loss:   0.000358 => Gls Tokens per Sec:     2481 || Batch Translation Loss:   0.009136 => Txt Tokens per Sec:     7174 || Lr: 0.000050
2024-02-08 05:14:26,419 Epoch 7934: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:14:26,419 EPOCH 7935
2024-02-08 05:14:42,929 Epoch 7935: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:14:42,929 EPOCH 7936
2024-02-08 05:14:58,927 Epoch 7936: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:14:58,927 EPOCH 7937
2024-02-08 05:15:15,618 Epoch 7937: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:15:15,619 EPOCH 7938
2024-02-08 05:15:32,055 Epoch 7938: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:15:32,055 EPOCH 7939
2024-02-08 05:15:47,872 Epoch 7939: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:15:47,873 EPOCH 7940
2024-02-08 05:16:04,042 Epoch 7940: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:16:04,042 EPOCH 7941
2024-02-08 05:16:20,535 Epoch 7941: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:16:20,537 EPOCH 7942
2024-02-08 05:16:36,907 Epoch 7942: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:16:36,908 EPOCH 7943
2024-02-08 05:16:53,361 Epoch 7943: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:16:53,363 EPOCH 7944
2024-02-08 05:17:09,585 Epoch 7944: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:17:09,585 EPOCH 7945
2024-02-08 05:17:14,372 [Epoch: 7945 Step: 00071500] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:     1070 || Batch Translation Loss:   0.020036 => Txt Tokens per Sec:     2924 || Lr: 0.000050
2024-02-08 05:17:26,142 Epoch 7945: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:17:26,144 EPOCH 7946
2024-02-08 05:17:42,982 Epoch 7946: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:17:42,982 EPOCH 7947
2024-02-08 05:17:59,396 Epoch 7947: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:17:59,396 EPOCH 7948
2024-02-08 05:18:16,111 Epoch 7948: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:18:16,112 EPOCH 7949
2024-02-08 05:18:32,602 Epoch 7949: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:18:32,603 EPOCH 7950
2024-02-08 05:18:48,834 Epoch 7950: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:18:48,835 EPOCH 7951
2024-02-08 05:19:04,903 Epoch 7951: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:19:04,904 EPOCH 7952
2024-02-08 05:19:21,447 Epoch 7952: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:19:21,448 EPOCH 7953
2024-02-08 05:19:37,577 Epoch 7953: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:19:37,578 EPOCH 7954
2024-02-08 05:19:54,016 Epoch 7954: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:19:54,018 EPOCH 7955
2024-02-08 05:20:10,101 Epoch 7955: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:20:10,101 EPOCH 7956
2024-02-08 05:20:16,437 [Epoch: 7956 Step: 00071600] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:      868 || Batch Translation Loss:   0.013652 => Txt Tokens per Sec:     2382 || Lr: 0.000050
2024-02-08 05:20:26,140 Epoch 7956: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:20:26,141 EPOCH 7957
2024-02-08 05:20:42,873 Epoch 7957: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 05:20:42,874 EPOCH 7958
2024-02-08 05:20:59,158 Epoch 7958: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 05:20:59,158 EPOCH 7959
2024-02-08 05:21:15,556 Epoch 7959: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 05:21:15,556 EPOCH 7960
2024-02-08 05:21:31,676 Epoch 7960: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 05:21:31,678 EPOCH 7961
2024-02-08 05:21:48,340 Epoch 7961: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 05:21:48,341 EPOCH 7962
2024-02-08 05:22:04,433 Epoch 7962: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 05:22:04,435 EPOCH 7963
2024-02-08 05:22:20,903 Epoch 7963: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 05:22:20,904 EPOCH 7964
2024-02-08 05:22:37,016 Epoch 7964: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 05:22:37,017 EPOCH 7965
2024-02-08 05:22:53,584 Epoch 7965: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 05:22:53,586 EPOCH 7966
2024-02-08 05:23:09,568 Epoch 7966: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 05:23:09,569 EPOCH 7967
2024-02-08 05:23:15,377 [Epoch: 7967 Step: 00071700] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:     1323 || Batch Translation Loss:   0.009928 => Txt Tokens per Sec:     3575 || Lr: 0.000050
2024-02-08 05:23:26,173 Epoch 7967: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:23:26,174 EPOCH 7968
2024-02-08 05:23:42,647 Epoch 7968: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:23:42,649 EPOCH 7969
2024-02-08 05:23:58,900 Epoch 7969: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:23:58,901 EPOCH 7970
2024-02-08 05:24:15,289 Epoch 7970: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:24:15,291 EPOCH 7971
2024-02-08 05:24:32,248 Epoch 7971: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:24:32,249 EPOCH 7972
2024-02-08 05:24:48,353 Epoch 7972: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:24:48,354 EPOCH 7973
2024-02-08 05:25:05,516 Epoch 7973: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:25:05,517 EPOCH 7974
2024-02-08 05:25:21,969 Epoch 7974: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:25:21,970 EPOCH 7975
2024-02-08 05:25:38,457 Epoch 7975: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:25:38,459 EPOCH 7976
2024-02-08 05:25:55,159 Epoch 7976: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:25:55,160 EPOCH 7977
2024-02-08 05:26:11,342 Epoch 7977: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:26:11,342 EPOCH 7978
2024-02-08 05:26:19,753 [Epoch: 7978 Step: 00071800] Batch Recognition Loss:   0.000502 => Gls Tokens per Sec:     1065 || Batch Translation Loss:   0.015013 => Txt Tokens per Sec:     2844 || Lr: 0.000050
2024-02-08 05:26:27,448 Epoch 7978: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:26:27,449 EPOCH 7979
2024-02-08 05:26:43,391 Epoch 7979: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:26:43,393 EPOCH 7980
2024-02-08 05:26:59,984 Epoch 7980: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:26:59,984 EPOCH 7981
2024-02-08 05:27:16,417 Epoch 7981: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:27:16,418 EPOCH 7982
2024-02-08 05:27:32,525 Epoch 7982: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:27:32,526 EPOCH 7983
2024-02-08 05:27:48,917 Epoch 7983: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:27:48,918 EPOCH 7984
2024-02-08 05:28:04,963 Epoch 7984: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:28:04,964 EPOCH 7985
2024-02-08 05:28:21,253 Epoch 7985: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:28:21,254 EPOCH 7986
2024-02-08 05:28:37,297 Epoch 7986: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:28:37,299 EPOCH 7987
2024-02-08 05:28:53,243 Epoch 7987: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:28:53,244 EPOCH 7988
2024-02-08 05:29:09,561 Epoch 7988: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:29:09,561 EPOCH 7989
2024-02-08 05:29:19,561 [Epoch: 7989 Step: 00071900] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:      934 || Batch Translation Loss:   0.013894 => Txt Tokens per Sec:     2497 || Lr: 0.000050
2024-02-08 05:29:25,200 Epoch 7989: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-08 05:29:25,201 EPOCH 7990
2024-02-08 05:29:41,921 Epoch 7990: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-08 05:29:41,922 EPOCH 7991
2024-02-08 05:29:57,727 Epoch 7991: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-08 05:29:57,727 EPOCH 7992
2024-02-08 05:30:14,042 Epoch 7992: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-08 05:30:14,044 EPOCH 7993
2024-02-08 05:30:30,193 Epoch 7993: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.48 
2024-02-08 05:30:30,194 EPOCH 7994
2024-02-08 05:30:46,461 Epoch 7994: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.55 
2024-02-08 05:30:46,462 EPOCH 7995
2024-02-08 05:31:02,294 Epoch 7995: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.93 
2024-02-08 05:31:02,294 EPOCH 7996
2024-02-08 05:31:18,343 Epoch 7996: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-08 05:31:18,343 EPOCH 7997
2024-02-08 05:31:34,409 Epoch 7997: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-08 05:31:34,410 EPOCH 7998
2024-02-08 05:31:50,506 Epoch 7998: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-08 05:31:50,507 EPOCH 7999
2024-02-08 05:32:07,091 Epoch 7999: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-08 05:32:07,093 EPOCH 8000
2024-02-08 05:32:23,508 [Epoch: 8000 Step: 00072000] Batch Recognition Loss:   0.000707 => Gls Tokens per Sec:      647 || Batch Translation Loss:   0.013807 => Txt Tokens per Sec:     1790 || Lr: 0.000050
2024-02-08 05:33:31,895 Validation result at epoch 8000, step    72000: duration: 68.3860s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.28297	Translation Loss: 112334.33594	PPL: 74605.44531
	Eval Metric: BLEU
	WER 2.33	(DEL: 0.00,	INS: 0.00,	SUB: 2.33)
	BLEU-4 0.66	(BLEU-1: 10.10,	BLEU-2: 3.07,	BLEU-3: 1.24,	BLEU-4: 0.66)
	CHRF 16.61	ROUGE 8.63
2024-02-08 05:33:31,897 Logging Recognition and Translation Outputs
2024-02-08 05:33:31,897 ========================================================================================================================
2024-02-08 05:33:31,898 Logging Sequence: 101_92.00
2024-02-08 05:33:31,898 	Gloss Reference :	A B+C+D+E
2024-02-08 05:33:31,898 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 05:33:31,898 	Gloss Alignment :	         
2024-02-08 05:33:31,901 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 05:33:31,902 	Text Reference  :	**** **** india had    to  score 190  runs to    win       
2024-02-08 05:33:31,902 	Text Hypothesis :	that time a     scored icc was   only in   quick succession
2024-02-08 05:33:31,902 	Text Alignment  :	I    I    S     S      S   S     S    S    S     S         
2024-02-08 05:33:31,902 ========================================================================================================================
2024-02-08 05:33:31,902 Logging Sequence: 164_412.00
2024-02-08 05:33:31,903 	Gloss Reference :	A B+C+D+E
2024-02-08 05:33:31,903 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 05:33:31,903 	Gloss Alignment :	         
2024-02-08 05:33:31,903 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 05:33:31,904 	Text Reference  :	if you divide these two figures you will be shocked  to  know  that each ball's worth      is rs   50    lakhs  
2024-02-08 05:33:31,905 	Text Hypothesis :	** *** ****** these *** ******* *** **** ** rankings are based on   the  team's perfomance of also thank matches
2024-02-08 05:33:31,905 	Text Alignment  :	D  D   D            D   D       D   D    D  S        S   S     S    S    S      S          S  S    S     S      
2024-02-08 05:33:31,905 ========================================================================================================================
2024-02-08 05:33:31,905 Logging Sequence: 177_160.00
2024-02-08 05:33:31,905 	Gloss Reference :	A B+C+D+E
2024-02-08 05:33:31,906 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 05:33:31,906 	Gloss Alignment :	         
2024-02-08 05:33:31,906 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 05:33:31,907 	Text Reference  :	the police also          said that sushil had  asked  his friend to record a video
2024-02-08 05:33:31,907 	Text Hypothesis :	*** ****** unfortunately this the  teams  were broken his ****** ** ****** * help 
2024-02-08 05:33:31,907 	Text Alignment  :	D   D      S             S    S    S      S    S          D      D  D      D S    
2024-02-08 05:33:31,907 ========================================================================================================================
2024-02-08 05:33:31,907 Logging Sequence: 124_62.00
2024-02-08 05:33:31,907 	Gloss Reference :	A B+C+D+E
2024-02-08 05:33:31,908 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 05:33:31,908 	Gloss Alignment :	         
2024-02-08 05:33:31,908 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 05:33:31,909 	Text Reference  :	however dhoni has said that     he   will continue to   play   for the team  
2024-02-08 05:33:31,909 	Text Hypothesis :	******* ***** *** my   daughter were all  a        huge number of  his jersey
2024-02-08 05:33:31,909 	Text Alignment  :	D       D     D   S    S        S    S    S        S    S      S   S   S     
2024-02-08 05:33:31,909 ========================================================================================================================
2024-02-08 05:33:31,909 Logging Sequence: 71_149.00
2024-02-08 05:33:31,910 	Gloss Reference :	A B+C+D+E
2024-02-08 05:33:31,910 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 05:33:31,910 	Gloss Alignment :	         
2024-02-08 05:33:31,910 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 05:33:31,912 	Text Reference  :	his coach sanjay had   suggested his name  for  the ******** **** ** **** ** **** ** **** ** **** ** ** madhya pradesh ranji trophy team  
2024-02-08 05:33:31,913 	Text Hypothesis :	*** just  like   there was       a   doubt with the umpire's call of 2020 as late as 1206 am part of us know   what    the   family agreed
2024-02-08 05:33:31,913 	Text Alignment  :	D   S     S      S     S         S   S     S        I        I    I  I    I  I    I  I    I  I    I  I  S      S       S     S      S     
2024-02-08 05:33:31,913 ========================================================================================================================
2024-02-08 05:33:31,918 Epoch 8000: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-08 05:33:31,918 EPOCH 8001
2024-02-08 05:33:48,652 Epoch 8001: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-08 05:33:48,653 EPOCH 8002
2024-02-08 05:34:04,728 Epoch 8002: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 05:34:04,728 EPOCH 8003
2024-02-08 05:34:20,733 Epoch 8003: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-08 05:34:20,733 EPOCH 8004
2024-02-08 05:34:37,124 Epoch 8004: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 05:34:37,125 EPOCH 8005
2024-02-08 05:34:53,279 Epoch 8005: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 05:34:53,280 EPOCH 8006
2024-02-08 05:35:09,387 Epoch 8006: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-08 05:35:09,387 EPOCH 8007
2024-02-08 05:35:25,004 Epoch 8007: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 05:35:25,004 EPOCH 8008
2024-02-08 05:35:41,132 Epoch 8008: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 05:35:41,133 EPOCH 8009
2024-02-08 05:35:57,150 Epoch 8009: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 05:35:57,151 EPOCH 8010
2024-02-08 05:36:13,173 Epoch 8010: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 05:36:13,174 EPOCH 8011
2024-02-08 05:36:29,479 Epoch 8011: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 05:36:29,479 EPOCH 8012
2024-02-08 05:36:30,040 [Epoch: 8012 Step: 00072100] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:     2290 || Batch Translation Loss:   0.012614 => Txt Tokens per Sec:     6603 || Lr: 0.000050
2024-02-08 05:36:45,435 Epoch 8012: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 05:36:45,435 EPOCH 8013
2024-02-08 05:37:01,605 Epoch 8013: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 05:37:01,605 EPOCH 8014
2024-02-08 05:37:17,736 Epoch 8014: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 05:37:17,737 EPOCH 8015
2024-02-08 05:37:34,001 Epoch 8015: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 05:37:34,002 EPOCH 8016
2024-02-08 05:37:50,186 Epoch 8016: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 05:37:50,186 EPOCH 8017
2024-02-08 05:38:06,301 Epoch 8017: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 05:38:06,301 EPOCH 8018
2024-02-08 05:38:22,662 Epoch 8018: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:38:22,663 EPOCH 8019
2024-02-08 05:38:38,776 Epoch 8019: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 05:38:38,776 EPOCH 8020
2024-02-08 05:38:54,710 Epoch 8020: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:38:54,710 EPOCH 8021
2024-02-08 05:39:10,461 Epoch 8021: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:39:10,462 EPOCH 8022
2024-02-08 05:39:26,766 Epoch 8022: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:39:26,767 EPOCH 8023
2024-02-08 05:39:31,402 [Epoch: 8023 Step: 00072200] Batch Recognition Loss:   0.000479 => Gls Tokens per Sec:      358 || Batch Translation Loss:   0.005055 => Txt Tokens per Sec:      787 || Lr: 0.000050
2024-02-08 05:39:43,009 Epoch 8023: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:39:43,010 EPOCH 8024
2024-02-08 05:39:59,120 Epoch 8024: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:39:59,121 EPOCH 8025
2024-02-08 05:40:14,909 Epoch 8025: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:40:14,910 EPOCH 8026
2024-02-08 05:40:31,126 Epoch 8026: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 05:40:31,127 EPOCH 8027
2024-02-08 05:40:47,319 Epoch 8027: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:40:47,319 EPOCH 8028
2024-02-08 05:41:03,532 Epoch 8028: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:41:03,533 EPOCH 8029
2024-02-08 05:41:19,464 Epoch 8029: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:41:19,464 EPOCH 8030
2024-02-08 05:41:35,413 Epoch 8030: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:41:35,413 EPOCH 8031
2024-02-08 05:41:51,051 Epoch 8031: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:41:51,052 EPOCH 8032
2024-02-08 05:42:07,220 Epoch 8032: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.08 
2024-02-08 05:42:07,221 EPOCH 8033
2024-02-08 05:42:23,399 Epoch 8033: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:42:23,400 EPOCH 8034
2024-02-08 05:42:27,640 [Epoch: 8034 Step: 00072300] Batch Recognition Loss:   0.000457 => Gls Tokens per Sec:      906 || Batch Translation Loss:   0.016004 => Txt Tokens per Sec:     2714 || Lr: 0.000050
2024-02-08 05:42:39,381 Epoch 8034: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:42:39,382 EPOCH 8035
2024-02-08 05:42:55,429 Epoch 8035: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:42:55,429 EPOCH 8036
2024-02-08 05:43:11,302 Epoch 8036: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:43:11,302 EPOCH 8037
2024-02-08 05:43:27,272 Epoch 8037: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:43:27,273 EPOCH 8038
2024-02-08 05:43:43,076 Epoch 8038: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:43:43,077 EPOCH 8039
2024-02-08 05:43:59,080 Epoch 8039: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:43:59,080 EPOCH 8040
2024-02-08 05:44:15,543 Epoch 8040: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:44:15,543 EPOCH 8041
2024-02-08 05:44:31,516 Epoch 8041: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:44:31,516 EPOCH 8042
2024-02-08 05:44:47,657 Epoch 8042: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:44:47,658 EPOCH 8043
2024-02-08 05:45:03,653 Epoch 8043: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:45:03,654 EPOCH 8044
2024-02-08 05:45:19,558 Epoch 8044: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:45:19,559 EPOCH 8045
2024-02-08 05:45:28,056 [Epoch: 8045 Step: 00072400] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:      497 || Batch Translation Loss:   0.008439 => Txt Tokens per Sec:     1488 || Lr: 0.000050
2024-02-08 05:45:35,475 Epoch 8045: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:45:35,476 EPOCH 8046
2024-02-08 05:45:51,909 Epoch 8046: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:45:51,909 EPOCH 8047
2024-02-08 05:46:07,841 Epoch 8047: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:46:07,842 EPOCH 8048
2024-02-08 05:46:23,986 Epoch 8048: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:46:23,986 EPOCH 8049
2024-02-08 05:46:39,958 Epoch 8049: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:46:39,958 EPOCH 8050
2024-02-08 05:46:56,184 Epoch 8050: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:46:56,184 EPOCH 8051
2024-02-08 05:47:11,962 Epoch 8051: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:47:11,962 EPOCH 8052
2024-02-08 05:47:28,556 Epoch 8052: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:47:28,556 EPOCH 8053
2024-02-08 05:47:44,716 Epoch 8053: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:47:44,717 EPOCH 8054
2024-02-08 05:48:00,610 Epoch 8054: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:48:00,611 EPOCH 8055
2024-02-08 05:48:16,664 Epoch 8055: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:48:16,665 EPOCH 8056
2024-02-08 05:48:26,665 [Epoch: 8056 Step: 00072500] Batch Recognition Loss:   0.000259 => Gls Tokens per Sec:      640 || Batch Translation Loss:   0.010892 => Txt Tokens per Sec:     1691 || Lr: 0.000050
2024-02-08 05:48:32,678 Epoch 8056: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:48:32,678 EPOCH 8057
2024-02-08 05:48:48,757 Epoch 8057: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:48:48,758 EPOCH 8058
2024-02-08 05:49:04,597 Epoch 8058: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:49:04,598 EPOCH 8059
2024-02-08 05:49:20,773 Epoch 8059: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:49:20,774 EPOCH 8060
2024-02-08 05:49:37,305 Epoch 8060: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-08 05:49:37,306 EPOCH 8061
2024-02-08 05:49:53,143 Epoch 8061: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.09 
2024-02-08 05:49:53,143 EPOCH 8062
2024-02-08 05:50:08,996 Epoch 8062: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.09 
2024-02-08 05:50:08,997 EPOCH 8063
2024-02-08 05:50:25,267 Epoch 8063: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:50:25,268 EPOCH 8064
2024-02-08 05:50:41,368 Epoch 8064: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 05:50:41,369 EPOCH 8065
2024-02-08 05:50:57,409 Epoch 8065: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:50:57,409 EPOCH 8066
2024-02-08 05:51:13,745 Epoch 8066: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:51:13,745 EPOCH 8067
2024-02-08 05:51:18,685 [Epoch: 8067 Step: 00072600] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     1555 || Batch Translation Loss:   0.007338 => Txt Tokens per Sec:     3990 || Lr: 0.000050
2024-02-08 05:51:29,568 Epoch 8067: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:51:29,569 EPOCH 8068
2024-02-08 05:51:45,640 Epoch 8068: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:51:45,640 EPOCH 8069
2024-02-08 05:52:01,546 Epoch 8069: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:52:01,547 EPOCH 8070
2024-02-08 05:52:17,809 Epoch 8070: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:52:17,809 EPOCH 8071
2024-02-08 05:52:33,962 Epoch 8071: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:52:33,962 EPOCH 8072
2024-02-08 05:52:50,171 Epoch 8072: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:52:50,171 EPOCH 8073
2024-02-08 05:53:06,416 Epoch 8073: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 05:53:06,417 EPOCH 8074
2024-02-08 05:53:22,680 Epoch 8074: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:53:22,680 EPOCH 8075
2024-02-08 05:53:38,474 Epoch 8075: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:53:38,474 EPOCH 8076
2024-02-08 05:53:54,529 Epoch 8076: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:53:54,529 EPOCH 8077
2024-02-08 05:54:10,431 Epoch 8077: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:54:10,432 EPOCH 8078
2024-02-08 05:54:20,055 [Epoch: 8078 Step: 00072700] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:      838 || Batch Translation Loss:   0.009505 => Txt Tokens per Sec:     2212 || Lr: 0.000050
2024-02-08 05:54:26,442 Epoch 8078: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:54:26,442 EPOCH 8079
2024-02-08 05:54:42,327 Epoch 8079: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:54:42,327 EPOCH 8080
2024-02-08 05:54:58,141 Epoch 8080: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:54:58,141 EPOCH 8081
2024-02-08 05:55:14,582 Epoch 8081: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:55:14,583 EPOCH 8082
2024-02-08 05:55:30,521 Epoch 8082: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:55:30,522 EPOCH 8083
2024-02-08 05:55:46,750 Epoch 8083: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:55:46,751 EPOCH 8084
2024-02-08 05:56:02,982 Epoch 8084: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 05:56:02,983 EPOCH 8085
2024-02-08 05:56:19,345 Epoch 8085: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 05:56:19,345 EPOCH 8086
2024-02-08 05:56:35,487 Epoch 8086: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 05:56:35,488 EPOCH 8087
2024-02-08 05:56:51,679 Epoch 8087: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-08 05:56:51,680 EPOCH 8088
2024-02-08 05:57:07,775 Epoch 8088: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 05:57:07,776 EPOCH 8089
2024-02-08 05:57:23,037 [Epoch: 8089 Step: 00072800] Batch Recognition Loss:   0.000114 => Gls Tokens per Sec:      612 || Batch Translation Loss:   0.029604 => Txt Tokens per Sec:     1682 || Lr: 0.000050
2024-02-08 05:57:23,556 Epoch 8089: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 05:57:23,556 EPOCH 8090
2024-02-08 05:57:39,826 Epoch 8090: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 05:57:39,827 EPOCH 8091
2024-02-08 05:57:55,925 Epoch 8091: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 05:57:55,925 EPOCH 8092
2024-02-08 05:58:12,111 Epoch 8092: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 05:58:12,112 EPOCH 8093
2024-02-08 05:58:28,124 Epoch 8093: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 05:58:28,125 EPOCH 8094
2024-02-08 05:58:43,810 Epoch 8094: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 05:58:43,810 EPOCH 8095
2024-02-08 05:58:59,600 Epoch 8095: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 05:58:59,601 EPOCH 8096
2024-02-08 05:59:16,025 Epoch 8096: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 05:59:16,025 EPOCH 8097
2024-02-08 05:59:32,061 Epoch 8097: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 05:59:32,061 EPOCH 8098
2024-02-08 05:59:48,636 Epoch 8098: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 05:59:48,636 EPOCH 8099
2024-02-08 06:00:04,480 Epoch 8099: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-08 06:00:04,480 EPOCH 8100
2024-02-08 06:00:21,131 [Epoch: 8100 Step: 00072900] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:      638 || Batch Translation Loss:   0.009293 => Txt Tokens per Sec:     1765 || Lr: 0.000050
2024-02-08 06:00:21,132 Epoch 8100: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-08 06:00:21,132 EPOCH 8101
2024-02-08 06:00:37,139 Epoch 8101: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-08 06:00:37,141 EPOCH 8102
2024-02-08 06:00:53,500 Epoch 8102: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 06:00:53,500 EPOCH 8103
2024-02-08 06:01:09,424 Epoch 8103: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 06:01:09,424 EPOCH 8104
2024-02-08 06:01:25,881 Epoch 8104: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 06:01:25,882 EPOCH 8105
2024-02-08 06:01:41,828 Epoch 8105: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 06:01:41,828 EPOCH 8106
2024-02-08 06:01:57,675 Epoch 8106: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 06:01:57,676 EPOCH 8107
2024-02-08 06:02:13,935 Epoch 8107: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 06:02:13,936 EPOCH 8108
2024-02-08 06:02:30,070 Epoch 8108: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 06:02:30,071 EPOCH 8109
2024-02-08 06:02:46,124 Epoch 8109: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 06:02:46,124 EPOCH 8110
2024-02-08 06:03:02,565 Epoch 8110: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 06:03:02,566 EPOCH 8111
2024-02-08 06:03:18,552 Epoch 8111: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 06:03:18,553 EPOCH 8112
2024-02-08 06:03:24,148 [Epoch: 8112 Step: 00073000] Batch Recognition Loss:   0.000289 => Gls Tokens per Sec:      229 || Batch Translation Loss:   0.013872 => Txt Tokens per Sec:      790 || Lr: 0.000050
2024-02-08 06:03:34,660 Epoch 8112: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 06:03:34,661 EPOCH 8113
2024-02-08 06:03:50,783 Epoch 8113: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 06:03:50,784 EPOCH 8114
2024-02-08 06:04:07,027 Epoch 8114: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 06:04:07,028 EPOCH 8115
2024-02-08 06:04:22,705 Epoch 8115: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 06:04:22,706 EPOCH 8116
2024-02-08 06:04:39,102 Epoch 8116: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 06:04:39,102 EPOCH 8117
2024-02-08 06:04:55,206 Epoch 8117: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 06:04:55,206 EPOCH 8118
2024-02-08 06:05:11,344 Epoch 8118: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 06:05:11,344 EPOCH 8119
2024-02-08 06:05:27,344 Epoch 8119: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 06:05:27,345 EPOCH 8120
2024-02-08 06:05:43,365 Epoch 8120: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-08 06:05:43,365 EPOCH 8121
2024-02-08 06:05:59,258 Epoch 8121: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-08 06:05:59,259 EPOCH 8122
2024-02-08 06:06:15,208 Epoch 8122: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-08 06:06:15,208 EPOCH 8123
2024-02-08 06:06:15,827 [Epoch: 8123 Step: 00073100] Batch Recognition Loss:   0.000225 => Gls Tokens per Sec:     4149 || Batch Translation Loss:   0.017953 => Txt Tokens per Sec:     9540 || Lr: 0.000050
2024-02-08 06:06:31,300 Epoch 8123: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-08 06:06:31,301 EPOCH 8124
2024-02-08 06:06:47,920 Epoch 8124: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-08 06:06:47,920 EPOCH 8125
2024-02-08 06:07:03,999 Epoch 8125: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-08 06:07:03,999 EPOCH 8126
2024-02-08 06:07:19,824 Epoch 8126: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-08 06:07:19,825 EPOCH 8127
2024-02-08 06:07:36,039 Epoch 8127: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-08 06:07:36,040 EPOCH 8128
2024-02-08 06:07:52,104 Epoch 8128: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-08 06:07:52,104 EPOCH 8129
2024-02-08 06:08:08,057 Epoch 8129: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-08 06:08:08,058 EPOCH 8130
2024-02-08 06:08:24,449 Epoch 8130: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-08 06:08:24,449 EPOCH 8131
2024-02-08 06:08:40,520 Epoch 8131: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-08 06:08:40,520 EPOCH 8132
2024-02-08 06:08:56,588 Epoch 8132: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-08 06:08:56,589 EPOCH 8133
2024-02-08 06:09:12,725 Epoch 8133: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-08 06:09:12,725 EPOCH 8134
2024-02-08 06:09:23,333 [Epoch: 8134 Step: 00073200] Batch Recognition Loss:   0.000347 => Gls Tokens per Sec:      277 || Batch Translation Loss:   0.284586 => Txt Tokens per Sec:      851 || Lr: 0.000050
2024-02-08 06:09:29,121 Epoch 8134: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.45 
2024-02-08 06:09:29,121 EPOCH 8135
2024-02-08 06:09:45,183 Epoch 8135: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.82 
2024-02-08 06:09:45,183 EPOCH 8136
2024-02-08 06:10:01,494 Epoch 8136: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.58 
2024-02-08 06:10:01,494 EPOCH 8137
2024-02-08 06:10:17,551 Epoch 8137: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.65 
2024-02-08 06:10:17,552 EPOCH 8138
2024-02-08 06:10:33,964 Epoch 8138: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.03 
2024-02-08 06:10:33,965 EPOCH 8139
2024-02-08 06:10:49,971 Epoch 8139: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-08 06:10:49,972 EPOCH 8140
2024-02-08 06:11:06,078 Epoch 8140: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-08 06:11:06,078 EPOCH 8141
2024-02-08 06:11:22,541 Epoch 8141: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-08 06:11:22,541 EPOCH 8142
2024-02-08 06:11:38,411 Epoch 8142: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-08 06:11:38,411 EPOCH 8143
2024-02-08 06:11:54,560 Epoch 8143: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-08 06:11:54,560 EPOCH 8144
2024-02-08 06:12:10,434 Epoch 8144: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-08 06:12:10,434 EPOCH 8145
2024-02-08 06:12:18,638 [Epoch: 8145 Step: 00073300] Batch Recognition Loss:   0.000382 => Gls Tokens per Sec:      514 || Batch Translation Loss:   0.011196 => Txt Tokens per Sec:     1439 || Lr: 0.000050
2024-02-08 06:12:26,306 Epoch 8145: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 06:12:26,306 EPOCH 8146
2024-02-08 06:12:42,462 Epoch 8146: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-08 06:12:42,463 EPOCH 8147
2024-02-08 06:12:58,582 Epoch 8147: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-08 06:12:58,583 EPOCH 8148
2024-02-08 06:13:14,483 Epoch 8148: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 06:13:14,484 EPOCH 8149
2024-02-08 06:13:30,635 Epoch 8149: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 06:13:30,636 EPOCH 8150
2024-02-08 06:13:46,577 Epoch 8150: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 06:13:46,578 EPOCH 8151
2024-02-08 06:14:02,851 Epoch 8151: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-08 06:14:02,852 EPOCH 8152
2024-02-08 06:14:19,026 Epoch 8152: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-08 06:14:19,027 EPOCH 8153
2024-02-08 06:14:35,065 Epoch 8153: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 06:14:35,066 EPOCH 8154
2024-02-08 06:14:50,952 Epoch 8154: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 06:14:50,952 EPOCH 8155
2024-02-08 06:15:07,192 Epoch 8155: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 06:15:07,193 EPOCH 8156
2024-02-08 06:15:21,741 [Epoch: 8156 Step: 00073400] Batch Recognition Loss:   0.000422 => Gls Tokens per Sec:      378 || Batch Translation Loss:   0.014230 => Txt Tokens per Sec:     1182 || Lr: 0.000050
2024-02-08 06:15:23,240 Epoch 8156: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-08 06:15:23,241 EPOCH 8157
2024-02-08 06:15:39,280 Epoch 8157: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 06:15:39,280 EPOCH 8158
2024-02-08 06:15:55,356 Epoch 8158: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 06:15:55,357 EPOCH 8159
2024-02-08 06:16:11,059 Epoch 8159: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 06:16:11,060 EPOCH 8160
2024-02-08 06:16:27,436 Epoch 8160: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 06:16:27,437 EPOCH 8161
2024-02-08 06:16:43,638 Epoch 8161: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 06:16:43,639 EPOCH 8162
2024-02-08 06:16:59,710 Epoch 8162: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 06:16:59,711 EPOCH 8163
2024-02-08 06:17:15,793 Epoch 8163: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 06:17:15,794 EPOCH 8164
2024-02-08 06:17:31,801 Epoch 8164: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 06:17:31,802 EPOCH 8165
2024-02-08 06:17:47,932 Epoch 8165: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 06:17:47,933 EPOCH 8166
2024-02-08 06:18:04,059 Epoch 8166: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 06:18:04,059 EPOCH 8167
2024-02-08 06:18:14,621 [Epoch: 8167 Step: 00073500] Batch Recognition Loss:   0.000310 => Gls Tokens per Sec:      727 || Batch Translation Loss:   0.012186 => Txt Tokens per Sec:     1950 || Lr: 0.000050
2024-02-08 06:18:20,023 Epoch 8167: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 06:18:20,023 EPOCH 8168
2024-02-08 06:18:35,862 Epoch 8168: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 06:18:35,862 EPOCH 8169
2024-02-08 06:18:51,872 Epoch 8169: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 06:18:51,872 EPOCH 8170
2024-02-08 06:19:08,072 Epoch 8170: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 06:19:08,072 EPOCH 8171
2024-02-08 06:19:24,088 Epoch 8171: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 06:19:24,088 EPOCH 8172
2024-02-08 06:19:40,368 Epoch 8172: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 06:19:40,369 EPOCH 8173
2024-02-08 06:19:56,419 Epoch 8173: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 06:19:56,420 EPOCH 8174
2024-02-08 06:20:12,524 Epoch 8174: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 06:20:12,524 EPOCH 8175
2024-02-08 06:20:28,389 Epoch 8175: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 06:20:28,390 EPOCH 8176
2024-02-08 06:20:44,372 Epoch 8176: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 06:20:44,373 EPOCH 8177
2024-02-08 06:21:00,157 Epoch 8177: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 06:21:00,158 EPOCH 8178
2024-02-08 06:21:12,534 [Epoch: 8178 Step: 00073600] Batch Recognition Loss:   0.000674 => Gls Tokens per Sec:      651 || Batch Translation Loss:   0.004582 => Txt Tokens per Sec:     1744 || Lr: 0.000050
2024-02-08 06:21:16,295 Epoch 8178: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 06:21:16,296 EPOCH 8179
2024-02-08 06:21:32,753 Epoch 8179: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 06:21:32,754 EPOCH 8180
2024-02-08 06:21:49,164 Epoch 8180: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 06:21:49,165 EPOCH 8181
2024-02-08 06:22:05,231 Epoch 8181: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 06:22:05,231 EPOCH 8182
2024-02-08 06:22:21,552 Epoch 8182: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 06:22:21,553 EPOCH 8183
2024-02-08 06:22:37,609 Epoch 8183: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 06:22:37,609 EPOCH 8184
2024-02-08 06:22:53,980 Epoch 8184: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 06:22:53,981 EPOCH 8185
2024-02-08 06:23:09,983 Epoch 8185: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 06:23:09,983 EPOCH 8186
2024-02-08 06:23:25,945 Epoch 8186: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.09 
2024-02-08 06:23:25,945 EPOCH 8187
2024-02-08 06:23:42,041 Epoch 8187: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 06:23:42,042 EPOCH 8188
2024-02-08 06:23:58,340 Epoch 8188: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 06:23:58,341 EPOCH 8189
2024-02-08 06:24:10,997 [Epoch: 8189 Step: 00073700] Batch Recognition Loss:   0.000178 => Gls Tokens per Sec:      738 || Batch Translation Loss:   0.012068 => Txt Tokens per Sec:     1996 || Lr: 0.000050
2024-02-08 06:24:14,216 Epoch 8189: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 06:24:14,217 EPOCH 8190
2024-02-08 06:24:30,693 Epoch 8190: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 06:24:30,693 EPOCH 8191
2024-02-08 06:24:46,715 Epoch 8191: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 06:24:46,716 EPOCH 8192
2024-02-08 06:25:02,900 Epoch 8192: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 06:25:02,901 EPOCH 8193
2024-02-08 06:25:19,277 Epoch 8193: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 06:25:19,278 EPOCH 8194
2024-02-08 06:25:35,473 Epoch 8194: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 06:25:35,473 EPOCH 8195
2024-02-08 06:25:51,648 Epoch 8195: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-08 06:25:51,648 EPOCH 8196
2024-02-08 06:26:07,858 Epoch 8196: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 06:26:07,858 EPOCH 8197
2024-02-08 06:26:23,849 Epoch 8197: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 06:26:23,850 EPOCH 8198
2024-02-08 06:26:39,865 Epoch 8198: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 06:26:39,866 EPOCH 8199
2024-02-08 06:26:55,980 Epoch 8199: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-08 06:26:55,980 EPOCH 8200
2024-02-08 06:27:12,330 [Epoch: 8200 Step: 00073800] Batch Recognition Loss:   0.000193 => Gls Tokens per Sec:      650 || Batch Translation Loss:   0.027584 => Txt Tokens per Sec:     1797 || Lr: 0.000050
2024-02-08 06:27:12,331 Epoch 8200: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-08 06:27:12,331 EPOCH 8201
2024-02-08 06:27:28,295 Epoch 8201: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 06:27:28,295 EPOCH 8202
2024-02-08 06:27:44,900 Epoch 8202: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 06:27:44,900 EPOCH 8203
2024-02-08 06:28:00,669 Epoch 8203: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 06:28:00,669 EPOCH 8204
2024-02-08 06:28:16,619 Epoch 8204: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 06:28:16,620 EPOCH 8205
2024-02-08 06:28:32,588 Epoch 8205: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 06:28:32,589 EPOCH 8206
2024-02-08 06:28:48,782 Epoch 8206: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 06:28:48,783 EPOCH 8207
2024-02-08 06:29:04,655 Epoch 8207: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 06:29:04,656 EPOCH 8208
2024-02-08 06:29:20,666 Epoch 8208: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 06:29:20,667 EPOCH 8209
2024-02-08 06:29:36,804 Epoch 8209: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 06:29:36,805 EPOCH 8210
2024-02-08 06:29:52,888 Epoch 8210: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 06:29:52,889 EPOCH 8211
2024-02-08 06:30:09,249 Epoch 8211: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 06:30:09,249 EPOCH 8212
2024-02-08 06:30:09,853 [Epoch: 8212 Step: 00073900] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:     2123 || Batch Translation Loss:   0.017257 => Txt Tokens per Sec:     6435 || Lr: 0.000050
2024-02-08 06:30:25,365 Epoch 8212: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 06:30:25,366 EPOCH 8213
2024-02-08 06:30:41,678 Epoch 8213: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 06:30:41,678 EPOCH 8214
2024-02-08 06:30:57,733 Epoch 8214: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 06:30:57,733 EPOCH 8215
2024-02-08 06:31:13,900 Epoch 8215: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 06:31:13,901 EPOCH 8216
2024-02-08 06:31:30,105 Epoch 8216: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 06:31:30,106 EPOCH 8217
2024-02-08 06:31:46,291 Epoch 8217: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 06:31:46,292 EPOCH 8218
2024-02-08 06:32:02,154 Epoch 8218: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 06:32:02,155 EPOCH 8219
2024-02-08 06:32:18,439 Epoch 8219: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 06:32:18,439 EPOCH 8220
2024-02-08 06:32:34,568 Epoch 8220: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 06:32:34,568 EPOCH 8221
2024-02-08 06:32:50,521 Epoch 8221: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-08 06:32:50,522 EPOCH 8222
2024-02-08 06:33:06,805 Epoch 8222: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-08 06:33:06,805 EPOCH 8223
2024-02-08 06:33:10,230 [Epoch: 8223 Step: 00074000] Batch Recognition Loss:   0.000175 => Gls Tokens per Sec:      748 || Batch Translation Loss:   0.013234 => Txt Tokens per Sec:     1880 || Lr: 0.000050
2024-02-08 06:34:18,312 Validation result at epoch 8223, step    74000: duration: 68.0823s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.28588	Translation Loss: 112910.52344	PPL: 79024.96875
	Eval Metric: BLEU
	WER 2.40	(DEL: 0.00,	INS: 0.00,	SUB: 2.40)
	BLEU-4 0.45	(BLEU-1: 9.60,	BLEU-2: 2.72,	BLEU-3: 0.98,	BLEU-4: 0.45)
	CHRF 16.48	ROUGE 8.02
2024-02-08 06:34:18,315 Logging Recognition and Translation Outputs
2024-02-08 06:34:18,315 ========================================================================================================================
2024-02-08 06:34:18,315 Logging Sequence: 77_172.00
2024-02-08 06:34:18,315 	Gloss Reference :	A B+C+D+E
2024-02-08 06:34:18,315 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 06:34:18,315 	Gloss Alignment :	         
2024-02-08 06:34:18,316 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 06:34:18,319 	Text Reference  :	he scored 2  runs on   the    fourth ball  on   the 5th 6th jadeja scored a 6 and a boundary respectively
2024-02-08 06:34:18,319 	Text Hypothesis :	** as     we have many medals ms     dhoni with the *** *** ****** ****** * * *** * brand    ambassador  
2024-02-08 06:34:18,319 	Text Alignment  :	D  S      S  S    S    S      S      S     S        D   D   D      D      D D D   D S        S           
2024-02-08 06:34:18,319 ========================================================================================================================
2024-02-08 06:34:18,319 Logging Sequence: 173_39.00
2024-02-08 06:34:18,319 	Gloss Reference :	A B+C+D+E
2024-02-08 06:34:18,320 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 06:34:18,320 	Gloss Alignment :	         
2024-02-08 06:34:18,320 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 06:34:18,320 	Text Reference  :	********* kohli will step down as          india' captain
2024-02-08 06:34:18,321 	Text Hypothesis :	similarly there was  a    huge controversy ms     dhoni  
2024-02-08 06:34:18,321 	Text Alignment  :	I         S     S    S    S    S           S      S      
2024-02-08 06:34:18,321 ========================================================================================================================
2024-02-08 06:34:18,321 Logging Sequence: 138_224.00
2024-02-08 06:34:18,321 	Gloss Reference :	A B+C+D+E
2024-02-08 06:34:18,321 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 06:34:18,322 	Gloss Alignment :	         
2024-02-08 06:34:18,322 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 06:34:18,323 	Text Reference  :	then people wrote positive messages and  stuck them     on the ******** plastic sheets    
2024-02-08 06:34:18,323 	Text Hypothesis :	**** ****** ***** and      the      goal is    defended by the opposing team's  goalkeeper
2024-02-08 06:34:18,323 	Text Alignment  :	D    D      D     S        S        S    S     S        S      I        S       S         
2024-02-08 06:34:18,323 ========================================================================================================================
2024-02-08 06:34:18,323 Logging Sequence: 128_98.00
2024-02-08 06:34:18,323 	Gloss Reference :	A B+C+D+E
2024-02-08 06:34:18,324 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 06:34:18,324 	Gloss Alignment :	         
2024-02-08 06:34:18,324 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 06:34:18,325 	Text Reference  :	with 8 wickets and 43  balls remaining they     won the      match in such  a   short time  
2024-02-08 06:34:18,325 	Text Hypothesis :	**** * ******* and was a     huge      surprise for everyone as    he still the huge  dating
2024-02-08 06:34:18,325 	Text Alignment  :	D    D D           S   S     S         S        S   S        S     S  S     S   S     S     
2024-02-08 06:34:18,325 ========================================================================================================================
2024-02-08 06:34:18,326 Logging Sequence: 126_159.00
2024-02-08 06:34:18,326 	Gloss Reference :	A B+C+D+E
2024-02-08 06:34:18,326 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 06:34:18,326 	Gloss Alignment :	         
2024-02-08 06:34:18,326 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 06:34:18,327 	Text Reference  :	despite multiple challenges and   injuries you     did   not give   up      
2024-02-08 06:34:18,327 	Text Hypothesis :	******* now      athletes   could not      winning while the entire olympics
2024-02-08 06:34:18,327 	Text Alignment  :	D       S        S          S     S        S       S     S   S      S       
2024-02-08 06:34:18,327 ========================================================================================================================
2024-02-08 06:34:18,331 Training ended since there were no improvements inthe last learning rate step: 0.000050
2024-02-08 06:34:18,334 Best validation result at step    22000:   0.86 eval_metric.
2024-02-08 06:34:44,245 ------------------------------------------------------------
2024-02-08 06:34:44,245 [DEV] partition [RECOGNITION] experiment [BW]: 1
2024-02-08 06:35:52,081 finished in 67.8334s 
2024-02-08 06:35:52,084 ************************************************************
2024-02-08 06:35:52,084 [DEV] partition [RECOGNITION] results:
	New Best CTC Decode Beam Size: 1
	WER 3.88	(DEL: 0.00,	INS: 0.00,	SUB: 3.88)
2024-02-08 06:35:52,084 ************************************************************
2024-02-08 06:35:52,084 ------------------------------------------------------------
2024-02-08 06:35:52,085 [DEV] partition [RECOGNITION] experiment [BW]: 2
2024-02-08 06:36:59,491 finished in 67.4058s 
2024-02-08 06:36:59,493 ************************************************************
2024-02-08 06:36:59,493 [DEV] partition [RECOGNITION] results:
	New Best CTC Decode Beam Size: 2
	WER 3.81	(DEL: 0.00,	INS: 0.00,	SUB: 3.81)
2024-02-08 06:36:59,493 ************************************************************
2024-02-08 06:36:59,493 ------------------------------------------------------------
2024-02-08 06:36:59,493 [DEV] partition [RECOGNITION] experiment [BW]: 3
2024-02-08 06:38:06,970 finished in 67.4772s 
2024-02-08 06:38:06,972 ------------------------------------------------------------
2024-02-08 06:38:06,972 [DEV] partition [RECOGNITION] experiment [BW]: 4
2024-02-08 06:39:14,459 finished in 67.4864s 
2024-02-08 06:39:14,460 ------------------------------------------------------------
2024-02-08 06:39:14,460 [DEV] partition [RECOGNITION] experiment [BW]: 5
2024-02-08 06:40:21,732 finished in 67.2717s 
2024-02-08 06:40:21,734 ------------------------------------------------------------
2024-02-08 06:40:21,734 [DEV] partition [RECOGNITION] experiment [BW]: 6
2024-02-08 06:41:29,200 finished in 67.4649s 
2024-02-08 06:41:29,201 ------------------------------------------------------------
2024-02-08 06:41:29,201 [DEV] partition [RECOGNITION] experiment [BW]: 7
2024-02-08 06:42:36,532 finished in 67.3316s 
2024-02-08 06:42:36,534 ------------------------------------------------------------
2024-02-08 06:42:36,534 [DEV] partition [RECOGNITION] experiment [BW]: 8
2024-02-08 06:43:44,002 finished in 67.4663s 
2024-02-08 06:43:44,003 ------------------------------------------------------------
2024-02-08 06:43:44,003 [DEV] partition [RECOGNITION] experiment [BW]: 9
2024-02-08 06:44:51,458 finished in 67.4555s 
2024-02-08 06:44:51,459 ------------------------------------------------------------
2024-02-08 06:44:51,460 [DEV] partition [RECOGNITION] experiment [BW]: 10
2024-02-08 06:45:58,804 finished in 67.3441s 
2024-02-08 06:45:58,806 ============================================================
2024-02-08 06:47:06,007 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 1 and Alpha: -1
	BLEU-4 0.86	(BLEU-1: 11.24,	BLEU-2: 3.76,	BLEU-3: 1.63,	BLEU-4: 0.86)
	CHRF 17.26	ROUGE 9.55
2024-02-08 06:47:06,008 ------------------------------------------------------------
2024-02-08 11:18:04,314 ************************************************************
2024-02-08 11:18:04,318 [DEV] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 2
	Best Translation Beam Size: 1 and Alpha: -1
	WER 3.81	(DEL: 0.00,	INS: 0.00,	SUB: 3.81)
	BLEU-4 0.86	(BLEU-1: 11.24,	BLEU-2: 3.76,	BLEU-3: 1.63,	BLEU-4: 0.86)
	CHRF 17.26	ROUGE 9.55
2024-02-08 11:18:04,319 ************************************************************
2024-02-08 11:19:25,814 [TEST] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 2
	Best Translation Beam Size: 1 and Alpha: -1
	WER 3.39	(DEL: 0.00,	INS: 0.00,	SUB: 3.39)
	BLEU-4 0.62	(BLEU-1: 10.53,	BLEU-2: 3.37,	BLEU-3: 1.28,	BLEU-4: 0.62)
	CHRF 17.27	ROUGE 8.75
2024-02-08 11:19:25,816 ************************************************************
