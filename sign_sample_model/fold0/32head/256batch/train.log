2024-02-01 09:15:02,512 Hello! This is Joey-NMT.
2024-02-01 09:15:02,518 Total params: 25639944
2024-02-01 09:15:02,519 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'gloss_output_layer.bias', 'gloss_output_layer.weight', 'sgn_embed.ln.bias', 'sgn_embed.ln.weight', 'sgn_embed.norm.norm.bias', 'sgn_embed.norm.norm.weight', 'txt_embed.norm.norm.bias', 'txt_embed.norm.norm.weight']
2024-02-01 09:15:03,583 cfg.name                           : sign_experiment
2024-02-01 09:15:03,583 cfg.data.data_path                 : ./data/Sports_dataset/0/
2024-02-01 09:15:03,584 cfg.data.version                   : phoenix_2014_trans
2024-02-01 09:15:03,584 cfg.data.sgn                       : sign
2024-02-01 09:15:03,584 cfg.data.txt                       : text
2024-02-01 09:15:03,584 cfg.data.gls                       : gloss
2024-02-01 09:15:03,584 cfg.data.train                     : excel_data.train
2024-02-01 09:15:03,584 cfg.data.dev                       : excel_data.dev
2024-02-01 09:15:03,584 cfg.data.test                      : excel_data.test
2024-02-01 09:15:03,585 cfg.data.feature_size              : 2560
2024-02-01 09:15:03,585 cfg.data.level                     : word
2024-02-01 09:15:03,585 cfg.data.txt_lowercase             : True
2024-02-01 09:15:03,585 cfg.data.max_sent_length           : 500
2024-02-01 09:15:03,585 cfg.data.random_train_subset       : -1
2024-02-01 09:15:03,585 cfg.data.random_dev_subset         : -1
2024-02-01 09:15:03,585 cfg.testing.recognition_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-01 09:15:03,585 cfg.testing.translation_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-01 09:15:03,585 cfg.testing.translation_beam_alphas : [-1, 0, 1, 2, 3, 4, 5]
2024-02-01 09:15:03,586 cfg.training.reset_best_ckpt       : False
2024-02-01 09:15:03,586 cfg.training.reset_scheduler       : False
2024-02-01 09:15:03,586 cfg.training.reset_optimizer       : False
2024-02-01 09:15:03,586 cfg.training.random_seed           : 42
2024-02-01 09:15:03,586 cfg.training.model_dir             : ./sign_sample_model/fold0/32head/256batch
2024-02-01 09:15:03,586 cfg.training.recognition_loss_weight : 1.0
2024-02-01 09:15:03,586 cfg.training.translation_loss_weight : 1.0
2024-02-01 09:15:03,586 cfg.training.eval_metric           : bleu
2024-02-01 09:15:03,587 cfg.training.optimizer             : adam
2024-02-01 09:15:03,587 cfg.training.learning_rate         : 0.0001
2024-02-01 09:15:03,587 cfg.training.batch_size            : 256
2024-02-01 09:15:03,587 cfg.training.num_valid_log         : 5
2024-02-01 09:15:03,587 cfg.training.epochs                : 50000
2024-02-01 09:15:03,587 cfg.training.early_stopping_metric : eval_metric
2024-02-01 09:15:03,587 cfg.training.batch_type            : sentence
2024-02-01 09:15:03,587 cfg.training.translation_normalization : batch
2024-02-01 09:15:03,587 cfg.training.eval_recognition_beam_size : 1
2024-02-01 09:15:03,588 cfg.training.eval_translation_beam_size : 1
2024-02-01 09:15:03,588 cfg.training.eval_translation_beam_alpha : -1
2024-02-01 09:15:03,588 cfg.training.overwrite             : True
2024-02-01 09:15:03,588 cfg.training.shuffle               : True
2024-02-01 09:15:03,588 cfg.training.use_cuda              : True
2024-02-01 09:15:03,588 cfg.training.translation_max_output_length : 40
2024-02-01 09:15:03,588 cfg.training.keep_last_ckpts       : 1
2024-02-01 09:15:03,588 cfg.training.batch_multiplier      : 1
2024-02-01 09:15:03,588 cfg.training.logging_freq          : 100
2024-02-01 09:15:03,589 cfg.training.validation_freq       : 2000
2024-02-01 09:15:03,589 cfg.training.betas                 : [0.9, 0.998]
2024-02-01 09:15:03,589 cfg.training.scheduling            : plateau
2024-02-01 09:15:03,589 cfg.training.learning_rate_min     : 1e-08
2024-02-01 09:15:03,589 cfg.training.weight_decay          : 0.0001
2024-02-01 09:15:03,589 cfg.training.patience              : 12
2024-02-01 09:15:03,589 cfg.training.decrease_factor       : 0.5
2024-02-01 09:15:03,589 cfg.training.label_smoothing       : 0.0
2024-02-01 09:15:03,589 cfg.model.initializer              : xavier
2024-02-01 09:15:03,590 cfg.model.bias_initializer         : zeros
2024-02-01 09:15:03,590 cfg.model.init_gain                : 1.0
2024-02-01 09:15:03,590 cfg.model.embed_initializer        : xavier
2024-02-01 09:15:03,590 cfg.model.embed_init_gain          : 1.0
2024-02-01 09:15:03,590 cfg.model.tied_softmax             : True
2024-02-01 09:15:03,590 cfg.model.encoder.type             : transformer
2024-02-01 09:15:03,590 cfg.model.encoder.num_layers       : 3
2024-02-01 09:15:03,590 cfg.model.encoder.num_heads        : 32
2024-02-01 09:15:03,590 cfg.model.encoder.embeddings.embedding_dim : 512
2024-02-01 09:15:03,591 cfg.model.encoder.embeddings.scale : False
2024-02-01 09:15:03,591 cfg.model.encoder.embeddings.dropout : 0.1
2024-02-01 09:15:03,591 cfg.model.encoder.embeddings.norm_type : batch
2024-02-01 09:15:03,591 cfg.model.encoder.embeddings.activation_type : softsign
2024-02-01 09:15:03,591 cfg.model.encoder.hidden_size      : 512
2024-02-01 09:15:03,591 cfg.model.encoder.ff_size          : 2048
2024-02-01 09:15:03,591 cfg.model.encoder.dropout          : 0.1
2024-02-01 09:15:03,591 cfg.model.decoder.type             : transformer
2024-02-01 09:15:03,591 cfg.model.decoder.num_layers       : 3
2024-02-01 09:15:03,592 cfg.model.decoder.num_heads        : 32
2024-02-01 09:15:03,592 cfg.model.decoder.embeddings.embedding_dim : 512
2024-02-01 09:15:03,592 cfg.model.decoder.embeddings.scale : False
2024-02-01 09:15:03,592 cfg.model.decoder.embeddings.dropout : 0.1
2024-02-01 09:15:03,592 cfg.model.decoder.embeddings.norm_type : batch
2024-02-01 09:15:03,592 cfg.model.decoder.embeddings.activation_type : softsign
2024-02-01 09:15:03,592 cfg.model.decoder.hidden_size      : 512
2024-02-01 09:15:03,592 cfg.model.decoder.ff_size          : 2048
2024-02-01 09:15:03,593 cfg.model.decoder.dropout          : 0.1
2024-02-01 09:15:03,593 Data set sizes: 
	train 2126,
	valid 708,
	test 706
2024-02-01 09:15:03,593 First training example:
	[GLS] A B C D E
	[TXT] although new zealand was disappointed to faltered at the finals against australia they did well throughout the tournament
2024-02-01 09:15:03,593 First 10 words (gls): (0) <si> (1) <unk> (2) <pad> (3) A (4) B (5) C (6) D (7) E
2024-02-01 09:15:03,593 First 10 words (txt): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) and (6) to (7) a (8) in (9) of
2024-02-01 09:15:03,593 Number of unique glosses (types): 8
2024-02-01 09:15:03,593 Number of unique words (types): 4397
2024-02-01 09:15:03,593 SignModel(
	encoder=TransformerEncoder(num_layers=3, num_heads=32),
	decoder=TransformerDecoder(num_layers=3, num_heads=32),
	sgn_embed=SpatialEmbeddings(embedding_dim=512, input_size=2560),
	txt_embed=Embeddings(embedding_dim=512, vocab_size=4397))
2024-02-01 09:15:03,597 EPOCH 1
2024-02-01 09:16:00,763 Epoch   1: Total Training Recognition Loss 90.68  Total Training Translation Loss 1002.11 
2024-02-01 09:16:00,767 EPOCH 2
2024-02-01 09:16:18,314 Epoch   2: Total Training Recognition Loss 35.95  Total Training Translation Loss 923.08 
2024-02-01 09:16:18,315 EPOCH 3
2024-02-01 09:16:35,479 Epoch   3: Total Training Recognition Loss 22.59  Total Training Translation Loss 886.20 
2024-02-01 09:16:35,479 EPOCH 4
2024-02-01 09:16:52,469 Epoch   4: Total Training Recognition Loss 12.85  Total Training Translation Loss 856.28 
2024-02-01 09:16:52,469 EPOCH 5
2024-02-01 09:17:09,572 Epoch   5: Total Training Recognition Loss 9.36  Total Training Translation Loss 835.29 
2024-02-01 09:17:09,572 EPOCH 6
2024-02-01 09:17:26,377 Epoch   6: Total Training Recognition Loss 6.70  Total Training Translation Loss 823.28 
2024-02-01 09:17:26,377 EPOCH 7
2024-02-01 09:17:43,199 Epoch   7: Total Training Recognition Loss 5.00  Total Training Translation Loss 815.01 
2024-02-01 09:17:43,199 EPOCH 8
2024-02-01 09:17:59,989 Epoch   8: Total Training Recognition Loss 3.63  Total Training Translation Loss 808.94 
2024-02-01 09:17:59,990 EPOCH 9
2024-02-01 09:18:16,852 Epoch   9: Total Training Recognition Loss 2.65  Total Training Translation Loss 805.11 
2024-02-01 09:18:16,853 EPOCH 10
2024-02-01 09:18:33,758 Epoch  10: Total Training Recognition Loss 1.87  Total Training Translation Loss 801.28 
2024-02-01 09:18:33,758 EPOCH 11
2024-02-01 09:18:51,281 Epoch  11: Total Training Recognition Loss 1.27  Total Training Translation Loss 797.88 
2024-02-01 09:18:51,282 EPOCH 12
2024-02-01 09:18:55,148 [Epoch: 012 Step: 00000100] Batch Recognition Loss:   0.105643 => Gls Tokens per Sec:      331 || Batch Translation Loss: 100.761742 => Txt Tokens per Sec:     1051 || Lr: 0.000100
2024-02-01 09:19:09,075 Epoch  12: Total Training Recognition Loss 0.87  Total Training Translation Loss 793.69 
2024-02-01 09:19:09,076 EPOCH 13
2024-02-01 09:19:26,415 Epoch  13: Total Training Recognition Loss 0.62  Total Training Translation Loss 785.92 
2024-02-01 09:19:26,416 EPOCH 14
2024-02-01 09:19:44,337 Epoch  14: Total Training Recognition Loss 0.46  Total Training Translation Loss 778.70 
2024-02-01 09:19:44,337 EPOCH 15
2024-02-01 09:20:01,565 Epoch  15: Total Training Recognition Loss 0.35  Total Training Translation Loss 768.96 
2024-02-01 09:20:01,565 EPOCH 16
2024-02-01 09:20:19,114 Epoch  16: Total Training Recognition Loss 0.28  Total Training Translation Loss 759.32 
2024-02-01 09:20:19,114 EPOCH 17
2024-02-01 09:20:35,955 Epoch  17: Total Training Recognition Loss 0.23  Total Training Translation Loss 747.40 
2024-02-01 09:20:35,955 EPOCH 18
2024-02-01 09:20:52,677 Epoch  18: Total Training Recognition Loss 0.20  Total Training Translation Loss 734.80 
2024-02-01 09:20:52,677 EPOCH 19
2024-02-01 09:21:09,404 Epoch  19: Total Training Recognition Loss 0.17  Total Training Translation Loss 727.84 
2024-02-01 09:21:09,404 EPOCH 20
2024-02-01 09:21:26,747 Epoch  20: Total Training Recognition Loss 0.15  Total Training Translation Loss 716.71 
2024-02-01 09:21:26,747 EPOCH 21
2024-02-01 09:21:44,049 Epoch  21: Total Training Recognition Loss 0.14  Total Training Translation Loss 709.38 
2024-02-01 09:21:44,050 EPOCH 22
2024-02-01 09:22:00,930 Epoch  22: Total Training Recognition Loss 0.13  Total Training Translation Loss 695.90 
2024-02-01 09:22:00,930 EPOCH 23
2024-02-01 09:22:01,425 [Epoch: 023 Step: 00000200] Batch Recognition Loss:   0.015204 => Gls Tokens per Sec:     5176 || Batch Translation Loss:  60.831425 => Txt Tokens per Sec:    10503 || Lr: 0.000100
2024-02-01 09:22:17,763 Epoch  23: Total Training Recognition Loss 0.13  Total Training Translation Loss 687.86 
2024-02-01 09:22:17,763 EPOCH 24
2024-02-01 09:22:35,459 Epoch  24: Total Training Recognition Loss 0.12  Total Training Translation Loss 679.32 
2024-02-01 09:22:35,460 EPOCH 25
2024-02-01 09:22:52,494 Epoch  25: Total Training Recognition Loss 0.10  Total Training Translation Loss 665.11 
2024-02-01 09:22:52,495 EPOCH 26
2024-02-01 09:23:09,420 Epoch  26: Total Training Recognition Loss 0.11  Total Training Translation Loss 653.60 
2024-02-01 09:23:09,420 EPOCH 27
2024-02-01 09:23:26,222 Epoch  27: Total Training Recognition Loss 0.11  Total Training Translation Loss 644.97 
2024-02-01 09:23:26,223 EPOCH 28
2024-02-01 09:23:42,994 Epoch  28: Total Training Recognition Loss 0.10  Total Training Translation Loss 635.27 
2024-02-01 09:23:42,994 EPOCH 29
2024-02-01 09:23:59,683 Epoch  29: Total Training Recognition Loss 0.10  Total Training Translation Loss 625.17 
2024-02-01 09:23:59,683 EPOCH 30
2024-02-01 09:24:16,331 Epoch  30: Total Training Recognition Loss 0.09  Total Training Translation Loss 615.76 
2024-02-01 09:24:16,332 EPOCH 31
2024-02-01 09:24:33,002 Epoch  31: Total Training Recognition Loss 0.09  Total Training Translation Loss 609.15 
2024-02-01 09:24:33,002 EPOCH 32
2024-02-01 09:24:49,867 Epoch  32: Total Training Recognition Loss 0.09  Total Training Translation Loss 604.21 
2024-02-01 09:24:49,867 EPOCH 33
2024-02-01 09:25:07,676 Epoch  33: Total Training Recognition Loss 0.09  Total Training Translation Loss 596.58 
2024-02-01 09:25:07,677 EPOCH 34
2024-02-01 09:25:18,027 [Epoch: 034 Step: 00000300] Batch Recognition Loss:   0.006819 => Gls Tokens per Sec:      371 || Batch Translation Loss:  57.405247 => Txt Tokens per Sec:     1134 || Lr: 0.000100
2024-02-01 09:25:24,925 Epoch  34: Total Training Recognition Loss 0.08  Total Training Translation Loss 591.53 
2024-02-01 09:25:24,925 EPOCH 35
2024-02-01 09:25:41,662 Epoch  35: Total Training Recognition Loss 0.10  Total Training Translation Loss 579.51 
2024-02-01 09:25:41,662 EPOCH 36
2024-02-01 09:25:58,469 Epoch  36: Total Training Recognition Loss 0.08  Total Training Translation Loss 568.19 
2024-02-01 09:25:58,470 EPOCH 37
2024-02-01 09:26:15,297 Epoch  37: Total Training Recognition Loss 0.09  Total Training Translation Loss 560.97 
2024-02-01 09:26:15,297 EPOCH 38
2024-02-01 09:26:32,911 Epoch  38: Total Training Recognition Loss 0.09  Total Training Translation Loss 549.09 
2024-02-01 09:26:32,911 EPOCH 39
2024-02-01 09:26:50,061 Epoch  39: Total Training Recognition Loss 0.08  Total Training Translation Loss 543.53 
2024-02-01 09:26:50,061 EPOCH 40
2024-02-01 09:27:06,733 Epoch  40: Total Training Recognition Loss 0.08  Total Training Translation Loss 536.58 
2024-02-01 09:27:06,733 EPOCH 41
2024-02-01 09:27:23,447 Epoch  41: Total Training Recognition Loss 0.08  Total Training Translation Loss 526.49 
2024-02-01 09:27:23,447 EPOCH 42
2024-02-01 09:27:40,182 Epoch  42: Total Training Recognition Loss 0.09  Total Training Translation Loss 520.11 
2024-02-01 09:27:40,183 EPOCH 43
2024-02-01 09:27:56,907 Epoch  43: Total Training Recognition Loss 0.08  Total Training Translation Loss 517.97 
2024-02-01 09:27:56,907 EPOCH 44
2024-02-01 09:28:13,629 Epoch  44: Total Training Recognition Loss 0.08  Total Training Translation Loss 510.66 
2024-02-01 09:28:13,629 EPOCH 45
2024-02-01 09:28:23,695 [Epoch: 045 Step: 00000400] Batch Recognition Loss:   0.010154 => Gls Tokens per Sec:      420 || Batch Translation Loss:  66.867767 => Txt Tokens per Sec:     1264 || Lr: 0.000100
2024-02-01 09:28:31,552 Epoch  45: Total Training Recognition Loss 0.08  Total Training Translation Loss 501.77 
2024-02-01 09:28:31,552 EPOCH 46
2024-02-01 09:28:48,507 Epoch  46: Total Training Recognition Loss 0.09  Total Training Translation Loss 492.51 
2024-02-01 09:28:48,507 EPOCH 47
2024-02-01 09:29:05,255 Epoch  47: Total Training Recognition Loss 0.09  Total Training Translation Loss 484.85 
2024-02-01 09:29:05,255 EPOCH 48
2024-02-01 09:29:21,995 Epoch  48: Total Training Recognition Loss 0.09  Total Training Translation Loss 487.88 
2024-02-01 09:29:21,995 EPOCH 49
2024-02-01 09:29:38,763 Epoch  49: Total Training Recognition Loss 0.10  Total Training Translation Loss 477.30 
2024-02-01 09:29:38,763 EPOCH 50
2024-02-01 09:29:55,496 Epoch  50: Total Training Recognition Loss 0.10  Total Training Translation Loss 466.39 
2024-02-01 09:29:55,497 EPOCH 51
2024-02-01 09:30:12,134 Epoch  51: Total Training Recognition Loss 0.11  Total Training Translation Loss 458.43 
2024-02-01 09:30:12,134 EPOCH 52
2024-02-01 09:30:28,904 Epoch  52: Total Training Recognition Loss 0.09  Total Training Translation Loss 451.88 
2024-02-01 09:30:28,905 EPOCH 53
2024-02-01 09:30:46,909 Epoch  53: Total Training Recognition Loss 0.10  Total Training Translation Loss 444.82 
2024-02-01 09:30:46,909 EPOCH 54
2024-02-01 09:31:03,707 Epoch  54: Total Training Recognition Loss 0.09  Total Training Translation Loss 439.56 
2024-02-01 09:31:03,708 EPOCH 55
2024-02-01 09:31:20,473 Epoch  55: Total Training Recognition Loss 0.09  Total Training Translation Loss 430.78 
2024-02-01 09:31:20,474 EPOCH 56
2024-02-01 09:31:32,439 [Epoch: 056 Step: 00000500] Batch Recognition Loss:   0.024332 => Gls Tokens per Sec:      461 || Batch Translation Loss:  38.520317 => Txt Tokens per Sec:     1329 || Lr: 0.000100
2024-02-01 09:31:37,274 Epoch  56: Total Training Recognition Loss 0.11  Total Training Translation Loss 426.18 
2024-02-01 09:31:37,274 EPOCH 57
2024-02-01 09:31:54,082 Epoch  57: Total Training Recognition Loss 0.10  Total Training Translation Loss 420.01 
2024-02-01 09:31:54,082 EPOCH 58
2024-02-01 09:32:10,965 Epoch  58: Total Training Recognition Loss 0.11  Total Training Translation Loss 412.06 
2024-02-01 09:32:10,965 EPOCH 59
2024-02-01 09:32:28,527 Epoch  59: Total Training Recognition Loss 0.11  Total Training Translation Loss 404.67 
2024-02-01 09:32:28,527 EPOCH 60
2024-02-01 09:32:45,572 Epoch  60: Total Training Recognition Loss 0.10  Total Training Translation Loss 399.66 
2024-02-01 09:32:45,572 EPOCH 61
2024-02-01 09:33:02,305 Epoch  61: Total Training Recognition Loss 0.10  Total Training Translation Loss 395.13 
2024-02-01 09:33:02,305 EPOCH 62
2024-02-01 09:33:19,296 Epoch  62: Total Training Recognition Loss 0.11  Total Training Translation Loss 388.35 
2024-02-01 09:33:19,296 EPOCH 63
2024-02-01 09:33:36,139 Epoch  63: Total Training Recognition Loss 0.12  Total Training Translation Loss 383.18 
2024-02-01 09:33:36,140 EPOCH 64
2024-02-01 09:33:52,905 Epoch  64: Total Training Recognition Loss 0.11  Total Training Translation Loss 381.05 
2024-02-01 09:33:52,906 EPOCH 65
2024-02-01 09:34:09,765 Epoch  65: Total Training Recognition Loss 0.14  Total Training Translation Loss 376.99 
2024-02-01 09:34:09,765 EPOCH 66
2024-02-01 09:34:26,368 Epoch  66: Total Training Recognition Loss 0.13  Total Training Translation Loss 372.93 
2024-02-01 09:34:26,368 EPOCH 67
2024-02-01 09:34:36,509 [Epoch: 067 Step: 00000600] Batch Recognition Loss:   0.016973 => Gls Tokens per Sec:      670 || Batch Translation Loss:  50.183727 => Txt Tokens per Sec:     1804 || Lr: 0.000100
2024-02-01 09:34:43,238 Epoch  67: Total Training Recognition Loss 0.14  Total Training Translation Loss 370.09 
2024-02-01 09:34:43,239 EPOCH 68
2024-02-01 09:35:00,029 Epoch  68: Total Training Recognition Loss 0.13  Total Training Translation Loss 361.05 
2024-02-01 09:35:00,029 EPOCH 69
2024-02-01 09:35:16,800 Epoch  69: Total Training Recognition Loss 0.14  Total Training Translation Loss 352.45 
2024-02-01 09:35:16,800 EPOCH 70
2024-02-01 09:35:33,558 Epoch  70: Total Training Recognition Loss 0.15  Total Training Translation Loss 344.22 
2024-02-01 09:35:33,558 EPOCH 71
2024-02-01 09:35:50,347 Epoch  71: Total Training Recognition Loss 0.13  Total Training Translation Loss 341.59 
2024-02-01 09:35:50,347 EPOCH 72
2024-02-01 09:36:07,044 Epoch  72: Total Training Recognition Loss 0.14  Total Training Translation Loss 334.68 
2024-02-01 09:36:07,044 EPOCH 73
2024-02-01 09:36:23,757 Epoch  73: Total Training Recognition Loss 0.14  Total Training Translation Loss 327.78 
2024-02-01 09:36:23,757 EPOCH 74
2024-02-01 09:36:40,529 Epoch  74: Total Training Recognition Loss 0.14  Total Training Translation Loss 322.36 
2024-02-01 09:36:40,529 EPOCH 75
2024-02-01 09:36:57,225 Epoch  75: Total Training Recognition Loss 0.15  Total Training Translation Loss 315.87 
2024-02-01 09:36:57,225 EPOCH 76
2024-02-01 09:37:13,900 Epoch  76: Total Training Recognition Loss 0.14  Total Training Translation Loss 311.22 
2024-02-01 09:37:13,901 EPOCH 77
2024-02-01 09:37:30,549 Epoch  77: Total Training Recognition Loss 0.15  Total Training Translation Loss 304.29 
2024-02-01 09:37:30,550 EPOCH 78
2024-02-01 09:37:46,330 [Epoch: 078 Step: 00000700] Batch Recognition Loss:   0.019678 => Gls Tokens per Sec:      511 || Batch Translation Loss:  46.400261 => Txt Tokens per Sec:     1439 || Lr: 0.000100
2024-02-01 09:37:47,311 Epoch  78: Total Training Recognition Loss 0.15  Total Training Translation Loss 300.50 
2024-02-01 09:37:47,311 EPOCH 79
2024-02-01 09:38:04,043 Epoch  79: Total Training Recognition Loss 0.15  Total Training Translation Loss 294.06 
2024-02-01 09:38:04,043 EPOCH 80
2024-02-01 09:38:20,826 Epoch  80: Total Training Recognition Loss 0.15  Total Training Translation Loss 292.00 
2024-02-01 09:38:20,826 EPOCH 81
2024-02-01 09:38:37,648 Epoch  81: Total Training Recognition Loss 0.15  Total Training Translation Loss 290.07 
2024-02-01 09:38:37,648 EPOCH 82
2024-02-01 09:38:54,443 Epoch  82: Total Training Recognition Loss 0.18  Total Training Translation Loss 285.22 
2024-02-01 09:38:54,443 EPOCH 83
2024-02-01 09:39:11,158 Epoch  83: Total Training Recognition Loss 0.14  Total Training Translation Loss 280.02 
2024-02-01 09:39:11,158 EPOCH 84
2024-02-01 09:39:27,926 Epoch  84: Total Training Recognition Loss 0.18  Total Training Translation Loss 275.19 
2024-02-01 09:39:27,926 EPOCH 85
2024-02-01 09:39:44,740 Epoch  85: Total Training Recognition Loss 0.17  Total Training Translation Loss 268.61 
2024-02-01 09:39:44,740 EPOCH 86
2024-02-01 09:40:01,542 Epoch  86: Total Training Recognition Loss 0.17  Total Training Translation Loss 263.93 
2024-02-01 09:40:01,542 EPOCH 87
2024-02-01 09:40:18,358 Epoch  87: Total Training Recognition Loss 0.17  Total Training Translation Loss 258.98 
2024-02-01 09:40:18,358 EPOCH 88
2024-02-01 09:40:34,998 Epoch  88: Total Training Recognition Loss 0.17  Total Training Translation Loss 254.53 
2024-02-01 09:40:34,998 EPOCH 89
2024-02-01 09:40:45,703 [Epoch: 089 Step: 00000800] Batch Recognition Loss:   0.020641 => Gls Tokens per Sec:      874 || Batch Translation Loss:  17.388641 => Txt Tokens per Sec:     2339 || Lr: 0.000100
2024-02-01 09:40:51,657 Epoch  89: Total Training Recognition Loss 0.18  Total Training Translation Loss 248.27 
2024-02-01 09:40:51,657 EPOCH 90
2024-02-01 09:41:08,420 Epoch  90: Total Training Recognition Loss 0.17  Total Training Translation Loss 240.51 
2024-02-01 09:41:08,420 EPOCH 91
2024-02-01 09:41:25,309 Epoch  91: Total Training Recognition Loss 0.18  Total Training Translation Loss 236.74 
2024-02-01 09:41:25,309 EPOCH 92
2024-02-01 09:41:41,870 Epoch  92: Total Training Recognition Loss 0.17  Total Training Translation Loss 233.50 
2024-02-01 09:41:41,870 EPOCH 93
2024-02-01 09:41:58,689 Epoch  93: Total Training Recognition Loss 0.19  Total Training Translation Loss 230.98 
2024-02-01 09:41:58,689 EPOCH 94
2024-02-01 09:42:15,469 Epoch  94: Total Training Recognition Loss 0.17  Total Training Translation Loss 226.10 
2024-02-01 09:42:15,469 EPOCH 95
2024-02-01 09:42:32,186 Epoch  95: Total Training Recognition Loss 0.17  Total Training Translation Loss 220.99 
2024-02-01 09:42:32,186 EPOCH 96
2024-02-01 09:42:48,910 Epoch  96: Total Training Recognition Loss 0.20  Total Training Translation Loss 216.37 
2024-02-01 09:42:48,910 EPOCH 97
2024-02-01 09:43:05,628 Epoch  97: Total Training Recognition Loss 0.19  Total Training Translation Loss 215.06 
2024-02-01 09:43:05,628 EPOCH 98
2024-02-01 09:43:22,390 Epoch  98: Total Training Recognition Loss 0.19  Total Training Translation Loss 208.26 
2024-02-01 09:43:22,390 EPOCH 99
2024-02-01 09:43:39,059 Epoch  99: Total Training Recognition Loss 0.19  Total Training Translation Loss 206.23 
2024-02-01 09:43:39,059 EPOCH 100
2024-02-01 09:43:55,724 [Epoch: 100 Step: 00000900] Batch Recognition Loss:   0.033652 => Gls Tokens per Sec:      638 || Batch Translation Loss:  28.106808 => Txt Tokens per Sec:     1771 || Lr: 0.000100
2024-02-01 09:43:55,725 Epoch 100: Total Training Recognition Loss 0.22  Total Training Translation Loss 202.86 
2024-02-01 09:43:55,725 EPOCH 101
2024-02-01 09:44:12,583 Epoch 101: Total Training Recognition Loss 0.21  Total Training Translation Loss 204.10 
2024-02-01 09:44:12,583 EPOCH 102
2024-02-01 09:44:29,360 Epoch 102: Total Training Recognition Loss 0.19  Total Training Translation Loss 202.21 
2024-02-01 09:44:29,360 EPOCH 103
2024-02-01 09:44:46,274 Epoch 103: Total Training Recognition Loss 0.25  Total Training Translation Loss 200.78 
2024-02-01 09:44:46,274 EPOCH 104
2024-02-01 09:45:03,100 Epoch 104: Total Training Recognition Loss 0.20  Total Training Translation Loss 191.51 
2024-02-01 09:45:03,100 EPOCH 105
2024-02-01 09:45:19,894 Epoch 105: Total Training Recognition Loss 0.21  Total Training Translation Loss 185.25 
2024-02-01 09:45:19,894 EPOCH 106
2024-02-01 09:45:36,639 Epoch 106: Total Training Recognition Loss 0.18  Total Training Translation Loss 178.45 
2024-02-01 09:45:36,640 EPOCH 107
2024-02-01 09:45:53,362 Epoch 107: Total Training Recognition Loss 0.21  Total Training Translation Loss 178.37 
2024-02-01 09:45:53,362 EPOCH 108
2024-02-01 09:46:09,968 Epoch 108: Total Training Recognition Loss 0.22  Total Training Translation Loss 179.38 
2024-02-01 09:46:09,968 EPOCH 109
2024-02-01 09:46:26,684 Epoch 109: Total Training Recognition Loss 0.22  Total Training Translation Loss 174.29 
2024-02-01 09:46:26,684 EPOCH 110
2024-02-01 09:46:43,464 Epoch 110: Total Training Recognition Loss 0.20  Total Training Translation Loss 166.06 
2024-02-01 09:46:43,465 EPOCH 111
2024-02-01 09:47:00,233 Epoch 111: Total Training Recognition Loss 0.20  Total Training Translation Loss 162.33 
2024-02-01 09:47:00,233 EPOCH 112
2024-02-01 09:47:00,599 [Epoch: 112 Step: 00001000] Batch Recognition Loss:   0.013863 => Gls Tokens per Sec:     3509 || Batch Translation Loss:  19.205038 => Txt Tokens per Sec:     9615 || Lr: 0.000100
2024-02-01 09:47:16,804 Epoch 112: Total Training Recognition Loss 0.20  Total Training Translation Loss 158.33 
2024-02-01 09:47:16,804 EPOCH 113
2024-02-01 09:47:33,522 Epoch 113: Total Training Recognition Loss 0.21  Total Training Translation Loss 153.36 
2024-02-01 09:47:33,522 EPOCH 114
2024-02-01 09:47:50,234 Epoch 114: Total Training Recognition Loss 0.19  Total Training Translation Loss 148.33 
2024-02-01 09:47:50,234 EPOCH 115
2024-02-01 09:48:07,217 Epoch 115: Total Training Recognition Loss 0.20  Total Training Translation Loss 144.95 
2024-02-01 09:48:07,218 EPOCH 116
2024-02-01 09:48:24,088 Epoch 116: Total Training Recognition Loss 0.19  Total Training Translation Loss 142.75 
2024-02-01 09:48:24,088 EPOCH 117
2024-02-01 09:48:40,717 Epoch 117: Total Training Recognition Loss 0.20  Total Training Translation Loss 141.11 
2024-02-01 09:48:40,717 EPOCH 118
2024-02-01 09:48:57,377 Epoch 118: Total Training Recognition Loss 0.21  Total Training Translation Loss 135.38 
2024-02-01 09:48:57,377 EPOCH 119
2024-02-01 09:49:14,153 Epoch 119: Total Training Recognition Loss 0.19  Total Training Translation Loss 132.41 
2024-02-01 09:49:14,153 EPOCH 120
2024-02-01 09:49:31,003 Epoch 120: Total Training Recognition Loss 0.19  Total Training Translation Loss 130.18 
2024-02-01 09:49:31,004 EPOCH 121
2024-02-01 09:49:47,887 Epoch 121: Total Training Recognition Loss 0.20  Total Training Translation Loss 127.77 
2024-02-01 09:49:47,887 EPOCH 122
2024-02-01 09:50:04,613 Epoch 122: Total Training Recognition Loss 0.19  Total Training Translation Loss 124.63 
2024-02-01 09:50:04,613 EPOCH 123
2024-02-01 09:50:12,998 [Epoch: 123 Step: 00001100] Batch Recognition Loss:   0.030371 => Gls Tokens per Sec:      199 || Batch Translation Loss:  17.493780 => Txt Tokens per Sec:      656 || Lr: 0.000100
2024-02-01 09:50:21,264 Epoch 123: Total Training Recognition Loss 0.21  Total Training Translation Loss 120.56 
2024-02-01 09:50:21,264 EPOCH 124
2024-02-01 09:50:37,956 Epoch 124: Total Training Recognition Loss 0.19  Total Training Translation Loss 117.52 
2024-02-01 09:50:37,957 EPOCH 125
2024-02-01 09:50:54,621 Epoch 125: Total Training Recognition Loss 0.19  Total Training Translation Loss 114.80 
2024-02-01 09:50:54,621 EPOCH 126
2024-02-01 09:51:11,475 Epoch 126: Total Training Recognition Loss 0.18  Total Training Translation Loss 111.26 
2024-02-01 09:51:11,475 EPOCH 127
2024-02-01 09:51:28,208 Epoch 127: Total Training Recognition Loss 0.19  Total Training Translation Loss 107.47 
2024-02-01 09:51:28,208 EPOCH 128
2024-02-01 09:51:44,936 Epoch 128: Total Training Recognition Loss 0.19  Total Training Translation Loss 105.23 
2024-02-01 09:51:44,937 EPOCH 129
2024-02-01 09:52:01,634 Epoch 129: Total Training Recognition Loss 0.18  Total Training Translation Loss 101.79 
2024-02-01 09:52:01,634 EPOCH 130
2024-02-01 09:52:18,465 Epoch 130: Total Training Recognition Loss 0.17  Total Training Translation Loss 102.42 
2024-02-01 09:52:18,465 EPOCH 131
2024-02-01 09:52:35,305 Epoch 131: Total Training Recognition Loss 0.20  Total Training Translation Loss 99.69 
2024-02-01 09:52:35,305 EPOCH 132
2024-02-01 09:52:52,068 Epoch 132: Total Training Recognition Loss 0.20  Total Training Translation Loss 99.74 
2024-02-01 09:52:52,068 EPOCH 133
2024-02-01 09:53:08,902 Epoch 133: Total Training Recognition Loss 0.19  Total Training Translation Loss 95.41 
2024-02-01 09:53:08,902 EPOCH 134
2024-02-01 09:53:19,904 [Epoch: 134 Step: 00001200] Batch Recognition Loss:   0.027162 => Gls Tokens per Sec:      268 || Batch Translation Loss:  14.562624 => Txt Tokens per Sec:      856 || Lr: 0.000100
2024-02-01 09:53:25,682 Epoch 134: Total Training Recognition Loss 0.18  Total Training Translation Loss 91.34 
2024-02-01 09:53:25,682 EPOCH 135
2024-02-01 09:53:42,471 Epoch 135: Total Training Recognition Loss 0.16  Total Training Translation Loss 88.38 
2024-02-01 09:53:42,471 EPOCH 136
2024-02-01 09:53:59,248 Epoch 136: Total Training Recognition Loss 0.18  Total Training Translation Loss 86.80 
2024-02-01 09:53:59,248 EPOCH 137
2024-02-01 09:54:15,912 Epoch 137: Total Training Recognition Loss 0.18  Total Training Translation Loss 86.03 
2024-02-01 09:54:15,912 EPOCH 138
2024-02-01 09:54:32,722 Epoch 138: Total Training Recognition Loss 0.19  Total Training Translation Loss 84.41 
2024-02-01 09:54:32,722 EPOCH 139
2024-02-01 09:54:49,469 Epoch 139: Total Training Recognition Loss 0.17  Total Training Translation Loss 81.87 
2024-02-01 09:54:49,470 EPOCH 140
2024-02-01 09:55:06,361 Epoch 140: Total Training Recognition Loss 0.18  Total Training Translation Loss 79.81 
2024-02-01 09:55:06,362 EPOCH 141
2024-02-01 09:55:24,161 Epoch 141: Total Training Recognition Loss 0.17  Total Training Translation Loss 78.00 
2024-02-01 09:55:24,161 EPOCH 142
2024-02-01 09:55:40,917 Epoch 142: Total Training Recognition Loss 0.18  Total Training Translation Loss 76.29 
2024-02-01 09:55:40,917 EPOCH 143
2024-02-01 09:55:57,753 Epoch 143: Total Training Recognition Loss 0.20  Total Training Translation Loss 74.70 
2024-02-01 09:55:57,753 EPOCH 144
2024-02-01 09:56:14,479 Epoch 144: Total Training Recognition Loss 0.18  Total Training Translation Loss 73.48 
2024-02-01 09:56:14,479 EPOCH 145
2024-02-01 09:56:20,020 [Epoch: 145 Step: 00001300] Batch Recognition Loss:   0.016879 => Gls Tokens per Sec:      763 || Batch Translation Loss:   6.805230 => Txt Tokens per Sec:     1823 || Lr: 0.000100
2024-02-01 09:56:31,174 Epoch 145: Total Training Recognition Loss 0.17  Total Training Translation Loss 70.24 
2024-02-01 09:56:31,174 EPOCH 146
2024-02-01 09:56:48,021 Epoch 146: Total Training Recognition Loss 0.17  Total Training Translation Loss 68.34 
2024-02-01 09:56:48,021 EPOCH 147
2024-02-01 09:57:04,811 Epoch 147: Total Training Recognition Loss 0.16  Total Training Translation Loss 66.20 
2024-02-01 09:57:04,811 EPOCH 148
2024-02-01 09:57:21,560 Epoch 148: Total Training Recognition Loss 0.16  Total Training Translation Loss 64.52 
2024-02-01 09:57:21,560 EPOCH 149
2024-02-01 09:57:38,478 Epoch 149: Total Training Recognition Loss 0.17  Total Training Translation Loss 63.07 
2024-02-01 09:57:38,478 EPOCH 150
2024-02-01 09:57:55,224 Epoch 150: Total Training Recognition Loss 0.16  Total Training Translation Loss 64.10 
2024-02-01 09:57:55,224 EPOCH 151
2024-02-01 09:58:12,089 Epoch 151: Total Training Recognition Loss 0.17  Total Training Translation Loss 59.73 
2024-02-01 09:58:12,089 EPOCH 152
2024-02-01 09:58:28,902 Epoch 152: Total Training Recognition Loss 0.16  Total Training Translation Loss 57.61 
2024-02-01 09:58:28,903 EPOCH 153
2024-02-01 09:58:45,611 Epoch 153: Total Training Recognition Loss 0.15  Total Training Translation Loss 56.11 
2024-02-01 09:58:45,611 EPOCH 154
2024-02-01 09:59:02,407 Epoch 154: Total Training Recognition Loss 0.16  Total Training Translation Loss 54.76 
2024-02-01 09:59:02,407 EPOCH 155
2024-02-01 09:59:19,184 Epoch 155: Total Training Recognition Loss 0.16  Total Training Translation Loss 52.65 
2024-02-01 09:59:19,184 EPOCH 156
2024-02-01 09:59:28,739 [Epoch: 156 Step: 00001400] Batch Recognition Loss:   0.017657 => Gls Tokens per Sec:      577 || Batch Translation Loss:   1.876001 => Txt Tokens per Sec:     1575 || Lr: 0.000100
2024-02-01 09:59:36,020 Epoch 156: Total Training Recognition Loss 0.15  Total Training Translation Loss 51.24 
2024-02-01 09:59:36,020 EPOCH 157
2024-02-01 09:59:52,709 Epoch 157: Total Training Recognition Loss 0.14  Total Training Translation Loss 49.52 
2024-02-01 09:59:52,709 EPOCH 158
2024-02-01 10:00:09,416 Epoch 158: Total Training Recognition Loss 0.14  Total Training Translation Loss 47.74 
2024-02-01 10:00:09,416 EPOCH 159
2024-02-01 10:00:26,344 Epoch 159: Total Training Recognition Loss 0.13  Total Training Translation Loss 47.17 
2024-02-01 10:00:26,344 EPOCH 160
2024-02-01 10:00:43,081 Epoch 160: Total Training Recognition Loss 0.14  Total Training Translation Loss 45.05 
2024-02-01 10:00:43,081 EPOCH 161
2024-02-01 10:00:59,827 Epoch 161: Total Training Recognition Loss 0.13  Total Training Translation Loss 45.90 
2024-02-01 10:00:59,827 EPOCH 162
2024-02-01 10:01:16,598 Epoch 162: Total Training Recognition Loss 0.14  Total Training Translation Loss 44.56 
2024-02-01 10:01:16,599 EPOCH 163
2024-02-01 10:01:33,371 Epoch 163: Total Training Recognition Loss 0.13  Total Training Translation Loss 44.38 
2024-02-01 10:01:33,371 EPOCH 164
2024-02-01 10:01:50,118 Epoch 164: Total Training Recognition Loss 0.13  Total Training Translation Loss 42.65 
2024-02-01 10:01:50,118 EPOCH 165
2024-02-01 10:02:06,847 Epoch 165: Total Training Recognition Loss 0.13  Total Training Translation Loss 41.67 
2024-02-01 10:02:06,847 EPOCH 166
2024-02-01 10:02:23,594 Epoch 166: Total Training Recognition Loss 0.13  Total Training Translation Loss 39.63 
2024-02-01 10:02:23,594 EPOCH 167
2024-02-01 10:02:35,100 [Epoch: 167 Step: 00001500] Batch Recognition Loss:   0.014580 => Gls Tokens per Sec:      668 || Batch Translation Loss:   5.333277 => Txt Tokens per Sec:     1990 || Lr: 0.000100
2024-02-01 10:02:40,312 Epoch 167: Total Training Recognition Loss 0.13  Total Training Translation Loss 38.94 
2024-02-01 10:02:40,313 EPOCH 168
2024-02-01 10:02:57,235 Epoch 168: Total Training Recognition Loss 0.13  Total Training Translation Loss 39.23 
2024-02-01 10:02:57,236 EPOCH 169
2024-02-01 10:03:13,990 Epoch 169: Total Training Recognition Loss 0.13  Total Training Translation Loss 37.54 
2024-02-01 10:03:13,991 EPOCH 170
2024-02-01 10:03:30,706 Epoch 170: Total Training Recognition Loss 0.13  Total Training Translation Loss 37.86 
2024-02-01 10:03:30,707 EPOCH 171
2024-02-01 10:03:47,526 Epoch 171: Total Training Recognition Loss 0.12  Total Training Translation Loss 36.43 
2024-02-01 10:03:47,526 EPOCH 172
2024-02-01 10:04:04,375 Epoch 172: Total Training Recognition Loss 0.13  Total Training Translation Loss 34.70 
2024-02-01 10:04:04,375 EPOCH 173
2024-02-01 10:04:21,140 Epoch 173: Total Training Recognition Loss 0.12  Total Training Translation Loss 33.13 
2024-02-01 10:04:21,140 EPOCH 174
2024-02-01 10:04:37,939 Epoch 174: Total Training Recognition Loss 0.12  Total Training Translation Loss 33.42 
2024-02-01 10:04:37,939 EPOCH 175
2024-02-01 10:04:54,705 Epoch 175: Total Training Recognition Loss 0.11  Total Training Translation Loss 31.96 
2024-02-01 10:04:54,705 EPOCH 176
2024-02-01 10:05:11,439 Epoch 176: Total Training Recognition Loss 0.11  Total Training Translation Loss 30.66 
2024-02-01 10:05:11,440 EPOCH 177
2024-02-01 10:05:28,315 Epoch 177: Total Training Recognition Loss 0.11  Total Training Translation Loss 31.05 
2024-02-01 10:05:28,316 EPOCH 178
2024-02-01 10:05:38,585 [Epoch: 178 Step: 00001600] Batch Recognition Loss:   0.010348 => Gls Tokens per Sec:      786 || Batch Translation Loss:   2.899161 => Txt Tokens per Sec:     2096 || Lr: 0.000100
2024-02-01 10:05:44,952 Epoch 178: Total Training Recognition Loss 0.11  Total Training Translation Loss 30.37 
2024-02-01 10:05:44,953 EPOCH 179
2024-02-01 10:06:01,699 Epoch 179: Total Training Recognition Loss 0.11  Total Training Translation Loss 28.95 
2024-02-01 10:06:01,699 EPOCH 180
2024-02-01 10:06:18,410 Epoch 180: Total Training Recognition Loss 0.11  Total Training Translation Loss 28.57 
2024-02-01 10:06:18,410 EPOCH 181
2024-02-01 10:06:35,342 Epoch 181: Total Training Recognition Loss 0.10  Total Training Translation Loss 27.84 
2024-02-01 10:06:35,342 EPOCH 182
2024-02-01 10:06:52,041 Epoch 182: Total Training Recognition Loss 0.10  Total Training Translation Loss 27.25 
2024-02-01 10:06:52,041 EPOCH 183
2024-02-01 10:07:08,884 Epoch 183: Total Training Recognition Loss 0.11  Total Training Translation Loss 26.50 
2024-02-01 10:07:08,884 EPOCH 184
2024-02-01 10:07:25,559 Epoch 184: Total Training Recognition Loss 0.11  Total Training Translation Loss 25.29 
2024-02-01 10:07:25,559 EPOCH 185
2024-02-01 10:07:42,375 Epoch 185: Total Training Recognition Loss 0.09  Total Training Translation Loss 25.21 
2024-02-01 10:07:42,375 EPOCH 186
2024-02-01 10:07:59,070 Epoch 186: Total Training Recognition Loss 0.10  Total Training Translation Loss 24.74 
2024-02-01 10:07:59,070 EPOCH 187
2024-02-01 10:08:15,710 Epoch 187: Total Training Recognition Loss 0.09  Total Training Translation Loss 23.89 
2024-02-01 10:08:15,710 EPOCH 188
2024-02-01 10:08:32,464 Epoch 188: Total Training Recognition Loss 0.10  Total Training Translation Loss 23.88 
2024-02-01 10:08:32,464 EPOCH 189
2024-02-01 10:08:43,163 [Epoch: 189 Step: 00001700] Batch Recognition Loss:   0.010205 => Gls Tokens per Sec:      874 || Batch Translation Loss:   2.514980 => Txt Tokens per Sec:     2339 || Lr: 0.000100
2024-02-01 10:08:49,093 Epoch 189: Total Training Recognition Loss 0.10  Total Training Translation Loss 23.49 
2024-02-01 10:08:49,093 EPOCH 190
2024-02-01 10:09:05,930 Epoch 190: Total Training Recognition Loss 0.09  Total Training Translation Loss 22.18 
2024-02-01 10:09:05,931 EPOCH 191
2024-02-01 10:09:22,661 Epoch 191: Total Training Recognition Loss 0.09  Total Training Translation Loss 22.11 
2024-02-01 10:09:22,661 EPOCH 192
2024-02-01 10:09:39,403 Epoch 192: Total Training Recognition Loss 0.10  Total Training Translation Loss 21.24 
2024-02-01 10:09:39,404 EPOCH 193
2024-02-01 10:09:56,104 Epoch 193: Total Training Recognition Loss 0.09  Total Training Translation Loss 21.83 
2024-02-01 10:09:56,104 EPOCH 194
2024-02-01 10:10:12,859 Epoch 194: Total Training Recognition Loss 0.09  Total Training Translation Loss 20.78 
2024-02-01 10:10:12,859 EPOCH 195
2024-02-01 10:10:29,604 Epoch 195: Total Training Recognition Loss 0.09  Total Training Translation Loss 20.77 
2024-02-01 10:10:29,604 EPOCH 196
2024-02-01 10:10:46,357 Epoch 196: Total Training Recognition Loss 0.09  Total Training Translation Loss 19.90 
2024-02-01 10:10:46,357 EPOCH 197
2024-02-01 10:11:03,116 Epoch 197: Total Training Recognition Loss 0.09  Total Training Translation Loss 19.85 
2024-02-01 10:11:03,116 EPOCH 198
2024-02-01 10:11:19,787 Epoch 198: Total Training Recognition Loss 0.09  Total Training Translation Loss 19.29 
2024-02-01 10:11:19,787 EPOCH 199
2024-02-01 10:11:36,661 Epoch 199: Total Training Recognition Loss 0.08  Total Training Translation Loss 19.51 
2024-02-01 10:11:36,661 EPOCH 200
2024-02-01 10:11:53,403 [Epoch: 200 Step: 00001800] Batch Recognition Loss:   0.008060 => Gls Tokens per Sec:      635 || Batch Translation Loss:   2.292879 => Txt Tokens per Sec:     1763 || Lr: 0.000100
2024-02-01 10:11:53,403 Epoch 200: Total Training Recognition Loss 0.08  Total Training Translation Loss 18.51 
2024-02-01 10:11:53,403 EPOCH 201
2024-02-01 10:12:10,053 Epoch 201: Total Training Recognition Loss 0.08  Total Training Translation Loss 18.03 
2024-02-01 10:12:10,054 EPOCH 202
2024-02-01 10:12:26,752 Epoch 202: Total Training Recognition Loss 0.08  Total Training Translation Loss 18.03 
2024-02-01 10:12:26,752 EPOCH 203
2024-02-01 10:12:43,526 Epoch 203: Total Training Recognition Loss 0.08  Total Training Translation Loss 17.22 
2024-02-01 10:12:43,526 EPOCH 204
2024-02-01 10:13:00,211 Epoch 204: Total Training Recognition Loss 0.08  Total Training Translation Loss 16.92 
2024-02-01 10:13:00,211 EPOCH 205
2024-02-01 10:13:16,859 Epoch 205: Total Training Recognition Loss 0.08  Total Training Translation Loss 17.35 
2024-02-01 10:13:16,859 EPOCH 206
2024-02-01 10:13:33,648 Epoch 206: Total Training Recognition Loss 0.08  Total Training Translation Loss 16.62 
2024-02-01 10:13:33,648 EPOCH 207
2024-02-01 10:13:50,343 Epoch 207: Total Training Recognition Loss 0.07  Total Training Translation Loss 16.00 
2024-02-01 10:13:50,344 EPOCH 208
2024-02-01 10:14:07,115 Epoch 208: Total Training Recognition Loss 0.08  Total Training Translation Loss 15.54 
2024-02-01 10:14:07,116 EPOCH 209
2024-02-01 10:14:23,912 Epoch 209: Total Training Recognition Loss 0.07  Total Training Translation Loss 15.31 
2024-02-01 10:14:23,912 EPOCH 210
2024-02-01 10:14:40,638 Epoch 210: Total Training Recognition Loss 0.07  Total Training Translation Loss 15.14 
2024-02-01 10:14:40,638 EPOCH 211
2024-02-01 10:14:57,662 Epoch 211: Total Training Recognition Loss 0.07  Total Training Translation Loss 14.93 
2024-02-01 10:14:57,662 EPOCH 212
2024-02-01 10:15:01,464 [Epoch: 212 Step: 00001900] Batch Recognition Loss:   0.009923 => Gls Tokens per Sec:      337 || Batch Translation Loss:   2.040107 => Txt Tokens per Sec:     1068 || Lr: 0.000100
2024-02-01 10:15:14,278 Epoch 212: Total Training Recognition Loss 0.07  Total Training Translation Loss 14.21 
2024-02-01 10:15:14,278 EPOCH 213
2024-02-01 10:15:31,016 Epoch 213: Total Training Recognition Loss 0.07  Total Training Translation Loss 14.02 
2024-02-01 10:15:31,016 EPOCH 214
2024-02-01 10:15:47,765 Epoch 214: Total Training Recognition Loss 0.07  Total Training Translation Loss 13.61 
2024-02-01 10:15:47,766 EPOCH 215
2024-02-01 10:16:04,536 Epoch 215: Total Training Recognition Loss 0.06  Total Training Translation Loss 13.55 
2024-02-01 10:16:04,536 EPOCH 216
2024-02-01 10:16:21,315 Epoch 216: Total Training Recognition Loss 0.07  Total Training Translation Loss 13.48 
2024-02-01 10:16:21,315 EPOCH 217
2024-02-01 10:16:38,115 Epoch 217: Total Training Recognition Loss 0.06  Total Training Translation Loss 13.20 
2024-02-01 10:16:38,115 EPOCH 218
2024-02-01 10:16:54,883 Epoch 218: Total Training Recognition Loss 0.06  Total Training Translation Loss 13.29 
2024-02-01 10:16:54,884 EPOCH 219
2024-02-01 10:17:11,604 Epoch 219: Total Training Recognition Loss 0.06  Total Training Translation Loss 12.74 
2024-02-01 10:17:11,604 EPOCH 220
2024-02-01 10:17:28,464 Epoch 220: Total Training Recognition Loss 0.06  Total Training Translation Loss 12.68 
2024-02-01 10:17:28,464 EPOCH 221
2024-02-01 10:17:45,082 Epoch 221: Total Training Recognition Loss 0.06  Total Training Translation Loss 12.24 
2024-02-01 10:17:45,082 EPOCH 222
2024-02-01 10:18:01,814 Epoch 222: Total Training Recognition Loss 0.06  Total Training Translation Loss 12.31 
2024-02-01 10:18:01,814 EPOCH 223
2024-02-01 10:18:06,871 [Epoch: 223 Step: 00002000] Batch Recognition Loss:   0.006363 => Gls Tokens per Sec:      330 || Batch Translation Loss:   0.517761 => Txt Tokens per Sec:      971 || Lr: 0.000100
2024-02-01 10:20:19,575 Hooray! New best validation result [eval_metric]!
2024-02-01 10:20:19,575 Saving new checkpoint.
2024-02-01 10:20:19,985 Validation result at epoch 223, step     2000: duration: 133.1134s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00599	Translation Loss: 75709.63281	PPL: 1951.33057
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 1.14	(BLEU-1: 12.71,	BLEU-2: 4.82,	BLEU-3: 2.14,	BLEU-4: 1.14)
	CHRF 17.87	ROUGE 11.04
2024-02-01 10:20:19,986 Logging Recognition and Translation Outputs
2024-02-01 10:20:19,986 ========================================================================================================================
2024-02-01 10:20:19,986 Logging Sequence: 182_115.00
2024-02-01 10:20:19,987 	Gloss Reference :	A B+C+D+E
2024-02-01 10:20:19,987 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 10:20:19,987 	Gloss Alignment :	         
2024-02-01 10:20:19,987 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 10:20:19,988 	Text Reference  :	fans are unclear whether yuvraj will be  returning to play test match odi or in  t20   leagues from february 2022   
2024-02-01 10:20:19,989 	Text Hypothesis :	**** *** ******* ******* she    did  not want      to **** **** ***** *** ** get these rights  for  special  matches
2024-02-01 10:20:19,989 	Text Alignment  :	D    D   D       D       S      S    S   S            D    D    D     D   D  S   S     S       S    S        S      
2024-02-01 10:20:19,989 ========================================================================================================================
2024-02-01 10:20:19,989 Logging Sequence: 140_120.00
2024-02-01 10:20:19,989 	Gloss Reference :	A B+C+D+E
2024-02-01 10:20:19,989 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 10:20:19,989 	Gloss Alignment :	         
2024-02-01 10:20:19,989 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 10:20:19,991 	Text Reference  :	but why so it is because pant is a talented player and it  will help encouraging the youth         of      uttarakhand toward   sports
2024-02-01 10:20:19,991 	Text Hypothesis :	*** *** ** ** ** ******* pant ** * ******** ****** *** has made his  debut       in  international cricket in          february 2017  
2024-02-01 10:20:19,991 	Text Alignment  :	D   D   D  D  D  D            D  D D        D      D   S   S    S    S           S   S             S       S           S        S     
2024-02-01 10:20:19,991 ========================================================================================================================
2024-02-01 10:20:19,991 Logging Sequence: 85_36.00
2024-02-01 10:20:19,992 	Gloss Reference :	A B+C+D+E
2024-02-01 10:20:19,992 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 10:20:19,992 	Gloss Alignment :	         
2024-02-01 10:20:19,992 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 10:20:19,993 	Text Reference  :	**** symonds has scored 2   centuries in  26    tests that he played for his country
2024-02-01 10:20:19,993 	Text Hypothesis :	when symonds *** ****** was driving   the first match when he played for his bat    
2024-02-01 10:20:19,993 	Text Alignment  :	I            D   D      S   S         S   S     S     S                      S      
2024-02-01 10:20:19,994 ========================================================================================================================
2024-02-01 10:20:19,994 Logging Sequence: 164_100.00
2024-02-01 10:20:19,994 	Gloss Reference :	A B+C+D+E
2024-02-01 10:20:19,994 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 10:20:19,994 	Gloss Alignment :	         
2024-02-01 10:20:19,994 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 10:20:19,996 	Text Reference  :	the tv rights for broadcasting ipl matches in   india for  the     next  5  years   went to   star india for rs 23575 crore
2024-02-01 10:20:19,996 	Text Hypothesis :	*** ** group  a   consisted    of  mena    that was   made several final of india's best team on   tv    for ** ***** this 
2024-02-01 10:20:19,997 	Text Alignment  :	D   D  S      S   S            S   S       S    S     S    S       S     S  S       S    S    S    S         D  D     S    
2024-02-01 10:20:19,997 ========================================================================================================================
2024-02-01 10:20:19,997 Logging Sequence: 76_79.00
2024-02-01 10:20:19,997 	Gloss Reference :	A B+C+D+E
2024-02-01 10:20:19,997 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 10:20:19,997 	Gloss Alignment :	         
2024-02-01 10:20:19,997 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 10:20:19,998 	Text Reference  :	** **** ***** **** ***** ***** speaking to ani    csk     ceo  kasi     viswanathan said
2024-02-01 10:20:19,998 	Text Hypothesis :	on 23rd april 2021 virat kohli posted   a  mumbai indians were stressed on          hold
2024-02-01 10:20:19,998 	Text Alignment  :	I  I    I     I    I     I     S        S  S      S       S    S        S           S   
2024-02-01 10:20:19,999 ========================================================================================================================
2024-02-01 10:20:28,256 Epoch 223: Total Training Recognition Loss 0.06  Total Training Translation Loss 12.04 
2024-02-01 10:20:28,256 EPOCH 224
2024-02-01 10:20:41,253 Epoch 224: Total Training Recognition Loss 0.06  Total Training Translation Loss 11.95 
2024-02-01 10:20:41,254 EPOCH 225
2024-02-01 10:20:54,126 Epoch 225: Total Training Recognition Loss 0.06  Total Training Translation Loss 11.57 
2024-02-01 10:20:54,127 EPOCH 226
2024-02-01 10:21:07,004 Epoch 226: Total Training Recognition Loss 0.06  Total Training Translation Loss 11.24 
2024-02-01 10:21:07,005 EPOCH 227
2024-02-01 10:21:19,832 Epoch 227: Total Training Recognition Loss 0.05  Total Training Translation Loss 11.08 
2024-02-01 10:21:19,832 EPOCH 228
2024-02-01 10:21:32,722 Epoch 228: Total Training Recognition Loss 0.06  Total Training Translation Loss 10.98 
2024-02-01 10:21:32,722 EPOCH 229
2024-02-01 10:21:45,652 Epoch 229: Total Training Recognition Loss 0.06  Total Training Translation Loss 10.86 
2024-02-01 10:21:45,652 EPOCH 230
2024-02-01 10:21:58,549 Epoch 230: Total Training Recognition Loss 0.06  Total Training Translation Loss 10.58 
2024-02-01 10:21:58,549 EPOCH 231
2024-02-01 10:22:11,492 Epoch 231: Total Training Recognition Loss 0.05  Total Training Translation Loss 10.59 
2024-02-01 10:22:11,492 EPOCH 232
2024-02-01 10:22:24,361 Epoch 232: Total Training Recognition Loss 0.05  Total Training Translation Loss 10.34 
2024-02-01 10:22:24,361 EPOCH 233
2024-02-01 10:22:37,261 Epoch 233: Total Training Recognition Loss 0.05  Total Training Translation Loss 10.19 
2024-02-01 10:22:37,261 EPOCH 234
2024-02-01 10:22:40,686 [Epoch: 234 Step: 00002100] Batch Recognition Loss:   0.006370 => Gls Tokens per Sec:     1121 || Batch Translation Loss:   0.723210 => Txt Tokens per Sec:     2717 || Lr: 0.000100
2024-02-01 10:22:50,175 Epoch 234: Total Training Recognition Loss 0.05  Total Training Translation Loss 10.15 
2024-02-01 10:22:50,175 EPOCH 235
2024-02-01 10:23:03,019 Epoch 235: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.86 
2024-02-01 10:23:03,019 EPOCH 236
2024-02-01 10:23:15,933 Epoch 236: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.95 
2024-02-01 10:23:15,934 EPOCH 237
2024-02-01 10:23:28,808 Epoch 237: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.93 
2024-02-01 10:23:28,809 EPOCH 238
2024-02-01 10:23:41,684 Epoch 238: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.62 
2024-02-01 10:23:41,685 EPOCH 239
2024-02-01 10:23:54,587 Epoch 239: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.32 
2024-02-01 10:23:54,587 EPOCH 240
2024-02-01 10:24:07,431 Epoch 240: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.33 
2024-02-01 10:24:07,431 EPOCH 241
2024-02-01 10:24:20,332 Epoch 241: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.34 
2024-02-01 10:24:20,333 EPOCH 242
2024-02-01 10:24:33,233 Epoch 242: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.31 
2024-02-01 10:24:33,233 EPOCH 243
2024-02-01 10:24:45,986 Epoch 243: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.30 
2024-02-01 10:24:45,986 EPOCH 244
2024-02-01 10:24:58,915 Epoch 244: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.05 
2024-02-01 10:24:58,915 EPOCH 245
2024-02-01 10:25:05,601 [Epoch: 245 Step: 00002200] Batch Recognition Loss:   0.005543 => Gls Tokens per Sec:      633 || Batch Translation Loss:   0.617119 => Txt Tokens per Sec:     1599 || Lr: 0.000100
2024-02-01 10:25:11,892 Epoch 245: Total Training Recognition Loss 0.05  Total Training Translation Loss 8.98 
2024-02-01 10:25:11,893 EPOCH 246
2024-02-01 10:25:25,675 Epoch 246: Total Training Recognition Loss 0.05  Total Training Translation Loss 8.63 
2024-02-01 10:25:25,675 EPOCH 247
2024-02-01 10:25:38,565 Epoch 247: Total Training Recognition Loss 0.05  Total Training Translation Loss 8.53 
2024-02-01 10:25:38,566 EPOCH 248
2024-02-01 10:25:51,561 Epoch 248: Total Training Recognition Loss 0.04  Total Training Translation Loss 8.62 
2024-02-01 10:25:51,561 EPOCH 249
2024-02-01 10:26:04,473 Epoch 249: Total Training Recognition Loss 0.05  Total Training Translation Loss 8.22 
2024-02-01 10:26:04,473 EPOCH 250
2024-02-01 10:26:17,356 Epoch 250: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.92 
2024-02-01 10:26:17,356 EPOCH 251
2024-02-01 10:26:30,311 Epoch 251: Total Training Recognition Loss 0.05  Total Training Translation Loss 8.02 
2024-02-01 10:26:30,311 EPOCH 252
2024-02-01 10:26:43,057 Epoch 252: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.84 
2024-02-01 10:26:43,057 EPOCH 253
2024-02-01 10:26:56,035 Epoch 253: Total Training Recognition Loss 0.05  Total Training Translation Loss 7.75 
2024-02-01 10:26:56,035 EPOCH 254
2024-02-01 10:27:08,896 Epoch 254: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.74 
2024-02-01 10:27:08,896 EPOCH 255
2024-02-01 10:27:21,777 Epoch 255: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.73 
2024-02-01 10:27:21,777 EPOCH 256
2024-02-01 10:27:24,835 [Epoch: 256 Step: 00002300] Batch Recognition Loss:   0.004538 => Gls Tokens per Sec:     2094 || Batch Translation Loss:   0.748726 => Txt Tokens per Sec:     5650 || Lr: 0.000100
2024-02-01 10:27:34,593 Epoch 256: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.92 
2024-02-01 10:27:34,593 EPOCH 257
2024-02-01 10:27:47,385 Epoch 257: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.88 
2024-02-01 10:27:47,386 EPOCH 258
2024-02-01 10:28:00,468 Epoch 258: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.76 
2024-02-01 10:28:00,468 EPOCH 259
2024-02-01 10:28:13,393 Epoch 259: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.64 
2024-02-01 10:28:13,394 EPOCH 260
2024-02-01 10:28:26,208 Epoch 260: Total Training Recognition Loss 0.05  Total Training Translation Loss 7.54 
2024-02-01 10:28:26,209 EPOCH 261
2024-02-01 10:28:38,976 Epoch 261: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.18 
2024-02-01 10:28:38,977 EPOCH 262
2024-02-01 10:28:51,779 Epoch 262: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.11 
2024-02-01 10:28:51,779 EPOCH 263
2024-02-01 10:29:04,782 Epoch 263: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.88 
2024-02-01 10:29:04,782 EPOCH 264
2024-02-01 10:29:17,784 Epoch 264: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.52 
2024-02-01 10:29:17,784 EPOCH 265
2024-02-01 10:29:30,583 Epoch 265: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.53 
2024-02-01 10:29:30,584 EPOCH 266
2024-02-01 10:29:43,459 Epoch 266: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.53 
2024-02-01 10:29:43,459 EPOCH 267
2024-02-01 10:29:52,311 [Epoch: 267 Step: 00002400] Batch Recognition Loss:   0.004150 => Gls Tokens per Sec:      767 || Batch Translation Loss:   0.818067 => Txt Tokens per Sec:     2210 || Lr: 0.000100
2024-02-01 10:29:56,324 Epoch 267: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.33 
2024-02-01 10:29:56,324 EPOCH 268
2024-02-01 10:30:08,996 Epoch 268: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.34 
2024-02-01 10:30:08,996 EPOCH 269
2024-02-01 10:30:21,969 Epoch 269: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.24 
2024-02-01 10:30:21,969 EPOCH 270
2024-02-01 10:30:34,874 Epoch 270: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.16 
2024-02-01 10:30:34,874 EPOCH 271
2024-02-01 10:30:47,690 Epoch 271: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.02 
2024-02-01 10:30:47,690 EPOCH 272
2024-02-01 10:31:00,593 Epoch 272: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.08 
2024-02-01 10:31:00,593 EPOCH 273
2024-02-01 10:31:13,442 Epoch 273: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.01 
2024-02-01 10:31:13,442 EPOCH 274
2024-02-01 10:31:26,344 Epoch 274: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.06 
2024-02-01 10:31:26,344 EPOCH 275
2024-02-01 10:31:39,274 Epoch 275: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.95 
2024-02-01 10:31:39,274 EPOCH 276
2024-02-01 10:31:52,245 Epoch 276: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.84 
2024-02-01 10:31:52,245 EPOCH 277
2024-02-01 10:32:05,188 Epoch 277: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.69 
2024-02-01 10:32:05,188 EPOCH 278
2024-02-01 10:32:15,964 [Epoch: 278 Step: 00002500] Batch Recognition Loss:   0.003001 => Gls Tokens per Sec:      749 || Batch Translation Loss:   0.255813 => Txt Tokens per Sec:     2031 || Lr: 0.000100
2024-02-01 10:32:18,032 Epoch 278: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.52 
2024-02-01 10:32:18,033 EPOCH 279
2024-02-01 10:32:30,933 Epoch 279: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.62 
2024-02-01 10:32:30,933 EPOCH 280
2024-02-01 10:32:43,851 Epoch 280: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.85 
2024-02-01 10:32:43,851 EPOCH 281
2024-02-01 10:32:56,813 Epoch 281: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.11 
2024-02-01 10:32:56,813 EPOCH 282
2024-02-01 10:33:09,706 Epoch 282: Total Training Recognition Loss 0.04  Total Training Translation Loss 11.16 
2024-02-01 10:33:09,706 EPOCH 283
2024-02-01 10:33:22,667 Epoch 283: Total Training Recognition Loss 0.07  Total Training Translation Loss 47.42 
2024-02-01 10:33:22,667 EPOCH 284
2024-02-01 10:33:35,483 Epoch 284: Total Training Recognition Loss 0.25  Total Training Translation Loss 22.27 
2024-02-01 10:33:35,483 EPOCH 285
2024-02-01 10:33:48,298 Epoch 285: Total Training Recognition Loss 0.10  Total Training Translation Loss 17.08 
2024-02-01 10:33:48,298 EPOCH 286
2024-02-01 10:34:01,311 Epoch 286: Total Training Recognition Loss 0.07  Total Training Translation Loss 12.53 
2024-02-01 10:34:01,311 EPOCH 287
2024-02-01 10:34:14,097 Epoch 287: Total Training Recognition Loss 0.09  Total Training Translation Loss 9.62 
2024-02-01 10:34:14,097 EPOCH 288
2024-02-01 10:34:27,083 Epoch 288: Total Training Recognition Loss 0.06  Total Training Translation Loss 8.07 
2024-02-01 10:34:27,083 EPOCH 289
2024-02-01 10:34:37,063 [Epoch: 289 Step: 00002600] Batch Recognition Loss:   0.006276 => Gls Tokens per Sec:      937 || Batch Translation Loss:   0.928219 => Txt Tokens per Sec:     2543 || Lr: 0.000100
2024-02-01 10:34:39,986 Epoch 289: Total Training Recognition Loss 0.05  Total Training Translation Loss 7.17 
2024-02-01 10:34:39,986 EPOCH 290
2024-02-01 10:34:52,874 Epoch 290: Total Training Recognition Loss 0.05  Total Training Translation Loss 6.43 
2024-02-01 10:34:52,874 EPOCH 291
2024-02-01 10:35:05,739 Epoch 291: Total Training Recognition Loss 0.05  Total Training Translation Loss 6.09 
2024-02-01 10:35:05,740 EPOCH 292
2024-02-01 10:35:18,628 Epoch 292: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.76 
2024-02-01 10:35:18,629 EPOCH 293
2024-02-01 10:35:31,538 Epoch 293: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.42 
2024-02-01 10:35:31,538 EPOCH 294
2024-02-01 10:35:44,404 Epoch 294: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.34 
2024-02-01 10:35:44,404 EPOCH 295
2024-02-01 10:35:57,238 Epoch 295: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.09 
2024-02-01 10:35:57,238 EPOCH 296
2024-02-01 10:36:10,238 Epoch 296: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.88 
2024-02-01 10:36:10,238 EPOCH 297
2024-02-01 10:36:23,163 Epoch 297: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.77 
2024-02-01 10:36:23,163 EPOCH 298
2024-02-01 10:36:35,983 Epoch 298: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.62 
2024-02-01 10:36:35,984 EPOCH 299
2024-02-01 10:36:48,834 Epoch 299: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.52 
2024-02-01 10:36:48,834 EPOCH 300
2024-02-01 10:37:01,773 [Epoch: 300 Step: 00002700] Batch Recognition Loss:   0.003952 => Gls Tokens per Sec:      822 || Batch Translation Loss:   0.649831 => Txt Tokens per Sec:     2281 || Lr: 0.000100
2024-02-01 10:37:01,773 Epoch 300: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.49 
2024-02-01 10:37:01,773 EPOCH 301
2024-02-01 10:37:14,645 Epoch 301: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.31 
2024-02-01 10:37:14,645 EPOCH 302
2024-02-01 10:37:27,414 Epoch 302: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.25 
2024-02-01 10:37:27,415 EPOCH 303
2024-02-01 10:37:40,286 Epoch 303: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.14 
2024-02-01 10:37:40,287 EPOCH 304
2024-02-01 10:37:53,144 Epoch 304: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.17 
2024-02-01 10:37:53,144 EPOCH 305
2024-02-01 10:38:06,005 Epoch 305: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.04 
2024-02-01 10:38:06,005 EPOCH 306
2024-02-01 10:38:18,871 Epoch 306: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.00 
2024-02-01 10:38:18,872 EPOCH 307
2024-02-01 10:38:31,760 Epoch 307: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.03 
2024-02-01 10:38:31,760 EPOCH 308
2024-02-01 10:38:44,687 Epoch 308: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.94 
2024-02-01 10:38:44,688 EPOCH 309
2024-02-01 10:38:57,647 Epoch 309: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.90 
2024-02-01 10:38:57,647 EPOCH 310
2024-02-01 10:39:10,592 Epoch 310: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.81 
2024-02-01 10:39:10,592 EPOCH 311
2024-02-01 10:39:23,363 Epoch 311: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.78 
2024-02-01 10:39:23,363 EPOCH 312
2024-02-01 10:39:24,994 [Epoch: 312 Step: 00002800] Batch Recognition Loss:   0.003412 => Gls Tokens per Sec:      785 || Batch Translation Loss:   0.520609 => Txt Tokens per Sec:     2392 || Lr: 0.000100
2024-02-01 10:39:36,305 Epoch 312: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.71 
2024-02-01 10:39:36,305 EPOCH 313
2024-02-01 10:39:49,260 Epoch 313: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.71 
2024-02-01 10:39:49,260 EPOCH 314
2024-02-01 10:40:02,218 Epoch 314: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.61 
2024-02-01 10:40:02,218 EPOCH 315
2024-02-01 10:40:15,202 Epoch 315: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.79 
2024-02-01 10:40:15,202 EPOCH 316
2024-02-01 10:40:28,052 Epoch 316: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.56 
2024-02-01 10:40:28,052 EPOCH 317
2024-02-01 10:40:41,011 Epoch 317: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.55 
2024-02-01 10:40:41,011 EPOCH 318
2024-02-01 10:40:53,784 Epoch 318: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.48 
2024-02-01 10:40:53,785 EPOCH 319
2024-02-01 10:41:06,675 Epoch 319: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.41 
2024-02-01 10:41:06,675 EPOCH 320
2024-02-01 10:41:19,410 Epoch 320: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.41 
2024-02-01 10:41:19,410 EPOCH 321
2024-02-01 10:41:32,316 Epoch 321: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.33 
2024-02-01 10:41:32,316 EPOCH 322
2024-02-01 10:41:45,168 Epoch 322: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.23 
2024-02-01 10:41:45,168 EPOCH 323
2024-02-01 10:41:47,038 [Epoch: 323 Step: 00002900] Batch Recognition Loss:   0.002607 => Gls Tokens per Sec:     1370 || Batch Translation Loss:   0.329948 => Txt Tokens per Sec:     3615 || Lr: 0.000100
2024-02-01 10:41:58,000 Epoch 323: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.30 
2024-02-01 10:41:58,000 EPOCH 324
2024-02-01 10:42:10,890 Epoch 324: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.25 
2024-02-01 10:42:10,890 EPOCH 325
2024-02-01 10:42:23,673 Epoch 325: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.26 
2024-02-01 10:42:23,674 EPOCH 326
2024-02-01 10:42:36,578 Epoch 326: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.26 
2024-02-01 10:42:36,578 EPOCH 327
2024-02-01 10:42:49,419 Epoch 327: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.25 
2024-02-01 10:42:49,419 EPOCH 328
2024-02-01 10:43:02,158 Epoch 328: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.16 
2024-02-01 10:43:02,158 EPOCH 329
2024-02-01 10:43:15,102 Epoch 329: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.15 
2024-02-01 10:43:15,102 EPOCH 330
2024-02-01 10:43:27,952 Epoch 330: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.01 
2024-02-01 10:43:27,952 EPOCH 331
2024-02-01 10:43:40,777 Epoch 331: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.05 
2024-02-01 10:43:40,778 EPOCH 332
2024-02-01 10:43:53,536 Epoch 332: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.08 
2024-02-01 10:43:53,536 EPOCH 333
2024-02-01 10:44:06,522 Epoch 333: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.07 
2024-02-01 10:44:06,523 EPOCH 334
2024-02-01 10:44:08,707 [Epoch: 334 Step: 00003000] Batch Recognition Loss:   0.002000 => Gls Tokens per Sec:     1758 || Batch Translation Loss:   0.289192 => Txt Tokens per Sec:     4589 || Lr: 0.000100
2024-02-01 10:44:19,261 Epoch 334: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.99 
2024-02-01 10:44:19,261 EPOCH 335
2024-02-01 10:44:32,299 Epoch 335: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.01 
2024-02-01 10:44:32,300 EPOCH 336
2024-02-01 10:44:45,178 Epoch 336: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.81 
2024-02-01 10:44:45,178 EPOCH 337
2024-02-01 10:44:58,176 Epoch 337: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.91 
2024-02-01 10:44:58,177 EPOCH 338
2024-02-01 10:45:11,050 Epoch 338: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.80 
2024-02-01 10:45:11,050 EPOCH 339
2024-02-01 10:45:24,022 Epoch 339: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.78 
2024-02-01 10:45:24,023 EPOCH 340
2024-02-01 10:45:36,919 Epoch 340: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.82 
2024-02-01 10:45:36,919 EPOCH 341
2024-02-01 10:45:49,704 Epoch 341: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.80 
2024-02-01 10:45:49,704 EPOCH 342
2024-02-01 10:46:02,563 Epoch 342: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.84 
2024-02-01 10:46:02,563 EPOCH 343
2024-02-01 10:46:15,393 Epoch 343: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.81 
2024-02-01 10:46:15,394 EPOCH 344
2024-02-01 10:46:28,241 Epoch 344: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.73 
2024-02-01 10:46:28,242 EPOCH 345
2024-02-01 10:46:36,144 [Epoch: 345 Step: 00003100] Batch Recognition Loss:   0.002245 => Gls Tokens per Sec:      535 || Batch Translation Loss:   0.360045 => Txt Tokens per Sec:     1586 || Lr: 0.000100
2024-02-01 10:46:40,922 Epoch 345: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.79 
2024-02-01 10:46:40,922 EPOCH 346
2024-02-01 10:46:53,939 Epoch 346: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.89 
2024-02-01 10:46:53,939 EPOCH 347
2024-02-01 10:47:06,752 Epoch 347: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.79 
2024-02-01 10:47:06,752 EPOCH 348
2024-02-01 10:47:19,718 Epoch 348: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.80 
2024-02-01 10:47:19,718 EPOCH 349
2024-02-01 10:47:32,698 Epoch 349: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.83 
2024-02-01 10:47:32,698 EPOCH 350
2024-02-01 10:47:45,594 Epoch 350: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.04 
2024-02-01 10:47:45,595 EPOCH 351
2024-02-01 10:47:58,601 Epoch 351: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.92 
2024-02-01 10:47:58,601 EPOCH 352
2024-02-01 10:48:11,578 Epoch 352: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.86 
2024-02-01 10:48:11,578 EPOCH 353
2024-02-01 10:48:24,588 Epoch 353: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.01 
2024-02-01 10:48:24,588 EPOCH 354
2024-02-01 10:48:37,425 Epoch 354: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.95 
2024-02-01 10:48:37,426 EPOCH 355
2024-02-01 10:48:50,369 Epoch 355: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.83 
2024-02-01 10:48:50,370 EPOCH 356
2024-02-01 10:48:57,645 [Epoch: 356 Step: 00003200] Batch Recognition Loss:   0.003274 => Gls Tokens per Sec:      757 || Batch Translation Loss:   0.174292 => Txt Tokens per Sec:     2024 || Lr: 0.000100
2024-02-01 10:49:03,414 Epoch 356: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.64 
2024-02-01 10:49:03,415 EPOCH 357
2024-02-01 10:49:16,266 Epoch 357: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.76 
2024-02-01 10:49:16,267 EPOCH 358
2024-02-01 10:49:29,053 Epoch 358: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.67 
2024-02-01 10:49:29,053 EPOCH 359
2024-02-01 10:49:42,021 Epoch 359: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.49 
2024-02-01 10:49:42,021 EPOCH 360
2024-02-01 10:49:54,993 Epoch 360: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.55 
2024-02-01 10:49:54,993 EPOCH 361
2024-02-01 10:50:07,801 Epoch 361: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.50 
2024-02-01 10:50:07,802 EPOCH 362
2024-02-01 10:50:20,432 Epoch 362: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.45 
2024-02-01 10:50:20,432 EPOCH 363
2024-02-01 10:50:33,343 Epoch 363: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.43 
2024-02-01 10:50:33,343 EPOCH 364
2024-02-01 10:50:46,396 Epoch 364: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.37 
2024-02-01 10:50:46,396 EPOCH 365
2024-02-01 10:50:59,276 Epoch 365: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.35 
2024-02-01 10:50:59,276 EPOCH 366
2024-02-01 10:51:12,203 Epoch 366: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.27 
2024-02-01 10:51:12,203 EPOCH 367
2024-02-01 10:51:18,530 [Epoch: 367 Step: 00003300] Batch Recognition Loss:   0.001504 => Gls Tokens per Sec:     1214 || Batch Translation Loss:   0.193437 => Txt Tokens per Sec:     3212 || Lr: 0.000100
2024-02-01 10:51:25,112 Epoch 367: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.27 
2024-02-01 10:51:25,113 EPOCH 368
2024-02-01 10:51:37,988 Epoch 368: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.26 
2024-02-01 10:51:37,988 EPOCH 369
2024-02-01 10:51:50,899 Epoch 369: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.46 
2024-02-01 10:51:50,900 EPOCH 370
2024-02-01 10:52:03,956 Epoch 370: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.32 
2024-02-01 10:52:03,957 EPOCH 371
2024-02-01 10:52:16,805 Epoch 371: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.13 
2024-02-01 10:52:16,805 EPOCH 372
2024-02-01 10:52:29,689 Epoch 372: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.22 
2024-02-01 10:52:29,690 EPOCH 373
2024-02-01 10:52:42,535 Epoch 373: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.17 
2024-02-01 10:52:42,536 EPOCH 374
2024-02-01 10:52:55,484 Epoch 374: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.06 
2024-02-01 10:52:55,484 EPOCH 375
2024-02-01 10:53:08,302 Epoch 375: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.16 
2024-02-01 10:53:08,303 EPOCH 376
2024-02-01 10:53:20,979 Epoch 376: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.15 
2024-02-01 10:53:20,979 EPOCH 377
2024-02-01 10:53:33,979 Epoch 377: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.17 
2024-02-01 10:53:33,979 EPOCH 378
2024-02-01 10:53:45,930 [Epoch: 378 Step: 00003400] Batch Recognition Loss:   0.001979 => Gls Tokens per Sec:      675 || Batch Translation Loss:   0.173294 => Txt Tokens per Sec:     1862 || Lr: 0.000100
2024-02-01 10:53:46,833 Epoch 378: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.24 
2024-02-01 10:53:46,834 EPOCH 379
2024-02-01 10:53:59,655 Epoch 379: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.07 
2024-02-01 10:53:59,655 EPOCH 380
2024-02-01 10:54:12,544 Epoch 380: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.32 
2024-02-01 10:54:12,544 EPOCH 381
2024-02-01 10:54:25,346 Epoch 381: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.24 
2024-02-01 10:54:25,346 EPOCH 382
2024-02-01 10:54:38,236 Epoch 382: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.33 
2024-02-01 10:54:38,236 EPOCH 383
2024-02-01 10:54:51,131 Epoch 383: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.45 
2024-02-01 10:54:51,131 EPOCH 384
2024-02-01 10:55:03,873 Epoch 384: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.43 
2024-02-01 10:55:03,874 EPOCH 385
2024-02-01 10:55:16,705 Epoch 385: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.64 
2024-02-01 10:55:16,706 EPOCH 386
2024-02-01 10:55:30,255 Epoch 386: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.74 
2024-02-01 10:55:30,255 EPOCH 387
2024-02-01 10:55:43,210 Epoch 387: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.37 
2024-02-01 10:55:43,210 EPOCH 388
2024-02-01 10:55:56,043 Epoch 388: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.43 
2024-02-01 10:55:56,043 EPOCH 389
2024-02-01 10:56:08,742 [Epoch: 389 Step: 00003500] Batch Recognition Loss:   0.001703 => Gls Tokens per Sec:      736 || Batch Translation Loss:   0.197644 => Txt Tokens per Sec:     2145 || Lr: 0.000100
2024-02-01 10:56:08,956 Epoch 389: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.32 
2024-02-01 10:56:08,956 EPOCH 390
2024-02-01 10:56:21,872 Epoch 390: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.20 
2024-02-01 10:56:21,872 EPOCH 391
2024-02-01 10:56:34,769 Epoch 391: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.20 
2024-02-01 10:56:34,769 EPOCH 392
2024-02-01 10:56:47,624 Epoch 392: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.08 
2024-02-01 10:56:47,624 EPOCH 393
2024-02-01 10:57:00,599 Epoch 393: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.96 
2024-02-01 10:57:00,599 EPOCH 394
2024-02-01 10:57:13,555 Epoch 394: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.88 
2024-02-01 10:57:13,555 EPOCH 395
2024-02-01 10:57:26,452 Epoch 395: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.86 
2024-02-01 10:57:26,452 EPOCH 396
2024-02-01 10:57:39,403 Epoch 396: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.93 
2024-02-01 10:57:39,403 EPOCH 397
2024-02-01 10:57:52,173 Epoch 397: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.85 
2024-02-01 10:57:52,173 EPOCH 398
2024-02-01 10:58:05,129 Epoch 398: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.90 
2024-02-01 10:58:05,130 EPOCH 399
2024-02-01 10:58:18,066 Epoch 399: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.83 
2024-02-01 10:58:18,066 EPOCH 400
2024-02-01 10:58:30,870 [Epoch: 400 Step: 00003600] Batch Recognition Loss:   0.001741 => Gls Tokens per Sec:      830 || Batch Translation Loss:   0.248153 => Txt Tokens per Sec:     2305 || Lr: 0.000100
2024-02-01 10:58:30,870 Epoch 400: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.89 
2024-02-01 10:58:30,871 EPOCH 401
2024-02-01 10:58:43,842 Epoch 401: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.09 
2024-02-01 10:58:43,842 EPOCH 402
2024-02-01 10:58:56,769 Epoch 402: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.12 
2024-02-01 10:58:56,769 EPOCH 403
2024-02-01 10:59:09,787 Epoch 403: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.02 
2024-02-01 10:59:09,787 EPOCH 404
2024-02-01 10:59:22,742 Epoch 404: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.05 
2024-02-01 10:59:22,743 EPOCH 405
2024-02-01 10:59:35,634 Epoch 405: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.95 
2024-02-01 10:59:35,635 EPOCH 406
2024-02-01 10:59:48,605 Epoch 406: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.07 
2024-02-01 10:59:48,605 EPOCH 407
2024-02-01 11:00:01,431 Epoch 407: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.03 
2024-02-01 11:00:01,431 EPOCH 408
2024-02-01 11:00:14,341 Epoch 408: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.94 
2024-02-01 11:00:14,341 EPOCH 409
2024-02-01 11:00:27,232 Epoch 409: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.88 
2024-02-01 11:00:27,233 EPOCH 410
2024-02-01 11:00:40,164 Epoch 410: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.96 
2024-02-01 11:00:40,164 EPOCH 411
2024-02-01 11:00:53,081 Epoch 411: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.04 
2024-02-01 11:00:53,081 EPOCH 412
2024-02-01 11:00:56,270 [Epoch: 412 Step: 00003700] Batch Recognition Loss:   0.001285 => Gls Tokens per Sec:      122 || Batch Translation Loss:   0.099856 => Txt Tokens per Sec:      438 || Lr: 0.000100
2024-02-01 11:01:05,868 Epoch 412: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.94 
2024-02-01 11:01:05,868 EPOCH 413
2024-02-01 11:01:18,900 Epoch 413: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.94 
2024-02-01 11:01:18,900 EPOCH 414
2024-02-01 11:01:31,771 Epoch 414: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.95 
2024-02-01 11:01:31,771 EPOCH 415
2024-02-01 11:01:44,614 Epoch 415: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.89 
2024-02-01 11:01:44,614 EPOCH 416
2024-02-01 11:01:57,521 Epoch 416: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.89 
2024-02-01 11:01:57,522 EPOCH 417
2024-02-01 11:02:10,360 Epoch 417: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.09 
2024-02-01 11:02:10,360 EPOCH 418
2024-02-01 11:02:23,467 Epoch 418: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.22 
2024-02-01 11:02:23,467 EPOCH 419
2024-02-01 11:02:36,296 Epoch 419: Total Training Recognition Loss 0.02  Total Training Translation Loss 9.69 
2024-02-01 11:02:36,296 EPOCH 420
2024-02-01 11:02:49,250 Epoch 420: Total Training Recognition Loss 0.06  Total Training Translation Loss 9.18 
2024-02-01 11:02:49,250 EPOCH 421
2024-02-01 11:03:02,204 Epoch 421: Total Training Recognition Loss 0.05  Total Training Translation Loss 8.08 
2024-02-01 11:03:02,205 EPOCH 422
2024-02-01 11:03:15,171 Epoch 422: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.79 
2024-02-01 11:03:15,171 EPOCH 423
2024-02-01 11:03:18,960 [Epoch: 423 Step: 00003800] Batch Recognition Loss:   0.002232 => Gls Tokens per Sec:      676 || Batch Translation Loss:   0.382772 => Txt Tokens per Sec:     2021 || Lr: 0.000100
2024-02-01 11:03:28,133 Epoch 423: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.61 
2024-02-01 11:03:28,133 EPOCH 424
2024-02-01 11:03:41,034 Epoch 424: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.23 
2024-02-01 11:03:41,034 EPOCH 425
2024-02-01 11:03:53,889 Epoch 425: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.07 
2024-02-01 11:03:53,889 EPOCH 426
2024-02-01 11:04:06,697 Epoch 426: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.77 
2024-02-01 11:04:06,697 EPOCH 427
2024-02-01 11:04:19,759 Epoch 427: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.15 
2024-02-01 11:04:19,759 EPOCH 428
2024-02-01 11:04:32,653 Epoch 428: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.63 
2024-02-01 11:04:32,654 EPOCH 429
2024-02-01 11:04:45,545 Epoch 429: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.32 
2024-02-01 11:04:45,545 EPOCH 430
2024-02-01 11:04:58,554 Epoch 430: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.07 
2024-02-01 11:04:58,554 EPOCH 431
2024-02-01 11:05:11,368 Epoch 431: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.83 
2024-02-01 11:05:11,369 EPOCH 432
2024-02-01 11:05:24,266 Epoch 432: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.69 
2024-02-01 11:05:24,266 EPOCH 433
2024-02-01 11:05:37,193 Epoch 433: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.63 
2024-02-01 11:05:37,193 EPOCH 434
2024-02-01 11:05:38,288 [Epoch: 434 Step: 00003900] Batch Recognition Loss:   0.001396 => Gls Tokens per Sec:     3510 || Batch Translation Loss:   0.180068 => Txt Tokens per Sec:     9059 || Lr: 0.000100
2024-02-01 11:05:49,976 Epoch 434: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.65 
2024-02-01 11:05:49,976 EPOCH 435
2024-02-01 11:06:02,980 Epoch 435: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.56 
2024-02-01 11:06:02,981 EPOCH 436
2024-02-01 11:06:15,832 Epoch 436: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.43 
2024-02-01 11:06:15,832 EPOCH 437
2024-02-01 11:06:28,826 Epoch 437: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.42 
2024-02-01 11:06:28,826 EPOCH 438
2024-02-01 11:06:41,846 Epoch 438: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.40 
2024-02-01 11:06:41,846 EPOCH 439
2024-02-01 11:06:54,700 Epoch 439: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.43 
2024-02-01 11:06:54,700 EPOCH 440
2024-02-01 11:07:07,590 Epoch 440: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.44 
2024-02-01 11:07:07,590 EPOCH 441
2024-02-01 11:07:20,457 Epoch 441: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.38 
2024-02-01 11:07:20,458 EPOCH 442
2024-02-01 11:07:33,326 Epoch 442: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.30 
2024-02-01 11:07:33,326 EPOCH 443
2024-02-01 11:07:46,230 Epoch 443: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.34 
2024-02-01 11:07:46,230 EPOCH 444
2024-02-01 11:07:59,037 Epoch 444: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.29 
2024-02-01 11:07:59,037 EPOCH 445
2024-02-01 11:08:03,485 [Epoch: 445 Step: 00004000] Batch Recognition Loss:   0.001484 => Gls Tokens per Sec:     1151 || Batch Translation Loss:   0.202903 => Txt Tokens per Sec:     3179 || Lr: 0.000100
2024-02-01 11:08:22,010 Validation result at epoch 445, step     4000: duration: 18.5245s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00092	Translation Loss: 83831.10156	PPL: 4398.34131
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.99	(BLEU-1: 12.63,	BLEU-2: 4.52,	BLEU-3: 1.96,	BLEU-4: 0.99)
	CHRF 18.09	ROUGE 10.76
2024-02-01 11:08:22,011 Logging Recognition and Translation Outputs
2024-02-01 11:08:22,011 ========================================================================================================================
2024-02-01 11:08:22,011 Logging Sequence: 133_173.00
2024-02-01 11:08:22,011 	Gloss Reference :	A B+C+D+E
2024-02-01 11:08:22,012 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 11:08:22,012 	Gloss Alignment :	         
2024-02-01 11:08:22,012 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 11:08:22,013 	Text Reference  :	according to sources the leaders of  the two    countries are    set to           join the   commentary panel   as  well   
2024-02-01 11:08:22,014 	Text Hypothesis :	********* ** ******* you can     see his jovial behaviour during t20 championship is   virat kohli      through his arrival
2024-02-01 11:08:22,014 	Text Alignment  :	D         D  D       S   S       S   S   S      S         S      S   S            S    S     S          S       S   S      
2024-02-01 11:08:22,014 ========================================================================================================================
2024-02-01 11:08:22,014 Logging Sequence: 83_33.00
2024-02-01 11:08:22,014 	Gloss Reference :	A B+C+D+E
2024-02-01 11:08:22,014 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 11:08:22,015 	Gloss Alignment :	         
2024-02-01 11:08:22,015 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 11:08:22,016 	Text Reference  :	*** ******* ** a       football match lasts for *** ********** two  equal halves of   45   minutes
2024-02-01 11:08:22,016 	Text Hypothesis :	the denmark vs finland football match ***** for the tournament were going on     12th june 2023   
2024-02-01 11:08:22,016 	Text Alignment  :	I   I       I  S                      D         I   I          S    S     S      S    S    S      
2024-02-01 11:08:22,016 ========================================================================================================================
2024-02-01 11:08:22,016 Logging Sequence: 68_147.00
2024-02-01 11:08:22,016 	Gloss Reference :	A B+C+D+E
2024-02-01 11:08:22,017 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 11:08:22,017 	Gloss Alignment :	         
2024-02-01 11:08:22,017 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 11:08:22,019 	Text Reference  :	**** ** ** *********** **** **** **** **** remember the   2007 t20 world cup       amid a   lot      of sledging by english players
2024-02-01 11:08:22,019 	Text Hypothesis :	here is an interesting fact fans said that stuart   broad must be  very  irritated to   see smashing 35 runs     in just    balls  
2024-02-01 11:08:22,019 	Text Alignment  :	I    I  I  I           I    I    I    I    S        S     S    S   S     S         S    S   S        S  S        S  S       S      
2024-02-01 11:08:22,019 ========================================================================================================================
2024-02-01 11:08:22,019 Logging Sequence: 165_8.00
2024-02-01 11:08:22,019 	Gloss Reference :	A B+C+D+E
2024-02-01 11:08:22,020 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 11:08:22,020 	Gloss Alignment :	         
2024-02-01 11:08:22,020 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 11:08:22,021 	Text Reference  :	** ******** however many don't believe in     it      it varies among people
2024-02-01 11:08:22,021 	Text Hypothesis :	he believed that    his  bag   are     moving forward to the    world cup   
2024-02-01 11:08:22,021 	Text Alignment  :	I  I        S       S    S     S       S      S       S  S      S     S     
2024-02-01 11:08:22,021 ========================================================================================================================
2024-02-01 11:08:22,021 Logging Sequence: 119_71.00
2024-02-01 11:08:22,021 	Gloss Reference :	A B+C+D+E
2024-02-01 11:08:22,021 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 11:08:22,022 	Gloss Alignment :	         
2024-02-01 11:08:22,022 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 11:08:22,023 	Text Reference  :	the special gold devices have each player'    names and  jersey numbers next   to the     camera
2024-02-01 11:08:22,023 	Text Hypothesis :	*** idesign gold ******* is   a    contingent are   sold in     the     reason of idesign gold  
2024-02-01 11:08:22,023 	Text Alignment  :	D   S            D       S    S    S          S     S    S      S       S      S  S       S     
2024-02-01 11:08:22,023 ========================================================================================================================
2024-02-01 11:08:30,793 Epoch 445: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-01 11:08:30,793 EPOCH 446
2024-02-01 11:08:43,685 Epoch 446: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.22 
2024-02-01 11:08:43,685 EPOCH 447
2024-02-01 11:08:56,631 Epoch 447: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-01 11:08:56,631 EPOCH 448
2024-02-01 11:09:09,421 Epoch 448: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-01 11:09:09,421 EPOCH 449
2024-02-01 11:09:22,216 Epoch 449: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-01 11:09:22,216 EPOCH 450
2024-02-01 11:09:35,172 Epoch 450: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.22 
2024-02-01 11:09:35,172 EPOCH 451
2024-02-01 11:09:48,066 Epoch 451: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.21 
2024-02-01 11:09:48,066 EPOCH 452
2024-02-01 11:10:00,924 Epoch 452: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-01 11:10:00,924 EPOCH 453
2024-02-01 11:10:13,865 Epoch 453: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.30 
2024-02-01 11:10:13,865 EPOCH 454
2024-02-01 11:10:26,720 Epoch 454: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-01 11:10:26,720 EPOCH 455
2024-02-01 11:10:39,651 Epoch 455: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-01 11:10:39,652 EPOCH 456
2024-02-01 11:10:48,190 [Epoch: 456 Step: 00004100] Batch Recognition Loss:   0.001132 => Gls Tokens per Sec:      645 || Batch Translation Loss:   0.159947 => Txt Tokens per Sec:     1947 || Lr: 0.000100
2024-02-01 11:10:52,540 Epoch 456: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-01 11:10:52,540 EPOCH 457
2024-02-01 11:11:05,429 Epoch 457: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-01 11:11:05,429 EPOCH 458
2024-02-01 11:11:18,217 Epoch 458: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-01 11:11:18,217 EPOCH 459
2024-02-01 11:11:31,190 Epoch 459: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-01 11:11:31,190 EPOCH 460
2024-02-01 11:11:43,929 Epoch 460: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-01 11:11:43,930 EPOCH 461
2024-02-01 11:11:56,860 Epoch 461: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-01 11:11:56,860 EPOCH 462
2024-02-01 11:12:09,621 Epoch 462: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-01 11:12:09,622 EPOCH 463
2024-02-01 11:12:22,515 Epoch 463: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-01 11:12:22,515 EPOCH 464
2024-02-01 11:12:35,428 Epoch 464: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-01 11:12:35,428 EPOCH 465
2024-02-01 11:12:48,427 Epoch 465: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-01 11:12:48,427 EPOCH 466
2024-02-01 11:13:01,414 Epoch 466: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-01 11:13:01,414 EPOCH 467
2024-02-01 11:13:13,405 [Epoch: 467 Step: 00004200] Batch Recognition Loss:   0.000654 => Gls Tokens per Sec:      566 || Batch Translation Loss:   0.044260 => Txt Tokens per Sec:     1734 || Lr: 0.000100
2024-02-01 11:13:14,267 Epoch 467: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-01 11:13:14,267 EPOCH 468
2024-02-01 11:13:27,072 Epoch 468: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-01 11:13:27,073 EPOCH 469
2024-02-01 11:13:39,928 Epoch 469: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-01 11:13:39,928 EPOCH 470
2024-02-01 11:13:52,806 Epoch 470: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-01 11:13:52,806 EPOCH 471
2024-02-01 11:14:05,723 Epoch 471: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-01 11:14:05,723 EPOCH 472
2024-02-01 11:14:18,733 Epoch 472: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-01 11:14:18,733 EPOCH 473
2024-02-01 11:14:31,772 Epoch 473: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-01 11:14:31,772 EPOCH 474
2024-02-01 11:14:44,643 Epoch 474: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-01 11:14:44,643 EPOCH 475
2024-02-01 11:14:57,478 Epoch 475: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-01 11:14:57,478 EPOCH 476
2024-02-01 11:15:10,361 Epoch 476: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-01 11:15:10,362 EPOCH 477
2024-02-01 11:15:23,342 Epoch 477: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-01 11:15:23,342 EPOCH 478
2024-02-01 11:15:30,160 [Epoch: 478 Step: 00004300] Batch Recognition Loss:   0.000732 => Gls Tokens per Sec:     1314 || Batch Translation Loss:   0.100765 => Txt Tokens per Sec:     3524 || Lr: 0.000100
2024-02-01 11:15:36,196 Epoch 478: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-01 11:15:36,196 EPOCH 479
2024-02-01 11:15:49,112 Epoch 479: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-01 11:15:49,112 EPOCH 480
2024-02-01 11:16:02,082 Epoch 480: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-01 11:16:02,082 EPOCH 481
2024-02-01 11:16:14,948 Epoch 481: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.47 
2024-02-01 11:16:14,948 EPOCH 482
2024-02-01 11:16:27,669 Epoch 482: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.97 
2024-02-01 11:16:27,669 EPOCH 483
2024-02-01 11:16:40,541 Epoch 483: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.62 
2024-02-01 11:16:40,541 EPOCH 484
2024-02-01 11:16:53,489 Epoch 484: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-01 11:16:53,490 EPOCH 485
2024-02-01 11:17:06,430 Epoch 485: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.63 
2024-02-01 11:17:06,431 EPOCH 486
2024-02-01 11:17:19,479 Epoch 486: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.87 
2024-02-01 11:17:19,479 EPOCH 487
2024-02-01 11:17:32,447 Epoch 487: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.02 
2024-02-01 11:17:32,448 EPOCH 488
2024-02-01 11:17:45,274 Epoch 488: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.67 
2024-02-01 11:17:45,274 EPOCH 489
2024-02-01 11:17:57,875 [Epoch: 489 Step: 00004400] Batch Recognition Loss:   0.001563 => Gls Tokens per Sec:      742 || Batch Translation Loss:   0.277684 => Txt Tokens per Sec:     2161 || Lr: 0.000100
2024-02-01 11:17:58,177 Epoch 489: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.73 
2024-02-01 11:17:58,177 EPOCH 490
2024-02-01 11:18:11,008 Epoch 490: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.63 
2024-02-01 11:18:11,008 EPOCH 491
2024-02-01 11:18:23,877 Epoch 491: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.44 
2024-02-01 11:18:23,877 EPOCH 492
2024-02-01 11:18:36,736 Epoch 492: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.32 
2024-02-01 11:18:36,736 EPOCH 493
2024-02-01 11:18:49,495 Epoch 493: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-01 11:18:49,495 EPOCH 494
2024-02-01 11:19:02,364 Epoch 494: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-01 11:19:02,364 EPOCH 495
2024-02-01 11:19:15,263 Epoch 495: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-01 11:19:15,263 EPOCH 496
2024-02-01 11:19:28,113 Epoch 496: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-01 11:19:28,113 EPOCH 497
2024-02-01 11:19:40,933 Epoch 497: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-01 11:19:40,933 EPOCH 498
2024-02-01 11:19:53,755 Epoch 498: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.58 
2024-02-01 11:19:53,755 EPOCH 499
2024-02-01 11:20:06,612 Epoch 499: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.70 
2024-02-01 11:20:06,613 EPOCH 500
2024-02-01 11:20:19,541 [Epoch: 500 Step: 00004500] Batch Recognition Loss:   0.001030 => Gls Tokens per Sec:      822 || Batch Translation Loss:   0.195190 => Txt Tokens per Sec:     2282 || Lr: 0.000100
2024-02-01 11:20:19,541 Epoch 500: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.57 
2024-02-01 11:20:19,541 EPOCH 501
2024-02-01 11:20:32,396 Epoch 501: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.36 
2024-02-01 11:20:32,397 EPOCH 502
2024-02-01 11:20:45,272 Epoch 502: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.39 
2024-02-01 11:20:45,272 EPOCH 503
2024-02-01 11:20:58,157 Epoch 503: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.32 
2024-02-01 11:20:58,157 EPOCH 504
2024-02-01 11:21:11,174 Epoch 504: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.36 
2024-02-01 11:21:11,174 EPOCH 505
2024-02-01 11:21:24,020 Epoch 505: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.41 
2024-02-01 11:21:24,020 EPOCH 506
2024-02-01 11:21:36,913 Epoch 506: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.31 
2024-02-01 11:21:36,913 EPOCH 507
2024-02-01 11:21:49,747 Epoch 507: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.11 
2024-02-01 11:21:49,747 EPOCH 508
2024-02-01 11:22:02,483 Epoch 508: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-01 11:22:02,483 EPOCH 509
2024-02-01 11:22:15,453 Epoch 509: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-01 11:22:15,453 EPOCH 510
2024-02-01 11:22:28,383 Epoch 510: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-01 11:22:28,384 EPOCH 511
2024-02-01 11:22:41,205 Epoch 511: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-01 11:22:41,205 EPOCH 512
2024-02-01 11:22:41,686 [Epoch: 512 Step: 00004600] Batch Recognition Loss:   0.000911 => Gls Tokens per Sec:     2672 || Batch Translation Loss:   0.124135 => Txt Tokens per Sec:     7860 || Lr: 0.000100
2024-02-01 11:22:54,117 Epoch 512: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-01 11:22:54,117 EPOCH 513
2024-02-01 11:23:06,976 Epoch 513: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-01 11:23:06,976 EPOCH 514
2024-02-01 11:23:19,982 Epoch 514: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-01 11:23:19,982 EPOCH 515
2024-02-01 11:23:32,986 Epoch 515: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-01 11:23:32,986 EPOCH 516
2024-02-01 11:23:45,900 Epoch 516: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-01 11:23:45,900 EPOCH 517
2024-02-01 11:23:58,772 Epoch 517: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-01 11:23:58,772 EPOCH 518
2024-02-01 11:24:11,691 Epoch 518: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-01 11:24:11,691 EPOCH 519
2024-02-01 11:24:24,732 Epoch 519: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-01 11:24:24,732 EPOCH 520
2024-02-01 11:24:37,539 Epoch 520: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-01 11:24:37,539 EPOCH 521
2024-02-01 11:24:50,506 Epoch 521: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-01 11:24:50,506 EPOCH 522
2024-02-01 11:25:03,404 Epoch 522: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-01 11:25:03,404 EPOCH 523
2024-02-01 11:25:05,269 [Epoch: 523 Step: 00004700] Batch Recognition Loss:   0.000641 => Gls Tokens per Sec:     1374 || Batch Translation Loss:   0.085427 => Txt Tokens per Sec:     3675 || Lr: 0.000100
2024-02-01 11:25:16,324 Epoch 523: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-01 11:25:16,324 EPOCH 524
2024-02-01 11:25:29,571 Epoch 524: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-01 11:25:29,571 EPOCH 525
2024-02-01 11:25:43,098 Epoch 525: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-01 11:25:43,098 EPOCH 526
2024-02-01 11:25:56,058 Epoch 526: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-01 11:25:56,058 EPOCH 527
2024-02-01 11:26:08,906 Epoch 527: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.34 
2024-02-01 11:26:08,906 EPOCH 528
2024-02-01 11:26:21,805 Epoch 528: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.19 
2024-02-01 11:26:21,805 EPOCH 529
2024-02-01 11:26:34,679 Epoch 529: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-01 11:26:34,680 EPOCH 530
2024-02-01 11:26:47,482 Epoch 530: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-01 11:26:47,482 EPOCH 531
2024-02-01 11:27:00,289 Epoch 531: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-01 11:27:00,289 EPOCH 532
2024-02-01 11:27:13,290 Epoch 532: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-01 11:27:13,290 EPOCH 533
2024-02-01 11:27:26,149 Epoch 533: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-01 11:27:26,149 EPOCH 534
2024-02-01 11:27:30,175 [Epoch: 534 Step: 00004800] Batch Recognition Loss:   0.001117 => Gls Tokens per Sec:      954 || Batch Translation Loss:   0.057458 => Txt Tokens per Sec:     2472 || Lr: 0.000100
2024-02-01 11:27:39,006 Epoch 534: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-01 11:27:39,006 EPOCH 535
2024-02-01 11:27:51,897 Epoch 535: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-01 11:27:51,897 EPOCH 536
2024-02-01 11:28:04,719 Epoch 536: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-01 11:28:04,719 EPOCH 537
2024-02-01 11:28:17,681 Epoch 537: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-01 11:28:17,681 EPOCH 538
2024-02-01 11:28:30,452 Epoch 538: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-01 11:28:30,452 EPOCH 539
2024-02-01 11:28:43,393 Epoch 539: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-01 11:28:43,393 EPOCH 540
2024-02-01 11:28:56,426 Epoch 540: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-01 11:28:56,426 EPOCH 541
2024-02-01 11:29:09,202 Epoch 541: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-01 11:29:09,202 EPOCH 542
2024-02-01 11:29:22,068 Epoch 542: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-01 11:29:22,068 EPOCH 543
2024-02-01 11:29:34,900 Epoch 543: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-01 11:29:34,900 EPOCH 544
2024-02-01 11:29:47,765 Epoch 544: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-01 11:29:47,765 EPOCH 545
2024-02-01 11:29:56,118 [Epoch: 545 Step: 00004900] Batch Recognition Loss:   0.000899 => Gls Tokens per Sec:      613 || Batch Translation Loss:   0.118137 => Txt Tokens per Sec:     1908 || Lr: 0.000100
2024-02-01 11:30:00,620 Epoch 545: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.19 
2024-02-01 11:30:00,620 EPOCH 546
2024-02-01 11:30:13,502 Epoch 546: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.55 
2024-02-01 11:30:13,503 EPOCH 547
2024-02-01 11:30:26,337 Epoch 547: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.46 
2024-02-01 11:30:26,337 EPOCH 548
2024-02-01 11:30:39,200 Epoch 548: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.37 
2024-02-01 11:30:39,200 EPOCH 549
2024-02-01 11:30:52,040 Epoch 549: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-01 11:30:52,040 EPOCH 550
2024-02-01 11:31:04,992 Epoch 550: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-01 11:31:04,993 EPOCH 551
2024-02-01 11:31:17,820 Epoch 551: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-01 11:31:17,820 EPOCH 552
2024-02-01 11:31:30,745 Epoch 552: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-01 11:31:30,745 EPOCH 553
2024-02-01 11:31:43,650 Epoch 553: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-01 11:31:43,650 EPOCH 554
2024-02-01 11:31:56,516 Epoch 554: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-01 11:31:56,516 EPOCH 555
2024-02-01 11:32:09,408 Epoch 555: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.71 
2024-02-01 11:32:09,409 EPOCH 556
2024-02-01 11:32:17,823 [Epoch: 556 Step: 00005000] Batch Recognition Loss:   0.000849 => Gls Tokens per Sec:      655 || Batch Translation Loss:   0.211420 => Txt Tokens per Sec:     1884 || Lr: 0.000100
2024-02-01 11:32:22,430 Epoch 556: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.72 
2024-02-01 11:32:22,431 EPOCH 557
2024-02-01 11:32:35,262 Epoch 557: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.27 
2024-02-01 11:32:35,262 EPOCH 558
2024-02-01 11:32:48,109 Epoch 558: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.44 
2024-02-01 11:32:48,109 EPOCH 559
2024-02-01 11:33:01,005 Epoch 559: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.13 
2024-02-01 11:33:01,005 EPOCH 560
2024-02-01 11:33:13,795 Epoch 560: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.60 
2024-02-01 11:33:13,795 EPOCH 561
2024-02-01 11:33:26,722 Epoch 561: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.01 
2024-02-01 11:33:26,722 EPOCH 562
2024-02-01 11:33:39,657 Epoch 562: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.81 
2024-02-01 11:33:39,657 EPOCH 563
2024-02-01 11:33:52,547 Epoch 563: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.85 
2024-02-01 11:33:52,547 EPOCH 564
2024-02-01 11:34:05,481 Epoch 564: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.08 
2024-02-01 11:34:05,481 EPOCH 565
2024-02-01 11:34:18,388 Epoch 565: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.77 
2024-02-01 11:34:18,388 EPOCH 566
2024-02-01 11:34:31,382 Epoch 566: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.40 
2024-02-01 11:34:31,383 EPOCH 567
2024-02-01 11:34:43,332 [Epoch: 567 Step: 00005100] Batch Recognition Loss:   0.001099 => Gls Tokens per Sec:      568 || Batch Translation Loss:   0.159977 => Txt Tokens per Sec:     1720 || Lr: 0.000100
2024-02-01 11:34:44,220 Epoch 567: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.32 
2024-02-01 11:34:44,221 EPOCH 568
2024-02-01 11:34:57,230 Epoch 568: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-01 11:34:57,230 EPOCH 569
2024-02-01 11:35:10,182 Epoch 569: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.16 
2024-02-01 11:35:10,182 EPOCH 570
2024-02-01 11:35:23,062 Epoch 570: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-01 11:35:23,063 EPOCH 571
2024-02-01 11:35:36,058 Epoch 571: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-01 11:35:36,058 EPOCH 572
2024-02-01 11:35:48,939 Epoch 572: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-01 11:35:48,939 EPOCH 573
2024-02-01 11:36:01,793 Epoch 573: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-01 11:36:01,793 EPOCH 574
2024-02-01 11:36:14,560 Epoch 574: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-01 11:36:14,560 EPOCH 575
2024-02-01 11:36:27,490 Epoch 575: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-01 11:36:27,490 EPOCH 576
2024-02-01 11:36:40,431 Epoch 576: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-01 11:36:40,431 EPOCH 577
2024-02-01 11:36:53,282 Epoch 577: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-01 11:36:53,283 EPOCH 578
2024-02-01 11:37:03,987 [Epoch: 578 Step: 00005200] Batch Recognition Loss:   0.000837 => Gls Tokens per Sec:      754 || Batch Translation Loss:   0.090200 => Txt Tokens per Sec:     2046 || Lr: 0.000100
2024-02-01 11:37:06,130 Epoch 578: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-01 11:37:06,130 EPOCH 579
2024-02-01 11:37:19,088 Epoch 579: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-01 11:37:19,088 EPOCH 580
2024-02-01 11:37:32,119 Epoch 580: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-01 11:37:32,119 EPOCH 581
2024-02-01 11:37:45,076 Epoch 581: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-01 11:37:45,076 EPOCH 582
2024-02-01 11:37:57,945 Epoch 582: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-01 11:37:57,945 EPOCH 583
2024-02-01 11:38:10,790 Epoch 583: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-01 11:38:10,791 EPOCH 584
2024-02-01 11:38:23,589 Epoch 584: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-01 11:38:23,589 EPOCH 585
2024-02-01 11:38:36,473 Epoch 585: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-01 11:38:36,473 EPOCH 586
2024-02-01 11:38:49,362 Epoch 586: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-01 11:38:49,362 EPOCH 587
2024-02-01 11:39:02,282 Epoch 587: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-01 11:39:02,282 EPOCH 588
2024-02-01 11:39:15,131 Epoch 588: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-01 11:39:15,131 EPOCH 589
2024-02-01 11:39:27,840 [Epoch: 589 Step: 00005300] Batch Recognition Loss:   0.000607 => Gls Tokens per Sec:      736 || Batch Translation Loss:   0.093012 => Txt Tokens per Sec:     2142 || Lr: 0.000100
2024-02-01 11:39:28,052 Epoch 589: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-01 11:39:28,053 EPOCH 590
2024-02-01 11:39:40,985 Epoch 590: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-01 11:39:40,986 EPOCH 591
2024-02-01 11:39:53,852 Epoch 591: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-01 11:39:53,852 EPOCH 592
2024-02-01 11:40:06,810 Epoch 592: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-01 11:40:06,811 EPOCH 593
2024-02-01 11:40:19,536 Epoch 593: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-01 11:40:19,536 EPOCH 594
2024-02-01 11:40:32,382 Epoch 594: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-01 11:40:32,382 EPOCH 595
2024-02-01 11:40:45,278 Epoch 595: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-01 11:40:45,278 EPOCH 596
2024-02-01 11:40:58,143 Epoch 596: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-01 11:40:58,143 EPOCH 597
2024-02-01 11:41:11,167 Epoch 597: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-01 11:41:11,167 EPOCH 598
2024-02-01 11:41:24,028 Epoch 598: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-01 11:41:24,029 EPOCH 599
2024-02-01 11:41:36,932 Epoch 599: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-01 11:41:36,932 EPOCH 600
2024-02-01 11:41:49,903 [Epoch: 600 Step: 00005400] Batch Recognition Loss:   0.000377 => Gls Tokens per Sec:      820 || Batch Translation Loss:   0.059902 => Txt Tokens per Sec:     2275 || Lr: 0.000100
2024-02-01 11:41:49,904 Epoch 600: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-01 11:41:49,904 EPOCH 601
2024-02-01 11:42:02,759 Epoch 601: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-01 11:42:02,759 EPOCH 602
2024-02-01 11:42:15,575 Epoch 602: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-01 11:42:15,575 EPOCH 603
2024-02-01 11:42:28,485 Epoch 603: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-01 11:42:28,486 EPOCH 604
2024-02-01 11:42:41,347 Epoch 604: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-01 11:42:41,348 EPOCH 605
2024-02-01 11:42:54,251 Epoch 605: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-01 11:42:54,251 EPOCH 606
2024-02-01 11:43:07,162 Epoch 606: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-01 11:43:07,162 EPOCH 607
2024-02-01 11:43:20,168 Epoch 607: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-01 11:43:20,168 EPOCH 608
2024-02-01 11:43:32,954 Epoch 608: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-01 11:43:32,954 EPOCH 609
2024-02-01 11:43:45,987 Epoch 609: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-01 11:43:45,987 EPOCH 610
2024-02-01 11:43:58,890 Epoch 610: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-01 11:43:58,890 EPOCH 611
2024-02-01 11:44:11,703 Epoch 611: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-01 11:44:11,703 EPOCH 612
2024-02-01 11:44:14,567 [Epoch: 612 Step: 00005500] Batch Recognition Loss:   0.000496 => Gls Tokens per Sec:      447 || Batch Translation Loss:   0.072998 => Txt Tokens per Sec:     1421 || Lr: 0.000100
2024-02-01 11:44:24,641 Epoch 612: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-01 11:44:24,641 EPOCH 613
2024-02-01 11:44:37,528 Epoch 613: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-01 11:44:37,528 EPOCH 614
2024-02-01 11:44:50,451 Epoch 614: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-01 11:44:50,451 EPOCH 615
2024-02-01 11:45:03,183 Epoch 615: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.78 
2024-02-01 11:45:03,183 EPOCH 616
2024-02-01 11:45:16,139 Epoch 616: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-01 11:45:16,139 EPOCH 617
2024-02-01 11:45:29,090 Epoch 617: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.00 
2024-02-01 11:45:29,090 EPOCH 618
2024-02-01 11:45:42,105 Epoch 618: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.13 
2024-02-01 11:45:42,106 EPOCH 619
2024-02-01 11:45:54,967 Epoch 619: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.00 
2024-02-01 11:45:54,967 EPOCH 620
2024-02-01 11:46:07,792 Epoch 620: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.42 
2024-02-01 11:46:07,792 EPOCH 621
2024-02-01 11:46:20,691 Epoch 621: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.34 
2024-02-01 11:46:20,691 EPOCH 622
2024-02-01 11:46:33,598 Epoch 622: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-01 11:46:33,598 EPOCH 623
2024-02-01 11:46:37,071 [Epoch: 623 Step: 00005600] Batch Recognition Loss:   0.000934 => Gls Tokens per Sec:      737 || Batch Translation Loss:   0.319052 => Txt Tokens per Sec:     2254 || Lr: 0.000100
2024-02-01 11:46:46,409 Epoch 623: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.89 
2024-02-01 11:46:46,409 EPOCH 624
2024-02-01 11:46:59,207 Epoch 624: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.19 
2024-02-01 11:46:59,208 EPOCH 625
2024-02-01 11:47:11,977 Epoch 625: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.45 
2024-02-01 11:47:11,977 EPOCH 626
2024-02-01 11:47:24,796 Epoch 626: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.21 
2024-02-01 11:47:24,797 EPOCH 627
2024-02-01 11:47:37,778 Epoch 627: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.90 
2024-02-01 11:47:37,778 EPOCH 628
2024-02-01 11:47:50,554 Epoch 628: Total Training Recognition Loss 0.06  Total Training Translation Loss 16.08 
2024-02-01 11:47:50,555 EPOCH 629
2024-02-01 11:48:03,365 Epoch 629: Total Training Recognition Loss 0.16  Total Training Translation Loss 14.60 
2024-02-01 11:48:03,365 EPOCH 630
2024-02-01 11:48:16,327 Epoch 630: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.12 
2024-02-01 11:48:16,328 EPOCH 631
2024-02-01 11:48:29,166 Epoch 631: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.85 
2024-02-01 11:48:29,166 EPOCH 632
2024-02-01 11:48:42,063 Epoch 632: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.64 
2024-02-01 11:48:42,064 EPOCH 633
2024-02-01 11:48:54,953 Epoch 633: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.78 
2024-02-01 11:48:54,953 EPOCH 634
2024-02-01 11:48:59,170 [Epoch: 634 Step: 00005700] Batch Recognition Loss:   0.002260 => Gls Tokens per Sec:      911 || Batch Translation Loss:   0.229834 => Txt Tokens per Sec:     2647 || Lr: 0.000100
2024-02-01 11:49:07,787 Epoch 634: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-01 11:49:07,787 EPOCH 635
2024-02-01 11:49:20,683 Epoch 635: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-01 11:49:20,683 EPOCH 636
2024-02-01 11:49:33,492 Epoch 636: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-01 11:49:33,492 EPOCH 637
2024-02-01 11:49:46,276 Epoch 637: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-01 11:49:46,276 EPOCH 638
2024-02-01 11:49:59,198 Epoch 638: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-01 11:49:59,198 EPOCH 639
2024-02-01 11:50:11,966 Epoch 639: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-01 11:50:11,966 EPOCH 640
2024-02-01 11:50:24,728 Epoch 640: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-01 11:50:24,729 EPOCH 641
2024-02-01 11:50:37,714 Epoch 641: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-01 11:50:37,714 EPOCH 642
2024-02-01 11:50:50,665 Epoch 642: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-01 11:50:50,666 EPOCH 643
2024-02-01 11:51:03,464 Epoch 643: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-01 11:51:03,464 EPOCH 644
2024-02-01 11:51:16,379 Epoch 644: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-01 11:51:16,380 EPOCH 645
2024-02-01 11:51:20,688 [Epoch: 645 Step: 00005800] Batch Recognition Loss:   0.000588 => Gls Tokens per Sec:      982 || Batch Translation Loss:   0.043185 => Txt Tokens per Sec:     2625 || Lr: 0.000100
2024-02-01 11:51:29,305 Epoch 645: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-01 11:51:29,305 EPOCH 646
2024-02-01 11:51:42,179 Epoch 646: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-01 11:51:42,179 EPOCH 647
2024-02-01 11:51:55,039 Epoch 647: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-01 11:51:55,039 EPOCH 648
2024-02-01 11:52:08,034 Epoch 648: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-01 11:52:08,034 EPOCH 649
2024-02-01 11:52:20,849 Epoch 649: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-01 11:52:20,849 EPOCH 650
2024-02-01 11:52:33,754 Epoch 650: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-01 11:52:33,754 EPOCH 651
2024-02-01 11:52:46,583 Epoch 651: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-01 11:52:46,584 EPOCH 652
2024-02-01 11:52:59,390 Epoch 652: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-01 11:52:59,390 EPOCH 653
2024-02-01 11:53:12,087 Epoch 653: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-01 11:53:12,087 EPOCH 654
2024-02-01 11:53:24,927 Epoch 654: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-01 11:53:24,927 EPOCH 655
2024-02-01 11:53:37,874 Epoch 655: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-01 11:53:37,874 EPOCH 656
2024-02-01 11:53:46,551 [Epoch: 656 Step: 00005900] Batch Recognition Loss:   0.000464 => Gls Tokens per Sec:      635 || Batch Translation Loss:   0.037129 => Txt Tokens per Sec:     1722 || Lr: 0.000100
2024-02-01 11:53:50,680 Epoch 656: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-01 11:53:50,681 EPOCH 657
2024-02-01 11:54:03,608 Epoch 657: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-01 11:54:03,608 EPOCH 658
2024-02-01 11:54:16,549 Epoch 658: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-01 11:54:16,549 EPOCH 659
2024-02-01 11:54:29,527 Epoch 659: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-01 11:54:29,527 EPOCH 660
2024-02-01 11:54:42,509 Epoch 660: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-01 11:54:42,509 EPOCH 661
2024-02-01 11:54:55,347 Epoch 661: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-01 11:54:55,348 EPOCH 662
2024-02-01 11:55:08,267 Epoch 662: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-01 11:55:08,267 EPOCH 663
2024-02-01 11:55:21,095 Epoch 663: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-01 11:55:21,095 EPOCH 664
2024-02-01 11:55:34,058 Epoch 664: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-01 11:55:34,058 EPOCH 665
2024-02-01 11:55:47,777 Epoch 665: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-01 11:55:47,777 EPOCH 666
2024-02-01 11:56:00,845 Epoch 666: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-01 11:56:00,845 EPOCH 667
2024-02-01 11:56:09,864 [Epoch: 667 Step: 00006000] Batch Recognition Loss:   0.000423 => Gls Tokens per Sec:      753 || Batch Translation Loss:   0.040052 => Txt Tokens per Sec:     2085 || Lr: 0.000100
2024-02-01 11:56:28,822 Validation result at epoch 667, step     6000: duration: 18.9576s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00025	Translation Loss: 89822.82812	PPL: 8011.03271
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 1.12	(BLEU-1: 12.39,	BLEU-2: 4.49,	BLEU-3: 2.04,	BLEU-4: 1.12)
	CHRF 17.88	ROUGE 10.70
2024-02-01 11:56:28,823 Logging Recognition and Translation Outputs
2024-02-01 11:56:28,824 ========================================================================================================================
2024-02-01 11:56:28,824 Logging Sequence: 89_111.00
2024-02-01 11:56:28,824 	Gloss Reference :	A B+C+D+E
2024-02-01 11:56:28,825 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 11:56:28,825 	Gloss Alignment :	         
2024-02-01 11:56:28,825 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 11:56:28,826 	Text Reference  :	*** ******* ******** * ******** ** ******** however selectors never selected me   for the   team
2024-02-01 11:56:28,826 	Text Hypothesis :	the spinner received a plethora of messages and     posts     from  all      over a   short over
2024-02-01 11:56:28,826 	Text Alignment  :	I   I       I        I I        I  I        S       S         S     S        S    S   S     S   
2024-02-01 11:56:28,826 ========================================================================================================================
2024-02-01 11:56:28,826 Logging Sequence: 137_23.00
2024-02-01 11:56:28,826 	Gloss Reference :	A B+C+D+E
2024-02-01 11:56:28,827 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 11:56:28,827 	Gloss Alignment :	         
2024-02-01 11:56:28,827 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 11:56:28,828 	Text Reference  :	fan from around the world are  in     qatar for the fifa world cup 
2024-02-01 11:56:28,828 	Text Hypothesis :	*** **** ****** the fans  were ranked based on  the **** ***** loss
2024-02-01 11:56:28,828 	Text Alignment  :	D   D    D          S     S    S      S     S       D    D     S   
2024-02-01 11:56:28,828 ========================================================================================================================
2024-02-01 11:56:28,828 Logging Sequence: 128_145.00
2024-02-01 11:56:28,828 	Gloss Reference :	A B+C+D+E
2024-02-01 11:56:28,828 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 11:56:28,828 	Gloss Alignment :	         
2024-02-01 11:56:28,829 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 11:56:28,829 	Text Reference  :	icc also uploaded a video of   the  same 
2024-02-01 11:56:28,829 	Text Hypothesis :	*** **** ******** * the   news went viral
2024-02-01 11:56:28,829 	Text Alignment  :	D   D    D        D S     S    S    S    
2024-02-01 11:56:28,829 ========================================================================================================================
2024-02-01 11:56:28,829 Logging Sequence: 165_192.00
2024-02-01 11:56:28,830 	Gloss Reference :	A B+C+D+E
2024-02-01 11:56:28,830 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 11:56:28,830 	Gloss Alignment :	         
2024-02-01 11:56:28,830 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 11:56:28,831 	Text Reference  :	** 3   ravichandran ashwin believes that his bag is      lucky
2024-02-01 11:56:28,831 	Text Hypothesis :	he won all          out    and      food at  the women's team 
2024-02-01 11:56:28,831 	Text Alignment  :	I  S   S            S      S        S    S   S   S       S    
2024-02-01 11:56:28,831 ========================================================================================================================
2024-02-01 11:56:28,831 Logging Sequence: 180_494.00
2024-02-01 11:56:28,831 	Gloss Reference :	A B+C+D+E
2024-02-01 11:56:28,831 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 11:56:28,832 	Gloss Alignment :	         
2024-02-01 11:56:28,832 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 11:56:28,833 	Text Reference  :	the     women wrestlers spoke angrily against the police and      the controversy in front  of  the media    
2024-02-01 11:56:28,833 	Text Hypothesis :	however an    wrestlers said  it      wasn't  a   minor  argument the *********** ** police hit the wrestlers
2024-02-01 11:56:28,833 	Text Alignment  :	S       S               S     S       S       S   S      S            D           D  S      S       S        
2024-02-01 11:56:28,833 ========================================================================================================================
2024-02-01 11:56:33,053 Epoch 667: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-01 11:56:33,053 EPOCH 668
2024-02-01 11:56:46,444 Epoch 668: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 11:56:46,444 EPOCH 669
2024-02-01 11:56:59,370 Epoch 669: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 11:56:59,370 EPOCH 670
2024-02-01 11:57:12,260 Epoch 670: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 11:57:12,260 EPOCH 671
2024-02-01 11:57:25,223 Epoch 671: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-01 11:57:25,223 EPOCH 672
2024-02-01 11:57:38,109 Epoch 672: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-01 11:57:38,109 EPOCH 673
2024-02-01 11:57:51,001 Epoch 673: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-01 11:57:51,001 EPOCH 674
2024-02-01 11:58:03,937 Epoch 674: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 11:58:03,937 EPOCH 675
2024-02-01 11:58:16,748 Epoch 675: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 11:58:16,748 EPOCH 676
2024-02-01 11:58:29,616 Epoch 676: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 11:58:29,616 EPOCH 677
2024-02-01 11:58:42,476 Epoch 677: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 11:58:42,477 EPOCH 678
2024-02-01 11:58:51,596 [Epoch: 678 Step: 00006100] Batch Recognition Loss:   0.000367 => Gls Tokens per Sec:      885 || Batch Translation Loss:   0.031375 => Txt Tokens per Sec:     2494 || Lr: 0.000100
2024-02-01 11:58:55,351 Epoch 678: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 11:58:55,351 EPOCH 679
2024-02-01 11:59:08,301 Epoch 679: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-01 11:59:08,302 EPOCH 680
2024-02-01 11:59:21,148 Epoch 680: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-01 11:59:21,148 EPOCH 681
2024-02-01 11:59:34,088 Epoch 681: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-01 11:59:34,089 EPOCH 682
2024-02-01 11:59:47,014 Epoch 682: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-01 11:59:47,014 EPOCH 683
2024-02-01 11:59:59,854 Epoch 683: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-01 11:59:59,854 EPOCH 684
2024-02-01 12:00:12,736 Epoch 684: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 12:00:12,737 EPOCH 685
2024-02-01 12:00:25,752 Epoch 685: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-01 12:00:25,753 EPOCH 686
2024-02-01 12:00:38,637 Epoch 686: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-01 12:00:38,637 EPOCH 687
2024-02-01 12:00:51,760 Epoch 687: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 12:00:51,761 EPOCH 688
2024-02-01 12:01:04,787 Epoch 688: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 12:01:04,787 EPOCH 689
2024-02-01 12:01:17,097 [Epoch: 689 Step: 00006200] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:      760 || Batch Translation Loss:   0.035923 => Txt Tokens per Sec:     2091 || Lr: 0.000100
2024-02-01 12:01:17,620 Epoch 689: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 12:01:17,621 EPOCH 690
2024-02-01 12:01:30,405 Epoch 690: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 12:01:30,406 EPOCH 691
2024-02-01 12:01:43,200 Epoch 691: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-01 12:01:43,200 EPOCH 692
2024-02-01 12:01:55,980 Epoch 692: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 12:01:55,980 EPOCH 693
2024-02-01 12:02:08,886 Epoch 693: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 12:02:08,886 EPOCH 694
2024-02-01 12:02:21,737 Epoch 694: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 12:02:21,738 EPOCH 695
2024-02-01 12:02:34,595 Epoch 695: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 12:02:34,595 EPOCH 696
2024-02-01 12:02:47,541 Epoch 696: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 12:02:47,541 EPOCH 697
2024-02-01 12:03:00,685 Epoch 697: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 12:03:00,686 EPOCH 698
2024-02-01 12:03:13,671 Epoch 698: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 12:03:13,671 EPOCH 699
2024-02-01 12:03:26,476 Epoch 699: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-01 12:03:26,476 EPOCH 700
2024-02-01 12:03:39,344 [Epoch: 700 Step: 00006300] Batch Recognition Loss:   0.000307 => Gls Tokens per Sec:      826 || Batch Translation Loss:   0.102774 => Txt Tokens per Sec:     2293 || Lr: 0.000100
2024-02-01 12:03:39,345 Epoch 700: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-01 12:03:39,345 EPOCH 701
2024-02-01 12:03:52,093 Epoch 701: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-01 12:03:52,093 EPOCH 702
2024-02-01 12:04:04,911 Epoch 702: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-01 12:04:04,911 EPOCH 703
2024-02-01 12:04:17,809 Epoch 703: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-01 12:04:17,809 EPOCH 704
2024-02-01 12:04:30,699 Epoch 704: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-01 12:04:30,699 EPOCH 705
2024-02-01 12:04:43,605 Epoch 705: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-01 12:04:43,605 EPOCH 706
2024-02-01 12:04:56,472 Epoch 706: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-01 12:04:56,472 EPOCH 707
2024-02-01 12:05:09,464 Epoch 707: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.80 
2024-02-01 12:05:09,464 EPOCH 708
2024-02-01 12:05:22,460 Epoch 708: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.78 
2024-02-01 12:05:22,460 EPOCH 709
2024-02-01 12:05:35,352 Epoch 709: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-01 12:05:35,352 EPOCH 710
2024-02-01 12:05:48,304 Epoch 710: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-01 12:05:48,305 EPOCH 711
2024-02-01 12:06:01,231 Epoch 711: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-01 12:06:01,231 EPOCH 712
2024-02-01 12:06:01,757 [Epoch: 712 Step: 00006400] Batch Recognition Loss:   0.000384 => Gls Tokens per Sec:     2438 || Batch Translation Loss:   0.059867 => Txt Tokens per Sec:     7151 || Lr: 0.000100
2024-02-01 12:06:14,069 Epoch 712: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-01 12:06:14,069 EPOCH 713
2024-02-01 12:06:27,081 Epoch 713: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-01 12:06:27,082 EPOCH 714
2024-02-01 12:06:39,808 Epoch 714: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-01 12:06:39,809 EPOCH 715
2024-02-01 12:06:52,727 Epoch 715: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-01 12:06:52,728 EPOCH 716
2024-02-01 12:07:05,642 Epoch 716: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-01 12:07:05,642 EPOCH 717
2024-02-01 12:07:18,554 Epoch 717: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-01 12:07:18,554 EPOCH 718
2024-02-01 12:07:31,390 Epoch 718: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-01 12:07:31,390 EPOCH 719
2024-02-01 12:07:44,286 Epoch 719: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 12:07:44,287 EPOCH 720
2024-02-01 12:07:57,202 Epoch 720: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 12:07:57,203 EPOCH 721
2024-02-01 12:08:10,087 Epoch 721: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 12:08:10,087 EPOCH 722
2024-02-01 12:08:22,852 Epoch 722: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 12:08:22,852 EPOCH 723
2024-02-01 12:08:29,237 [Epoch: 723 Step: 00006500] Batch Recognition Loss:   0.000330 => Gls Tokens per Sec:      401 || Batch Translation Loss:   0.047989 => Txt Tokens per Sec:     1343 || Lr: 0.000100
2024-02-01 12:08:35,747 Epoch 723: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-01 12:08:35,747 EPOCH 724
2024-02-01 12:08:48,508 Epoch 724: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 12:08:48,508 EPOCH 725
2024-02-01 12:09:01,289 Epoch 725: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-01 12:09:01,290 EPOCH 726
2024-02-01 12:09:14,063 Epoch 726: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-01 12:09:14,063 EPOCH 727
2024-02-01 12:09:26,866 Epoch 727: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-01 12:09:26,866 EPOCH 728
2024-02-01 12:09:39,693 Epoch 728: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-01 12:09:39,694 EPOCH 729
2024-02-01 12:09:52,517 Epoch 729: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.86 
2024-02-01 12:09:52,517 EPOCH 730
2024-02-01 12:10:05,335 Epoch 730: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.20 
2024-02-01 12:10:05,335 EPOCH 731
2024-02-01 12:10:18,289 Epoch 731: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.07 
2024-02-01 12:10:18,289 EPOCH 732
2024-02-01 12:10:31,136 Epoch 732: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.11 
2024-02-01 12:10:31,136 EPOCH 733
2024-02-01 12:10:44,012 Epoch 733: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.07 
2024-02-01 12:10:44,013 EPOCH 734
2024-02-01 12:10:47,983 [Epoch: 734 Step: 00006600] Batch Recognition Loss:   0.001279 => Gls Tokens per Sec:      743 || Batch Translation Loss:   0.633219 => Txt Tokens per Sec:     2097 || Lr: 0.000100
2024-02-01 12:10:56,814 Epoch 734: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.66 
2024-02-01 12:10:56,814 EPOCH 735
2024-02-01 12:11:09,616 Epoch 735: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.49 
2024-02-01 12:11:09,617 EPOCH 736
2024-02-01 12:11:22,542 Epoch 736: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.55 
2024-02-01 12:11:22,542 EPOCH 737
2024-02-01 12:11:35,423 Epoch 737: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-01 12:11:35,423 EPOCH 738
2024-02-01 12:11:48,367 Epoch 738: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-01 12:11:48,368 EPOCH 739
2024-02-01 12:12:01,182 Epoch 739: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-01 12:12:01,183 EPOCH 740
2024-02-01 12:12:14,012 Epoch 740: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-01 12:12:14,013 EPOCH 741
2024-02-01 12:12:26,969 Epoch 741: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-01 12:12:26,969 EPOCH 742
2024-02-01 12:12:39,688 Epoch 742: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-01 12:12:39,688 EPOCH 743
2024-02-01 12:12:52,573 Epoch 743: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-01 12:12:52,573 EPOCH 744
2024-02-01 12:13:05,447 Epoch 744: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-01 12:13:05,447 EPOCH 745
2024-02-01 12:13:09,808 [Epoch: 745 Step: 00006700] Batch Recognition Loss:   0.000446 => Gls Tokens per Sec:      970 || Batch Translation Loss:   0.046480 => Txt Tokens per Sec:     2662 || Lr: 0.000100
2024-02-01 12:13:18,336 Epoch 745: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-01 12:13:18,337 EPOCH 746
2024-02-01 12:13:31,251 Epoch 746: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-01 12:13:31,251 EPOCH 747
2024-02-01 12:13:44,269 Epoch 747: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-01 12:13:44,269 EPOCH 748
2024-02-01 12:13:57,015 Epoch 748: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-01 12:13:57,015 EPOCH 749
2024-02-01 12:14:09,817 Epoch 749: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-01 12:14:09,818 EPOCH 750
2024-02-01 12:14:22,669 Epoch 750: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-01 12:14:22,669 EPOCH 751
2024-02-01 12:14:35,720 Epoch 751: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-01 12:14:35,720 EPOCH 752
2024-02-01 12:14:48,672 Epoch 752: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-01 12:14:48,672 EPOCH 753
2024-02-01 12:15:01,568 Epoch 753: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-01 12:15:01,568 EPOCH 754
2024-02-01 12:15:14,532 Epoch 754: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-01 12:15:14,532 EPOCH 755
2024-02-01 12:15:27,380 Epoch 755: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-01 12:15:27,380 EPOCH 756
2024-02-01 12:15:34,958 [Epoch: 756 Step: 00006800] Batch Recognition Loss:   0.000411 => Gls Tokens per Sec:      727 || Batch Translation Loss:   0.070158 => Txt Tokens per Sec:     2051 || Lr: 0.000100
2024-02-01 12:15:40,292 Epoch 756: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-01 12:15:40,292 EPOCH 757
2024-02-01 12:15:53,233 Epoch 757: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-01 12:15:53,233 EPOCH 758
2024-02-01 12:16:06,151 Epoch 758: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-01 12:16:06,151 EPOCH 759
2024-02-01 12:16:18,942 Epoch 759: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-01 12:16:18,942 EPOCH 760
2024-02-01 12:16:31,854 Epoch 760: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 12:16:31,855 EPOCH 761
2024-02-01 12:16:44,720 Epoch 761: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 12:16:44,720 EPOCH 762
2024-02-01 12:16:57,549 Epoch 762: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-01 12:16:57,549 EPOCH 763
2024-02-01 12:17:10,441 Epoch 763: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-01 12:17:10,441 EPOCH 764
2024-02-01 12:17:23,285 Epoch 764: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 12:17:23,285 EPOCH 765
2024-02-01 12:17:36,211 Epoch 765: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-01 12:17:36,212 EPOCH 766
2024-02-01 12:17:48,996 Epoch 766: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 12:17:48,996 EPOCH 767
2024-02-01 12:17:58,085 [Epoch: 767 Step: 00006900] Batch Recognition Loss:   0.000303 => Gls Tokens per Sec:      845 || Batch Translation Loss:   0.039726 => Txt Tokens per Sec:     2383 || Lr: 0.000100
2024-02-01 12:18:01,901 Epoch 767: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 12:18:01,901 EPOCH 768
2024-02-01 12:18:14,837 Epoch 768: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 12:18:14,837 EPOCH 769
2024-02-01 12:18:27,627 Epoch 769: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 12:18:27,627 EPOCH 770
2024-02-01 12:18:40,518 Epoch 770: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 12:18:40,518 EPOCH 771
2024-02-01 12:18:53,308 Epoch 771: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-01 12:18:53,309 EPOCH 772
2024-02-01 12:19:06,126 Epoch 772: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-01 12:19:06,127 EPOCH 773
2024-02-01 12:19:18,958 Epoch 773: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.82 
2024-02-01 12:19:18,958 EPOCH 774
2024-02-01 12:19:31,845 Epoch 774: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.96 
2024-02-01 12:19:31,845 EPOCH 775
2024-02-01 12:19:44,674 Epoch 775: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.82 
2024-02-01 12:19:44,674 EPOCH 776
2024-02-01 12:19:57,507 Epoch 776: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.84 
2024-02-01 12:19:57,507 EPOCH 777
2024-02-01 12:20:10,399 Epoch 777: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.85 
2024-02-01 12:20:10,399 EPOCH 778
2024-02-01 12:20:22,783 [Epoch: 778 Step: 00007000] Batch Recognition Loss:   0.000474 => Gls Tokens per Sec:      652 || Batch Translation Loss:   0.082304 => Txt Tokens per Sec:     1941 || Lr: 0.000100
2024-02-01 12:20:23,317 Epoch 778: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-01 12:20:23,317 EPOCH 779
2024-02-01 12:20:36,151 Epoch 779: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-01 12:20:36,151 EPOCH 780
2024-02-01 12:20:48,931 Epoch 780: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-01 12:20:48,932 EPOCH 781
2024-02-01 12:21:01,802 Epoch 781: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-01 12:21:01,802 EPOCH 782
2024-02-01 12:21:14,629 Epoch 782: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-01 12:21:14,629 EPOCH 783
2024-02-01 12:21:27,601 Epoch 783: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-01 12:21:27,601 EPOCH 784
2024-02-01 12:21:40,498 Epoch 784: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-01 12:21:40,498 EPOCH 785
2024-02-01 12:21:53,385 Epoch 785: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-01 12:21:53,385 EPOCH 786
2024-02-01 12:22:06,247 Epoch 786: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-01 12:22:06,248 EPOCH 787
2024-02-01 12:22:19,106 Epoch 787: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-01 12:22:19,106 EPOCH 788
2024-02-01 12:22:31,985 Epoch 788: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-01 12:22:31,985 EPOCH 789
2024-02-01 12:22:44,447 [Epoch: 789 Step: 00007100] Batch Recognition Loss:   0.000320 => Gls Tokens per Sec:      750 || Batch Translation Loss:   0.045858 => Txt Tokens per Sec:     2068 || Lr: 0.000100
2024-02-01 12:22:44,964 Epoch 789: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-01 12:22:44,964 EPOCH 790
2024-02-01 12:22:57,839 Epoch 790: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.76 
2024-02-01 12:22:57,839 EPOCH 791
2024-02-01 12:23:10,670 Epoch 791: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.20 
2024-02-01 12:23:10,670 EPOCH 792
2024-02-01 12:23:23,496 Epoch 792: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.50 
2024-02-01 12:23:23,497 EPOCH 793
2024-02-01 12:23:36,419 Epoch 793: Total Training Recognition Loss 0.00  Total Training Translation Loss 2.09 
2024-02-01 12:23:36,419 EPOCH 794
2024-02-01 12:23:49,224 Epoch 794: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.80 
2024-02-01 12:23:49,224 EPOCH 795
2024-02-01 12:24:01,976 Epoch 795: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.69 
2024-02-01 12:24:01,976 EPOCH 796
2024-02-01 12:24:15,007 Epoch 796: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.25 
2024-02-01 12:24:15,007 EPOCH 797
2024-02-01 12:24:27,839 Epoch 797: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.55 
2024-02-01 12:24:27,839 EPOCH 798
2024-02-01 12:24:40,722 Epoch 798: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.47 
2024-02-01 12:24:40,723 EPOCH 799
2024-02-01 12:24:53,642 Epoch 799: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.20 
2024-02-01 12:24:53,643 EPOCH 800
2024-02-01 12:25:06,512 [Epoch: 800 Step: 00007200] Batch Recognition Loss:   0.000723 => Gls Tokens per Sec:      826 || Batch Translation Loss:   0.183796 => Txt Tokens per Sec:     2293 || Lr: 0.000100
2024-02-01 12:25:06,512 Epoch 800: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.60 
2024-02-01 12:25:06,512 EPOCH 801
2024-02-01 12:25:19,433 Epoch 801: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-01 12:25:19,434 EPOCH 802
2024-02-01 12:25:32,259 Epoch 802: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-01 12:25:32,259 EPOCH 803
2024-02-01 12:25:45,343 Epoch 803: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-01 12:25:45,344 EPOCH 804
2024-02-01 12:25:59,082 Epoch 804: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-01 12:25:59,082 EPOCH 805
2024-02-01 12:26:12,134 Epoch 805: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-01 12:26:12,134 EPOCH 806
2024-02-01 12:26:24,926 Epoch 806: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-01 12:26:24,927 EPOCH 807
2024-02-01 12:26:37,663 Epoch 807: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-01 12:26:37,664 EPOCH 808
2024-02-01 12:26:50,576 Epoch 808: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-01 12:26:50,576 EPOCH 809
2024-02-01 12:27:03,463 Epoch 809: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-01 12:27:03,464 EPOCH 810
2024-02-01 12:27:16,332 Epoch 810: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-01 12:27:16,332 EPOCH 811
2024-02-01 12:27:29,223 Epoch 811: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 12:27:29,223 EPOCH 812
2024-02-01 12:27:32,423 [Epoch: 812 Step: 00007300] Batch Recognition Loss:   0.000410 => Gls Tokens per Sec:      122 || Batch Translation Loss:   0.015017 => Txt Tokens per Sec:      436 || Lr: 0.000100
2024-02-01 12:27:42,160 Epoch 812: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-01 12:27:42,161 EPOCH 813
2024-02-01 12:27:54,984 Epoch 813: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 12:27:54,984 EPOCH 814
2024-02-01 12:28:07,772 Epoch 814: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-01 12:28:07,773 EPOCH 815
2024-02-01 12:28:20,758 Epoch 815: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 12:28:20,758 EPOCH 816
2024-02-01 12:28:33,571 Epoch 816: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-01 12:28:33,572 EPOCH 817
2024-02-01 12:28:46,304 Epoch 817: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 12:28:46,304 EPOCH 818
2024-02-01 12:28:59,042 Epoch 818: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 12:28:59,042 EPOCH 819
2024-02-01 12:29:11,994 Epoch 819: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 12:29:11,995 EPOCH 820
2024-02-01 12:29:24,955 Epoch 820: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 12:29:24,955 EPOCH 821
2024-02-01 12:29:37,838 Epoch 821: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 12:29:37,838 EPOCH 822
2024-02-01 12:29:50,598 Epoch 822: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 12:29:50,598 EPOCH 823
2024-02-01 12:29:54,313 [Epoch: 823 Step: 00007400] Batch Recognition Loss:   0.000486 => Gls Tokens per Sec:      449 || Batch Translation Loss:   0.013761 => Txt Tokens per Sec:     1375 || Lr: 0.000100
2024-02-01 12:30:03,561 Epoch 823: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 12:30:03,561 EPOCH 824
2024-02-01 12:30:16,473 Epoch 824: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 12:30:16,473 EPOCH 825
2024-02-01 12:30:29,427 Epoch 825: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 12:30:29,427 EPOCH 826
2024-02-01 12:30:42,377 Epoch 826: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 12:30:42,377 EPOCH 827
2024-02-01 12:30:55,288 Epoch 827: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 12:30:55,289 EPOCH 828
2024-02-01 12:31:08,309 Epoch 828: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 12:31:08,309 EPOCH 829
2024-02-01 12:31:21,278 Epoch 829: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 12:31:21,278 EPOCH 830
2024-02-01 12:31:34,034 Epoch 830: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 12:31:34,035 EPOCH 831
2024-02-01 12:31:46,892 Epoch 831: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 12:31:46,892 EPOCH 832
2024-02-01 12:31:59,837 Epoch 832: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 12:31:59,837 EPOCH 833
2024-02-01 12:32:12,688 Epoch 833: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-01 12:32:12,688 EPOCH 834
2024-02-01 12:32:22,253 [Epoch: 834 Step: 00007500] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:      308 || Batch Translation Loss:   0.013245 => Txt Tokens per Sec:     1043 || Lr: 0.000100
2024-02-01 12:32:25,457 Epoch 834: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 12:32:25,458 EPOCH 835
2024-02-01 12:32:38,318 Epoch 835: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 12:32:38,318 EPOCH 836
2024-02-01 12:32:51,124 Epoch 836: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 12:32:51,125 EPOCH 837
2024-02-01 12:33:04,062 Epoch 837: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 12:33:04,063 EPOCH 838
2024-02-01 12:33:16,967 Epoch 838: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 12:33:16,967 EPOCH 839
2024-02-01 12:33:29,917 Epoch 839: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 12:33:29,917 EPOCH 840
2024-02-01 12:33:42,821 Epoch 840: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-01 12:33:42,822 EPOCH 841
2024-02-01 12:33:55,671 Epoch 841: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 12:33:55,671 EPOCH 842
2024-02-01 12:34:08,635 Epoch 842: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 12:34:08,635 EPOCH 843
2024-02-01 12:34:21,599 Epoch 843: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 12:34:21,600 EPOCH 844
2024-02-01 12:34:34,471 Epoch 844: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 12:34:34,471 EPOCH 845
2024-02-01 12:34:37,084 [Epoch: 845 Step: 00007600] Batch Recognition Loss:   0.000333 => Gls Tokens per Sec:     1960 || Batch Translation Loss:   0.024730 => Txt Tokens per Sec:     4930 || Lr: 0.000100
2024-02-01 12:34:47,364 Epoch 845: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 12:34:47,364 EPOCH 846
2024-02-01 12:35:00,176 Epoch 846: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 12:35:00,176 EPOCH 847
2024-02-01 12:35:13,077 Epoch 847: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 12:35:13,077 EPOCH 848
2024-02-01 12:35:26,000 Epoch 848: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 12:35:26,000 EPOCH 849
2024-02-01 12:35:38,867 Epoch 849: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 12:35:38,868 EPOCH 850
2024-02-01 12:35:51,747 Epoch 850: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-01 12:35:51,747 EPOCH 851
2024-02-01 12:36:04,621 Epoch 851: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 12:36:04,621 EPOCH 852
2024-02-01 12:36:17,528 Epoch 852: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 12:36:17,529 EPOCH 853
2024-02-01 12:36:30,403 Epoch 853: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 12:36:30,403 EPOCH 854
2024-02-01 12:36:43,332 Epoch 854: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 12:36:43,333 EPOCH 855
2024-02-01 12:36:56,246 Epoch 855: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 12:36:56,246 EPOCH 856
2024-02-01 12:37:03,790 [Epoch: 856 Step: 00007700] Batch Recognition Loss:   0.000221 => Gls Tokens per Sec:      848 || Batch Translation Loss:   0.038172 => Txt Tokens per Sec:     2406 || Lr: 0.000100
2024-02-01 12:37:09,197 Epoch 856: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 12:37:09,198 EPOCH 857
2024-02-01 12:37:22,095 Epoch 857: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 12:37:22,096 EPOCH 858
2024-02-01 12:37:34,923 Epoch 858: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 12:37:34,923 EPOCH 859
2024-02-01 12:37:47,765 Epoch 859: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 12:37:47,765 EPOCH 860
2024-02-01 12:38:00,625 Epoch 860: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 12:38:00,626 EPOCH 861
2024-02-01 12:38:13,535 Epoch 861: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 12:38:13,535 EPOCH 862
2024-02-01 12:38:26,478 Epoch 862: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 12:38:26,479 EPOCH 863
2024-02-01 12:38:39,337 Epoch 863: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 12:38:39,337 EPOCH 864
2024-02-01 12:38:52,136 Epoch 864: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-01 12:38:52,136 EPOCH 865
2024-02-01 12:39:05,062 Epoch 865: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-01 12:39:05,063 EPOCH 866
2024-02-01 12:39:17,855 Epoch 866: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-01 12:39:17,856 EPOCH 867
2024-02-01 12:39:25,561 [Epoch: 867 Step: 00007800] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:      997 || Batch Translation Loss:   0.051071 => Txt Tokens per Sec:     2658 || Lr: 0.000100
2024-02-01 12:39:30,894 Epoch 867: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-01 12:39:30,894 EPOCH 868
2024-02-01 12:39:43,720 Epoch 868: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-01 12:39:43,720 EPOCH 869
2024-02-01 12:39:56,657 Epoch 869: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-01 12:39:56,657 EPOCH 870
2024-02-01 12:40:09,533 Epoch 870: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 12:40:09,533 EPOCH 871
2024-02-01 12:40:22,381 Epoch 871: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-01 12:40:22,381 EPOCH 872
2024-02-01 12:40:35,245 Epoch 872: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-01 12:40:35,245 EPOCH 873
2024-02-01 12:40:48,156 Epoch 873: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-01 12:40:48,156 EPOCH 874
2024-02-01 12:41:01,024 Epoch 874: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-01 12:41:01,024 EPOCH 875
2024-02-01 12:41:14,008 Epoch 875: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-01 12:41:14,008 EPOCH 876
2024-02-01 12:41:26,847 Epoch 876: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-01 12:41:26,847 EPOCH 877
2024-02-01 12:41:39,780 Epoch 877: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-01 12:41:39,780 EPOCH 878
2024-02-01 12:41:48,846 [Epoch: 878 Step: 00007900] Batch Recognition Loss:   0.000221 => Gls Tokens per Sec:      890 || Batch Translation Loss:   0.064659 => Txt Tokens per Sec:     2407 || Lr: 0.000100
2024-02-01 12:41:52,558 Epoch 878: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-01 12:41:52,558 EPOCH 879
2024-02-01 12:42:05,511 Epoch 879: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-01 12:42:05,511 EPOCH 880
2024-02-01 12:42:18,304 Epoch 880: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-01 12:42:18,304 EPOCH 881
2024-02-01 12:42:31,187 Epoch 881: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-01 12:42:31,187 EPOCH 882
2024-02-01 12:42:44,005 Epoch 882: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.77 
2024-02-01 12:42:44,005 EPOCH 883
2024-02-01 12:42:56,814 Epoch 883: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.81 
2024-02-01 12:42:56,814 EPOCH 884
2024-02-01 12:43:09,682 Epoch 884: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-01 12:43:09,682 EPOCH 885
2024-02-01 12:43:22,466 Epoch 885: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.32 
2024-02-01 12:43:22,466 EPOCH 886
2024-02-01 12:43:35,462 Epoch 886: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.46 
2024-02-01 12:43:35,463 EPOCH 887
2024-02-01 12:43:48,293 Epoch 887: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.13 
2024-02-01 12:43:48,293 EPOCH 888
2024-02-01 12:44:01,278 Epoch 888: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.02 
2024-02-01 12:44:01,279 EPOCH 889
2024-02-01 12:44:11,138 [Epoch: 889 Step: 00008000] Batch Recognition Loss:   0.000288 => Gls Tokens per Sec:      948 || Batch Translation Loss:   0.159812 => Txt Tokens per Sec:     2576 || Lr: 0.000100
2024-02-01 12:44:30,017 Validation result at epoch 889, step     8000: duration: 18.8792s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00025	Translation Loss: 91530.74219	PPL: 9504.16895
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.87	(BLEU-1: 12.01,	BLEU-2: 4.25,	BLEU-3: 1.74,	BLEU-4: 0.87)
	CHRF 18.10	ROUGE 10.31
2024-02-01 12:44:30,018 Logging Recognition and Translation Outputs
2024-02-01 12:44:30,018 ========================================================================================================================
2024-02-01 12:44:30,019 Logging Sequence: 88_57.00
2024-02-01 12:44:30,019 	Gloss Reference :	A B+C+D+E
2024-02-01 12:44:30,019 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 12:44:30,019 	Gloss Alignment :	         
2024-02-01 12:44:30,019 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 12:44:30,021 	Text Reference  :	which   stated  messi we're  waiting for  you     to   come here      you  will be finished when    you come
2024-02-01 12:44:30,021 	Text Hypothesis :	notably rosario has   become the     most violent city in   argentina with 250  to 300      murders in  2022
2024-02-01 12:44:30,021 	Text Alignment  :	S       S       S     S      S       S    S       S    S    S         S    S    S  S        S       S   S   
2024-02-01 12:44:30,021 ========================================================================================================================
2024-02-01 12:44:30,022 Logging Sequence: 171_142.00
2024-02-01 12:44:30,022 	Gloss Reference :	A B+C+D+E
2024-02-01 12:44:30,022 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 12:44:30,022 	Gloss Alignment :	         
2024-02-01 12:44:30,022 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 12:44:30,023 	Text Reference  :	***** *** this   decision on        dhoni made   a   significant impact as    pathirana claimed two           tough wickets
2024-02-01 12:44:30,024 	Text Hypothesis :	since the couple were     residents of    mumbai the mumbai      police cyber cell      began   investigating the   matter 
2024-02-01 12:44:30,024 	Text Alignment  :	I     I   S      S        S         S     S      S   S           S      S     S         S       S             S     S      
2024-02-01 12:44:30,024 ========================================================================================================================
2024-02-01 12:44:30,024 Logging Sequence: 125_207.00
2024-02-01 12:44:30,024 	Gloss Reference :	A B+C+D+E
2024-02-01 12:44:30,024 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 12:44:30,024 	Gloss Alignment :	         
2024-02-01 12:44:30,024 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 12:44:30,026 	Text Reference  :	he had not ***** **** *** ******* ** practised since he  returned and he          had also fallen sick
2024-02-01 12:44:30,026 	Text Hypothesis :	i  am  not happy with the javelin at the       2020  but decided  to  participate in  the  indian love
2024-02-01 12:44:30,026 	Text Alignment  :	S  S       I     I    I   I       I  S         S     S   S        S   S           S   S    S      S   
2024-02-01 12:44:30,026 ========================================================================================================================
2024-02-01 12:44:30,026 Logging Sequence: 68_230.00
2024-02-01 12:44:30,027 	Gloss Reference :	A B+C+D+E
2024-02-01 12:44:30,027 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 12:44:30,027 	Gloss Alignment :	         
2024-02-01 12:44:30,027 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 12:44:30,028 	Text Reference  :	**** **** **** * ****** *** ****** **** **** let us know what      you think in the comments  below   
2024-02-01 12:44:30,028 	Text Hypothesis :	they also sent a person who bowled very well and he is   extremely fit but   he is  extremely personal
2024-02-01 12:44:30,028 	Text Alignment  :	I    I    I    I I      I   I      I    I    S   S  S    S         S   S     S  S   S         S       
2024-02-01 12:44:30,028 ========================================================================================================================
2024-02-01 12:44:30,029 Logging Sequence: 126_82.00
2024-02-01 12:44:30,029 	Gloss Reference :	A B+C+D+E
2024-02-01 12:44:30,029 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 12:44:30,029 	Gloss Alignment :	         
2024-02-01 12:44:30,029 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 12:44:30,031 	Text Reference  :	** ** * ******** neeraj  also dedicated his gold   medal ** to      former indian olympians who     came close to  winning medals
2024-02-01 12:44:30,031 	Text Hypothesis :	he is a talented players who  won       a   silver medal in javelin throw  at     the       winning you  did   not winning ******
2024-02-01 12:44:30,031 	Text Alignment  :	I  I  I I        S       S    S         S   S            I  S       S      S      S         S       S    S     S           D     
2024-02-01 12:44:30,031 ========================================================================================================================
2024-02-01 12:44:33,169 Epoch 889: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.08 
2024-02-01 12:44:33,169 EPOCH 890
2024-02-01 12:44:46,003 Epoch 890: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-01 12:44:46,003 EPOCH 891
2024-02-01 12:44:58,909 Epoch 891: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.04 
2024-02-01 12:44:58,910 EPOCH 892
2024-02-01 12:45:11,807 Epoch 892: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.79 
2024-02-01 12:45:11,807 EPOCH 893
2024-02-01 12:45:24,692 Epoch 893: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-01 12:45:24,692 EPOCH 894
2024-02-01 12:45:37,439 Epoch 894: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-01 12:45:37,440 EPOCH 895
2024-02-01 12:45:50,194 Epoch 895: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-01 12:45:50,194 EPOCH 896
2024-02-01 12:46:02,986 Epoch 896: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-01 12:46:02,987 EPOCH 897
2024-02-01 12:46:15,982 Epoch 897: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 12:46:15,982 EPOCH 898
2024-02-01 12:46:28,762 Epoch 898: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-01 12:46:28,762 EPOCH 899
2024-02-01 12:46:41,688 Epoch 899: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 12:46:41,688 EPOCH 900
2024-02-01 12:46:54,484 [Epoch: 900 Step: 00008100] Batch Recognition Loss:   0.000221 => Gls Tokens per Sec:      831 || Batch Translation Loss:   0.075569 => Txt Tokens per Sec:     2306 || Lr: 0.000100
2024-02-01 12:46:54,484 Epoch 900: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-01 12:46:54,484 EPOCH 901
2024-02-01 12:47:07,285 Epoch 901: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 12:47:07,285 EPOCH 902
2024-02-01 12:47:20,562 Epoch 902: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 12:47:20,562 EPOCH 903
2024-02-01 12:47:34,152 Epoch 903: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 12:47:34,153 EPOCH 904
2024-02-01 12:47:47,280 Epoch 904: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 12:47:47,280 EPOCH 905
2024-02-01 12:48:00,212 Epoch 905: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 12:48:00,212 EPOCH 906
2024-02-01 12:48:13,209 Epoch 906: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 12:48:13,209 EPOCH 907
2024-02-01 12:48:26,138 Epoch 907: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 12:48:26,139 EPOCH 908
2024-02-01 12:48:39,295 Epoch 908: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 12:48:39,295 EPOCH 909
2024-02-01 12:48:52,233 Epoch 909: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 12:48:52,233 EPOCH 910
2024-02-01 12:49:05,090 Epoch 910: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 12:49:05,090 EPOCH 911
2024-02-01 12:49:17,999 Epoch 911: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 12:49:17,999 EPOCH 912
2024-02-01 12:49:20,964 [Epoch: 912 Step: 00008200] Batch Recognition Loss:   0.000248 => Gls Tokens per Sec:      432 || Batch Translation Loss:   0.031945 => Txt Tokens per Sec:     1382 || Lr: 0.000100
2024-02-01 12:49:30,954 Epoch 912: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 12:49:30,954 EPOCH 913
2024-02-01 12:49:43,893 Epoch 913: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 12:49:43,893 EPOCH 914
2024-02-01 12:49:56,801 Epoch 914: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 12:49:56,801 EPOCH 915
2024-02-01 12:50:09,791 Epoch 915: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 12:50:09,791 EPOCH 916
2024-02-01 12:50:22,785 Epoch 916: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 12:50:22,786 EPOCH 917
2024-02-01 12:50:35,859 Epoch 917: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 12:50:35,859 EPOCH 918
2024-02-01 12:50:48,853 Epoch 918: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-01 12:50:48,853 EPOCH 919
2024-02-01 12:51:01,907 Epoch 919: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-01 12:51:01,907 EPOCH 920
2024-02-01 12:51:14,851 Epoch 920: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-01 12:51:14,851 EPOCH 921
2024-02-01 12:51:27,788 Epoch 921: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-01 12:51:27,788 EPOCH 922
2024-02-01 12:51:40,674 Epoch 922: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-01 12:51:40,674 EPOCH 923
2024-02-01 12:51:42,668 [Epoch: 923 Step: 00008300] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:     1285 || Batch Translation Loss:   0.059013 => Txt Tokens per Sec:     3069 || Lr: 0.000100
2024-02-01 12:51:53,802 Epoch 923: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.81 
2024-02-01 12:51:53,803 EPOCH 924
2024-02-01 12:52:06,749 Epoch 924: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-01 12:52:06,750 EPOCH 925
2024-02-01 12:52:19,731 Epoch 925: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-01 12:52:19,731 EPOCH 926
2024-02-01 12:52:32,789 Epoch 926: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-01 12:52:32,789 EPOCH 927
2024-02-01 12:52:45,692 Epoch 927: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-01 12:52:45,693 EPOCH 928
2024-02-01 12:52:58,567 Epoch 928: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-01 12:52:58,567 EPOCH 929
2024-02-01 12:53:11,468 Epoch 929: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.78 
2024-02-01 12:53:11,469 EPOCH 930
2024-02-01 12:53:24,490 Epoch 930: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.74 
2024-02-01 12:53:24,490 EPOCH 931
2024-02-01 12:53:37,277 Epoch 931: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.11 
2024-02-01 12:53:37,277 EPOCH 932
2024-02-01 12:53:50,061 Epoch 932: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.76 
2024-02-01 12:53:50,062 EPOCH 933
2024-02-01 12:54:02,910 Epoch 933: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.64 
2024-02-01 12:54:02,911 EPOCH 934
2024-02-01 12:54:09,393 [Epoch: 934 Step: 00008400] Batch Recognition Loss:   0.000827 => Gls Tokens per Sec:      455 || Batch Translation Loss:   0.322264 => Txt Tokens per Sec:     1340 || Lr: 0.000100
2024-02-01 12:54:15,846 Epoch 934: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.58 
2024-02-01 12:54:15,846 EPOCH 935
2024-02-01 12:54:28,625 Epoch 935: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.96 
2024-02-01 12:54:28,625 EPOCH 936
2024-02-01 12:54:41,461 Epoch 936: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-01 12:54:41,461 EPOCH 937
2024-02-01 12:54:54,236 Epoch 937: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-01 12:54:54,236 EPOCH 938
2024-02-01 12:55:07,075 Epoch 938: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-01 12:55:07,075 EPOCH 939
2024-02-01 12:55:19,950 Epoch 939: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-01 12:55:19,950 EPOCH 940
2024-02-01 12:55:32,853 Epoch 940: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-01 12:55:32,853 EPOCH 941
2024-02-01 12:55:45,745 Epoch 941: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-01 12:55:45,745 EPOCH 942
2024-02-01 12:55:59,345 Epoch 942: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-01 12:55:59,346 EPOCH 943
2024-02-01 12:56:13,220 Epoch 943: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-01 12:56:13,220 EPOCH 944
2024-02-01 12:56:31,382 Epoch 944: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-01 12:56:31,382 EPOCH 945
2024-02-01 12:56:39,262 [Epoch: 945 Step: 00008500] Batch Recognition Loss:   0.000354 => Gls Tokens per Sec:      537 || Batch Translation Loss:   0.051728 => Txt Tokens per Sec:     1590 || Lr: 0.000100
2024-02-01 12:56:50,144 Epoch 945: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-01 12:56:50,145 EPOCH 946
2024-02-01 12:57:06,428 Epoch 946: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 12:57:06,428 EPOCH 947
2024-02-01 12:57:20,283 Epoch 947: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 12:57:20,283 EPOCH 948
2024-02-01 12:57:34,076 Epoch 948: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 12:57:34,076 EPOCH 949
2024-02-01 12:57:47,887 Epoch 949: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 12:57:47,888 EPOCH 950
2024-02-01 12:58:01,629 Epoch 950: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 12:58:01,629 EPOCH 951
2024-02-01 12:58:15,307 Epoch 951: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 12:58:15,307 EPOCH 952
2024-02-01 12:58:28,882 Epoch 952: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 12:58:28,882 EPOCH 953
2024-02-01 12:58:42,323 Epoch 953: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-01 12:58:42,323 EPOCH 954
2024-02-01 12:58:56,110 Epoch 954: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 12:58:56,110 EPOCH 955
2024-02-01 12:59:09,755 Epoch 955: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 12:59:09,756 EPOCH 956
2024-02-01 12:59:16,573 [Epoch: 956 Step: 00008600] Batch Recognition Loss:   0.000235 => Gls Tokens per Sec:      939 || Batch Translation Loss:   0.030239 => Txt Tokens per Sec:     2571 || Lr: 0.000100
2024-02-01 12:59:23,398 Epoch 956: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 12:59:23,398 EPOCH 957
2024-02-01 12:59:36,753 Epoch 957: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 12:59:36,753 EPOCH 958
2024-02-01 12:59:50,294 Epoch 958: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 12:59:50,295 EPOCH 959
2024-02-01 13:00:03,687 Epoch 959: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 13:00:03,687 EPOCH 960
2024-02-01 13:00:17,084 Epoch 960: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 13:00:17,084 EPOCH 961
2024-02-01 13:00:30,561 Epoch 961: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 13:00:30,562 EPOCH 962
2024-02-01 13:00:44,068 Epoch 962: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 13:00:44,069 EPOCH 963
2024-02-01 13:00:57,564 Epoch 963: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 13:00:57,564 EPOCH 964
2024-02-01 13:01:11,291 Epoch 964: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 13:01:11,291 EPOCH 965
2024-02-01 13:01:25,006 Epoch 965: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 13:01:25,006 EPOCH 966
2024-02-01 13:01:38,568 Epoch 966: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 13:01:38,569 EPOCH 967
2024-02-01 13:01:47,316 [Epoch: 967 Step: 00008700] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:      776 || Batch Translation Loss:   0.022141 => Txt Tokens per Sec:     2095 || Lr: 0.000100
2024-02-01 13:01:52,195 Epoch 967: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 13:01:52,196 EPOCH 968
2024-02-01 13:02:05,724 Epoch 968: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 13:02:05,724 EPOCH 969
2024-02-01 13:02:19,283 Epoch 969: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 13:02:19,284 EPOCH 970
2024-02-01 13:02:32,605 Epoch 970: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 13:02:32,606 EPOCH 971
2024-02-01 13:02:46,010 Epoch 971: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 13:02:46,011 EPOCH 972
2024-02-01 13:02:59,581 Epoch 972: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 13:02:59,581 EPOCH 973
2024-02-01 13:03:13,213 Epoch 973: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 13:03:13,213 EPOCH 974
2024-02-01 13:03:26,544 Epoch 974: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 13:03:26,544 EPOCH 975
2024-02-01 13:03:39,703 Epoch 975: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 13:03:39,703 EPOCH 976
2024-02-01 13:03:53,128 Epoch 976: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 13:03:53,128 EPOCH 977
2024-02-01 13:04:06,416 Epoch 977: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 13:04:06,416 EPOCH 978
2024-02-01 13:04:16,163 [Epoch: 978 Step: 00008800] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:      919 || Batch Translation Loss:   0.025152 => Txt Tokens per Sec:     2583 || Lr: 0.000100
2024-02-01 13:04:19,603 Epoch 978: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 13:04:19,603 EPOCH 979
2024-02-01 13:04:32,897 Epoch 979: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 13:04:32,898 EPOCH 980
2024-02-01 13:04:46,230 Epoch 980: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 13:04:46,230 EPOCH 981
2024-02-01 13:04:59,473 Epoch 981: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 13:04:59,473 EPOCH 982
2024-02-01 13:05:12,746 Epoch 982: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 13:05:12,746 EPOCH 983
2024-02-01 13:05:26,114 Epoch 983: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 13:05:26,114 EPOCH 984
2024-02-01 13:05:39,409 Epoch 984: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 13:05:39,409 EPOCH 985
2024-02-01 13:05:52,683 Epoch 985: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 13:05:52,684 EPOCH 986
2024-02-01 13:06:05,911 Epoch 986: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 13:06:05,911 EPOCH 987
2024-02-01 13:06:19,072 Epoch 987: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-01 13:06:19,072 EPOCH 988
2024-02-01 13:06:32,306 Epoch 988: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-01 13:06:32,306 EPOCH 989
2024-02-01 13:06:45,188 [Epoch: 989 Step: 00008900] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:      726 || Batch Translation Loss:   0.022579 => Txt Tokens per Sec:     2021 || Lr: 0.000100
2024-02-01 13:06:45,582 Epoch 989: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 13:06:45,582 EPOCH 990
2024-02-01 13:06:58,881 Epoch 990: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 13:06:58,881 EPOCH 991
2024-02-01 13:07:12,153 Epoch 991: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-01 13:07:12,153 EPOCH 992
2024-02-01 13:07:25,249 Epoch 992: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-01 13:07:25,249 EPOCH 993
2024-02-01 13:07:38,487 Epoch 993: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-01 13:07:38,487 EPOCH 994
2024-02-01 13:07:51,944 Epoch 994: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-01 13:07:51,945 EPOCH 995
2024-02-01 13:08:05,158 Epoch 995: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-01 13:08:05,158 EPOCH 996
2024-02-01 13:08:18,217 Epoch 996: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-01 13:08:18,217 EPOCH 997
2024-02-01 13:08:31,482 Epoch 997: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.87 
2024-02-01 13:08:31,482 EPOCH 998
2024-02-01 13:08:44,612 Epoch 998: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.82 
2024-02-01 13:08:44,613 EPOCH 999
2024-02-01 13:08:57,739 Epoch 999: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-01 13:08:57,739 EPOCH 1000
2024-02-01 13:09:10,948 [Epoch: 1000 Step: 00009000] Batch Recognition Loss:   0.000314 => Gls Tokens per Sec:      805 || Batch Translation Loss:   0.081418 => Txt Tokens per Sec:     2234 || Lr: 0.000100
2024-02-01 13:09:10,948 Epoch 1000: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-01 13:09:10,948 EPOCH 1001
2024-02-01 13:09:24,160 Epoch 1001: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-01 13:09:24,160 EPOCH 1002
2024-02-01 13:09:37,209 Epoch 1002: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-01 13:09:37,210 EPOCH 1003
2024-02-01 13:09:50,352 Epoch 1003: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.88 
2024-02-01 13:09:50,352 EPOCH 1004
2024-02-01 13:10:03,568 Epoch 1004: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.76 
2024-02-01 13:10:03,568 EPOCH 1005
2024-02-01 13:10:16,923 Epoch 1005: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-01 13:10:16,923 EPOCH 1006
2024-02-01 13:10:30,171 Epoch 1006: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-01 13:10:30,171 EPOCH 1007
2024-02-01 13:10:43,462 Epoch 1007: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-01 13:10:43,462 EPOCH 1008
2024-02-01 13:10:56,711 Epoch 1008: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-01 13:10:56,711 EPOCH 1009
2024-02-01 13:11:09,894 Epoch 1009: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-01 13:11:09,894 EPOCH 1010
2024-02-01 13:11:23,112 Epoch 1010: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-01 13:11:23,113 EPOCH 1011
2024-02-01 13:11:36,442 Epoch 1011: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-01 13:11:36,442 EPOCH 1012
2024-02-01 13:11:36,674 [Epoch: 1012 Step: 00009100] Batch Recognition Loss:   0.000234 => Gls Tokens per Sec:     5541 || Batch Translation Loss:   0.018251 => Txt Tokens per Sec:     9887 || Lr: 0.000100
2024-02-01 13:11:49,608 Epoch 1012: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 13:11:49,608 EPOCH 1013
2024-02-01 13:12:02,866 Epoch 1013: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-01 13:12:02,866 EPOCH 1014
2024-02-01 13:12:16,286 Epoch 1014: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-01 13:12:16,286 EPOCH 1015
2024-02-01 13:12:29,554 Epoch 1015: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-01 13:12:29,554 EPOCH 1016
2024-02-01 13:12:42,949 Epoch 1016: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-01 13:12:42,949 EPOCH 1017
2024-02-01 13:12:56,210 Epoch 1017: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-01 13:12:56,210 EPOCH 1018
2024-02-01 13:13:09,490 Epoch 1018: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-01 13:13:09,491 EPOCH 1019
2024-02-01 13:13:22,822 Epoch 1019: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-01 13:13:22,822 EPOCH 1020
2024-02-01 13:13:36,065 Epoch 1020: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.78 
2024-02-01 13:13:36,065 EPOCH 1021
2024-02-01 13:13:49,224 Epoch 1021: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-01 13:13:49,224 EPOCH 1022
2024-02-01 13:14:02,593 Epoch 1022: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-01 13:14:02,594 EPOCH 1023
2024-02-01 13:14:04,404 [Epoch: 1023 Step: 00009200] Batch Recognition Loss:   0.000302 => Gls Tokens per Sec:     1414 || Batch Translation Loss:   0.039296 => Txt Tokens per Sec:     3391 || Lr: 0.000100
2024-02-01 13:14:15,802 Epoch 1023: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.82 
2024-02-01 13:14:15,802 EPOCH 1024
2024-02-01 13:14:29,101 Epoch 1024: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.85 
2024-02-01 13:14:29,101 EPOCH 1025
2024-02-01 13:14:42,204 Epoch 1025: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-01 13:14:42,204 EPOCH 1026
2024-02-01 13:14:55,414 Epoch 1026: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-01 13:14:55,414 EPOCH 1027
2024-02-01 13:15:08,599 Epoch 1027: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-01 13:15:08,599 EPOCH 1028
2024-02-01 13:15:21,755 Epoch 1028: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-01 13:15:21,756 EPOCH 1029
2024-02-01 13:15:35,111 Epoch 1029: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.79 
2024-02-01 13:15:35,111 EPOCH 1030
2024-02-01 13:15:48,433 Epoch 1030: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-01 13:15:48,434 EPOCH 1031
2024-02-01 13:16:01,510 Epoch 1031: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.83 
2024-02-01 13:16:01,510 EPOCH 1032
2024-02-01 13:16:14,550 Epoch 1032: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-01 13:16:14,551 EPOCH 1033
2024-02-01 13:16:27,480 Epoch 1033: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-01 13:16:27,480 EPOCH 1034
2024-02-01 13:16:31,875 [Epoch: 1034 Step: 00009300] Batch Recognition Loss:   0.000267 => Gls Tokens per Sec:      874 || Batch Translation Loss:   0.042777 => Txt Tokens per Sec:     2337 || Lr: 0.000100
2024-02-01 13:16:40,641 Epoch 1034: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-01 13:16:40,641 EPOCH 1035
2024-02-01 13:16:53,750 Epoch 1035: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-01 13:16:53,750 EPOCH 1036
2024-02-01 13:17:06,957 Epoch 1036: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-01 13:17:06,958 EPOCH 1037
2024-02-01 13:17:20,120 Epoch 1037: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 13:17:20,120 EPOCH 1038
2024-02-01 13:17:33,337 Epoch 1038: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 13:17:33,337 EPOCH 1039
2024-02-01 13:17:46,490 Epoch 1039: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 13:17:46,490 EPOCH 1040
2024-02-01 13:17:59,615 Epoch 1040: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 13:17:59,616 EPOCH 1041
2024-02-01 13:18:12,737 Epoch 1041: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 13:18:12,737 EPOCH 1042
2024-02-01 13:18:25,741 Epoch 1042: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 13:18:25,741 EPOCH 1043
2024-02-01 13:18:39,001 Epoch 1043: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 13:18:39,001 EPOCH 1044
2024-02-01 13:18:52,071 Epoch 1044: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 13:18:52,071 EPOCH 1045
2024-02-01 13:18:57,457 [Epoch: 1045 Step: 00009400] Batch Recognition Loss:   0.000228 => Gls Tokens per Sec:      951 || Batch Translation Loss:   0.031540 => Txt Tokens per Sec:     2718 || Lr: 0.000100
2024-02-01 13:19:05,193 Epoch 1045: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 13:19:05,194 EPOCH 1046
2024-02-01 13:19:18,405 Epoch 1046: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 13:19:18,405 EPOCH 1047
2024-02-01 13:19:31,514 Epoch 1047: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 13:19:31,514 EPOCH 1048
2024-02-01 13:19:44,721 Epoch 1048: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 13:19:44,721 EPOCH 1049
2024-02-01 13:19:57,964 Epoch 1049: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 13:19:57,966 EPOCH 1050
2024-02-01 13:20:11,201 Epoch 1050: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 13:20:11,202 EPOCH 1051
2024-02-01 13:20:24,223 Epoch 1051: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 13:20:24,223 EPOCH 1052
2024-02-01 13:20:37,262 Epoch 1052: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 13:20:37,262 EPOCH 1053
2024-02-01 13:20:50,372 Epoch 1053: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 13:20:50,372 EPOCH 1054
2024-02-01 13:21:03,495 Epoch 1054: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 13:21:03,496 EPOCH 1055
2024-02-01 13:21:16,618 Epoch 1055: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 13:21:16,618 EPOCH 1056
2024-02-01 13:21:24,352 [Epoch: 1056 Step: 00009500] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:      828 || Batch Translation Loss:   0.027211 => Txt Tokens per Sec:     2341 || Lr: 0.000100
2024-02-01 13:21:29,754 Epoch 1056: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 13:21:29,754 EPOCH 1057
2024-02-01 13:21:42,816 Epoch 1057: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 13:21:42,817 EPOCH 1058
2024-02-01 13:21:55,936 Epoch 1058: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 13:21:55,936 EPOCH 1059
2024-02-01 13:22:09,083 Epoch 1059: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 13:22:09,083 EPOCH 1060
2024-02-01 13:22:22,233 Epoch 1060: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 13:22:22,233 EPOCH 1061
2024-02-01 13:22:35,156 Epoch 1061: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 13:22:35,156 EPOCH 1062
2024-02-01 13:22:48,146 Epoch 1062: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-01 13:22:48,147 EPOCH 1063
2024-02-01 13:23:01,133 Epoch 1063: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-01 13:23:01,133 EPOCH 1064
2024-02-01 13:23:14,149 Epoch 1064: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.97 
2024-02-01 13:23:14,149 EPOCH 1065
2024-02-01 13:23:27,420 Epoch 1065: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.12 
2024-02-01 13:23:27,420 EPOCH 1066
2024-02-01 13:23:40,446 Epoch 1066: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.98 
2024-02-01 13:23:40,446 EPOCH 1067
2024-02-01 13:23:51,125 [Epoch: 1067 Step: 00009600] Batch Recognition Loss:   0.000436 => Gls Tokens per Sec:      636 || Batch Translation Loss:   0.053083 => Txt Tokens per Sec:     1749 || Lr: 0.000100
2024-02-01 13:23:53,630 Epoch 1067: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.22 
2024-02-01 13:23:53,630 EPOCH 1068
2024-02-01 13:24:06,786 Epoch 1068: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.92 
2024-02-01 13:24:06,786 EPOCH 1069
2024-02-01 13:24:19,838 Epoch 1069: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.96 
2024-02-01 13:24:19,838 EPOCH 1070
2024-02-01 13:24:33,227 Epoch 1070: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.77 
2024-02-01 13:24:33,227 EPOCH 1071
2024-02-01 13:24:46,461 Epoch 1071: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-01 13:24:46,461 EPOCH 1072
2024-02-01 13:24:59,531 Epoch 1072: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-01 13:24:59,531 EPOCH 1073
2024-02-01 13:25:12,668 Epoch 1073: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-01 13:25:12,668 EPOCH 1074
2024-02-01 13:25:25,742 Epoch 1074: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-01 13:25:25,742 EPOCH 1075
2024-02-01 13:25:38,819 Epoch 1075: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-01 13:25:38,819 EPOCH 1076
2024-02-01 13:25:51,967 Epoch 1076: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-01 13:25:51,967 EPOCH 1077
2024-02-01 13:26:05,614 Epoch 1077: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-01 13:26:05,615 EPOCH 1078
2024-02-01 13:26:16,050 [Epoch: 1078 Step: 00009700] Batch Recognition Loss:   0.000271 => Gls Tokens per Sec:      773 || Batch Translation Loss:   0.050651 => Txt Tokens per Sec:     2099 || Lr: 0.000100
2024-02-01 13:26:19,525 Epoch 1078: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-01 13:26:19,526 EPOCH 1079
2024-02-01 13:26:33,537 Epoch 1079: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 13:26:33,538 EPOCH 1080
2024-02-01 13:26:47,663 Epoch 1080: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 13:26:47,664 EPOCH 1081
2024-02-01 13:27:01,645 Epoch 1081: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 13:27:01,645 EPOCH 1082
2024-02-01 13:27:15,670 Epoch 1082: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-01 13:27:15,670 EPOCH 1083
2024-02-01 13:27:29,587 Epoch 1083: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 13:27:29,588 EPOCH 1084
2024-02-01 13:27:43,421 Epoch 1084: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 13:27:43,422 EPOCH 1085
2024-02-01 13:27:57,694 Epoch 1085: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 13:27:57,694 EPOCH 1086
2024-02-01 13:28:11,561 Epoch 1086: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 13:28:11,561 EPOCH 1087
2024-02-01 13:28:25,489 Epoch 1087: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 13:28:25,490 EPOCH 1088
2024-02-01 13:28:38,913 Epoch 1088: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-01 13:28:38,913 EPOCH 1089
2024-02-01 13:28:52,155 [Epoch: 1089 Step: 00009800] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:      706 || Batch Translation Loss:   0.035953 => Txt Tokens per Sec:     1944 || Lr: 0.000100
2024-02-01 13:28:52,871 Epoch 1089: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 13:28:52,872 EPOCH 1090
2024-02-01 13:29:06,838 Epoch 1090: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 13:29:06,839 EPOCH 1091
2024-02-01 13:29:20,711 Epoch 1091: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 13:29:20,711 EPOCH 1092
2024-02-01 13:29:34,847 Epoch 1092: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 13:29:34,848 EPOCH 1093
2024-02-01 13:29:48,692 Epoch 1093: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 13:29:48,693 EPOCH 1094
2024-02-01 13:30:02,648 Epoch 1094: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 13:30:02,648 EPOCH 1095
2024-02-01 13:30:16,514 Epoch 1095: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 13:30:16,515 EPOCH 1096
2024-02-01 13:30:30,785 Epoch 1096: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 13:30:30,786 EPOCH 1097
2024-02-01 13:30:44,948 Epoch 1097: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 13:30:44,949 EPOCH 1098
2024-02-01 13:30:58,799 Epoch 1098: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 13:30:58,800 EPOCH 1099
2024-02-01 13:31:12,554 Epoch 1099: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 13:31:12,554 EPOCH 1100
2024-02-01 13:31:26,450 [Epoch: 1100 Step: 00009900] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:      765 || Batch Translation Loss:   0.018491 => Txt Tokens per Sec:     2124 || Lr: 0.000100
2024-02-01 13:31:26,451 Epoch 1100: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 13:31:26,451 EPOCH 1101
2024-02-01 13:31:40,359 Epoch 1101: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 13:31:40,360 EPOCH 1102
2024-02-01 13:31:54,117 Epoch 1102: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 13:31:54,117 EPOCH 1103
2024-02-01 13:32:08,207 Epoch 1103: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 13:32:08,208 EPOCH 1104
2024-02-01 13:32:22,088 Epoch 1104: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 13:32:22,088 EPOCH 1105
2024-02-01 13:32:35,937 Epoch 1105: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 13:32:35,938 EPOCH 1106
2024-02-01 13:32:50,005 Epoch 1106: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 13:32:50,005 EPOCH 1107
2024-02-01 13:33:03,984 Epoch 1107: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 13:33:03,984 EPOCH 1108
2024-02-01 13:33:17,680 Epoch 1108: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 13:33:17,680 EPOCH 1109
2024-02-01 13:33:31,374 Epoch 1109: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 13:33:31,374 EPOCH 1110
2024-02-01 13:33:45,525 Epoch 1110: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 13:33:45,526 EPOCH 1111
2024-02-01 13:33:59,379 Epoch 1111: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 13:33:59,379 EPOCH 1112
2024-02-01 13:34:02,831 [Epoch: 1112 Step: 00010000] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:      371 || Batch Translation Loss:   0.029726 => Txt Tokens per Sec:     1300 || Lr: 0.000100
2024-02-01 13:34:23,244 Validation result at epoch 1112, step    10000: duration: 20.4134s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00008	Translation Loss: 93878.45312	PPL: 12021.13477
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.95	(BLEU-1: 12.16,	BLEU-2: 4.17,	BLEU-3: 1.84,	BLEU-4: 0.95)
	CHRF 17.79	ROUGE 10.63
2024-02-01 13:34:23,245 Logging Recognition and Translation Outputs
2024-02-01 13:34:23,245 ========================================================================================================================
2024-02-01 13:34:23,245 Logging Sequence: 159_139.00
2024-02-01 13:34:23,245 	Gloss Reference :	A B+C+D+E
2024-02-01 13:34:23,246 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 13:34:23,246 	Gloss Alignment :	         
2024-02-01 13:34:23,246 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 13:34:23,247 	Text Reference  :	he took time and finally   was ready for the ********** asia    cup   where he     scored the  century 
2024-02-01 13:34:23,247 	Text Hypothesis :	** **** **** *** yesterday was ***** *** the semi-final between india and   hardik pandya were watching
2024-02-01 13:34:23,247 	Text Alignment  :	D  D    D    D   S             D     D       I          S       S     S     S      S      S    S       
2024-02-01 13:34:23,248 ========================================================================================================================
2024-02-01 13:34:23,248 Logging Sequence: 159_159.00
2024-02-01 13:34:23,248 	Gloss Reference :	A B+C+D+E
2024-02-01 13:34:23,248 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 13:34:23,248 	Gloss Alignment :	         
2024-02-01 13:34:23,248 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 13:34:23,250 	Text Reference  :	he said it wasn't easy the mind has   to  be     focussed and he   is  glad   that he is back in form with the asia cup century
2024-02-01 13:34:23,250 	Text Hypothesis :	** **** ** kohli  is   the **** first day people will     not find any takers and  he is **** ** **** **** *** **** *** *******
2024-02-01 13:34:23,250 	Text Alignment  :	D  D    D  S      S        D    S     S   S      S        S   S    S   S      S          D    D  D    D    D   D    D   D      
2024-02-01 13:34:23,251 ========================================================================================================================
2024-02-01 13:34:23,251 Logging Sequence: 103_8.00
2024-02-01 13:34:23,251 	Gloss Reference :	A B+C+D+E
2024-02-01 13:34:23,251 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 13:34:23,251 	Gloss Alignment :	         
2024-02-01 13:34:23,251 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 13:34:23,252 	Text Reference  :	were going on in    birmingham england from  28th july   to  8th     august 2022 
2024-02-01 13:34:23,252 	Text Hypothesis :	**** ***** ** since then       2022    india had  booked the british empire games
2024-02-01 13:34:23,252 	Text Alignment  :	D    D     D  S     S          S       S     S    S      S   S       S      S    
2024-02-01 13:34:23,252 ========================================================================================================================
2024-02-01 13:34:23,253 Logging Sequence: 164_546.00
2024-02-01 13:34:23,253 	Gloss Reference :	A B+C+D+E
2024-02-01 13:34:23,253 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 13:34:23,253 	Gloss Alignment :	         
2024-02-01 13:34:23,253 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 13:34:23,254 	Text Reference  :	reliance has turned  out    to    be the strongest company  
2024-02-01 13:34:23,254 	Text Hypothesis :	after    the british empire games it has been      postponed
2024-02-01 13:34:23,254 	Text Alignment  :	S        S   S       S      S     S  S   S         S        
2024-02-01 13:34:23,254 ========================================================================================================================
2024-02-01 13:34:23,254 Logging Sequence: 132_173.00
2024-02-01 13:34:23,254 	Gloss Reference :	A B+C+D+E
2024-02-01 13:34:23,254 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 13:34:23,255 	Gloss Alignment :	         
2024-02-01 13:34:23,255 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 13:34:23,255 	Text Reference  :	usman is ******** ** *** **** ***** australia' first muslim player  
2024-02-01 13:34:23,255 	Text Hypothesis :	he    is survived by his wife their son        and   a      daughter
2024-02-01 13:34:23,255 	Text Alignment  :	S        I        I  I   I    I     S          S     S      S       
2024-02-01 13:34:23,256 ========================================================================================================================
2024-02-01 13:34:33,795 Epoch 1112: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 13:34:33,796 EPOCH 1113
2024-02-01 13:34:47,745 Epoch 1113: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 13:34:47,745 EPOCH 1114
2024-02-01 13:35:01,604 Epoch 1114: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 13:35:01,604 EPOCH 1115
2024-02-01 13:35:15,498 Epoch 1115: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 13:35:15,498 EPOCH 1116
2024-02-01 13:35:29,572 Epoch 1116: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-01 13:35:29,572 EPOCH 1117
2024-02-01 13:35:43,596 Epoch 1117: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-01 13:35:43,597 EPOCH 1118
2024-02-01 13:35:57,647 Epoch 1118: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-01 13:35:57,648 EPOCH 1119
2024-02-01 13:36:11,989 Epoch 1119: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-01 13:36:11,989 EPOCH 1120
2024-02-01 13:36:26,011 Epoch 1120: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-01 13:36:26,011 EPOCH 1121
2024-02-01 13:36:40,055 Epoch 1121: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-01 13:36:40,056 EPOCH 1122
2024-02-01 13:36:54,103 Epoch 1122: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-01 13:36:54,104 EPOCH 1123
2024-02-01 13:36:57,774 [Epoch: 1123 Step: 00010100] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:      455 || Batch Translation Loss:   0.030053 => Txt Tokens per Sec:     1184 || Lr: 0.000100
2024-02-01 13:37:08,160 Epoch 1123: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-01 13:37:08,160 EPOCH 1124
2024-02-01 13:37:22,040 Epoch 1124: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-01 13:37:22,041 EPOCH 1125
2024-02-01 13:37:36,205 Epoch 1125: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-01 13:37:36,205 EPOCH 1126
2024-02-01 13:37:50,384 Epoch 1126: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-01 13:37:50,385 EPOCH 1127
2024-02-01 13:38:04,346 Epoch 1127: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.83 
2024-02-01 13:38:04,346 EPOCH 1128
2024-02-01 13:38:18,173 Epoch 1128: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-01 13:38:18,173 EPOCH 1129
2024-02-01 13:38:32,326 Epoch 1129: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-01 13:38:32,327 EPOCH 1130
2024-02-01 13:38:46,395 Epoch 1130: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-01 13:38:46,396 EPOCH 1131
2024-02-01 13:39:00,206 Epoch 1131: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-01 13:39:00,206 EPOCH 1132
2024-02-01 13:39:14,314 Epoch 1132: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-01 13:39:14,315 EPOCH 1133
2024-02-01 13:39:28,198 Epoch 1133: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-01 13:39:28,199 EPOCH 1134
2024-02-01 13:39:35,413 [Epoch: 1134 Step: 00010200] Batch Recognition Loss:   0.000415 => Gls Tokens per Sec:      532 || Batch Translation Loss:   0.078397 => Txt Tokens per Sec:     1601 || Lr: 0.000100
2024-02-01 13:39:42,369 Epoch 1134: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-01 13:39:42,370 EPOCH 1135
2024-02-01 13:39:56,326 Epoch 1135: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-01 13:39:56,326 EPOCH 1136
2024-02-01 13:40:10,358 Epoch 1136: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.77 
2024-02-01 13:40:10,359 EPOCH 1137
2024-02-01 13:40:24,272 Epoch 1137: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.31 
2024-02-01 13:40:24,273 EPOCH 1138
2024-02-01 13:40:38,319 Epoch 1138: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.92 
2024-02-01 13:40:38,319 EPOCH 1139
2024-02-01 13:40:52,231 Epoch 1139: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.37 
2024-02-01 13:40:52,232 EPOCH 1140
2024-02-01 13:41:06,313 Epoch 1140: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.76 
2024-02-01 13:41:06,313 EPOCH 1141
2024-02-01 13:41:20,360 Epoch 1141: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-01 13:41:20,361 EPOCH 1142
2024-02-01 13:41:34,460 Epoch 1142: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.93 
2024-02-01 13:41:34,460 EPOCH 1143
2024-02-01 13:41:48,409 Epoch 1143: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.96 
2024-02-01 13:41:48,409 EPOCH 1144
2024-02-01 13:42:02,631 Epoch 1144: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.91 
2024-02-01 13:42:02,632 EPOCH 1145
2024-02-01 13:42:04,336 [Epoch: 1145 Step: 00010300] Batch Recognition Loss:   0.000286 => Gls Tokens per Sec:     3009 || Batch Translation Loss:   0.051484 => Txt Tokens per Sec:     7860 || Lr: 0.000100
2024-02-01 13:42:16,771 Epoch 1145: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.89 
2024-02-01 13:42:16,772 EPOCH 1146
2024-02-01 13:42:30,784 Epoch 1146: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-01 13:42:30,784 EPOCH 1147
2024-02-01 13:42:44,804 Epoch 1147: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.78 
2024-02-01 13:42:44,805 EPOCH 1148
2024-02-01 13:42:58,758 Epoch 1148: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.83 
2024-02-01 13:42:58,759 EPOCH 1149
2024-02-01 13:43:12,942 Epoch 1149: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-01 13:43:12,943 EPOCH 1150
2024-02-01 13:43:26,961 Epoch 1150: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-01 13:43:26,961 EPOCH 1151
2024-02-01 13:43:41,114 Epoch 1151: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-01 13:43:41,114 EPOCH 1152
2024-02-01 13:43:55,361 Epoch 1152: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-01 13:43:55,361 EPOCH 1153
2024-02-01 13:44:09,407 Epoch 1153: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-01 13:44:09,408 EPOCH 1154
2024-02-01 13:44:23,387 Epoch 1154: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-01 13:44:23,387 EPOCH 1155
2024-02-01 13:44:37,485 Epoch 1155: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-01 13:44:37,486 EPOCH 1156
2024-02-01 13:44:46,584 [Epoch: 1156 Step: 00010400] Batch Recognition Loss:   0.000225 => Gls Tokens per Sec:      704 || Batch Translation Loss:   0.019433 => Txt Tokens per Sec:     1970 || Lr: 0.000100
2024-02-01 13:44:51,440 Epoch 1156: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 13:44:51,440 EPOCH 1157
2024-02-01 13:45:05,476 Epoch 1157: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 13:45:05,477 EPOCH 1158
2024-02-01 13:45:18,897 Epoch 1158: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 13:45:18,897 EPOCH 1159
2024-02-01 13:45:32,705 Epoch 1159: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 13:45:32,705 EPOCH 1160
2024-02-01 13:45:46,537 Epoch 1160: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 13:45:46,538 EPOCH 1161
2024-02-01 13:46:00,395 Epoch 1161: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 13:46:00,395 EPOCH 1162
2024-02-01 13:46:14,167 Epoch 1162: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 13:46:14,168 EPOCH 1163
2024-02-01 13:46:28,203 Epoch 1163: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 13:46:28,203 EPOCH 1164
2024-02-01 13:46:42,336 Epoch 1164: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 13:46:42,336 EPOCH 1165
2024-02-01 13:46:56,050 Epoch 1165: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 13:46:56,051 EPOCH 1166
2024-02-01 13:47:10,029 Epoch 1166: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 13:47:10,029 EPOCH 1167
2024-02-01 13:47:22,908 [Epoch: 1167 Step: 00010500] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:      527 || Batch Translation Loss:   0.020456 => Txt Tokens per Sec:     1594 || Lr: 0.000100
2024-02-01 13:47:23,969 Epoch 1167: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 13:47:23,970 EPOCH 1168
2024-02-01 13:47:37,876 Epoch 1168: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 13:47:37,877 EPOCH 1169
2024-02-01 13:47:51,376 Epoch 1169: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 13:47:51,377 EPOCH 1170
2024-02-01 13:48:05,515 Epoch 1170: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 13:48:05,516 EPOCH 1171
2024-02-01 13:48:19,380 Epoch 1171: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 13:48:19,380 EPOCH 1172
2024-02-01 13:48:33,118 Epoch 1172: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 13:48:33,118 EPOCH 1173
2024-02-01 13:48:47,112 Epoch 1173: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 13:48:47,112 EPOCH 1174
2024-02-01 13:49:00,842 Epoch 1174: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 13:49:00,843 EPOCH 1175
2024-02-01 13:49:15,236 Epoch 1175: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 13:49:15,237 EPOCH 1176
2024-02-01 13:49:28,983 Epoch 1176: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 13:49:28,984 EPOCH 1177
2024-02-01 13:49:43,125 Epoch 1177: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 13:49:43,125 EPOCH 1178
2024-02-01 13:49:56,015 [Epoch: 1178 Step: 00010600] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:      626 || Batch Translation Loss:   0.020724 => Txt Tokens per Sec:     1751 || Lr: 0.000100
2024-02-01 13:49:56,998 Epoch 1178: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 13:49:56,998 EPOCH 1179
2024-02-01 13:50:10,657 Epoch 1179: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 13:50:10,657 EPOCH 1180
2024-02-01 13:50:24,770 Epoch 1180: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 13:50:24,770 EPOCH 1181
2024-02-01 13:50:38,603 Epoch 1181: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 13:50:38,603 EPOCH 1182
2024-02-01 13:50:52,604 Epoch 1182: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 13:50:52,604 EPOCH 1183
2024-02-01 13:51:06,459 Epoch 1183: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 13:51:06,460 EPOCH 1184
2024-02-01 13:51:20,463 Epoch 1184: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 13:51:20,464 EPOCH 1185
2024-02-01 13:51:34,361 Epoch 1185: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 13:51:34,362 EPOCH 1186
2024-02-01 13:51:48,260 Epoch 1186: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 13:51:48,260 EPOCH 1187
2024-02-01 13:52:02,203 Epoch 1187: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 13:52:02,204 EPOCH 1188
2024-02-01 13:52:16,076 Epoch 1188: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 13:52:16,077 EPOCH 1189
2024-02-01 13:52:29,645 [Epoch: 1189 Step: 00010700] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:      689 || Batch Translation Loss:   0.075714 => Txt Tokens per Sec:     1940 || Lr: 0.000100
2024-02-01 13:52:30,035 Epoch 1189: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 13:52:30,036 EPOCH 1190
2024-02-01 13:52:44,015 Epoch 1190: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 13:52:44,015 EPOCH 1191
2024-02-01 13:52:57,870 Epoch 1191: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 13:52:57,871 EPOCH 1192
2024-02-01 13:53:11,884 Epoch 1192: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 13:53:11,885 EPOCH 1193
2024-02-01 13:53:25,765 Epoch 1193: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 13:53:25,766 EPOCH 1194
2024-02-01 13:53:39,675 Epoch 1194: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 13:53:39,676 EPOCH 1195
2024-02-01 13:53:53,951 Epoch 1195: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 13:53:53,951 EPOCH 1196
2024-02-01 13:54:08,094 Epoch 1196: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 13:54:08,095 EPOCH 1197
2024-02-01 13:54:21,919 Epoch 1197: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 13:54:21,920 EPOCH 1198
2024-02-01 13:54:35,932 Epoch 1198: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 13:54:35,932 EPOCH 1199
2024-02-01 13:54:50,301 Epoch 1199: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 13:54:50,302 EPOCH 1200
2024-02-01 13:55:04,378 [Epoch: 1200 Step: 00010800] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:      755 || Batch Translation Loss:   0.007933 => Txt Tokens per Sec:     2096 || Lr: 0.000100
2024-02-01 13:55:04,378 Epoch 1200: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 13:55:04,378 EPOCH 1201
2024-02-01 13:55:18,183 Epoch 1201: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 13:55:18,184 EPOCH 1202
2024-02-01 13:55:31,927 Epoch 1202: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 13:55:31,928 EPOCH 1203
2024-02-01 13:55:45,636 Epoch 1203: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 13:55:45,636 EPOCH 1204
2024-02-01 13:55:59,735 Epoch 1204: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 13:55:59,736 EPOCH 1205
2024-02-01 13:56:13,758 Epoch 1205: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 13:56:13,759 EPOCH 1206
2024-02-01 13:56:27,878 Epoch 1206: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 13:56:27,878 EPOCH 1207
2024-02-01 13:56:41,812 Epoch 1207: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 13:56:41,813 EPOCH 1208
2024-02-01 13:56:55,817 Epoch 1208: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-01 13:56:55,818 EPOCH 1209
2024-02-01 13:57:09,968 Epoch 1209: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-01 13:57:09,969 EPOCH 1210
2024-02-01 13:57:23,749 Epoch 1210: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.00 
2024-02-01 13:57:23,749 EPOCH 1211
2024-02-01 13:57:37,612 Epoch 1211: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.21 
2024-02-01 13:57:37,613 EPOCH 1212
2024-02-01 13:57:38,004 [Epoch: 1212 Step: 00010900] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:     3282 || Batch Translation Loss:   0.054064 => Txt Tokens per Sec:     7595 || Lr: 0.000100
2024-02-01 13:57:51,905 Epoch 1212: Total Training Recognition Loss 0.00  Total Training Translation Loss 2.23 
2024-02-01 13:57:51,905 EPOCH 1213
2024-02-01 13:58:05,966 Epoch 1213: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.83 
2024-02-01 13:58:05,966 EPOCH 1214
2024-02-01 13:58:20,046 Epoch 1214: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.39 
2024-02-01 13:58:20,047 EPOCH 1215
2024-02-01 13:58:33,902 Epoch 1215: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.83 
2024-02-01 13:58:33,903 EPOCH 1216
2024-02-01 13:58:47,779 Epoch 1216: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.34 
2024-02-01 13:58:47,779 EPOCH 1217
2024-02-01 13:59:01,460 Epoch 1217: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.29 
2024-02-01 13:59:01,461 EPOCH 1218
2024-02-01 13:59:15,463 Epoch 1218: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.00 
2024-02-01 13:59:15,463 EPOCH 1219
2024-02-01 13:59:29,462 Epoch 1219: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.14 
2024-02-01 13:59:29,462 EPOCH 1220
2024-02-01 13:59:43,361 Epoch 1220: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.63 
2024-02-01 13:59:43,362 EPOCH 1221
2024-02-01 13:59:57,624 Epoch 1221: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-01 13:59:57,625 EPOCH 1222
2024-02-01 14:00:11,636 Epoch 1222: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-01 14:00:11,637 EPOCH 1223
2024-02-01 14:00:15,158 [Epoch: 1223 Step: 00011000] Batch Recognition Loss:   0.000456 => Gls Tokens per Sec:      727 || Batch Translation Loss:   0.062044 => Txt Tokens per Sec:     2159 || Lr: 0.000100
2024-02-01 14:00:25,275 Epoch 1223: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-01 14:00:25,276 EPOCH 1224
2024-02-01 14:00:39,371 Epoch 1224: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-01 14:00:39,371 EPOCH 1225
2024-02-01 14:00:53,217 Epoch 1225: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-01 14:00:53,218 EPOCH 1226
2024-02-01 14:01:07,256 Epoch 1226: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-01 14:01:07,257 EPOCH 1227
2024-02-01 14:01:21,203 Epoch 1227: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 14:01:21,204 EPOCH 1228
2024-02-01 14:01:35,254 Epoch 1228: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-01 14:01:35,255 EPOCH 1229
2024-02-01 14:01:49,081 Epoch 1229: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-01 14:01:49,082 EPOCH 1230
2024-02-01 14:02:03,231 Epoch 1230: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 14:02:03,231 EPOCH 1231
2024-02-01 14:02:17,195 Epoch 1231: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 14:02:17,196 EPOCH 1232
2024-02-01 14:02:31,263 Epoch 1232: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 14:02:31,263 EPOCH 1233
2024-02-01 14:02:45,149 Epoch 1233: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 14:02:45,149 EPOCH 1234
2024-02-01 14:02:52,206 [Epoch: 1234 Step: 00011100] Batch Recognition Loss:   0.000328 => Gls Tokens per Sec:      418 || Batch Translation Loss:   0.033285 => Txt Tokens per Sec:     1309 || Lr: 0.000100
2024-02-01 14:02:59,065 Epoch 1234: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 14:02:59,066 EPOCH 1235
2024-02-01 14:03:12,803 Epoch 1235: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 14:03:12,803 EPOCH 1236
2024-02-01 14:03:26,592 Epoch 1236: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 14:03:26,593 EPOCH 1237
2024-02-01 14:03:40,819 Epoch 1237: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 14:03:40,820 EPOCH 1238
2024-02-01 14:03:54,650 Epoch 1238: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 14:03:54,650 EPOCH 1239
2024-02-01 14:04:08,225 Epoch 1239: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 14:04:08,226 EPOCH 1240
2024-02-01 14:04:22,294 Epoch 1240: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 14:04:22,295 EPOCH 1241
2024-02-01 14:04:36,362 Epoch 1241: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 14:04:36,362 EPOCH 1242
2024-02-01 14:04:50,460 Epoch 1242: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 14:04:50,460 EPOCH 1243
2024-02-01 14:05:04,238 Epoch 1243: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 14:05:04,238 EPOCH 1244
2024-02-01 14:05:18,030 Epoch 1244: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 14:05:18,031 EPOCH 1245
2024-02-01 14:05:23,457 [Epoch: 1245 Step: 00011200] Batch Recognition Loss:   0.000220 => Gls Tokens per Sec:      944 || Batch Translation Loss:   0.025438 => Txt Tokens per Sec:     2432 || Lr: 0.000100
2024-02-01 14:05:31,951 Epoch 1245: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 14:05:31,951 EPOCH 1246
2024-02-01 14:05:46,132 Epoch 1246: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 14:05:46,133 EPOCH 1247
2024-02-01 14:06:00,267 Epoch 1247: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 14:06:00,267 EPOCH 1248
2024-02-01 14:06:14,047 Epoch 1248: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 14:06:14,047 EPOCH 1249
2024-02-01 14:06:28,120 Epoch 1249: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 14:06:28,121 EPOCH 1250
2024-02-01 14:06:41,889 Epoch 1250: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 14:06:41,889 EPOCH 1251
2024-02-01 14:06:55,602 Epoch 1251: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 14:06:55,603 EPOCH 1252
2024-02-01 14:07:09,457 Epoch 1252: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 14:07:09,458 EPOCH 1253
2024-02-01 14:07:23,300 Epoch 1253: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 14:07:23,301 EPOCH 1254
2024-02-01 14:07:37,347 Epoch 1254: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 14:07:37,347 EPOCH 1255
2024-02-01 14:07:51,094 Epoch 1255: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 14:07:51,095 EPOCH 1256
2024-02-01 14:07:57,880 [Epoch: 1256 Step: 00011300] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:      943 || Batch Translation Loss:   0.019442 => Txt Tokens per Sec:     2553 || Lr: 0.000100
2024-02-01 14:08:05,187 Epoch 1256: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 14:08:05,188 EPOCH 1257
2024-02-01 14:08:19,052 Epoch 1257: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 14:08:19,053 EPOCH 1258
2024-02-01 14:08:32,920 Epoch 1258: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 14:08:32,920 EPOCH 1259
2024-02-01 14:08:46,763 Epoch 1259: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 14:08:46,763 EPOCH 1260
2024-02-01 14:09:01,137 Epoch 1260: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 14:09:01,137 EPOCH 1261
2024-02-01 14:09:14,949 Epoch 1261: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 14:09:14,950 EPOCH 1262
2024-02-01 14:09:28,702 Epoch 1262: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 14:09:28,702 EPOCH 1263
2024-02-01 14:09:42,223 Epoch 1263: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 14:09:42,223 EPOCH 1264
2024-02-01 14:09:56,053 Epoch 1264: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 14:09:56,053 EPOCH 1265
2024-02-01 14:10:10,279 Epoch 1265: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 14:10:10,280 EPOCH 1266
2024-02-01 14:10:24,267 Epoch 1266: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 14:10:24,268 EPOCH 1267
2024-02-01 14:10:35,855 [Epoch: 1267 Step: 00011400] Batch Recognition Loss:   0.000106 => Gls Tokens per Sec:      586 || Batch Translation Loss:   0.007551 => Txt Tokens per Sec:     1719 || Lr: 0.000100
2024-02-01 14:10:38,214 Epoch 1267: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 14:10:38,214 EPOCH 1268
2024-02-01 14:10:51,963 Epoch 1268: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 14:10:51,963 EPOCH 1269
2024-02-01 14:11:05,953 Epoch 1269: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 14:11:05,954 EPOCH 1270
2024-02-01 14:11:19,764 Epoch 1270: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 14:11:19,764 EPOCH 1271
2024-02-01 14:11:33,593 Epoch 1271: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 14:11:33,594 EPOCH 1272
2024-02-01 14:11:47,377 Epoch 1272: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 14:11:47,377 EPOCH 1273
2024-02-01 14:12:01,354 Epoch 1273: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 14:12:01,354 EPOCH 1274
2024-02-01 14:12:15,381 Epoch 1274: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 14:12:15,382 EPOCH 1275
2024-02-01 14:12:29,249 Epoch 1275: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 14:12:29,249 EPOCH 1276
2024-02-01 14:12:43,156 Epoch 1276: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 14:12:43,156 EPOCH 1277
2024-02-01 14:12:57,075 Epoch 1277: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 14:12:57,076 EPOCH 1278
2024-02-01 14:13:04,264 [Epoch: 1278 Step: 00011500] Batch Recognition Loss:   0.000131 => Gls Tokens per Sec:     1247 || Batch Translation Loss:   0.013904 => Txt Tokens per Sec:     3346 || Lr: 0.000100
2024-02-01 14:13:10,587 Epoch 1278: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 14:13:10,587 EPOCH 1279
2024-02-01 14:13:24,405 Epoch 1279: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 14:13:24,406 EPOCH 1280
2024-02-01 14:13:38,164 Epoch 1280: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 14:13:38,165 EPOCH 1281
2024-02-01 14:13:52,113 Epoch 1281: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 14:13:52,114 EPOCH 1282
2024-02-01 14:14:06,034 Epoch 1282: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-01 14:14:06,034 EPOCH 1283
2024-02-01 14:14:19,868 Epoch 1283: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-01 14:14:19,869 EPOCH 1284
2024-02-01 14:14:33,676 Epoch 1284: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 14:14:33,676 EPOCH 1285
2024-02-01 14:14:47,520 Epoch 1285: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 14:14:47,520 EPOCH 1286
2024-02-01 14:15:01,670 Epoch 1286: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 14:15:01,671 EPOCH 1287
2024-02-01 14:15:15,472 Epoch 1287: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 14:15:15,473 EPOCH 1288
2024-02-01 14:15:29,188 Epoch 1288: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-01 14:15:29,189 EPOCH 1289
2024-02-01 14:15:39,455 [Epoch: 1289 Step: 00011600] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:      911 || Batch Translation Loss:   0.076654 => Txt Tokens per Sec:     2438 || Lr: 0.000100
2024-02-01 14:15:43,054 Epoch 1289: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-01 14:15:43,055 EPOCH 1290
2024-02-01 14:15:56,871 Epoch 1290: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-01 14:15:56,871 EPOCH 1291
2024-02-01 14:16:10,769 Epoch 1291: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-01 14:16:10,770 EPOCH 1292
2024-02-01 14:16:24,690 Epoch 1292: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-01 14:16:24,691 EPOCH 1293
2024-02-01 14:16:38,773 Epoch 1293: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 14:16:38,773 EPOCH 1294
2024-02-01 14:16:52,778 Epoch 1294: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 14:16:52,778 EPOCH 1295
2024-02-01 14:17:06,653 Epoch 1295: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-01 14:17:06,654 EPOCH 1296
2024-02-01 14:17:20,999 Epoch 1296: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 14:17:21,000 EPOCH 1297
2024-02-01 14:17:34,625 Epoch 1297: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 14:17:34,625 EPOCH 1298
2024-02-01 14:17:49,011 Epoch 1298: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 14:17:49,012 EPOCH 1299
2024-02-01 14:18:02,783 Epoch 1299: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 14:18:02,784 EPOCH 1300
2024-02-01 14:18:16,640 [Epoch: 1300 Step: 00011700] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:      767 || Batch Translation Loss:   0.020966 => Txt Tokens per Sec:     2130 || Lr: 0.000100
2024-02-01 14:18:16,640 Epoch 1300: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 14:18:16,641 EPOCH 1301
2024-02-01 14:18:30,471 Epoch 1301: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 14:18:30,471 EPOCH 1302
2024-02-01 14:18:44,235 Epoch 1302: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 14:18:44,236 EPOCH 1303
2024-02-01 14:18:58,333 Epoch 1303: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 14:18:58,333 EPOCH 1304
2024-02-01 14:19:12,074 Epoch 1304: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 14:19:12,074 EPOCH 1305
2024-02-01 14:19:26,009 Epoch 1305: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 14:19:26,010 EPOCH 1306
2024-02-01 14:19:39,451 Epoch 1306: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 14:19:39,452 EPOCH 1307
2024-02-01 14:19:53,351 Epoch 1307: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 14:19:53,351 EPOCH 1308
2024-02-01 14:20:07,266 Epoch 1308: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 14:20:07,266 EPOCH 1309
2024-02-01 14:20:20,723 Epoch 1309: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 14:20:20,724 EPOCH 1310
2024-02-01 14:20:34,886 Epoch 1310: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 14:20:34,887 EPOCH 1311
2024-02-01 14:20:49,026 Epoch 1311: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 14:20:49,027 EPOCH 1312
2024-02-01 14:20:50,639 [Epoch: 1312 Step: 00011800] Batch Recognition Loss:   0.000178 => Gls Tokens per Sec:      795 || Batch Translation Loss:   0.031671 => Txt Tokens per Sec:     2385 || Lr: 0.000100
2024-02-01 14:21:03,031 Epoch 1312: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 14:21:03,032 EPOCH 1313
2024-02-01 14:21:16,910 Epoch 1313: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 14:21:16,911 EPOCH 1314
2024-02-01 14:21:30,869 Epoch 1314: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 14:21:30,869 EPOCH 1315
2024-02-01 14:21:44,872 Epoch 1315: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 14:21:44,873 EPOCH 1316
2024-02-01 14:21:58,454 Epoch 1316: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 14:21:58,454 EPOCH 1317
2024-02-01 14:22:12,454 Epoch 1317: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 14:22:12,455 EPOCH 1318
2024-02-01 14:22:26,577 Epoch 1318: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 14:22:26,577 EPOCH 1319
2024-02-01 14:22:40,385 Epoch 1319: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 14:22:40,385 EPOCH 1320
2024-02-01 14:22:54,303 Epoch 1320: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 14:22:54,303 EPOCH 1321
2024-02-01 14:23:08,217 Epoch 1321: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 14:23:08,218 EPOCH 1322
2024-02-01 14:23:22,534 Epoch 1322: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 14:23:22,535 EPOCH 1323
2024-02-01 14:23:25,973 [Epoch: 1323 Step: 00011900] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:      745 || Batch Translation Loss:   0.015677 => Txt Tokens per Sec:     2205 || Lr: 0.000100
2024-02-01 14:23:36,206 Epoch 1323: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 14:23:36,206 EPOCH 1324
2024-02-01 14:23:49,941 Epoch 1324: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 14:23:49,942 EPOCH 1325
2024-02-01 14:24:03,646 Epoch 1325: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 14:24:03,647 EPOCH 1326
2024-02-01 14:24:17,419 Epoch 1326: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 14:24:17,420 EPOCH 1327
2024-02-01 14:24:31,504 Epoch 1327: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 14:24:31,504 EPOCH 1328
2024-02-01 14:24:45,137 Epoch 1328: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 14:24:45,138 EPOCH 1329
2024-02-01 14:24:59,134 Epoch 1329: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 14:24:59,135 EPOCH 1330
2024-02-01 14:25:13,504 Epoch 1330: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 14:25:13,505 EPOCH 1331
2024-02-01 14:25:27,585 Epoch 1331: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-01 14:25:27,586 EPOCH 1332
2024-02-01 14:25:41,598 Epoch 1332: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-01 14:25:41,599 EPOCH 1333
2024-02-01 14:25:55,511 Epoch 1333: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-01 14:25:55,512 EPOCH 1334
2024-02-01 14:26:03,690 [Epoch: 1334 Step: 00012000] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:      361 || Batch Translation Loss:   0.030633 => Txt Tokens per Sec:     1142 || Lr: 0.000100
2024-02-01 14:26:23,370 Validation result at epoch 1334, step    12000: duration: 19.6792s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00016	Translation Loss: 91902.53125	PPL: 9864.42871
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 1.07	(BLEU-1: 10.93,	BLEU-2: 3.80,	BLEU-3: 1.87,	BLEU-4: 1.07)
	CHRF 17.41	ROUGE 9.55
2024-02-01 14:26:23,372 Logging Recognition and Translation Outputs
2024-02-01 14:26:23,372 ========================================================================================================================
2024-02-01 14:26:23,372 Logging Sequence: 177_50.00
2024-02-01 14:26:23,372 	Gloss Reference :	A B+C+D+E
2024-02-01 14:26:23,372 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 14:26:23,372 	Gloss Alignment :	         
2024-02-01 14:26:23,372 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 14:26:23,374 	Text Reference  :	a similar reward of    rs    50000  was announced    for    information against his associate ajay   kumar
2024-02-01 14:26:23,374 	Text Hypothesis :	* after   the    delhi court issued a   non-bailable arrest warrant     against *** ********* sushil kumar
2024-02-01 14:26:23,374 	Text Alignment  :	D S       S      S     S     S      S   S            S      S                   D   D         S           
2024-02-01 14:26:23,374 ========================================================================================================================
2024-02-01 14:26:23,374 Logging Sequence: 122_86.00
2024-02-01 14:26:23,375 	Gloss Reference :	A B+C+D+E
2024-02-01 14:26:23,375 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 14:26:23,375 	Gloss Alignment :	         
2024-02-01 14:26:23,375 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 14:26:23,376 	Text Reference  :	after winning chanu spoke to  the media and said  
2024-02-01 14:26:23,376 	Text Hypothesis :	after ******* ***** india won the 1st   odi series
2024-02-01 14:26:23,376 	Text Alignment  :	      D       D     S     S       S     S   S     
2024-02-01 14:26:23,376 ========================================================================================================================
2024-02-01 14:26:23,376 Logging Sequence: 165_27.00
2024-02-01 14:26:23,376 	Gloss Reference :	A B+C+D+E
2024-02-01 14:26:23,376 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 14:26:23,376 	Gloss Alignment :	         
2024-02-01 14:26:23,377 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 14:26:23,377 	Text Reference  :	so then they change their routes some people believe in   this while some don't
2024-02-01 14:26:23,377 	Text Hypothesis :	** **** **** ****** ***** ****** many teams  will    have been from  6    balls
2024-02-01 14:26:23,378 	Text Alignment  :	D  D    D    D      D     D      S    S      S       S    S    S     S    S    
2024-02-01 14:26:23,378 ========================================================================================================================
2024-02-01 14:26:23,378 Logging Sequence: 70_65.00
2024-02-01 14:26:23,378 	Gloss Reference :	A B+C+D+E
2024-02-01 14:26:23,378 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 14:26:23,378 	Gloss Alignment :	         
2024-02-01 14:26:23,378 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 14:26:23,379 	Text Reference  :	during the press conference a    table was placed in        front of the ***** media   
2024-02-01 14:26:23,379 	Text Hypothesis :	****** the ***** ********** euro 2020  has been   postponed due   to the covid pandemic
2024-02-01 14:26:23,380 	Text Alignment  :	D          D     D          S    S     S   S      S         S     S      I     S       
2024-02-01 14:26:23,380 ========================================================================================================================
2024-02-01 14:26:23,380 Logging Sequence: 149_65.00
2024-02-01 14:26:23,380 	Gloss Reference :	A B+C+D+E
2024-02-01 14:26:23,380 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 14:26:23,380 	Gloss Alignment :	         
2024-02-01 14:26:23,380 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 14:26:23,382 	Text Reference  :	at 6am on 6th november 2022  the police reached sri  lankan   team's hotel  in          sydney australia's central business district cbd  
2024-02-01 14:26:23,382 	Text Hypothesis :	** *** ** *** ******** after the woman  alleged that danushka had    sexual intercourse with   her         without any      sporting arena
2024-02-01 14:26:23,382 	Text Alignment  :	D  D   D  D   D        S         S      S       S    S        S      S      S           S      S           S       S        S        S    
2024-02-01 14:26:23,382 ========================================================================================================================
2024-02-01 14:26:29,206 Epoch 1334: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-01 14:26:29,206 EPOCH 1335
2024-02-01 14:26:42,946 Epoch 1335: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-01 14:26:42,946 EPOCH 1336
2024-02-01 14:26:56,570 Epoch 1336: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-01 14:26:56,571 EPOCH 1337
2024-02-01 14:27:10,642 Epoch 1337: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 14:27:10,642 EPOCH 1338
2024-02-01 14:27:24,595 Epoch 1338: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 14:27:24,595 EPOCH 1339
2024-02-01 14:27:38,645 Epoch 1339: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 14:27:38,645 EPOCH 1340
2024-02-01 14:27:52,343 Epoch 1340: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 14:27:52,343 EPOCH 1341
2024-02-01 14:28:06,365 Epoch 1341: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 14:28:06,365 EPOCH 1342
2024-02-01 14:28:19,952 Epoch 1342: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 14:28:19,952 EPOCH 1343
2024-02-01 14:28:34,017 Epoch 1343: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 14:28:34,017 EPOCH 1344
2024-02-01 14:28:47,634 Epoch 1344: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 14:28:47,635 EPOCH 1345
2024-02-01 14:28:55,579 [Epoch: 1345 Step: 00012100] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:      533 || Batch Translation Loss:   0.034802 => Txt Tokens per Sec:     1498 || Lr: 0.000100
2024-02-01 14:29:01,785 Epoch 1345: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 14:29:01,785 EPOCH 1346
2024-02-01 14:29:15,660 Epoch 1346: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 14:29:15,660 EPOCH 1347
2024-02-01 14:29:29,531 Epoch 1347: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 14:29:29,532 EPOCH 1348
2024-02-01 14:29:43,546 Epoch 1348: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 14:29:43,546 EPOCH 1349
2024-02-01 14:29:57,344 Epoch 1349: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 14:29:57,345 EPOCH 1350
2024-02-01 14:30:10,971 Epoch 1350: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 14:30:10,971 EPOCH 1351
2024-02-01 14:30:25,022 Epoch 1351: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 14:30:25,022 EPOCH 1352
2024-02-01 14:30:38,926 Epoch 1352: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 14:30:38,926 EPOCH 1353
2024-02-01 14:30:52,811 Epoch 1353: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 14:30:52,811 EPOCH 1354
2024-02-01 14:31:06,898 Epoch 1354: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 14:31:06,898 EPOCH 1355
2024-02-01 14:31:20,741 Epoch 1355: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 14:31:20,741 EPOCH 1356
2024-02-01 14:31:30,242 [Epoch: 1356 Step: 00012200] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:      580 || Batch Translation Loss:   0.014803 => Txt Tokens per Sec:     1708 || Lr: 0.000100
2024-02-01 14:31:34,569 Epoch 1356: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 14:31:34,569 EPOCH 1357
2024-02-01 14:31:48,488 Epoch 1357: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 14:31:48,489 EPOCH 1358
2024-02-01 14:32:02,194 Epoch 1358: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 14:32:02,195 EPOCH 1359
2024-02-01 14:32:16,286 Epoch 1359: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 14:32:16,287 EPOCH 1360
2024-02-01 14:32:30,183 Epoch 1360: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 14:32:30,184 EPOCH 1361
2024-02-01 14:32:44,161 Epoch 1361: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 14:32:44,162 EPOCH 1362
2024-02-01 14:32:57,949 Epoch 1362: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 14:32:57,950 EPOCH 1363
2024-02-01 14:33:11,756 Epoch 1363: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 14:33:11,757 EPOCH 1364
2024-02-01 14:33:25,512 Epoch 1364: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 14:33:25,513 EPOCH 1365
2024-02-01 14:33:39,389 Epoch 1365: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 14:33:39,389 EPOCH 1366
2024-02-01 14:33:53,464 Epoch 1366: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-01 14:33:53,464 EPOCH 1367
2024-02-01 14:34:00,243 [Epoch: 1367 Step: 00012300] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:     1133 || Batch Translation Loss:   0.049966 => Txt Tokens per Sec:     3074 || Lr: 0.000100
2024-02-01 14:34:07,338 Epoch 1367: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 14:34:07,339 EPOCH 1368
2024-02-01 14:34:21,158 Epoch 1368: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-01 14:34:21,159 EPOCH 1369
2024-02-01 14:34:35,206 Epoch 1369: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-01 14:34:35,206 EPOCH 1370
2024-02-01 14:34:49,271 Epoch 1370: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 14:34:49,272 EPOCH 1371
2024-02-01 14:35:03,076 Epoch 1371: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-01 14:35:03,076 EPOCH 1372
2024-02-01 14:35:16,909 Epoch 1372: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 14:35:16,910 EPOCH 1373
2024-02-01 14:35:30,951 Epoch 1373: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 14:35:30,951 EPOCH 1374
2024-02-01 14:35:44,765 Epoch 1374: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 14:35:44,765 EPOCH 1375
2024-02-01 14:35:58,707 Epoch 1375: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-01 14:35:58,707 EPOCH 1376
2024-02-01 14:36:12,505 Epoch 1376: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-01 14:36:12,506 EPOCH 1377
2024-02-01 14:36:26,646 Epoch 1377: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-01 14:36:26,647 EPOCH 1378
2024-02-01 14:36:39,559 [Epoch: 1378 Step: 00012400] Batch Recognition Loss:   0.000323 => Gls Tokens per Sec:      625 || Batch Translation Loss:   0.050614 => Txt Tokens per Sec:     1749 || Lr: 0.000100
2024-02-01 14:36:40,540 Epoch 1378: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-01 14:36:40,540 EPOCH 1379
2024-02-01 14:36:54,450 Epoch 1379: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-01 14:36:54,451 EPOCH 1380
2024-02-01 14:37:08,082 Epoch 1380: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-01 14:37:08,083 EPOCH 1381
2024-02-01 14:37:22,114 Epoch 1381: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 14:37:22,115 EPOCH 1382
2024-02-01 14:37:35,978 Epoch 1382: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 14:37:35,978 EPOCH 1383
2024-02-01 14:37:49,722 Epoch 1383: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 14:37:49,723 EPOCH 1384
2024-02-01 14:38:03,849 Epoch 1384: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 14:38:03,850 EPOCH 1385
2024-02-01 14:38:17,891 Epoch 1385: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 14:38:17,891 EPOCH 1386
2024-02-01 14:38:32,059 Epoch 1386: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 14:38:32,060 EPOCH 1387
2024-02-01 14:38:45,817 Epoch 1387: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 14:38:45,818 EPOCH 1388
2024-02-01 14:38:59,889 Epoch 1388: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 14:38:59,889 EPOCH 1389
2024-02-01 14:39:13,150 [Epoch: 1389 Step: 00012500] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:      705 || Batch Translation Loss:   0.043963 => Txt Tokens per Sec:     2004 || Lr: 0.000100
2024-02-01 14:39:13,600 Epoch 1389: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 14:39:13,600 EPOCH 1390
2024-02-01 14:39:27,359 Epoch 1390: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-01 14:39:27,359 EPOCH 1391
2024-02-01 14:39:41,364 Epoch 1391: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-01 14:39:41,365 EPOCH 1392
2024-02-01 14:39:55,378 Epoch 1392: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 14:39:55,379 EPOCH 1393
2024-02-01 14:40:09,284 Epoch 1393: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-01 14:40:09,284 EPOCH 1394
2024-02-01 14:40:23,200 Epoch 1394: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-01 14:40:23,200 EPOCH 1395
2024-02-01 14:40:37,275 Epoch 1395: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 14:40:37,276 EPOCH 1396
2024-02-01 14:40:51,051 Epoch 1396: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 14:40:51,051 EPOCH 1397
2024-02-01 14:41:04,841 Epoch 1397: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 14:41:04,842 EPOCH 1398
2024-02-01 14:41:18,845 Epoch 1398: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 14:41:18,846 EPOCH 1399
2024-02-01 14:41:32,802 Epoch 1399: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 14:41:32,802 EPOCH 1400
2024-02-01 14:41:46,957 [Epoch: 1400 Step: 00012600] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:      751 || Batch Translation Loss:   0.064725 => Txt Tokens per Sec:     2085 || Lr: 0.000100
2024-02-01 14:41:46,957 Epoch 1400: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-01 14:41:46,957 EPOCH 1401
2024-02-01 14:42:00,778 Epoch 1401: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 14:42:00,778 EPOCH 1402
2024-02-01 14:42:14,833 Epoch 1402: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 14:42:14,833 EPOCH 1403
2024-02-01 14:42:28,760 Epoch 1403: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 14:42:28,761 EPOCH 1404
2024-02-01 14:42:42,665 Epoch 1404: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 14:42:42,666 EPOCH 1405
2024-02-01 14:42:56,673 Epoch 1405: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 14:42:56,674 EPOCH 1406
2024-02-01 14:43:10,829 Epoch 1406: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 14:43:10,829 EPOCH 1407
2024-02-01 14:43:24,735 Epoch 1407: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-01 14:43:24,736 EPOCH 1408
2024-02-01 14:43:38,670 Epoch 1408: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-01 14:43:38,670 EPOCH 1409
2024-02-01 14:43:52,743 Epoch 1409: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-01 14:43:52,743 EPOCH 1410
2024-02-01 14:44:06,633 Epoch 1410: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-01 14:44:06,633 EPOCH 1411
2024-02-01 14:44:20,371 Epoch 1411: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.33 
2024-02-01 14:44:20,371 EPOCH 1412
2024-02-01 14:44:20,761 [Epoch: 1412 Step: 00012700] Batch Recognition Loss:   0.000415 => Gls Tokens per Sec:     3290 || Batch Translation Loss:   0.050357 => Txt Tokens per Sec:     7589 || Lr: 0.000100
2024-02-01 14:44:34,311 Epoch 1412: Total Training Recognition Loss 0.00  Total Training Translation Loss 2.45 
2024-02-01 14:44:34,312 EPOCH 1413
2024-02-01 14:44:48,311 Epoch 1413: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.00 
2024-02-01 14:44:48,311 EPOCH 1414
2024-02-01 14:45:02,184 Epoch 1414: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.71 
2024-02-01 14:45:02,184 EPOCH 1415
2024-02-01 14:45:15,832 Epoch 1415: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.71 
2024-02-01 14:45:15,833 EPOCH 1416
2024-02-01 14:45:29,998 Epoch 1416: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.70 
2024-02-01 14:45:29,998 EPOCH 1417
2024-02-01 14:45:43,722 Epoch 1417: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.78 
2024-02-01 14:45:43,723 EPOCH 1418
2024-02-01 14:45:57,266 Epoch 1418: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.50 
2024-02-01 14:45:57,266 EPOCH 1419
2024-02-01 14:46:11,161 Epoch 1419: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.99 
2024-02-01 14:46:11,161 EPOCH 1420
2024-02-01 14:46:25,002 Epoch 1420: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-01 14:46:25,003 EPOCH 1421
2024-02-01 14:46:38,857 Epoch 1421: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-01 14:46:38,858 EPOCH 1422
2024-02-01 14:46:52,867 Epoch 1422: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-01 14:46:52,868 EPOCH 1423
2024-02-01 14:46:56,413 [Epoch: 1423 Step: 00012800] Batch Recognition Loss:   0.000372 => Gls Tokens per Sec:      722 || Batch Translation Loss:   0.045223 => Txt Tokens per Sec:     2059 || Lr: 0.000100
2024-02-01 14:47:06,757 Epoch 1423: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-01 14:47:06,758 EPOCH 1424
2024-02-01 14:47:21,121 Epoch 1424: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-01 14:47:21,122 EPOCH 1425
2024-02-01 14:47:34,932 Epoch 1425: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 14:47:34,933 EPOCH 1426
2024-02-01 14:47:49,121 Epoch 1426: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 14:47:49,122 EPOCH 1427
2024-02-01 14:48:02,902 Epoch 1427: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 14:48:02,902 EPOCH 1428
2024-02-01 14:48:16,867 Epoch 1428: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 14:48:16,868 EPOCH 1429
2024-02-01 14:48:30,771 Epoch 1429: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 14:48:30,772 EPOCH 1430
2024-02-01 14:48:44,653 Epoch 1430: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 14:48:44,653 EPOCH 1431
2024-02-01 14:48:58,744 Epoch 1431: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 14:48:58,744 EPOCH 1432
2024-02-01 14:49:12,745 Epoch 1432: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 14:49:12,746 EPOCH 1433
2024-02-01 14:49:26,632 Epoch 1433: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 14:49:26,633 EPOCH 1434
2024-02-01 14:49:30,515 [Epoch: 1434 Step: 00012900] Batch Recognition Loss:   0.000113 => Gls Tokens per Sec:      760 || Batch Translation Loss:   0.013972 => Txt Tokens per Sec:     1852 || Lr: 0.000100
2024-02-01 14:49:40,369 Epoch 1434: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 14:49:40,369 EPOCH 1435
2024-02-01 14:49:54,482 Epoch 1435: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 14:49:54,482 EPOCH 1436
2024-02-01 14:50:08,481 Epoch 1436: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 14:50:08,481 EPOCH 1437
2024-02-01 14:50:22,085 Epoch 1437: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 14:50:22,085 EPOCH 1438
2024-02-01 14:50:36,097 Epoch 1438: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 14:50:36,097 EPOCH 1439
2024-02-01 14:50:50,039 Epoch 1439: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 14:50:50,040 EPOCH 1440
2024-02-01 14:51:03,667 Epoch 1440: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 14:51:03,667 EPOCH 1441
2024-02-01 14:51:17,162 Epoch 1441: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 14:51:17,162 EPOCH 1442
2024-02-01 14:51:31,030 Epoch 1442: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 14:51:31,031 EPOCH 1443
2024-02-01 14:51:44,969 Epoch 1443: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 14:51:44,970 EPOCH 1444
2024-02-01 14:51:58,582 Epoch 1444: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 14:51:58,583 EPOCH 1445
2024-02-01 14:52:06,035 [Epoch: 1445 Step: 00013000] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:      687 || Batch Translation Loss:   0.017197 => Txt Tokens per Sec:     1920 || Lr: 0.000100
2024-02-01 14:52:12,448 Epoch 1445: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 14:52:12,448 EPOCH 1446
2024-02-01 14:52:26,425 Epoch 1446: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 14:52:26,425 EPOCH 1447
2024-02-01 14:52:40,351 Epoch 1447: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 14:52:40,351 EPOCH 1448
2024-02-01 14:52:54,238 Epoch 1448: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 14:52:54,238 EPOCH 1449
2024-02-01 14:53:08,309 Epoch 1449: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 14:53:08,309 EPOCH 1450
2024-02-01 14:53:21,971 Epoch 1450: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 14:53:21,972 EPOCH 1451
2024-02-01 14:53:35,853 Epoch 1451: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 14:53:35,854 EPOCH 1452
2024-02-01 14:53:49,826 Epoch 1452: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 14:53:49,827 EPOCH 1453
2024-02-01 14:54:03,569 Epoch 1453: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 14:54:03,569 EPOCH 1454
2024-02-01 14:54:17,184 Epoch 1454: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 14:54:17,185 EPOCH 1455
2024-02-01 14:54:31,094 Epoch 1455: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 14:54:31,094 EPOCH 1456
2024-02-01 14:54:37,535 [Epoch: 1456 Step: 00013100] Batch Recognition Loss:   0.000106 => Gls Tokens per Sec:      856 || Batch Translation Loss:   0.007740 => Txt Tokens per Sec:     2440 || Lr: 0.000100
2024-02-01 14:54:45,058 Epoch 1456: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 14:54:45,059 EPOCH 1457
2024-02-01 14:54:59,291 Epoch 1457: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 14:54:59,292 EPOCH 1458
2024-02-01 14:55:13,193 Epoch 1458: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 14:55:13,194 EPOCH 1459
2024-02-01 14:55:26,999 Epoch 1459: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 14:55:27,000 EPOCH 1460
2024-02-01 14:55:40,905 Epoch 1460: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 14:55:40,906 EPOCH 1461
2024-02-01 14:55:54,849 Epoch 1461: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 14:55:54,850 EPOCH 1462
2024-02-01 14:56:08,919 Epoch 1462: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 14:56:08,919 EPOCH 1463
2024-02-01 14:56:22,891 Epoch 1463: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 14:56:22,891 EPOCH 1464
2024-02-01 14:56:37,108 Epoch 1464: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 14:56:37,108 EPOCH 1465
2024-02-01 14:56:50,922 Epoch 1465: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 14:56:50,923 EPOCH 1466
2024-02-01 14:57:04,638 Epoch 1466: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 14:57:04,639 EPOCH 1467
2024-02-01 14:57:12,777 [Epoch: 1467 Step: 00013200] Batch Recognition Loss:   0.000120 => Gls Tokens per Sec:      835 || Batch Translation Loss:   0.018573 => Txt Tokens per Sec:     2317 || Lr: 0.000100
2024-02-01 14:57:18,673 Epoch 1467: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 14:57:18,674 EPOCH 1468
2024-02-01 14:57:32,490 Epoch 1468: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 14:57:32,491 EPOCH 1469
2024-02-01 14:57:46,327 Epoch 1469: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 14:57:46,328 EPOCH 1470
2024-02-01 14:58:00,257 Epoch 1470: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 14:58:00,258 EPOCH 1471
2024-02-01 14:58:14,099 Epoch 1471: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 14:58:14,100 EPOCH 1472
2024-02-01 14:58:27,789 Epoch 1472: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 14:58:27,790 EPOCH 1473
2024-02-01 14:58:41,794 Epoch 1473: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 14:58:41,795 EPOCH 1474
2024-02-01 14:58:55,488 Epoch 1474: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 14:58:55,488 EPOCH 1475
2024-02-01 14:59:09,705 Epoch 1475: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 14:59:09,706 EPOCH 1476
2024-02-01 14:59:23,736 Epoch 1476: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 14:59:23,737 EPOCH 1477
2024-02-01 14:59:37,742 Epoch 1477: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-01 14:59:37,742 EPOCH 1478
2024-02-01 14:59:47,981 [Epoch: 1478 Step: 00013300] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:      875 || Batch Translation Loss:   0.067464 => Txt Tokens per Sec:     2524 || Lr: 0.000100
2024-02-01 14:59:51,593 Epoch 1478: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-01 14:59:51,593 EPOCH 1479
2024-02-01 15:00:05,710 Epoch 1479: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-01 15:00:05,710 EPOCH 1480
2024-02-01 15:00:19,475 Epoch 1480: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.85 
2024-02-01 15:00:19,476 EPOCH 1481
2024-02-01 15:00:33,754 Epoch 1481: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-01 15:00:33,755 EPOCH 1482
2024-02-01 15:00:47,521 Epoch 1482: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-01 15:00:47,521 EPOCH 1483
2024-02-01 15:01:01,522 Epoch 1483: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-01 15:01:01,522 EPOCH 1484
2024-02-01 15:01:15,475 Epoch 1484: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-01 15:01:15,476 EPOCH 1485
2024-02-01 15:01:29,211 Epoch 1485: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 15:01:29,211 EPOCH 1486
2024-02-01 15:01:43,290 Epoch 1486: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 15:01:43,291 EPOCH 1487
2024-02-01 15:01:57,322 Epoch 1487: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-01 15:01:57,322 EPOCH 1488
2024-02-01 15:02:11,538 Epoch 1488: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-01 15:02:11,538 EPOCH 1489
2024-02-01 15:02:24,947 [Epoch: 1489 Step: 00013400] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:      697 || Batch Translation Loss:   0.053868 => Txt Tokens per Sec:     1921 || Lr: 0.000100
2024-02-01 15:02:25,641 Epoch 1489: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 15:02:25,642 EPOCH 1490
2024-02-01 15:02:39,416 Epoch 1490: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 15:02:39,417 EPOCH 1491
2024-02-01 15:02:53,594 Epoch 1491: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-01 15:02:53,595 EPOCH 1492
2024-02-01 15:03:07,420 Epoch 1492: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 15:03:07,420 EPOCH 1493
2024-02-01 15:03:21,585 Epoch 1493: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-01 15:03:21,585 EPOCH 1494
2024-02-01 15:03:35,586 Epoch 1494: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-01 15:03:35,587 EPOCH 1495
2024-02-01 15:03:49,112 Epoch 1495: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-01 15:03:49,113 EPOCH 1496
2024-02-01 15:04:03,055 Epoch 1496: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.99 
2024-02-01 15:04:03,056 EPOCH 1497
2024-02-01 15:04:16,841 Epoch 1497: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.80 
2024-02-01 15:04:16,841 EPOCH 1498
2024-02-01 15:04:30,981 Epoch 1498: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-01 15:04:30,982 EPOCH 1499
2024-02-01 15:04:44,616 Epoch 1499: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-01 15:04:44,617 EPOCH 1500
2024-02-01 15:04:58,608 [Epoch: 1500 Step: 00013500] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:      760 || Batch Translation Loss:   0.025946 => Txt Tokens per Sec:     2109 || Lr: 0.000100
2024-02-01 15:04:58,609 Epoch 1500: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 15:04:58,609 EPOCH 1501
2024-02-01 15:05:12,581 Epoch 1501: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-01 15:05:12,581 EPOCH 1502
2024-02-01 15:05:26,419 Epoch 1502: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-01 15:05:26,419 EPOCH 1503
2024-02-01 15:05:40,243 Epoch 1503: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-01 15:05:40,244 EPOCH 1504
2024-02-01 15:05:54,176 Epoch 1504: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-01 15:05:54,176 EPOCH 1505
2024-02-01 15:06:07,947 Epoch 1505: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-01 15:06:07,947 EPOCH 1506
2024-02-01 15:06:21,932 Epoch 1506: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-01 15:06:21,933 EPOCH 1507
2024-02-01 15:06:35,715 Epoch 1507: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-01 15:06:35,715 EPOCH 1508
2024-02-01 15:06:49,660 Epoch 1508: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-01 15:06:49,661 EPOCH 1509
2024-02-01 15:07:03,340 Epoch 1509: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 15:07:03,340 EPOCH 1510
2024-02-01 15:07:17,045 Epoch 1510: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 15:07:17,045 EPOCH 1511
2024-02-01 15:07:30,887 Epoch 1511: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 15:07:30,888 EPOCH 1512
2024-02-01 15:07:32,491 [Epoch: 1512 Step: 00013600] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:      799 || Batch Translation Loss:   0.042119 => Txt Tokens per Sec:     2413 || Lr: 0.000100
2024-02-01 15:07:44,300 Epoch 1512: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 15:07:44,301 EPOCH 1513
2024-02-01 15:07:58,717 Epoch 1513: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 15:07:58,717 EPOCH 1514
2024-02-01 15:08:12,396 Epoch 1514: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.71 
2024-02-01 15:08:12,397 EPOCH 1515
2024-02-01 15:08:26,301 Epoch 1515: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-01 15:08:26,302 EPOCH 1516
2024-02-01 15:08:40,283 Epoch 1516: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-01 15:08:40,284 EPOCH 1517
2024-02-01 15:08:54,142 Epoch 1517: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.97 
2024-02-01 15:08:54,143 EPOCH 1518
2024-02-01 15:09:08,024 Epoch 1518: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.64 
2024-02-01 15:09:08,025 EPOCH 1519
2024-02-01 15:09:22,032 Epoch 1519: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.94 
2024-02-01 15:09:22,032 EPOCH 1520
2024-02-01 15:09:36,198 Epoch 1520: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-01 15:09:36,198 EPOCH 1521
2024-02-01 15:09:50,095 Epoch 1521: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-01 15:09:50,096 EPOCH 1522
2024-02-01 15:10:04,259 Epoch 1522: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-01 15:10:04,260 EPOCH 1523
2024-02-01 15:10:04,942 [Epoch: 1523 Step: 00013700] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:     3765 || Batch Translation Loss:   0.038019 => Txt Tokens per Sec:     7953 || Lr: 0.000100
2024-02-01 15:10:18,087 Epoch 1523: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-01 15:10:18,088 EPOCH 1524
2024-02-01 15:10:32,138 Epoch 1524: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 15:10:32,139 EPOCH 1525
2024-02-01 15:10:45,986 Epoch 1525: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 15:10:45,987 EPOCH 1526
2024-02-01 15:10:59,993 Epoch 1526: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 15:10:59,993 EPOCH 1527
2024-02-01 15:11:14,124 Epoch 1527: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 15:11:14,124 EPOCH 1528
2024-02-01 15:11:27,933 Epoch 1528: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 15:11:27,933 EPOCH 1529
2024-02-01 15:11:41,968 Epoch 1529: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 15:11:41,969 EPOCH 1530
2024-02-01 15:11:55,716 Epoch 1530: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 15:11:55,716 EPOCH 1531
2024-02-01 15:12:09,788 Epoch 1531: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 15:12:09,789 EPOCH 1532
2024-02-01 15:12:23,667 Epoch 1532: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 15:12:23,667 EPOCH 1533
2024-02-01 15:12:37,736 Epoch 1533: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 15:12:37,736 EPOCH 1534
2024-02-01 15:12:41,924 [Epoch: 1534 Step: 00013800] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:      705 || Batch Translation Loss:   0.029931 => Txt Tokens per Sec:     1936 || Lr: 0.000100
2024-02-01 15:12:51,600 Epoch 1534: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 15:12:51,601 EPOCH 1535
2024-02-01 15:13:05,721 Epoch 1535: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 15:13:05,721 EPOCH 1536
2024-02-01 15:13:19,564 Epoch 1536: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 15:13:19,565 EPOCH 1537
2024-02-01 15:13:33,429 Epoch 1537: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 15:13:33,429 EPOCH 1538
2024-02-01 15:13:47,361 Epoch 1538: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 15:13:47,361 EPOCH 1539
2024-02-01 15:14:01,136 Epoch 1539: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 15:14:01,137 EPOCH 1540
2024-02-01 15:14:15,099 Epoch 1540: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 15:14:15,100 EPOCH 1541
2024-02-01 15:14:29,040 Epoch 1541: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 15:14:29,041 EPOCH 1542
2024-02-01 15:14:42,876 Epoch 1542: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 15:14:42,876 EPOCH 1543
2024-02-01 15:14:56,843 Epoch 1543: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 15:14:56,844 EPOCH 1544
2024-02-01 15:15:10,997 Epoch 1544: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 15:15:10,998 EPOCH 1545
2024-02-01 15:15:13,924 [Epoch: 1545 Step: 00013900] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:     1750 || Batch Translation Loss:   0.025644 => Txt Tokens per Sec:     4576 || Lr: 0.000100
2024-02-01 15:15:24,976 Epoch 1545: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 15:15:24,976 EPOCH 1546
2024-02-01 15:15:38,923 Epoch 1546: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 15:15:38,923 EPOCH 1547
2024-02-01 15:15:52,561 Epoch 1547: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 15:15:52,561 EPOCH 1548
2024-02-01 15:16:06,390 Epoch 1548: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 15:16:06,391 EPOCH 1549
2024-02-01 15:16:20,578 Epoch 1549: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 15:16:20,578 EPOCH 1550
2024-02-01 15:16:34,610 Epoch 1550: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-01 15:16:34,611 EPOCH 1551
2024-02-01 15:16:48,448 Epoch 1551: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-01 15:16:48,448 EPOCH 1552
2024-02-01 15:17:02,584 Epoch 1552: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.84 
2024-02-01 15:17:02,584 EPOCH 1553
2024-02-01 15:17:16,381 Epoch 1553: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-01 15:17:16,382 EPOCH 1554
2024-02-01 15:17:30,250 Epoch 1554: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-01 15:17:30,251 EPOCH 1555
2024-02-01 15:17:44,110 Epoch 1555: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-01 15:17:44,110 EPOCH 1556
2024-02-01 15:17:51,747 [Epoch: 1556 Step: 00014000] Batch Recognition Loss:   0.000385 => Gls Tokens per Sec:      722 || Batch Translation Loss:   0.067654 => Txt Tokens per Sec:     1924 || Lr: 0.000100
2024-02-01 15:18:11,047 Validation result at epoch 1556, step    14000: duration: 19.3006s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00020	Translation Loss: 93516.82812	PPL: 11593.88867
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.81	(BLEU-1: 11.20,	BLEU-2: 3.73,	BLEU-3: 1.55,	BLEU-4: 0.81)
	CHRF 17.51	ROUGE 9.82
2024-02-01 15:18:11,048 Logging Recognition and Translation Outputs
2024-02-01 15:18:11,048 ========================================================================================================================
2024-02-01 15:18:11,048 Logging Sequence: 141_40.00
2024-02-01 15:18:11,049 	Gloss Reference :	A B+C+D+E
2024-02-01 15:18:11,049 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 15:18:11,049 	Gloss Alignment :	         
2024-02-01 15:18:11,049 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 15:18:11,052 	Text Reference  :	got infected with covid-19 he    was quarantined and   could     not take    part  in ******* ****** the warmup match
2024-02-01 15:18:11,052 	Text Hypothesis :	he  won      a    gold     medal in  javelin     throw including an  olympic games in mirabai called me  tell   you  
2024-02-01 15:18:11,052 	Text Alignment  :	S   S        S    S        S     S   S           S     S         S   S       S        I       I      S   S      S    
2024-02-01 15:18:11,052 ========================================================================================================================
2024-02-01 15:18:11,052 Logging Sequence: 117_37.00
2024-02-01 15:18:11,052 	Gloss Reference :	A B+C+D+E
2024-02-01 15:18:11,053 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 15:18:11,053 	Gloss Alignment :	         
2024-02-01 15:18:11,053 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 15:18:11,054 	Text Reference  :	****** shikhar dhawan put  up   a      wonderful performance scoring 98     runs
2024-02-01 15:18:11,054 	Text Hypothesis :	during the     match  ajaz took played very      well        and     scored 3175
2024-02-01 15:18:11,054 	Text Alignment  :	I      S       S      S    S    S      S         S           S       S      S   
2024-02-01 15:18:11,054 ========================================================================================================================
2024-02-01 15:18:11,054 Logging Sequence: 64_13.00
2024-02-01 15:18:11,054 	Gloss Reference :	A B+C+D+E
2024-02-01 15:18:11,054 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 15:18:11,054 	Gloss Alignment :	         
2024-02-01 15:18:11,055 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 15:18:11,056 	Text Reference  :	arrangements were made to  move    all the **** ipl matches to the wankhede stadium in     mumbai  
2024-02-01 15:18:11,056 	Text Hypothesis :	************ **** for  the players won the toss and decided to *** ******** bowl    indian athletes
2024-02-01 15:18:11,056 	Text Alignment  :	D            D    S    S   S       S       I    S   S          D   D        S       S      S       
2024-02-01 15:18:11,056 ========================================================================================================================
2024-02-01 15:18:11,056 Logging Sequence: 98_121.00
2024-02-01 15:18:11,056 	Gloss Reference :	A B+C+D+E
2024-02-01 15:18:11,057 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 15:18:11,057 	Gloss Alignment :	         
2024-02-01 15:18:11,057 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 15:18:11,058 	Text Reference  :	** **** so      then england legends    and bangladesh legends were added to the  tournament
2024-02-01 15:18:11,058 	Text Hypothesis :	we were present at   the     tournament was played     between 3    days  to take place     
2024-02-01 15:18:11,058 	Text Alignment  :	I  I    S       S    S       S          S   S          S       S    S        S    S         
2024-02-01 15:18:11,058 ========================================================================================================================
2024-02-01 15:18:11,058 Logging Sequence: 179_414.00
2024-02-01 15:18:11,058 	Gloss Reference :	A B+C+D+E
2024-02-01 15:18:11,059 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 15:18:11,060 	Gloss Alignment :	         
2024-02-01 15:18:11,060 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 15:18:11,060 	Text Reference  :	we could not travel to delhi as there was a lockdown in     our home town     haryana
2024-02-01 15:18:11,060 	Text Hypothesis :	** ***** *** ****** ** ***** ** ***** *** * ******** phogat got her  schedule changed
2024-02-01 15:18:11,061 	Text Alignment  :	D  D     D   D      D  D     D  D     D   D D        S      S   S    S        S      
2024-02-01 15:18:11,061 ========================================================================================================================
2024-02-01 15:18:17,296 Epoch 1556: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-01 15:18:17,297 EPOCH 1557
2024-02-01 15:18:31,206 Epoch 1557: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 15:18:31,206 EPOCH 1558
2024-02-01 15:18:45,027 Epoch 1558: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-01 15:18:45,028 EPOCH 1559
2024-02-01 15:18:59,192 Epoch 1559: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-01 15:18:59,192 EPOCH 1560
2024-02-01 15:19:13,127 Epoch 1560: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 15:19:13,128 EPOCH 1561
2024-02-01 15:19:26,872 Epoch 1561: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 15:19:26,873 EPOCH 1562
2024-02-01 15:19:41,352 Epoch 1562: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-01 15:19:41,353 EPOCH 1563
2024-02-01 15:19:55,454 Epoch 1563: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 15:19:55,455 EPOCH 1564
2024-02-01 15:20:09,465 Epoch 1564: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.81 
2024-02-01 15:20:09,466 EPOCH 1565
2024-02-01 15:20:23,056 Epoch 1565: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-01 15:20:23,057 EPOCH 1566
2024-02-01 15:20:36,568 Epoch 1566: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-01 15:20:36,569 EPOCH 1567
2024-02-01 15:20:42,303 [Epoch: 1567 Step: 00014100] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     1340 || Batch Translation Loss:   0.042615 => Txt Tokens per Sec:     3508 || Lr: 0.000100
2024-02-01 15:20:50,435 Epoch 1567: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.96 
2024-02-01 15:20:50,435 EPOCH 1568
2024-02-01 15:21:04,365 Epoch 1568: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.16 
2024-02-01 15:21:04,366 EPOCH 1569
2024-02-01 15:21:18,432 Epoch 1569: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.06 
2024-02-01 15:21:18,432 EPOCH 1570
2024-02-01 15:21:32,226 Epoch 1570: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.87 
2024-02-01 15:21:32,226 EPOCH 1571
2024-02-01 15:21:46,073 Epoch 1571: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-01 15:21:46,074 EPOCH 1572
2024-02-01 15:22:00,114 Epoch 1572: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-01 15:22:00,114 EPOCH 1573
2024-02-01 15:22:13,941 Epoch 1573: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-01 15:22:13,942 EPOCH 1574
2024-02-01 15:22:27,788 Epoch 1574: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-01 15:22:27,789 EPOCH 1575
2024-02-01 15:22:41,978 Epoch 1575: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 15:22:41,978 EPOCH 1576
2024-02-01 15:22:56,247 Epoch 1576: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 15:22:56,248 EPOCH 1577
2024-02-01 15:23:10,398 Epoch 1577: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 15:23:10,398 EPOCH 1578
2024-02-01 15:23:20,907 [Epoch: 1578 Step: 00014200] Batch Recognition Loss:   0.000265 => Gls Tokens per Sec:      853 || Batch Translation Loss:   0.033344 => Txt Tokens per Sec:     2396 || Lr: 0.000100
2024-02-01 15:23:24,518 Epoch 1578: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 15:23:24,518 EPOCH 1579
2024-02-01 15:23:38,924 Epoch 1579: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 15:23:38,925 EPOCH 1580
2024-02-01 15:23:52,799 Epoch 1580: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 15:23:52,800 EPOCH 1581
2024-02-01 15:24:06,401 Epoch 1581: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 15:24:06,402 EPOCH 1582
2024-02-01 15:24:20,422 Epoch 1582: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 15:24:20,423 EPOCH 1583
2024-02-01 15:24:34,431 Epoch 1583: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 15:24:34,431 EPOCH 1584
2024-02-01 15:24:48,341 Epoch 1584: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 15:24:48,342 EPOCH 1585
2024-02-01 15:25:02,547 Epoch 1585: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 15:25:02,548 EPOCH 1586
2024-02-01 15:25:16,060 Epoch 1586: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 15:25:16,061 EPOCH 1587
2024-02-01 15:25:29,807 Epoch 1587: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 15:25:29,807 EPOCH 1588
2024-02-01 15:25:43,649 Epoch 1588: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 15:25:43,649 EPOCH 1589
2024-02-01 15:25:56,883 [Epoch: 1589 Step: 00014300] Batch Recognition Loss:   0.000111 => Gls Tokens per Sec:      707 || Batch Translation Loss:   0.019288 => Txt Tokens per Sec:     1949 || Lr: 0.000100
2024-02-01 15:25:57,499 Epoch 1589: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 15:25:57,499 EPOCH 1590
2024-02-01 15:26:10,970 Epoch 1590: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 15:26:10,971 EPOCH 1591
2024-02-01 15:26:25,219 Epoch 1591: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 15:26:25,220 EPOCH 1592
2024-02-01 15:26:39,198 Epoch 1592: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 15:26:39,199 EPOCH 1593
2024-02-01 15:26:53,298 Epoch 1593: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 15:26:53,298 EPOCH 1594
2024-02-01 15:27:07,241 Epoch 1594: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 15:27:07,241 EPOCH 1595
2024-02-01 15:27:21,273 Epoch 1595: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 15:27:21,274 EPOCH 1596
2024-02-01 15:27:35,052 Epoch 1596: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 15:27:35,053 EPOCH 1597
2024-02-01 15:27:48,992 Epoch 1597: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 15:27:48,992 EPOCH 1598
2024-02-01 15:28:03,058 Epoch 1598: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 15:28:03,058 EPOCH 1599
2024-02-01 15:28:16,741 Epoch 1599: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 15:28:16,742 EPOCH 1600
2024-02-01 15:28:30,663 [Epoch: 1600 Step: 00014400] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:      764 || Batch Translation Loss:   0.023988 => Txt Tokens per Sec:     2120 || Lr: 0.000100
2024-02-01 15:28:30,663 Epoch 1600: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 15:28:30,664 EPOCH 1601
2024-02-01 15:28:44,758 Epoch 1601: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 15:28:44,759 EPOCH 1602
2024-02-01 15:28:58,824 Epoch 1602: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 15:28:58,824 EPOCH 1603
2024-02-01 15:29:12,611 Epoch 1603: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 15:29:12,611 EPOCH 1604
2024-02-01 15:29:26,442 Epoch 1604: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 15:29:26,443 EPOCH 1605
2024-02-01 15:29:40,500 Epoch 1605: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 15:29:40,501 EPOCH 1606
2024-02-01 15:29:54,278 Epoch 1606: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 15:29:54,278 EPOCH 1607
2024-02-01 15:30:08,176 Epoch 1607: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 15:30:08,177 EPOCH 1608
2024-02-01 15:30:21,952 Epoch 1608: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 15:30:21,953 EPOCH 1609
2024-02-01 15:30:35,733 Epoch 1609: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 15:30:35,734 EPOCH 1610
2024-02-01 15:30:49,698 Epoch 1610: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 15:30:49,699 EPOCH 1611
2024-02-01 15:31:03,676 Epoch 1611: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 15:31:03,677 EPOCH 1612
2024-02-01 15:31:06,943 [Epoch: 1612 Step: 00014500] Batch Recognition Loss:   0.000125 => Gls Tokens per Sec:      119 || Batch Translation Loss:   0.006852 => Txt Tokens per Sec:      429 || Lr: 0.000100
2024-02-01 15:31:17,350 Epoch 1612: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 15:31:17,351 EPOCH 1613
2024-02-01 15:31:30,897 Epoch 1613: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 15:31:30,897 EPOCH 1614
2024-02-01 15:31:44,859 Epoch 1614: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 15:31:44,860 EPOCH 1615
2024-02-01 15:31:59,001 Epoch 1615: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 15:31:59,002 EPOCH 1616
2024-02-01 15:32:12,677 Epoch 1616: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 15:32:12,678 EPOCH 1617
2024-02-01 15:32:26,585 Epoch 1617: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 15:32:26,586 EPOCH 1618
2024-02-01 15:32:40,680 Epoch 1618: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 15:32:40,681 EPOCH 1619
2024-02-01 15:32:54,469 Epoch 1619: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 15:32:54,470 EPOCH 1620
2024-02-01 15:33:08,812 Epoch 1620: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 15:33:08,812 EPOCH 1621
2024-02-01 15:33:22,701 Epoch 1621: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 15:33:22,701 EPOCH 1622
2024-02-01 15:33:36,643 Epoch 1622: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 15:33:36,643 EPOCH 1623
2024-02-01 15:33:40,464 [Epoch: 1623 Step: 00014600] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:      670 || Batch Translation Loss:   0.030791 => Txt Tokens per Sec:     1765 || Lr: 0.000100
2024-02-01 15:33:50,404 Epoch 1623: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 15:33:50,404 EPOCH 1624
2024-02-01 15:34:04,332 Epoch 1624: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 15:34:04,333 EPOCH 1625
2024-02-01 15:34:17,605 Epoch 1625: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 15:34:17,605 EPOCH 1626
2024-02-01 15:34:31,728 Epoch 1626: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 15:34:31,728 EPOCH 1627
2024-02-01 15:34:45,923 Epoch 1627: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 15:34:45,924 EPOCH 1628
2024-02-01 15:34:59,818 Epoch 1628: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-01 15:34:59,819 EPOCH 1629
2024-02-01 15:35:13,864 Epoch 1629: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-01 15:35:13,864 EPOCH 1630
2024-02-01 15:35:27,867 Epoch 1630: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-01 15:35:27,868 EPOCH 1631
2024-02-01 15:35:41,617 Epoch 1631: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.18 
2024-02-01 15:35:41,617 EPOCH 1632
2024-02-01 15:35:55,485 Epoch 1632: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.15 
2024-02-01 15:35:55,486 EPOCH 1633
2024-02-01 15:36:09,509 Epoch 1633: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.85 
2024-02-01 15:36:09,510 EPOCH 1634
2024-02-01 15:36:13,839 [Epoch: 1634 Step: 00014700] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:      887 || Batch Translation Loss:   0.095660 => Txt Tokens per Sec:     2235 || Lr: 0.000100
2024-02-01 15:36:23,462 Epoch 1634: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.14 
2024-02-01 15:36:23,462 EPOCH 1635
2024-02-01 15:36:37,276 Epoch 1635: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.95 
2024-02-01 15:36:37,276 EPOCH 1636
2024-02-01 15:36:51,268 Epoch 1636: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.48 
2024-02-01 15:36:51,269 EPOCH 1637
2024-02-01 15:37:05,309 Epoch 1637: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.53 
2024-02-01 15:37:05,310 EPOCH 1638
2024-02-01 15:37:19,095 Epoch 1638: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.73 
2024-02-01 15:37:19,095 EPOCH 1639
2024-02-01 15:37:32,779 Epoch 1639: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.50 
2024-02-01 15:37:32,780 EPOCH 1640
2024-02-01 15:37:46,921 Epoch 1640: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-01 15:37:46,921 EPOCH 1641
2024-02-01 15:38:00,723 Epoch 1641: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-01 15:38:00,724 EPOCH 1642
2024-02-01 15:38:14,436 Epoch 1642: Total Training Recognition Loss 0.00  Total Training Translation Loss 2.08 
2024-02-01 15:38:14,437 EPOCH 1643
2024-02-01 15:38:28,626 Epoch 1643: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.95 
2024-02-01 15:38:28,626 EPOCH 1644
2024-02-01 15:38:42,598 Epoch 1644: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.50 
2024-02-01 15:38:42,599 EPOCH 1645
2024-02-01 15:38:50,621 [Epoch: 1645 Step: 00014800] Batch Recognition Loss:   0.001789 => Gls Tokens per Sec:      527 || Batch Translation Loss:   0.249592 => Txt Tokens per Sec:     1568 || Lr: 0.000100
2024-02-01 15:38:56,477 Epoch 1645: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.39 
2024-02-01 15:38:56,477 EPOCH 1646
2024-02-01 15:39:10,329 Epoch 1646: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-01 15:39:10,329 EPOCH 1647
2024-02-01 15:39:24,164 Epoch 1647: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-01 15:39:24,165 EPOCH 1648
2024-02-01 15:39:38,149 Epoch 1648: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-01 15:39:38,149 EPOCH 1649
2024-02-01 15:39:51,908 Epoch 1649: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-01 15:39:51,909 EPOCH 1650
2024-02-01 15:40:05,688 Epoch 1650: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 15:40:05,688 EPOCH 1651
2024-02-01 15:40:19,701 Epoch 1651: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 15:40:19,701 EPOCH 1652
2024-02-01 15:40:33,790 Epoch 1652: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 15:40:33,791 EPOCH 1653
2024-02-01 15:40:47,681 Epoch 1653: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 15:40:47,681 EPOCH 1654
2024-02-01 15:41:01,750 Epoch 1654: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 15:41:01,750 EPOCH 1655
2024-02-01 15:41:15,696 Epoch 1655: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 15:41:15,697 EPOCH 1656
2024-02-01 15:41:23,428 [Epoch: 1656 Step: 00014900] Batch Recognition Loss:   0.000285 => Gls Tokens per Sec:      828 || Batch Translation Loss:   0.033602 => Txt Tokens per Sec:     2360 || Lr: 0.000100
2024-02-01 15:41:29,548 Epoch 1656: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 15:41:29,548 EPOCH 1657
2024-02-01 15:41:43,506 Epoch 1657: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-01 15:41:43,507 EPOCH 1658
2024-02-01 15:41:57,448 Epoch 1658: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 15:41:57,448 EPOCH 1659
2024-02-01 15:42:11,360 Epoch 1659: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-01 15:42:11,361 EPOCH 1660
2024-02-01 15:42:25,144 Epoch 1660: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 15:42:25,145 EPOCH 1661
2024-02-01 15:42:39,097 Epoch 1661: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 15:42:39,097 EPOCH 1662
2024-02-01 15:42:53,056 Epoch 1662: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 15:42:53,057 EPOCH 1663
2024-02-01 15:43:07,216 Epoch 1663: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 15:43:07,217 EPOCH 1664
2024-02-01 15:43:21,089 Epoch 1664: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 15:43:21,090 EPOCH 1665
2024-02-01 15:43:34,885 Epoch 1665: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 15:43:34,886 EPOCH 1666
2024-02-01 15:43:49,088 Epoch 1666: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 15:43:49,089 EPOCH 1667
2024-02-01 15:44:00,862 [Epoch: 1667 Step: 00015000] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:      577 || Batch Translation Loss:   0.026388 => Txt Tokens per Sec:     1734 || Lr: 0.000100
2024-02-01 15:44:03,277 Epoch 1667: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 15:44:03,278 EPOCH 1668
2024-02-01 15:44:17,196 Epoch 1668: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 15:44:17,197 EPOCH 1669
2024-02-01 15:44:31,341 Epoch 1669: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 15:44:31,342 EPOCH 1670
2024-02-01 15:44:45,355 Epoch 1670: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 15:44:45,356 EPOCH 1671
2024-02-01 15:44:59,232 Epoch 1671: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 15:44:59,233 EPOCH 1672
2024-02-01 15:45:13,190 Epoch 1672: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 15:45:13,191 EPOCH 1673
2024-02-01 15:45:27,555 Epoch 1673: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 15:45:27,556 EPOCH 1674
2024-02-01 15:45:41,451 Epoch 1674: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 15:45:41,452 EPOCH 1675
2024-02-01 15:45:55,460 Epoch 1675: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 15:45:55,461 EPOCH 1676
2024-02-01 15:46:09,407 Epoch 1676: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 15:46:09,408 EPOCH 1677
2024-02-01 15:46:23,496 Epoch 1677: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 15:46:23,497 EPOCH 1678
2024-02-01 15:46:33,784 [Epoch: 1678 Step: 00015100] Batch Recognition Loss:   0.000163 => Gls Tokens per Sec:      785 || Batch Translation Loss:   0.027825 => Txt Tokens per Sec:     2132 || Lr: 0.000100
2024-02-01 15:46:37,376 Epoch 1678: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 15:46:37,377 EPOCH 1679
2024-02-01 15:46:51,178 Epoch 1679: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 15:46:51,178 EPOCH 1680
2024-02-01 15:47:05,078 Epoch 1680: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 15:47:05,079 EPOCH 1681
2024-02-01 15:47:18,939 Epoch 1681: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 15:47:18,940 EPOCH 1682
2024-02-01 15:47:32,899 Epoch 1682: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 15:47:32,899 EPOCH 1683
2024-02-01 15:47:46,671 Epoch 1683: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 15:47:46,672 EPOCH 1684
2024-02-01 15:48:00,697 Epoch 1684: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 15:48:00,698 EPOCH 1685
2024-02-01 15:48:14,624 Epoch 1685: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 15:48:14,624 EPOCH 1686
2024-02-01 15:48:28,518 Epoch 1686: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 15:48:28,519 EPOCH 1687
2024-02-01 15:48:42,554 Epoch 1687: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 15:48:42,555 EPOCH 1688
2024-02-01 15:48:56,475 Epoch 1688: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 15:48:56,475 EPOCH 1689
2024-02-01 15:49:07,036 [Epoch: 1689 Step: 00015200] Batch Recognition Loss:   0.000131 => Gls Tokens per Sec:      970 || Batch Translation Loss:   0.011482 => Txt Tokens per Sec:     2662 || Lr: 0.000100
2024-02-01 15:49:10,380 Epoch 1689: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 15:49:10,380 EPOCH 1690
2024-02-01 15:49:24,263 Epoch 1690: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 15:49:24,263 EPOCH 1691
2024-02-01 15:49:38,336 Epoch 1691: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 15:49:38,337 EPOCH 1692
2024-02-01 15:49:52,400 Epoch 1692: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 15:49:52,400 EPOCH 1693
2024-02-01 15:50:06,128 Epoch 1693: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 15:50:06,129 EPOCH 1694
2024-02-01 15:50:20,031 Epoch 1694: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 15:50:20,032 EPOCH 1695
2024-02-01 15:50:34,242 Epoch 1695: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 15:50:34,242 EPOCH 1696
2024-02-01 15:50:48,049 Epoch 1696: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 15:50:48,050 EPOCH 1697
2024-02-01 15:51:02,181 Epoch 1697: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 15:51:02,181 EPOCH 1698
2024-02-01 15:51:16,076 Epoch 1698: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 15:51:16,076 EPOCH 1699
2024-02-01 15:51:29,917 Epoch 1699: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 15:51:29,918 EPOCH 1700
2024-02-01 15:51:43,860 [Epoch: 1700 Step: 00015300] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:      762 || Batch Translation Loss:   0.014129 => Txt Tokens per Sec:     2117 || Lr: 0.000100
2024-02-01 15:51:43,861 Epoch 1700: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 15:51:43,861 EPOCH 1701
2024-02-01 15:51:57,765 Epoch 1701: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 15:51:57,765 EPOCH 1702
2024-02-01 15:52:11,841 Epoch 1702: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 15:52:11,842 EPOCH 1703
2024-02-01 15:52:25,802 Epoch 1703: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 15:52:25,803 EPOCH 1704
2024-02-01 15:52:39,627 Epoch 1704: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 15:52:39,628 EPOCH 1705
2024-02-01 15:52:53,678 Epoch 1705: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 15:52:53,679 EPOCH 1706
2024-02-01 15:53:07,371 Epoch 1706: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 15:53:07,371 EPOCH 1707
2024-02-01 15:53:21,524 Epoch 1707: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 15:53:21,525 EPOCH 1708
2024-02-01 15:53:35,427 Epoch 1708: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 15:53:35,427 EPOCH 1709
2024-02-01 15:53:49,707 Epoch 1709: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 15:53:49,708 EPOCH 1710
2024-02-01 15:54:03,524 Epoch 1710: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 15:54:03,525 EPOCH 1711
2024-02-01 15:54:17,537 Epoch 1711: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-01 15:54:17,538 EPOCH 1712
2024-02-01 15:54:17,848 [Epoch: 1712 Step: 00015400] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:     4129 || Batch Translation Loss:   0.013773 => Txt Tokens per Sec:     9548 || Lr: 0.000100
2024-02-01 15:54:31,590 Epoch 1712: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 15:54:31,591 EPOCH 1713
2024-02-01 15:54:45,517 Epoch 1713: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 15:54:45,518 EPOCH 1714
2024-02-01 15:54:59,397 Epoch 1714: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 15:54:59,397 EPOCH 1715
2024-02-01 15:55:13,298 Epoch 1715: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 15:55:13,299 EPOCH 1716
2024-02-01 15:55:27,470 Epoch 1716: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-01 15:55:27,471 EPOCH 1717
2024-02-01 15:55:41,387 Epoch 1717: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 15:55:41,388 EPOCH 1718
2024-02-01 15:55:55,401 Epoch 1718: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 15:55:55,402 EPOCH 1719
2024-02-01 15:56:09,342 Epoch 1719: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 15:56:09,343 EPOCH 1720
2024-02-01 15:56:23,128 Epoch 1720: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 15:56:23,128 EPOCH 1721
2024-02-01 15:56:36,904 Epoch 1721: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 15:56:36,905 EPOCH 1722
2024-02-01 15:56:51,353 Epoch 1722: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 15:56:51,354 EPOCH 1723
2024-02-01 15:56:54,672 [Epoch: 1723 Step: 00015500] Batch Recognition Loss:   0.000265 => Gls Tokens per Sec:      772 || Batch Translation Loss:   0.032247 => Txt Tokens per Sec:     1926 || Lr: 0.000100
2024-02-01 15:57:05,494 Epoch 1723: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 15:57:05,494 EPOCH 1724
2024-02-01 15:57:19,265 Epoch 1724: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-01 15:57:19,266 EPOCH 1725
2024-02-01 15:57:33,278 Epoch 1725: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.10 
2024-02-01 15:57:33,278 EPOCH 1726
2024-02-01 15:57:47,179 Epoch 1726: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-01 15:57:47,179 EPOCH 1727
2024-02-01 15:58:00,945 Epoch 1727: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.88 
2024-02-01 15:58:00,946 EPOCH 1728
2024-02-01 15:58:14,991 Epoch 1728: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-01 15:58:14,992 EPOCH 1729
2024-02-01 15:58:29,110 Epoch 1729: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.87 
2024-02-01 15:58:29,110 EPOCH 1730
2024-02-01 15:58:43,095 Epoch 1730: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.78 
2024-02-01 15:58:43,095 EPOCH 1731
2024-02-01 15:58:56,821 Epoch 1731: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-01 15:58:56,821 EPOCH 1732
2024-02-01 15:59:10,832 Epoch 1732: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-01 15:59:10,832 EPOCH 1733
2024-02-01 15:59:24,513 Epoch 1733: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-01 15:59:24,514 EPOCH 1734
2024-02-01 15:59:29,760 [Epoch: 1734 Step: 00015600] Batch Recognition Loss:   0.000257 => Gls Tokens per Sec:      732 || Batch Translation Loss:   0.045403 => Txt Tokens per Sec:     2182 || Lr: 0.000100
2024-02-01 15:59:38,498 Epoch 1734: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-01 15:59:38,498 EPOCH 1735
2024-02-01 15:59:52,667 Epoch 1735: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.84 
2024-02-01 15:59:52,667 EPOCH 1736
2024-02-01 16:00:06,611 Epoch 1736: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-01 16:00:06,611 EPOCH 1737
2024-02-01 16:00:20,578 Epoch 1737: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-01 16:00:20,579 EPOCH 1738
2024-02-01 16:00:34,741 Epoch 1738: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-01 16:00:34,742 EPOCH 1739
2024-02-01 16:00:48,594 Epoch 1739: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-01 16:00:48,595 EPOCH 1740
2024-02-01 16:01:02,371 Epoch 1740: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-01 16:01:02,372 EPOCH 1741
2024-02-01 16:01:16,377 Epoch 1741: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 16:01:16,378 EPOCH 1742
2024-02-01 16:01:30,505 Epoch 1742: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 16:01:30,506 EPOCH 1743
2024-02-01 16:01:44,386 Epoch 1743: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 16:01:44,386 EPOCH 1744
2024-02-01 16:01:58,442 Epoch 1744: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 16:01:58,442 EPOCH 1745
2024-02-01 16:02:02,842 [Epoch: 1745 Step: 00015700] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:      962 || Batch Translation Loss:   0.010546 => Txt Tokens per Sec:     2499 || Lr: 0.000100
2024-02-01 16:02:12,264 Epoch 1745: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 16:02:12,265 EPOCH 1746
2024-02-01 16:02:26,289 Epoch 1746: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 16:02:26,289 EPOCH 1747
2024-02-01 16:02:40,246 Epoch 1747: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 16:02:40,247 EPOCH 1748
2024-02-01 16:02:54,293 Epoch 1748: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 16:02:54,294 EPOCH 1749
2024-02-01 16:03:08,236 Epoch 1749: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 16:03:08,236 EPOCH 1750
2024-02-01 16:03:22,015 Epoch 1750: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 16:03:22,015 EPOCH 1751
2024-02-01 16:03:36,201 Epoch 1751: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 16:03:36,202 EPOCH 1752
2024-02-01 16:03:49,707 Epoch 1752: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 16:03:49,707 EPOCH 1753
2024-02-01 16:04:03,793 Epoch 1753: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 16:04:03,794 EPOCH 1754
2024-02-01 16:04:17,726 Epoch 1754: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 16:04:17,726 EPOCH 1755
2024-02-01 16:04:31,702 Epoch 1755: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 16:04:31,702 EPOCH 1756
2024-02-01 16:04:42,936 [Epoch: 1756 Step: 00015800] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:      491 || Batch Translation Loss:   0.023002 => Txt Tokens per Sec:     1498 || Lr: 0.000100
2024-02-01 16:04:45,632 Epoch 1756: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 16:04:45,632 EPOCH 1757
2024-02-01 16:04:59,252 Epoch 1757: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 16:04:59,252 EPOCH 1758
2024-02-01 16:05:13,319 Epoch 1758: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 16:05:13,320 EPOCH 1759
2024-02-01 16:05:27,114 Epoch 1759: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 16:05:27,115 EPOCH 1760
2024-02-01 16:05:40,911 Epoch 1760: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 16:05:40,911 EPOCH 1761
2024-02-01 16:05:54,876 Epoch 1761: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 16:05:54,877 EPOCH 1762
2024-02-01 16:06:08,860 Epoch 1762: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 16:06:08,861 EPOCH 1763
2024-02-01 16:06:22,789 Epoch 1763: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 16:06:22,790 EPOCH 1764
2024-02-01 16:06:36,685 Epoch 1764: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 16:06:36,686 EPOCH 1765
2024-02-01 16:06:50,693 Epoch 1765: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 16:06:50,694 EPOCH 1766
2024-02-01 16:07:04,542 Epoch 1766: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 16:07:04,542 EPOCH 1767
2024-02-01 16:07:11,790 [Epoch: 1767 Step: 00015900] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:     1060 || Batch Translation Loss:   0.007562 => Txt Tokens per Sec:     2835 || Lr: 0.000100
2024-02-01 16:07:18,684 Epoch 1767: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 16:07:18,685 EPOCH 1768
2024-02-01 16:07:32,502 Epoch 1768: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 16:07:32,503 EPOCH 1769
2024-02-01 16:07:46,225 Epoch 1769: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 16:07:46,226 EPOCH 1770
2024-02-01 16:08:00,206 Epoch 1770: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 16:08:00,206 EPOCH 1771
2024-02-01 16:08:14,243 Epoch 1771: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 16:08:14,244 EPOCH 1772
2024-02-01 16:08:28,088 Epoch 1772: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 16:08:28,089 EPOCH 1773
2024-02-01 16:08:41,911 Epoch 1773: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 16:08:41,912 EPOCH 1774
2024-02-01 16:08:55,779 Epoch 1774: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 16:08:55,780 EPOCH 1775
2024-02-01 16:09:09,616 Epoch 1775: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 16:09:09,617 EPOCH 1776
2024-02-01 16:09:23,734 Epoch 1776: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 16:09:23,734 EPOCH 1777
2024-02-01 16:09:37,601 Epoch 1777: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 16:09:37,602 EPOCH 1778
2024-02-01 16:09:50,648 [Epoch: 1778 Step: 00016000] Batch Recognition Loss:   0.000129 => Gls Tokens per Sec:      619 || Batch Translation Loss:   0.023845 => Txt Tokens per Sec:     1817 || Lr: 0.000100
2024-02-01 16:10:10,330 Validation result at epoch 1778, step    16000: duration: 19.6824s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00010	Translation Loss: 94747.35938	PPL: 13113.17676
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 1.03	(BLEU-1: 12.53,	BLEU-2: 4.29,	BLEU-3: 1.94,	BLEU-4: 1.03)
	CHRF 18.05	ROUGE 10.80
2024-02-01 16:10:10,331 Logging Recognition and Translation Outputs
2024-02-01 16:10:10,331 ========================================================================================================================
2024-02-01 16:10:10,332 Logging Sequence: 147_132.00
2024-02-01 16:10:10,332 	Gloss Reference :	A B+C+D+E
2024-02-01 16:10:10,332 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 16:10:10,332 	Gloss Alignment :	         
2024-02-01 16:10:10,332 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 16:10:10,334 	Text Reference  :	** i  can not ***** ** *** **** earlier i      used to    have    fun   in  gymnastics
2024-02-01 16:10:10,334 	Text Hypothesis :	if it is  not known as the same even    during the  match between india and england   
2024-02-01 16:10:10,334 	Text Alignment  :	I  S  S       I     I  I   I    S       S      S    S     S       S     S   S         
2024-02-01 16:10:10,334 ========================================================================================================================
2024-02-01 16:10:10,334 Logging Sequence: 116_162.00
2024-02-01 16:10:10,334 	Gloss Reference :	A B+C+D+E
2024-02-01 16:10:10,334 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 16:10:10,335 	Gloss Alignment :	         
2024-02-01 16:10:10,335 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 16:10:10,336 	Text Reference  :	* ***** *** **** turned  out the    video was  shared on social media by    a staff at the  hotel    
2024-02-01 16:10:10,336 	Text Hypothesis :	a babar fan said 'œthere is  speech but   went viral  on ****** this  means a ***** ** poor household
2024-02-01 16:10:10,338 	Text Alignment  :	I I     I   I    S       S   S      S     S    S         D      S     S       D     D  S    S        
2024-02-01 16:10:10,338 ========================================================================================================================
2024-02-01 16:10:10,338 Logging Sequence: 73_79.00
2024-02-01 16:10:10,339 	Gloss Reference :	A B+C+D+E
2024-02-01 16:10:10,339 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 16:10:10,339 	Gloss Alignment :	         
2024-02-01 16:10:10,339 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 16:10:10,340 	Text Reference  :	raina resturant has food  from the rich spices     of ****** north india to   the aromatic curries of south india  
2024-02-01 16:10:10,340 	Text Hypothesis :	***** ********* *** there were a   very protective of vamika and   never lets the ******** ******* ** ***** country
2024-02-01 16:10:10,340 	Text Alignment  :	D     D         D   S     S    S   S    S             I      S     S     S        D        D       D  D     S      
2024-02-01 16:10:10,341 ========================================================================================================================
2024-02-01 16:10:10,341 Logging Sequence: 165_523.00
2024-02-01 16:10:10,341 	Gloss Reference :	A B+C+D+E
2024-02-01 16:10:10,341 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 16:10:10,341 	Gloss Alignment :	         
2024-02-01 16:10:10,341 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 16:10:10,343 	Text Reference  :	*** **** ********* as     he  believed that his   team    might lose ** if     he takes off his batting pads
2024-02-01 16:10:10,343 	Text Hypothesis :	but then rajasthan royals has won      the  match however now   lose at jantar of just  off *** ******* ****
2024-02-01 16:10:10,343 	Text Alignment  :	I   I    I         S      S   S        S    S     S       S          I  S      S  S         D   D       D   
2024-02-01 16:10:10,343 ========================================================================================================================
2024-02-01 16:10:10,343 Logging Sequence: 125_72.00
2024-02-01 16:10:10,343 	Gloss Reference :	A B+C+D+E
2024-02-01 16:10:10,343 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 16:10:10,344 	Gloss Alignment :	         
2024-02-01 16:10:10,344 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 16:10:10,345 	Text Reference  :	****** some   said the pakistani javelineer had milicious intentions of     tampering with the javelin out of jealousy
2024-02-01 16:10:10,345 	Text Hypothesis :	neeraj chopra at   the ********* ********** *** game      many       people blamed    for  the ******* *** ** finals  
2024-02-01 16:10:10,345 	Text Alignment  :	I      S      S        D         D          D   S         S          S      S         S        D       D   D  S       
2024-02-01 16:10:10,345 ========================================================================================================================
2024-02-01 16:10:11,141 Epoch 1778: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 16:10:11,142 EPOCH 1779
2024-02-01 16:10:24,953 Epoch 1779: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 16:10:24,954 EPOCH 1780
2024-02-01 16:10:38,708 Epoch 1780: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 16:10:38,709 EPOCH 1781
2024-02-01 16:10:52,379 Epoch 1781: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 16:10:52,380 EPOCH 1782
2024-02-01 16:11:06,302 Epoch 1782: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 16:11:06,302 EPOCH 1783
2024-02-01 16:11:20,274 Epoch 1783: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 16:11:20,276 EPOCH 1784
2024-02-01 16:11:33,864 Epoch 1784: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 16:11:33,864 EPOCH 1785
2024-02-01 16:11:47,832 Epoch 1785: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 16:11:47,833 EPOCH 1786
2024-02-01 16:12:01,430 Epoch 1786: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 16:12:01,430 EPOCH 1787
2024-02-01 16:12:15,440 Epoch 1787: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 16:12:15,440 EPOCH 1788
2024-02-01 16:12:29,268 Epoch 1788: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 16:12:29,268 EPOCH 1789
2024-02-01 16:12:42,971 [Epoch: 1789 Step: 00016100] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:      682 || Batch Translation Loss:   0.066549 => Txt Tokens per Sec:     1935 || Lr: 0.000100
2024-02-01 16:12:43,302 Epoch 1789: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-01 16:12:43,302 EPOCH 1790
2024-02-01 16:12:56,778 Epoch 1790: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-01 16:12:56,779 EPOCH 1791
2024-02-01 16:13:10,667 Epoch 1791: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-01 16:13:10,667 EPOCH 1792
2024-02-01 16:13:24,569 Epoch 1792: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-01 16:13:24,569 EPOCH 1793
2024-02-01 16:13:38,330 Epoch 1793: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-01 16:13:38,331 EPOCH 1794
2024-02-01 16:13:52,293 Epoch 1794: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.88 
2024-02-01 16:13:52,293 EPOCH 1795
2024-02-01 16:14:06,434 Epoch 1795: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-01 16:14:06,434 EPOCH 1796
2024-02-01 16:14:20,134 Epoch 1796: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.86 
2024-02-01 16:14:20,134 EPOCH 1797
2024-02-01 16:14:34,146 Epoch 1797: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.83 
2024-02-01 16:14:34,146 EPOCH 1798
2024-02-01 16:14:48,144 Epoch 1798: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.00 
2024-02-01 16:14:48,144 EPOCH 1799
2024-02-01 16:15:02,274 Epoch 1799: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-01 16:15:02,275 EPOCH 1800
2024-02-01 16:15:16,196 [Epoch: 1800 Step: 00016200] Batch Recognition Loss:   0.000383 => Gls Tokens per Sec:      764 || Batch Translation Loss:   0.068545 => Txt Tokens per Sec:     2120 || Lr: 0.000100
2024-02-01 16:15:16,197 Epoch 1800: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-01 16:15:16,197 EPOCH 1801
2024-02-01 16:15:30,170 Epoch 1801: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-01 16:15:30,171 EPOCH 1802
2024-02-01 16:15:44,354 Epoch 1802: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-01 16:15:44,355 EPOCH 1803
2024-02-01 16:15:58,543 Epoch 1803: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-01 16:15:58,543 EPOCH 1804
2024-02-01 16:16:12,073 Epoch 1804: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-01 16:16:12,074 EPOCH 1805
2024-02-01 16:16:26,039 Epoch 1805: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-01 16:16:26,040 EPOCH 1806
2024-02-01 16:16:39,959 Epoch 1806: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-01 16:16:39,959 EPOCH 1807
2024-02-01 16:16:53,773 Epoch 1807: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 16:16:53,773 EPOCH 1808
2024-02-01 16:17:07,645 Epoch 1808: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 16:17:07,645 EPOCH 1809
2024-02-01 16:17:21,487 Epoch 1809: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 16:17:21,487 EPOCH 1810
2024-02-01 16:17:35,555 Epoch 1810: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 16:17:35,555 EPOCH 1811
2024-02-01 16:17:49,713 Epoch 1811: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 16:17:49,714 EPOCH 1812
2024-02-01 16:17:50,402 [Epoch: 1812 Step: 00016300] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     1863 || Batch Translation Loss:   0.032210 => Txt Tokens per Sec:     5492 || Lr: 0.000100
2024-02-01 16:18:03,707 Epoch 1812: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 16:18:03,708 EPOCH 1813
2024-02-01 16:18:17,639 Epoch 1813: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 16:18:17,639 EPOCH 1814
2024-02-01 16:18:31,646 Epoch 1814: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 16:18:31,647 EPOCH 1815
2024-02-01 16:18:45,286 Epoch 1815: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 16:18:45,287 EPOCH 1816
2024-02-01 16:18:59,502 Epoch 1816: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 16:18:59,503 EPOCH 1817
2024-02-01 16:19:13,500 Epoch 1817: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 16:19:13,500 EPOCH 1818
2024-02-01 16:19:27,293 Epoch 1818: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 16:19:27,293 EPOCH 1819
2024-02-01 16:19:41,275 Epoch 1819: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 16:19:41,275 EPOCH 1820
2024-02-01 16:19:55,363 Epoch 1820: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 16:19:55,364 EPOCH 1821
2024-02-01 16:20:09,006 Epoch 1821: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 16:20:09,007 EPOCH 1822
2024-02-01 16:20:23,154 Epoch 1822: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 16:20:23,154 EPOCH 1823
2024-02-01 16:20:27,252 [Epoch: 1823 Step: 00016400] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:      625 || Batch Translation Loss:   0.034532 => Txt Tokens per Sec:     1864 || Lr: 0.000100
2024-02-01 16:20:37,239 Epoch 1823: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 16:20:37,240 EPOCH 1824
2024-02-01 16:20:50,879 Epoch 1824: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 16:20:50,880 EPOCH 1825
2024-02-01 16:21:04,548 Epoch 1825: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 16:21:04,549 EPOCH 1826
2024-02-01 16:21:18,216 Epoch 1826: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 16:21:18,216 EPOCH 1827
2024-02-01 16:21:32,477 Epoch 1827: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 16:21:32,478 EPOCH 1828
2024-02-01 16:21:46,241 Epoch 1828: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 16:21:46,242 EPOCH 1829
2024-02-01 16:22:00,195 Epoch 1829: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 16:22:00,195 EPOCH 1830
2024-02-01 16:22:14,024 Epoch 1830: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 16:22:14,025 EPOCH 1831
2024-02-01 16:22:27,853 Epoch 1831: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 16:22:27,854 EPOCH 1832
2024-02-01 16:22:41,701 Epoch 1832: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 16:22:41,701 EPOCH 1833
2024-02-01 16:22:55,565 Epoch 1833: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 16:22:55,566 EPOCH 1834
2024-02-01 16:22:59,054 [Epoch: 1834 Step: 00016500] Batch Recognition Loss:   0.000112 => Gls Tokens per Sec:     1101 || Batch Translation Loss:   0.012729 => Txt Tokens per Sec:     2657 || Lr: 0.000100
2024-02-01 16:23:09,492 Epoch 1834: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 16:23:09,492 EPOCH 1835
2024-02-01 16:23:23,115 Epoch 1835: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 16:23:23,116 EPOCH 1836
2024-02-01 16:23:37,115 Epoch 1836: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 16:23:37,116 EPOCH 1837
2024-02-01 16:23:51,148 Epoch 1837: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 16:23:51,149 EPOCH 1838
2024-02-01 16:24:05,074 Epoch 1838: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 16:24:05,075 EPOCH 1839
2024-02-01 16:24:18,943 Epoch 1839: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 16:24:18,944 EPOCH 1840
2024-02-01 16:24:33,076 Epoch 1840: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 16:24:33,076 EPOCH 1841
2024-02-01 16:24:47,011 Epoch 1841: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 16:24:47,011 EPOCH 1842
2024-02-01 16:25:00,885 Epoch 1842: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 16:25:00,886 EPOCH 1843
2024-02-01 16:25:14,655 Epoch 1843: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 16:25:14,655 EPOCH 1844
2024-02-01 16:25:28,693 Epoch 1844: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 16:25:28,694 EPOCH 1845
2024-02-01 16:25:35,969 [Epoch: 1845 Step: 00016600] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:      704 || Batch Translation Loss:   0.021152 => Txt Tokens per Sec:     2065 || Lr: 0.000100
2024-02-01 16:25:42,612 Epoch 1845: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 16:25:42,613 EPOCH 1846
2024-02-01 16:25:56,419 Epoch 1846: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 16:25:56,420 EPOCH 1847
2024-02-01 16:26:10,796 Epoch 1847: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 16:26:10,797 EPOCH 1848
2024-02-01 16:26:24,638 Epoch 1848: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 16:26:24,639 EPOCH 1849
2024-02-01 16:26:38,819 Epoch 1849: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 16:26:38,820 EPOCH 1850
2024-02-01 16:26:52,882 Epoch 1850: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 16:26:52,883 EPOCH 1851
2024-02-01 16:27:06,653 Epoch 1851: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 16:27:06,654 EPOCH 1852
2024-02-01 16:27:20,742 Epoch 1852: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-01 16:27:20,742 EPOCH 1853
2024-02-01 16:27:34,850 Epoch 1853: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-01 16:27:34,851 EPOCH 1854
2024-02-01 16:27:48,453 Epoch 1854: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-01 16:27:48,454 EPOCH 1855
2024-02-01 16:28:02,329 Epoch 1855: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-01 16:28:02,329 EPOCH 1856
2024-02-01 16:28:11,897 [Epoch: 1856 Step: 00016700] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:      669 || Batch Translation Loss:   0.085870 => Txt Tokens per Sec:     1998 || Lr: 0.000100
2024-02-01 16:28:16,290 Epoch 1856: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-01 16:28:16,290 EPOCH 1857
2024-02-01 16:28:30,356 Epoch 1857: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.77 
2024-02-01 16:28:30,357 EPOCH 1858
2024-02-01 16:28:44,165 Epoch 1858: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-01 16:28:44,165 EPOCH 1859
2024-02-01 16:28:58,222 Epoch 1859: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.89 
2024-02-01 16:28:58,223 EPOCH 1860
2024-02-01 16:29:12,045 Epoch 1860: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.50 
2024-02-01 16:29:12,045 EPOCH 1861
2024-02-01 16:29:25,827 Epoch 1861: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.33 
2024-02-01 16:29:25,828 EPOCH 1862
2024-02-01 16:29:40,018 Epoch 1862: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.77 
2024-02-01 16:29:40,018 EPOCH 1863
2024-02-01 16:29:54,019 Epoch 1863: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.33 
2024-02-01 16:29:54,019 EPOCH 1864
2024-02-01 16:30:07,605 Epoch 1864: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.11 
2024-02-01 16:30:07,606 EPOCH 1865
2024-02-01 16:30:21,296 Epoch 1865: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.88 
2024-02-01 16:30:21,296 EPOCH 1866
2024-02-01 16:30:35,574 Epoch 1866: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.76 
2024-02-01 16:30:35,574 EPOCH 1867
2024-02-01 16:30:45,108 [Epoch: 1867 Step: 00016800] Batch Recognition Loss:   0.000466 => Gls Tokens per Sec:      712 || Batch Translation Loss:   0.062162 => Txt Tokens per Sec:     2019 || Lr: 0.000100
2024-02-01 16:30:49,609 Epoch 1867: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-01 16:30:49,610 EPOCH 1868
2024-02-01 16:31:03,781 Epoch 1868: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-01 16:31:03,782 EPOCH 1869
2024-02-01 16:31:17,719 Epoch 1869: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-01 16:31:17,719 EPOCH 1870
2024-02-01 16:31:31,507 Epoch 1870: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-01 16:31:31,508 EPOCH 1871
2024-02-01 16:31:45,636 Epoch 1871: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-01 16:31:45,636 EPOCH 1872
2024-02-01 16:31:59,429 Epoch 1872: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-01 16:31:59,430 EPOCH 1873
2024-02-01 16:32:13,078 Epoch 1873: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 16:32:13,078 EPOCH 1874
2024-02-01 16:32:26,829 Epoch 1874: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 16:32:26,830 EPOCH 1875
2024-02-01 16:32:40,642 Epoch 1875: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 16:32:40,642 EPOCH 1876
2024-02-01 16:32:54,843 Epoch 1876: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 16:32:54,844 EPOCH 1877
2024-02-01 16:33:08,885 Epoch 1877: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 16:33:08,885 EPOCH 1878
2024-02-01 16:33:17,367 [Epoch: 1878 Step: 00016900] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:      952 || Batch Translation Loss:   0.025842 => Txt Tokens per Sec:     2537 || Lr: 0.000100
2024-02-01 16:33:22,336 Epoch 1878: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 16:33:22,336 EPOCH 1879
2024-02-01 16:33:36,200 Epoch 1879: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 16:33:36,200 EPOCH 1880
2024-02-01 16:33:50,304 Epoch 1880: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 16:33:50,304 EPOCH 1881
2024-02-01 16:34:04,206 Epoch 1881: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 16:34:04,207 EPOCH 1882
2024-02-01 16:34:18,097 Epoch 1882: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 16:34:18,097 EPOCH 1883
2024-02-01 16:34:31,679 Epoch 1883: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 16:34:31,680 EPOCH 1884
2024-02-01 16:34:45,581 Epoch 1884: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 16:34:45,581 EPOCH 1885
2024-02-01 16:34:59,447 Epoch 1885: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 16:34:59,447 EPOCH 1886
2024-02-01 16:35:13,150 Epoch 1886: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 16:35:13,150 EPOCH 1887
2024-02-01 16:35:27,086 Epoch 1887: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 16:35:27,087 EPOCH 1888
2024-02-01 16:35:40,911 Epoch 1888: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 16:35:40,912 EPOCH 1889
2024-02-01 16:35:51,129 [Epoch: 1889 Step: 00017000] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:      915 || Batch Translation Loss:   0.012697 => Txt Tokens per Sec:     2449 || Lr: 0.000100
2024-02-01 16:35:54,669 Epoch 1889: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 16:35:54,670 EPOCH 1890
2024-02-01 16:36:08,822 Epoch 1890: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 16:36:08,822 EPOCH 1891
2024-02-01 16:36:22,850 Epoch 1891: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 16:36:22,850 EPOCH 1892
2024-02-01 16:36:36,909 Epoch 1892: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 16:36:36,909 EPOCH 1893
2024-02-01 16:36:50,795 Epoch 1893: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 16:36:50,796 EPOCH 1894
2024-02-01 16:37:04,691 Epoch 1894: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 16:37:04,692 EPOCH 1895
2024-02-01 16:37:18,526 Epoch 1895: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 16:37:18,527 EPOCH 1896
2024-02-01 16:37:32,677 Epoch 1896: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 16:37:32,678 EPOCH 1897
2024-02-01 16:37:46,677 Epoch 1897: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 16:37:46,677 EPOCH 1898
2024-02-01 16:38:00,798 Epoch 1898: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 16:38:00,799 EPOCH 1899
2024-02-01 16:38:14,610 Epoch 1899: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 16:38:14,611 EPOCH 1900
2024-02-01 16:38:28,756 [Epoch: 1900 Step: 00017100] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:      752 || Batch Translation Loss:   0.015692 => Txt Tokens per Sec:     2086 || Lr: 0.000100
2024-02-01 16:38:28,757 Epoch 1900: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 16:38:28,757 EPOCH 1901
2024-02-01 16:38:42,244 Epoch 1901: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 16:38:42,245 EPOCH 1902
2024-02-01 16:38:56,209 Epoch 1902: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 16:38:56,210 EPOCH 1903
2024-02-01 16:39:10,088 Epoch 1903: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 16:39:10,089 EPOCH 1904
2024-02-01 16:39:24,022 Epoch 1904: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 16:39:24,022 EPOCH 1905
2024-02-01 16:39:38,360 Epoch 1905: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 16:39:38,361 EPOCH 1906
2024-02-01 16:39:52,169 Epoch 1906: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 16:39:52,170 EPOCH 1907
2024-02-01 16:40:06,015 Epoch 1907: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 16:40:06,016 EPOCH 1908
2024-02-01 16:40:20,092 Epoch 1908: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 16:40:20,093 EPOCH 1909
2024-02-01 16:40:34,134 Epoch 1909: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 16:40:34,134 EPOCH 1910
2024-02-01 16:40:48,029 Epoch 1910: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 16:40:48,030 EPOCH 1911
2024-02-01 16:41:01,961 Epoch 1911: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 16:41:01,962 EPOCH 1912
2024-02-01 16:41:02,232 [Epoch: 1912 Step: 00017200] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:     4741 || Batch Translation Loss:   0.009930 => Txt Tokens per Sec:     8444 || Lr: 0.000100
2024-02-01 16:41:15,905 Epoch 1912: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 16:41:15,906 EPOCH 1913
2024-02-01 16:41:30,142 Epoch 1913: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 16:41:30,143 EPOCH 1914
2024-02-01 16:41:44,410 Epoch 1914: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 16:41:44,411 EPOCH 1915
2024-02-01 16:41:58,256 Epoch 1915: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 16:41:58,257 EPOCH 1916
2024-02-01 16:42:12,363 Epoch 1916: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 16:42:12,363 EPOCH 1917
2024-02-01 16:42:26,348 Epoch 1917: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 16:42:26,349 EPOCH 1918
2024-02-01 16:42:40,237 Epoch 1918: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 16:42:40,237 EPOCH 1919
2024-02-01 16:42:54,080 Epoch 1919: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 16:42:54,081 EPOCH 1920
2024-02-01 16:43:08,309 Epoch 1920: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 16:43:08,310 EPOCH 1921
2024-02-01 16:43:22,169 Epoch 1921: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 16:43:22,170 EPOCH 1922
2024-02-01 16:43:36,003 Epoch 1922: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 16:43:36,004 EPOCH 1923
2024-02-01 16:43:38,387 [Epoch: 1923 Step: 00017300] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:     1075 || Batch Translation Loss:   0.020910 => Txt Tokens per Sec:     3107 || Lr: 0.000100
2024-02-01 16:43:50,220 Epoch 1923: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 16:43:50,220 EPOCH 1924
2024-02-01 16:44:04,219 Epoch 1924: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 16:44:04,220 EPOCH 1925
2024-02-01 16:44:17,848 Epoch 1925: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 16:44:17,849 EPOCH 1926
2024-02-01 16:44:31,920 Epoch 1926: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 16:44:31,921 EPOCH 1927
2024-02-01 16:44:45,855 Epoch 1927: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 16:44:45,855 EPOCH 1928
2024-02-01 16:44:59,717 Epoch 1928: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 16:44:59,717 EPOCH 1929
2024-02-01 16:45:13,484 Epoch 1929: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 16:45:13,484 EPOCH 1930
2024-02-01 16:45:27,282 Epoch 1930: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 16:45:27,283 EPOCH 1931
2024-02-01 16:45:41,403 Epoch 1931: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 16:45:41,404 EPOCH 1932
2024-02-01 16:45:55,432 Epoch 1932: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 16:45:55,433 EPOCH 1933
2024-02-01 16:46:09,383 Epoch 1933: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 16:46:09,384 EPOCH 1934
2024-02-01 16:46:10,654 [Epoch: 1934 Step: 00017400] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:     3024 || Batch Translation Loss:   0.023252 => Txt Tokens per Sec:     7038 || Lr: 0.000100
2024-02-01 16:46:23,232 Epoch 1934: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 16:46:23,232 EPOCH 1935
2024-02-01 16:46:36,726 Epoch 1935: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 16:46:36,726 EPOCH 1936
2024-02-01 16:46:50,652 Epoch 1936: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 16:46:50,652 EPOCH 1937
2024-02-01 16:47:04,732 Epoch 1937: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 16:47:04,732 EPOCH 1938
2024-02-01 16:47:18,776 Epoch 1938: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 16:47:18,777 EPOCH 1939
2024-02-01 16:47:32,379 Epoch 1939: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-01 16:47:32,380 EPOCH 1940
2024-02-01 16:47:46,585 Epoch 1940: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 16:47:46,586 EPOCH 1941
2024-02-01 16:48:00,338 Epoch 1941: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 16:48:00,338 EPOCH 1942
2024-02-01 16:48:14,260 Epoch 1942: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 16:48:14,261 EPOCH 1943
2024-02-01 16:48:28,192 Epoch 1943: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 16:48:28,192 EPOCH 1944
2024-02-01 16:48:42,197 Epoch 1944: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 16:48:42,198 EPOCH 1945
2024-02-01 16:48:50,191 [Epoch: 1945 Step: 00017500] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:      529 || Batch Translation Loss:   0.043858 => Txt Tokens per Sec:     1646 || Lr: 0.000100
2024-02-01 16:48:55,908 Epoch 1945: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 16:48:55,909 EPOCH 1946
2024-02-01 16:49:10,169 Epoch 1946: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 16:49:10,170 EPOCH 1947
2024-02-01 16:49:24,061 Epoch 1947: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-01 16:49:24,062 EPOCH 1948
2024-02-01 16:49:37,865 Epoch 1948: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-01 16:49:37,866 EPOCH 1949
2024-02-01 16:49:51,605 Epoch 1949: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-01 16:49:51,606 EPOCH 1950
2024-02-01 16:50:05,541 Epoch 1950: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-01 16:50:05,541 EPOCH 1951
2024-02-01 16:50:19,554 Epoch 1951: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-01 16:50:19,555 EPOCH 1952
2024-02-01 16:50:33,437 Epoch 1952: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-01 16:50:33,438 EPOCH 1953
2024-02-01 16:50:47,720 Epoch 1953: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.04 
2024-02-01 16:50:47,720 EPOCH 1954
2024-02-01 16:51:01,472 Epoch 1954: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.96 
2024-02-01 16:51:01,472 EPOCH 1955
2024-02-01 16:51:15,205 Epoch 1955: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.15 
2024-02-01 16:51:15,206 EPOCH 1956
2024-02-01 16:51:24,194 [Epoch: 1956 Step: 00017600] Batch Recognition Loss:   0.000887 => Gls Tokens per Sec:      613 || Batch Translation Loss:   0.332408 => Txt Tokens per Sec:     1845 || Lr: 0.000100
2024-02-01 16:51:28,903 Epoch 1956: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.67 
2024-02-01 16:51:28,904 EPOCH 1957
2024-02-01 16:51:42,680 Epoch 1957: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.35 
2024-02-01 16:51:42,680 EPOCH 1958
2024-02-01 16:51:56,583 Epoch 1958: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.14 
2024-02-01 16:51:56,584 EPOCH 1959
2024-02-01 16:52:10,589 Epoch 1959: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.49 
2024-02-01 16:52:10,589 EPOCH 1960
2024-02-01 16:52:24,376 Epoch 1960: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-01 16:52:24,376 EPOCH 1961
2024-02-01 16:52:38,398 Epoch 1961: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.88 
2024-02-01 16:52:38,399 EPOCH 1962
2024-02-01 16:52:52,219 Epoch 1962: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-01 16:52:52,219 EPOCH 1963
2024-02-01 16:53:06,017 Epoch 1963: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-01 16:53:06,017 EPOCH 1964
2024-02-01 16:53:19,859 Epoch 1964: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 16:53:19,860 EPOCH 1965
2024-02-01 16:53:33,445 Epoch 1965: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 16:53:33,446 EPOCH 1966
2024-02-01 16:53:47,582 Epoch 1966: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 16:53:47,582 EPOCH 1967
2024-02-01 16:53:54,423 [Epoch: 1967 Step: 00017700] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:      993 || Batch Translation Loss:   0.027295 => Txt Tokens per Sec:     2594 || Lr: 0.000100
2024-02-01 16:54:01,650 Epoch 1967: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 16:54:01,651 EPOCH 1968
2024-02-01 16:54:15,538 Epoch 1968: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 16:54:15,538 EPOCH 1969
2024-02-01 16:54:29,319 Epoch 1969: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 16:54:29,320 EPOCH 1970
2024-02-01 16:54:43,274 Epoch 1970: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 16:54:43,274 EPOCH 1971
2024-02-01 16:54:57,044 Epoch 1971: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 16:54:57,044 EPOCH 1972
2024-02-01 16:55:10,994 Epoch 1972: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 16:55:10,995 EPOCH 1973
2024-02-01 16:55:24,941 Epoch 1973: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 16:55:24,942 EPOCH 1974
2024-02-01 16:55:39,079 Epoch 1974: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 16:55:39,080 EPOCH 1975
2024-02-01 16:55:53,052 Epoch 1975: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 16:55:53,053 EPOCH 1976
2024-02-01 16:56:06,923 Epoch 1976: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 16:56:06,924 EPOCH 1977
2024-02-01 16:56:20,583 Epoch 1977: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 16:56:20,583 EPOCH 1978
2024-02-01 16:56:29,396 [Epoch: 1978 Step: 00017800] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:     1017 || Batch Translation Loss:   0.014458 => Txt Tokens per Sec:     2750 || Lr: 0.000100
2024-02-01 16:56:34,532 Epoch 1978: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 16:56:34,533 EPOCH 1979
2024-02-01 16:56:48,652 Epoch 1979: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 16:56:48,653 EPOCH 1980
2024-02-01 16:57:02,537 Epoch 1980: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 16:57:02,538 EPOCH 1981
2024-02-01 16:57:16,751 Epoch 1981: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 16:57:16,751 EPOCH 1982
2024-02-01 16:57:30,730 Epoch 1982: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 16:57:30,730 EPOCH 1983
2024-02-01 16:57:44,580 Epoch 1983: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 16:57:44,580 EPOCH 1984
2024-02-01 16:57:58,301 Epoch 1984: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 16:57:58,302 EPOCH 1985
2024-02-01 16:58:12,448 Epoch 1985: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 16:58:12,448 EPOCH 1986
2024-02-01 16:58:26,367 Epoch 1986: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 16:58:26,368 EPOCH 1987
2024-02-01 16:58:40,087 Epoch 1987: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 16:58:40,088 EPOCH 1988
2024-02-01 16:58:54,192 Epoch 1988: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 16:58:54,192 EPOCH 1989
2024-02-01 16:59:07,926 [Epoch: 1989 Step: 00017900] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:      681 || Batch Translation Loss:   0.025128 => Txt Tokens per Sec:     1986 || Lr: 0.000100
2024-02-01 16:59:08,195 Epoch 1989: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 16:59:08,195 EPOCH 1990
2024-02-01 16:59:22,250 Epoch 1990: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 16:59:22,250 EPOCH 1991
2024-02-01 16:59:36,214 Epoch 1991: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 16:59:36,214 EPOCH 1992
2024-02-01 16:59:50,022 Epoch 1992: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 16:59:50,023 EPOCH 1993
2024-02-01 17:00:03,913 Epoch 1993: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 17:00:03,913 EPOCH 1994
2024-02-01 17:00:17,937 Epoch 1994: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 17:00:17,937 EPOCH 1995
2024-02-01 17:00:31,938 Epoch 1995: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 17:00:31,939 EPOCH 1996
2024-02-01 17:00:46,082 Epoch 1996: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 17:00:46,083 EPOCH 1997
2024-02-01 17:00:59,923 Epoch 1997: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 17:00:59,923 EPOCH 1998
2024-02-01 17:01:13,849 Epoch 1998: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 17:01:13,850 EPOCH 1999
2024-02-01 17:01:27,642 Epoch 1999: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 17:01:27,642 EPOCH 2000
2024-02-01 17:01:41,741 [Epoch: 2000 Step: 00018000] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:      754 || Batch Translation Loss:   0.016695 => Txt Tokens per Sec:     2093 || Lr: 0.000100
2024-02-01 17:02:01,295 Validation result at epoch 2000, step    18000: duration: 19.5537s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00011	Translation Loss: 96012.21875	PPL: 14882.58984
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.71	(BLEU-1: 11.47,	BLEU-2: 3.59,	BLEU-3: 1.44,	BLEU-4: 0.71)
	CHRF 17.23	ROUGE 9.90
2024-02-01 17:02:01,296 Logging Recognition and Translation Outputs
2024-02-01 17:02:01,296 ========================================================================================================================
2024-02-01 17:02:01,297 Logging Sequence: 155_119.00
2024-02-01 17:02:01,297 	Gloss Reference :	A B+C+D+E
2024-02-01 17:02:01,297 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 17:02:01,297 	Gloss Alignment :	         
2024-02-01 17:02:01,297 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 17:02:01,299 	Text Reference  :	a report said    that the taliban wanted icc to   replace the afghan     flag with its own     
2024-02-01 17:02:01,299 	Text Hypothesis :	* indian cricket team is  also    given  a   part of      the tournament on   the  tv  channels
2024-02-01 17:02:01,299 	Text Alignment  :	D S      S       S    S   S       S      S   S    S           S          S    S    S   S       
2024-02-01 17:02:01,299 ========================================================================================================================
2024-02-01 17:02:01,299 Logging Sequence: 153_43.00
2024-02-01 17:02:01,299 	Gloss Reference :	A B+C+D+E
2024-02-01 17:02:01,300 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 17:02:01,300 	Gloss Alignment :	         
2024-02-01 17:02:01,300 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 17:02:01,300 	Text Reference  :	these runs were all because of   hardik pandya  and  virat    kohli   
2024-02-01 17:02:01,301 	Text Hypothesis :	***** **** **** *** now     they got    several such baseless comments
2024-02-01 17:02:01,301 	Text Alignment  :	D     D    D    D   S       S    S      S       S    S        S       
2024-02-01 17:02:01,301 ========================================================================================================================
2024-02-01 17:02:01,301 Logging Sequence: 150_35.00
2024-02-01 17:02:01,301 	Gloss Reference :	A B+C+D+E
2024-02-01 17:02:01,301 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 17:02:01,301 	Gloss Alignment :	         
2024-02-01 17:02:01,301 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 17:02:01,302 	Text Reference  :	wow india football team  is   really strong 
2024-02-01 17:02:01,302 	Text Hypothesis :	*** ***** ******** let's wait for    updates
2024-02-01 17:02:01,302 	Text Alignment  :	D   D     D        S     S    S      S      
2024-02-01 17:02:01,302 ========================================================================================================================
2024-02-01 17:02:01,302 Logging Sequence: 146_154.00
2024-02-01 17:02:01,302 	Gloss Reference :	A B+C+D+E
2024-02-01 17:02:01,303 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 17:02:01,303 	Gloss Alignment :	         
2024-02-01 17:02:01,303 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 17:02:01,305 	Text Reference  :	*** bwf   said that testing protocols have been implemented to      ensure  the health    and ** safety of    all participants
2024-02-01 17:02:01,305 	Text Hypothesis :	the ashes is   a    special series    of   test matches     between england and australia and is held   every 2   years       
2024-02-01 17:02:01,305 	Text Alignment  :	I   S     S    S    S       S         S    S    S           S       S       S   S             I  S      S     S   S           
2024-02-01 17:02:01,305 ========================================================================================================================
2024-02-01 17:02:01,305 Logging Sequence: 76_79.00
2024-02-01 17:02:01,305 	Gloss Reference :	A B+C+D+E
2024-02-01 17:02:01,305 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 17:02:01,305 	Gloss Alignment :	         
2024-02-01 17:02:01,306 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 17:02:01,306 	Text Reference  :	** **** ******** **** *** speaking to   ani csk      ceo kasi viswanathan said  
2024-02-01 17:02:01,306 	Text Hypothesis :	on 13th february 2023 the indian   team was supposed to  play against     mumbai
2024-02-01 17:02:01,307 	Text Alignment  :	I  I    I        I    I   S        S    S   S        S   S    S           S     
2024-02-01 17:02:01,307 ========================================================================================================================
2024-02-01 17:02:01,310 Epoch 2000: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 17:02:01,311 EPOCH 2001
2024-02-01 17:02:15,463 Epoch 2001: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 17:02:15,464 EPOCH 2002
2024-02-01 17:02:29,116 Epoch 2002: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 17:02:29,117 EPOCH 2003
2024-02-01 17:02:43,150 Epoch 2003: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 17:02:43,151 EPOCH 2004
2024-02-01 17:02:57,522 Epoch 2004: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 17:02:57,523 EPOCH 2005
2024-02-01 17:03:11,374 Epoch 2005: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 17:03:11,375 EPOCH 2006
2024-02-01 17:03:25,232 Epoch 2006: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 17:03:25,232 EPOCH 2007
2024-02-01 17:03:39,361 Epoch 2007: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 17:03:39,362 EPOCH 2008
2024-02-01 17:03:53,188 Epoch 2008: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 17:03:53,189 EPOCH 2009
2024-02-01 17:04:06,809 Epoch 2009: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 17:04:06,810 EPOCH 2010
2024-02-01 17:04:20,633 Epoch 2010: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 17:04:20,633 EPOCH 2011
2024-02-01 17:04:34,462 Epoch 2011: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 17:04:34,463 EPOCH 2012
2024-02-01 17:04:37,500 [Epoch: 2012 Step: 00018100] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:      421 || Batch Translation Loss:   0.018707 => Txt Tokens per Sec:     1345 || Lr: 0.000100
2024-02-01 17:04:48,228 Epoch 2012: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 17:04:48,229 EPOCH 2013
2024-02-01 17:05:01,964 Epoch 2013: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 17:05:01,965 EPOCH 2014
2024-02-01 17:05:15,960 Epoch 2014: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 17:05:15,960 EPOCH 2015
2024-02-01 17:05:29,874 Epoch 2015: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 17:05:29,875 EPOCH 2016
2024-02-01 17:05:43,977 Epoch 2016: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 17:05:43,977 EPOCH 2017
2024-02-01 17:05:57,878 Epoch 2017: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 17:05:57,878 EPOCH 2018
2024-02-01 17:06:11,926 Epoch 2018: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 17:06:11,927 EPOCH 2019
2024-02-01 17:06:25,937 Epoch 2019: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 17:06:25,937 EPOCH 2020
2024-02-01 17:06:39,808 Epoch 2020: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 17:06:39,809 EPOCH 2021
2024-02-01 17:06:53,722 Epoch 2021: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 17:06:53,723 EPOCH 2022
2024-02-01 17:07:07,637 Epoch 2022: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 17:07:07,638 EPOCH 2023
2024-02-01 17:07:14,172 [Epoch: 2023 Step: 00018200] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:      392 || Batch Translation Loss:   0.021420 => Txt Tokens per Sec:     1311 || Lr: 0.000100
2024-02-01 17:07:21,281 Epoch 2023: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 17:07:21,282 EPOCH 2024
2024-02-01 17:07:35,187 Epoch 2024: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 17:07:35,187 EPOCH 2025
2024-02-01 17:07:48,747 Epoch 2025: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 17:07:48,747 EPOCH 2026
2024-02-01 17:08:03,012 Epoch 2026: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 17:08:03,013 EPOCH 2027
2024-02-01 17:08:16,739 Epoch 2027: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 17:08:16,740 EPOCH 2028
2024-02-01 17:08:30,997 Epoch 2028: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 17:08:30,998 EPOCH 2029
2024-02-01 17:08:44,667 Epoch 2029: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 17:08:44,667 EPOCH 2030
2024-02-01 17:08:58,708 Epoch 2030: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 17:08:58,709 EPOCH 2031
2024-02-01 17:09:12,714 Epoch 2031: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-01 17:09:12,714 EPOCH 2032
2024-02-01 17:09:26,787 Epoch 2032: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.94 
2024-02-01 17:09:26,787 EPOCH 2033
2024-02-01 17:09:40,789 Epoch 2033: Total Training Recognition Loss 0.00  Total Training Translation Loss 2.92 
2024-02-01 17:09:40,790 EPOCH 2034
2024-02-01 17:09:45,153 [Epoch: 2034 Step: 00018300] Batch Recognition Loss:   0.000735 => Gls Tokens per Sec:      676 || Batch Translation Loss:   0.042313 => Txt Tokens per Sec:     1853 || Lr: 0.000100
2024-02-01 17:09:54,715 Epoch 2034: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.93 
2024-02-01 17:09:54,715 EPOCH 2035
2024-02-01 17:10:08,694 Epoch 2035: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.32 
2024-02-01 17:10:08,694 EPOCH 2036
2024-02-01 17:10:22,408 Epoch 2036: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-01 17:10:22,409 EPOCH 2037
2024-02-01 17:10:36,563 Epoch 2037: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.83 
2024-02-01 17:10:36,564 EPOCH 2038
2024-02-01 17:10:50,432 Epoch 2038: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-01 17:10:50,433 EPOCH 2039
2024-02-01 17:11:04,418 Epoch 2039: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-01 17:11:04,419 EPOCH 2040
2024-02-01 17:11:18,147 Epoch 2040: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 17:11:18,148 EPOCH 2041
2024-02-01 17:11:32,127 Epoch 2041: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 17:11:32,128 EPOCH 2042
2024-02-01 17:11:46,075 Epoch 2042: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 17:11:46,076 EPOCH 2043
2024-02-01 17:11:59,899 Epoch 2043: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 17:11:59,900 EPOCH 2044
2024-02-01 17:12:13,898 Epoch 2044: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 17:12:13,899 EPOCH 2045
2024-02-01 17:12:25,803 [Epoch: 2045 Step: 00018400] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:      355 || Batch Translation Loss:   0.010787 => Txt Tokens per Sec:     1164 || Lr: 0.000100
2024-02-01 17:12:27,876 Epoch 2045: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 17:12:27,876 EPOCH 2046
2024-02-01 17:12:41,650 Epoch 2046: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 17:12:41,651 EPOCH 2047
2024-02-01 17:12:55,396 Epoch 2047: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 17:12:55,397 EPOCH 2048
2024-02-01 17:13:09,499 Epoch 2048: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 17:13:09,500 EPOCH 2049
2024-02-01 17:13:23,394 Epoch 2049: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 17:13:23,394 EPOCH 2050
2024-02-01 17:13:37,544 Epoch 2050: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 17:13:37,544 EPOCH 2051
2024-02-01 17:13:51,246 Epoch 2051: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 17:13:51,246 EPOCH 2052
2024-02-01 17:14:05,244 Epoch 2052: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 17:14:05,244 EPOCH 2053
2024-02-01 17:14:19,202 Epoch 2053: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 17:14:19,203 EPOCH 2054
2024-02-01 17:14:32,944 Epoch 2054: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 17:14:32,945 EPOCH 2055
2024-02-01 17:14:46,407 Epoch 2055: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 17:14:46,407 EPOCH 2056
2024-02-01 17:14:54,594 [Epoch: 2056 Step: 00018500] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:      782 || Batch Translation Loss:   0.018087 => Txt Tokens per Sec:     2324 || Lr: 0.000100
2024-02-01 17:15:00,380 Epoch 2056: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 17:15:00,380 EPOCH 2057
2024-02-01 17:15:14,343 Epoch 2057: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 17:15:14,343 EPOCH 2058
2024-02-01 17:15:28,532 Epoch 2058: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 17:15:28,533 EPOCH 2059
2024-02-01 17:15:42,508 Epoch 2059: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 17:15:42,509 EPOCH 2060
2024-02-01 17:15:56,384 Epoch 2060: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 17:15:56,385 EPOCH 2061
2024-02-01 17:16:10,194 Epoch 2061: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 17:16:10,194 EPOCH 2062
2024-02-01 17:16:24,299 Epoch 2062: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 17:16:24,299 EPOCH 2063
2024-02-01 17:16:38,365 Epoch 2063: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 17:16:38,366 EPOCH 2064
2024-02-01 17:16:52,294 Epoch 2064: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 17:16:52,295 EPOCH 2065
2024-02-01 17:17:06,241 Epoch 2065: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 17:17:06,242 EPOCH 2066
2024-02-01 17:17:20,302 Epoch 2066: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 17:17:20,303 EPOCH 2067
2024-02-01 17:17:33,149 [Epoch: 2067 Step: 00018600] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:      529 || Batch Translation Loss:   0.023062 => Txt Tokens per Sec:     1529 || Lr: 0.000100
2024-02-01 17:17:34,566 Epoch 2067: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 17:17:34,567 EPOCH 2068
2024-02-01 17:17:48,365 Epoch 2068: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 17:17:48,365 EPOCH 2069
2024-02-01 17:18:02,191 Epoch 2069: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 17:18:02,191 EPOCH 2070
2024-02-01 17:18:16,472 Epoch 2070: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 17:18:16,472 EPOCH 2071
2024-02-01 17:18:30,392 Epoch 2071: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 17:18:30,392 EPOCH 2072
2024-02-01 17:18:44,733 Epoch 2072: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 17:18:44,734 EPOCH 2073
2024-02-01 17:18:58,647 Epoch 2073: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 17:18:58,647 EPOCH 2074
2024-02-01 17:19:12,570 Epoch 2074: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 17:19:12,570 EPOCH 2075
2024-02-01 17:19:26,569 Epoch 2075: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 17:19:26,569 EPOCH 2076
2024-02-01 17:19:40,674 Epoch 2076: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 17:19:40,675 EPOCH 2077
2024-02-01 17:19:54,616 Epoch 2077: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 17:19:54,617 EPOCH 2078
2024-02-01 17:20:07,968 [Epoch: 2078 Step: 00018700] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:      604 || Batch Translation Loss:   0.016691 => Txt Tokens per Sec:     1776 || Lr: 0.000100
2024-02-01 17:20:08,663 Epoch 2078: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 17:20:08,663 EPOCH 2079
2024-02-01 17:20:22,690 Epoch 2079: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 17:20:22,691 EPOCH 2080
2024-02-01 17:20:36,826 Epoch 2080: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 17:20:36,826 EPOCH 2081
2024-02-01 17:20:51,002 Epoch 2081: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 17:20:51,003 EPOCH 2082
2024-02-01 17:21:04,710 Epoch 2082: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 17:21:04,710 EPOCH 2083
2024-02-01 17:21:18,853 Epoch 2083: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 17:21:18,854 EPOCH 2084
2024-02-01 17:21:32,984 Epoch 2084: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 17:21:32,985 EPOCH 2085
2024-02-01 17:21:46,587 Epoch 2085: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 17:21:46,587 EPOCH 2086
2024-02-01 17:22:00,558 Epoch 2086: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 17:22:00,559 EPOCH 2087
2024-02-01 17:22:14,804 Epoch 2087: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 17:22:14,804 EPOCH 2088
2024-02-01 17:22:29,008 Epoch 2088: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 17:22:29,008 EPOCH 2089
2024-02-01 17:22:39,044 [Epoch: 2089 Step: 00018800] Batch Recognition Loss:   0.000091 => Gls Tokens per Sec:      932 || Batch Translation Loss:   0.006978 => Txt Tokens per Sec:     2493 || Lr: 0.000100
2024-02-01 17:22:42,572 Epoch 2089: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 17:22:42,572 EPOCH 2090
2024-02-01 17:22:56,738 Epoch 2090: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 17:22:56,738 EPOCH 2091
2024-02-01 17:23:10,736 Epoch 2091: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 17:23:10,737 EPOCH 2092
2024-02-01 17:23:24,722 Epoch 2092: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 17:23:24,723 EPOCH 2093
2024-02-01 17:23:38,812 Epoch 2093: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 17:23:38,812 EPOCH 2094
2024-02-01 17:23:53,012 Epoch 2094: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 17:23:53,013 EPOCH 2095
2024-02-01 17:24:06,687 Epoch 2095: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 17:24:06,687 EPOCH 2096
2024-02-01 17:24:20,322 Epoch 2096: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 17:24:20,322 EPOCH 2097
2024-02-01 17:24:34,424 Epoch 2097: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 17:24:34,425 EPOCH 2098
2024-02-01 17:24:48,615 Epoch 2098: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 17:24:48,616 EPOCH 2099
2024-02-01 17:25:02,634 Epoch 2099: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 17:25:02,635 EPOCH 2100
2024-02-01 17:25:16,823 [Epoch: 2100 Step: 00018900] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:      749 || Batch Translation Loss:   0.016569 => Txt Tokens per Sec:     2080 || Lr: 0.000100
2024-02-01 17:25:16,823 Epoch 2100: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 17:25:16,824 EPOCH 2101
2024-02-01 17:25:30,495 Epoch 2101: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 17:25:30,495 EPOCH 2102
2024-02-01 17:25:44,259 Epoch 2102: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 17:25:44,260 EPOCH 2103
2024-02-01 17:25:58,129 Epoch 2103: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 17:25:58,130 EPOCH 2104
2024-02-01 17:26:12,030 Epoch 2104: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 17:26:12,031 EPOCH 2105
2024-02-01 17:26:25,676 Epoch 2105: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 17:26:25,676 EPOCH 2106
2024-02-01 17:26:40,123 Epoch 2106: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 17:26:40,124 EPOCH 2107
2024-02-01 17:26:54,220 Epoch 2107: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 17:26:54,221 EPOCH 2108
2024-02-01 17:27:08,523 Epoch 2108: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 17:27:08,523 EPOCH 2109
2024-02-01 17:27:22,471 Epoch 2109: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 17:27:22,471 EPOCH 2110
2024-02-01 17:27:36,478 Epoch 2110: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 17:27:36,478 EPOCH 2111
2024-02-01 17:27:50,616 Epoch 2111: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 17:27:50,616 EPOCH 2112
2024-02-01 17:27:50,927 [Epoch: 2112 Step: 00019000] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     4138 || Batch Translation Loss:   0.018166 => Txt Tokens per Sec:     9571 || Lr: 0.000100
2024-02-01 17:28:04,554 Epoch 2112: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-01 17:28:04,555 EPOCH 2113
2024-02-01 17:28:18,482 Epoch 2113: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 17:28:18,483 EPOCH 2114
2024-02-01 17:28:32,419 Epoch 2114: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 17:28:32,420 EPOCH 2115
2024-02-01 17:28:46,161 Epoch 2115: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 17:28:46,162 EPOCH 2116
2024-02-01 17:28:59,984 Epoch 2116: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 17:28:59,984 EPOCH 2117
2024-02-01 17:29:13,928 Epoch 2117: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 17:29:13,928 EPOCH 2118
2024-02-01 17:29:27,756 Epoch 2118: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 17:29:27,757 EPOCH 2119
2024-02-01 17:29:41,655 Epoch 2119: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 17:29:41,656 EPOCH 2120
2024-02-01 17:29:55,609 Epoch 2120: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 17:29:55,610 EPOCH 2121
2024-02-01 17:30:09,561 Epoch 2121: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 17:30:09,562 EPOCH 2122
2024-02-01 17:30:23,532 Epoch 2122: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 17:30:23,533 EPOCH 2123
2024-02-01 17:30:24,163 [Epoch: 2123 Step: 00019100] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:     4070 || Batch Translation Loss:   0.016973 => Txt Tokens per Sec:    10280 || Lr: 0.000100
2024-02-01 17:30:37,179 Epoch 2123: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 17:30:37,180 EPOCH 2124
2024-02-01 17:30:51,112 Epoch 2124: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 17:30:51,113 EPOCH 2125
2024-02-01 17:31:05,128 Epoch 2125: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 17:31:05,129 EPOCH 2126
2024-02-01 17:31:19,353 Epoch 2126: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 17:31:19,354 EPOCH 2127
2024-02-01 17:31:33,405 Epoch 2127: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 17:31:33,406 EPOCH 2128
2024-02-01 17:31:47,292 Epoch 2128: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-01 17:31:47,293 EPOCH 2129
2024-02-01 17:32:01,200 Epoch 2129: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-01 17:32:01,201 EPOCH 2130
2024-02-01 17:32:15,020 Epoch 2130: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-01 17:32:15,020 EPOCH 2131
2024-02-01 17:32:29,037 Epoch 2131: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-01 17:32:29,038 EPOCH 2132
2024-02-01 17:32:43,055 Epoch 2132: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-01 17:32:43,055 EPOCH 2133
2024-02-01 17:32:57,155 Epoch 2133: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-01 17:32:57,156 EPOCH 2134
2024-02-01 17:33:04,481 [Epoch: 2134 Step: 00019200] Batch Recognition Loss:   0.000277 => Gls Tokens per Sec:      524 || Batch Translation Loss:   0.063175 => Txt Tokens per Sec:     1647 || Lr: 0.000100
2024-02-01 17:33:11,084 Epoch 2134: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-01 17:33:11,084 EPOCH 2135
2024-02-01 17:33:26,448 Epoch 2135: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-01 17:33:26,449 EPOCH 2136
2024-02-01 17:33:44,790 Epoch 2136: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-01 17:33:44,791 EPOCH 2137
2024-02-01 17:33:59,484 Epoch 2137: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-01 17:33:59,485 EPOCH 2138
2024-02-01 17:34:13,768 Epoch 2138: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-01 17:34:13,768 EPOCH 2139
2024-02-01 17:34:27,862 Epoch 2139: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-01 17:34:27,863 EPOCH 2140
2024-02-01 17:34:41,670 Epoch 2140: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-01 17:34:41,670 EPOCH 2141
2024-02-01 17:34:56,078 Epoch 2141: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 17:34:56,079 EPOCH 2142
2024-02-01 17:35:10,353 Epoch 2142: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 17:35:10,354 EPOCH 2143
2024-02-01 17:35:24,502 Epoch 2143: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 17:35:24,502 EPOCH 2144
2024-02-01 17:35:38,551 Epoch 2144: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 17:35:38,552 EPOCH 2145
2024-02-01 17:35:47,108 [Epoch: 2145 Step: 00019300] Batch Recognition Loss:   0.000234 => Gls Tokens per Sec:      495 || Batch Translation Loss:   0.026754 => Txt Tokens per Sec:     1535 || Lr: 0.000100
2024-02-01 17:35:52,738 Epoch 2145: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 17:35:52,738 EPOCH 2146
2024-02-01 17:36:06,874 Epoch 2146: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-01 17:36:06,875 EPOCH 2147
2024-02-01 17:36:21,101 Epoch 2147: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-01 17:36:21,101 EPOCH 2148
2024-02-01 17:36:35,198 Epoch 2148: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-01 17:36:35,198 EPOCH 2149
2024-02-01 17:36:49,264 Epoch 2149: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-01 17:36:49,264 EPOCH 2150
2024-02-01 17:37:03,187 Epoch 2150: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-01 17:37:03,188 EPOCH 2151
2024-02-01 17:37:16,883 Epoch 2151: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.71 
2024-02-01 17:37:16,884 EPOCH 2152
2024-02-01 17:37:30,488 Epoch 2152: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-01 17:37:30,489 EPOCH 2153
2024-02-01 17:37:44,398 Epoch 2153: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-01 17:37:44,398 EPOCH 2154
2024-02-01 17:37:58,405 Epoch 2154: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-01 17:37:58,405 EPOCH 2155
2024-02-01 17:38:12,381 Epoch 2155: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-01 17:38:12,382 EPOCH 2156
2024-02-01 17:38:19,430 [Epoch: 2156 Step: 00019400] Batch Recognition Loss:   0.000279 => Gls Tokens per Sec:      908 || Batch Translation Loss:   0.080388 => Txt Tokens per Sec:     2664 || Lr: 0.000100
2024-02-01 17:38:26,427 Epoch 2156: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-01 17:38:26,428 EPOCH 2157
2024-02-01 17:38:40,588 Epoch 2157: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-01 17:38:40,588 EPOCH 2158
2024-02-01 17:38:54,580 Epoch 2158: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.05 
2024-02-01 17:38:54,581 EPOCH 2159
2024-02-01 17:39:08,405 Epoch 2159: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.14 
2024-02-01 17:39:08,405 EPOCH 2160
2024-02-01 17:39:22,461 Epoch 2160: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.18 
2024-02-01 17:39:22,462 EPOCH 2161
2024-02-01 17:39:36,483 Epoch 2161: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.43 
2024-02-01 17:39:36,484 EPOCH 2162
2024-02-01 17:39:50,144 Epoch 2162: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-01 17:39:50,145 EPOCH 2163
2024-02-01 17:40:04,191 Epoch 2163: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-01 17:40:04,191 EPOCH 2164
2024-02-01 17:40:18,149 Epoch 2164: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-01 17:40:18,150 EPOCH 2165
2024-02-01 17:40:32,306 Epoch 2165: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 17:40:32,306 EPOCH 2166
2024-02-01 17:40:46,348 Epoch 2166: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 17:40:46,348 EPOCH 2167
2024-02-01 17:40:53,177 [Epoch: 2167 Step: 00019500] Batch Recognition Loss:   0.000274 => Gls Tokens per Sec:      995 || Batch Translation Loss:   0.021739 => Txt Tokens per Sec:     2732 || Lr: 0.000100
2024-02-01 17:41:00,276 Epoch 2167: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 17:41:00,277 EPOCH 2168
2024-02-01 17:41:14,588 Epoch 2168: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 17:41:14,589 EPOCH 2169
2024-02-01 17:41:28,420 Epoch 2169: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 17:41:28,421 EPOCH 2170
2024-02-01 17:41:42,417 Epoch 2170: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 17:41:42,418 EPOCH 2171
2024-02-01 17:41:56,330 Epoch 2171: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 17:41:56,330 EPOCH 2172
2024-02-01 17:42:10,327 Epoch 2172: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 17:42:10,327 EPOCH 2173
2024-02-01 17:42:24,248 Epoch 2173: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 17:42:24,249 EPOCH 2174
2024-02-01 17:42:38,131 Epoch 2174: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-01 17:42:38,132 EPOCH 2175
2024-02-01 17:42:52,203 Epoch 2175: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 17:42:52,204 EPOCH 2176
2024-02-01 17:43:06,403 Epoch 2176: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 17:43:06,404 EPOCH 2177
2024-02-01 17:43:20,400 Epoch 2177: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 17:43:20,401 EPOCH 2178
2024-02-01 17:43:33,492 [Epoch: 2178 Step: 00019600] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:      616 || Batch Translation Loss:   0.021171 => Txt Tokens per Sec:     1760 || Lr: 0.000100
2024-02-01 17:43:34,329 Epoch 2178: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 17:43:34,330 EPOCH 2179
2024-02-01 17:43:48,421 Epoch 2179: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 17:43:48,421 EPOCH 2180
2024-02-01 17:44:02,419 Epoch 2180: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 17:44:02,419 EPOCH 2181
2024-02-01 17:44:16,447 Epoch 2181: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 17:44:16,448 EPOCH 2182
2024-02-01 17:44:30,416 Epoch 2182: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 17:44:30,416 EPOCH 2183
2024-02-01 17:44:44,579 Epoch 2183: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 17:44:44,580 EPOCH 2184
2024-02-01 17:44:58,797 Epoch 2184: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 17:44:58,797 EPOCH 2185
2024-02-01 17:45:12,761 Epoch 2185: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 17:45:12,762 EPOCH 2186
2024-02-01 17:45:26,693 Epoch 2186: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 17:45:26,694 EPOCH 2187
2024-02-01 17:45:40,657 Epoch 2187: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 17:45:40,657 EPOCH 2188
2024-02-01 17:45:54,493 Epoch 2188: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 17:45:54,494 EPOCH 2189
2024-02-01 17:46:06,780 [Epoch: 2189 Step: 00019700] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:      761 || Batch Translation Loss:   0.013697 => Txt Tokens per Sec:     2089 || Lr: 0.000100
2024-02-01 17:46:08,466 Epoch 2189: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 17:46:08,467 EPOCH 2190
2024-02-01 17:46:22,325 Epoch 2190: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 17:46:22,326 EPOCH 2191
2024-02-01 17:46:36,573 Epoch 2191: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 17:46:36,574 EPOCH 2192
2024-02-01 17:46:50,600 Epoch 2192: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 17:46:50,601 EPOCH 2193
2024-02-01 17:47:04,503 Epoch 2193: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 17:47:04,503 EPOCH 2194
2024-02-01 17:47:18,347 Epoch 2194: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 17:47:18,348 EPOCH 2195
2024-02-01 17:47:32,029 Epoch 2195: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 17:47:32,030 EPOCH 2196
2024-02-01 17:47:45,685 Epoch 2196: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 17:47:45,686 EPOCH 2197
2024-02-01 17:47:59,764 Epoch 2197: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 17:47:59,765 EPOCH 2198
2024-02-01 17:48:13,664 Epoch 2198: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 17:48:13,664 EPOCH 2199
2024-02-01 17:48:27,516 Epoch 2199: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 17:48:27,516 EPOCH 2200
2024-02-01 17:48:41,164 [Epoch: 2200 Step: 00019800] Batch Recognition Loss:   0.000106 => Gls Tokens per Sec:      779 || Batch Translation Loss:   0.006443 => Txt Tokens per Sec:     2162 || Lr: 0.000100
2024-02-01 17:48:41,164 Epoch 2200: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 17:48:41,165 EPOCH 2201
2024-02-01 17:48:55,153 Epoch 2201: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 17:48:55,153 EPOCH 2202
2024-02-01 17:49:09,169 Epoch 2202: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 17:49:09,170 EPOCH 2203
2024-02-01 17:49:23,160 Epoch 2203: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 17:49:23,161 EPOCH 2204
2024-02-01 17:49:37,344 Epoch 2204: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 17:49:37,344 EPOCH 2205
2024-02-01 17:49:51,262 Epoch 2205: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 17:49:51,263 EPOCH 2206
2024-02-01 17:50:05,216 Epoch 2206: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 17:50:05,217 EPOCH 2207
2024-02-01 17:50:19,098 Epoch 2207: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 17:50:19,099 EPOCH 2208
2024-02-01 17:50:33,129 Epoch 2208: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 17:50:33,129 EPOCH 2209
2024-02-01 17:50:47,048 Epoch 2209: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 17:50:47,048 EPOCH 2210
2024-02-01 17:51:01,051 Epoch 2210: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 17:51:01,052 EPOCH 2211
2024-02-01 17:51:14,775 Epoch 2211: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 17:51:14,776 EPOCH 2212
2024-02-01 17:51:15,055 [Epoch: 2212 Step: 00019900] Batch Recognition Loss:   0.000125 => Gls Tokens per Sec:     4604 || Batch Translation Loss:   0.009845 => Txt Tokens per Sec:    10460 || Lr: 0.000100
2024-02-01 17:51:28,893 Epoch 2212: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 17:51:28,893 EPOCH 2213
2024-02-01 17:51:42,987 Epoch 2213: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 17:51:42,987 EPOCH 2214
2024-02-01 17:51:56,899 Epoch 2214: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 17:51:56,899 EPOCH 2215
2024-02-01 17:52:10,974 Epoch 2215: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 17:52:10,974 EPOCH 2216
2024-02-01 17:52:24,828 Epoch 2216: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 17:52:24,829 EPOCH 2217
2024-02-01 17:52:38,742 Epoch 2217: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 17:52:38,743 EPOCH 2218
2024-02-01 17:52:52,599 Epoch 2218: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 17:52:52,600 EPOCH 2219
2024-02-01 17:53:06,708 Epoch 2219: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-01 17:53:06,709 EPOCH 2220
2024-02-01 17:53:20,304 Epoch 2220: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-01 17:53:20,304 EPOCH 2221
2024-02-01 17:53:34,531 Epoch 2221: Total Training Recognition Loss 0.00  Total Training Translation Loss 2.25 
2024-02-01 17:53:34,532 EPOCH 2222
2024-02-01 17:53:48,451 Epoch 2222: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.98 
2024-02-01 17:53:48,451 EPOCH 2223
2024-02-01 17:53:50,829 [Epoch: 2223 Step: 00020000] Batch Recognition Loss:   0.001066 => Gls Tokens per Sec:     1077 || Batch Translation Loss:   0.272406 => Txt Tokens per Sec:     3209 || Lr: 0.000100
2024-02-01 17:54:13,151 Validation result at epoch 2223, step    20000: duration: 22.3216s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00058	Translation Loss: 91713.09375	PPL: 9679.19922
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.71	(BLEU-1: 12.04,	BLEU-2: 3.65,	BLEU-3: 1.45,	BLEU-4: 0.71)
	CHRF 17.55	ROUGE 10.47
2024-02-01 17:54:13,152 Logging Recognition and Translation Outputs
2024-02-01 17:54:13,152 ========================================================================================================================
2024-02-01 17:54:13,152 Logging Sequence: 174_121.00
2024-02-01 17:54:13,153 	Gloss Reference :	A B+C+D+E
2024-02-01 17:54:13,153 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 17:54:13,153 	Gloss Alignment :	         
2024-02-01 17:54:13,153 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 17:54:13,155 	Text Reference  :	there was a    strong competition and a       difficult auction for      the     5       franchise owners     
2024-02-01 17:54:13,155 	Text Hypothesis :	***** *** even social media       was flooded with      vicious comments against india's abysmal   performance
2024-02-01 17:54:13,155 	Text Alignment  :	D     D   S    S      S           S   S       S         S       S        S       S       S         S          
2024-02-01 17:54:13,155 ========================================================================================================================
2024-02-01 17:54:13,156 Logging Sequence: 170_24.00
2024-02-01 17:54:13,156 	Gloss Reference :	A B+C+D+E
2024-02-01 17:54:13,156 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 17:54:13,156 	Gloss Alignment :	         
2024-02-01 17:54:13,156 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 17:54:13,157 	Text Reference  :	let me tell you about it
2024-02-01 17:54:13,157 	Text Hypothesis :	let me tell you about it
2024-02-01 17:54:13,157 	Text Alignment  :	                        
2024-02-01 17:54:13,157 ========================================================================================================================
2024-02-01 17:54:13,157 Logging Sequence: 73_79.00
2024-02-01 17:54:13,158 	Gloss Reference :	A B+C+D+E
2024-02-01 17:54:13,158 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 17:54:13,158 	Gloss Alignment :	         
2024-02-01 17:54:13,158 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 17:54:13,160 	Text Reference  :	raina resturant has food from the   rich spices of   north india     to the aromatic curries of south india 
2024-02-01 17:54:13,160 	Text Hypothesis :	***** ********* *** **** **** there was  a      huge fan   following in the ******** ******* ** ***** league
2024-02-01 17:54:13,160 	Text Alignment  :	D     D         D   D    D    S     S    S      S    S     S         S      D        D       D  D     S     
2024-02-01 17:54:13,160 ========================================================================================================================
2024-02-01 17:54:13,160 Logging Sequence: 140_2.00
2024-02-01 17:54:13,160 	Gloss Reference :	A B+C+D+E
2024-02-01 17:54:13,161 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 17:54:13,161 	Gloss Alignment :	         
2024-02-01 17:54:13,161 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 17:54:13,162 	Text Reference  :	** *** **** ********* *** *** indian batsman-wicket keeper   rishabh pant  has outstanding skills in  cricket
2024-02-01 17:54:13,162 	Text Hypothesis :	he has also following the ban are    spreading      everyone are     given a   proud       of     the person 
2024-02-01 17:54:13,163 	Text Alignment  :	I  I   I    I         I   I   S      S              S        S       S     S   S           S      S   S      
2024-02-01 17:54:13,163 ========================================================================================================================
2024-02-01 17:54:13,163 Logging Sequence: 81_470.00
2024-02-01 17:54:13,163 	Gloss Reference :	A B+C+D+E
2024-02-01 17:54:13,163 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 17:54:13,164 	Gloss Alignment :	         
2024-02-01 17:54:13,164 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 17:54:13,165 	Text Reference  :	or you don't know if  you  do       let us      know   in      the  comments
2024-02-01 17:54:13,165 	Text Hypothesis :	** *** ***** the  two runs suffered a   similar defeat against each other   
2024-02-01 17:54:13,165 	Text Alignment  :	D  D   D     S    S   S    S        S   S       S      S       S    S       
2024-02-01 17:54:13,166 ========================================================================================================================
2024-02-01 17:54:25,985 Epoch 2223: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.76 
2024-02-01 17:54:25,986 EPOCH 2224
2024-02-01 17:54:42,255 Epoch 2224: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.44 
2024-02-01 17:54:42,256 EPOCH 2225
2024-02-01 17:54:57,013 Epoch 2225: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.56 
2024-02-01 17:54:57,014 EPOCH 2226
2024-02-01 17:55:11,316 Epoch 2226: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.21 
2024-02-01 17:55:11,316 EPOCH 2227
2024-02-01 17:55:25,394 Epoch 2227: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.77 
2024-02-01 17:55:25,394 EPOCH 2228
2024-02-01 17:55:39,771 Epoch 2228: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-01 17:55:39,771 EPOCH 2229
2024-02-01 17:55:54,095 Epoch 2229: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-01 17:55:54,095 EPOCH 2230
2024-02-01 17:56:08,281 Epoch 2230: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 17:56:08,281 EPOCH 2231
2024-02-01 17:56:22,515 Epoch 2231: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 17:56:22,516 EPOCH 2232
2024-02-01 17:56:36,341 Epoch 2232: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 17:56:36,342 EPOCH 2233
2024-02-01 17:56:50,378 Epoch 2233: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 17:56:50,379 EPOCH 2234
2024-02-01 17:56:55,256 [Epoch: 2234 Step: 00020100] Batch Recognition Loss:   0.000207 => Gls Tokens per Sec:      788 || Batch Translation Loss:   0.020002 => Txt Tokens per Sec:     2351 || Lr: 0.000100
2024-02-01 17:57:04,586 Epoch 2234: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 17:57:04,587 EPOCH 2235
2024-02-01 17:57:18,919 Epoch 2235: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 17:57:18,919 EPOCH 2236
2024-02-01 17:57:32,817 Epoch 2236: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 17:57:32,817 EPOCH 2237
2024-02-01 17:57:47,258 Epoch 2237: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 17:57:47,258 EPOCH 2238
2024-02-01 17:58:01,332 Epoch 2238: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 17:58:01,332 EPOCH 2239
2024-02-01 17:58:15,274 Epoch 2239: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 17:58:15,274 EPOCH 2240
2024-02-01 17:58:29,389 Epoch 2240: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 17:58:29,389 EPOCH 2241
2024-02-01 17:58:43,432 Epoch 2241: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 17:58:43,432 EPOCH 2242
2024-02-01 17:58:57,128 Epoch 2242: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 17:58:57,129 EPOCH 2243
2024-02-01 17:59:11,577 Epoch 2243: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 17:59:11,577 EPOCH 2244
2024-02-01 17:59:25,486 Epoch 2244: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 17:59:25,487 EPOCH 2245
2024-02-01 17:59:31,788 [Epoch: 2245 Step: 00020200] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:      671 || Batch Translation Loss:   0.016512 => Txt Tokens per Sec:     1945 || Lr: 0.000100
2024-02-01 17:59:39,773 Epoch 2245: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 17:59:39,773 EPOCH 2246
2024-02-01 17:59:53,702 Epoch 2246: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 17:59:53,703 EPOCH 2247
2024-02-01 18:00:07,761 Epoch 2247: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 18:00:07,762 EPOCH 2248
2024-02-01 18:00:21,632 Epoch 2248: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:00:21,632 EPOCH 2249
2024-02-01 18:00:35,395 Epoch 2249: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:00:35,395 EPOCH 2250
2024-02-01 18:00:49,279 Epoch 2250: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 18:00:49,279 EPOCH 2251
2024-02-01 18:01:03,434 Epoch 2251: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 18:01:03,434 EPOCH 2252
2024-02-01 18:01:17,329 Epoch 2252: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 18:01:17,330 EPOCH 2253
2024-02-01 18:01:31,112 Epoch 2253: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 18:01:31,112 EPOCH 2254
2024-02-01 18:01:45,093 Epoch 2254: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 18:01:45,094 EPOCH 2255
2024-02-01 18:01:59,163 Epoch 2255: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 18:01:59,164 EPOCH 2256
2024-02-01 18:02:07,383 [Epoch: 2256 Step: 00020300] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:      670 || Batch Translation Loss:   0.008624 => Txt Tokens per Sec:     1802 || Lr: 0.000100
2024-02-01 18:02:13,194 Epoch 2256: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 18:02:13,194 EPOCH 2257
2024-02-01 18:02:26,997 Epoch 2257: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 18:02:26,998 EPOCH 2258
2024-02-01 18:02:40,866 Epoch 2258: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:02:40,867 EPOCH 2259
2024-02-01 18:02:54,568 Epoch 2259: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:02:54,569 EPOCH 2260
2024-02-01 18:03:08,360 Epoch 2260: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:03:08,361 EPOCH 2261
2024-02-01 18:03:22,249 Epoch 2261: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:03:22,250 EPOCH 2262
2024-02-01 18:03:36,239 Epoch 2262: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:03:36,240 EPOCH 2263
2024-02-01 18:03:50,193 Epoch 2263: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:03:50,194 EPOCH 2264
2024-02-01 18:04:04,244 Epoch 2264: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:04:04,245 EPOCH 2265
2024-02-01 18:04:17,996 Epoch 2265: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:04:17,997 EPOCH 2266
2024-02-01 18:04:31,789 Epoch 2266: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 18:04:31,789 EPOCH 2267
2024-02-01 18:04:40,432 [Epoch: 2267 Step: 00020400] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:      889 || Batch Translation Loss:   0.007422 => Txt Tokens per Sec:     2439 || Lr: 0.000100
2024-02-01 18:04:45,962 Epoch 2267: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 18:04:45,962 EPOCH 2268
2024-02-01 18:04:59,877 Epoch 2268: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:04:59,878 EPOCH 2269
2024-02-01 18:05:13,602 Epoch 2269: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 18:05:13,603 EPOCH 2270
2024-02-01 18:05:27,254 Epoch 2270: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 18:05:27,255 EPOCH 2271
2024-02-01 18:05:41,299 Epoch 2271: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:05:41,300 EPOCH 2272
2024-02-01 18:05:55,042 Epoch 2272: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:05:55,042 EPOCH 2273
2024-02-01 18:06:08,770 Epoch 2273: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 18:06:08,771 EPOCH 2274
2024-02-01 18:06:22,350 Epoch 2274: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:06:22,350 EPOCH 2275
2024-02-01 18:06:36,478 Epoch 2275: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:06:36,478 EPOCH 2276
2024-02-01 18:06:50,186 Epoch 2276: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:06:50,186 EPOCH 2277
2024-02-01 18:07:04,172 Epoch 2277: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:07:04,173 EPOCH 2278
2024-02-01 18:07:17,270 [Epoch: 2278 Step: 00020500] Batch Recognition Loss:   0.000129 => Gls Tokens per Sec:      616 || Batch Translation Loss:   0.007261 => Txt Tokens per Sec:     1742 || Lr: 0.000100
2024-02-01 18:07:18,188 Epoch 2278: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:07:18,188 EPOCH 2279
2024-02-01 18:07:32,371 Epoch 2279: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 18:07:32,371 EPOCH 2280
2024-02-01 18:07:46,008 Epoch 2280: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 18:07:46,009 EPOCH 2281
2024-02-01 18:08:00,141 Epoch 2281: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:08:00,141 EPOCH 2282
2024-02-01 18:08:13,793 Epoch 2282: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 18:08:13,794 EPOCH 2283
2024-02-01 18:08:27,562 Epoch 2283: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 18:08:27,563 EPOCH 2284
2024-02-01 18:08:41,515 Epoch 2284: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:08:41,516 EPOCH 2285
2024-02-01 18:08:55,873 Epoch 2285: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 18:08:55,874 EPOCH 2286
2024-02-01 18:09:09,630 Epoch 2286: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:09:09,631 EPOCH 2287
2024-02-01 18:09:23,551 Epoch 2287: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 18:09:23,552 EPOCH 2288
2024-02-01 18:09:37,368 Epoch 2288: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 18:09:37,369 EPOCH 2289
2024-02-01 18:09:47,646 [Epoch: 2289 Step: 00020600] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:      910 || Batch Translation Loss:   0.013999 => Txt Tokens per Sec:     2436 || Lr: 0.000100
2024-02-01 18:09:51,232 Epoch 2289: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 18:09:51,232 EPOCH 2290
2024-02-01 18:10:05,424 Epoch 2290: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:10:05,424 EPOCH 2291
2024-02-01 18:10:19,532 Epoch 2291: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:10:19,533 EPOCH 2292
2024-02-01 18:10:33,309 Epoch 2292: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:10:33,310 EPOCH 2293
2024-02-01 18:10:47,262 Epoch 2293: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 18:10:47,263 EPOCH 2294
2024-02-01 18:11:01,585 Epoch 2294: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:11:01,586 EPOCH 2295
2024-02-01 18:11:15,656 Epoch 2295: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:11:15,656 EPOCH 2296
2024-02-01 18:11:29,608 Epoch 2296: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:11:29,609 EPOCH 2297
2024-02-01 18:11:43,549 Epoch 2297: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:11:43,549 EPOCH 2298
2024-02-01 18:11:57,615 Epoch 2298: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 18:11:57,616 EPOCH 2299
2024-02-01 18:12:11,419 Epoch 2299: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 18:12:11,420 EPOCH 2300
2024-02-01 18:12:25,572 [Epoch: 2300 Step: 00020700] Batch Recognition Loss:   0.000121 => Gls Tokens per Sec:      751 || Batch Translation Loss:   0.013609 => Txt Tokens per Sec:     2085 || Lr: 0.000100
2024-02-01 18:12:25,573 Epoch 2300: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 18:12:25,573 EPOCH 2301
2024-02-01 18:12:39,674 Epoch 2301: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 18:12:39,675 EPOCH 2302
2024-02-01 18:12:53,421 Epoch 2302: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 18:12:53,422 EPOCH 2303
2024-02-01 18:13:07,293 Epoch 2303: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:13:07,293 EPOCH 2304
2024-02-01 18:13:21,446 Epoch 2304: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:13:21,446 EPOCH 2305
2024-02-01 18:13:34,757 Epoch 2305: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:13:34,758 EPOCH 2306
2024-02-01 18:13:49,103 Epoch 2306: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:13:49,104 EPOCH 2307
2024-02-01 18:14:03,359 Epoch 2307: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 18:14:03,360 EPOCH 2308
2024-02-01 18:14:17,623 Epoch 2308: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:14:17,624 EPOCH 2309
2024-02-01 18:14:31,529 Epoch 2309: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 18:14:31,530 EPOCH 2310
2024-02-01 18:14:45,451 Epoch 2310: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 18:14:45,452 EPOCH 2311
2024-02-01 18:14:59,284 Epoch 2311: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 18:14:59,285 EPOCH 2312
2024-02-01 18:15:02,322 [Epoch: 2312 Step: 00020800] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:      422 || Batch Translation Loss:   0.062498 => Txt Tokens per Sec:     1344 || Lr: 0.000100
2024-02-01 18:15:13,116 Epoch 2312: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 18:15:13,117 EPOCH 2313
2024-02-01 18:15:27,100 Epoch 2313: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 18:15:27,100 EPOCH 2314
2024-02-01 18:15:41,368 Epoch 2314: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 18:15:41,369 EPOCH 2315
2024-02-01 18:15:55,254 Epoch 2315: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 18:15:55,255 EPOCH 2316
2024-02-01 18:16:09,035 Epoch 2316: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 18:16:09,036 EPOCH 2317
2024-02-01 18:16:23,065 Epoch 2317: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 18:16:23,065 EPOCH 2318
2024-02-01 18:16:37,052 Epoch 2318: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 18:16:37,052 EPOCH 2319
2024-02-01 18:16:50,847 Epoch 2319: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 18:16:50,848 EPOCH 2320
2024-02-01 18:17:04,865 Epoch 2320: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 18:17:04,866 EPOCH 2321
2024-02-01 18:17:18,628 Epoch 2321: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 18:17:18,629 EPOCH 2322
2024-02-01 18:17:32,499 Epoch 2322: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-01 18:17:32,500 EPOCH 2323
2024-02-01 18:17:33,071 [Epoch: 2323 Step: 00020900] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     4489 || Batch Translation Loss:   0.021107 => Txt Tokens per Sec:     9198 || Lr: 0.000100
2024-02-01 18:17:46,294 Epoch 2323: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.06 
2024-02-01 18:17:46,295 EPOCH 2324
2024-02-01 18:18:00,507 Epoch 2324: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.29 
2024-02-01 18:18:00,507 EPOCH 2325
2024-02-01 18:18:14,357 Epoch 2325: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.64 
2024-02-01 18:18:14,358 EPOCH 2326
2024-02-01 18:18:28,618 Epoch 2326: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.30 
2024-02-01 18:18:28,619 EPOCH 2327
2024-02-01 18:18:42,585 Epoch 2327: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.40 
2024-02-01 18:18:42,586 EPOCH 2328
2024-02-01 18:18:56,594 Epoch 2328: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.15 
2024-02-01 18:18:56,594 EPOCH 2329
2024-02-01 18:19:10,486 Epoch 2329: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.98 
2024-02-01 18:19:10,487 EPOCH 2330
2024-02-01 18:19:24,402 Epoch 2330: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-01 18:19:24,403 EPOCH 2331
2024-02-01 18:19:38,463 Epoch 2331: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-01 18:19:38,464 EPOCH 2332
2024-02-01 18:19:52,400 Epoch 2332: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-01 18:19:52,401 EPOCH 2333
2024-02-01 18:20:06,379 Epoch 2333: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 18:20:06,379 EPOCH 2334
2024-02-01 18:20:13,264 [Epoch: 2334 Step: 00021000] Batch Recognition Loss:   0.000368 => Gls Tokens per Sec:      558 || Batch Translation Loss:   0.034203 => Txt Tokens per Sec:     1573 || Lr: 0.000100
2024-02-01 18:20:20,010 Epoch 2334: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 18:20:20,010 EPOCH 2335
2024-02-01 18:20:33,536 Epoch 2335: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 18:20:33,537 EPOCH 2336
2024-02-01 18:20:47,510 Epoch 2336: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 18:20:47,511 EPOCH 2337
2024-02-01 18:21:01,442 Epoch 2337: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 18:21:01,442 EPOCH 2338
2024-02-01 18:21:15,564 Epoch 2338: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 18:21:15,565 EPOCH 2339
2024-02-01 18:21:29,577 Epoch 2339: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 18:21:29,577 EPOCH 2340
2024-02-01 18:21:43,551 Epoch 2340: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 18:21:43,552 EPOCH 2341
2024-02-01 18:21:57,527 Epoch 2341: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 18:21:57,528 EPOCH 2342
2024-02-01 18:22:11,428 Epoch 2342: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 18:22:11,429 EPOCH 2343
2024-02-01 18:22:25,664 Epoch 2343: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 18:22:25,665 EPOCH 2344
2024-02-01 18:22:39,633 Epoch 2344: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 18:22:39,634 EPOCH 2345
2024-02-01 18:22:48,800 [Epoch: 2345 Step: 00021100] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:      462 || Batch Translation Loss:   0.023431 => Txt Tokens per Sec:     1447 || Lr: 0.000100
2024-02-01 18:22:53,493 Epoch 2345: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 18:22:53,494 EPOCH 2346
2024-02-01 18:23:07,436 Epoch 2346: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 18:23:07,436 EPOCH 2347
2024-02-01 18:23:21,446 Epoch 2347: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 18:23:21,446 EPOCH 2348
2024-02-01 18:23:35,287 Epoch 2348: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 18:23:35,288 EPOCH 2349
2024-02-01 18:23:49,283 Epoch 2349: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 18:23:49,283 EPOCH 2350
2024-02-01 18:24:03,052 Epoch 2350: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 18:24:03,053 EPOCH 2351
2024-02-01 18:24:17,122 Epoch 2351: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 18:24:17,123 EPOCH 2352
2024-02-01 18:24:32,003 Epoch 2352: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 18:24:32,003 EPOCH 2353
2024-02-01 18:24:46,218 Epoch 2353: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 18:24:46,219 EPOCH 2354
2024-02-01 18:25:00,085 Epoch 2354: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 18:25:00,085 EPOCH 2355
2024-02-01 18:25:14,313 Epoch 2355: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:25:14,314 EPOCH 2356
2024-02-01 18:25:20,966 [Epoch: 2356 Step: 00021200] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:      962 || Batch Translation Loss:   0.010465 => Txt Tokens per Sec:     2570 || Lr: 0.000100
2024-02-01 18:25:28,349 Epoch 2356: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 18:25:28,350 EPOCH 2357
2024-02-01 18:25:42,169 Epoch 2357: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 18:25:42,170 EPOCH 2358
2024-02-01 18:25:56,140 Epoch 2358: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:25:56,141 EPOCH 2359
2024-02-01 18:26:10,137 Epoch 2359: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:26:10,137 EPOCH 2360
2024-02-01 18:26:24,004 Epoch 2360: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:26:24,005 EPOCH 2361
2024-02-01 18:26:38,063 Epoch 2361: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:26:38,064 EPOCH 2362
2024-02-01 18:26:51,843 Epoch 2362: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 18:26:51,844 EPOCH 2363
2024-02-01 18:27:06,034 Epoch 2363: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:27:06,035 EPOCH 2364
2024-02-01 18:27:20,219 Epoch 2364: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 18:27:20,219 EPOCH 2365
2024-02-01 18:27:34,203 Epoch 2365: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 18:27:34,204 EPOCH 2366
2024-02-01 18:27:48,229 Epoch 2366: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 18:27:48,229 EPOCH 2367
2024-02-01 18:27:58,337 [Epoch: 2367 Step: 00021300] Batch Recognition Loss:   0.000122 => Gls Tokens per Sec:      672 || Batch Translation Loss:   0.013231 => Txt Tokens per Sec:     1907 || Lr: 0.000100
2024-02-01 18:28:02,074 Epoch 2367: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:28:02,075 EPOCH 2368
2024-02-01 18:28:15,847 Epoch 2368: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 18:28:15,847 EPOCH 2369
2024-02-01 18:28:29,778 Epoch 2369: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:28:29,778 EPOCH 2370
2024-02-01 18:28:43,560 Epoch 2370: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 18:28:43,561 EPOCH 2371
2024-02-01 18:28:57,606 Epoch 2371: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 18:28:57,607 EPOCH 2372
2024-02-01 18:29:11,560 Epoch 2372: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 18:29:11,560 EPOCH 2373
2024-02-01 18:29:25,027 Epoch 2373: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 18:29:25,027 EPOCH 2374
2024-02-01 18:29:39,047 Epoch 2374: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 18:29:39,047 EPOCH 2375
2024-02-01 18:29:52,874 Epoch 2375: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 18:29:52,875 EPOCH 2376
2024-02-01 18:30:06,945 Epoch 2376: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 18:30:06,945 EPOCH 2377
2024-02-01 18:30:21,066 Epoch 2377: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 18:30:21,067 EPOCH 2378
2024-02-01 18:30:28,161 [Epoch: 2378 Step: 00021400] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     1138 || Batch Translation Loss:   0.008371 => Txt Tokens per Sec:     2957 || Lr: 0.000100
2024-02-01 18:30:35,106 Epoch 2378: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 18:30:35,107 EPOCH 2379
2024-02-01 18:30:48,997 Epoch 2379: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 18:30:48,998 EPOCH 2380
2024-02-01 18:31:02,953 Epoch 2380: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 18:31:02,954 EPOCH 2381
2024-02-01 18:31:16,758 Epoch 2381: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 18:31:16,759 EPOCH 2382
2024-02-01 18:31:31,094 Epoch 2382: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 18:31:31,095 EPOCH 2383
2024-02-01 18:31:45,195 Epoch 2383: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 18:31:45,196 EPOCH 2384
2024-02-01 18:31:59,408 Epoch 2384: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 18:31:59,408 EPOCH 2385
2024-02-01 18:32:13,284 Epoch 2385: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 18:32:13,285 EPOCH 2386
2024-02-01 18:32:26,935 Epoch 2386: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 18:32:26,935 EPOCH 2387
2024-02-01 18:32:41,120 Epoch 2387: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 18:32:41,120 EPOCH 2388
2024-02-01 18:32:55,081 Epoch 2388: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 18:32:55,081 EPOCH 2389
2024-02-01 18:33:05,653 [Epoch: 2389 Step: 00021500] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:      885 || Batch Translation Loss:   0.026173 => Txt Tokens per Sec:     2404 || Lr: 0.000100
2024-02-01 18:33:08,763 Epoch 2389: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 18:33:08,763 EPOCH 2390
2024-02-01 18:33:22,764 Epoch 2390: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 18:33:22,765 EPOCH 2391
2024-02-01 18:33:36,959 Epoch 2391: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 18:33:36,960 EPOCH 2392
2024-02-01 18:33:50,836 Epoch 2392: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 18:33:50,837 EPOCH 2393
2024-02-01 18:34:05,148 Epoch 2393: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 18:34:05,149 EPOCH 2394
2024-02-01 18:34:19,613 Epoch 2394: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 18:34:19,614 EPOCH 2395
2024-02-01 18:34:33,276 Epoch 2395: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 18:34:33,277 EPOCH 2396
2024-02-01 18:34:47,379 Epoch 2396: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 18:34:47,379 EPOCH 2397
2024-02-01 18:35:01,246 Epoch 2397: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 18:35:01,246 EPOCH 2398
2024-02-01 18:35:15,061 Epoch 2398: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 18:35:15,062 EPOCH 2399
2024-02-01 18:35:28,695 Epoch 2399: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 18:35:28,695 EPOCH 2400
2024-02-01 18:35:42,379 [Epoch: 2400 Step: 00021600] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:      777 || Batch Translation Loss:   0.021814 => Txt Tokens per Sec:     2157 || Lr: 0.000100
2024-02-01 18:35:42,380 Epoch 2400: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:35:42,380 EPOCH 2401
2024-02-01 18:35:56,415 Epoch 2401: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:35:56,416 EPOCH 2402
2024-02-01 18:36:10,115 Epoch 2402: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:36:10,115 EPOCH 2403
2024-02-01 18:36:24,234 Epoch 2403: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:36:24,235 EPOCH 2404
2024-02-01 18:36:38,329 Epoch 2404: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:36:38,330 EPOCH 2405
2024-02-01 18:36:52,338 Epoch 2405: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:36:52,338 EPOCH 2406
2024-02-01 18:37:06,463 Epoch 2406: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:37:06,463 EPOCH 2407
2024-02-01 18:37:19,987 Epoch 2407: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:37:19,987 EPOCH 2408
2024-02-01 18:37:33,985 Epoch 2408: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 18:37:33,986 EPOCH 2409
2024-02-01 18:37:47,845 Epoch 2409: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 18:37:47,846 EPOCH 2410
2024-02-01 18:38:01,499 Epoch 2410: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 18:38:01,500 EPOCH 2411
2024-02-01 18:38:15,379 Epoch 2411: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-01 18:38:15,379 EPOCH 2412
2024-02-01 18:38:15,600 [Epoch: 2412 Step: 00021700] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:     5836 || Batch Translation Loss:   0.011523 => Txt Tokens per Sec:    10399 || Lr: 0.000100
2024-02-01 18:38:29,266 Epoch 2412: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-01 18:38:29,266 EPOCH 2413
2024-02-01 18:38:43,252 Epoch 2413: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-01 18:38:43,252 EPOCH 2414
2024-02-01 18:38:57,091 Epoch 2414: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.00 
2024-02-01 18:38:57,091 EPOCH 2415
2024-02-01 18:39:11,052 Epoch 2415: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.29 
2024-02-01 18:39:11,053 EPOCH 2416
2024-02-01 18:39:24,890 Epoch 2416: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.27 
2024-02-01 18:39:24,891 EPOCH 2417
2024-02-01 18:39:38,568 Epoch 2417: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.61 
2024-02-01 18:39:38,569 EPOCH 2418
2024-02-01 18:39:52,542 Epoch 2418: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.39 
2024-02-01 18:39:52,542 EPOCH 2419
2024-02-01 18:40:06,555 Epoch 2419: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.16 
2024-02-01 18:40:06,555 EPOCH 2420
2024-02-01 18:40:20,484 Epoch 2420: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.95 
2024-02-01 18:40:20,485 EPOCH 2421
2024-02-01 18:40:34,566 Epoch 2421: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.09 
2024-02-01 18:40:34,567 EPOCH 2422
2024-02-01 18:40:48,632 Epoch 2422: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.81 
2024-02-01 18:40:48,632 EPOCH 2423
2024-02-01 18:40:52,207 [Epoch: 2423 Step: 00021800] Batch Recognition Loss:   0.000568 => Gls Tokens per Sec:      467 || Batch Translation Loss:   0.051913 => Txt Tokens per Sec:     1220 || Lr: 0.000100
2024-02-01 18:41:02,476 Epoch 2423: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-01 18:41:02,477 EPOCH 2424
2024-02-01 18:41:16,400 Epoch 2424: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-01 18:41:16,400 EPOCH 2425
2024-02-01 18:41:30,734 Epoch 2425: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-01 18:41:30,734 EPOCH 2426
2024-02-01 18:41:44,671 Epoch 2426: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-01 18:41:44,671 EPOCH 2427
2024-02-01 18:41:58,746 Epoch 2427: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.79 
2024-02-01 18:41:58,747 EPOCH 2428
2024-02-01 18:42:12,498 Epoch 2428: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-01 18:42:12,499 EPOCH 2429
2024-02-01 18:42:26,478 Epoch 2429: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-01 18:42:26,479 EPOCH 2430
2024-02-01 18:42:40,406 Epoch 2430: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 18:42:40,407 EPOCH 2431
2024-02-01 18:42:54,407 Epoch 2431: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-01 18:42:54,407 EPOCH 2432
2024-02-01 18:43:08,132 Epoch 2432: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 18:43:08,133 EPOCH 2433
2024-02-01 18:43:22,077 Epoch 2433: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 18:43:22,078 EPOCH 2434
2024-02-01 18:43:27,758 [Epoch: 2434 Step: 00021900] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:      519 || Batch Translation Loss:   0.029943 => Txt Tokens per Sec:     1584 || Lr: 0.000100
2024-02-01 18:43:35,828 Epoch 2434: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 18:43:35,828 EPOCH 2435
2024-02-01 18:43:49,513 Epoch 2435: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 18:43:49,513 EPOCH 2436
2024-02-01 18:44:03,586 Epoch 2436: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 18:44:03,586 EPOCH 2437
2024-02-01 18:44:17,440 Epoch 2437: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 18:44:17,441 EPOCH 2438
2024-02-01 18:44:31,282 Epoch 2438: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 18:44:31,282 EPOCH 2439
2024-02-01 18:44:44,907 Epoch 2439: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 18:44:44,907 EPOCH 2440
2024-02-01 18:44:58,781 Epoch 2440: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 18:44:58,781 EPOCH 2441
2024-02-01 18:45:12,858 Epoch 2441: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 18:45:12,859 EPOCH 2442
2024-02-01 18:45:26,893 Epoch 2442: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 18:45:26,893 EPOCH 2443
2024-02-01 18:45:41,091 Epoch 2443: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 18:45:41,092 EPOCH 2444
2024-02-01 18:45:55,270 Epoch 2444: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 18:45:55,270 EPOCH 2445
2024-02-01 18:46:02,474 [Epoch: 2445 Step: 00022000] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:      711 || Batch Translation Loss:   0.020014 => Txt Tokens per Sec:     2000 || Lr: 0.000100
2024-02-01 18:46:22,750 Validation result at epoch 2445, step    22000: duration: 20.2759s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00016	Translation Loss: 96149.03125	PPL: 15087.74707
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.58	(BLEU-1: 11.10,	BLEU-2: 3.22,	BLEU-3: 1.26,	BLEU-4: 0.58)
	CHRF 17.14	ROUGE 9.29
2024-02-01 18:46:22,751 Logging Recognition and Translation Outputs
2024-02-01 18:46:22,751 ========================================================================================================================
2024-02-01 18:46:22,751 Logging Sequence: 146_56.00
2024-02-01 18:46:22,752 	Gloss Reference :	A B+C+D+E
2024-02-01 18:46:22,752 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 18:46:22,752 	Gloss Alignment :	         
2024-02-01 18:46:22,752 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 18:46:22,754 	Text Reference  :	when the players go      back   to   the hotel as per rules all of    them have    to      undergo rtpcr test      for covid-19 everyday
2024-02-01 18:46:22,755 	Text Hypothesis :	**** the indian  premier league 2023 the ***** ** *** ***** *** world test matches between kkr     and   australia and the      match   
2024-02-01 18:46:22,755 	Text Alignment  :	D        S       S       S      S        D     D  D   D     D   S     S    S       S       S       S     S         S   S        S       
2024-02-01 18:46:22,755 ========================================================================================================================
2024-02-01 18:46:22,755 Logging Sequence: 118_338.00
2024-02-01 18:46:22,755 	Gloss Reference :	A B+C+D+E
2024-02-01 18:46:22,755 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 18:46:22,755 	Gloss Alignment :	         
2024-02-01 18:46:22,756 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 18:46:22,756 	Text Reference  :	*** *** this   is         why even messi wore it        
2024-02-01 18:46:22,756 	Text Hypothesis :	and the second federation or  left for   the  tournament
2024-02-01 18:46:22,756 	Text Alignment  :	I   I   S      S          S   S    S     S    S         
2024-02-01 18:46:22,757 ========================================================================================================================
2024-02-01 18:46:22,757 Logging Sequence: 66_61.00
2024-02-01 18:46:22,757 	Gloss Reference :	A B+C+D+E
2024-02-01 18:46:22,757 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 18:46:22,757 	Gloss Alignment :	         
2024-02-01 18:46:22,757 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 18:46:22,758 	Text Reference  :	instead of returning back      to     his homeland because of     his injury
2024-02-01 18:46:22,758 	Text Hypothesis :	******* ** ********* rajasthan royals ben stokes   had     broken his finger
2024-02-01 18:46:22,758 	Text Alignment  :	D       D  D         S         S      S   S        S       S          S     
2024-02-01 18:46:22,758 ========================================================================================================================
2024-02-01 18:46:22,758 Logging Sequence: 81_278.00
2024-02-01 18:46:22,759 	Gloss Reference :	A B+C+D+E
2024-02-01 18:46:22,759 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 18:46:22,759 	Gloss Alignment :	         
2024-02-01 18:46:22,759 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 18:46:22,761 	Text Reference  :	of this amrapali group paid rs 3570 crore the remaining rs 652 crore was paid by      amrapali sapphire developers a ******** subsidiary of amrapali group   
2024-02-01 18:46:22,761 	Text Hypothesis :	** **** ******** ***** **** ** **** ***** the ********* ** *** ***** *** **** supreme court    then     ordered    a forensic audit      of amrapali builders
2024-02-01 18:46:22,761 	Text Alignment  :	D  D    D        D     D    D  D    D         D         D  D   D     D   D    S       S        S        S            I        S                      S       
2024-02-01 18:46:22,761 ========================================================================================================================
2024-02-01 18:46:22,761 Logging Sequence: 162_125.00
2024-02-01 18:46:22,761 	Gloss Reference :	A B+C+D+E
2024-02-01 18:46:22,761 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 18:46:22,761 	Gloss Alignment :	         
2024-02-01 18:46:22,762 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 18:46:22,763 	Text Reference  :	**** *** *** ********* *** in response to  this kohli     received many  hate comments on   social media  
2024-02-01 18:46:22,763 	Text Hypothesis :	fans are now wondering why he did      not have returning to       india and  he       then the    country
2024-02-01 18:46:22,763 	Text Alignment  :	I    I   I   I         I   S  S        S   S    S         S        S     S    S        S    S      S      
2024-02-01 18:46:22,763 ========================================================================================================================
2024-02-01 18:46:29,310 Epoch 2445: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 18:46:29,310 EPOCH 2446
2024-02-01 18:46:43,230 Epoch 2446: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 18:46:43,231 EPOCH 2447
2024-02-01 18:46:57,131 Epoch 2447: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 18:46:57,132 EPOCH 2448
2024-02-01 18:47:10,449 Epoch 2448: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 18:47:10,449 EPOCH 2449
2024-02-01 18:47:24,799 Epoch 2449: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 18:47:24,800 EPOCH 2450
2024-02-01 18:47:38,805 Epoch 2450: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 18:47:38,806 EPOCH 2451
2024-02-01 18:47:52,641 Epoch 2451: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 18:47:52,642 EPOCH 2452
2024-02-01 18:48:06,533 Epoch 2452: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 18:48:06,534 EPOCH 2453
2024-02-01 18:48:20,738 Epoch 2453: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 18:48:20,739 EPOCH 2454
2024-02-01 18:48:34,497 Epoch 2454: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 18:48:34,497 EPOCH 2455
2024-02-01 18:48:48,460 Epoch 2455: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 18:48:48,460 EPOCH 2456
2024-02-01 18:48:54,663 [Epoch: 2456 Step: 00022100] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     1032 || Batch Translation Loss:   0.021152 => Txt Tokens per Sec:     2878 || Lr: 0.000100
2024-02-01 18:49:02,406 Epoch 2456: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 18:49:02,406 EPOCH 2457
2024-02-01 18:49:16,610 Epoch 2457: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 18:49:16,611 EPOCH 2458
2024-02-01 18:49:30,563 Epoch 2458: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 18:49:30,563 EPOCH 2459
2024-02-01 18:49:44,661 Epoch 2459: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 18:49:44,662 EPOCH 2460
2024-02-01 18:49:58,557 Epoch 2460: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 18:49:58,558 EPOCH 2461
2024-02-01 18:50:12,426 Epoch 2461: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 18:50:12,426 EPOCH 2462
2024-02-01 18:50:26,561 Epoch 2462: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 18:50:26,561 EPOCH 2463
2024-02-01 18:50:40,226 Epoch 2463: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 18:50:40,226 EPOCH 2464
2024-02-01 18:50:54,493 Epoch 2464: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 18:50:54,494 EPOCH 2465
2024-02-01 18:51:08,440 Epoch 2465: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 18:51:08,440 EPOCH 2466
2024-02-01 18:51:22,367 Epoch 2466: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 18:51:22,368 EPOCH 2467
2024-02-01 18:51:32,184 [Epoch: 2467 Step: 00022200] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:      783 || Batch Translation Loss:   0.020720 => Txt Tokens per Sec:     2241 || Lr: 0.000100
2024-02-01 18:51:36,317 Epoch 2467: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 18:51:36,318 EPOCH 2468
2024-02-01 18:51:50,180 Epoch 2468: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 18:51:50,181 EPOCH 2469
2024-02-01 18:52:04,509 Epoch 2469: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 18:52:04,509 EPOCH 2470
2024-02-01 18:52:18,399 Epoch 2470: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 18:52:18,400 EPOCH 2471
2024-02-01 18:52:32,294 Epoch 2471: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 18:52:32,294 EPOCH 2472
2024-02-01 18:52:46,267 Epoch 2472: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 18:52:46,267 EPOCH 2473
2024-02-01 18:52:59,940 Epoch 2473: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 18:52:59,940 EPOCH 2474
2024-02-01 18:53:13,823 Epoch 2474: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 18:53:13,823 EPOCH 2475
2024-02-01 18:53:27,428 Epoch 2475: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 18:53:27,429 EPOCH 2476
2024-02-01 18:53:41,645 Epoch 2476: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 18:53:41,645 EPOCH 2477
2024-02-01 18:53:55,507 Epoch 2477: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 18:53:55,508 EPOCH 2478
2024-02-01 18:54:05,761 [Epoch: 2478 Step: 00022300] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:      874 || Batch Translation Loss:   0.025845 => Txt Tokens per Sec:     2520 || Lr: 0.000100
2024-02-01 18:54:09,382 Epoch 2478: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 18:54:09,382 EPOCH 2479
2024-02-01 18:54:23,187 Epoch 2479: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 18:54:23,187 EPOCH 2480
2024-02-01 18:54:37,093 Epoch 2480: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 18:54:37,093 EPOCH 2481
2024-02-01 18:54:50,838 Epoch 2481: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 18:54:50,838 EPOCH 2482
2024-02-01 18:55:05,187 Epoch 2482: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:55:05,188 EPOCH 2483
2024-02-01 18:55:18,713 Epoch 2483: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:55:18,713 EPOCH 2484
2024-02-01 18:55:32,715 Epoch 2484: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 18:55:32,716 EPOCH 2485
2024-02-01 18:55:46,623 Epoch 2485: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 18:55:46,623 EPOCH 2486
2024-02-01 18:56:00,267 Epoch 2486: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:56:00,268 EPOCH 2487
2024-02-01 18:56:14,178 Epoch 2487: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:56:14,178 EPOCH 2488
2024-02-01 18:56:28,134 Epoch 2488: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 18:56:28,134 EPOCH 2489
2024-02-01 18:56:39,018 [Epoch: 2489 Step: 00022400] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:      859 || Batch Translation Loss:   0.025987 => Txt Tokens per Sec:     2335 || Lr: 0.000100
2024-02-01 18:56:42,080 Epoch 2489: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 18:56:42,080 EPOCH 2490
2024-02-01 18:56:56,184 Epoch 2490: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 18:56:56,185 EPOCH 2491
2024-02-01 18:57:10,109 Epoch 2491: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 18:57:10,109 EPOCH 2492
2024-02-01 18:57:24,026 Epoch 2492: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 18:57:24,026 EPOCH 2493
2024-02-01 18:57:37,774 Epoch 2493: Total Training Recognition Loss 0.00  Total Training Translation Loss 2.12 
2024-02-01 18:57:37,775 EPOCH 2494
2024-02-01 18:57:51,739 Epoch 2494: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.75 
2024-02-01 18:57:51,740 EPOCH 2495
2024-02-01 18:58:05,672 Epoch 2495: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.81 
2024-02-01 18:58:05,673 EPOCH 2496
2024-02-01 18:58:19,196 Epoch 2496: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.73 
2024-02-01 18:58:19,197 EPOCH 2497
2024-02-01 18:58:33,077 Epoch 2497: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.33 
2024-02-01 18:58:33,078 EPOCH 2498
2024-02-01 18:58:47,145 Epoch 2498: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.46 
2024-02-01 18:58:47,146 EPOCH 2499
2024-02-01 18:59:01,199 Epoch 2499: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.07 
2024-02-01 18:59:01,199 EPOCH 2500
2024-02-01 18:59:15,086 [Epoch: 2500 Step: 00022500] Batch Recognition Loss:   0.000792 => Gls Tokens per Sec:      766 || Batch Translation Loss:   0.759501 => Txt Tokens per Sec:     2125 || Lr: 0.000100
2024-02-01 18:59:15,087 Epoch 2500: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.74 
2024-02-01 18:59:15,087 EPOCH 2501
2024-02-01 18:59:28,978 Epoch 2501: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.34 
2024-02-01 18:59:28,978 EPOCH 2502
2024-02-01 18:59:42,892 Epoch 2502: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-01 18:59:42,893 EPOCH 2503
2024-02-01 18:59:56,748 Epoch 2503: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-01 18:59:56,749 EPOCH 2504
2024-02-01 19:00:10,471 Epoch 2504: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-01 19:00:10,472 EPOCH 2505
2024-02-01 19:00:24,559 Epoch 2505: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 19:00:24,559 EPOCH 2506
2024-02-01 19:00:38,609 Epoch 2506: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 19:00:38,610 EPOCH 2507
2024-02-01 19:00:52,449 Epoch 2507: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 19:00:52,450 EPOCH 2508
2024-02-01 19:01:06,332 Epoch 2508: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 19:01:06,333 EPOCH 2509
2024-02-01 19:01:20,481 Epoch 2509: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 19:01:20,482 EPOCH 2510
2024-02-01 19:01:34,520 Epoch 2510: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 19:01:34,521 EPOCH 2511
2024-02-01 19:01:48,314 Epoch 2511: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 19:01:48,315 EPOCH 2512
2024-02-01 19:01:51,973 [Epoch: 2512 Step: 00022600] Batch Recognition Loss:   0.000341 => Gls Tokens per Sec:      350 || Batch Translation Loss:   0.025315 => Txt Tokens per Sec:     1226 || Lr: 0.000100
2024-02-01 19:02:02,217 Epoch 2512: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 19:02:02,218 EPOCH 2513
2024-02-01 19:02:15,936 Epoch 2513: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 19:02:15,936 EPOCH 2514
2024-02-01 19:02:30,237 Epoch 2514: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 19:02:30,238 EPOCH 2515
2024-02-01 19:02:44,449 Epoch 2515: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 19:02:44,449 EPOCH 2516
2024-02-01 19:02:58,298 Epoch 2516: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 19:02:58,299 EPOCH 2517
2024-02-01 19:03:12,550 Epoch 2517: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 19:03:12,550 EPOCH 2518
2024-02-01 19:03:26,124 Epoch 2518: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 19:03:26,125 EPOCH 2519
2024-02-01 19:03:39,866 Epoch 2519: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 19:03:39,867 EPOCH 2520
2024-02-01 19:03:53,712 Epoch 2520: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 19:03:53,713 EPOCH 2521
2024-02-01 19:04:07,638 Epoch 2521: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 19:04:07,638 EPOCH 2522
2024-02-01 19:04:21,525 Epoch 2522: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 19:04:21,525 EPOCH 2523
2024-02-01 19:04:22,601 [Epoch: 2523 Step: 00022700] Batch Recognition Loss:   0.000175 => Gls Tokens per Sec:     2381 || Batch Translation Loss:   0.013956 => Txt Tokens per Sec:     6167 || Lr: 0.000100
2024-02-01 19:04:35,551 Epoch 2523: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 19:04:35,551 EPOCH 2524
2024-02-01 19:04:49,583 Epoch 2524: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 19:04:49,583 EPOCH 2525
2024-02-01 19:05:03,294 Epoch 2525: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:05:03,295 EPOCH 2526
2024-02-01 19:05:17,445 Epoch 2526: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:05:17,445 EPOCH 2527
2024-02-01 19:05:31,217 Epoch 2527: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:05:31,218 EPOCH 2528
2024-02-01 19:05:45,051 Epoch 2528: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:05:45,052 EPOCH 2529
2024-02-01 19:05:58,861 Epoch 2529: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 19:05:58,861 EPOCH 2530
2024-02-01 19:06:12,717 Epoch 2530: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 19:06:12,718 EPOCH 2531
2024-02-01 19:06:26,964 Epoch 2531: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:06:26,965 EPOCH 2532
2024-02-01 19:06:41,087 Epoch 2532: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:06:41,087 EPOCH 2533
2024-02-01 19:06:54,902 Epoch 2533: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:06:54,903 EPOCH 2534
2024-02-01 19:06:56,130 [Epoch: 2534 Step: 00022800] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:     3130 || Batch Translation Loss:   0.015292 => Txt Tokens per Sec:     8060 || Lr: 0.000100
2024-02-01 19:07:08,882 Epoch 2534: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:07:08,883 EPOCH 2535
2024-02-01 19:07:22,868 Epoch 2535: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:07:22,868 EPOCH 2536
2024-02-01 19:07:36,784 Epoch 2536: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:07:36,784 EPOCH 2537
2024-02-01 19:07:50,937 Epoch 2537: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:07:50,938 EPOCH 2538
2024-02-01 19:08:04,962 Epoch 2538: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:08:04,963 EPOCH 2539
2024-02-01 19:08:18,421 Epoch 2539: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:08:18,422 EPOCH 2540
2024-02-01 19:08:32,381 Epoch 2540: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:08:32,381 EPOCH 2541
2024-02-01 19:08:46,036 Epoch 2541: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:08:46,037 EPOCH 2542
2024-02-01 19:08:59,998 Epoch 2542: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:08:59,998 EPOCH 2543
2024-02-01 19:09:13,826 Epoch 2543: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:09:13,827 EPOCH 2544
2024-02-01 19:09:27,581 Epoch 2544: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:09:27,581 EPOCH 2545
2024-02-01 19:09:35,925 [Epoch: 2545 Step: 00022900] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:      507 || Batch Translation Loss:   0.006943 => Txt Tokens per Sec:     1395 || Lr: 0.000100
2024-02-01 19:09:41,458 Epoch 2545: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:09:41,458 EPOCH 2546
2024-02-01 19:09:55,214 Epoch 2546: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:09:55,215 EPOCH 2547
2024-02-01 19:10:09,272 Epoch 2547: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:10:09,272 EPOCH 2548
2024-02-01 19:10:23,170 Epoch 2548: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:10:23,171 EPOCH 2549
2024-02-01 19:10:36,980 Epoch 2549: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:10:36,980 EPOCH 2550
2024-02-01 19:10:50,983 Epoch 2550: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:10:50,984 EPOCH 2551
2024-02-01 19:11:04,776 Epoch 2551: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 19:11:04,777 EPOCH 2552
2024-02-01 19:11:18,490 Epoch 2552: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:11:18,490 EPOCH 2553
2024-02-01 19:11:32,688 Epoch 2553: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:11:32,689 EPOCH 2554
2024-02-01 19:11:46,602 Epoch 2554: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:11:46,602 EPOCH 2555
2024-02-01 19:12:00,410 Epoch 2555: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:12:00,411 EPOCH 2556
2024-02-01 19:12:07,146 [Epoch: 2556 Step: 00023000] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:      950 || Batch Translation Loss:   0.014045 => Txt Tokens per Sec:     2568 || Lr: 0.000100
2024-02-01 19:12:14,434 Epoch 2556: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:12:14,435 EPOCH 2557
2024-02-01 19:12:28,257 Epoch 2557: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:12:28,258 EPOCH 2558
2024-02-01 19:12:42,278 Epoch 2558: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:12:42,279 EPOCH 2559
2024-02-01 19:12:56,551 Epoch 2559: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:12:56,552 EPOCH 2560
2024-02-01 19:13:10,163 Epoch 2560: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:13:10,164 EPOCH 2561
2024-02-01 19:13:24,266 Epoch 2561: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:13:24,267 EPOCH 2562
2024-02-01 19:13:38,120 Epoch 2562: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:13:38,121 EPOCH 2563
2024-02-01 19:13:52,034 Epoch 2563: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:13:52,034 EPOCH 2564
2024-02-01 19:14:05,880 Epoch 2564: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 19:14:05,881 EPOCH 2565
2024-02-01 19:14:20,177 Epoch 2565: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:14:20,177 EPOCH 2566
2024-02-01 19:14:34,369 Epoch 2566: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:14:34,370 EPOCH 2567
2024-02-01 19:14:44,495 [Epoch: 2567 Step: 00023100] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:      671 || Batch Translation Loss:   0.021824 => Txt Tokens per Sec:     1997 || Lr: 0.000100
2024-02-01 19:14:48,246 Epoch 2567: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:14:48,247 EPOCH 2568
2024-02-01 19:15:02,253 Epoch 2568: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:15:02,254 EPOCH 2569
2024-02-01 19:15:16,525 Epoch 2569: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:15:16,526 EPOCH 2570
2024-02-01 19:15:30,568 Epoch 2570: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:15:30,569 EPOCH 2571
2024-02-01 19:15:44,396 Epoch 2571: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:15:44,397 EPOCH 2572
2024-02-01 19:15:58,585 Epoch 2572: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:15:58,586 EPOCH 2573
2024-02-01 19:16:12,580 Epoch 2573: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:16:12,581 EPOCH 2574
2024-02-01 19:16:26,627 Epoch 2574: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:16:26,627 EPOCH 2575
2024-02-01 19:16:40,591 Epoch 2575: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:16:40,591 EPOCH 2576
2024-02-01 19:16:54,286 Epoch 2576: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:16:54,286 EPOCH 2577
2024-02-01 19:17:08,499 Epoch 2577: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 19:17:08,499 EPOCH 2578
2024-02-01 19:17:17,478 [Epoch: 2578 Step: 00023200] Batch Recognition Loss:   0.000117 => Gls Tokens per Sec:      899 || Batch Translation Loss:   0.008239 => Txt Tokens per Sec:     2398 || Lr: 0.000100
2024-02-01 19:17:22,493 Epoch 2578: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 19:17:22,494 EPOCH 2579
2024-02-01 19:17:36,244 Epoch 2579: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 19:17:36,244 EPOCH 2580
2024-02-01 19:17:49,877 Epoch 2580: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 19:17:49,877 EPOCH 2581
2024-02-01 19:18:03,954 Epoch 2581: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 19:18:03,954 EPOCH 2582
2024-02-01 19:18:18,209 Epoch 2582: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 19:18:18,210 EPOCH 2583
2024-02-01 19:18:32,183 Epoch 2583: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-01 19:18:32,183 EPOCH 2584
2024-02-01 19:18:46,084 Epoch 2584: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-01 19:18:46,084 EPOCH 2585
2024-02-01 19:19:00,249 Epoch 2585: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.76 
2024-02-01 19:19:00,249 EPOCH 2586
2024-02-01 19:19:14,177 Epoch 2586: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-01 19:19:14,178 EPOCH 2587
2024-02-01 19:19:27,948 Epoch 2587: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-01 19:19:27,949 EPOCH 2588
2024-02-01 19:19:41,842 Epoch 2588: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-01 19:19:41,843 EPOCH 2589
2024-02-01 19:19:51,980 [Epoch: 2589 Step: 00023300] Batch Recognition Loss:   0.000383 => Gls Tokens per Sec:      922 || Batch Translation Loss:   0.095416 => Txt Tokens per Sec:     2469 || Lr: 0.000100
2024-02-01 19:19:55,794 Epoch 2589: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-01 19:19:55,795 EPOCH 2590
2024-02-01 19:20:09,640 Epoch 2590: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-01 19:20:09,641 EPOCH 2591
2024-02-01 19:20:23,674 Epoch 2591: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-01 19:20:23,674 EPOCH 2592
2024-02-01 19:20:37,339 Epoch 2592: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-01 19:20:37,339 EPOCH 2593
2024-02-01 19:20:51,014 Epoch 2593: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-01 19:20:51,015 EPOCH 2594
2024-02-01 19:21:04,962 Epoch 2594: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-01 19:21:04,963 EPOCH 2595
2024-02-01 19:21:19,006 Epoch 2595: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-01 19:21:19,006 EPOCH 2596
2024-02-01 19:21:33,102 Epoch 2596: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-01 19:21:33,103 EPOCH 2597
2024-02-01 19:21:46,817 Epoch 2597: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-01 19:21:46,817 EPOCH 2598
2024-02-01 19:22:00,794 Epoch 2598: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 19:22:00,795 EPOCH 2599
2024-02-01 19:22:14,762 Epoch 2599: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-01 19:22:14,763 EPOCH 2600
2024-02-01 19:22:28,656 [Epoch: 2600 Step: 00023400] Batch Recognition Loss:   0.000254 => Gls Tokens per Sec:      765 || Batch Translation Loss:   0.020542 => Txt Tokens per Sec:     2124 || Lr: 0.000100
2024-02-01 19:22:28,656 Epoch 2600: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 19:22:28,657 EPOCH 2601
2024-02-01 19:22:42,658 Epoch 2601: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-01 19:22:42,659 EPOCH 2602
2024-02-01 19:22:56,223 Epoch 2602: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 19:22:56,223 EPOCH 2603
2024-02-01 19:23:10,157 Epoch 2603: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 19:23:10,157 EPOCH 2604
2024-02-01 19:23:24,165 Epoch 2604: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 19:23:24,165 EPOCH 2605
2024-02-01 19:23:38,035 Epoch 2605: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 19:23:38,036 EPOCH 2606
2024-02-01 19:23:51,773 Epoch 2606: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 19:23:51,774 EPOCH 2607
2024-02-01 19:24:05,796 Epoch 2607: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 19:24:05,797 EPOCH 2608
2024-02-01 19:24:19,881 Epoch 2608: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 19:24:19,882 EPOCH 2609
2024-02-01 19:24:33,670 Epoch 2609: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 19:24:33,671 EPOCH 2610
2024-02-01 19:24:47,666 Epoch 2610: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 19:24:47,666 EPOCH 2611
2024-02-01 19:25:01,357 Epoch 2611: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 19:25:01,358 EPOCH 2612
2024-02-01 19:25:01,844 [Epoch: 2612 Step: 00023500] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     2643 || Batch Translation Loss:   0.016181 => Txt Tokens per Sec:     6544 || Lr: 0.000100
2024-02-01 19:25:15,194 Epoch 2612: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 19:25:15,195 EPOCH 2613
2024-02-01 19:25:29,290 Epoch 2613: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 19:25:29,290 EPOCH 2614
2024-02-01 19:25:43,406 Epoch 2614: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 19:25:43,407 EPOCH 2615
2024-02-01 19:25:57,123 Epoch 2615: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 19:25:57,123 EPOCH 2616
2024-02-01 19:26:10,925 Epoch 2616: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 19:26:10,925 EPOCH 2617
2024-02-01 19:26:24,611 Epoch 2617: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 19:26:24,612 EPOCH 2618
2024-02-01 19:26:38,745 Epoch 2618: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 19:26:38,746 EPOCH 2619
2024-02-01 19:26:52,677 Epoch 2619: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 19:26:52,677 EPOCH 2620
2024-02-01 19:27:06,658 Epoch 2620: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:27:06,659 EPOCH 2621
2024-02-01 19:27:20,436 Epoch 2621: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 19:27:20,437 EPOCH 2622
2024-02-01 19:27:34,541 Epoch 2622: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 19:27:34,542 EPOCH 2623
2024-02-01 19:27:40,145 [Epoch: 2623 Step: 00023600] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:      457 || Batch Translation Loss:   0.026941 => Txt Tokens per Sec:     1488 || Lr: 0.000100
2024-02-01 19:27:48,758 Epoch 2623: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 19:27:48,758 EPOCH 2624
2024-02-01 19:28:02,828 Epoch 2624: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 19:28:02,829 EPOCH 2625
2024-02-01 19:28:16,709 Epoch 2625: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 19:28:16,710 EPOCH 2626
2024-02-01 19:28:30,590 Epoch 2626: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 19:28:30,591 EPOCH 2627
2024-02-01 19:28:44,500 Epoch 2627: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 19:28:44,500 EPOCH 2628
2024-02-01 19:28:58,154 Epoch 2628: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 19:28:58,155 EPOCH 2629
2024-02-01 19:29:12,091 Epoch 2629: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 19:29:12,092 EPOCH 2630
2024-02-01 19:29:26,137 Epoch 2630: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:29:26,138 EPOCH 2631
2024-02-01 19:29:40,211 Epoch 2631: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:29:40,211 EPOCH 2632
2024-02-01 19:29:54,516 Epoch 2632: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:29:54,516 EPOCH 2633
2024-02-01 19:30:08,262 Epoch 2633: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:30:08,263 EPOCH 2634
2024-02-01 19:30:13,814 [Epoch: 2634 Step: 00023700] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:      532 || Batch Translation Loss:   0.006925 => Txt Tokens per Sec:     1364 || Lr: 0.000100
2024-02-01 19:30:22,321 Epoch 2634: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:30:22,321 EPOCH 2635
2024-02-01 19:30:36,381 Epoch 2635: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:30:36,382 EPOCH 2636
2024-02-01 19:30:50,134 Epoch 2636: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:30:50,135 EPOCH 2637
2024-02-01 19:31:04,137 Epoch 2637: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:31:04,137 EPOCH 2638
2024-02-01 19:31:18,027 Epoch 2638: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:31:18,028 EPOCH 2639
2024-02-01 19:31:32,076 Epoch 2639: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:31:32,077 EPOCH 2640
2024-02-01 19:31:45,942 Epoch 2640: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 19:31:45,942 EPOCH 2641
2024-02-01 19:31:59,750 Epoch 2641: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 19:31:59,750 EPOCH 2642
2024-02-01 19:32:13,462 Epoch 2642: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 19:32:13,462 EPOCH 2643
2024-02-01 19:32:27,810 Epoch 2643: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 19:32:27,810 EPOCH 2644
2024-02-01 19:32:41,760 Epoch 2644: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 19:32:41,760 EPOCH 2645
2024-02-01 19:32:50,814 [Epoch: 2645 Step: 00023800] Batch Recognition Loss:   0.000120 => Gls Tokens per Sec:      467 || Batch Translation Loss:   0.008128 => Txt Tokens per Sec:     1328 || Lr: 0.000100
2024-02-01 19:32:55,648 Epoch 2645: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 19:32:55,649 EPOCH 2646
2024-02-01 19:33:09,522 Epoch 2646: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 19:33:09,523 EPOCH 2647
2024-02-01 19:33:23,200 Epoch 2647: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 19:33:23,201 EPOCH 2648
2024-02-01 19:33:36,879 Epoch 2648: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:33:36,880 EPOCH 2649
2024-02-01 19:33:50,826 Epoch 2649: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:33:50,826 EPOCH 2650
2024-02-01 19:34:04,982 Epoch 2650: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:34:04,982 EPOCH 2651
2024-02-01 19:34:18,421 Epoch 2651: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:34:18,422 EPOCH 2652
2024-02-01 19:34:32,148 Epoch 2652: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:34:32,149 EPOCH 2653
2024-02-01 19:34:46,295 Epoch 2653: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:34:46,295 EPOCH 2654
2024-02-01 19:35:00,361 Epoch 2654: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:35:00,362 EPOCH 2655
2024-02-01 19:35:14,096 Epoch 2655: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:35:14,097 EPOCH 2656
2024-02-01 19:35:23,068 [Epoch: 2656 Step: 00023900] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:      614 || Batch Translation Loss:   0.011215 => Txt Tokens per Sec:     1645 || Lr: 0.000100
2024-02-01 19:35:28,107 Epoch 2656: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:35:28,107 EPOCH 2657
2024-02-01 19:35:42,065 Epoch 2657: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 19:35:42,066 EPOCH 2658
2024-02-01 19:35:55,722 Epoch 2658: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 19:35:55,722 EPOCH 2659
2024-02-01 19:36:09,862 Epoch 2659: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 19:36:09,863 EPOCH 2660
2024-02-01 19:36:23,865 Epoch 2660: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 19:36:23,866 EPOCH 2661
2024-02-01 19:36:37,843 Epoch 2661: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 19:36:37,843 EPOCH 2662
2024-02-01 19:36:51,560 Epoch 2662: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 19:36:51,561 EPOCH 2663
2024-02-01 19:37:05,707 Epoch 2663: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 19:37:05,707 EPOCH 2664
2024-02-01 19:37:19,501 Epoch 2664: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 19:37:19,501 EPOCH 2665
2024-02-01 19:37:33,338 Epoch 2665: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 19:37:33,338 EPOCH 2666
2024-02-01 19:37:47,086 Epoch 2666: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 19:37:47,087 EPOCH 2667
2024-02-01 19:37:59,730 [Epoch: 2667 Step: 00024000] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:      537 || Batch Translation Loss:   0.033311 => Txt Tokens per Sec:     1571 || Lr: 0.000100
2024-02-01 19:38:19,305 Validation result at epoch 2667, step    24000: duration: 19.5743s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00017	Translation Loss: 96143.27344	PPL: 15079.05957
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.83	(BLEU-1: 11.51,	BLEU-2: 3.78,	BLEU-3: 1.61,	BLEU-4: 0.83)
	CHRF 17.32	ROUGE 9.93
2024-02-01 19:38:19,307 Logging Recognition and Translation Outputs
2024-02-01 19:38:19,307 ========================================================================================================================
2024-02-01 19:38:19,307 Logging Sequence: 169_165.00
2024-02-01 19:38:19,307 	Gloss Reference :	A B+C+D+E
2024-02-01 19:38:19,308 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 19:38:19,308 	Gloss Alignment :	         
2024-02-01 19:38:19,308 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 19:38:19,310 	Text Reference  :	** the indian government was       outraged by          the incident and  these changes were undone by         wikipedia
2024-02-01 19:38:19,310 	Text Hypothesis :	do you know   that       wikipedia provides information on  celebs   like their height  age  family background etc      
2024-02-01 19:38:19,310 	Text Alignment  :	I  S   S      S          S         S        S           S   S        S    S     S       S    S      S          S        
2024-02-01 19:38:19,310 ========================================================================================================================
2024-02-01 19:38:19,311 Logging Sequence: 175_60.00
2024-02-01 19:38:19,311 	Gloss Reference :	A B+C+D+E
2024-02-01 19:38:19,311 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 19:38:19,311 	Gloss Alignment :	         
2024-02-01 19:38:19,311 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 19:38:19,312 	Text Reference  :	******* that is    how    india bagged 9   medals in       the youth tournament
2024-02-01 19:38:19,312 	Text Hypothesis :	despite the  delay people were  all    out and    pakistan has been  made      
2024-02-01 19:38:19,312 	Text Alignment  :	I       S    S     S      S     S      S   S      S        S   S     S         
2024-02-01 19:38:19,312 ========================================================================================================================
2024-02-01 19:38:19,312 Logging Sequence: 61_255.00
2024-02-01 19:38:19,313 	Gloss Reference :	A B+C+D+E
2024-02-01 19:38:19,313 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 19:38:19,313 	Gloss Alignment :	         
2024-02-01 19:38:19,313 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 19:38:19,314 	Text Reference  :	**** *** in   2011 we    decided to marry and    informed our families
2024-02-01 19:38:19,314 	Text Hypothesis :	when rcb lost the  match fees    by the   couple were     all out     
2024-02-01 19:38:19,314 	Text Alignment  :	I    I   S    S    S     S       S  S     S      S        S   S       
2024-02-01 19:38:19,314 ========================================================================================================================
2024-02-01 19:38:19,314 Logging Sequence: 173_39.00
2024-02-01 19:38:19,314 	Gloss Reference :	A B+C+D+E
2024-02-01 19:38:19,315 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 19:38:19,315 	Gloss Alignment :	         
2024-02-01 19:38:19,315 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 19:38:19,315 	Text Reference  :	** *** ******* kohli will step      down as   india' captain
2024-02-01 19:38:19,315 	Text Hypothesis :	as two opening game  was  extremely fit  when the    captain
2024-02-01 19:38:19,316 	Text Alignment  :	I  I   I       S     S    S         S    S    S             
2024-02-01 19:38:19,316 ========================================================================================================================
2024-02-01 19:38:19,316 Logging Sequence: 172_82.00
2024-02-01 19:38:19,316 	Gloss Reference :	A B+C+D+E
2024-02-01 19:38:19,316 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 19:38:19,316 	Gloss Alignment :	         
2024-02-01 19:38:19,316 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 19:38:19,318 	Text Reference  :	you all know that the toss was about to start at 700 pm   but it   started raining at around 630 pm  
2024-02-01 19:38:19,318 	Text Hypothesis :	*** *** **** that *** **** *** ***** ** ***** ** *** time she said 'i      am      a  match  as  well
2024-02-01 19:38:19,318 	Text Alignment  :	D   D   D         D   D    D   D     D  D     D  D   S    S   S    S       S       S  S      S   S   
2024-02-01 19:38:19,318 ========================================================================================================================
2024-02-01 19:38:20,602 Epoch 2667: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 19:38:20,602 EPOCH 2668
2024-02-01 19:38:34,377 Epoch 2668: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 19:38:34,378 EPOCH 2669
2024-02-01 19:38:48,435 Epoch 2669: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 19:38:48,435 EPOCH 2670
2024-02-01 19:39:02,575 Epoch 2670: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-01 19:39:02,576 EPOCH 2671
2024-02-01 19:39:16,615 Epoch 2671: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.18 
2024-02-01 19:39:16,616 EPOCH 2672
2024-02-01 19:39:30,369 Epoch 2672: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.06 
2024-02-01 19:39:30,370 EPOCH 2673
2024-02-01 19:39:44,003 Epoch 2673: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.00 
2024-02-01 19:39:44,003 EPOCH 2674
2024-02-01 19:39:57,660 Epoch 2674: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.70 
2024-02-01 19:39:57,661 EPOCH 2675
2024-02-01 19:40:11,602 Epoch 2675: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.93 
2024-02-01 19:40:11,603 EPOCH 2676
2024-02-01 19:40:25,816 Epoch 2676: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.14 
2024-02-01 19:40:25,817 EPOCH 2677
2024-02-01 19:40:39,753 Epoch 2677: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.43 
2024-02-01 19:40:39,754 EPOCH 2678
2024-02-01 19:40:49,908 [Epoch: 2678 Step: 00024100] Batch Recognition Loss:   0.006642 => Gls Tokens per Sec:      795 || Batch Translation Loss:   0.347374 => Txt Tokens per Sec:     2134 || Lr: 0.000100
2024-02-01 19:40:53,705 Epoch 2678: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.41 
2024-02-01 19:40:53,706 EPOCH 2679
2024-02-01 19:41:07,866 Epoch 2679: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.56 
2024-02-01 19:41:07,866 EPOCH 2680
2024-02-01 19:41:21,826 Epoch 2680: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-01 19:41:21,827 EPOCH 2681
2024-02-01 19:41:35,674 Epoch 2681: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-01 19:41:35,675 EPOCH 2682
2024-02-01 19:41:49,363 Epoch 2682: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-01 19:41:49,364 EPOCH 2683
2024-02-01 19:42:03,344 Epoch 2683: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-01 19:42:03,344 EPOCH 2684
2024-02-01 19:42:17,440 Epoch 2684: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 19:42:17,440 EPOCH 2685
2024-02-01 19:42:31,186 Epoch 2685: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 19:42:31,187 EPOCH 2686
2024-02-01 19:42:45,028 Epoch 2686: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 19:42:45,028 EPOCH 2687
2024-02-01 19:42:59,291 Epoch 2687: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 19:42:59,292 EPOCH 2688
2024-02-01 19:43:13,307 Epoch 2688: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 19:43:13,308 EPOCH 2689
2024-02-01 19:43:26,853 [Epoch: 2689 Step: 00024200] Batch Recognition Loss:   0.000310 => Gls Tokens per Sec:      690 || Batch Translation Loss:   0.030315 => Txt Tokens per Sec:     1919 || Lr: 0.000100
2024-02-01 19:43:27,293 Epoch 2689: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 19:43:27,293 EPOCH 2690
2024-02-01 19:43:41,128 Epoch 2690: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 19:43:41,128 EPOCH 2691
2024-02-01 19:43:54,853 Epoch 2691: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 19:43:54,853 EPOCH 2692
2024-02-01 19:44:08,773 Epoch 2692: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 19:44:08,774 EPOCH 2693
2024-02-01 19:44:22,900 Epoch 2693: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 19:44:22,901 EPOCH 2694
2024-02-01 19:44:36,840 Epoch 2694: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 19:44:36,841 EPOCH 2695
2024-02-01 19:44:50,576 Epoch 2695: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 19:44:50,577 EPOCH 2696
2024-02-01 19:45:04,366 Epoch 2696: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 19:45:04,367 EPOCH 2697
2024-02-01 19:45:18,329 Epoch 2697: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 19:45:18,330 EPOCH 2698
2024-02-01 19:45:32,322 Epoch 2698: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 19:45:32,322 EPOCH 2699
2024-02-01 19:45:46,404 Epoch 2699: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 19:45:46,405 EPOCH 2700
2024-02-01 19:46:00,342 [Epoch: 2700 Step: 00024300] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:      763 || Batch Translation Loss:   0.008995 => Txt Tokens per Sec:     2117 || Lr: 0.000100
2024-02-01 19:46:00,343 Epoch 2700: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 19:46:00,343 EPOCH 2701
2024-02-01 19:46:14,283 Epoch 2701: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 19:46:14,284 EPOCH 2702
2024-02-01 19:46:28,404 Epoch 2702: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 19:46:28,404 EPOCH 2703
2024-02-01 19:46:42,243 Epoch 2703: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:46:42,244 EPOCH 2704
2024-02-01 19:46:55,957 Epoch 2704: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:46:55,958 EPOCH 2705
2024-02-01 19:47:10,032 Epoch 2705: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 19:47:10,033 EPOCH 2706
2024-02-01 19:47:23,565 Epoch 2706: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:47:23,565 EPOCH 2707
2024-02-01 19:47:37,664 Epoch 2707: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:47:37,664 EPOCH 2708
2024-02-01 19:47:51,563 Epoch 2708: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 19:47:51,564 EPOCH 2709
2024-02-01 19:48:05,705 Epoch 2709: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 19:48:05,706 EPOCH 2710
2024-02-01 19:48:19,518 Epoch 2710: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:48:19,518 EPOCH 2711
2024-02-01 19:48:33,783 Epoch 2711: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:48:33,784 EPOCH 2712
2024-02-01 19:48:36,847 [Epoch: 2712 Step: 00024400] Batch Recognition Loss:   0.000197 => Gls Tokens per Sec:      418 || Batch Translation Loss:   0.017542 => Txt Tokens per Sec:     1330 || Lr: 0.000100
2024-02-01 19:48:47,832 Epoch 2712: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 19:48:47,832 EPOCH 2713
2024-02-01 19:49:01,743 Epoch 2713: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:49:01,744 EPOCH 2714
2024-02-01 19:49:15,640 Epoch 2714: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:49:15,641 EPOCH 2715
2024-02-01 19:49:29,681 Epoch 2715: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:49:29,681 EPOCH 2716
2024-02-01 19:49:43,511 Epoch 2716: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:49:43,511 EPOCH 2717
2024-02-01 19:49:57,296 Epoch 2717: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:49:57,297 EPOCH 2718
2024-02-01 19:50:11,401 Epoch 2718: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:50:11,401 EPOCH 2719
2024-02-01 19:50:25,182 Epoch 2719: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:50:25,183 EPOCH 2720
2024-02-01 19:50:39,335 Epoch 2720: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:50:39,335 EPOCH 2721
2024-02-01 19:50:53,157 Epoch 2721: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 19:50:53,158 EPOCH 2722
2024-02-01 19:51:07,182 Epoch 2722: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:51:07,183 EPOCH 2723
2024-02-01 19:51:07,910 [Epoch: 2723 Step: 00024500] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:     3532 || Batch Translation Loss:   0.014023 => Txt Tokens per Sec:     9222 || Lr: 0.000100
2024-02-01 19:51:21,039 Epoch 2723: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 19:51:21,040 EPOCH 2724
2024-02-01 19:51:34,942 Epoch 2724: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 19:51:34,942 EPOCH 2725
2024-02-01 19:51:48,825 Epoch 2725: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:51:48,826 EPOCH 2726
2024-02-01 19:52:02,786 Epoch 2726: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:52:02,787 EPOCH 2727
2024-02-01 19:52:16,965 Epoch 2727: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:52:16,965 EPOCH 2728
2024-02-01 19:52:30,702 Epoch 2728: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:52:30,702 EPOCH 2729
2024-02-01 19:52:44,712 Epoch 2729: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 19:52:44,712 EPOCH 2730
2024-02-01 19:52:58,575 Epoch 2730: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 19:52:58,575 EPOCH 2731
2024-02-01 19:53:12,446 Epoch 2731: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 19:53:12,446 EPOCH 2732
2024-02-01 19:53:26,303 Epoch 2732: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 19:53:26,304 EPOCH 2733
2024-02-01 19:53:40,234 Epoch 2733: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 19:53:40,235 EPOCH 2734
2024-02-01 19:53:44,458 [Epoch: 2734 Step: 00024600] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:      910 || Batch Translation Loss:   0.014384 => Txt Tokens per Sec:     2518 || Lr: 0.000100
2024-02-01 19:53:54,171 Epoch 2734: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 19:53:54,171 EPOCH 2735
2024-02-01 19:54:08,049 Epoch 2735: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 19:54:08,049 EPOCH 2736
2024-02-01 19:54:22,136 Epoch 2736: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 19:54:22,137 EPOCH 2737
2024-02-01 19:54:35,798 Epoch 2737: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 19:54:35,799 EPOCH 2738
2024-02-01 19:54:49,712 Epoch 2738: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 19:54:49,713 EPOCH 2739
2024-02-01 19:55:03,807 Epoch 2739: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 19:55:03,807 EPOCH 2740
2024-02-01 19:55:17,565 Epoch 2740: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 19:55:17,566 EPOCH 2741
2024-02-01 19:55:31,655 Epoch 2741: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:55:31,656 EPOCH 2742
2024-02-01 19:55:45,367 Epoch 2742: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 19:55:45,367 EPOCH 2743
2024-02-01 19:55:59,305 Epoch 2743: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 19:55:59,305 EPOCH 2744
2024-02-01 19:56:13,225 Epoch 2744: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 19:56:13,226 EPOCH 2745
2024-02-01 19:56:20,550 [Epoch: 2745 Step: 00024700] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:      699 || Batch Translation Loss:   0.019546 => Txt Tokens per Sec:     1884 || Lr: 0.000100
2024-02-01 19:56:27,141 Epoch 2745: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 19:56:27,142 EPOCH 2746
2024-02-01 19:56:40,730 Epoch 2746: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 19:56:40,732 EPOCH 2747
2024-02-01 19:56:54,606 Epoch 2747: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 19:56:54,606 EPOCH 2748
2024-02-01 19:57:08,587 Epoch 2748: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:57:08,588 EPOCH 2749
2024-02-01 19:57:22,543 Epoch 2749: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:57:22,544 EPOCH 2750
2024-02-01 19:57:36,744 Epoch 2750: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:57:36,745 EPOCH 2751
2024-02-01 19:57:50,851 Epoch 2751: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:57:50,852 EPOCH 2752
2024-02-01 19:58:04,454 Epoch 2752: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:58:04,454 EPOCH 2753
2024-02-01 19:58:18,670 Epoch 2753: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:58:18,670 EPOCH 2754
2024-02-01 19:58:32,694 Epoch 2754: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:58:32,695 EPOCH 2755
2024-02-01 19:58:46,368 Epoch 2755: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:58:46,369 EPOCH 2756
2024-02-01 19:58:53,105 [Epoch: 2756 Step: 00024800] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:      950 || Batch Translation Loss:   0.012787 => Txt Tokens per Sec:     2668 || Lr: 0.000100
2024-02-01 19:59:00,330 Epoch 2756: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:59:00,330 EPOCH 2757
2024-02-01 19:59:14,700 Epoch 2757: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:59:14,700 EPOCH 2758
2024-02-01 19:59:28,353 Epoch 2758: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:59:28,354 EPOCH 2759
2024-02-01 19:59:42,271 Epoch 2759: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 19:59:42,272 EPOCH 2760
2024-02-01 19:59:56,071 Epoch 2760: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 19:59:56,072 EPOCH 2761
2024-02-01 20:00:10,159 Epoch 2761: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 20:00:10,160 EPOCH 2762
2024-02-01 20:00:23,849 Epoch 2762: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 20:00:23,849 EPOCH 2763
2024-02-01 20:00:37,567 Epoch 2763: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 20:00:37,568 EPOCH 2764
2024-02-01 20:00:51,612 Epoch 2764: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:00:51,613 EPOCH 2765
2024-02-01 20:01:05,942 Epoch 2765: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 20:01:05,943 EPOCH 2766
2024-02-01 20:01:20,050 Epoch 2766: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 20:01:20,051 EPOCH 2767
2024-02-01 20:01:29,007 [Epoch: 2767 Step: 00024900] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:      758 || Batch Translation Loss:   0.007519 => Txt Tokens per Sec:     2019 || Lr: 0.000100
2024-02-01 20:01:33,724 Epoch 2767: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 20:01:33,724 EPOCH 2768
2024-02-01 20:01:47,762 Epoch 2768: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 20:01:47,763 EPOCH 2769
2024-02-01 20:02:01,721 Epoch 2769: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 20:02:01,722 EPOCH 2770
2024-02-01 20:02:15,602 Epoch 2770: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 20:02:15,602 EPOCH 2771
2024-02-01 20:02:29,577 Epoch 2771: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 20:02:29,578 EPOCH 2772
2024-02-01 20:02:43,519 Epoch 2772: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 20:02:43,520 EPOCH 2773
2024-02-01 20:02:57,343 Epoch 2773: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:02:57,344 EPOCH 2774
2024-02-01 20:03:10,998 Epoch 2774: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:03:10,998 EPOCH 2775
2024-02-01 20:03:25,130 Epoch 2775: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 20:03:25,131 EPOCH 2776
2024-02-01 20:03:39,125 Epoch 2776: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:03:39,125 EPOCH 2777
2024-02-01 20:03:53,461 Epoch 2777: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 20:03:53,461 EPOCH 2778
2024-02-01 20:04:03,899 [Epoch: 2778 Step: 00025000] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:      859 || Batch Translation Loss:   0.025998 => Txt Tokens per Sec:     2475 || Lr: 0.000100
2024-02-01 20:04:07,474 Epoch 2778: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 20:04:07,474 EPOCH 2779
2024-02-01 20:04:21,724 Epoch 2779: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 20:04:21,725 EPOCH 2780
2024-02-01 20:04:35,813 Epoch 2780: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 20:04:35,814 EPOCH 2781
2024-02-01 20:04:49,630 Epoch 2781: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 20:04:49,631 EPOCH 2782
2024-02-01 20:05:03,402 Epoch 2782: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 20:05:03,403 EPOCH 2783
2024-02-01 20:05:17,407 Epoch 2783: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:05:17,408 EPOCH 2784
2024-02-01 20:05:31,326 Epoch 2784: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 20:05:31,326 EPOCH 2785
2024-02-01 20:05:45,012 Epoch 2785: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 20:05:45,013 EPOCH 2786
2024-02-01 20:05:58,963 Epoch 2786: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 20:05:58,964 EPOCH 2787
2024-02-01 20:06:12,837 Epoch 2787: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 20:06:12,837 EPOCH 2788
2024-02-01 20:06:26,544 Epoch 2788: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:06:26,544 EPOCH 2789
2024-02-01 20:06:36,658 [Epoch: 2789 Step: 00025100] Batch Recognition Loss:   0.000116 => Gls Tokens per Sec:      925 || Batch Translation Loss:   0.020449 => Txt Tokens per Sec:     2474 || Lr: 0.000100
2024-02-01 20:06:40,231 Epoch 2789: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:06:40,231 EPOCH 2790
2024-02-01 20:06:53,879 Epoch 2790: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 20:06:53,880 EPOCH 2791
2024-02-01 20:07:07,932 Epoch 2791: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 20:07:07,933 EPOCH 2792
2024-02-01 20:07:21,838 Epoch 2792: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 20:07:21,839 EPOCH 2793
2024-02-01 20:07:35,823 Epoch 2793: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 20:07:35,823 EPOCH 2794
2024-02-01 20:07:49,747 Epoch 2794: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 20:07:49,748 EPOCH 2795
2024-02-01 20:08:03,516 Epoch 2795: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 20:08:03,516 EPOCH 2796
2024-02-01 20:08:17,310 Epoch 2796: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 20:08:17,311 EPOCH 2797
2024-02-01 20:08:31,437 Epoch 2797: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 20:08:31,437 EPOCH 2798
2024-02-01 20:08:45,661 Epoch 2798: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-01 20:08:45,661 EPOCH 2799
2024-02-01 20:08:59,386 Epoch 2799: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 20:08:59,386 EPOCH 2800
2024-02-01 20:09:13,347 [Epoch: 2800 Step: 00025200] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:      762 || Batch Translation Loss:   0.085832 => Txt Tokens per Sec:     2114 || Lr: 0.000100
2024-02-01 20:09:13,348 Epoch 2800: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 20:09:13,348 EPOCH 2801
2024-02-01 20:09:27,019 Epoch 2801: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 20:09:27,020 EPOCH 2802
2024-02-01 20:09:41,152 Epoch 2802: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-01 20:09:41,153 EPOCH 2803
2024-02-01 20:09:55,073 Epoch 2803: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-01 20:09:55,074 EPOCH 2804
2024-02-01 20:10:08,936 Epoch 2804: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.87 
2024-02-01 20:10:08,937 EPOCH 2805
2024-02-01 20:10:22,990 Epoch 2805: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.90 
2024-02-01 20:10:22,990 EPOCH 2806
2024-02-01 20:10:36,927 Epoch 2806: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.57 
2024-02-01 20:10:36,928 EPOCH 2807
2024-02-01 20:10:50,974 Epoch 2807: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-01 20:10:50,975 EPOCH 2808
2024-02-01 20:11:05,086 Epoch 2808: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-01 20:11:05,086 EPOCH 2809
2024-02-01 20:11:19,091 Epoch 2809: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.02 
2024-02-01 20:11:19,091 EPOCH 2810
2024-02-01 20:11:32,797 Epoch 2810: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.92 
2024-02-01 20:11:32,798 EPOCH 2811
2024-02-01 20:11:46,899 Epoch 2811: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-01 20:11:46,899 EPOCH 2812
2024-02-01 20:11:49,914 [Epoch: 2812 Step: 00025300] Batch Recognition Loss:   0.000560 => Gls Tokens per Sec:      425 || Batch Translation Loss:   0.071802 => Txt Tokens per Sec:     1355 || Lr: 0.000100
2024-02-01 20:12:00,770 Epoch 2812: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-01 20:12:00,771 EPOCH 2813
2024-02-01 20:12:14,705 Epoch 2813: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-01 20:12:14,705 EPOCH 2814
2024-02-01 20:12:28,730 Epoch 2814: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-01 20:12:28,731 EPOCH 2815
2024-02-01 20:12:42,896 Epoch 2815: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 20:12:42,896 EPOCH 2816
2024-02-01 20:12:56,748 Epoch 2816: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 20:12:56,748 EPOCH 2817
2024-02-01 20:13:10,605 Epoch 2817: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 20:13:10,606 EPOCH 2818
2024-02-01 20:13:24,514 Epoch 2818: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 20:13:24,514 EPOCH 2819
2024-02-01 20:13:38,437 Epoch 2819: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 20:13:38,437 EPOCH 2820
2024-02-01 20:13:52,345 Epoch 2820: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 20:13:52,346 EPOCH 2821
2024-02-01 20:14:06,328 Epoch 2821: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 20:14:06,328 EPOCH 2822
2024-02-01 20:14:20,064 Epoch 2822: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 20:14:20,065 EPOCH 2823
2024-02-01 20:14:23,720 [Epoch: 2823 Step: 00025400] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:      457 || Batch Translation Loss:   0.011279 => Txt Tokens per Sec:     1181 || Lr: 0.000100
2024-02-01 20:14:34,249 Epoch 2823: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 20:14:34,250 EPOCH 2824
2024-02-01 20:14:48,241 Epoch 2824: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 20:14:48,242 EPOCH 2825
2024-02-01 20:15:02,096 Epoch 2825: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 20:15:02,096 EPOCH 2826
2024-02-01 20:15:16,160 Epoch 2826: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 20:15:16,161 EPOCH 2827
2024-02-01 20:15:30,105 Epoch 2827: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 20:15:30,106 EPOCH 2828
2024-02-01 20:15:43,786 Epoch 2828: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 20:15:43,787 EPOCH 2829
2024-02-01 20:15:57,823 Epoch 2829: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 20:15:57,824 EPOCH 2830
2024-02-01 20:16:11,819 Epoch 2830: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 20:16:11,819 EPOCH 2831
2024-02-01 20:16:25,597 Epoch 2831: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 20:16:25,598 EPOCH 2832
2024-02-01 20:16:39,452 Epoch 2832: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 20:16:39,453 EPOCH 2833
2024-02-01 20:16:53,417 Epoch 2833: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 20:16:53,418 EPOCH 2834
2024-02-01 20:16:59,205 [Epoch: 2834 Step: 00025500] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:      664 || Batch Translation Loss:   0.008761 => Txt Tokens per Sec:     1830 || Lr: 0.000100
2024-02-01 20:17:07,460 Epoch 2834: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 20:17:07,460 EPOCH 2835
2024-02-01 20:17:21,378 Epoch 2835: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:17:21,379 EPOCH 2836
2024-02-01 20:17:35,298 Epoch 2836: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 20:17:35,299 EPOCH 2837
2024-02-01 20:17:49,156 Epoch 2837: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-01 20:17:49,157 EPOCH 2838
2024-02-01 20:18:03,416 Epoch 2838: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-01 20:18:03,417 EPOCH 2839
2024-02-01 20:18:17,313 Epoch 2839: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-01 20:18:17,313 EPOCH 2840
2024-02-01 20:18:31,140 Epoch 2840: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 20:18:31,140 EPOCH 2841
2024-02-01 20:18:45,054 Epoch 2841: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-01 20:18:45,055 EPOCH 2842
2024-02-01 20:18:59,129 Epoch 2842: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 20:18:59,129 EPOCH 2843
2024-02-01 20:19:12,857 Epoch 2843: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 20:19:12,858 EPOCH 2844
2024-02-01 20:19:26,693 Epoch 2844: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 20:19:26,694 EPOCH 2845
2024-02-01 20:19:31,331 [Epoch: 2845 Step: 00025600] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:      912 || Batch Translation Loss:   0.029002 => Txt Tokens per Sec:     2499 || Lr: 0.000100
2024-02-01 20:19:40,546 Epoch 2845: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 20:19:40,547 EPOCH 2846
2024-02-01 20:19:54,076 Epoch 2846: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 20:19:54,077 EPOCH 2847
2024-02-01 20:20:08,115 Epoch 2847: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 20:20:08,116 EPOCH 2848
2024-02-01 20:20:22,086 Epoch 2848: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 20:20:22,086 EPOCH 2849
2024-02-01 20:20:36,062 Epoch 2849: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:20:36,062 EPOCH 2850
2024-02-01 20:20:50,023 Epoch 2850: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 20:20:50,024 EPOCH 2851
2024-02-01 20:21:03,692 Epoch 2851: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:21:03,693 EPOCH 2852
2024-02-01 20:21:18,135 Epoch 2852: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 20:21:18,136 EPOCH 2853
2024-02-01 20:21:31,883 Epoch 2853: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 20:21:31,883 EPOCH 2854
2024-02-01 20:21:45,728 Epoch 2854: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 20:21:45,729 EPOCH 2855
2024-02-01 20:21:59,782 Epoch 2855: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 20:21:59,782 EPOCH 2856
2024-02-01 20:22:04,072 [Epoch: 2856 Step: 00025700] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     1492 || Batch Translation Loss:   0.007244 => Txt Tokens per Sec:     3739 || Lr: 0.000100
2024-02-01 20:22:13,460 Epoch 2856: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:22:13,461 EPOCH 2857
2024-02-01 20:22:27,576 Epoch 2857: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:22:27,577 EPOCH 2858
2024-02-01 20:22:41,280 Epoch 2858: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 20:22:41,281 EPOCH 2859
2024-02-01 20:22:55,185 Epoch 2859: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 20:22:55,186 EPOCH 2860
2024-02-01 20:23:09,171 Epoch 2860: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 20:23:09,171 EPOCH 2861
2024-02-01 20:23:23,133 Epoch 2861: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:23:23,133 EPOCH 2862
2024-02-01 20:23:37,033 Epoch 2862: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 20:23:37,033 EPOCH 2863
2024-02-01 20:23:50,682 Epoch 2863: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:23:50,682 EPOCH 2864
2024-02-01 20:24:04,542 Epoch 2864: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 20:24:04,543 EPOCH 2865
2024-02-01 20:24:18,396 Epoch 2865: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:24:18,396 EPOCH 2866
2024-02-01 20:24:32,234 Epoch 2866: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:24:32,235 EPOCH 2867
2024-02-01 20:24:42,011 [Epoch: 2867 Step: 00025800] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:      786 || Batch Translation Loss:   0.008461 => Txt Tokens per Sec:     2195 || Lr: 0.000100
2024-02-01 20:24:46,261 Epoch 2867: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:24:46,261 EPOCH 2868
2024-02-01 20:25:00,091 Epoch 2868: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 20:25:00,091 EPOCH 2869
2024-02-01 20:25:14,197 Epoch 2869: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:25:14,197 EPOCH 2870
2024-02-01 20:25:28,445 Epoch 2870: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:25:28,445 EPOCH 2871
2024-02-01 20:25:42,442 Epoch 2871: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 20:25:42,443 EPOCH 2872
2024-02-01 20:25:56,117 Epoch 2872: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:25:56,118 EPOCH 2873
2024-02-01 20:26:10,170 Epoch 2873: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 20:26:10,170 EPOCH 2874
2024-02-01 20:26:24,482 Epoch 2874: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 20:26:24,482 EPOCH 2875
2024-02-01 20:26:38,333 Epoch 2875: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 20:26:38,333 EPOCH 2876
2024-02-01 20:26:52,368 Epoch 2876: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 20:26:52,368 EPOCH 2877
2024-02-01 20:27:06,066 Epoch 2877: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-01 20:27:06,066 EPOCH 2878
2024-02-01 20:27:15,813 [Epoch: 2878 Step: 00025900] Batch Recognition Loss:   0.000368 => Gls Tokens per Sec:      919 || Batch Translation Loss:   0.062979 => Txt Tokens per Sec:     2495 || Lr: 0.000100
2024-02-01 20:27:19,778 Epoch 2878: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.81 
2024-02-01 20:27:19,779 EPOCH 2879
2024-02-01 20:27:33,938 Epoch 2879: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-01 20:27:33,939 EPOCH 2880
2024-02-01 20:27:48,059 Epoch 2880: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-01 20:27:48,059 EPOCH 2881
2024-02-01 20:28:02,094 Epoch 2881: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 20:28:02,094 EPOCH 2882
2024-02-01 20:28:16,009 Epoch 2882: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 20:28:16,010 EPOCH 2883
2024-02-01 20:28:29,771 Epoch 2883: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 20:28:29,772 EPOCH 2884
2024-02-01 20:28:43,787 Epoch 2884: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 20:28:43,787 EPOCH 2885
2024-02-01 20:28:57,662 Epoch 2885: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 20:28:57,663 EPOCH 2886
2024-02-01 20:29:11,701 Epoch 2886: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 20:29:11,701 EPOCH 2887
2024-02-01 20:29:25,567 Epoch 2887: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-01 20:29:25,567 EPOCH 2888
2024-02-01 20:29:39,464 Epoch 2888: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 20:29:39,465 EPOCH 2889
2024-02-01 20:29:51,778 [Epoch: 2889 Step: 00026000] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:      759 || Batch Translation Loss:   0.029482 => Txt Tokens per Sec:     2080 || Lr: 0.000100
2024-02-01 20:30:11,286 Validation result at epoch 2889, step    26000: duration: 19.5064s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00018	Translation Loss: 98434.63281	PPL: 18965.16211
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.77	(BLEU-1: 11.66,	BLEU-2: 3.81,	BLEU-3: 1.62,	BLEU-4: 0.77)
	CHRF 17.48	ROUGE 10.09
2024-02-01 20:30:11,287 Logging Recognition and Translation Outputs
2024-02-01 20:30:11,287 ========================================================================================================================
2024-02-01 20:30:11,287 Logging Sequence: 130_139.00
2024-02-01 20:30:11,287 	Gloss Reference :	A B+C+D+E
2024-02-01 20:30:11,287 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 20:30:11,288 	Gloss Alignment :	         
2024-02-01 20:30:11,288 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 20:30:11,291 	Text Reference  :	he shared a ***** picture of   a  little pouch he  knit   for his olympic gold medal with uk flag on    one side  and  japanese flag on the  other
2024-02-01 20:30:11,292 	Text Hypothesis :	he played a diver but     what 10 2021   and   the reason for *** ******* **** ***** **** ** the  comes to  india when the      2012 to when it   
2024-02-01 20:30:11,292 	Text Alignment  :	   S        I     S       S    S  S      S     S   S          D   D       D    D     D    D  S    S     S   S     S    S        S    S  S    S    
2024-02-01 20:30:11,292 ========================================================================================================================
2024-02-01 20:30:11,292 Logging Sequence: 72_194.00
2024-02-01 20:30:11,292 	Gloss Reference :	A B+C+D+E
2024-02-01 20:30:11,292 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 20:30:11,292 	Gloss Alignment :	         
2024-02-01 20:30:11,293 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 20:30:11,294 	Text Reference  :	shah told her to do    what she wants and filed a    police complaint against her 
2024-02-01 20:30:11,294 	Text Hypothesis :	**** **** *** ** babar kept me  tell  you all   know in     the       same    team
2024-02-01 20:30:11,294 	Text Alignment  :	D    D    D   D  S     S    S   S     S   S     S    S      S         S       S   
2024-02-01 20:30:11,294 ========================================================================================================================
2024-02-01 20:30:11,294 Logging Sequence: 69_177.00
2024-02-01 20:30:11,294 	Gloss Reference :	A B+C+D+E
2024-02-01 20:30:11,294 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 20:30:11,295 	Gloss Alignment :	         
2024-02-01 20:30:11,295 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 20:30:11,296 	Text Reference  :	he said 'i will continue playing i        know it's  about time    i   retire i   also    have a     knee condition
2024-02-01 20:30:11,296 	Text Hypothesis :	** **** ** when csk      have    captured the  first 9     minutes the score  was delayed the  match in   mumbai   
2024-02-01 20:30:11,297 	Text Alignment  :	D  D    D  S    S        S       S        S    S     S     S       S   S      S   S       S    S     S    S        
2024-02-01 20:30:11,297 ========================================================================================================================
2024-02-01 20:30:11,297 Logging Sequence: 95_118.00
2024-02-01 20:30:11,297 	Gloss Reference :	A B+C+D+E
2024-02-01 20:30:11,297 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 20:30:11,297 	Gloss Alignment :	         
2024-02-01 20:30:11,297 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 20:30:11,298 	Text Reference  :	******* ** ***** ******** ******** ** the ***** ******* game was  stopped strangely due    to excessive sunlight
2024-02-01 20:30:11,298 	Text Hypothesis :	however an undue incident occurred on the pitch scoring 81   runs for     all       thanks to ********* the     
2024-02-01 20:30:11,299 	Text Alignment  :	I       I  I     I        I        I      I     I       S    S    S       S         S         D         S       
2024-02-01 20:30:11,299 ========================================================================================================================
2024-02-01 20:30:11,299 Logging Sequence: 112_8.00
2024-02-01 20:30:11,299 	Gloss Reference :	A B+C+D+E
2024-02-01 20:30:11,299 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 20:30:11,299 	Gloss Alignment :	         
2024-02-01 20:30:11,299 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 20:30:11,301 	Text Reference  :	before there were 8 teams such as mumbai indians delhi capitals  punjab kings etc and  now there will be  10   teams in   2022     
2024-02-01 20:30:11,301 	Text Hypothesis :	****** ***** **** * ***** **** ** ****** but     you   practised very   hard  to  know in  the   ipl  the bcci has   been postponed
2024-02-01 20:30:11,301 	Text Alignment  :	D      D     D    D D     D    D  D      S       S     S         S      S     S   S    S   S     S    S   S    S     S    S        
2024-02-01 20:30:11,302 ========================================================================================================================
2024-02-01 20:30:13,166 Epoch 2889: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 20:30:13,166 EPOCH 2890
2024-02-01 20:30:27,261 Epoch 2890: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 20:30:27,262 EPOCH 2891
2024-02-01 20:30:41,072 Epoch 2891: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 20:30:41,073 EPOCH 2892
2024-02-01 20:30:54,648 Epoch 2892: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 20:30:54,649 EPOCH 2893
2024-02-01 20:31:08,497 Epoch 2893: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-01 20:31:08,498 EPOCH 2894
2024-02-01 20:31:22,425 Epoch 2894: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-01 20:31:22,425 EPOCH 2895
2024-02-01 20:31:36,564 Epoch 2895: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-01 20:31:36,565 EPOCH 2896
2024-02-01 20:31:50,516 Epoch 2896: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-01 20:31:50,516 EPOCH 2897
2024-02-01 20:32:04,480 Epoch 2897: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.83 
2024-02-01 20:32:04,480 EPOCH 2898
2024-02-01 20:32:18,247 Epoch 2898: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.97 
2024-02-01 20:32:18,248 EPOCH 2899
2024-02-01 20:32:32,310 Epoch 2899: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-01 20:32:32,311 EPOCH 2900
2024-02-01 20:32:46,398 [Epoch: 2900 Step: 00026100] Batch Recognition Loss:   0.000571 => Gls Tokens per Sec:      755 || Batch Translation Loss:   0.286708 => Txt Tokens per Sec:     2095 || Lr: 0.000100
2024-02-01 20:32:46,398 Epoch 2900: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-01 20:32:46,398 EPOCH 2901
2024-02-01 20:33:00,537 Epoch 2901: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.70 
2024-02-01 20:33:00,538 EPOCH 2902
2024-02-01 20:33:14,283 Epoch 2902: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-01 20:33:14,284 EPOCH 2903
2024-02-01 20:33:28,319 Epoch 2903: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-01 20:33:28,320 EPOCH 2904
2024-02-01 20:33:42,449 Epoch 2904: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-01 20:33:42,449 EPOCH 2905
2024-02-01 20:33:56,577 Epoch 2905: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-01 20:33:56,578 EPOCH 2906
2024-02-01 20:34:10,525 Epoch 2906: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-01 20:34:10,526 EPOCH 2907
2024-02-01 20:34:24,655 Epoch 2907: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-01 20:34:24,656 EPOCH 2908
2024-02-01 20:34:38,684 Epoch 2908: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-01 20:34:38,684 EPOCH 2909
2024-02-01 20:34:52,448 Epoch 2909: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 20:34:52,449 EPOCH 2910
2024-02-01 20:35:06,342 Epoch 2910: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-01 20:35:06,343 EPOCH 2911
2024-02-01 20:35:20,478 Epoch 2911: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 20:35:20,479 EPOCH 2912
2024-02-01 20:35:20,863 [Epoch: 2912 Step: 00026200] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     3333 || Batch Translation Loss:   0.027294 => Txt Tokens per Sec:     8312 || Lr: 0.000100
2024-02-01 20:35:34,708 Epoch 2912: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-01 20:35:34,709 EPOCH 2913
2024-02-01 20:35:48,998 Epoch 2913: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 20:35:48,999 EPOCH 2914
2024-02-01 20:36:03,180 Epoch 2914: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 20:36:03,181 EPOCH 2915
2024-02-01 20:36:17,261 Epoch 2915: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 20:36:17,261 EPOCH 2916
2024-02-01 20:36:30,988 Epoch 2916: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 20:36:30,988 EPOCH 2917
2024-02-01 20:36:45,268 Epoch 2917: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 20:36:45,269 EPOCH 2918
2024-02-01 20:36:59,395 Epoch 2918: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 20:36:59,395 EPOCH 2919
2024-02-01 20:37:13,611 Epoch 2919: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 20:37:13,612 EPOCH 2920
2024-02-01 20:37:27,815 Epoch 2920: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 20:37:27,815 EPOCH 2921
2024-02-01 20:37:41,631 Epoch 2921: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-01 20:37:41,632 EPOCH 2922
2024-02-01 20:37:55,769 Epoch 2922: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.84 
2024-02-01 20:37:55,770 EPOCH 2923
2024-02-01 20:37:56,307 [Epoch: 2923 Step: 00026300] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     4773 || Batch Translation Loss:   0.261432 => Txt Tokens per Sec:    10167 || Lr: 0.000100
2024-02-01 20:38:09,589 Epoch 2923: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.86 
2024-02-01 20:38:09,590 EPOCH 2924
2024-02-01 20:38:23,696 Epoch 2924: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.99 
2024-02-01 20:38:23,696 EPOCH 2925
2024-02-01 20:38:37,692 Epoch 2925: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.97 
2024-02-01 20:38:37,692 EPOCH 2926
2024-02-01 20:38:52,012 Epoch 2926: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-01 20:38:52,013 EPOCH 2927
2024-02-01 20:39:05,917 Epoch 2927: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-01 20:39:05,918 EPOCH 2928
2024-02-01 20:39:19,713 Epoch 2928: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-01 20:39:19,713 EPOCH 2929
2024-02-01 20:39:33,718 Epoch 2929: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-01 20:39:33,719 EPOCH 2930
2024-02-01 20:39:47,491 Epoch 2930: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-01 20:39:47,492 EPOCH 2931
2024-02-01 20:40:01,490 Epoch 2931: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 20:40:01,491 EPOCH 2932
2024-02-01 20:40:15,542 Epoch 2932: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-01 20:40:15,542 EPOCH 2933
2024-02-01 20:40:29,596 Epoch 2933: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 20:40:29,597 EPOCH 2934
2024-02-01 20:40:33,716 [Epoch: 2934 Step: 00026400] Batch Recognition Loss:   0.000210 => Gls Tokens per Sec:      716 || Batch Translation Loss:   0.023167 => Txt Tokens per Sec:     1966 || Lr: 0.000100
2024-02-01 20:40:43,377 Epoch 2934: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 20:40:43,378 EPOCH 2935
2024-02-01 20:40:57,448 Epoch 2935: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 20:40:57,449 EPOCH 2936
2024-02-01 20:41:11,156 Epoch 2936: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 20:41:11,157 EPOCH 2937
2024-02-01 20:41:25,286 Epoch 2937: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 20:41:25,286 EPOCH 2938
2024-02-01 20:41:39,105 Epoch 2938: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 20:41:39,106 EPOCH 2939
2024-02-01 20:41:53,296 Epoch 2939: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 20:41:53,296 EPOCH 2940
2024-02-01 20:42:07,408 Epoch 2940: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 20:42:07,408 EPOCH 2941
2024-02-01 20:42:21,197 Epoch 2941: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 20:42:21,198 EPOCH 2942
2024-02-01 20:42:35,178 Epoch 2942: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 20:42:35,178 EPOCH 2943
2024-02-01 20:42:49,024 Epoch 2943: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 20:42:49,025 EPOCH 2944
2024-02-01 20:43:02,924 Epoch 2944: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 20:43:02,924 EPOCH 2945
2024-02-01 20:43:11,817 [Epoch: 2945 Step: 00026500] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:      576 || Batch Translation Loss:   0.020122 => Txt Tokens per Sec:     1758 || Lr: 0.000100
2024-02-01 20:43:17,023 Epoch 2945: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:43:17,023 EPOCH 2946
2024-02-01 20:43:31,175 Epoch 2946: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 20:43:31,175 EPOCH 2947
2024-02-01 20:43:44,947 Epoch 2947: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:43:44,947 EPOCH 2948
2024-02-01 20:43:58,623 Epoch 2948: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:43:58,624 EPOCH 2949
2024-02-01 20:44:12,546 Epoch 2949: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:44:12,547 EPOCH 2950
2024-02-01 20:44:26,117 Epoch 2950: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 20:44:26,118 EPOCH 2951
2024-02-01 20:44:40,028 Epoch 2951: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:44:40,029 EPOCH 2952
2024-02-01 20:44:53,884 Epoch 2952: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:44:53,885 EPOCH 2953
2024-02-01 20:45:07,600 Epoch 2953: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 20:45:07,601 EPOCH 2954
2024-02-01 20:45:21,591 Epoch 2954: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 20:45:21,591 EPOCH 2955
2024-02-01 20:45:35,321 Epoch 2955: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 20:45:35,321 EPOCH 2956
2024-02-01 20:45:42,127 [Epoch: 2956 Step: 00026600] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:      940 || Batch Translation Loss:   0.013551 => Txt Tokens per Sec:     2765 || Lr: 0.000100
2024-02-01 20:45:49,132 Epoch 2956: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 20:45:49,132 EPOCH 2957
2024-02-01 20:46:02,965 Epoch 2957: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 20:46:02,965 EPOCH 2958
2024-02-01 20:46:16,836 Epoch 2958: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 20:46:16,836 EPOCH 2959
2024-02-01 20:46:30,830 Epoch 2959: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 20:46:30,831 EPOCH 2960
2024-02-01 20:46:44,471 Epoch 2960: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 20:46:44,472 EPOCH 2961
2024-02-01 20:46:58,468 Epoch 2961: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 20:46:58,468 EPOCH 2962
2024-02-01 20:47:12,271 Epoch 2962: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 20:47:12,272 EPOCH 2963
2024-02-01 20:47:26,186 Epoch 2963: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 20:47:26,186 EPOCH 2964
2024-02-01 20:47:40,243 Epoch 2964: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:47:40,243 EPOCH 2965
2024-02-01 20:47:53,743 Epoch 2965: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 20:47:53,743 EPOCH 2966
2024-02-01 20:48:07,795 Epoch 2966: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 20:48:07,796 EPOCH 2967
2024-02-01 20:48:19,199 [Epoch: 2967 Step: 00026700] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:      596 || Batch Translation Loss:   0.025034 => Txt Tokens per Sec:     1659 || Lr: 0.000100
2024-02-01 20:48:21,976 Epoch 2967: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 20:48:21,976 EPOCH 2968
2024-02-01 20:48:35,920 Epoch 2968: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 20:48:35,921 EPOCH 2969
2024-02-01 20:48:49,621 Epoch 2969: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 20:48:49,621 EPOCH 2970
2024-02-01 20:49:03,719 Epoch 2970: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 20:49:03,720 EPOCH 2971
2024-02-01 20:49:17,754 Epoch 2971: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 20:49:17,755 EPOCH 2972
2024-02-01 20:49:31,644 Epoch 2972: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 20:49:31,645 EPOCH 2973
2024-02-01 20:49:45,647 Epoch 2973: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 20:49:45,647 EPOCH 2974
2024-02-01 20:49:59,399 Epoch 2974: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 20:49:59,400 EPOCH 2975
2024-02-01 20:50:13,393 Epoch 2975: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 20:50:13,394 EPOCH 2976
2024-02-01 20:50:27,063 Epoch 2976: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 20:50:27,063 EPOCH 2977
2024-02-01 20:50:40,899 Epoch 2977: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 20:50:40,900 EPOCH 2978
2024-02-01 20:50:50,778 [Epoch: 2978 Step: 00026800] Batch Recognition Loss:   0.000110 => Gls Tokens per Sec:      817 || Batch Translation Loss:   0.013086 => Txt Tokens per Sec:     2303 || Lr: 0.000100
2024-02-01 20:50:54,571 Epoch 2978: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 20:50:54,571 EPOCH 2979
2024-02-01 20:51:08,430 Epoch 2979: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 20:51:08,431 EPOCH 2980
2024-02-01 20:51:22,543 Epoch 2980: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:51:22,544 EPOCH 2981
2024-02-01 20:51:36,584 Epoch 2981: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 20:51:36,584 EPOCH 2982
2024-02-01 20:51:50,345 Epoch 2982: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 20:51:50,345 EPOCH 2983
2024-02-01 20:52:04,231 Epoch 2983: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:52:04,231 EPOCH 2984
2024-02-01 20:52:18,248 Epoch 2984: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:52:18,248 EPOCH 2985
2024-02-01 20:52:32,347 Epoch 2985: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 20:52:32,348 EPOCH 2986
2024-02-01 20:52:46,232 Epoch 2986: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 20:52:46,233 EPOCH 2987
2024-02-01 20:53:00,262 Epoch 2987: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 20:53:00,262 EPOCH 2988
2024-02-01 20:53:14,229 Epoch 2988: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 20:53:14,229 EPOCH 2989
2024-02-01 20:53:26,448 [Epoch: 2989 Step: 00026900] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:      765 || Batch Translation Loss:   0.020196 => Txt Tokens per Sec:     2097 || Lr: 0.000100
2024-02-01 20:53:28,260 Epoch 2989: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 20:53:28,260 EPOCH 2990
2024-02-01 20:53:41,926 Epoch 2990: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 20:53:41,926 EPOCH 2991
2024-02-01 20:53:55,864 Epoch 2991: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:53:55,865 EPOCH 2992
2024-02-01 20:54:09,568 Epoch 2992: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 20:54:09,569 EPOCH 2993
2024-02-01 20:54:23,157 Epoch 2993: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 20:54:23,157 EPOCH 2994
2024-02-01 20:54:37,229 Epoch 2994: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 20:54:37,230 EPOCH 2995
2024-02-01 20:54:51,421 Epoch 2995: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 20:54:51,421 EPOCH 2996
2024-02-01 20:55:05,116 Epoch 2996: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 20:55:05,116 EPOCH 2997
2024-02-01 20:55:18,969 Epoch 2997: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 20:55:18,970 EPOCH 2998
2024-02-01 20:55:32,843 Epoch 2998: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 20:55:32,844 EPOCH 2999
2024-02-01 20:55:46,737 Epoch 2999: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 20:55:46,737 EPOCH 3000
2024-02-01 20:56:00,774 [Epoch: 3000 Step: 00027000] Batch Recognition Loss:   0.000110 => Gls Tokens per Sec:      757 || Batch Translation Loss:   0.013536 => Txt Tokens per Sec:     2103 || Lr: 0.000100
2024-02-01 20:56:00,774 Epoch 3000: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 20:56:00,774 EPOCH 3001
2024-02-01 20:56:14,797 Epoch 3001: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 20:56:14,798 EPOCH 3002
2024-02-01 20:56:28,745 Epoch 3002: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 20:56:28,745 EPOCH 3003
2024-02-01 20:56:42,757 Epoch 3003: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 20:56:42,757 EPOCH 3004
2024-02-01 20:56:56,678 Epoch 3004: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 20:56:56,678 EPOCH 3005
2024-02-01 20:57:10,534 Epoch 3005: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:57:10,535 EPOCH 3006
2024-02-01 20:57:24,576 Epoch 3006: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 20:57:24,577 EPOCH 3007
2024-02-01 20:57:38,603 Epoch 3007: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 20:57:38,604 EPOCH 3008
2024-02-01 20:57:52,536 Epoch 3008: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 20:57:52,537 EPOCH 3009
2024-02-01 20:58:06,621 Epoch 3009: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:58:06,621 EPOCH 3010
2024-02-01 20:58:20,476 Epoch 3010: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 20:58:20,476 EPOCH 3011
2024-02-01 20:58:34,232 Epoch 3011: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 20:58:34,232 EPOCH 3012
2024-02-01 20:58:37,692 [Epoch: 3012 Step: 00027100] Batch Recognition Loss:   0.000166 => Gls Tokens per Sec:      113 || Batch Translation Loss:   0.007776 => Txt Tokens per Sec:      404 || Lr: 0.000100
2024-02-01 20:58:48,417 Epoch 3012: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 20:58:48,417 EPOCH 3013
2024-02-01 20:59:02,367 Epoch 3013: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 20:59:02,368 EPOCH 3014
2024-02-01 20:59:16,209 Epoch 3014: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 20:59:16,210 EPOCH 3015
2024-02-01 20:59:30,074 Epoch 3015: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 20:59:30,075 EPOCH 3016
2024-02-01 20:59:44,258 Epoch 3016: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 20:59:44,258 EPOCH 3017
2024-02-01 20:59:57,982 Epoch 3017: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 20:59:57,982 EPOCH 3018
2024-02-01 21:00:11,972 Epoch 3018: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 21:00:11,973 EPOCH 3019
2024-02-01 21:00:25,841 Epoch 3019: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 21:00:25,841 EPOCH 3020
2024-02-01 21:00:39,960 Epoch 3020: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 21:00:39,961 EPOCH 3021
2024-02-01 21:00:54,208 Epoch 3021: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 21:00:54,208 EPOCH 3022
2024-02-01 21:01:07,923 Epoch 3022: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 21:01:07,923 EPOCH 3023
2024-02-01 21:01:14,649 [Epoch: 3023 Step: 00027200] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:      381 || Batch Translation Loss:   0.028831 => Txt Tokens per Sec:     1276 || Lr: 0.000100
2024-02-01 21:01:22,038 Epoch 3023: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 21:01:22,038 EPOCH 3024
2024-02-01 21:01:35,746 Epoch 3024: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 21:01:35,746 EPOCH 3025
2024-02-01 21:01:49,726 Epoch 3025: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 21:01:49,726 EPOCH 3026
2024-02-01 21:02:03,739 Epoch 3026: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-01 21:02:03,739 EPOCH 3027
2024-02-01 21:02:17,382 Epoch 3027: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-01 21:02:17,382 EPOCH 3028
2024-02-01 21:02:31,189 Epoch 3028: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-01 21:02:31,190 EPOCH 3029
2024-02-01 21:02:45,152 Epoch 3029: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-01 21:02:45,153 EPOCH 3030
2024-02-01 21:02:59,037 Epoch 3030: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-01 21:02:59,037 EPOCH 3031
2024-02-01 21:03:13,088 Epoch 3031: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-01 21:03:13,088 EPOCH 3032
2024-02-01 21:03:27,281 Epoch 3032: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-01 21:03:27,281 EPOCH 3033
2024-02-01 21:03:41,466 Epoch 3033: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-01 21:03:41,467 EPOCH 3034
2024-02-01 21:03:45,407 [Epoch: 3034 Step: 00027300] Batch Recognition Loss:   0.000340 => Gls Tokens per Sec:      975 || Batch Translation Loss:   0.046570 => Txt Tokens per Sec:     2742 || Lr: 0.000100
2024-02-01 21:03:55,307 Epoch 3034: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-01 21:03:55,308 EPOCH 3035
2024-02-01 21:04:09,465 Epoch 3035: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 21:04:09,466 EPOCH 3036
2024-02-01 21:04:23,198 Epoch 3036: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 21:04:23,198 EPOCH 3037
2024-02-01 21:04:37,218 Epoch 3037: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-01 21:04:37,219 EPOCH 3038
2024-02-01 21:04:51,089 Epoch 3038: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-01 21:04:51,089 EPOCH 3039
2024-02-01 21:05:05,252 Epoch 3039: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 21:05:05,252 EPOCH 3040
2024-02-01 21:05:19,027 Epoch 3040: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 21:05:19,027 EPOCH 3041
2024-02-01 21:05:32,927 Epoch 3041: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 21:05:32,928 EPOCH 3042
2024-02-01 21:05:46,860 Epoch 3042: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 21:05:46,860 EPOCH 3043
2024-02-01 21:06:00,649 Epoch 3043: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 21:06:00,650 EPOCH 3044
2024-02-01 21:06:14,796 Epoch 3044: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 21:06:14,797 EPOCH 3045
2024-02-01 21:06:20,591 [Epoch: 3045 Step: 00027400] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:      730 || Batch Translation Loss:   0.039280 => Txt Tokens per Sec:     1968 || Lr: 0.000100
2024-02-01 21:06:28,463 Epoch 3045: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 21:06:28,463 EPOCH 3046
2024-02-01 21:06:42,352 Epoch 3046: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 21:06:42,353 EPOCH 3047
2024-02-01 21:06:56,319 Epoch 3047: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 21:06:56,320 EPOCH 3048
2024-02-01 21:07:10,490 Epoch 3048: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 21:07:10,490 EPOCH 3049
2024-02-01 21:07:24,527 Epoch 3049: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 21:07:24,528 EPOCH 3050
2024-02-01 21:07:38,433 Epoch 3050: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 21:07:38,433 EPOCH 3051
2024-02-01 21:07:52,068 Epoch 3051: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 21:07:52,069 EPOCH 3052
2024-02-01 21:08:06,005 Epoch 3052: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 21:08:06,006 EPOCH 3053
2024-02-01 21:08:19,811 Epoch 3053: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 21:08:19,812 EPOCH 3054
2024-02-01 21:08:33,786 Epoch 3054: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 21:08:33,787 EPOCH 3055
2024-02-01 21:08:47,689 Epoch 3055: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 21:08:47,689 EPOCH 3056
2024-02-01 21:08:52,270 [Epoch: 3056 Step: 00027500] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     1398 || Batch Translation Loss:   0.019695 => Txt Tokens per Sec:     3492 || Lr: 0.000100
2024-02-01 21:09:01,563 Epoch 3056: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 21:09:01,564 EPOCH 3057
2024-02-01 21:09:15,239 Epoch 3057: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 21:09:15,240 EPOCH 3058
2024-02-01 21:09:29,340 Epoch 3058: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 21:09:29,341 EPOCH 3059
2024-02-01 21:09:43,411 Epoch 3059: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 21:09:43,411 EPOCH 3060
2024-02-01 21:09:57,392 Epoch 3060: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 21:09:57,393 EPOCH 3061
2024-02-01 21:10:11,163 Epoch 3061: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 21:10:11,164 EPOCH 3062
2024-02-01 21:10:25,218 Epoch 3062: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 21:10:25,219 EPOCH 3063
2024-02-01 21:10:39,325 Epoch 3063: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 21:10:39,326 EPOCH 3064
2024-02-01 21:10:53,679 Epoch 3064: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 21:10:53,680 EPOCH 3065
2024-02-01 21:11:07,573 Epoch 3065: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 21:11:07,574 EPOCH 3066
2024-02-01 21:11:21,574 Epoch 3066: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 21:11:21,574 EPOCH 3067
2024-02-01 21:11:31,423 [Epoch: 3067 Step: 00027600] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:      690 || Batch Translation Loss:   0.015722 => Txt Tokens per Sec:     1926 || Lr: 0.000100
2024-02-01 21:11:35,396 Epoch 3067: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 21:11:35,397 EPOCH 3068
2024-02-01 21:11:49,131 Epoch 3068: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 21:11:49,131 EPOCH 3069
2024-02-01 21:12:02,999 Epoch 3069: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 21:12:02,999 EPOCH 3070
2024-02-01 21:12:16,890 Epoch 3070: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 21:12:16,891 EPOCH 3071
2024-02-01 21:12:30,770 Epoch 3071: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-01 21:12:30,771 EPOCH 3072
2024-02-01 21:12:44,768 Epoch 3072: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 21:12:44,768 EPOCH 3073
2024-02-01 21:12:58,690 Epoch 3073: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-01 21:12:58,690 EPOCH 3074
2024-02-01 21:13:12,857 Epoch 3074: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-01 21:13:12,858 EPOCH 3075
2024-02-01 21:13:26,937 Epoch 3075: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 21:13:26,937 EPOCH 3076
2024-02-01 21:13:40,806 Epoch 3076: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-01 21:13:40,807 EPOCH 3077
2024-02-01 21:13:54,779 Epoch 3077: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.11 
2024-02-01 21:13:54,780 EPOCH 3078
2024-02-01 21:14:07,842 [Epoch: 3078 Step: 00027700] Batch Recognition Loss:   0.000274 => Gls Tokens per Sec:      618 || Batch Translation Loss:   0.138610 => Txt Tokens per Sec:     1767 || Lr: 0.000100
2024-02-01 21:14:08,597 Epoch 3078: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.69 
2024-02-01 21:14:08,598 EPOCH 3079
2024-02-01 21:14:22,410 Epoch 3079: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.84 
2024-02-01 21:14:22,411 EPOCH 3080
2024-02-01 21:14:36,263 Epoch 3080: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.57 
2024-02-01 21:14:36,264 EPOCH 3081
2024-02-01 21:14:50,152 Epoch 3081: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.40 
2024-02-01 21:14:50,152 EPOCH 3082
2024-02-01 21:15:03,948 Epoch 3082: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-01 21:15:03,948 EPOCH 3083
2024-02-01 21:15:18,025 Epoch 3083: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-01 21:15:18,025 EPOCH 3084
2024-02-01 21:15:32,055 Epoch 3084: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-01 21:15:32,055 EPOCH 3085
2024-02-01 21:15:45,783 Epoch 3085: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-01 21:15:45,783 EPOCH 3086
2024-02-01 21:15:59,695 Epoch 3086: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-01 21:15:59,696 EPOCH 3087
2024-02-01 21:16:13,671 Epoch 3087: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 21:16:13,672 EPOCH 3088
2024-02-01 21:16:27,802 Epoch 3088: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-01 21:16:27,803 EPOCH 3089
2024-02-01 21:16:41,099 [Epoch: 3089 Step: 00027800] Batch Recognition Loss:   0.000263 => Gls Tokens per Sec:      703 || Batch Translation Loss:   0.045148 => Txt Tokens per Sec:     1938 || Lr: 0.000100
2024-02-01 21:16:41,800 Epoch 3089: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-01 21:16:41,800 EPOCH 3090
2024-02-01 21:16:55,822 Epoch 3090: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-01 21:16:55,822 EPOCH 3091
2024-02-01 21:17:09,955 Epoch 3091: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 21:17:09,956 EPOCH 3092
2024-02-01 21:17:23,714 Epoch 3092: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 21:17:23,714 EPOCH 3093
2024-02-01 21:17:37,844 Epoch 3093: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 21:17:37,845 EPOCH 3094
2024-02-01 21:17:51,618 Epoch 3094: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 21:17:51,619 EPOCH 3095
2024-02-01 21:18:05,565 Epoch 3095: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 21:18:05,566 EPOCH 3096
2024-02-01 21:18:19,447 Epoch 3096: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 21:18:19,448 EPOCH 3097
2024-02-01 21:18:33,381 Epoch 3097: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 21:18:33,381 EPOCH 3098
2024-02-01 21:18:47,164 Epoch 3098: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 21:18:47,165 EPOCH 3099
2024-02-01 21:19:00,966 Epoch 3099: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 21:19:00,967 EPOCH 3100
2024-02-01 21:19:14,813 [Epoch: 3100 Step: 00027900] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:      768 || Batch Translation Loss:   0.008953 => Txt Tokens per Sec:     2131 || Lr: 0.000100
2024-02-01 21:19:14,814 Epoch 3100: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 21:19:14,814 EPOCH 3101
2024-02-01 21:19:28,627 Epoch 3101: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 21:19:28,628 EPOCH 3102
2024-02-01 21:19:42,523 Epoch 3102: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 21:19:42,523 EPOCH 3103
2024-02-01 21:19:56,437 Epoch 3103: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 21:19:56,438 EPOCH 3104
2024-02-01 21:20:10,435 Epoch 3104: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 21:20:10,436 EPOCH 3105
2024-02-01 21:20:24,279 Epoch 3105: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 21:20:24,280 EPOCH 3106
2024-02-01 21:20:38,123 Epoch 3106: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 21:20:38,124 EPOCH 3107
2024-02-01 21:20:52,160 Epoch 3107: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 21:20:52,161 EPOCH 3108
2024-02-01 21:21:06,069 Epoch 3108: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 21:21:06,069 EPOCH 3109
2024-02-01 21:21:20,017 Epoch 3109: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 21:21:20,017 EPOCH 3110
2024-02-01 21:21:34,103 Epoch 3110: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 21:21:34,103 EPOCH 3111
2024-02-01 21:21:48,046 Epoch 3111: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 21:21:48,046 EPOCH 3112
2024-02-01 21:21:48,297 [Epoch: 3112 Step: 00028000] Batch Recognition Loss:   0.000118 => Gls Tokens per Sec:     5133 || Batch Translation Loss:   0.009688 => Txt Tokens per Sec:     9204 || Lr: 0.000100
2024-02-01 21:22:07,680 Validation result at epoch 3112, step    28000: duration: 19.3834s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00020	Translation Loss: 98706.78125	PPL: 19488.75781
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.67	(BLEU-1: 10.35,	BLEU-2: 3.15,	BLEU-3: 1.27,	BLEU-4: 0.67)
	CHRF 17.15	ROUGE 8.96
2024-02-01 21:22:07,682 Logging Recognition and Translation Outputs
2024-02-01 21:22:07,682 ========================================================================================================================
2024-02-01 21:22:07,682 Logging Sequence: 67_98.00
2024-02-01 21:22:07,682 	Gloss Reference :	A B+C+D+E
2024-02-01 21:22:07,682 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 21:22:07,682 	Gloss Alignment :	         
2024-02-01 21:22:07,683 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 21:22:07,684 	Text Reference  :	it saddens me to  see  people suffering and dying due   to  lack of   oxygen 
2024-02-01 21:22:07,684 	Text Hypothesis :	** ******* ** she said that   there     was a     world cup in   fast bowlers
2024-02-01 21:22:07,684 	Text Alignment  :	D  D       D  S   S    S      S         S   S     S     S   S    S    S      
2024-02-01 21:22:07,684 ========================================================================================================================
2024-02-01 21:22:07,684 Logging Sequence: 157_83.00
2024-02-01 21:22:07,685 	Gloss Reference :	A B+C+D+E
2024-02-01 21:22:07,685 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 21:22:07,685 	Gloss Alignment :	         
2024-02-01 21:22:07,685 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 21:22:07,687 	Text Reference  :	also when you eat sandwich at a streetside hawker or    stall the **** sandwich maker will first apply butter with a        knife
2024-02-01 21:22:07,687 	Text Hypothesis :	**** **** *** *** ******** ** * ********** the    final of    the asia cup      2023  will be    held  on     5th  february 2022 
2024-02-01 21:22:07,687 	Text Alignment  :	D    D    D   D   D        D  D D          S      S     S         I    S        S          S     S     S      S    S        S    
2024-02-01 21:22:07,687 ========================================================================================================================
2024-02-01 21:22:07,687 Logging Sequence: 76_35.00
2024-02-01 21:22:07,687 	Gloss Reference :	A B+C+D+E
2024-02-01 21:22:07,688 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 21:22:07,688 	Gloss Alignment :	         
2024-02-01 21:22:07,688 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 21:22:07,688 	Text Reference  :	bcci president sourav ganguly along with board secretary jay shah
2024-02-01 21:22:07,689 	Text Hypothesis :	**** this      is     because the   toss and   chose     to  bowl
2024-02-01 21:22:07,689 	Text Alignment  :	D    S         S      S       S     S    S     S         S   S   
2024-02-01 21:22:07,689 ========================================================================================================================
2024-02-01 21:22:07,689 Logging Sequence: 139_180.00
2024-02-01 21:22:07,689 	Gloss Reference :	A B+C+D+E
2024-02-01 21:22:07,689 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 21:22:07,689 	Gloss Alignment :	         
2024-02-01 21:22:07,689 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 21:22:07,690 	Text Reference  :	********* ********* **** ****** ******** netherlands also faced     similar riots     
2024-02-01 21:22:07,690 	Text Hypothesis :	ahmedabad witnesses huge crowds everyone is          also following the     tournament
2024-02-01 21:22:07,690 	Text Alignment  :	I         I         I    I      I        S                S         S       S         
2024-02-01 21:22:07,690 ========================================================================================================================
2024-02-01 21:22:07,690 Logging Sequence: 98_87.00
2024-02-01 21:22:07,691 	Gloss Reference :	A B+C+D+E
2024-02-01 21:22:07,691 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 21:22:07,691 	Gloss Alignment :	         
2024-02-01 21:22:07,691 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 21:22:07,692 	Text Reference  :	instead of starting afresh in 2021 the organizers opted to       resume with     the     previous edition   
2024-02-01 21:22:07,692 	Text Hypothesis :	******* ** ******** ****** ** **** *** and        was   consoled by     denmark' captain and      goalkeeper
2024-02-01 21:22:07,692 	Text Alignment  :	D       D  D        D      D  D    D   S          S     S        S      S        S       S        S         
2024-02-01 21:22:07,692 ========================================================================================================================
2024-02-01 21:22:21,176 Epoch 3112: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 21:22:21,176 EPOCH 3113
2024-02-01 21:22:35,193 Epoch 3113: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 21:22:35,194 EPOCH 3114
2024-02-01 21:22:49,155 Epoch 3114: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 21:22:49,156 EPOCH 3115
2024-02-01 21:23:03,028 Epoch 3115: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 21:23:03,029 EPOCH 3116
2024-02-01 21:23:16,673 Epoch 3116: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:23:16,673 EPOCH 3117
2024-02-01 21:23:31,116 Epoch 3117: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:23:31,117 EPOCH 3118
2024-02-01 21:23:45,187 Epoch 3118: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:23:45,188 EPOCH 3119
2024-02-01 21:23:58,768 Epoch 3119: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:23:58,769 EPOCH 3120
2024-02-01 21:24:12,749 Epoch 3120: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:24:12,750 EPOCH 3121
2024-02-01 21:24:26,729 Epoch 3121: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:24:26,729 EPOCH 3122
2024-02-01 21:24:40,654 Epoch 3122: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-01 21:24:40,655 EPOCH 3123
2024-02-01 21:24:42,765 [Epoch: 3123 Step: 00028100] Batch Recognition Loss:   0.000099 => Gls Tokens per Sec:     1214 || Batch Translation Loss:   0.008894 => Txt Tokens per Sec:     3236 || Lr: 0.000050
2024-02-01 21:24:54,351 Epoch 3123: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:24:54,352 EPOCH 3124
2024-02-01 21:25:08,393 Epoch 3124: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:25:08,393 EPOCH 3125
2024-02-01 21:25:22,342 Epoch 3125: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:25:22,343 EPOCH 3126
2024-02-01 21:25:36,381 Epoch 3126: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-01 21:25:36,381 EPOCH 3127
2024-02-01 21:25:50,213 Epoch 3127: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:25:50,214 EPOCH 3128
2024-02-01 21:26:04,111 Epoch 3128: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:26:04,112 EPOCH 3129
2024-02-01 21:26:17,957 Epoch 3129: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:26:17,958 EPOCH 3130
2024-02-01 21:26:31,974 Epoch 3130: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:26:31,975 EPOCH 3131
2024-02-01 21:26:46,094 Epoch 3131: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:26:46,094 EPOCH 3132
2024-02-01 21:27:00,092 Epoch 3132: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-01 21:27:00,092 EPOCH 3133
2024-02-01 21:27:14,194 Epoch 3133: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:27:14,194 EPOCH 3134
2024-02-01 21:27:18,471 [Epoch: 3134 Step: 00028200] Batch Recognition Loss:   0.000114 => Gls Tokens per Sec:      690 || Batch Translation Loss:   0.012753 => Txt Tokens per Sec:     1728 || Lr: 0.000050
2024-02-01 21:27:28,246 Epoch 3134: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 21:27:28,246 EPOCH 3135
2024-02-01 21:27:42,109 Epoch 3135: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:27:42,110 EPOCH 3136
2024-02-01 21:27:56,103 Epoch 3136: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:27:56,103 EPOCH 3137
2024-02-01 21:28:10,165 Epoch 3137: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:28:10,166 EPOCH 3138
2024-02-01 21:28:23,916 Epoch 3138: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:28:23,917 EPOCH 3139
2024-02-01 21:28:38,146 Epoch 3139: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:28:38,146 EPOCH 3140
2024-02-01 21:28:52,153 Epoch 3140: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-01 21:28:52,153 EPOCH 3141
2024-02-01 21:29:06,411 Epoch 3141: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:29:06,412 EPOCH 3142
2024-02-01 21:29:20,011 Epoch 3142: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:29:20,011 EPOCH 3143
2024-02-01 21:29:34,157 Epoch 3143: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:29:34,158 EPOCH 3144
2024-02-01 21:29:47,970 Epoch 3144: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:29:47,971 EPOCH 3145
2024-02-01 21:29:55,402 [Epoch: 3145 Step: 00028300] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:      689 || Batch Translation Loss:   0.016071 => Txt Tokens per Sec:     1981 || Lr: 0.000050
2024-02-01 21:30:01,809 Epoch 3145: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:30:01,810 EPOCH 3146
2024-02-01 21:30:15,475 Epoch 3146: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-01 21:30:15,476 EPOCH 3147
2024-02-01 21:30:29,584 Epoch 3147: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:30:29,584 EPOCH 3148
2024-02-01 21:30:43,402 Epoch 3148: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-01 21:30:43,403 EPOCH 3149
2024-02-01 21:30:57,258 Epoch 3149: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:30:57,259 EPOCH 3150
2024-02-01 21:31:11,360 Epoch 3150: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:31:11,361 EPOCH 3151
2024-02-01 21:31:25,377 Epoch 3151: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-01 21:31:25,377 EPOCH 3152
2024-02-01 21:31:39,361 Epoch 3152: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:31:39,361 EPOCH 3153
2024-02-01 21:31:53,240 Epoch 3153: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:31:53,241 EPOCH 3154
2024-02-01 21:32:07,287 Epoch 3154: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:32:07,287 EPOCH 3155
2024-02-01 21:32:20,948 Epoch 3155: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-01 21:32:20,948 EPOCH 3156
2024-02-01 21:32:28,889 [Epoch: 3156 Step: 00028400] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:      694 || Batch Translation Loss:   0.013816 => Txt Tokens per Sec:     1940 || Lr: 0.000050
2024-02-01 21:32:34,997 Epoch 3156: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-01 21:32:34,997 EPOCH 3157
2024-02-01 21:32:48,850 Epoch 3157: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:32:48,851 EPOCH 3158
2024-02-01 21:33:02,859 Epoch 3158: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-01 21:33:02,860 EPOCH 3159
2024-02-01 21:33:16,731 Epoch 3159: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-01 21:33:16,732 EPOCH 3160
2024-02-01 21:33:30,729 Epoch 3160: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:33:30,729 EPOCH 3161
2024-02-01 21:33:44,339 Epoch 3161: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:33:44,340 EPOCH 3162
2024-02-01 21:33:58,354 Epoch 3162: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:33:58,355 EPOCH 3163
2024-02-01 21:34:12,342 Epoch 3163: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-01 21:34:12,343 EPOCH 3164
2024-02-01 21:34:26,105 Epoch 3164: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:34:26,105 EPOCH 3165
2024-02-01 21:34:40,089 Epoch 3165: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:34:40,089 EPOCH 3166
2024-02-01 21:34:53,996 Epoch 3166: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-01 21:34:53,997 EPOCH 3167
2024-02-01 21:35:03,863 [Epoch: 3167 Step: 00028500] Batch Recognition Loss:   0.000098 => Gls Tokens per Sec:      778 || Batch Translation Loss:   0.011491 => Txt Tokens per Sec:     2296 || Lr: 0.000050
2024-02-01 21:35:07,843 Epoch 3167: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-01 21:35:07,843 EPOCH 3168
2024-02-01 21:35:21,728 Epoch 3168: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:35:21,728 EPOCH 3169
2024-02-01 21:35:35,877 Epoch 3169: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:35:35,877 EPOCH 3170
2024-02-01 21:35:49,539 Epoch 3170: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:35:49,540 EPOCH 3171
2024-02-01 21:36:03,372 Epoch 3171: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:36:03,373 EPOCH 3172
2024-02-01 21:36:17,277 Epoch 3172: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:36:17,277 EPOCH 3173
2024-02-01 21:36:30,953 Epoch 3173: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:36:30,954 EPOCH 3174
2024-02-01 21:36:44,882 Epoch 3174: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:36:44,883 EPOCH 3175
2024-02-01 21:36:58,715 Epoch 3175: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:36:58,715 EPOCH 3176
2024-02-01 21:37:12,601 Epoch 3176: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:37:12,602 EPOCH 3177
2024-02-01 21:37:26,551 Epoch 3177: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:37:26,552 EPOCH 3178
2024-02-01 21:37:38,367 [Epoch: 3178 Step: 00028600] Batch Recognition Loss:   0.000105 => Gls Tokens per Sec:      683 || Batch Translation Loss:   0.010388 => Txt Tokens per Sec:     1869 || Lr: 0.000050
2024-02-01 21:37:40,493 Epoch 3178: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:37:40,493 EPOCH 3179
2024-02-01 21:37:54,070 Epoch 3179: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-01 21:37:54,070 EPOCH 3180
2024-02-01 21:38:08,190 Epoch 3180: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:38:08,191 EPOCH 3181
2024-02-01 21:38:21,707 Epoch 3181: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:38:21,708 EPOCH 3182
2024-02-01 21:38:35,535 Epoch 3182: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:38:35,535 EPOCH 3183
2024-02-01 21:38:49,637 Epoch 3183: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:38:49,638 EPOCH 3184
2024-02-01 21:39:03,656 Epoch 3184: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:39:03,657 EPOCH 3185
2024-02-01 21:39:17,389 Epoch 3185: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:39:17,389 EPOCH 3186
2024-02-01 21:39:31,201 Epoch 3186: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:39:31,202 EPOCH 3187
2024-02-01 21:39:45,095 Epoch 3187: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:39:45,095 EPOCH 3188
2024-02-01 21:39:59,024 Epoch 3188: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:39:59,025 EPOCH 3189
2024-02-01 21:40:12,797 [Epoch: 3189 Step: 00028700] Batch Recognition Loss:   0.000099 => Gls Tokens per Sec:      679 || Batch Translation Loss:   0.011199 => Txt Tokens per Sec:     1931 || Lr: 0.000050
2024-02-01 21:40:13,111 Epoch 3189: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:40:13,111 EPOCH 3190
2024-02-01 21:40:26,870 Epoch 3190: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:40:26,871 EPOCH 3191
2024-02-01 21:40:40,915 Epoch 3191: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:40:40,915 EPOCH 3192
2024-02-01 21:40:54,651 Epoch 3192: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:40:54,652 EPOCH 3193
2024-02-01 21:41:08,710 Epoch 3193: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:41:08,711 EPOCH 3194
2024-02-01 21:41:22,357 Epoch 3194: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:41:22,357 EPOCH 3195
2024-02-01 21:41:36,026 Epoch 3195: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:41:36,027 EPOCH 3196
2024-02-01 21:41:49,859 Epoch 3196: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 21:41:49,859 EPOCH 3197
2024-02-01 21:42:03,807 Epoch 3197: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:42:03,808 EPOCH 3198
2024-02-01 21:42:17,628 Epoch 3198: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:42:17,630 EPOCH 3199
2024-02-01 21:42:31,300 Epoch 3199: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 21:42:31,301 EPOCH 3200
2024-02-01 21:42:45,313 [Epoch: 3200 Step: 00028800] Batch Recognition Loss:   0.000099 => Gls Tokens per Sec:      759 || Batch Translation Loss:   0.012667 => Txt Tokens per Sec:     2106 || Lr: 0.000050
2024-02-01 21:42:45,314 Epoch 3200: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:42:45,314 EPOCH 3201
2024-02-01 21:42:59,127 Epoch 3201: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 21:42:59,127 EPOCH 3202
2024-02-01 21:43:12,927 Epoch 3202: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:43:12,927 EPOCH 3203
2024-02-01 21:43:26,999 Epoch 3203: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:43:26,999 EPOCH 3204
2024-02-01 21:43:41,051 Epoch 3204: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:43:41,051 EPOCH 3205
2024-02-01 21:43:54,996 Epoch 3205: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 21:43:54,997 EPOCH 3206
2024-02-01 21:44:08,989 Epoch 3206: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:44:08,989 EPOCH 3207
2024-02-01 21:44:22,753 Epoch 3207: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:44:22,754 EPOCH 3208
2024-02-01 21:44:36,905 Epoch 3208: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 21:44:36,905 EPOCH 3209
2024-02-01 21:44:50,904 Epoch 3209: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 21:44:50,905 EPOCH 3210
2024-02-01 21:45:04,555 Epoch 3210: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 21:45:04,555 EPOCH 3211
2024-02-01 21:45:18,034 Epoch 3211: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:45:18,035 EPOCH 3212
2024-02-01 21:45:18,732 [Epoch: 3212 Step: 00028900] Batch Recognition Loss:   0.000106 => Gls Tokens per Sec:     1839 || Batch Translation Loss:   0.013315 => Txt Tokens per Sec:     5447 || Lr: 0.000050
2024-02-01 21:45:32,118 Epoch 3212: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:45:32,119 EPOCH 3213
2024-02-01 21:45:46,144 Epoch 3213: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-01 21:45:46,145 EPOCH 3214
2024-02-01 21:46:00,251 Epoch 3214: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 21:46:00,251 EPOCH 3215
2024-02-01 21:46:14,137 Epoch 3215: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 21:46:14,138 EPOCH 3216
2024-02-01 21:46:28,140 Epoch 3216: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 21:46:28,141 EPOCH 3217
2024-02-01 21:46:41,499 Epoch 3217: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 21:46:41,499 EPOCH 3218
2024-02-01 21:46:55,141 Epoch 3218: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 21:46:55,141 EPOCH 3219
2024-02-01 21:47:09,097 Epoch 3219: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 21:47:09,097 EPOCH 3220
2024-02-01 21:47:22,997 Epoch 3220: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 21:47:22,998 EPOCH 3221
2024-02-01 21:47:36,969 Epoch 3221: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 21:47:36,970 EPOCH 3222
2024-02-01 21:47:50,843 Epoch 3222: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 21:47:50,844 EPOCH 3223
2024-02-01 21:47:57,773 [Epoch: 3223 Step: 00029000] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:      241 || Batch Translation Loss:   0.020013 => Txt Tokens per Sec:      849 || Lr: 0.000050
2024-02-01 21:48:04,780 Epoch 3223: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 21:48:04,781 EPOCH 3224
2024-02-01 21:48:18,581 Epoch 3224: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 21:48:18,582 EPOCH 3225
2024-02-01 21:48:32,412 Epoch 3225: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 21:48:32,412 EPOCH 3226
2024-02-01 21:48:46,418 Epoch 3226: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 21:48:46,418 EPOCH 3227
2024-02-01 21:49:00,439 Epoch 3227: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 21:49:00,439 EPOCH 3228
2024-02-01 21:49:14,441 Epoch 3228: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 21:49:14,442 EPOCH 3229
2024-02-01 21:49:28,394 Epoch 3229: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 21:49:28,395 EPOCH 3230
2024-02-01 21:49:42,354 Epoch 3230: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 21:49:42,355 EPOCH 3231
2024-02-01 21:49:56,379 Epoch 3231: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 21:49:56,380 EPOCH 3232
2024-02-01 21:50:10,284 Epoch 3232: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 21:50:10,285 EPOCH 3233
2024-02-01 21:50:24,121 Epoch 3233: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 21:50:24,121 EPOCH 3234
2024-02-01 21:50:29,689 [Epoch: 3234 Step: 00029100] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:      530 || Batch Translation Loss:   0.021062 => Txt Tokens per Sec:     1578 || Lr: 0.000050
2024-02-01 21:50:38,335 Epoch 3234: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 21:50:38,336 EPOCH 3235
2024-02-01 21:50:52,207 Epoch 3235: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 21:50:52,207 EPOCH 3236
2024-02-01 21:51:06,370 Epoch 3236: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 21:51:06,371 EPOCH 3237
2024-02-01 21:51:20,487 Epoch 3237: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 21:51:20,487 EPOCH 3238
2024-02-01 21:51:34,458 Epoch 3238: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 21:51:34,459 EPOCH 3239
2024-02-01 21:51:48,339 Epoch 3239: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 21:51:48,339 EPOCH 3240
2024-02-01 21:52:02,284 Epoch 3240: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 21:52:02,285 EPOCH 3241
2024-02-01 21:52:16,083 Epoch 3241: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.82 
2024-02-01 21:52:16,084 EPOCH 3242
2024-02-01 21:52:30,090 Epoch 3242: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.99 
2024-02-01 21:52:30,090 EPOCH 3243
2024-02-01 21:52:43,877 Epoch 3243: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-01 21:52:43,878 EPOCH 3244
2024-02-01 21:52:57,660 Epoch 3244: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-01 21:52:57,660 EPOCH 3245
2024-02-01 21:53:03,523 [Epoch: 3245 Step: 00029200] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:      722 || Batch Translation Loss:   0.028326 => Txt Tokens per Sec:     1927 || Lr: 0.000050
2024-02-01 21:53:11,631 Epoch 3245: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-01 21:53:11,631 EPOCH 3246
2024-02-01 21:53:25,364 Epoch 3246: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-01 21:53:25,365 EPOCH 3247
2024-02-01 21:53:39,288 Epoch 3247: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 21:53:39,288 EPOCH 3248
2024-02-01 21:53:53,485 Epoch 3248: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 21:53:53,486 EPOCH 3249
2024-02-01 21:54:07,313 Epoch 3249: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 21:54:07,313 EPOCH 3250
2024-02-01 21:54:21,418 Epoch 3250: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 21:54:21,418 EPOCH 3251
2024-02-01 21:54:35,270 Epoch 3251: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 21:54:35,271 EPOCH 3252
2024-02-01 21:54:49,227 Epoch 3252: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 21:54:49,228 EPOCH 3253
2024-02-01 21:55:03,220 Epoch 3253: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 21:55:03,221 EPOCH 3254
2024-02-01 21:55:17,397 Epoch 3254: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 21:55:17,398 EPOCH 3255
2024-02-01 21:55:31,474 Epoch 3255: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 21:55:31,475 EPOCH 3256
2024-02-01 21:55:43,562 [Epoch: 3256 Step: 00029300] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:      456 || Batch Translation Loss:   0.024634 => Txt Tokens per Sec:     1389 || Lr: 0.000050
2024-02-01 21:55:45,415 Epoch 3256: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 21:55:45,416 EPOCH 3257
2024-02-01 21:55:58,940 Epoch 3257: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 21:55:58,940 EPOCH 3258
2024-02-01 21:56:12,867 Epoch 3258: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 21:56:12,868 EPOCH 3259
2024-02-01 21:56:26,742 Epoch 3259: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 21:56:26,742 EPOCH 3260
2024-02-01 21:56:40,409 Epoch 3260: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 21:56:40,410 EPOCH 3261
2024-02-01 21:56:54,208 Epoch 3261: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 21:56:54,208 EPOCH 3262
2024-02-01 21:57:08,266 Epoch 3262: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 21:57:08,267 EPOCH 3263
2024-02-01 21:57:22,171 Epoch 3263: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 21:57:22,172 EPOCH 3264
2024-02-01 21:57:35,688 Epoch 3264: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 21:57:35,688 EPOCH 3265
2024-02-01 21:57:49,561 Epoch 3265: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 21:57:49,562 EPOCH 3266
2024-02-01 21:58:03,452 Epoch 3266: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 21:58:03,453 EPOCH 3267
2024-02-01 21:58:11,936 [Epoch: 3267 Step: 00029400] Batch Recognition Loss:   0.000121 => Gls Tokens per Sec:      801 || Batch Translation Loss:   0.013080 => Txt Tokens per Sec:     2228 || Lr: 0.000050
2024-02-01 21:58:17,638 Epoch 3267: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 21:58:17,639 EPOCH 3268
2024-02-01 21:58:31,677 Epoch 3268: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 21:58:31,678 EPOCH 3269
2024-02-01 21:58:45,727 Epoch 3269: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 21:58:45,728 EPOCH 3270
2024-02-01 21:59:00,009 Epoch 3270: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 21:59:00,009 EPOCH 3271
2024-02-01 21:59:13,772 Epoch 3271: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 21:59:13,772 EPOCH 3272
2024-02-01 21:59:27,652 Epoch 3272: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 21:59:27,652 EPOCH 3273
2024-02-01 21:59:41,643 Epoch 3273: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 21:59:41,643 EPOCH 3274
2024-02-01 21:59:55,573 Epoch 3274: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 21:59:55,574 EPOCH 3275
2024-02-01 22:00:09,353 Epoch 3275: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:00:09,353 EPOCH 3276
2024-02-01 22:00:23,113 Epoch 3276: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 22:00:23,113 EPOCH 3277
2024-02-01 22:00:36,976 Epoch 3277: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 22:00:36,977 EPOCH 3278
2024-02-01 22:00:48,627 [Epoch: 3278 Step: 00029500] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:      693 || Batch Translation Loss:   0.018990 => Txt Tokens per Sec:     1931 || Lr: 0.000050
2024-02-01 22:00:51,031 Epoch 3278: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:00:51,031 EPOCH 3279
2024-02-01 22:01:04,787 Epoch 3279: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:01:04,788 EPOCH 3280
2024-02-01 22:01:18,759 Epoch 3280: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 22:01:18,759 EPOCH 3281
2024-02-01 22:01:32,577 Epoch 3281: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 22:01:32,578 EPOCH 3282
2024-02-01 22:01:46,351 Epoch 3282: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:01:46,351 EPOCH 3283
2024-02-01 22:02:00,404 Epoch 3283: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 22:02:00,405 EPOCH 3284
2024-02-01 22:02:14,489 Epoch 3284: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 22:02:14,490 EPOCH 3285
2024-02-01 22:02:28,213 Epoch 3285: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:02:28,213 EPOCH 3286
2024-02-01 22:02:42,234 Epoch 3286: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:02:42,234 EPOCH 3287
2024-02-01 22:02:56,448 Epoch 3287: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 22:02:56,448 EPOCH 3288
2024-02-01 22:03:10,426 Epoch 3288: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 22:03:10,427 EPOCH 3289
2024-02-01 22:03:20,698 [Epoch: 3289 Step: 00029600] Batch Recognition Loss:   0.000115 => Gls Tokens per Sec:      911 || Batch Translation Loss:   0.010776 => Txt Tokens per Sec:     2437 || Lr: 0.000050
2024-02-01 22:03:24,256 Epoch 3289: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:03:24,256 EPOCH 3290
2024-02-01 22:03:38,386 Epoch 3290: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:03:38,387 EPOCH 3291
2024-02-01 22:03:52,460 Epoch 3291: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:03:52,461 EPOCH 3292
2024-02-01 22:04:08,789 Epoch 3292: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:04:08,790 EPOCH 3293
2024-02-01 22:04:25,718 Epoch 3293: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:04:25,719 EPOCH 3294
2024-02-01 22:04:40,344 Epoch 3294: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 22:04:40,344 EPOCH 3295
2024-02-01 22:04:54,216 Epoch 3295: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 22:04:54,217 EPOCH 3296
2024-02-01 22:05:08,280 Epoch 3296: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 22:05:08,281 EPOCH 3297
2024-02-01 22:05:22,365 Epoch 3297: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 22:05:22,366 EPOCH 3298
2024-02-01 22:05:36,481 Epoch 3298: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 22:05:36,482 EPOCH 3299
2024-02-01 22:05:50,480 Epoch 3299: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 22:05:50,481 EPOCH 3300
2024-02-01 22:06:04,377 [Epoch: 3300 Step: 00029700] Batch Recognition Loss:   0.000110 => Gls Tokens per Sec:      765 || Batch Translation Loss:   0.010395 => Txt Tokens per Sec:     2124 || Lr: 0.000050
2024-02-01 22:06:04,378 Epoch 3300: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:06:04,378 EPOCH 3301
2024-02-01 22:06:18,010 Epoch 3301: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 22:06:18,010 EPOCH 3302
2024-02-01 22:06:32,162 Epoch 3302: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 22:06:32,162 EPOCH 3303
2024-02-01 22:06:46,073 Epoch 3303: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 22:06:46,073 EPOCH 3304
2024-02-01 22:06:59,790 Epoch 3304: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-01 22:06:59,790 EPOCH 3305
2024-02-01 22:07:13,477 Epoch 3305: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-01 22:07:13,477 EPOCH 3306
2024-02-01 22:07:27,617 Epoch 3306: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-01 22:07:27,618 EPOCH 3307
2024-02-01 22:07:41,335 Epoch 3307: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-01 22:07:41,335 EPOCH 3308
2024-02-01 22:07:54,948 Epoch 3308: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 22:07:54,948 EPOCH 3309
2024-02-01 22:08:09,029 Epoch 3309: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 22:08:09,029 EPOCH 3310
2024-02-01 22:08:23,235 Epoch 3310: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 22:08:23,236 EPOCH 3311
2024-02-01 22:08:37,219 Epoch 3311: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 22:08:37,220 EPOCH 3312
2024-02-01 22:08:40,480 [Epoch: 3312 Step: 00029800] Batch Recognition Loss:   0.000337 => Gls Tokens per Sec:      120 || Batch Translation Loss:   0.009616 => Txt Tokens per Sec:      428 || Lr: 0.000050
2024-02-01 22:08:51,085 Epoch 3312: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 22:08:51,085 EPOCH 3313
2024-02-01 22:09:04,524 Epoch 3313: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 22:09:04,525 EPOCH 3314
2024-02-01 22:09:18,482 Epoch 3314: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:09:18,482 EPOCH 3315
2024-02-01 22:09:32,498 Epoch 3315: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 22:09:32,498 EPOCH 3316
2024-02-01 22:09:46,834 Epoch 3316: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:09:46,834 EPOCH 3317
2024-02-01 22:10:00,791 Epoch 3317: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:10:00,791 EPOCH 3318
2024-02-01 22:10:14,362 Epoch 3318: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:10:14,362 EPOCH 3319
2024-02-01 22:10:28,425 Epoch 3319: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:10:28,426 EPOCH 3320
2024-02-01 22:10:42,242 Epoch 3320: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:10:42,243 EPOCH 3321
2024-02-01 22:10:55,882 Epoch 3321: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:10:55,883 EPOCH 3322
2024-02-01 22:11:09,989 Epoch 3322: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:11:09,990 EPOCH 3323
2024-02-01 22:11:13,690 [Epoch: 3323 Step: 00029900] Batch Recognition Loss:   0.000107 => Gls Tokens per Sec:      451 || Batch Translation Loss:   0.011335 => Txt Tokens per Sec:     1174 || Lr: 0.000050
2024-02-01 22:11:23,993 Epoch 3323: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:11:23,994 EPOCH 3324
2024-02-01 22:11:37,784 Epoch 3324: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:11:37,784 EPOCH 3325
2024-02-01 22:11:51,646 Epoch 3325: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:11:51,647 EPOCH 3326
2024-02-01 22:12:05,693 Epoch 3326: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:12:05,694 EPOCH 3327
2024-02-01 22:12:19,405 Epoch 3327: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:12:19,406 EPOCH 3328
2024-02-01 22:12:33,419 Epoch 3328: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 22:12:33,420 EPOCH 3329
2024-02-01 22:12:47,315 Epoch 3329: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 22:12:47,315 EPOCH 3330
2024-02-01 22:13:01,373 Epoch 3330: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 22:13:01,373 EPOCH 3331
2024-02-01 22:13:15,191 Epoch 3331: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 22:13:15,191 EPOCH 3332
2024-02-01 22:13:28,917 Epoch 3332: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 22:13:28,917 EPOCH 3333
2024-02-01 22:13:42,976 Epoch 3333: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 22:13:42,977 EPOCH 3334
2024-02-01 22:13:45,575 [Epoch: 3334 Step: 00030000] Batch Recognition Loss:   0.000107 => Gls Tokens per Sec:     1479 || Batch Translation Loss:   0.011402 => Txt Tokens per Sec:     3970 || Lr: 0.000050
2024-02-01 22:14:05,844 Validation result at epoch 3334, step    30000: duration: 20.2680s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00019	Translation Loss: 98192.26562	PPL: 18510.71680
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.58	(BLEU-1: 10.38,	BLEU-2: 3.02,	BLEU-3: 1.18,	BLEU-4: 0.58)
	CHRF 17.09	ROUGE 8.79
2024-02-01 22:14:05,845 Logging Recognition and Translation Outputs
2024-02-01 22:14:05,845 ========================================================================================================================
2024-02-01 22:14:05,846 Logging Sequence: 165_502.00
2024-02-01 22:14:05,846 	Gloss Reference :	A B+C+D+E
2024-02-01 22:14:05,846 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 22:14:05,846 	Gloss Alignment :	         
2024-02-01 22:14:05,846 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 22:14:05,848 	Text Reference  :	****** tendulkar would sit in   the  pavilion wearing both     his batting pads even after he     got out  
2024-02-01 22:14:05,848 	Text Hypothesis :	sachin tendulkar ***** *** also been an       auction happened as  they    did  not  lose  before any match
2024-02-01 22:14:05,848 	Text Alignment  :	I                D     D   S    S    S        S       S        S   S       S    S    S     S      S   S    
2024-02-01 22:14:05,848 ========================================================================================================================
2024-02-01 22:14:05,848 Logging Sequence: 127_57.00
2024-02-01 22:14:05,849 	Gloss Reference :	A B+C+D+E
2024-02-01 22:14:05,849 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 22:14:05,849 	Gloss Alignment :	         
2024-02-01 22:14:05,849 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 22:14:05,851 	Text Reference  :	*** till date india had won only 2 medals at the championships which like the   olympics is the highest level championship
2024-02-01 22:14:05,851 	Text Hypothesis :	the here is   india *** *** **** * ****** ** *** i             am    very first time     in the ******* world cup         
2024-02-01 22:14:05,851 	Text Alignment  :	I   S    S          D   D   D    D D      D  D   S             S     S    S     S        S      D       S     S           
2024-02-01 22:14:05,851 ========================================================================================================================
2024-02-01 22:14:05,851 Logging Sequence: 169_10.00
2024-02-01 22:14:05,851 	Gloss Reference :	A B+C+D+E
2024-02-01 22:14:05,851 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 22:14:05,852 	Gloss Alignment :	         
2024-02-01 22:14:05,852 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 22:14:05,853 	Text Reference  :	the 18th  over was  bowled by       ravi    bishnoi with  khushdil shah  and     asif ali     on the ********* crease
2024-02-01 22:14:05,853 	Text Hypothesis :	*** kohli said that though arshdeep dropped the     catch he       still removed it   because of the khalistan cause 
2024-02-01 22:14:05,854 	Text Alignment  :	D   S     S    S    S      S        S       S       S     S        S     S       S    S       S      I         S     
2024-02-01 22:14:05,854 ========================================================================================================================
2024-02-01 22:14:05,854 Logging Sequence: 64_89.00
2024-02-01 22:14:05,854 	Gloss Reference :	A B+C+D+E
2024-02-01 22:14:05,854 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 22:14:05,854 	Gloss Alignment :	         
2024-02-01 22:14:05,854 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 22:14:05,856 	Text Reference  :	but this can not go on   amidst the rising cases    human lives need to  be   safeguarded
2024-02-01 22:14:05,856 	Text Hypothesis :	*** **** *** *** ** with is     ipl will   continue to    play  he   was very time       
2024-02-01 22:14:05,856 	Text Alignment  :	D   D    D   D   D  S    S      S   S      S        S     S     S    S   S    S          
2024-02-01 22:14:05,856 ========================================================================================================================
2024-02-01 22:14:05,856 Logging Sequence: 166_261.00
2024-02-01 22:14:05,856 	Gloss Reference :	A B+C+D+E
2024-02-01 22:14:05,856 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 22:14:05,857 	Gloss Alignment :	         
2024-02-01 22:14:05,857 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 22:14:05,857 	Text Reference  :	for       all organizational matters  and  the **** *** ****** ** * *** *** schedule
2024-02-01 22:14:05,858 	Text Hypothesis :	yesterday on  18th           december 2021 the team are graded in a day and england 
2024-02-01 22:14:05,858 	Text Alignment  :	S         S   S              S        S        I    I   I      I  I I   I   S       
2024-02-01 22:14:05,858 ========================================================================================================================
2024-02-01 22:14:17,546 Epoch 3334: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:14:17,546 EPOCH 3335
2024-02-01 22:14:31,451 Epoch 3335: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:14:31,451 EPOCH 3336
2024-02-01 22:14:45,257 Epoch 3336: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:14:45,257 EPOCH 3337
2024-02-01 22:14:58,949 Epoch 3337: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:14:58,950 EPOCH 3338
2024-02-01 22:15:12,819 Epoch 3338: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:15:12,819 EPOCH 3339
2024-02-01 22:15:26,653 Epoch 3339: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:15:26,654 EPOCH 3340
2024-02-01 22:15:40,325 Epoch 3340: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:15:40,325 EPOCH 3341
2024-02-01 22:15:54,204 Epoch 3341: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:15:54,204 EPOCH 3342
2024-02-01 22:16:07,759 Epoch 3342: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:16:07,759 EPOCH 3343
2024-02-01 22:16:21,958 Epoch 3343: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:16:21,959 EPOCH 3344
2024-02-01 22:16:35,764 Epoch 3344: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:16:35,765 EPOCH 3345
2024-02-01 22:16:40,268 [Epoch: 3345 Step: 00030100] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:     1137 || Batch Translation Loss:   0.022173 => Txt Tokens per Sec:     2938 || Lr: 0.000050
2024-02-01 22:16:49,675 Epoch 3345: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:16:49,676 EPOCH 3346
2024-02-01 22:17:03,394 Epoch 3346: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 22:17:03,394 EPOCH 3347
2024-02-01 22:17:17,234 Epoch 3347: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:17:17,235 EPOCH 3348
2024-02-01 22:17:31,233 Epoch 3348: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:17:31,234 EPOCH 3349
2024-02-01 22:17:45,000 Epoch 3349: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:17:45,001 EPOCH 3350
2024-02-01 22:17:58,664 Epoch 3350: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:17:58,665 EPOCH 3351
2024-02-01 22:18:12,773 Epoch 3351: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:18:12,774 EPOCH 3352
2024-02-01 22:18:26,853 Epoch 3352: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:18:26,853 EPOCH 3353
2024-02-01 22:18:40,895 Epoch 3353: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:18:40,896 EPOCH 3354
2024-02-01 22:18:54,596 Epoch 3354: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:18:54,597 EPOCH 3355
2024-02-01 22:19:08,671 Epoch 3355: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:19:08,671 EPOCH 3356
2024-02-01 22:19:17,718 [Epoch: 3356 Step: 00030200] Batch Recognition Loss:   0.000106 => Gls Tokens per Sec:      708 || Batch Translation Loss:   0.008560 => Txt Tokens per Sec:     1987 || Lr: 0.000050
2024-02-01 22:19:22,539 Epoch 3356: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:19:22,539 EPOCH 3357
2024-02-01 22:19:36,534 Epoch 3357: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:19:36,535 EPOCH 3358
2024-02-01 22:19:50,551 Epoch 3358: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:19:50,551 EPOCH 3359
2024-02-01 22:20:04,503 Epoch 3359: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:20:04,503 EPOCH 3360
2024-02-01 22:20:18,576 Epoch 3360: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:20:18,576 EPOCH 3361
2024-02-01 22:20:32,449 Epoch 3361: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:20:32,450 EPOCH 3362
2024-02-01 22:20:46,375 Epoch 3362: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:20:46,376 EPOCH 3363
2024-02-01 22:21:00,598 Epoch 3363: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 22:21:00,598 EPOCH 3364
2024-02-01 22:21:14,339 Epoch 3364: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:21:14,339 EPOCH 3365
2024-02-01 22:21:28,155 Epoch 3365: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:21:28,156 EPOCH 3366
2024-02-01 22:21:41,997 Epoch 3366: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:21:41,998 EPOCH 3367
2024-02-01 22:21:53,415 [Epoch: 3367 Step: 00030300] Batch Recognition Loss:   0.000100 => Gls Tokens per Sec:      595 || Batch Translation Loss:   0.011611 => Txt Tokens per Sec:     1677 || Lr: 0.000050
2024-02-01 22:21:55,994 Epoch 3367: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:21:55,995 EPOCH 3368
2024-02-01 22:22:10,072 Epoch 3368: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:22:10,072 EPOCH 3369
2024-02-01 22:22:24,071 Epoch 3369: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 22:22:24,071 EPOCH 3370
2024-02-01 22:22:37,957 Epoch 3370: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:22:37,957 EPOCH 3371
2024-02-01 22:22:51,908 Epoch 3371: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 22:22:51,909 EPOCH 3372
2024-02-01 22:23:05,636 Epoch 3372: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 22:23:05,637 EPOCH 3373
2024-02-01 22:23:19,715 Epoch 3373: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 22:23:19,716 EPOCH 3374
2024-02-01 22:23:33,492 Epoch 3374: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 22:23:33,493 EPOCH 3375
2024-02-01 22:23:47,527 Epoch 3375: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 22:23:47,528 EPOCH 3376
2024-02-01 22:24:01,360 Epoch 3376: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 22:24:01,360 EPOCH 3377
2024-02-01 22:24:15,596 Epoch 3377: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 22:24:15,596 EPOCH 3378
2024-02-01 22:24:27,423 [Epoch: 3378 Step: 00030400] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:      682 || Batch Translation Loss:   0.027259 => Txt Tokens per Sec:     1917 || Lr: 0.000050
2024-02-01 22:24:29,457 Epoch 3378: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 22:24:29,457 EPOCH 3379
2024-02-01 22:24:43,351 Epoch 3379: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 22:24:43,351 EPOCH 3380
2024-02-01 22:24:57,308 Epoch 3380: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 22:24:57,309 EPOCH 3381
2024-02-01 22:25:11,332 Epoch 3381: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 22:25:11,332 EPOCH 3382
2024-02-01 22:25:25,268 Epoch 3382: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 22:25:25,269 EPOCH 3383
2024-02-01 22:25:38,993 Epoch 3383: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 22:25:38,993 EPOCH 3384
2024-02-01 22:25:52,918 Epoch 3384: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 22:25:52,918 EPOCH 3385
2024-02-01 22:26:07,062 Epoch 3385: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 22:26:07,062 EPOCH 3386
2024-02-01 22:26:20,865 Epoch 3386: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 22:26:20,866 EPOCH 3387
2024-02-01 22:26:34,601 Epoch 3387: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 22:26:34,601 EPOCH 3388
2024-02-01 22:26:48,468 Epoch 3388: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 22:26:48,468 EPOCH 3389
2024-02-01 22:27:00,880 [Epoch: 3389 Step: 00030500] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:      753 || Batch Translation Loss:   0.017582 => Txt Tokens per Sec:     2066 || Lr: 0.000050
2024-02-01 22:27:02,631 Epoch 3389: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 22:27:02,631 EPOCH 3390
2024-02-01 22:27:16,549 Epoch 3390: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 22:27:16,550 EPOCH 3391
2024-02-01 22:27:30,418 Epoch 3391: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 22:27:30,419 EPOCH 3392
2024-02-01 22:27:44,410 Epoch 3392: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 22:27:44,411 EPOCH 3393
2024-02-01 22:27:57,980 Epoch 3393: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 22:27:57,981 EPOCH 3394
2024-02-01 22:28:12,120 Epoch 3394: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 22:28:12,120 EPOCH 3395
2024-02-01 22:28:26,119 Epoch 3395: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 22:28:26,120 EPOCH 3396
2024-02-01 22:28:39,940 Epoch 3396: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 22:28:39,940 EPOCH 3397
2024-02-01 22:28:53,977 Epoch 3397: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 22:28:53,978 EPOCH 3398
2024-02-01 22:29:07,982 Epoch 3398: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 22:29:07,983 EPOCH 3399
2024-02-01 22:29:21,873 Epoch 3399: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 22:29:21,874 EPOCH 3400
2024-02-01 22:29:35,742 [Epoch: 3400 Step: 00030600] Batch Recognition Loss:   0.000166 => Gls Tokens per Sec:      767 || Batch Translation Loss:   0.029337 => Txt Tokens per Sec:     2128 || Lr: 0.000050
2024-02-01 22:29:35,742 Epoch 3400: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 22:29:35,743 EPOCH 3401
2024-02-01 22:29:49,984 Epoch 3401: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 22:29:49,984 EPOCH 3402
2024-02-01 22:30:03,842 Epoch 3402: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 22:30:03,842 EPOCH 3403
2024-02-01 22:30:17,964 Epoch 3403: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 22:30:17,965 EPOCH 3404
2024-02-01 22:30:31,770 Epoch 3404: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:30:31,771 EPOCH 3405
2024-02-01 22:30:45,938 Epoch 3405: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 22:30:45,939 EPOCH 3406
2024-02-01 22:30:59,786 Epoch 3406: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 22:30:59,786 EPOCH 3407
2024-02-01 22:31:13,805 Epoch 3407: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 22:31:13,806 EPOCH 3408
2024-02-01 22:31:27,694 Epoch 3408: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 22:31:27,694 EPOCH 3409
2024-02-01 22:31:41,651 Epoch 3409: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:31:41,651 EPOCH 3410
2024-02-01 22:31:55,506 Epoch 3410: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:31:55,506 EPOCH 3411
2024-02-01 22:32:09,511 Epoch 3411: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:32:09,512 EPOCH 3412
2024-02-01 22:32:12,551 [Epoch: 3412 Step: 00030700] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:      421 || Batch Translation Loss:   0.022289 => Txt Tokens per Sec:     1346 || Lr: 0.000050
2024-02-01 22:32:23,548 Epoch 3412: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:32:23,549 EPOCH 3413
2024-02-01 22:32:37,264 Epoch 3413: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:32:37,265 EPOCH 3414
2024-02-01 22:32:51,191 Epoch 3414: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:32:51,192 EPOCH 3415
2024-02-01 22:33:04,928 Epoch 3415: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:33:04,929 EPOCH 3416
2024-02-01 22:33:18,651 Epoch 3416: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 22:33:18,651 EPOCH 3417
2024-02-01 22:33:32,755 Epoch 3417: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 22:33:32,756 EPOCH 3418
2024-02-01 22:33:46,604 Epoch 3418: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 22:33:46,605 EPOCH 3419
2024-02-01 22:34:00,454 Epoch 3419: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 22:34:00,455 EPOCH 3420
2024-02-01 22:34:14,440 Epoch 3420: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 22:34:14,441 EPOCH 3421
2024-02-01 22:34:28,742 Epoch 3421: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 22:34:28,742 EPOCH 3422
2024-02-01 22:34:42,429 Epoch 3422: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 22:34:42,430 EPOCH 3423
2024-02-01 22:34:43,261 [Epoch: 3423 Step: 00030800] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:     3084 || Batch Translation Loss:   0.015796 => Txt Tokens per Sec:     7799 || Lr: 0.000050
2024-02-01 22:34:56,463 Epoch 3423: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 22:34:56,463 EPOCH 3424
2024-02-01 22:35:10,536 Epoch 3424: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-01 22:35:10,537 EPOCH 3425
2024-02-01 22:35:24,543 Epoch 3425: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 22:35:24,544 EPOCH 3426
2024-02-01 22:35:38,252 Epoch 3426: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 22:35:38,253 EPOCH 3427
2024-02-01 22:35:51,919 Epoch 3427: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 22:35:51,920 EPOCH 3428
2024-02-01 22:36:06,110 Epoch 3428: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 22:36:06,110 EPOCH 3429
2024-02-01 22:36:19,841 Epoch 3429: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 22:36:19,841 EPOCH 3430
2024-02-01 22:36:33,646 Epoch 3430: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 22:36:33,646 EPOCH 3431
2024-02-01 22:36:47,455 Epoch 3431: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 22:36:47,455 EPOCH 3432
2024-02-01 22:37:01,574 Epoch 3432: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 22:37:01,575 EPOCH 3433
2024-02-01 22:37:15,610 Epoch 3433: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 22:37:15,611 EPOCH 3434
2024-02-01 22:37:22,436 [Epoch: 3434 Step: 00030900] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:      432 || Batch Translation Loss:   0.022854 => Txt Tokens per Sec:     1353 || Lr: 0.000050
2024-02-01 22:37:29,474 Epoch 3434: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 22:37:29,475 EPOCH 3435
2024-02-01 22:37:43,426 Epoch 3435: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 22:37:43,427 EPOCH 3436
2024-02-01 22:37:57,428 Epoch 3436: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 22:37:57,429 EPOCH 3437
2024-02-01 22:38:11,232 Epoch 3437: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 22:38:11,233 EPOCH 3438
2024-02-01 22:38:25,203 Epoch 3438: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 22:38:25,204 EPOCH 3439
2024-02-01 22:38:39,232 Epoch 3439: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 22:38:39,233 EPOCH 3440
2024-02-01 22:38:52,971 Epoch 3440: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 22:38:52,972 EPOCH 3441
2024-02-01 22:39:07,262 Epoch 3441: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 22:39:07,262 EPOCH 3442
2024-02-01 22:39:21,196 Epoch 3442: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 22:39:21,197 EPOCH 3443
2024-02-01 22:39:34,829 Epoch 3443: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 22:39:34,829 EPOCH 3444
2024-02-01 22:39:48,743 Epoch 3444: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 22:39:48,743 EPOCH 3445
2024-02-01 22:39:54,346 [Epoch: 3445 Step: 00031000] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:      914 || Batch Translation Loss:   0.027083 => Txt Tokens per Sec:     2615 || Lr: 0.000050
2024-02-01 22:40:02,393 Epoch 3445: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 22:40:02,394 EPOCH 3446
2024-02-01 22:40:16,408 Epoch 3446: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-01 22:40:16,409 EPOCH 3447
2024-02-01 22:40:29,901 Epoch 3447: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 22:40:29,901 EPOCH 3448
2024-02-01 22:40:44,132 Epoch 3448: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 22:40:44,132 EPOCH 3449
2024-02-01 22:40:58,199 Epoch 3449: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 22:40:58,200 EPOCH 3450
2024-02-01 22:41:12,363 Epoch 3450: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 22:41:12,364 EPOCH 3451
2024-02-01 22:41:26,359 Epoch 3451: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 22:41:26,360 EPOCH 3452
2024-02-01 22:41:40,128 Epoch 3452: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 22:41:40,129 EPOCH 3453
2024-02-01 22:41:54,138 Epoch 3453: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 22:41:54,139 EPOCH 3454
2024-02-01 22:42:08,049 Epoch 3454: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 22:42:08,050 EPOCH 3455
2024-02-01 22:42:21,980 Epoch 3455: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-01 22:42:21,980 EPOCH 3456
2024-02-01 22:42:31,729 [Epoch: 3456 Step: 00031100] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:      565 || Batch Translation Loss:   0.014801 => Txt Tokens per Sec:     1718 || Lr: 0.000050
2024-02-01 22:42:35,898 Epoch 3456: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 22:42:35,898 EPOCH 3457
2024-02-01 22:42:49,477 Epoch 3457: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 22:42:49,478 EPOCH 3458
2024-02-01 22:43:03,388 Epoch 3458: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 22:43:03,389 EPOCH 3459
2024-02-01 22:43:17,143 Epoch 3459: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 22:43:17,143 EPOCH 3460
2024-02-01 22:43:31,325 Epoch 3460: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 22:43:31,326 EPOCH 3461
2024-02-01 22:43:45,400 Epoch 3461: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 22:43:45,400 EPOCH 3462
2024-02-01 22:43:59,260 Epoch 3462: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 22:43:59,261 EPOCH 3463
2024-02-01 22:44:13,128 Epoch 3463: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 22:44:13,128 EPOCH 3464
2024-02-01 22:44:27,019 Epoch 3464: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 22:44:27,019 EPOCH 3465
2024-02-01 22:44:41,094 Epoch 3465: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 22:44:41,094 EPOCH 3466
2024-02-01 22:44:55,144 Epoch 3466: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 22:44:55,145 EPOCH 3467
2024-02-01 22:45:03,486 [Epoch: 3467 Step: 00031200] Batch Recognition Loss:   0.000118 => Gls Tokens per Sec:      921 || Batch Translation Loss:   0.010643 => Txt Tokens per Sec:     2458 || Lr: 0.000050
2024-02-01 22:45:09,149 Epoch 3467: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 22:45:09,150 EPOCH 3468
2024-02-01 22:45:22,925 Epoch 3468: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 22:45:22,925 EPOCH 3469
2024-02-01 22:45:36,565 Epoch 3469: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 22:45:36,566 EPOCH 3470
2024-02-01 22:45:50,618 Epoch 3470: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 22:45:50,618 EPOCH 3471
2024-02-01 22:46:04,617 Epoch 3471: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 22:46:04,618 EPOCH 3472
2024-02-01 22:46:18,498 Epoch 3472: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 22:46:18,499 EPOCH 3473
2024-02-01 22:46:32,466 Epoch 3473: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 22:46:32,467 EPOCH 3474
2024-02-01 22:46:46,387 Epoch 3474: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 22:46:46,387 EPOCH 3475
2024-02-01 22:47:00,101 Epoch 3475: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 22:47:00,101 EPOCH 3476
2024-02-01 22:47:14,047 Epoch 3476: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 22:47:14,048 EPOCH 3477
2024-02-01 22:47:28,176 Epoch 3477: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:47:28,177 EPOCH 3478
2024-02-01 22:47:38,235 [Epoch: 3478 Step: 00031300] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:      891 || Batch Translation Loss:   0.021291 => Txt Tokens per Sec:     2421 || Lr: 0.000050
2024-02-01 22:47:42,211 Epoch 3478: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 22:47:42,211 EPOCH 3479
2024-02-01 22:47:56,309 Epoch 3479: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:47:56,310 EPOCH 3480
2024-02-01 22:48:10,251 Epoch 3480: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 22:48:10,252 EPOCH 3481
2024-02-01 22:48:23,784 Epoch 3481: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 22:48:23,784 EPOCH 3482
2024-02-01 22:48:37,781 Epoch 3482: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 22:48:37,782 EPOCH 3483
2024-02-01 22:48:51,717 Epoch 3483: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 22:48:51,717 EPOCH 3484
2024-02-01 22:49:05,532 Epoch 3484: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 22:49:05,532 EPOCH 3485
2024-02-01 22:49:19,107 Epoch 3485: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 22:49:19,107 EPOCH 3486
2024-02-01 22:49:32,599 Epoch 3486: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 22:49:32,600 EPOCH 3487
2024-02-01 22:49:46,435 Epoch 3487: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:49:46,436 EPOCH 3488
2024-02-01 22:50:00,504 Epoch 3488: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:50:00,505 EPOCH 3489
2024-02-01 22:50:13,990 [Epoch: 3489 Step: 00031400] Batch Recognition Loss:   0.000104 => Gls Tokens per Sec:      693 || Batch Translation Loss:   0.014584 => Txt Tokens per Sec:     1951 || Lr: 0.000050
2024-02-01 22:50:14,371 Epoch 3489: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:50:14,371 EPOCH 3490
2024-02-01 22:50:28,130 Epoch 3490: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 22:50:28,131 EPOCH 3491
2024-02-01 22:50:42,163 Epoch 3491: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 22:50:42,164 EPOCH 3492
2024-02-01 22:50:56,157 Epoch 3492: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:50:56,158 EPOCH 3493
2024-02-01 22:51:10,110 Epoch 3493: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 22:51:10,110 EPOCH 3494
2024-02-01 22:51:23,956 Epoch 3494: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 22:51:23,957 EPOCH 3495
2024-02-01 22:51:37,746 Epoch 3495: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 22:51:37,747 EPOCH 3496
2024-02-01 22:51:51,627 Epoch 3496: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-01 22:51:51,628 EPOCH 3497
2024-02-01 22:52:05,613 Epoch 3497: Total Training Recognition Loss 0.00  Total Training Translation Loss 3.81 
2024-02-01 22:52:05,614 EPOCH 3498
2024-02-01 22:52:19,398 Epoch 3498: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.87 
2024-02-01 22:52:19,398 EPOCH 3499
2024-02-01 22:52:33,369 Epoch 3499: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.77 
2024-02-01 22:52:33,370 EPOCH 3500
2024-02-01 22:52:47,366 [Epoch: 3500 Step: 00031500] Batch Recognition Loss:   0.002565 => Gls Tokens per Sec:      760 || Batch Translation Loss:   0.500239 => Txt Tokens per Sec:     2109 || Lr: 0.000050
2024-02-01 22:52:47,367 Epoch 3500: Total Training Recognition Loss 0.01  Total Training Translation Loss 6.63 
2024-02-01 22:52:47,367 EPOCH 3501
2024-02-01 22:53:01,259 Epoch 3501: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.56 
2024-02-01 22:53:01,259 EPOCH 3502
2024-02-01 22:53:15,495 Epoch 3502: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-01 22:53:15,495 EPOCH 3503
2024-02-01 22:53:29,496 Epoch 3503: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-01 22:53:29,497 EPOCH 3504
2024-02-01 22:53:43,266 Epoch 3504: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-01 22:53:43,266 EPOCH 3505
2024-02-01 22:53:57,033 Epoch 3505: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-01 22:53:57,034 EPOCH 3506
2024-02-01 22:54:10,883 Epoch 3506: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-01 22:54:10,883 EPOCH 3507
2024-02-01 22:54:24,658 Epoch 3507: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 22:54:24,658 EPOCH 3508
2024-02-01 22:54:38,898 Epoch 3508: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 22:54:38,899 EPOCH 3509
2024-02-01 22:54:52,494 Epoch 3509: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 22:54:52,494 EPOCH 3510
2024-02-01 22:55:06,297 Epoch 3510: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 22:55:06,297 EPOCH 3511
2024-02-01 22:55:20,266 Epoch 3511: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 22:55:20,267 EPOCH 3512
2024-02-01 22:55:20,901 [Epoch: 3512 Step: 00031600] Batch Recognition Loss:   0.000265 => Gls Tokens per Sec:     2025 || Batch Translation Loss:   0.019225 => Txt Tokens per Sec:     5932 || Lr: 0.000050
2024-02-01 22:55:34,033 Epoch 3512: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 22:55:34,033 EPOCH 3513
2024-02-01 22:55:48,057 Epoch 3513: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 22:55:48,058 EPOCH 3514
2024-02-01 22:56:02,033 Epoch 3514: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 22:56:02,033 EPOCH 3515
2024-02-01 22:56:15,821 Epoch 3515: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 22:56:15,821 EPOCH 3516
2024-02-01 22:56:29,701 Epoch 3516: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 22:56:29,702 EPOCH 3517
2024-02-01 22:56:43,145 Epoch 3517: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 22:56:43,145 EPOCH 3518
2024-02-01 22:56:57,116 Epoch 3518: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 22:56:57,117 EPOCH 3519
2024-02-01 22:57:11,079 Epoch 3519: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 22:57:11,080 EPOCH 3520
2024-02-01 22:57:24,795 Epoch 3520: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:57:24,796 EPOCH 3521
2024-02-01 22:57:38,973 Epoch 3521: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:57:38,973 EPOCH 3522
2024-02-01 22:57:52,401 Epoch 3522: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:57:52,402 EPOCH 3523
2024-02-01 22:57:56,202 [Epoch: 3523 Step: 00031700] Batch Recognition Loss:   0.000246 => Gls Tokens per Sec:      440 || Batch Translation Loss:   0.007673 => Txt Tokens per Sec:     1144 || Lr: 0.000050
2024-02-01 22:58:06,773 Epoch 3523: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:58:06,774 EPOCH 3524
2024-02-01 22:58:20,887 Epoch 3524: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:58:20,887 EPOCH 3525
2024-02-01 22:58:34,950 Epoch 3525: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 22:58:34,951 EPOCH 3526
2024-02-01 22:58:48,703 Epoch 3526: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:58:48,704 EPOCH 3527
2024-02-01 22:59:02,246 Epoch 3527: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 22:59:02,246 EPOCH 3528
2024-02-01 22:59:16,031 Epoch 3528: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 22:59:16,032 EPOCH 3529
2024-02-01 22:59:29,752 Epoch 3529: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 22:59:29,752 EPOCH 3530
2024-02-01 22:59:43,705 Epoch 3530: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:59:43,706 EPOCH 3531
2024-02-01 22:59:57,630 Epoch 3531: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 22:59:57,630 EPOCH 3532
2024-02-01 23:00:12,015 Epoch 3532: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:00:12,015 EPOCH 3533
2024-02-01 23:00:25,955 Epoch 3533: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:00:25,956 EPOCH 3534
2024-02-01 23:00:36,051 [Epoch: 3534 Step: 00031800] Batch Recognition Loss:   0.000193 => Gls Tokens per Sec:      292 || Batch Translation Loss:   0.017274 => Txt Tokens per Sec:      988 || Lr: 0.000050
2024-02-01 23:00:39,810 Epoch 3534: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:00:39,810 EPOCH 3535
2024-02-01 23:00:53,373 Epoch 3535: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:00:53,373 EPOCH 3536
2024-02-01 23:01:07,419 Epoch 3536: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:01:07,420 EPOCH 3537
2024-02-01 23:01:21,285 Epoch 3537: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:01:21,286 EPOCH 3538
2024-02-01 23:01:35,235 Epoch 3538: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:01:35,235 EPOCH 3539
2024-02-01 23:01:49,184 Epoch 3539: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:01:49,184 EPOCH 3540
2024-02-01 23:02:03,246 Epoch 3540: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:02:03,247 EPOCH 3541
2024-02-01 23:02:17,158 Epoch 3541: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:02:17,158 EPOCH 3542
2024-02-01 23:02:30,800 Epoch 3542: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:02:30,801 EPOCH 3543
2024-02-01 23:02:44,623 Epoch 3543: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:02:44,624 EPOCH 3544
2024-02-01 23:02:58,878 Epoch 3544: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:02:58,879 EPOCH 3545
2024-02-01 23:03:04,893 [Epoch: 3545 Step: 00031900] Batch Recognition Loss:   0.000117 => Gls Tokens per Sec:      703 || Batch Translation Loss:   0.011298 => Txt Tokens per Sec:     2039 || Lr: 0.000050
2024-02-01 23:03:12,969 Epoch 3545: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:03:12,970 EPOCH 3546
2024-02-01 23:03:27,062 Epoch 3546: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:03:27,063 EPOCH 3547
2024-02-01 23:03:41,140 Epoch 3547: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:03:41,140 EPOCH 3548
2024-02-01 23:03:54,988 Epoch 3548: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:03:54,989 EPOCH 3549
2024-02-01 23:04:08,867 Epoch 3549: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:04:08,867 EPOCH 3550
2024-02-01 23:04:22,708 Epoch 3550: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:04:22,708 EPOCH 3551
2024-02-01 23:04:36,702 Epoch 3551: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:04:36,702 EPOCH 3552
2024-02-01 23:04:50,674 Epoch 3552: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:04:50,674 EPOCH 3553
2024-02-01 23:05:04,528 Epoch 3553: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:05:04,529 EPOCH 3554
2024-02-01 23:05:18,600 Epoch 3554: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:05:18,600 EPOCH 3555
2024-02-01 23:05:32,442 Epoch 3555: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:05:32,443 EPOCH 3556
2024-02-01 23:05:40,312 [Epoch: 3556 Step: 00032000] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:      813 || Batch Translation Loss:   0.016707 => Txt Tokens per Sec:     2306 || Lr: 0.000050
2024-02-01 23:06:00,632 Validation result at epoch 3556, step    32000: duration: 20.3202s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00021	Translation Loss: 98957.46094	PPL: 19983.82422
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.70	(BLEU-1: 11.03,	BLEU-2: 3.33,	BLEU-3: 1.34,	BLEU-4: 0.70)
	CHRF 17.26	ROUGE 9.41
2024-02-01 23:06:00,633 Logging Recognition and Translation Outputs
2024-02-01 23:06:00,634 ========================================================================================================================
2024-02-01 23:06:00,634 Logging Sequence: 86_11.00
2024-02-01 23:06:00,634 	Gloss Reference :	A B+C+D+E
2024-02-01 23:06:00,634 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 23:06:00,634 	Gloss Alignment :	         
2024-02-01 23:06:00,635 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 23:06:00,635 	Text Reference  :	he was ** ***** ********* *** **** 66 years     old   
2024-02-01 23:06:00,635 	Text Hypothesis :	** was to score extremely fit when a  statement saying
2024-02-01 23:06:00,635 	Text Alignment  :	D      I  I     I         I   I    S  S         S     
2024-02-01 23:06:00,636 ========================================================================================================================
2024-02-01 23:06:00,636 Logging Sequence: 67_16.00
2024-02-01 23:06:00,636 	Gloss Reference :	A B+C+D+E
2024-02-01 23:06:00,636 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 23:06:00,636 	Gloss Alignment :	         
2024-02-01 23:06:00,636 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 23:06:00,637 	Text Reference  :	**** * ***** ****** ****** to        help india's   fight    against the covid-19 pandemic
2024-02-01 23:06:00,637 	Text Hypothesis :	what a close friend andrew neophitou and  extremely saddened by      his his      life    
2024-02-01 23:06:00,637 	Text Alignment  :	I    I I     I      I      S         S    S         S        S       S   S        S       
2024-02-01 23:06:00,638 ========================================================================================================================
2024-02-01 23:06:00,638 Logging Sequence: 69_177.00
2024-02-01 23:06:00,638 	Gloss Reference :	A B+C+D+E
2024-02-01 23:06:00,638 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 23:06:00,638 	Gloss Alignment :	         
2024-02-01 23:06:00,638 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 23:06:00,640 	Text Reference  :	he said 'i will continue playing i       know    it's       about time   i       retire i   also    have   a   knee   condition
2024-02-01 23:06:00,640 	Text Hypothesis :	** **** ** **** ******** when    england started protesting was   played between mumbai and gujarat titans and punjab kings    
2024-02-01 23:06:00,640 	Text Alignment  :	D  D    D  D    D        S       S       S       S          S     S      S       S      S   S       S      S   S      S        
2024-02-01 23:06:00,640 ========================================================================================================================
2024-02-01 23:06:00,640 Logging Sequence: 165_615.00
2024-02-01 23:06:00,641 	Gloss Reference :	A B+C+D+E
2024-02-01 23:06:00,641 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 23:06:00,641 	Gloss Alignment :	         
2024-02-01 23:06:00,641 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 23:06:00,641 	Text Reference  :	** ** ** *********** ******* we defeated pakistan too  
2024-02-01 23:06:00,641 	Text Hypothesis :	it is an interesting history of india    and      japan
2024-02-01 23:06:00,642 	Text Alignment  :	I  I  I  I           I       S  S        S        S    
2024-02-01 23:06:00,642 ========================================================================================================================
2024-02-01 23:06:00,642 Logging Sequence: 61_5.00
2024-02-01 23:06:00,642 	Gloss Reference :	A B+C+D+E
2024-02-01 23:06:00,642 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 23:06:00,642 	Gloss Alignment :	         
2024-02-01 23:06:00,642 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 23:06:00,644 	Text Reference  :	******* *** they rivalry is   seen the ************** ***** ***** ** ****** *** most    during india pakistan cricket matches  
2024-02-01 23:06:00,644 	Text Hypothesis :	however you all  know    that at   the india-pakistan match would be played and excited to     see   the      fan     following
2024-02-01 23:06:00,644 	Text Alignment  :	I       I   S    S       S    S        I              I     I     I  I      I   S       S      S     S        S       S        
2024-02-01 23:06:00,644 ========================================================================================================================
2024-02-01 23:06:06,629 Epoch 3556: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:06:06,630 EPOCH 3557
2024-02-01 23:06:20,466 Epoch 3557: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:06:20,467 EPOCH 3558
2024-02-01 23:06:34,276 Epoch 3558: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:06:34,276 EPOCH 3559
2024-02-01 23:06:48,252 Epoch 3559: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:06:48,252 EPOCH 3560
2024-02-01 23:07:02,189 Epoch 3560: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:07:02,190 EPOCH 3561
2024-02-01 23:07:16,046 Epoch 3561: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:07:16,047 EPOCH 3562
2024-02-01 23:07:30,092 Epoch 3562: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:07:30,093 EPOCH 3563
2024-02-01 23:07:43,990 Epoch 3563: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:07:43,991 EPOCH 3564
2024-02-01 23:07:57,982 Epoch 3564: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:07:57,983 EPOCH 3565
2024-02-01 23:08:12,048 Epoch 3565: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:08:12,048 EPOCH 3566
2024-02-01 23:08:25,425 Epoch 3566: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:08:25,425 EPOCH 3567
2024-02-01 23:08:34,716 [Epoch: 3567 Step: 00032100] Batch Recognition Loss:   0.000111 => Gls Tokens per Sec:      731 || Batch Translation Loss:   0.011588 => Txt Tokens per Sec:     1913 || Lr: 0.000050
2024-02-01 23:08:39,411 Epoch 3567: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:08:39,411 EPOCH 3568
2024-02-01 23:08:53,133 Epoch 3568: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:08:53,133 EPOCH 3569
2024-02-01 23:09:06,984 Epoch 3569: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:09:06,985 EPOCH 3570
2024-02-01 23:09:20,557 Epoch 3570: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:09:20,557 EPOCH 3571
2024-02-01 23:09:34,432 Epoch 3571: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:09:34,433 EPOCH 3572
2024-02-01 23:09:48,260 Epoch 3572: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:09:48,261 EPOCH 3573
2024-02-01 23:10:02,426 Epoch 3573: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:10:02,426 EPOCH 3574
2024-02-01 23:10:16,359 Epoch 3574: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:10:16,360 EPOCH 3575
2024-02-01 23:10:30,334 Epoch 3575: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:10:30,335 EPOCH 3576
2024-02-01 23:10:44,327 Epoch 3576: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:10:44,328 EPOCH 3577
2024-02-01 23:10:58,481 Epoch 3577: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:10:58,482 EPOCH 3578
2024-02-01 23:11:07,989 [Epoch: 3578 Step: 00032200] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:      849 || Batch Translation Loss:   0.021357 => Txt Tokens per Sec:     2239 || Lr: 0.000050
2024-02-01 23:11:12,384 Epoch 3578: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:11:12,385 EPOCH 3579
2024-02-01 23:11:26,314 Epoch 3579: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:11:26,314 EPOCH 3580
2024-02-01 23:11:40,225 Epoch 3580: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:11:40,226 EPOCH 3581
2024-02-01 23:11:54,068 Epoch 3581: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:11:54,069 EPOCH 3582
2024-02-01 23:12:07,873 Epoch 3582: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:12:07,874 EPOCH 3583
2024-02-01 23:12:21,784 Epoch 3583: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:12:21,785 EPOCH 3584
2024-02-01 23:12:35,749 Epoch 3584: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:12:35,749 EPOCH 3585
2024-02-01 23:12:49,565 Epoch 3585: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:12:49,566 EPOCH 3586
2024-02-01 23:13:03,213 Epoch 3586: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:13:03,214 EPOCH 3587
2024-02-01 23:13:16,943 Epoch 3587: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:13:16,944 EPOCH 3588
2024-02-01 23:13:30,808 Epoch 3588: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:13:30,808 EPOCH 3589
2024-02-01 23:13:40,933 [Epoch: 3589 Step: 00032300] Batch Recognition Loss:   0.000101 => Gls Tokens per Sec:      924 || Batch Translation Loss:   0.012616 => Txt Tokens per Sec:     2472 || Lr: 0.000050
2024-02-01 23:13:44,527 Epoch 3589: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 23:13:44,527 EPOCH 3590
2024-02-01 23:13:58,577 Epoch 3590: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 23:13:58,578 EPOCH 3591
2024-02-01 23:14:12,497 Epoch 3591: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 23:14:12,497 EPOCH 3592
2024-02-01 23:14:26,469 Epoch 3592: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:14:26,469 EPOCH 3593
2024-02-01 23:14:40,392 Epoch 3593: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:14:40,393 EPOCH 3594
2024-02-01 23:14:54,104 Epoch 3594: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:14:54,104 EPOCH 3595
2024-02-01 23:15:08,004 Epoch 3595: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 23:15:08,005 EPOCH 3596
2024-02-01 23:15:21,947 Epoch 3596: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:15:21,947 EPOCH 3597
2024-02-01 23:15:36,088 Epoch 3597: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 23:15:36,088 EPOCH 3598
2024-02-01 23:15:49,897 Epoch 3598: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:15:49,897 EPOCH 3599
2024-02-01 23:16:03,810 Epoch 3599: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:16:03,811 EPOCH 3600
2024-02-01 23:16:17,800 [Epoch: 3600 Step: 00032400] Batch Recognition Loss:   0.000105 => Gls Tokens per Sec:      760 || Batch Translation Loss:   0.014812 => Txt Tokens per Sec:     2110 || Lr: 0.000050
2024-02-01 23:16:17,800 Epoch 3600: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 23:16:17,801 EPOCH 3601
2024-02-01 23:16:31,627 Epoch 3601: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:16:31,627 EPOCH 3602
2024-02-01 23:16:45,604 Epoch 3602: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:16:45,604 EPOCH 3603
2024-02-01 23:16:59,372 Epoch 3603: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 23:16:59,372 EPOCH 3604
2024-02-01 23:17:13,487 Epoch 3604: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 23:17:13,488 EPOCH 3605
2024-02-01 23:17:27,371 Epoch 3605: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 23:17:27,372 EPOCH 3606
2024-02-01 23:17:41,382 Epoch 3606: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 23:17:41,383 EPOCH 3607
2024-02-01 23:17:55,370 Epoch 3607: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 23:17:55,370 EPOCH 3608
2024-02-01 23:18:09,338 Epoch 3608: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 23:18:09,339 EPOCH 3609
2024-02-01 23:18:23,240 Epoch 3609: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 23:18:23,241 EPOCH 3610
2024-02-01 23:18:37,093 Epoch 3610: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 23:18:37,094 EPOCH 3611
2024-02-01 23:18:50,703 Epoch 3611: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 23:18:50,704 EPOCH 3612
2024-02-01 23:18:52,441 [Epoch: 3612 Step: 00032500] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:      737 || Batch Translation Loss:   0.042435 => Txt Tokens per Sec:     2231 || Lr: 0.000050
2024-02-01 23:19:04,859 Epoch 3612: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 23:19:04,859 EPOCH 3613
2024-02-01 23:19:18,855 Epoch 3613: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 23:19:18,856 EPOCH 3614
2024-02-01 23:19:32,312 Epoch 3614: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 23:19:32,313 EPOCH 3615
2024-02-01 23:19:46,200 Epoch 3615: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 23:19:46,200 EPOCH 3616
2024-02-01 23:19:59,871 Epoch 3616: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 23:19:59,872 EPOCH 3617
2024-02-01 23:20:13,628 Epoch 3617: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 23:20:13,629 EPOCH 3618
2024-02-01 23:20:27,588 Epoch 3618: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 23:20:27,589 EPOCH 3619
2024-02-01 23:20:41,220 Epoch 3619: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 23:20:41,220 EPOCH 3620
2024-02-01 23:20:55,016 Epoch 3620: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 23:20:55,017 EPOCH 3621
2024-02-01 23:21:08,979 Epoch 3621: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 23:21:08,980 EPOCH 3622
2024-02-01 23:21:22,793 Epoch 3622: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 23:21:22,794 EPOCH 3623
2024-02-01 23:21:24,740 [Epoch: 3623 Step: 00032600] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:     1317 || Batch Translation Loss:   0.028192 => Txt Tokens per Sec:     3516 || Lr: 0.000050
2024-02-01 23:21:36,657 Epoch 3623: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 23:21:36,657 EPOCH 3624
2024-02-01 23:21:50,427 Epoch 3624: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 23:21:50,427 EPOCH 3625
2024-02-01 23:22:04,082 Epoch 3625: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 23:22:04,082 EPOCH 3626
2024-02-01 23:22:18,188 Epoch 3626: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 23:22:18,189 EPOCH 3627
2024-02-01 23:22:32,064 Epoch 3627: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 23:22:32,065 EPOCH 3628
2024-02-01 23:22:45,680 Epoch 3628: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 23:22:45,681 EPOCH 3629
2024-02-01 23:22:59,716 Epoch 3629: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 23:22:59,716 EPOCH 3630
2024-02-01 23:23:13,827 Epoch 3630: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 23:23:13,828 EPOCH 3631
2024-02-01 23:23:27,497 Epoch 3631: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 23:23:27,498 EPOCH 3632
2024-02-01 23:23:41,497 Epoch 3632: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 23:23:41,498 EPOCH 3633
2024-02-01 23:23:55,353 Epoch 3633: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 23:23:55,354 EPOCH 3634
2024-02-01 23:23:59,942 [Epoch: 3634 Step: 00032700] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:      837 || Batch Translation Loss:   0.013203 => Txt Tokens per Sec:     2285 || Lr: 0.000050
2024-02-01 23:24:09,225 Epoch 3634: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 23:24:09,226 EPOCH 3635
2024-02-01 23:24:23,404 Epoch 3635: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 23:24:23,405 EPOCH 3636
2024-02-01 23:24:37,104 Epoch 3636: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 23:24:37,104 EPOCH 3637
2024-02-01 23:24:50,998 Epoch 3637: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 23:24:50,998 EPOCH 3638
2024-02-01 23:25:04,602 Epoch 3638: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 23:25:04,602 EPOCH 3639
2024-02-01 23:25:18,884 Epoch 3639: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 23:25:18,884 EPOCH 3640
2024-02-01 23:25:32,771 Epoch 3640: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 23:25:32,772 EPOCH 3641
2024-02-01 23:25:46,822 Epoch 3641: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 23:25:46,822 EPOCH 3642
2024-02-01 23:26:00,380 Epoch 3642: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 23:26:00,381 EPOCH 3643
2024-02-01 23:26:14,398 Epoch 3643: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 23:26:14,399 EPOCH 3644
2024-02-01 23:26:28,463 Epoch 3644: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 23:26:28,464 EPOCH 3645
2024-02-01 23:26:35,690 [Epoch: 3645 Step: 00032800] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:      585 || Batch Translation Loss:   0.023962 => Txt Tokens per Sec:     1683 || Lr: 0.000050
2024-02-01 23:26:42,336 Epoch 3645: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-01 23:26:42,337 EPOCH 3646
2024-02-01 23:26:56,237 Epoch 3646: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 23:26:56,238 EPOCH 3647
2024-02-01 23:27:09,923 Epoch 3647: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 23:27:09,923 EPOCH 3648
2024-02-01 23:27:23,629 Epoch 3648: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 23:27:23,630 EPOCH 3649
2024-02-01 23:27:37,739 Epoch 3649: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 23:27:37,739 EPOCH 3650
2024-02-01 23:27:51,881 Epoch 3650: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 23:27:51,881 EPOCH 3651
2024-02-01 23:28:05,834 Epoch 3651: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:28:05,835 EPOCH 3652
2024-02-01 23:28:19,580 Epoch 3652: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:28:19,581 EPOCH 3653
2024-02-01 23:28:33,880 Epoch 3653: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:28:33,880 EPOCH 3654
2024-02-01 23:28:47,747 Epoch 3654: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:28:47,747 EPOCH 3655
2024-02-01 23:29:01,654 Epoch 3655: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:29:01,654 EPOCH 3656
2024-02-01 23:29:09,090 [Epoch: 3656 Step: 00032900] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:      741 || Batch Translation Loss:   0.017969 => Txt Tokens per Sec:     1871 || Lr: 0.000050
2024-02-01 23:29:15,811 Epoch 3656: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:29:15,812 EPOCH 3657
2024-02-01 23:29:29,749 Epoch 3657: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:29:29,749 EPOCH 3658
2024-02-01 23:29:43,401 Epoch 3658: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:29:43,401 EPOCH 3659
2024-02-01 23:29:57,166 Epoch 3659: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 23:29:57,167 EPOCH 3660
2024-02-01 23:30:10,915 Epoch 3660: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:30:10,916 EPOCH 3661
2024-02-01 23:30:24,823 Epoch 3661: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 23:30:24,824 EPOCH 3662
2024-02-01 23:30:38,674 Epoch 3662: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.90 
2024-02-01 23:30:38,674 EPOCH 3663
2024-02-01 23:30:52,686 Epoch 3663: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.76 
2024-02-01 23:30:52,687 EPOCH 3664
2024-02-01 23:31:06,272 Epoch 3664: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-01 23:31:06,273 EPOCH 3665
2024-02-01 23:31:20,381 Epoch 3665: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-01 23:31:20,382 EPOCH 3666
2024-02-01 23:31:34,486 Epoch 3666: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 23:31:34,486 EPOCH 3667
2024-02-01 23:31:41,481 [Epoch: 3667 Step: 00033000] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:     1098 || Batch Translation Loss:   0.016013 => Txt Tokens per Sec:     2943 || Lr: 0.000050
2024-02-01 23:31:48,480 Epoch 3667: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-01 23:31:48,480 EPOCH 3668
2024-02-01 23:32:02,303 Epoch 3668: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 23:32:02,304 EPOCH 3669
2024-02-01 23:32:16,314 Epoch 3669: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-01 23:32:16,315 EPOCH 3670
2024-02-01 23:32:29,949 Epoch 3670: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 23:32:29,949 EPOCH 3671
2024-02-01 23:32:43,817 Epoch 3671: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 23:32:43,818 EPOCH 3672
2024-02-01 23:32:57,591 Epoch 3672: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 23:32:57,592 EPOCH 3673
2024-02-01 23:33:11,315 Epoch 3673: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 23:33:11,316 EPOCH 3674
2024-02-01 23:33:25,576 Epoch 3674: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 23:33:25,577 EPOCH 3675
2024-02-01 23:33:39,447 Epoch 3675: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 23:33:39,448 EPOCH 3676
2024-02-01 23:33:53,457 Epoch 3676: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:33:53,457 EPOCH 3677
2024-02-01 23:34:07,807 Epoch 3677: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:34:07,808 EPOCH 3678
2024-02-01 23:34:18,272 [Epoch: 3678 Step: 00033100] Batch Recognition Loss:   0.000112 => Gls Tokens per Sec:      771 || Batch Translation Loss:   0.015118 => Txt Tokens per Sec:     2215 || Lr: 0.000050
2024-02-01 23:34:21,587 Epoch 3678: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:34:21,587 EPOCH 3679
2024-02-01 23:34:35,630 Epoch 3679: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:34:35,631 EPOCH 3680
2024-02-01 23:34:49,288 Epoch 3680: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:34:49,288 EPOCH 3681
2024-02-01 23:35:02,968 Epoch 3681: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:35:02,968 EPOCH 3682
2024-02-01 23:35:17,005 Epoch 3682: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:35:17,006 EPOCH 3683
2024-02-01 23:35:30,980 Epoch 3683: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:35:30,980 EPOCH 3684
2024-02-01 23:35:44,782 Epoch 3684: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:35:44,783 EPOCH 3685
2024-02-01 23:35:58,578 Epoch 3685: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:35:58,579 EPOCH 3686
2024-02-01 23:36:12,469 Epoch 3686: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:36:12,469 EPOCH 3687
2024-02-01 23:36:26,196 Epoch 3687: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 23:36:26,197 EPOCH 3688
2024-02-01 23:36:39,912 Epoch 3688: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 23:36:39,912 EPOCH 3689
2024-02-01 23:36:50,723 [Epoch: 3689 Step: 00033200] Batch Recognition Loss:   0.000115 => Gls Tokens per Sec:      947 || Batch Translation Loss:   0.008193 => Txt Tokens per Sec:     2601 || Lr: 0.000050
2024-02-01 23:36:53,994 Epoch 3689: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:36:53,994 EPOCH 3690
2024-02-01 23:37:07,973 Epoch 3690: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 23:37:07,974 EPOCH 3691
2024-02-01 23:37:21,829 Epoch 3691: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:37:21,829 EPOCH 3692
2024-02-01 23:37:35,857 Epoch 3692: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:37:35,858 EPOCH 3693
2024-02-01 23:37:49,844 Epoch 3693: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:37:49,845 EPOCH 3694
2024-02-01 23:38:03,660 Epoch 3694: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:38:03,660 EPOCH 3695
2024-02-01 23:38:17,701 Epoch 3695: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 23:38:17,702 EPOCH 3696
2024-02-01 23:38:31,821 Epoch 3696: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 23:38:31,821 EPOCH 3697
2024-02-01 23:38:45,820 Epoch 3697: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:38:45,820 EPOCH 3698
2024-02-01 23:38:59,721 Epoch 3698: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:38:59,722 EPOCH 3699
2024-02-01 23:39:14,347 Epoch 3699: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:39:14,348 EPOCH 3700
2024-02-01 23:39:28,126 [Epoch: 3700 Step: 00033300] Batch Recognition Loss:   0.000104 => Gls Tokens per Sec:      772 || Batch Translation Loss:   0.011446 => Txt Tokens per Sec:     2142 || Lr: 0.000050
2024-02-01 23:39:28,126 Epoch 3700: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 23:39:28,127 EPOCH 3701
2024-02-01 23:39:41,907 Epoch 3701: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:39:41,907 EPOCH 3702
2024-02-01 23:39:55,852 Epoch 3702: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:39:55,852 EPOCH 3703
2024-02-01 23:40:09,877 Epoch 3703: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:40:09,878 EPOCH 3704
2024-02-01 23:40:23,693 Epoch 3704: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:40:23,694 EPOCH 3705
2024-02-01 23:40:37,593 Epoch 3705: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:40:37,593 EPOCH 3706
2024-02-01 23:40:51,524 Epoch 3706: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:40:51,524 EPOCH 3707
2024-02-01 23:41:05,396 Epoch 3707: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:41:05,396 EPOCH 3708
2024-02-01 23:41:19,341 Epoch 3708: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:41:19,341 EPOCH 3709
2024-02-01 23:41:33,433 Epoch 3709: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:41:33,433 EPOCH 3710
2024-02-01 23:41:47,314 Epoch 3710: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:41:47,315 EPOCH 3711
2024-02-01 23:42:01,136 Epoch 3711: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:42:01,137 EPOCH 3712
2024-02-01 23:42:01,822 [Epoch: 3712 Step: 00033400] Batch Recognition Loss:   0.000117 => Gls Tokens per Sec:     1873 || Batch Translation Loss:   0.013985 => Txt Tokens per Sec:     5460 || Lr: 0.000050
2024-02-01 23:42:15,081 Epoch 3712: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:42:15,082 EPOCH 3713
2024-02-01 23:42:28,925 Epoch 3713: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:42:28,926 EPOCH 3714
2024-02-01 23:42:42,611 Epoch 3714: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:42:42,612 EPOCH 3715
2024-02-01 23:42:56,633 Epoch 3715: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 23:42:56,633 EPOCH 3716
2024-02-01 23:43:10,676 Epoch 3716: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:43:10,677 EPOCH 3717
2024-02-01 23:43:24,703 Epoch 3717: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:43:24,704 EPOCH 3718
2024-02-01 23:43:38,388 Epoch 3718: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:43:38,388 EPOCH 3719
2024-02-01 23:43:52,144 Epoch 3719: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:43:52,144 EPOCH 3720
2024-02-01 23:44:06,280 Epoch 3720: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:44:06,280 EPOCH 3721
2024-02-01 23:44:20,310 Epoch 3721: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:44:20,310 EPOCH 3722
2024-02-01 23:44:34,440 Epoch 3722: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:44:34,441 EPOCH 3723
2024-02-01 23:44:38,117 [Epoch: 3723 Step: 00033500] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:      454 || Batch Translation Loss:   0.007756 => Txt Tokens per Sec:     1339 || Lr: 0.000050
2024-02-01 23:44:48,494 Epoch 3723: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:44:48,495 EPOCH 3724
2024-02-01 23:45:02,617 Epoch 3724: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:45:02,617 EPOCH 3725
2024-02-01 23:45:16,379 Epoch 3725: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:45:16,380 EPOCH 3726
2024-02-01 23:45:30,242 Epoch 3726: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:45:30,242 EPOCH 3727
2024-02-01 23:45:44,399 Epoch 3727: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:45:44,400 EPOCH 3728
2024-02-01 23:45:58,454 Epoch 3728: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:45:58,455 EPOCH 3729
2024-02-01 23:46:12,730 Epoch 3729: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:46:12,731 EPOCH 3730
2024-02-01 23:46:26,578 Epoch 3730: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 23:46:26,579 EPOCH 3731
2024-02-01 23:46:40,395 Epoch 3731: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-01 23:46:40,396 EPOCH 3732
2024-02-01 23:46:54,270 Epoch 3732: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-01 23:46:54,271 EPOCH 3733
2024-02-01 23:47:08,259 Epoch 3733: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-01 23:47:08,259 EPOCH 3734
2024-02-01 23:47:09,621 [Epoch: 3734 Step: 00033600] Batch Recognition Loss:   0.000178 => Gls Tokens per Sec:     2822 || Batch Translation Loss:   0.044099 => Txt Tokens per Sec:     6570 || Lr: 0.000050
2024-02-01 23:47:22,223 Epoch 3734: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.84 
2024-02-01 23:47:22,223 EPOCH 3735
2024-02-01 23:47:36,343 Epoch 3735: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.81 
2024-02-01 23:47:36,343 EPOCH 3736
2024-02-01 23:47:50,386 Epoch 3736: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.64 
2024-02-01 23:47:50,387 EPOCH 3737
2024-02-01 23:48:04,327 Epoch 3737: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-01 23:48:04,327 EPOCH 3738
2024-02-01 23:48:18,147 Epoch 3738: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-01 23:48:18,147 EPOCH 3739
2024-02-01 23:48:32,447 Epoch 3739: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-01 23:48:32,447 EPOCH 3740
2024-02-01 23:48:46,209 Epoch 3740: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-01 23:48:46,209 EPOCH 3741
2024-02-01 23:48:59,829 Epoch 3741: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-01 23:48:59,829 EPOCH 3742
2024-02-01 23:49:13,585 Epoch 3742: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 23:49:13,586 EPOCH 3743
2024-02-01 23:49:27,627 Epoch 3743: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-01 23:49:27,628 EPOCH 3744
2024-02-01 23:49:41,404 Epoch 3744: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 23:49:41,404 EPOCH 3745
2024-02-01 23:49:48,800 [Epoch: 3745 Step: 00033700] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:      572 || Batch Translation Loss:   0.017259 => Txt Tokens per Sec:     1527 || Lr: 0.000050
2024-02-01 23:49:55,464 Epoch 3745: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 23:49:55,464 EPOCH 3746
2024-02-01 23:50:09,414 Epoch 3746: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 23:50:09,415 EPOCH 3747
2024-02-01 23:50:23,394 Epoch 3747: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 23:50:23,395 EPOCH 3748
2024-02-01 23:50:37,560 Epoch 3748: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 23:50:37,561 EPOCH 3749
2024-02-01 23:50:51,236 Epoch 3749: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 23:50:51,237 EPOCH 3750
2024-02-01 23:51:05,313 Epoch 3750: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 23:51:05,313 EPOCH 3751
2024-02-01 23:51:19,390 Epoch 3751: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 23:51:19,391 EPOCH 3752
2024-02-01 23:51:33,481 Epoch 3752: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-01 23:51:33,481 EPOCH 3753
2024-02-01 23:51:47,425 Epoch 3753: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 23:51:47,425 EPOCH 3754
2024-02-01 23:52:01,121 Epoch 3754: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-01 23:52:01,121 EPOCH 3755
2024-02-01 23:52:15,146 Epoch 3755: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 23:52:15,146 EPOCH 3756
2024-02-01 23:52:27,679 [Epoch: 3756 Step: 00033800] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:      440 || Batch Translation Loss:   0.009141 => Txt Tokens per Sec:     1387 || Lr: 0.000050
2024-02-01 23:52:29,307 Epoch 3756: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-01 23:52:29,308 EPOCH 3757
2024-02-01 23:52:43,155 Epoch 3757: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 23:52:43,156 EPOCH 3758
2024-02-01 23:52:57,052 Epoch 3758: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-01 23:52:57,052 EPOCH 3759
2024-02-01 23:53:11,076 Epoch 3759: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-01 23:53:11,077 EPOCH 3760
2024-02-01 23:53:24,829 Epoch 3760: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:53:24,829 EPOCH 3761
2024-02-01 23:53:38,914 Epoch 3761: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:53:38,915 EPOCH 3762
2024-02-01 23:53:52,695 Epoch 3762: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:53:52,696 EPOCH 3763
2024-02-01 23:54:06,572 Epoch 3763: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:54:06,573 EPOCH 3764
2024-02-01 23:54:20,728 Epoch 3764: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:54:20,728 EPOCH 3765
2024-02-01 23:54:34,573 Epoch 3765: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:54:34,574 EPOCH 3766
2024-02-01 23:54:48,296 Epoch 3766: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:54:48,296 EPOCH 3767
2024-02-01 23:54:57,119 [Epoch: 3767 Step: 00033900] Batch Recognition Loss:   0.000116 => Gls Tokens per Sec:      770 || Batch Translation Loss:   0.011738 => Txt Tokens per Sec:     2042 || Lr: 0.000050
2024-02-01 23:55:02,337 Epoch 3767: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:55:02,337 EPOCH 3768
2024-02-01 23:55:16,408 Epoch 3768: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:55:16,408 EPOCH 3769
2024-02-01 23:55:30,420 Epoch 3769: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:55:30,421 EPOCH 3770
2024-02-01 23:55:44,473 Epoch 3770: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:55:44,473 EPOCH 3771
2024-02-01 23:55:58,074 Epoch 3771: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:55:58,074 EPOCH 3772
2024-02-01 23:56:11,549 Epoch 3772: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:56:11,549 EPOCH 3773
2024-02-01 23:56:25,426 Epoch 3773: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:56:25,427 EPOCH 3774
2024-02-01 23:56:39,593 Epoch 3774: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:56:39,594 EPOCH 3775
2024-02-01 23:56:53,630 Epoch 3775: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:56:53,631 EPOCH 3776
2024-02-01 23:57:07,488 Epoch 3776: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:57:07,488 EPOCH 3777
2024-02-01 23:57:21,405 Epoch 3777: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-01 23:57:21,406 EPOCH 3778
2024-02-01 23:57:31,941 [Epoch: 3778 Step: 00034000] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:      766 || Batch Translation Loss:   0.014797 => Txt Tokens per Sec:     2133 || Lr: 0.000050
2024-02-01 23:57:51,738 Validation result at epoch 3778, step    34000: duration: 19.7959s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00023	Translation Loss: 100281.07031	PPL: 22814.03906
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.66	(BLEU-1: 10.52,	BLEU-2: 3.10,	BLEU-3: 1.30,	BLEU-4: 0.66)
	CHRF 16.96	ROUGE 9.01
2024-02-01 23:57:51,739 Logging Recognition and Translation Outputs
2024-02-01 23:57:51,739 ========================================================================================================================
2024-02-01 23:57:51,739 Logging Sequence: 92_199.00
2024-02-01 23:57:51,739 	Gloss Reference :	A B+C+D+E
2024-02-01 23:57:51,740 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 23:57:51,740 	Gloss Alignment :	         
2024-02-01 23:57:51,740 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 23:57:51,740 	Text Reference  :	*** people on     social media  said that  
2024-02-01 23:57:51,740 	Text Hypothesis :	and the    police still  played very sushil
2024-02-01 23:57:51,741 	Text Alignment  :	I   S      S      S      S      S    S     
2024-02-01 23:57:51,741 ========================================================================================================================
2024-02-01 23:57:51,741 Logging Sequence: 109_64.00
2024-02-01 23:57:51,741 	Gloss Reference :	A B+C+D+E
2024-02-01 23:57:51,741 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 23:57:51,741 	Gloss Alignment :	         
2024-02-01 23:57:51,742 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 23:57:51,742 	Text Reference  :	the 2 players as      well       as the entire kkr      team have   been quarantined
2024-02-01 23:57:51,743 	Text Hypothesis :	*** * to      discuss management of the ****** upcoming team physio the  fir        
2024-02-01 23:57:51,743 	Text Alignment  :	D   D S       S       S          S      D      S             S      S    S          
2024-02-01 23:57:51,743 ========================================================================================================================
2024-02-01 23:57:51,743 Logging Sequence: 84_108.00
2024-02-01 23:57:51,743 	Gloss Reference :	A B+C+D+E
2024-02-01 23:57:51,743 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 23:57:51,743 	Gloss Alignment :	         
2024-02-01 23:57:51,744 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 23:57:51,745 	Text Reference  :	so in order to show their protest they covered  their     mouth   in the    photos which then   went     viral
2024-02-01 23:57:51,745 	Text Hypothesis :	** ** ***** ** **** ***** ******* the  incident triggered outrage on social media  with  people pointing out  
2024-02-01 23:57:51,745 	Text Alignment  :	D  D  D     D  D    D     D       S    S        S         S       S  S      S      S     S      S        S    
2024-02-01 23:57:51,745 ========================================================================================================================
2024-02-01 23:57:51,745 Logging Sequence: 115_24.00
2024-02-01 23:57:51,746 	Gloss Reference :	A B+C+D+E
2024-02-01 23:57:51,746 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 23:57:51,746 	Gloss Alignment :	         
2024-02-01 23:57:51,746 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 23:57:51,747 	Text Reference  :	bumrah also did not participate in  the  5        match t20        series
2024-02-01 23:57:51,747 	Text Hypothesis :	****** **** *** *** loss        are many negative and   merseyside police
2024-02-01 23:57:51,747 	Text Alignment  :	D      D    D   D   S           S   S    S        S     S          S     
2024-02-01 23:57:51,747 ========================================================================================================================
2024-02-01 23:57:51,747 Logging Sequence: 96_129.00
2024-02-01 23:57:51,747 	Gloss Reference :	A B+C+D+E
2024-02-01 23:57:51,747 	Gloss Hypothesis:	A B+C+D+E
2024-02-01 23:57:51,748 	Gloss Alignment :	         
2024-02-01 23:57:51,748 	--------------------------------------------------------------------------------------------------------------------
2024-02-01 23:57:51,748 	Text Reference  :	***** *** ********* viewers were very stressed
2024-02-01 23:57:51,748 	Text Hypothesis :	while the remaining can     play a    wicket  
2024-02-01 23:57:51,748 	Text Alignment  :	I     I   I         S       S    S    S       
2024-02-01 23:57:51,748 ========================================================================================================================
2024-02-01 23:57:55,394 Epoch 3778: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:57:55,394 EPOCH 3779
2024-02-01 23:58:09,167 Epoch 3779: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:58:09,168 EPOCH 3780
2024-02-01 23:58:23,273 Epoch 3780: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:58:23,274 EPOCH 3781
2024-02-01 23:58:37,358 Epoch 3781: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:58:37,358 EPOCH 3782
2024-02-01 23:58:51,332 Epoch 3782: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:58:51,332 EPOCH 3783
2024-02-01 23:59:05,049 Epoch 3783: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:59:05,050 EPOCH 3784
2024-02-01 23:59:18,920 Epoch 3784: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-01 23:59:18,920 EPOCH 3785
2024-02-01 23:59:33,065 Epoch 3785: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:59:33,066 EPOCH 3786
2024-02-01 23:59:46,759 Epoch 3786: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-01 23:59:46,760 EPOCH 3787
2024-02-02 00:00:00,644 Epoch 3787: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 00:00:00,645 EPOCH 3788
2024-02-02 00:00:14,420 Epoch 3788: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 00:00:14,421 EPOCH 3789
2024-02-02 00:00:25,225 [Epoch: 3789 Step: 00034100] Batch Recognition Loss:   0.000110 => Gls Tokens per Sec:      866 || Batch Translation Loss:   0.013690 => Txt Tokens per Sec:     2352 || Lr: 0.000050
2024-02-02 00:00:28,284 Epoch 3789: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:00:28,284 EPOCH 3790
2024-02-02 00:00:42,091 Epoch 3790: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 00:00:42,092 EPOCH 3791
2024-02-02 00:00:56,151 Epoch 3791: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:00:56,151 EPOCH 3792
2024-02-02 00:01:10,265 Epoch 3792: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 00:01:10,266 EPOCH 3793
2024-02-02 00:01:24,032 Epoch 3793: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 00:01:24,033 EPOCH 3794
2024-02-02 00:01:37,922 Epoch 3794: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:01:37,923 EPOCH 3795
2024-02-02 00:01:51,728 Epoch 3795: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 00:01:51,729 EPOCH 3796
2024-02-02 00:02:05,514 Epoch 3796: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 00:02:05,515 EPOCH 3797
2024-02-02 00:02:19,247 Epoch 3797: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:02:19,248 EPOCH 3798
2024-02-02 00:02:33,162 Epoch 3798: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 00:02:33,163 EPOCH 3799
2024-02-02 00:02:46,659 Epoch 3799: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:02:46,659 EPOCH 3800
2024-02-02 00:03:00,700 [Epoch: 3800 Step: 00034200] Batch Recognition Loss:   0.000101 => Gls Tokens per Sec:      757 || Batch Translation Loss:   0.014129 => Txt Tokens per Sec:     2102 || Lr: 0.000050
2024-02-02 00:03:00,701 Epoch 3800: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 00:03:00,701 EPOCH 3801
2024-02-02 00:03:14,403 Epoch 3801: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:03:14,404 EPOCH 3802
2024-02-02 00:03:28,551 Epoch 3802: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:03:28,552 EPOCH 3803
2024-02-02 00:03:42,525 Epoch 3803: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:03:42,526 EPOCH 3804
2024-02-02 00:03:56,335 Epoch 3804: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:03:56,336 EPOCH 3805
2024-02-02 00:04:10,260 Epoch 3805: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:04:10,261 EPOCH 3806
2024-02-02 00:04:24,539 Epoch 3806: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:04:24,540 EPOCH 3807
2024-02-02 00:04:38,463 Epoch 3807: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:04:38,464 EPOCH 3808
2024-02-02 00:04:52,285 Epoch 3808: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:04:52,286 EPOCH 3809
2024-02-02 00:05:06,596 Epoch 3809: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 00:05:06,596 EPOCH 3810
2024-02-02 00:05:20,593 Epoch 3810: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 00:05:20,593 EPOCH 3811
2024-02-02 00:05:34,341 Epoch 3811: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:05:34,342 EPOCH 3812
2024-02-02 00:05:38,069 [Epoch: 3812 Step: 00034300] Batch Recognition Loss:   0.000131 => Gls Tokens per Sec:      344 || Batch Translation Loss:   0.030670 => Txt Tokens per Sec:     1203 || Lr: 0.000050
2024-02-02 00:05:48,520 Epoch 3812: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:05:48,520 EPOCH 3813
2024-02-02 00:06:02,165 Epoch 3813: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 00:06:02,166 EPOCH 3814
2024-02-02 00:06:15,955 Epoch 3814: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 00:06:15,955 EPOCH 3815
2024-02-02 00:06:30,013 Epoch 3815: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 00:06:30,013 EPOCH 3816
2024-02-02 00:06:43,693 Epoch 3816: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 00:06:43,694 EPOCH 3817
2024-02-02 00:06:57,643 Epoch 3817: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 00:06:57,643 EPOCH 3818
2024-02-02 00:07:11,713 Epoch 3818: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 00:07:11,714 EPOCH 3819
2024-02-02 00:07:25,666 Epoch 3819: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 00:07:25,667 EPOCH 3820
2024-02-02 00:07:39,442 Epoch 3820: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 00:07:39,443 EPOCH 3821
2024-02-02 00:07:53,342 Epoch 3821: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 00:07:53,343 EPOCH 3822
2024-02-02 00:08:07,085 Epoch 3822: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 00:08:07,085 EPOCH 3823
2024-02-02 00:08:09,127 [Epoch: 3823 Step: 00034400] Batch Recognition Loss:   0.000231 => Gls Tokens per Sec:     1254 || Batch Translation Loss:   0.067575 => Txt Tokens per Sec:     3315 || Lr: 0.000050
2024-02-02 00:08:20,882 Epoch 3823: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 00:08:20,882 EPOCH 3824
2024-02-02 00:08:34,600 Epoch 3824: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-02 00:08:34,600 EPOCH 3825
2024-02-02 00:08:48,531 Epoch 3825: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 00:08:48,531 EPOCH 3826
2024-02-02 00:09:02,427 Epoch 3826: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 00:09:02,427 EPOCH 3827
2024-02-02 00:09:16,290 Epoch 3827: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 00:09:16,291 EPOCH 3828
2024-02-02 00:09:30,213 Epoch 3828: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 00:09:30,214 EPOCH 3829
2024-02-02 00:09:43,902 Epoch 3829: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 00:09:43,902 EPOCH 3830
2024-02-02 00:09:57,999 Epoch 3830: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 00:09:58,000 EPOCH 3831
2024-02-02 00:10:11,975 Epoch 3831: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 00:10:11,975 EPOCH 3832
2024-02-02 00:10:26,056 Epoch 3832: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 00:10:26,057 EPOCH 3833
2024-02-02 00:10:39,958 Epoch 3833: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 00:10:39,959 EPOCH 3834
2024-02-02 00:10:45,400 [Epoch: 3834 Step: 00034500] Batch Recognition Loss:   0.000109 => Gls Tokens per Sec:      542 || Batch Translation Loss:   0.014460 => Txt Tokens per Sec:     1504 || Lr: 0.000050
2024-02-02 00:10:53,827 Epoch 3834: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 00:10:53,827 EPOCH 3835
2024-02-02 00:11:07,512 Epoch 3835: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 00:11:07,512 EPOCH 3836
2024-02-02 00:11:21,340 Epoch 3836: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 00:11:21,340 EPOCH 3837
2024-02-02 00:11:35,324 Epoch 3837: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 00:11:35,325 EPOCH 3838
2024-02-02 00:11:49,307 Epoch 3838: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 00:11:49,308 EPOCH 3839
2024-02-02 00:12:03,106 Epoch 3839: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 00:12:03,106 EPOCH 3840
2024-02-02 00:12:16,935 Epoch 3840: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:12:16,935 EPOCH 3841
2024-02-02 00:12:30,839 Epoch 3841: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:12:30,839 EPOCH 3842
2024-02-02 00:12:44,966 Epoch 3842: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 00:12:44,967 EPOCH 3843
2024-02-02 00:12:58,972 Epoch 3843: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:12:58,972 EPOCH 3844
2024-02-02 00:13:12,914 Epoch 3844: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:13:12,914 EPOCH 3845
2024-02-02 00:13:17,270 [Epoch: 3845 Step: 00034600] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     1176 || Batch Translation Loss:   0.021081 => Txt Tokens per Sec:     3004 || Lr: 0.000050
2024-02-02 00:13:26,857 Epoch 3845: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:13:26,857 EPOCH 3846
2024-02-02 00:13:40,955 Epoch 3846: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 00:13:40,955 EPOCH 3847
2024-02-02 00:13:54,986 Epoch 3847: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 00:13:54,987 EPOCH 3848
2024-02-02 00:14:08,853 Epoch 3848: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:14:08,854 EPOCH 3849
2024-02-02 00:14:22,753 Epoch 3849: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:14:22,754 EPOCH 3850
2024-02-02 00:14:36,471 Epoch 3850: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 00:14:36,471 EPOCH 3851
2024-02-02 00:14:50,565 Epoch 3851: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:14:50,565 EPOCH 3852
2024-02-02 00:15:04,518 Epoch 3852: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:15:04,519 EPOCH 3853
2024-02-02 00:15:18,572 Epoch 3853: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:15:18,572 EPOCH 3854
2024-02-02 00:15:32,280 Epoch 3854: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:15:32,280 EPOCH 3855
2024-02-02 00:15:46,668 Epoch 3855: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:15:46,668 EPOCH 3856
2024-02-02 00:15:55,543 [Epoch: 3856 Step: 00034700] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:      621 || Batch Translation Loss:   0.007279 => Txt Tokens per Sec:     1786 || Lr: 0.000050
2024-02-02 00:16:00,781 Epoch 3856: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:16:00,782 EPOCH 3857
2024-02-02 00:16:14,514 Epoch 3857: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:16:14,515 EPOCH 3858
2024-02-02 00:16:28,418 Epoch 3858: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:16:28,419 EPOCH 3859
2024-02-02 00:16:42,140 Epoch 3859: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:16:42,140 EPOCH 3860
2024-02-02 00:16:55,957 Epoch 3860: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:16:55,958 EPOCH 3861
2024-02-02 00:17:10,094 Epoch 3861: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:17:10,095 EPOCH 3862
2024-02-02 00:17:23,721 Epoch 3862: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:17:23,722 EPOCH 3863
2024-02-02 00:17:37,722 Epoch 3863: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:17:37,723 EPOCH 3864
2024-02-02 00:17:51,987 Epoch 3864: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:17:51,987 EPOCH 3865
2024-02-02 00:18:05,606 Epoch 3865: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 00:18:05,606 EPOCH 3866
2024-02-02 00:18:19,834 Epoch 3866: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:18:19,834 EPOCH 3867
2024-02-02 00:18:31,131 [Epoch: 3867 Step: 00034800] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:      601 || Batch Translation Loss:   0.007728 => Txt Tokens per Sec:     1763 || Lr: 0.000050
2024-02-02 00:18:33,603 Epoch 3867: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:18:33,603 EPOCH 3868
2024-02-02 00:18:47,189 Epoch 3868: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:18:47,190 EPOCH 3869
2024-02-02 00:19:00,200 Epoch 3869: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:19:00,200 EPOCH 3870
2024-02-02 00:19:14,296 Epoch 3870: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:19:14,297 EPOCH 3871
2024-02-02 00:19:28,013 Epoch 3871: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 00:19:28,014 EPOCH 3872
2024-02-02 00:19:42,334 Epoch 3872: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 00:19:42,334 EPOCH 3873
2024-02-02 00:19:56,300 Epoch 3873: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 00:19:56,301 EPOCH 3874
2024-02-02 00:20:10,124 Epoch 3874: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 00:20:10,125 EPOCH 3875
2024-02-02 00:20:24,120 Epoch 3875: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 00:20:24,120 EPOCH 3876
2024-02-02 00:20:37,842 Epoch 3876: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 00:20:37,842 EPOCH 3877
2024-02-02 00:20:51,981 Epoch 3877: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 00:20:51,982 EPOCH 3878
2024-02-02 00:21:01,880 [Epoch: 3878 Step: 00034900] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:      815 || Batch Translation Loss:   0.020741 => Txt Tokens per Sec:     2299 || Lr: 0.000050
2024-02-02 00:21:05,803 Epoch 3878: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:21:05,804 EPOCH 3879
2024-02-02 00:21:19,759 Epoch 3879: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:21:19,760 EPOCH 3880
2024-02-02 00:21:33,672 Epoch 3880: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 00:21:33,672 EPOCH 3881
2024-02-02 00:21:47,379 Epoch 3881: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 00:21:47,380 EPOCH 3882
2024-02-02 00:22:01,395 Epoch 3882: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 00:22:01,396 EPOCH 3883
2024-02-02 00:22:15,202 Epoch 3883: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 00:22:15,202 EPOCH 3884
2024-02-02 00:22:29,265 Epoch 3884: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 00:22:29,265 EPOCH 3885
2024-02-02 00:22:43,197 Epoch 3885: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:22:43,197 EPOCH 3886
2024-02-02 00:22:57,032 Epoch 3886: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 00:22:57,032 EPOCH 3887
2024-02-02 00:23:10,966 Epoch 3887: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 00:23:10,966 EPOCH 3888
2024-02-02 00:23:24,609 Epoch 3888: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-02 00:23:24,610 EPOCH 3889
2024-02-02 00:23:35,277 [Epoch: 3889 Step: 00035000] Batch Recognition Loss:   0.001165 => Gls Tokens per Sec:      877 || Batch Translation Loss:   0.062106 => Txt Tokens per Sec:     2386 || Lr: 0.000050
2024-02-02 00:23:38,336 Epoch 3889: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-02 00:23:38,336 EPOCH 3890
2024-02-02 00:23:51,896 Epoch 3890: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-02 00:23:51,897 EPOCH 3891
2024-02-02 00:24:06,074 Epoch 3891: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 00:24:06,074 EPOCH 3892
2024-02-02 00:24:19,944 Epoch 3892: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 00:24:19,944 EPOCH 3893
2024-02-02 00:24:33,929 Epoch 3893: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 00:24:33,930 EPOCH 3894
2024-02-02 00:24:47,778 Epoch 3894: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 00:24:47,779 EPOCH 3895
2024-02-02 00:25:01,389 Epoch 3895: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 00:25:01,390 EPOCH 3896
2024-02-02 00:25:15,287 Epoch 3896: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 00:25:15,287 EPOCH 3897
2024-02-02 00:25:29,163 Epoch 3897: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 00:25:29,164 EPOCH 3898
2024-02-02 00:25:43,193 Epoch 3898: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 00:25:43,194 EPOCH 3899
2024-02-02 00:25:56,988 Epoch 3899: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 00:25:56,989 EPOCH 3900
2024-02-02 00:26:10,700 [Epoch: 3900 Step: 00035100] Batch Recognition Loss:   0.000110 => Gls Tokens per Sec:      775 || Batch Translation Loss:   0.009105 => Txt Tokens per Sec:     2152 || Lr: 0.000050
2024-02-02 00:26:10,701 Epoch 3900: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 00:26:10,701 EPOCH 3901
2024-02-02 00:26:24,610 Epoch 3901: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 00:26:24,611 EPOCH 3902
2024-02-02 00:26:38,496 Epoch 3902: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 00:26:38,496 EPOCH 3903
2024-02-02 00:26:52,232 Epoch 3903: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:26:52,232 EPOCH 3904
2024-02-02 00:27:06,258 Epoch 3904: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 00:27:06,259 EPOCH 3905
2024-02-02 00:27:20,248 Epoch 3905: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 00:27:20,249 EPOCH 3906
2024-02-02 00:27:34,220 Epoch 3906: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 00:27:34,220 EPOCH 3907
2024-02-02 00:27:48,244 Epoch 3907: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 00:27:48,245 EPOCH 3908
2024-02-02 00:28:02,221 Epoch 3908: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:28:02,222 EPOCH 3909
2024-02-02 00:28:16,271 Epoch 3909: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:28:16,272 EPOCH 3910
2024-02-02 00:28:30,095 Epoch 3910: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:28:30,096 EPOCH 3911
2024-02-02 00:28:44,112 Epoch 3911: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:28:44,113 EPOCH 3912
2024-02-02 00:28:44,643 [Epoch: 3912 Step: 00035200] Batch Recognition Loss:   0.000113 => Gls Tokens per Sec:     2420 || Batch Translation Loss:   0.015493 => Txt Tokens per Sec:     6686 || Lr: 0.000050
2024-02-02 00:28:57,995 Epoch 3912: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:28:57,996 EPOCH 3913
2024-02-02 00:29:11,845 Epoch 3913: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:29:11,846 EPOCH 3914
2024-02-02 00:29:25,736 Epoch 3914: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:29:25,737 EPOCH 3915
2024-02-02 00:29:39,728 Epoch 3915: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:29:39,729 EPOCH 3916
2024-02-02 00:29:53,460 Epoch 3916: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:29:53,461 EPOCH 3917
2024-02-02 00:30:07,590 Epoch 3917: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 00:30:07,591 EPOCH 3918
2024-02-02 00:30:21,637 Epoch 3918: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:30:21,637 EPOCH 3919
2024-02-02 00:30:35,569 Epoch 3919: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 00:30:35,570 EPOCH 3920
2024-02-02 00:30:49,320 Epoch 3920: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:30:49,321 EPOCH 3921
2024-02-02 00:31:03,083 Epoch 3921: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:31:03,083 EPOCH 3922
2024-02-02 00:31:17,432 Epoch 3922: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:31:17,432 EPOCH 3923
2024-02-02 00:31:18,307 [Epoch: 3923 Step: 00035300] Batch Recognition Loss:   0.000100 => Gls Tokens per Sec:     2929 || Batch Translation Loss:   0.011991 => Txt Tokens per Sec:     6904 || Lr: 0.000050
2024-02-02 00:31:31,389 Epoch 3923: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:31:31,390 EPOCH 3924
2024-02-02 00:31:45,224 Epoch 3924: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:31:45,224 EPOCH 3925
2024-02-02 00:31:59,170 Epoch 3925: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:31:59,171 EPOCH 3926
2024-02-02 00:32:13,230 Epoch 3926: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:32:13,230 EPOCH 3927
2024-02-02 00:32:26,962 Epoch 3927: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:32:26,963 EPOCH 3928
2024-02-02 00:32:40,916 Epoch 3928: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:32:40,917 EPOCH 3929
2024-02-02 00:32:54,907 Epoch 3929: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:32:54,907 EPOCH 3930
2024-02-02 00:33:08,613 Epoch 3930: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 00:33:08,614 EPOCH 3931
2024-02-02 00:33:22,519 Epoch 3931: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 00:33:22,520 EPOCH 3932
2024-02-02 00:33:36,342 Epoch 3932: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 00:33:36,342 EPOCH 3933
2024-02-02 00:33:50,332 Epoch 3933: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 00:33:50,333 EPOCH 3934
2024-02-02 00:33:51,625 [Epoch: 3934 Step: 00035400] Batch Recognition Loss:   0.000107 => Gls Tokens per Sec:     2976 || Batch Translation Loss:   0.018939 => Txt Tokens per Sec:     7674 || Lr: 0.000050
2024-02-02 00:34:04,337 Epoch 3934: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 00:34:04,338 EPOCH 3935
2024-02-02 00:34:18,167 Epoch 3935: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-02 00:34:18,168 EPOCH 3936
2024-02-02 00:34:32,243 Epoch 3936: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 00:34:32,244 EPOCH 3937
2024-02-02 00:34:45,958 Epoch 3937: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 00:34:45,959 EPOCH 3938
2024-02-02 00:35:00,041 Epoch 3938: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 00:35:00,042 EPOCH 3939
2024-02-02 00:35:13,916 Epoch 3939: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 00:35:13,916 EPOCH 3940
2024-02-02 00:35:27,833 Epoch 3940: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 00:35:27,834 EPOCH 3941
2024-02-02 00:35:41,412 Epoch 3941: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 00:35:41,412 EPOCH 3942
2024-02-02 00:35:55,640 Epoch 3942: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 00:35:55,641 EPOCH 3943
2024-02-02 00:36:09,622 Epoch 3943: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 00:36:09,623 EPOCH 3944
2024-02-02 00:36:23,462 Epoch 3944: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 00:36:23,462 EPOCH 3945
2024-02-02 00:36:32,482 [Epoch: 3945 Step: 00035500] Batch Recognition Loss:   0.000231 => Gls Tokens per Sec:      469 || Batch Translation Loss:   0.031250 => Txt Tokens per Sec:     1495 || Lr: 0.000050
2024-02-02 00:36:37,196 Epoch 3945: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 00:36:37,196 EPOCH 3946
2024-02-02 00:36:50,831 Epoch 3946: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 00:36:50,832 EPOCH 3947
2024-02-02 00:37:04,797 Epoch 3947: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 00:37:04,798 EPOCH 3948
2024-02-02 00:37:18,513 Epoch 3948: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 00:37:18,514 EPOCH 3949
2024-02-02 00:37:32,428 Epoch 3949: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 00:37:32,428 EPOCH 3950
2024-02-02 00:37:46,194 Epoch 3950: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 00:37:46,195 EPOCH 3951
2024-02-02 00:38:00,012 Epoch 3951: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 00:38:00,012 EPOCH 3952
2024-02-02 00:38:13,955 Epoch 3952: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 00:38:13,956 EPOCH 3953
2024-02-02 00:38:27,902 Epoch 3953: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:38:27,903 EPOCH 3954
2024-02-02 00:38:41,741 Epoch 3954: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 00:38:41,742 EPOCH 3955
2024-02-02 00:38:55,476 Epoch 3955: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 00:38:55,476 EPOCH 3956
2024-02-02 00:39:03,709 [Epoch: 3956 Step: 00035600] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:      777 || Batch Translation Loss:   0.016362 => Txt Tokens per Sec:     2279 || Lr: 0.000050
2024-02-02 00:39:09,367 Epoch 3956: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:39:09,368 EPOCH 3957
2024-02-02 00:39:23,311 Epoch 3957: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:39:23,311 EPOCH 3958
2024-02-02 00:39:37,084 Epoch 3958: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:39:37,084 EPOCH 3959
2024-02-02 00:39:50,967 Epoch 3959: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:39:50,968 EPOCH 3960
2024-02-02 00:40:04,773 Epoch 3960: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:40:04,774 EPOCH 3961
2024-02-02 00:40:18,595 Epoch 3961: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:40:18,596 EPOCH 3962
2024-02-02 00:40:32,577 Epoch 3962: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:40:32,577 EPOCH 3963
2024-02-02 00:40:46,440 Epoch 3963: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:40:46,441 EPOCH 3964
2024-02-02 00:41:00,367 Epoch 3964: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:41:00,368 EPOCH 3965
2024-02-02 00:41:14,095 Epoch 3965: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:41:14,095 EPOCH 3966
2024-02-02 00:41:28,068 Epoch 3966: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:41:28,069 EPOCH 3967
2024-02-02 00:41:34,600 [Epoch: 3967 Step: 00035700] Batch Recognition Loss:   0.000104 => Gls Tokens per Sec:     1176 || Batch Translation Loss:   0.011133 => Txt Tokens per Sec:     3270 || Lr: 0.000050
2024-02-02 00:41:41,737 Epoch 3967: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:41:41,738 EPOCH 3968
2024-02-02 00:41:55,760 Epoch 3968: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:41:55,761 EPOCH 3969
2024-02-02 00:42:09,632 Epoch 3969: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:42:09,633 EPOCH 3970
2024-02-02 00:42:23,543 Epoch 3970: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:42:23,544 EPOCH 3971
2024-02-02 00:42:37,784 Epoch 3971: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 00:42:37,784 EPOCH 3972
2024-02-02 00:42:51,705 Epoch 3972: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:42:51,706 EPOCH 3973
2024-02-02 00:43:05,753 Epoch 3973: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:43:05,754 EPOCH 3974
2024-02-02 00:43:19,465 Epoch 3974: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:43:19,466 EPOCH 3975
2024-02-02 00:43:33,386 Epoch 3975: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 00:43:33,387 EPOCH 3976
2024-02-02 00:43:47,444 Epoch 3976: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 00:43:47,445 EPOCH 3977
2024-02-02 00:44:01,217 Epoch 3977: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 00:44:01,217 EPOCH 3978
2024-02-02 00:44:10,872 [Epoch: 3978 Step: 00035800] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:      836 || Batch Translation Loss:   0.027745 => Txt Tokens per Sec:     2286 || Lr: 0.000050
2024-02-02 00:44:14,839 Epoch 3978: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 00:44:14,839 EPOCH 3979
2024-02-02 00:44:28,630 Epoch 3979: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 00:44:28,630 EPOCH 3980
2024-02-02 00:44:42,703 Epoch 3980: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:44:42,703 EPOCH 3981
2024-02-02 00:44:56,861 Epoch 3981: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:44:56,862 EPOCH 3982
2024-02-02 00:45:10,641 Epoch 3982: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:45:10,641 EPOCH 3983
2024-02-02 00:45:24,600 Epoch 3983: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:45:24,601 EPOCH 3984
2024-02-02 00:45:38,400 Epoch 3984: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 00:45:38,400 EPOCH 3985
2024-02-02 00:45:52,331 Epoch 3985: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:45:52,332 EPOCH 3986
2024-02-02 00:46:06,292 Epoch 3986: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:46:06,292 EPOCH 3987
2024-02-02 00:46:20,233 Epoch 3987: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:46:20,233 EPOCH 3988
2024-02-02 00:46:34,059 Epoch 3988: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:46:34,060 EPOCH 3989
2024-02-02 00:46:47,616 [Epoch: 3989 Step: 00035900] Batch Recognition Loss:   0.000112 => Gls Tokens per Sec:      690 || Batch Translation Loss:   0.014232 => Txt Tokens per Sec:     1918 || Lr: 0.000050
2024-02-02 00:46:48,052 Epoch 3989: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:46:48,053 EPOCH 3990
2024-02-02 00:47:01,923 Epoch 3990: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:47:01,924 EPOCH 3991
2024-02-02 00:47:15,824 Epoch 3991: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:47:15,825 EPOCH 3992
2024-02-02 00:47:29,596 Epoch 3992: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:47:29,596 EPOCH 3993
2024-02-02 00:47:43,674 Epoch 3993: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:47:43,675 EPOCH 3994
2024-02-02 00:47:57,454 Epoch 3994: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:47:57,455 EPOCH 3995
2024-02-02 00:48:11,682 Epoch 3995: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:48:11,682 EPOCH 3996
2024-02-02 00:48:25,531 Epoch 3996: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:48:25,532 EPOCH 3997
2024-02-02 00:48:39,506 Epoch 3997: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:48:39,507 EPOCH 3998
2024-02-02 00:48:53,460 Epoch 3998: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:48:53,460 EPOCH 3999
2024-02-02 00:49:07,393 Epoch 3999: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:49:07,393 EPOCH 4000
2024-02-02 00:49:21,481 [Epoch: 4000 Step: 00036000] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:      755 || Batch Translation Loss:   0.014710 => Txt Tokens per Sec:     2095 || Lr: 0.000050
2024-02-02 00:49:41,273 Validation result at epoch 4000, step    36000: duration: 19.7911s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00025	Translation Loss: 100479.09375	PPL: 23270.64062
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.70	(BLEU-1: 10.61,	BLEU-2: 3.32,	BLEU-3: 1.34,	BLEU-4: 0.70)
	CHRF 17.06	ROUGE 9.16
2024-02-02 00:49:41,275 Logging Recognition and Translation Outputs
2024-02-02 00:49:41,275 ========================================================================================================================
2024-02-02 00:49:41,275 Logging Sequence: 78_198.00
2024-02-02 00:49:41,275 	Gloss Reference :	A B+C+D+E
2024-02-02 00:49:41,276 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 00:49:41,276 	Gloss Alignment :	         
2024-02-02 00:49:41,276 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 00:49:41,277 	Text Reference  :	*** ****** *** * **** ** **** they   have been flooded with congratulations comments
2024-02-02 00:49:41,277 	Text Hypothesis :	and behind him a girl is seen laying on   the  bed     by   her             lingerie
2024-02-02 00:49:41,277 	Text Alignment  :	I   I      I   I I    I  I    S      S    S    S       S    S               S       
2024-02-02 00:49:41,278 ========================================================================================================================
2024-02-02 00:49:41,278 Logging Sequence: 145_216.00
2024-02-02 00:49:41,278 	Gloss Reference :	A B+C+D+E
2024-02-02 00:49:41,278 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 00:49:41,278 	Gloss Alignment :	         
2024-02-02 00:49:41,278 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 00:49:41,279 	Text Reference  :	asking him to include sameeha in the world championship as       she  was a   talented athlete
2024-02-02 00:49:41,279 	Text Hypothesis :	****** *** ** ******* ******* ** *** ***** he           believed that his bag brought  luck   
2024-02-02 00:49:41,280 	Text Alignment  :	D      D   D  D       D       D  D   D     S            S        S    S   S   S        S      
2024-02-02 00:49:41,280 ========================================================================================================================
2024-02-02 00:49:41,280 Logging Sequence: 70_137.00
2024-02-02 00:49:41,280 	Gloss Reference :	A B+C+D+E
2024-02-02 00:49:41,280 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 00:49:41,280 	Gloss Alignment :	         
2024-02-02 00:49:41,280 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 00:49:41,281 	Text Reference  :	the small gesture appeared to encourage people to drink water   instead of         aerated drinks  
2024-02-02 00:49:41,281 	Text Hypothesis :	*** ***** ******* ******** ** ********* ****** ** ***** however no      disrespect was     intended
2024-02-02 00:49:41,281 	Text Alignment  :	D   D     D       D        D  D         D      D  D     S       S       S          S       S       
2024-02-02 00:49:41,281 ========================================================================================================================
2024-02-02 00:49:41,282 Logging Sequence: 119_20.00
2024-02-02 00:49:41,282 	Gloss Reference :	A B+C+D+E
2024-02-02 00:49:41,282 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 00:49:41,282 	Gloss Alignment :	         
2024-02-02 00:49:41,282 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 00:49:41,284 	Text Reference  :	messi intended to gift something to  all    the   players and ******* the   staff    to special to    celebrate the moment 
2024-02-02 00:49:41,284 	Text Hypothesis :	***** ******** ** **** ********* the reason being played  and idesign gold' official to ******* messi receiving the iphones
2024-02-02 00:49:41,284 	Text Alignment  :	D     D        D  D    D         S   S      S     S           I       S     S           D       S     S             S      
2024-02-02 00:49:41,284 ========================================================================================================================
2024-02-02 00:49:41,284 Logging Sequence: 106_15.00
2024-02-02 00:49:41,284 	Gloss Reference :	A B+C+D+E
2024-02-02 00:49:41,284 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 00:49:41,285 	Gloss Alignment :	         
2024-02-02 00:49:41,285 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 00:49:41,286 	Text Reference  :	*** **** ******** ** but  what about women's cricket earlier we   never spoke about it       
2024-02-02 00:49:41,286 	Text Hypothesis :	and were selected to play he   was   given   a       fine    such a     huge  fan   following
2024-02-02 00:49:41,286 	Text Alignment  :	I   I    I        I  S    S    S     S       S       S       S    S     S     S     S        
2024-02-02 00:49:41,286 ========================================================================================================================
2024-02-02 00:49:41,292 Epoch 4000: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:49:41,292 EPOCH 4001
2024-02-02 00:49:55,366 Epoch 4001: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:49:55,367 EPOCH 4002
2024-02-02 00:50:09,431 Epoch 4002: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:50:09,432 EPOCH 4003
2024-02-02 00:50:23,367 Epoch 4003: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:50:23,368 EPOCH 4004
2024-02-02 00:50:37,300 Epoch 4004: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:50:37,300 EPOCH 4005
2024-02-02 00:50:51,247 Epoch 4005: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:50:51,247 EPOCH 4006
2024-02-02 00:51:05,377 Epoch 4006: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:51:05,377 EPOCH 4007
2024-02-02 00:51:19,446 Epoch 4007: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:51:19,447 EPOCH 4008
2024-02-02 00:51:33,242 Epoch 4008: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:51:33,242 EPOCH 4009
2024-02-02 00:51:47,217 Epoch 4009: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:51:47,218 EPOCH 4010
2024-02-02 00:52:00,939 Epoch 4010: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:52:00,940 EPOCH 4011
2024-02-02 00:52:15,030 Epoch 4011: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:52:15,031 EPOCH 4012
2024-02-02 00:52:18,727 [Epoch: 4012 Step: 00036100] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:      346 || Batch Translation Loss:   0.021970 => Txt Tokens per Sec:     1214 || Lr: 0.000050
2024-02-02 00:52:29,025 Epoch 4012: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:52:29,025 EPOCH 4013
2024-02-02 00:52:43,174 Epoch 4013: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 00:52:43,174 EPOCH 4014
2024-02-02 00:52:57,055 Epoch 4014: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:52:57,056 EPOCH 4015
2024-02-02 00:53:11,085 Epoch 4015: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 00:53:11,085 EPOCH 4016
2024-02-02 00:53:24,944 Epoch 4016: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:53:24,945 EPOCH 4017
2024-02-02 00:53:38,920 Epoch 4017: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:53:38,921 EPOCH 4018
2024-02-02 00:53:52,935 Epoch 4018: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:53:52,936 EPOCH 4019
2024-02-02 00:54:06,588 Epoch 4019: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:54:06,589 EPOCH 4020
2024-02-02 00:54:20,634 Epoch 4020: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:54:20,634 EPOCH 4021
2024-02-02 00:54:34,543 Epoch 4021: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:54:34,544 EPOCH 4022
2024-02-02 00:54:48,300 Epoch 4022: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:54:48,301 EPOCH 4023
2024-02-02 00:54:51,675 [Epoch: 4023 Step: 00036200] Batch Recognition Loss:   0.000099 => Gls Tokens per Sec:      759 || Batch Translation Loss:   0.010737 => Txt Tokens per Sec:     2074 || Lr: 0.000050
2024-02-02 00:55:02,272 Epoch 4023: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:55:02,273 EPOCH 4024
2024-02-02 00:55:16,135 Epoch 4024: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:55:16,135 EPOCH 4025
2024-02-02 00:55:29,991 Epoch 4025: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 00:55:29,992 EPOCH 4026
2024-02-02 00:55:43,973 Epoch 4026: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:55:43,973 EPOCH 4027
2024-02-02 00:55:57,783 Epoch 4027: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 00:55:57,783 EPOCH 4028
2024-02-02 00:56:11,618 Epoch 4028: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 00:56:11,619 EPOCH 4029
2024-02-02 00:56:25,319 Epoch 4029: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 00:56:25,320 EPOCH 4030
2024-02-02 00:56:39,714 Epoch 4030: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 00:56:39,714 EPOCH 4031
2024-02-02 00:56:53,555 Epoch 4031: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 00:56:53,556 EPOCH 4032
2024-02-02 00:57:07,344 Epoch 4032: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.25 
2024-02-02 00:57:07,345 EPOCH 4033
2024-02-02 00:57:21,320 Epoch 4033: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.84 
2024-02-02 00:57:21,320 EPOCH 4034
2024-02-02 00:57:22,868 [Epoch: 4034 Step: 00036300] Batch Recognition Loss:   0.000634 => Gls Tokens per Sec:     2483 || Batch Translation Loss:   0.118735 => Txt Tokens per Sec:     6772 || Lr: 0.000050
2024-02-02 00:57:35,142 Epoch 4034: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.61 
2024-02-02 00:57:35,143 EPOCH 4035
2024-02-02 00:57:48,890 Epoch 4035: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-02 00:57:48,891 EPOCH 4036
2024-02-02 00:58:02,538 Epoch 4036: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-02 00:58:02,538 EPOCH 4037
2024-02-02 00:58:16,376 Epoch 4037: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-02 00:58:16,376 EPOCH 4038
2024-02-02 00:58:30,229 Epoch 4038: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-02 00:58:30,229 EPOCH 4039
2024-02-02 00:58:44,029 Epoch 4039: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-02 00:58:44,030 EPOCH 4040
2024-02-02 00:58:58,262 Epoch 4040: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 00:58:58,262 EPOCH 4041
2024-02-02 00:59:12,001 Epoch 4041: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 00:59:12,002 EPOCH 4042
2024-02-02 00:59:25,888 Epoch 4042: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 00:59:25,889 EPOCH 4043
2024-02-02 00:59:39,657 Epoch 4043: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 00:59:39,658 EPOCH 4044
2024-02-02 00:59:53,339 Epoch 4044: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 00:59:53,340 EPOCH 4045
2024-02-02 00:59:59,091 [Epoch: 4045 Step: 00036400] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:      736 || Batch Translation Loss:   0.011778 => Txt Tokens per Sec:     1816 || Lr: 0.000050
2024-02-02 01:00:07,476 Epoch 4045: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 01:00:07,477 EPOCH 4046
2024-02-02 01:00:21,480 Epoch 4046: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 01:00:21,481 EPOCH 4047
2024-02-02 01:00:35,420 Epoch 4047: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 01:00:35,421 EPOCH 4048
2024-02-02 01:00:49,496 Epoch 4048: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 01:00:49,496 EPOCH 4049
2024-02-02 01:01:03,509 Epoch 4049: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 01:01:03,509 EPOCH 4050
2024-02-02 01:01:17,498 Epoch 4050: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 01:01:17,499 EPOCH 4051
2024-02-02 01:01:31,453 Epoch 4051: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:01:31,453 EPOCH 4052
2024-02-02 01:01:45,466 Epoch 4052: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 01:01:45,467 EPOCH 4053
2024-02-02 01:01:59,345 Epoch 4053: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 01:01:59,346 EPOCH 4054
2024-02-02 01:02:13,126 Epoch 4054: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 01:02:13,126 EPOCH 4055
2024-02-02 01:02:27,359 Epoch 4055: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:02:27,360 EPOCH 4056
2024-02-02 01:02:33,788 [Epoch: 4056 Step: 00036500] Batch Recognition Loss:   0.000122 => Gls Tokens per Sec:      996 || Batch Translation Loss:   0.012924 => Txt Tokens per Sec:     2691 || Lr: 0.000050
2024-02-02 01:02:41,225 Epoch 4056: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:02:41,225 EPOCH 4057
2024-02-02 01:02:55,110 Epoch 4057: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:02:55,110 EPOCH 4058
2024-02-02 01:03:08,941 Epoch 4058: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:03:08,941 EPOCH 4059
2024-02-02 01:03:23,208 Epoch 4059: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:03:23,208 EPOCH 4060
2024-02-02 01:03:36,908 Epoch 4060: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:03:36,909 EPOCH 4061
2024-02-02 01:03:50,840 Epoch 4061: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:03:50,841 EPOCH 4062
2024-02-02 01:04:04,909 Epoch 4062: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:04:04,910 EPOCH 4063
2024-02-02 01:04:19,152 Epoch 4063: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:04:19,152 EPOCH 4064
2024-02-02 01:04:32,986 Epoch 4064: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 01:04:32,987 EPOCH 4065
2024-02-02 01:04:46,907 Epoch 4065: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:04:46,907 EPOCH 4066
2024-02-02 01:05:00,959 Epoch 4066: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:05:00,959 EPOCH 4067
2024-02-02 01:05:09,501 [Epoch: 4067 Step: 00036600] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:      795 || Batch Translation Loss:   0.016289 => Txt Tokens per Sec:     2181 || Lr: 0.000050
2024-02-02 01:05:14,735 Epoch 4067: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:05:14,735 EPOCH 4068
2024-02-02 01:05:28,539 Epoch 4068: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:05:28,539 EPOCH 4069
2024-02-02 01:05:42,482 Epoch 4069: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:05:42,483 EPOCH 4070
2024-02-02 01:05:55,977 Epoch 4070: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:05:55,978 EPOCH 4071
2024-02-02 01:06:09,821 Epoch 4071: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:06:09,821 EPOCH 4072
2024-02-02 01:06:23,669 Epoch 4072: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:06:23,670 EPOCH 4073
2024-02-02 01:06:37,542 Epoch 4073: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:06:37,543 EPOCH 4074
2024-02-02 01:06:51,419 Epoch 4074: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:06:51,420 EPOCH 4075
2024-02-02 01:07:05,177 Epoch 4075: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:07:05,178 EPOCH 4076
2024-02-02 01:07:19,226 Epoch 4076: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:07:19,227 EPOCH 4077
2024-02-02 01:07:33,249 Epoch 4077: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:07:33,249 EPOCH 4078
2024-02-02 01:07:43,642 [Epoch: 4078 Step: 00036700] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:      777 || Batch Translation Loss:   0.019657 => Txt Tokens per Sec:     2134 || Lr: 0.000050
2024-02-02 01:07:47,436 Epoch 4078: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 01:07:47,437 EPOCH 4079
2024-02-02 01:08:01,503 Epoch 4079: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 01:08:01,503 EPOCH 4080
2024-02-02 01:08:15,368 Epoch 4080: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 01:08:15,369 EPOCH 4081
2024-02-02 01:08:29,522 Epoch 4081: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 01:08:29,523 EPOCH 4082
2024-02-02 01:08:43,140 Epoch 4082: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 01:08:43,141 EPOCH 4083
2024-02-02 01:08:57,188 Epoch 4083: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 01:08:57,189 EPOCH 4084
2024-02-02 01:09:10,969 Epoch 4084: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 01:09:10,970 EPOCH 4085
2024-02-02 01:09:24,971 Epoch 4085: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 01:09:24,971 EPOCH 4086
2024-02-02 01:09:38,658 Epoch 4086: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 01:09:38,658 EPOCH 4087
2024-02-02 01:09:52,552 Epoch 4087: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 01:09:52,552 EPOCH 4088
2024-02-02 01:10:06,347 Epoch 4088: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 01:10:06,348 EPOCH 4089
2024-02-02 01:10:18,522 [Epoch: 4089 Step: 00036800] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:      768 || Batch Translation Loss:   0.006444 => Txt Tokens per Sec:     2106 || Lr: 0.000050
2024-02-02 01:10:20,179 Epoch 4089: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 01:10:20,180 EPOCH 4090
2024-02-02 01:10:34,038 Epoch 4090: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:10:34,039 EPOCH 4091
2024-02-02 01:10:47,938 Epoch 4091: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:10:47,939 EPOCH 4092
2024-02-02 01:11:01,810 Epoch 4092: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:11:01,811 EPOCH 4093
2024-02-02 01:11:15,851 Epoch 4093: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:11:15,852 EPOCH 4094
2024-02-02 01:11:29,494 Epoch 4094: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:11:29,495 EPOCH 4095
2024-02-02 01:11:43,576 Epoch 4095: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:11:43,577 EPOCH 4096
2024-02-02 01:11:57,782 Epoch 4096: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:11:57,782 EPOCH 4097
2024-02-02 01:12:11,661 Epoch 4097: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:12:11,661 EPOCH 4098
2024-02-02 01:12:25,580 Epoch 4098: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:12:25,581 EPOCH 4099
2024-02-02 01:12:39,540 Epoch 4099: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 01:12:39,541 EPOCH 4100
2024-02-02 01:12:53,394 [Epoch: 4100 Step: 00036900] Batch Recognition Loss:   0.000117 => Gls Tokens per Sec:      767 || Batch Translation Loss:   0.015100 => Txt Tokens per Sec:     2130 || Lr: 0.000050
2024-02-02 01:12:53,395 Epoch 4100: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:12:53,395 EPOCH 4101
2024-02-02 01:13:07,368 Epoch 4101: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:13:07,369 EPOCH 4102
2024-02-02 01:13:21,095 Epoch 4102: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:13:21,096 EPOCH 4103
2024-02-02 01:13:35,219 Epoch 4103: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:13:35,220 EPOCH 4104
2024-02-02 01:13:49,272 Epoch 4104: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:13:49,273 EPOCH 4105
2024-02-02 01:14:03,121 Epoch 4105: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:14:03,121 EPOCH 4106
2024-02-02 01:14:16,880 Epoch 4106: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:14:16,880 EPOCH 4107
2024-02-02 01:14:30,973 Epoch 4107: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:14:30,974 EPOCH 4108
2024-02-02 01:14:44,797 Epoch 4108: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:14:44,798 EPOCH 4109
2024-02-02 01:14:58,572 Epoch 4109: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:14:58,573 EPOCH 4110
2024-02-02 01:15:12,618 Epoch 4110: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 01:15:12,619 EPOCH 4111
2024-02-02 01:15:26,501 Epoch 4111: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:15:26,502 EPOCH 4112
2024-02-02 01:15:26,958 [Epoch: 4112 Step: 00037000] Batch Recognition Loss:   0.000101 => Gls Tokens per Sec:     2807 || Batch Translation Loss:   0.015269 => Txt Tokens per Sec:     7645 || Lr: 0.000050
2024-02-02 01:15:40,523 Epoch 4112: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:15:40,523 EPOCH 4113
2024-02-02 01:15:54,471 Epoch 4113: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 01:15:54,471 EPOCH 4114
2024-02-02 01:16:08,500 Epoch 4114: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:16:08,501 EPOCH 4115
2024-02-02 01:16:22,709 Epoch 4115: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:16:22,709 EPOCH 4116
2024-02-02 01:16:36,700 Epoch 4116: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:16:36,700 EPOCH 4117
2024-02-02 01:16:50,903 Epoch 4117: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:16:50,903 EPOCH 4118
2024-02-02 01:17:04,639 Epoch 4118: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:17:04,639 EPOCH 4119
2024-02-02 01:17:18,715 Epoch 4119: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:17:18,716 EPOCH 4120
2024-02-02 01:17:32,532 Epoch 4120: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:17:32,533 EPOCH 4121
2024-02-02 01:17:46,241 Epoch 4121: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:17:46,242 EPOCH 4122
2024-02-02 01:18:00,146 Epoch 4122: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:18:00,147 EPOCH 4123
2024-02-02 01:18:00,893 [Epoch: 4123 Step: 00037100] Batch Recognition Loss:   0.000106 => Gls Tokens per Sec:     3441 || Batch Translation Loss:   0.013551 => Txt Tokens per Sec:     7726 || Lr: 0.000050
2024-02-02 01:18:14,102 Epoch 4123: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:18:14,102 EPOCH 4124
2024-02-02 01:18:27,882 Epoch 4124: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:18:27,883 EPOCH 4125
2024-02-02 01:18:41,835 Epoch 4125: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:18:41,836 EPOCH 4126
2024-02-02 01:18:55,874 Epoch 4126: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:18:55,874 EPOCH 4127
2024-02-02 01:19:09,571 Epoch 4127: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:19:09,572 EPOCH 4128
2024-02-02 01:19:23,413 Epoch 4128: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:19:23,414 EPOCH 4129
2024-02-02 01:19:37,353 Epoch 4129: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:19:37,354 EPOCH 4130
2024-02-02 01:19:51,298 Epoch 4130: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:19:51,299 EPOCH 4131
2024-02-02 01:20:05,319 Epoch 4131: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:20:05,320 EPOCH 4132
2024-02-02 01:20:19,199 Epoch 4132: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:20:19,199 EPOCH 4133
2024-02-02 01:20:33,116 Epoch 4133: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:20:33,117 EPOCH 4134
2024-02-02 01:20:36,962 [Epoch: 4134 Step: 00037200] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:      999 || Batch Translation Loss:   0.018877 => Txt Tokens per Sec:     2412 || Lr: 0.000050
2024-02-02 01:20:47,275 Epoch 4134: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:20:47,275 EPOCH 4135
2024-02-02 01:21:01,183 Epoch 4135: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:21:01,183 EPOCH 4136
2024-02-02 01:21:15,024 Epoch 4136: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:21:15,025 EPOCH 4137
2024-02-02 01:21:28,503 Epoch 4137: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 01:21:28,503 EPOCH 4138
2024-02-02 01:21:42,470 Epoch 4138: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:21:42,471 EPOCH 4139
2024-02-02 01:21:56,264 Epoch 4139: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:21:56,265 EPOCH 4140
2024-02-02 01:22:10,234 Epoch 4140: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:22:10,234 EPOCH 4141
2024-02-02 01:22:24,034 Epoch 4141: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:22:24,035 EPOCH 4142
2024-02-02 01:22:37,729 Epoch 4142: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 01:22:37,730 EPOCH 4143
2024-02-02 01:22:51,819 Epoch 4143: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 01:22:51,819 EPOCH 4144
2024-02-02 01:23:05,914 Epoch 4144: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:23:05,915 EPOCH 4145
2024-02-02 01:23:10,946 [Epoch: 4145 Step: 00037300] Batch Recognition Loss:   0.000110 => Gls Tokens per Sec:     1018 || Batch Translation Loss:   0.015303 => Txt Tokens per Sec:     2786 || Lr: 0.000050
2024-02-02 01:23:19,856 Epoch 4145: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:23:19,856 EPOCH 4146
2024-02-02 01:23:33,730 Epoch 4146: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:23:33,731 EPOCH 4147
2024-02-02 01:23:47,442 Epoch 4147: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:23:47,443 EPOCH 4148
2024-02-02 01:24:01,527 Epoch 4148: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 01:24:01,529 EPOCH 4149
2024-02-02 01:24:15,201 Epoch 4149: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 01:24:15,201 EPOCH 4150
2024-02-02 01:24:29,366 Epoch 4150: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 01:24:29,367 EPOCH 4151
2024-02-02 01:24:43,085 Epoch 4151: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 01:24:43,085 EPOCH 4152
2024-02-02 01:24:57,026 Epoch 4152: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:24:57,027 EPOCH 4153
2024-02-02 01:25:11,058 Epoch 4153: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 01:25:11,058 EPOCH 4154
2024-02-02 01:25:24,852 Epoch 4154: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:25:24,853 EPOCH 4155
2024-02-02 01:25:38,776 Epoch 4155: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:25:38,776 EPOCH 4156
2024-02-02 01:25:45,071 [Epoch: 4156 Step: 00037400] Batch Recognition Loss:   0.000098 => Gls Tokens per Sec:     1017 || Batch Translation Loss:   0.011729 => Txt Tokens per Sec:     2778 || Lr: 0.000050
2024-02-02 01:25:52,832 Epoch 4156: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:25:52,832 EPOCH 4157
2024-02-02 01:26:06,784 Epoch 4157: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:26:06,784 EPOCH 4158
2024-02-02 01:26:20,770 Epoch 4158: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:26:20,770 EPOCH 4159
2024-02-02 01:26:34,723 Epoch 4159: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:26:34,723 EPOCH 4160
2024-02-02 01:26:48,543 Epoch 4160: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:26:48,543 EPOCH 4161
2024-02-02 01:27:02,371 Epoch 4161: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:27:02,372 EPOCH 4162
2024-02-02 01:27:16,288 Epoch 4162: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:27:16,288 EPOCH 4163
2024-02-02 01:27:30,358 Epoch 4163: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:27:30,359 EPOCH 4164
2024-02-02 01:27:44,330 Epoch 4164: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:27:44,331 EPOCH 4165
2024-02-02 01:27:58,273 Epoch 4165: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 01:27:58,274 EPOCH 4166
2024-02-02 01:28:12,268 Epoch 4166: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:28:12,268 EPOCH 4167
2024-02-02 01:28:23,225 [Epoch: 4167 Step: 00037500] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:      620 || Batch Translation Loss:   0.024200 => Txt Tokens per Sec:     1732 || Lr: 0.000050
2024-02-02 01:28:26,110 Epoch 4167: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:28:26,110 EPOCH 4168
2024-02-02 01:28:39,808 Epoch 4168: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:28:39,809 EPOCH 4169
2024-02-02 01:28:53,712 Epoch 4169: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:28:53,712 EPOCH 4170
2024-02-02 01:29:07,762 Epoch 4170: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:29:07,762 EPOCH 4171
2024-02-02 01:29:21,517 Epoch 4171: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 01:29:21,518 EPOCH 4172
2024-02-02 01:29:35,798 Epoch 4172: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:29:35,799 EPOCH 4173
2024-02-02 01:29:49,679 Epoch 4173: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 01:29:49,680 EPOCH 4174
2024-02-02 01:30:03,546 Epoch 4174: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 01:30:03,547 EPOCH 4175
2024-02-02 01:30:17,380 Epoch 4175: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 01:30:17,380 EPOCH 4176
2024-02-02 01:30:31,506 Epoch 4176: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 01:30:31,506 EPOCH 4177
2024-02-02 01:30:45,283 Epoch 4177: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 01:30:45,284 EPOCH 4178
2024-02-02 01:30:55,545 [Epoch: 4178 Step: 00037600] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:      787 || Batch Translation Loss:   0.122201 => Txt Tokens per Sec:     2141 || Lr: 0.000050
2024-02-02 01:30:59,046 Epoch 4178: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 01:30:59,047 EPOCH 4179
2024-02-02 01:31:13,138 Epoch 4179: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 01:31:13,139 EPOCH 4180
2024-02-02 01:31:27,049 Epoch 4180: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 01:31:27,050 EPOCH 4181
2024-02-02 01:31:40,785 Epoch 4181: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 01:31:40,786 EPOCH 4182
2024-02-02 01:31:55,092 Epoch 4182: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 01:31:55,093 EPOCH 4183
2024-02-02 01:32:08,961 Epoch 4183: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 01:32:08,962 EPOCH 4184
2024-02-02 01:32:23,046 Epoch 4184: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 01:32:23,046 EPOCH 4185
2024-02-02 01:32:36,801 Epoch 4185: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 01:32:36,801 EPOCH 4186
2024-02-02 01:32:50,791 Epoch 4186: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:32:50,792 EPOCH 4187
2024-02-02 01:33:04,657 Epoch 4187: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 01:33:04,658 EPOCH 4188
2024-02-02 01:33:18,375 Epoch 4188: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 01:33:18,375 EPOCH 4189
2024-02-02 01:33:32,193 [Epoch: 4189 Step: 00037700] Batch Recognition Loss:   0.000104 => Gls Tokens per Sec:      677 || Batch Translation Loss:   0.013292 => Txt Tokens per Sec:     1972 || Lr: 0.000050
2024-02-02 01:33:32,443 Epoch 4189: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:33:32,443 EPOCH 4190
2024-02-02 01:33:46,100 Epoch 4190: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 01:33:46,100 EPOCH 4191
2024-02-02 01:33:59,785 Epoch 4191: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 01:33:59,785 EPOCH 4192
2024-02-02 01:34:13,630 Epoch 4192: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 01:34:13,631 EPOCH 4193
2024-02-02 01:34:27,366 Epoch 4193: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 01:34:27,367 EPOCH 4194
2024-02-02 01:34:41,282 Epoch 4194: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 01:34:41,283 EPOCH 4195
2024-02-02 01:34:55,255 Epoch 4195: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 01:34:55,256 EPOCH 4196
2024-02-02 01:35:09,008 Epoch 4196: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 01:35:09,008 EPOCH 4197
2024-02-02 01:35:23,024 Epoch 4197: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 01:35:23,024 EPOCH 4198
2024-02-02 01:35:36,774 Epoch 4198: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 01:35:36,774 EPOCH 4199
2024-02-02 01:35:50,766 Epoch 4199: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 01:35:50,767 EPOCH 4200
2024-02-02 01:36:04,406 [Epoch: 4200 Step: 00037800] Batch Recognition Loss:   0.000257 => Gls Tokens per Sec:      779 || Batch Translation Loss:   0.010186 => Txt Tokens per Sec:     2164 || Lr: 0.000050
2024-02-02 01:36:04,407 Epoch 4200: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 01:36:04,407 EPOCH 4201
2024-02-02 01:36:18,475 Epoch 4201: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 01:36:18,476 EPOCH 4202
2024-02-02 01:36:32,617 Epoch 4202: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 01:36:32,617 EPOCH 4203
2024-02-02 01:36:46,852 Epoch 4203: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 01:36:46,853 EPOCH 4204
2024-02-02 01:37:00,792 Epoch 4204: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 01:37:00,793 EPOCH 4205
2024-02-02 01:37:14,724 Epoch 4205: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 01:37:14,725 EPOCH 4206
2024-02-02 01:37:28,937 Epoch 4206: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 01:37:28,938 EPOCH 4207
2024-02-02 01:37:42,878 Epoch 4207: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:37:42,879 EPOCH 4208
2024-02-02 01:37:56,690 Epoch 4208: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 01:37:56,690 EPOCH 4209
2024-02-02 01:38:10,778 Epoch 4209: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 01:38:10,778 EPOCH 4210
2024-02-02 01:38:24,570 Epoch 4210: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 01:38:24,571 EPOCH 4211
2024-02-02 01:38:38,702 Epoch 4211: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 01:38:38,703 EPOCH 4212
2024-02-02 01:38:38,955 [Epoch: 4212 Step: 00037900] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:     5100 || Batch Translation Loss:   0.009947 => Txt Tokens per Sec:     9052 || Lr: 0.000050
2024-02-02 01:38:52,671 Epoch 4212: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 01:38:52,671 EPOCH 4213
2024-02-02 01:39:06,417 Epoch 4213: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:39:06,418 EPOCH 4214
2024-02-02 01:39:20,328 Epoch 4214: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:39:20,329 EPOCH 4215
2024-02-02 01:39:34,208 Epoch 4215: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:39:34,209 EPOCH 4216
2024-02-02 01:39:48,459 Epoch 4216: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:39:48,459 EPOCH 4217
2024-02-02 01:40:02,336 Epoch 4217: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 01:40:02,337 EPOCH 4218
2024-02-02 01:40:16,250 Epoch 4218: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 01:40:16,251 EPOCH 4219
2024-02-02 01:40:30,297 Epoch 4219: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 01:40:30,298 EPOCH 4220
2024-02-02 01:40:44,092 Epoch 4220: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 01:40:44,093 EPOCH 4221
2024-02-02 01:40:57,845 Epoch 4221: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 01:40:57,845 EPOCH 4222
2024-02-02 01:41:11,645 Epoch 4222: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:41:11,645 EPOCH 4223
2024-02-02 01:41:15,451 [Epoch: 4223 Step: 00038000] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:      439 || Batch Translation Loss:   0.018255 => Txt Tokens per Sec:     1288 || Lr: 0.000050
2024-02-02 01:41:35,126 Validation result at epoch 4223, step    38000: duration: 19.6736s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00036	Translation Loss: 102211.31250	PPL: 27675.18555
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.75	(BLEU-1: 11.30,	BLEU-2: 3.47,	BLEU-3: 1.47,	BLEU-4: 0.75)
	CHRF 17.59	ROUGE 9.40
2024-02-02 01:41:35,127 Logging Recognition and Translation Outputs
2024-02-02 01:41:35,127 ========================================================================================================================
2024-02-02 01:41:35,128 Logging Sequence: 72_194.00
2024-02-02 01:41:35,128 	Gloss Reference :	A B+C+D+E
2024-02-02 01:41:35,129 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 01:41:35,129 	Gloss Alignment :	         
2024-02-02 01:41:35,129 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 01:41:35,130 	Text Reference  :	***** **** shah told her to    do  what she  wants   and filed a police complaint against her      
2024-02-02 01:41:35,131 	Text Hypothesis :	babar kept me   tell you there and they have decided to  such  a ****** huge      fan     following
2024-02-02 01:41:35,131 	Text Alignment  :	I     I    S    S    S   S     S   S    S    S       S   S       D      S         S       S        
2024-02-02 01:41:35,131 ========================================================================================================================
2024-02-02 01:41:35,131 Logging Sequence: 108_59.00
2024-02-02 01:41:35,131 	Gloss Reference :	A B+C+D+E
2024-02-02 01:41:35,132 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 01:41:35,132 	Gloss Alignment :	         
2024-02-02 01:41:35,132 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 01:41:35,134 	Text Reference  :	ishan kishan remained the  biggest buy    of  ipl     as  mumbai indians paid    a    whopping rs 1525 crore to keep him
2024-02-02 01:41:35,134 	Text Hypothesis :	in    the    end      rpsg group   placed the winning bid for    the     lucknow team for      rs 7090 crore ** **** ***
2024-02-02 01:41:35,134 	Text Alignment  :	S     S      S        S    S       S      S   S       S   S      S       S       S    S           S          D  D    D  
2024-02-02 01:41:35,134 ========================================================================================================================
2024-02-02 01:41:35,134 Logging Sequence: 109_10.00
2024-02-02 01:41:35,135 	Gloss Reference :	A B+C+D+E
2024-02-02 01:41:35,135 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 01:41:35,135 	Gloss Alignment :	         
2024-02-02 01:41:35,135 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 01:41:35,136 	Text Reference  :	was     scheduled to *** ***** ** **** *** be played at   the narendra modi stadium in  ahmedabad
2024-02-02 01:41:35,136 	Text Hypothesis :	however up        to the match we have kkr ms dhoni  will be  banned   are  on      the players  
2024-02-02 01:41:35,136 	Text Alignment  :	S       S            I   I     I  I    I   S  S      S    S   S        S    S       S   S        
2024-02-02 01:41:35,137 ========================================================================================================================
2024-02-02 01:41:35,137 Logging Sequence: 103_202.00
2024-02-02 01:41:35,137 	Gloss Reference :	A B+C+D+E
2024-02-02 01:41:35,137 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 01:41:35,137 	Gloss Alignment :	         
2024-02-02 01:41:35,137 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 01:41:35,139 	Text Reference  :	***** **** ** *** ******** **** india    in    total has won     61 medals including 22 gold medals 16  silver medals 23    bronze medals
2024-02-02 01:41:35,140 	Text Hypothesis :	since then on 8th december 2021 replaced kohli as    odi captain as well   let       me tell you    can watch  the    right pad    bcci  
2024-02-02 01:41:35,140 	Text Alignment  :	I     I    I  I   I        I    S        S     S     S   S       S  S      S         S  S    S      S   S      S      S     S      S     
2024-02-02 01:41:35,140 ========================================================================================================================
2024-02-02 01:41:35,140 Logging Sequence: 149_77.00
2024-02-02 01:41:35,140 	Gloss Reference :	A B+C+D+E
2024-02-02 01:41:35,140 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 01:41:35,140 	Gloss Alignment :	         
2024-02-02 01:41:35,140 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 01:41:35,142 	Text Reference  :	and arrested danushka for alleged sexual assault of    a   29   year old       woman whose name has      not been disclosed
2024-02-02 01:41:35,142 	Text Hypothesis :	*** since    there    was no      one    day     which has left her  happiness while kaif  and  everyone is  also started' 
2024-02-02 01:41:35,143 	Text Alignment  :	D   S        S        S   S       S      S       S     S   S    S    S         S     S     S    S        S   S    S        
2024-02-02 01:41:35,143 ========================================================================================================================
2024-02-02 01:41:45,503 Epoch 4223: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 01:41:45,503 EPOCH 4224
2024-02-02 01:41:59,528 Epoch 4224: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 01:41:59,529 EPOCH 4225
2024-02-02 01:42:13,421 Epoch 4225: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 01:42:13,421 EPOCH 4226
2024-02-02 01:42:27,301 Epoch 4226: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 01:42:27,302 EPOCH 4227
2024-02-02 01:42:41,499 Epoch 4227: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 01:42:41,500 EPOCH 4228
2024-02-02 01:42:55,215 Epoch 4228: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 01:42:55,215 EPOCH 4229
2024-02-02 01:43:08,955 Epoch 4229: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 01:43:08,955 EPOCH 4230
2024-02-02 01:43:22,742 Epoch 4230: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 01:43:22,743 EPOCH 4231
2024-02-02 01:43:36,465 Epoch 4231: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 01:43:36,465 EPOCH 4232
2024-02-02 01:43:50,179 Epoch 4232: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 01:43:50,179 EPOCH 4233
2024-02-02 01:44:04,138 Epoch 4233: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 01:44:04,138 EPOCH 4234
2024-02-02 01:44:06,841 [Epoch: 4234 Step: 00038100] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:     1421 || Batch Translation Loss:   0.041350 => Txt Tokens per Sec:     3938 || Lr: 0.000050
2024-02-02 01:44:18,087 Epoch 4234: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 01:44:18,087 EPOCH 4235
2024-02-02 01:44:32,012 Epoch 4235: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 01:44:32,012 EPOCH 4236
2024-02-02 01:44:45,657 Epoch 4236: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 01:44:45,657 EPOCH 4237
2024-02-02 01:44:59,347 Epoch 4237: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 01:44:59,348 EPOCH 4238
2024-02-02 01:45:13,418 Epoch 4238: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 01:45:13,419 EPOCH 4239
2024-02-02 01:45:27,505 Epoch 4239: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 01:45:27,506 EPOCH 4240
2024-02-02 01:45:41,566 Epoch 4240: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 01:45:41,567 EPOCH 4241
2024-02-02 01:45:55,447 Epoch 4241: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 01:45:55,448 EPOCH 4242
2024-02-02 01:46:09,412 Epoch 4242: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 01:46:09,412 EPOCH 4243
2024-02-02 01:46:23,097 Epoch 4243: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 01:46:23,098 EPOCH 4244
2024-02-02 01:46:37,122 Epoch 4244: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:46:37,122 EPOCH 4245
2024-02-02 01:46:44,092 [Epoch: 4245 Step: 00038200] Batch Recognition Loss:   0.000117 => Gls Tokens per Sec:      607 || Batch Translation Loss:   0.012727 => Txt Tokens per Sec:     1571 || Lr: 0.000050
2024-02-02 01:46:51,002 Epoch 4245: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:46:51,003 EPOCH 4246
2024-02-02 01:47:04,579 Epoch 4246: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:47:04,579 EPOCH 4247
2024-02-02 01:47:18,339 Epoch 4247: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 01:47:18,340 EPOCH 4248
2024-02-02 01:47:32,133 Epoch 4248: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 01:47:32,134 EPOCH 4249
2024-02-02 01:47:46,186 Epoch 4249: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 01:47:46,186 EPOCH 4250
2024-02-02 01:48:00,320 Epoch 4250: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 01:48:00,321 EPOCH 4251
2024-02-02 01:48:14,132 Epoch 4251: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 01:48:14,133 EPOCH 4252
2024-02-02 01:48:27,952 Epoch 4252: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:48:27,952 EPOCH 4253
2024-02-02 01:48:42,028 Epoch 4253: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:48:42,029 EPOCH 4254
2024-02-02 01:48:55,754 Epoch 4254: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:48:55,755 EPOCH 4255
2024-02-02 01:49:09,634 Epoch 4255: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:49:09,635 EPOCH 4256
2024-02-02 01:49:16,044 [Epoch: 4256 Step: 00038300] Batch Recognition Loss:   0.000107 => Gls Tokens per Sec:      999 || Batch Translation Loss:   0.012764 => Txt Tokens per Sec:     2614 || Lr: 0.000050
2024-02-02 01:49:23,411 Epoch 4256: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:49:23,411 EPOCH 4257
2024-02-02 01:49:37,315 Epoch 4257: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:49:37,316 EPOCH 4258
2024-02-02 01:49:51,402 Epoch 4258: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:49:51,403 EPOCH 4259
2024-02-02 01:50:05,351 Epoch 4259: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:50:05,351 EPOCH 4260
2024-02-02 01:50:19,574 Epoch 4260: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:50:19,574 EPOCH 4261
2024-02-02 01:50:33,547 Epoch 4261: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:50:33,548 EPOCH 4262
2024-02-02 01:50:47,398 Epoch 4262: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:50:47,399 EPOCH 4263
2024-02-02 01:51:01,410 Epoch 4263: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 01:51:01,410 EPOCH 4264
2024-02-02 01:51:15,197 Epoch 4264: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:51:15,198 EPOCH 4265
2024-02-02 01:51:29,026 Epoch 4265: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:51:29,026 EPOCH 4266
2024-02-02 01:51:42,857 Epoch 4266: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:51:42,858 EPOCH 4267
2024-02-02 01:51:52,195 [Epoch: 4267 Step: 00038400] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:      727 || Batch Translation Loss:   0.016472 => Txt Tokens per Sec:     1963 || Lr: 0.000050
2024-02-02 01:51:56,693 Epoch 4267: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:51:56,693 EPOCH 4268
2024-02-02 01:52:10,293 Epoch 4268: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:52:10,293 EPOCH 4269
2024-02-02 01:52:24,140 Epoch 4269: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:52:24,141 EPOCH 4270
2024-02-02 01:52:37,935 Epoch 4270: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 01:52:37,936 EPOCH 4271
2024-02-02 01:52:51,922 Epoch 4271: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 01:52:51,923 EPOCH 4272
2024-02-02 01:53:05,733 Epoch 4272: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 01:53:05,733 EPOCH 4273
2024-02-02 01:53:19,723 Epoch 4273: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 01:53:19,724 EPOCH 4274
2024-02-02 01:53:33,538 Epoch 4274: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 01:53:33,539 EPOCH 4275
2024-02-02 01:53:47,365 Epoch 4275: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 01:53:47,365 EPOCH 4276
2024-02-02 01:54:01,168 Epoch 4276: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 01:54:01,168 EPOCH 4277
2024-02-02 01:54:14,885 Epoch 4277: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 01:54:14,885 EPOCH 4278
2024-02-02 01:54:25,358 [Epoch: 4278 Step: 00038500] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:      771 || Batch Translation Loss:   0.016063 => Txt Tokens per Sec:     2123 || Lr: 0.000050
2024-02-02 01:54:28,799 Epoch 4278: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 01:54:28,799 EPOCH 4279
2024-02-02 01:54:42,677 Epoch 4279: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 01:54:42,677 EPOCH 4280
2024-02-02 01:54:56,692 Epoch 4280: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:54:56,693 EPOCH 4281
2024-02-02 01:55:10,456 Epoch 4281: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 01:55:10,456 EPOCH 4282
2024-02-02 01:55:24,608 Epoch 4282: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 01:55:24,609 EPOCH 4283
2024-02-02 01:55:38,661 Epoch 4283: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:55:38,662 EPOCH 4284
2024-02-02 01:55:52,557 Epoch 4284: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 01:55:52,558 EPOCH 4285
2024-02-02 01:56:06,233 Epoch 4285: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:56:06,234 EPOCH 4286
2024-02-02 01:56:20,209 Epoch 4286: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:56:20,210 EPOCH 4287
2024-02-02 01:56:34,090 Epoch 4287: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 01:56:34,091 EPOCH 4288
2024-02-02 01:56:47,927 Epoch 4288: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:56:47,928 EPOCH 4289
2024-02-02 01:57:01,232 [Epoch: 4289 Step: 00038600] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:      703 || Batch Translation Loss:   0.020898 => Txt Tokens per Sec:     1954 || Lr: 0.000050
2024-02-02 01:57:01,773 Epoch 4289: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:57:01,773 EPOCH 4290
2024-02-02 01:57:15,544 Epoch 4290: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 01:57:15,544 EPOCH 4291
2024-02-02 01:57:29,539 Epoch 4291: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 01:57:29,540 EPOCH 4292
2024-02-02 01:57:43,659 Epoch 4292: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 01:57:43,660 EPOCH 4293
2024-02-02 01:57:57,760 Epoch 4293: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 01:57:57,760 EPOCH 4294
2024-02-02 01:58:11,512 Epoch 4294: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 01:58:11,512 EPOCH 4295
2024-02-02 01:58:25,180 Epoch 4295: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 01:58:25,180 EPOCH 4296
2024-02-02 01:58:39,181 Epoch 4296: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 01:58:39,181 EPOCH 4297
2024-02-02 01:58:53,005 Epoch 4297: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 01:58:53,005 EPOCH 4298
2024-02-02 01:59:07,183 Epoch 4298: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 01:59:07,183 EPOCH 4299
2024-02-02 01:59:21,119 Epoch 4299: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 01:59:21,120 EPOCH 4300
2024-02-02 01:59:35,035 [Epoch: 4300 Step: 00038700] Batch Recognition Loss:   0.000259 => Gls Tokens per Sec:      764 || Batch Translation Loss:   0.013730 => Txt Tokens per Sec:     2121 || Lr: 0.000050
2024-02-02 01:59:35,035 Epoch 4300: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 01:59:35,036 EPOCH 4301
2024-02-02 01:59:49,017 Epoch 4301: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 01:59:49,018 EPOCH 4302
2024-02-02 02:00:02,900 Epoch 4302: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 02:00:02,900 EPOCH 4303
2024-02-02 02:00:16,804 Epoch 4303: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 02:00:16,804 EPOCH 4304
2024-02-02 02:00:30,813 Epoch 4304: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 02:00:30,813 EPOCH 4305
2024-02-02 02:00:44,651 Epoch 4305: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 02:00:44,652 EPOCH 4306
2024-02-02 02:00:58,622 Epoch 4306: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 02:00:58,623 EPOCH 4307
2024-02-02 02:01:12,671 Epoch 4307: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 02:01:12,672 EPOCH 4308
2024-02-02 02:01:26,523 Epoch 4308: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 02:01:26,523 EPOCH 4309
2024-02-02 02:01:40,472 Epoch 4309: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 02:01:40,472 EPOCH 4310
2024-02-02 02:01:54,276 Epoch 4310: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-02 02:01:54,277 EPOCH 4311
2024-02-02 02:02:08,196 Epoch 4311: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.19 
2024-02-02 02:02:08,196 EPOCH 4312
2024-02-02 02:02:11,324 [Epoch: 4312 Step: 00038800] Batch Recognition Loss:   0.002160 => Gls Tokens per Sec:      409 || Batch Translation Loss:   0.398802 => Txt Tokens per Sec:     1324 || Lr: 0.000050
2024-02-02 02:02:21,940 Epoch 4312: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.64 
2024-02-02 02:02:21,941 EPOCH 4313
2024-02-02 02:02:35,987 Epoch 4313: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-02 02:02:35,988 EPOCH 4314
2024-02-02 02:02:49,922 Epoch 4314: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-02 02:02:49,923 EPOCH 4315
2024-02-02 02:03:03,937 Epoch 4315: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 02:03:03,937 EPOCH 4316
2024-02-02 02:03:17,692 Epoch 4316: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 02:03:17,693 EPOCH 4317
2024-02-02 02:03:31,544 Epoch 4317: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 02:03:31,545 EPOCH 4318
2024-02-02 02:03:45,598 Epoch 4318: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 02:03:45,599 EPOCH 4319
2024-02-02 02:03:59,384 Epoch 4319: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 02:03:59,385 EPOCH 4320
2024-02-02 02:04:13,530 Epoch 4320: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 02:04:13,531 EPOCH 4321
2024-02-02 02:04:27,633 Epoch 4321: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 02:04:27,633 EPOCH 4322
2024-02-02 02:04:41,570 Epoch 4322: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 02:04:41,571 EPOCH 4323
2024-02-02 02:04:43,663 [Epoch: 4323 Step: 00038900] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:     1224 || Batch Translation Loss:   0.016815 => Txt Tokens per Sec:     3363 || Lr: 0.000050
2024-02-02 02:04:55,456 Epoch 4323: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 02:04:55,456 EPOCH 4324
2024-02-02 02:05:09,336 Epoch 4324: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 02:05:09,337 EPOCH 4325
2024-02-02 02:05:23,100 Epoch 4325: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 02:05:23,101 EPOCH 4326
2024-02-02 02:05:36,933 Epoch 4326: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 02:05:36,934 EPOCH 4327
2024-02-02 02:05:50,879 Epoch 4327: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 02:05:50,880 EPOCH 4328
2024-02-02 02:06:04,998 Epoch 4328: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 02:06:04,998 EPOCH 4329
2024-02-02 02:06:18,526 Epoch 4329: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 02:06:18,526 EPOCH 4330
2024-02-02 02:06:32,398 Epoch 4330: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 02:06:32,399 EPOCH 4331
2024-02-02 02:06:46,378 Epoch 4331: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 02:06:46,379 EPOCH 4332
2024-02-02 02:07:00,196 Epoch 4332: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 02:07:00,196 EPOCH 4333
2024-02-02 02:07:14,264 Epoch 4333: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 02:07:14,265 EPOCH 4334
2024-02-02 02:07:18,468 [Epoch: 4334 Step: 00039000] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:      702 || Batch Translation Loss:   0.018345 => Txt Tokens per Sec:     1703 || Lr: 0.000050
2024-02-02 02:07:28,304 Epoch 4334: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:07:28,305 EPOCH 4335
2024-02-02 02:07:42,287 Epoch 4335: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:07:42,287 EPOCH 4336
2024-02-02 02:07:56,265 Epoch 4336: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 02:07:56,266 EPOCH 4337
2024-02-02 02:08:10,218 Epoch 4337: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:08:10,219 EPOCH 4338
2024-02-02 02:08:24,182 Epoch 4338: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:08:24,183 EPOCH 4339
2024-02-02 02:08:38,075 Epoch 4339: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:08:38,076 EPOCH 4340
2024-02-02 02:08:51,819 Epoch 4340: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:08:51,819 EPOCH 4341
2024-02-02 02:09:06,027 Epoch 4341: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:09:06,027 EPOCH 4342
2024-02-02 02:09:19,916 Epoch 4342: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 02:09:19,917 EPOCH 4343
2024-02-02 02:09:33,941 Epoch 4343: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 02:09:33,942 EPOCH 4344
2024-02-02 02:09:47,793 Epoch 4344: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 02:09:47,794 EPOCH 4345
2024-02-02 02:09:51,954 [Epoch: 4345 Step: 00039100] Batch Recognition Loss:   0.000120 => Gls Tokens per Sec:     1231 || Batch Translation Loss:   0.014221 => Txt Tokens per Sec:     3010 || Lr: 0.000050
2024-02-02 02:10:01,717 Epoch 4345: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 02:10:01,718 EPOCH 4346
2024-02-02 02:10:15,428 Epoch 4346: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 02:10:15,428 EPOCH 4347
2024-02-02 02:10:29,381 Epoch 4347: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 02:10:29,382 EPOCH 4348
2024-02-02 02:10:43,187 Epoch 4348: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 02:10:43,188 EPOCH 4349
2024-02-02 02:10:57,076 Epoch 4349: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:10:57,077 EPOCH 4350
2024-02-02 02:11:11,059 Epoch 4350: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:11:11,059 EPOCH 4351
2024-02-02 02:11:24,962 Epoch 4351: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:11:24,963 EPOCH 4352
2024-02-02 02:11:38,594 Epoch 4352: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:11:38,595 EPOCH 4353
2024-02-02 02:11:52,821 Epoch 4353: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:11:52,821 EPOCH 4354
2024-02-02 02:12:06,875 Epoch 4354: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:12:06,876 EPOCH 4355
2024-02-02 02:12:20,848 Epoch 4355: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:12:20,849 EPOCH 4356
2024-02-02 02:12:30,206 [Epoch: 4356 Step: 00039200] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:      684 || Batch Translation Loss:   0.020741 => Txt Tokens per Sec:     1974 || Lr: 0.000050
2024-02-02 02:12:34,791 Epoch 4356: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:12:34,791 EPOCH 4357
2024-02-02 02:12:48,873 Epoch 4357: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:12:48,874 EPOCH 4358
2024-02-02 02:13:02,783 Epoch 4358: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:13:02,783 EPOCH 4359
2024-02-02 02:13:16,894 Epoch 4359: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:13:16,895 EPOCH 4360
2024-02-02 02:13:30,870 Epoch 4360: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:13:30,870 EPOCH 4361
2024-02-02 02:13:44,677 Epoch 4361: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:13:44,677 EPOCH 4362
2024-02-02 02:13:58,595 Epoch 4362: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:13:58,596 EPOCH 4363
2024-02-02 02:14:12,282 Epoch 4363: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 02:14:12,283 EPOCH 4364
2024-02-02 02:14:26,246 Epoch 4364: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:14:26,246 EPOCH 4365
2024-02-02 02:14:39,943 Epoch 4365: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:14:39,943 EPOCH 4366
2024-02-02 02:14:53,959 Epoch 4366: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:14:53,960 EPOCH 4367
2024-02-02 02:15:03,723 [Epoch: 4367 Step: 00039300] Batch Recognition Loss:   0.000148 => Gls Tokens per Sec:      695 || Batch Translation Loss:   0.017964 => Txt Tokens per Sec:     1973 || Lr: 0.000050
2024-02-02 02:15:07,603 Epoch 4367: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:15:07,603 EPOCH 4368
2024-02-02 02:15:21,744 Epoch 4368: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:15:21,745 EPOCH 4369
2024-02-02 02:15:35,375 Epoch 4369: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:15:35,376 EPOCH 4370
2024-02-02 02:15:49,292 Epoch 4370: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 02:15:49,293 EPOCH 4371
2024-02-02 02:16:03,190 Epoch 4371: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 02:16:03,190 EPOCH 4372
2024-02-02 02:16:16,909 Epoch 4372: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 02:16:16,910 EPOCH 4373
2024-02-02 02:16:30,718 Epoch 4373: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 02:16:30,719 EPOCH 4374
2024-02-02 02:16:44,676 Epoch 4374: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 02:16:44,677 EPOCH 4375
2024-02-02 02:16:58,602 Epoch 4375: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:16:58,602 EPOCH 4376
2024-02-02 02:17:12,500 Epoch 4376: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:17:12,501 EPOCH 4377
2024-02-02 02:17:26,568 Epoch 4377: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 02:17:26,569 EPOCH 4378
2024-02-02 02:17:36,895 [Epoch: 4378 Step: 00039400] Batch Recognition Loss:   0.000175 => Gls Tokens per Sec:      782 || Batch Translation Loss:   0.006694 => Txt Tokens per Sec:     2153 || Lr: 0.000050
2024-02-02 02:17:40,336 Epoch 4378: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:17:40,336 EPOCH 4379
2024-02-02 02:17:54,327 Epoch 4379: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:17:54,327 EPOCH 4380
2024-02-02 02:18:08,202 Epoch 4380: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:18:08,202 EPOCH 4381
2024-02-02 02:18:22,032 Epoch 4381: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:18:22,033 EPOCH 4382
2024-02-02 02:18:35,925 Epoch 4382: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:18:35,926 EPOCH 4383
2024-02-02 02:18:49,834 Epoch 4383: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:18:49,835 EPOCH 4384
2024-02-02 02:19:03,691 Epoch 4384: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 02:19:03,692 EPOCH 4385
2024-02-02 02:19:17,416 Epoch 4385: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:19:17,417 EPOCH 4386
2024-02-02 02:19:31,204 Epoch 4386: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:19:31,205 EPOCH 4387
2024-02-02 02:19:45,100 Epoch 4387: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 02:19:45,101 EPOCH 4388
2024-02-02 02:19:58,856 Epoch 4388: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 02:19:58,857 EPOCH 4389
2024-02-02 02:20:09,675 [Epoch: 4389 Step: 00039500] Batch Recognition Loss:   0.000121 => Gls Tokens per Sec:      947 || Batch Translation Loss:   0.021118 => Txt Tokens per Sec:     2599 || Lr: 0.000050
2024-02-02 02:20:12,922 Epoch 4389: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:20:12,923 EPOCH 4390
2024-02-02 02:20:26,772 Epoch 4390: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:20:26,773 EPOCH 4391
2024-02-02 02:20:40,500 Epoch 4391: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 02:20:40,501 EPOCH 4392
2024-02-02 02:20:54,510 Epoch 4392: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 02:20:54,511 EPOCH 4393
2024-02-02 02:21:08,383 Epoch 4393: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 02:21:08,383 EPOCH 4394
2024-02-02 02:21:22,379 Epoch 4394: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 02:21:22,379 EPOCH 4395
2024-02-02 02:21:36,520 Epoch 4395: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:21:36,521 EPOCH 4396
2024-02-02 02:21:50,296 Epoch 4396: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:21:50,297 EPOCH 4397
2024-02-02 02:22:04,135 Epoch 4397: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 02:22:04,135 EPOCH 4398
2024-02-02 02:22:18,020 Epoch 4398: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:22:18,021 EPOCH 4399
2024-02-02 02:22:31,829 Epoch 4399: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:22:31,830 EPOCH 4400
2024-02-02 02:22:45,902 [Epoch: 4400 Step: 00039600] Batch Recognition Loss:   0.000089 => Gls Tokens per Sec:      756 || Batch Translation Loss:   0.010725 => Txt Tokens per Sec:     2097 || Lr: 0.000050
2024-02-02 02:22:45,902 Epoch 4400: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:22:45,903 EPOCH 4401
2024-02-02 02:22:59,890 Epoch 4401: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:22:59,890 EPOCH 4402
2024-02-02 02:23:13,971 Epoch 4402: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:23:13,971 EPOCH 4403
2024-02-02 02:23:27,708 Epoch 4403: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:23:27,709 EPOCH 4404
2024-02-02 02:23:41,876 Epoch 4404: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:23:41,877 EPOCH 4405
2024-02-02 02:23:56,028 Epoch 4405: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:23:56,029 EPOCH 4406
2024-02-02 02:24:09,815 Epoch 4406: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:24:09,816 EPOCH 4407
2024-02-02 02:24:23,609 Epoch 4407: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:24:23,610 EPOCH 4408
2024-02-02 02:24:37,662 Epoch 4408: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:24:37,663 EPOCH 4409
2024-02-02 02:24:51,404 Epoch 4409: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 02:24:51,405 EPOCH 4410
2024-02-02 02:25:05,385 Epoch 4410: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 02:25:05,386 EPOCH 4411
2024-02-02 02:25:18,977 Epoch 4411: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:25:18,977 EPOCH 4412
2024-02-02 02:25:22,063 [Epoch: 4412 Step: 00039700] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:      415 || Batch Translation Loss:   0.014466 => Txt Tokens per Sec:     1323 || Lr: 0.000050
2024-02-02 02:25:33,030 Epoch 4412: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:25:33,030 EPOCH 4413
2024-02-02 02:25:46,804 Epoch 4413: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:25:46,805 EPOCH 4414
2024-02-02 02:26:00,783 Epoch 4414: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:26:00,784 EPOCH 4415
2024-02-02 02:26:14,696 Epoch 4415: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:26:14,696 EPOCH 4416
2024-02-02 02:26:28,615 Epoch 4416: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:26:28,616 EPOCH 4417
2024-02-02 02:26:42,488 Epoch 4417: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 02:26:42,489 EPOCH 4418
2024-02-02 02:26:56,137 Epoch 4418: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:26:56,138 EPOCH 4419
2024-02-02 02:27:09,475 Epoch 4419: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:27:09,476 EPOCH 4420
2024-02-02 02:27:23,372 Epoch 4420: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:27:23,372 EPOCH 4421
2024-02-02 02:27:37,317 Epoch 4421: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 02:27:37,317 EPOCH 4422
2024-02-02 02:27:51,470 Epoch 4422: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:27:51,470 EPOCH 4423
2024-02-02 02:27:53,546 [Epoch: 4423 Step: 00039800] Batch Recognition Loss:   0.000125 => Gls Tokens per Sec:     1234 || Batch Translation Loss:   0.024249 => Txt Tokens per Sec:     3309 || Lr: 0.000050
2024-02-02 02:28:05,485 Epoch 4423: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:28:05,486 EPOCH 4424
2024-02-02 02:28:19,313 Epoch 4424: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:28:19,314 EPOCH 4425
2024-02-02 02:28:33,070 Epoch 4425: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:28:33,070 EPOCH 4426
2024-02-02 02:28:46,927 Epoch 4426: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 02:28:46,928 EPOCH 4427
2024-02-02 02:29:01,102 Epoch 4427: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 02:29:01,102 EPOCH 4428
2024-02-02 02:29:15,132 Epoch 4428: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:29:15,132 EPOCH 4429
2024-02-02 02:29:29,060 Epoch 4429: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 02:29:29,061 EPOCH 4430
2024-02-02 02:29:42,933 Epoch 4430: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:29:42,933 EPOCH 4431
2024-02-02 02:29:56,867 Epoch 4431: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:29:56,868 EPOCH 4432
2024-02-02 02:30:10,849 Epoch 4432: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:30:10,850 EPOCH 4433
2024-02-02 02:30:24,854 Epoch 4433: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:30:24,855 EPOCH 4434
2024-02-02 02:30:31,798 [Epoch: 4434 Step: 00039900] Batch Recognition Loss:   0.000108 => Gls Tokens per Sec:      425 || Batch Translation Loss:   0.011815 => Txt Tokens per Sec:     1292 || Lr: 0.000050
2024-02-02 02:30:38,846 Epoch 4434: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:30:38,846 EPOCH 4435
2024-02-02 02:30:52,770 Epoch 4435: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:30:52,771 EPOCH 4436
2024-02-02 02:31:06,640 Epoch 4436: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:31:06,640 EPOCH 4437
2024-02-02 02:31:20,559 Epoch 4437: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:31:20,560 EPOCH 4438
2024-02-02 02:31:34,781 Epoch 4438: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 02:31:34,781 EPOCH 4439
2024-02-02 02:31:48,506 Epoch 4439: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 02:31:48,506 EPOCH 4440
2024-02-02 02:32:02,307 Epoch 4440: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 02:32:02,307 EPOCH 4441
2024-02-02 02:32:16,556 Epoch 4441: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:32:16,556 EPOCH 4442
2024-02-02 02:32:30,721 Epoch 4442: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 02:32:30,721 EPOCH 4443
2024-02-02 02:32:44,845 Epoch 4443: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:32:44,845 EPOCH 4444
2024-02-02 02:32:58,701 Epoch 4444: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:32:58,701 EPOCH 4445
2024-02-02 02:33:03,764 [Epoch: 4445 Step: 00040000] Batch Recognition Loss:   0.000109 => Gls Tokens per Sec:     1012 || Batch Translation Loss:   0.014042 => Txt Tokens per Sec:     2766 || Lr: 0.000050
2024-02-02 02:33:23,239 Validation result at epoch 4445, step    40000: duration: 19.4747s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00036	Translation Loss: 102199.35938	PPL: 27642.08398
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.37	(BLEU-1: 10.76,	BLEU-2: 3.03,	BLEU-3: 1.09,	BLEU-4: 0.37)
	CHRF 16.77	ROUGE 9.10
2024-02-02 02:33:23,240 Logging Recognition and Translation Outputs
2024-02-02 02:33:23,240 ========================================================================================================================
2024-02-02 02:33:23,241 Logging Sequence: 123_104.00
2024-02-02 02:33:23,241 	Gloss Reference :	A B+C+D+E
2024-02-02 02:33:23,241 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 02:33:23,241 	Gloss Alignment :	         
2024-02-02 02:33:23,241 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 02:33:23,242 	Text Reference  :	the car    was   presented to  the former india cricketer from an   unknown person 
2024-02-02 02:33:23,242 	Text Hypothesis :	the former india had       won the ****** ***** toss      and  left out     captain
2024-02-02 02:33:23,243 	Text Alignment  :	    S      S     S         S       D      D     S         S    S    S       S      
2024-02-02 02:33:23,243 ========================================================================================================================
2024-02-02 02:33:23,243 Logging Sequence: 107_23.00
2024-02-02 02:33:23,243 	Gloss Reference :	A B+C+D+E
2024-02-02 02:33:23,243 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 02:33:23,243 	Gloss Alignment :	         
2024-02-02 02:33:23,243 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 02:33:23,244 	Text Reference  :	and viktor lilov who is  also    from the    usa   
2024-02-02 02:33:23,244 	Text Hypothesis :	*** ****** ***** if  the matches were became second
2024-02-02 02:33:23,244 	Text Alignment  :	D   D      D     S   S   S       S    S      S     
2024-02-02 02:33:23,244 ========================================================================================================================
2024-02-02 02:33:23,244 Logging Sequence: 134_212.00
2024-02-02 02:33:23,245 	Gloss Reference :	A B+C+D+E
2024-02-02 02:33:23,245 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 02:33:23,245 	Gloss Alignment :	         
2024-02-02 02:33:23,245 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 02:33:23,246 	Text Reference  :	*** dhanush said that he   practises little  yoga   
2024-02-02 02:33:23,246 	Text Hypothesis :	the video   of   the  deaf to        sanjana ganesan
2024-02-02 02:33:23,246 	Text Alignment  :	I   S       S    S    S    S         S       S      
2024-02-02 02:33:23,246 ========================================================================================================================
2024-02-02 02:33:23,246 Logging Sequence: 165_577.00
2024-02-02 02:33:23,247 	Gloss Reference :	A B+C+D+E
2024-02-02 02:33:23,247 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 02:33:23,247 	Gloss Alignment :	         
2024-02-02 02:33:23,247 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 02:33:23,248 	Text Reference  :	** then after 28    years india won the    world cup   again in  2011
2024-02-02 02:33:23,248 	Text Hypothesis :	it is   not   known as    god   of  vamika has   never lets  the team
2024-02-02 02:33:23,248 	Text Alignment  :	I  S    S     S     S     S     S   S      S     S     S     S   S   
2024-02-02 02:33:23,248 ========================================================================================================================
2024-02-02 02:33:23,249 Logging Sequence: 88_142.00
2024-02-02 02:33:23,249 	Gloss Reference :	A B+C+D+E
2024-02-02 02:33:23,249 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 02:33:23,249 	Gloss Alignment :	         
2024-02-02 02:33:23,249 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 02:33:23,250 	Text Reference  :	*** this        is      because the ****** ** ****** police  does  not  do  anything
2024-02-02 02:33:23,250 	Text Hypothesis :	the supermarket belongs to      the family of lionel messi's first over the police  
2024-02-02 02:33:23,250 	Text Alignment  :	I   S           S       S           I      I  I      S       S     S    S   S       
2024-02-02 02:33:23,250 ========================================================================================================================
2024-02-02 02:33:32,406 Epoch 4445: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:33:32,407 EPOCH 4446
2024-02-02 02:33:46,269 Epoch 4446: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:33:46,269 EPOCH 4447
2024-02-02 02:34:00,324 Epoch 4447: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:34:00,324 EPOCH 4448
2024-02-02 02:34:14,206 Epoch 4448: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:34:14,207 EPOCH 4449
2024-02-02 02:34:27,927 Epoch 4449: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 02:34:27,928 EPOCH 4450
2024-02-02 02:34:41,829 Epoch 4450: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 02:34:41,829 EPOCH 4451
2024-02-02 02:34:55,793 Epoch 4451: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 02:34:55,793 EPOCH 4452
2024-02-02 02:35:09,723 Epoch 4452: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 02:35:09,723 EPOCH 4453
2024-02-02 02:35:23,669 Epoch 4453: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 02:35:23,670 EPOCH 4454
2024-02-02 02:35:37,635 Epoch 4454: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 02:35:37,635 EPOCH 4455
2024-02-02 02:35:51,501 Epoch 4455: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 02:35:51,501 EPOCH 4456
2024-02-02 02:35:57,711 [Epoch: 4456 Step: 00040100] Batch Recognition Loss:   0.000157 => Gls Tokens per Sec:      887 || Batch Translation Loss:   0.068795 => Txt Tokens per Sec:     2436 || Lr: 0.000050
2024-02-02 02:36:05,175 Epoch 4456: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 02:36:05,175 EPOCH 4457
2024-02-02 02:36:19,181 Epoch 4457: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 02:36:19,182 EPOCH 4458
2024-02-02 02:36:33,057 Epoch 4458: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 02:36:33,057 EPOCH 4459
2024-02-02 02:36:47,064 Epoch 4459: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-02 02:36:47,065 EPOCH 4460
2024-02-02 02:37:00,788 Epoch 4460: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 02:37:00,789 EPOCH 4461
2024-02-02 02:37:14,699 Epoch 4461: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 02:37:14,700 EPOCH 4462
2024-02-02 02:37:28,823 Epoch 4462: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 02:37:28,824 EPOCH 4463
2024-02-02 02:37:42,943 Epoch 4463: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 02:37:42,944 EPOCH 4464
2024-02-02 02:37:56,860 Epoch 4464: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 02:37:56,860 EPOCH 4465
2024-02-02 02:38:10,847 Epoch 4465: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 02:38:10,848 EPOCH 4466
2024-02-02 02:38:25,233 Epoch 4466: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 02:38:25,234 EPOCH 4467
2024-02-02 02:38:37,760 [Epoch: 4467 Step: 00040200] Batch Recognition Loss:   0.000262 => Gls Tokens per Sec:      542 || Batch Translation Loss:   0.041423 => Txt Tokens per Sec:     1618 || Lr: 0.000050
2024-02-02 02:38:39,171 Epoch 4467: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 02:38:39,172 EPOCH 4468
2024-02-02 02:38:53,040 Epoch 4468: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 02:38:53,041 EPOCH 4469
2024-02-02 02:39:07,077 Epoch 4469: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 02:39:07,078 EPOCH 4470
2024-02-02 02:39:21,234 Epoch 4470: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 02:39:21,235 EPOCH 4471
2024-02-02 02:39:35,434 Epoch 4471: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 02:39:35,435 EPOCH 4472
2024-02-02 02:39:49,032 Epoch 4472: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 02:39:49,033 EPOCH 4473
2024-02-02 02:40:02,676 Epoch 4473: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 02:40:02,677 EPOCH 4474
2024-02-02 02:40:16,414 Epoch 4474: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 02:40:16,414 EPOCH 4475
2024-02-02 02:40:30,406 Epoch 4475: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 02:40:30,407 EPOCH 4476
2024-02-02 02:40:44,428 Epoch 4476: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 02:40:44,428 EPOCH 4477
2024-02-02 02:40:58,517 Epoch 4477: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 02:40:58,518 EPOCH 4478
2024-02-02 02:41:08,102 [Epoch: 4478 Step: 00040300] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:      842 || Batch Translation Loss:   0.025107 => Txt Tokens per Sec:     2219 || Lr: 0.000050
2024-02-02 02:41:12,301 Epoch 4478: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 02:41:12,302 EPOCH 4479
2024-02-02 02:41:26,428 Epoch 4479: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:41:26,429 EPOCH 4480
2024-02-02 02:41:40,186 Epoch 4480: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:41:40,186 EPOCH 4481
2024-02-02 02:41:54,378 Epoch 4481: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:41:54,378 EPOCH 4482
2024-02-02 02:42:08,147 Epoch 4482: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:42:08,148 EPOCH 4483
2024-02-02 02:42:22,110 Epoch 4483: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:42:22,111 EPOCH 4484
2024-02-02 02:42:36,076 Epoch 4484: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:42:36,077 EPOCH 4485
2024-02-02 02:42:50,107 Epoch 4485: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:42:50,108 EPOCH 4486
2024-02-02 02:43:04,162 Epoch 4486: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:43:04,163 EPOCH 4487
2024-02-02 02:43:18,357 Epoch 4487: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:43:18,357 EPOCH 4488
2024-02-02 02:43:32,261 Epoch 4488: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:43:32,262 EPOCH 4489
2024-02-02 02:43:46,050 [Epoch: 4489 Step: 00040400] Batch Recognition Loss:   0.000103 => Gls Tokens per Sec:      678 || Batch Translation Loss:   0.014392 => Txt Tokens per Sec:     1924 || Lr: 0.000050
2024-02-02 02:43:46,367 Epoch 4489: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:43:46,367 EPOCH 4490
2024-02-02 02:44:00,168 Epoch 4490: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 02:44:00,169 EPOCH 4491
2024-02-02 02:44:14,400 Epoch 4491: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:44:14,401 EPOCH 4492
2024-02-02 02:44:28,214 Epoch 4492: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:44:28,215 EPOCH 4493
2024-02-02 02:44:42,040 Epoch 4493: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:44:42,041 EPOCH 4494
2024-02-02 02:44:56,071 Epoch 4494: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:44:56,071 EPOCH 4495
2024-02-02 02:45:10,118 Epoch 4495: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:45:10,118 EPOCH 4496
2024-02-02 02:45:24,059 Epoch 4496: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:45:24,059 EPOCH 4497
2024-02-02 02:45:37,938 Epoch 4497: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:45:37,939 EPOCH 4498
2024-02-02 02:45:51,912 Epoch 4498: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:45:51,912 EPOCH 4499
2024-02-02 02:46:05,945 Epoch 4499: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:46:05,946 EPOCH 4500
2024-02-02 02:46:19,920 [Epoch: 4500 Step: 00040500] Batch Recognition Loss:   0.000115 => Gls Tokens per Sec:      761 || Batch Translation Loss:   0.013936 => Txt Tokens per Sec:     2112 || Lr: 0.000050
2024-02-02 02:46:19,920 Epoch 4500: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:46:19,921 EPOCH 4501
2024-02-02 02:46:34,114 Epoch 4501: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:46:34,115 EPOCH 4502
2024-02-02 02:46:48,195 Epoch 4502: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:46:48,195 EPOCH 4503
2024-02-02 02:47:02,163 Epoch 4503: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 02:47:02,163 EPOCH 4504
2024-02-02 02:47:16,197 Epoch 4504: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:47:16,198 EPOCH 4505
2024-02-02 02:47:30,306 Epoch 4505: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:47:30,307 EPOCH 4506
2024-02-02 02:47:44,054 Epoch 4506: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 02:47:44,054 EPOCH 4507
2024-02-02 02:47:58,102 Epoch 4507: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:47:58,103 EPOCH 4508
2024-02-02 02:48:11,961 Epoch 4508: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 02:48:11,962 EPOCH 4509
2024-02-02 02:48:25,846 Epoch 4509: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 02:48:25,846 EPOCH 4510
2024-02-02 02:48:39,765 Epoch 4510: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 02:48:39,766 EPOCH 4511
2024-02-02 02:48:53,685 Epoch 4511: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 02:48:53,685 EPOCH 4512
2024-02-02 02:48:54,308 [Epoch: 4512 Step: 00040600] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:     2058 || Batch Translation Loss:   0.016030 => Txt Tokens per Sec:     6064 || Lr: 0.000050
2024-02-02 02:49:07,458 Epoch 4512: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 02:49:07,459 EPOCH 4513
2024-02-02 02:49:21,550 Epoch 4513: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:49:21,551 EPOCH 4514
2024-02-02 02:49:35,176 Epoch 4514: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 02:49:35,177 EPOCH 4515
2024-02-02 02:49:49,042 Epoch 4515: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:49:49,043 EPOCH 4516
2024-02-02 02:50:02,919 Epoch 4516: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 02:50:02,919 EPOCH 4517
2024-02-02 02:50:16,838 Epoch 4517: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 02:50:16,838 EPOCH 4518
2024-02-02 02:50:31,095 Epoch 4518: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 02:50:31,095 EPOCH 4519
2024-02-02 02:50:44,955 Epoch 4519: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 02:50:44,955 EPOCH 4520
2024-02-02 02:50:58,941 Epoch 4520: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 02:50:58,942 EPOCH 4521
2024-02-02 02:51:12,877 Epoch 4521: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 02:51:12,877 EPOCH 4522
2024-02-02 02:51:26,607 Epoch 4522: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 02:51:26,608 EPOCH 4523
2024-02-02 02:51:28,836 [Epoch: 4523 Step: 00040700] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     1150 || Batch Translation Loss:   0.021304 => Txt Tokens per Sec:     3295 || Lr: 0.000050
2024-02-02 02:51:40,717 Epoch 4523: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 02:51:40,718 EPOCH 4524
2024-02-02 02:51:54,616 Epoch 4524: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 02:51:54,617 EPOCH 4525
2024-02-02 02:52:08,441 Epoch 4525: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 02:52:08,441 EPOCH 4526
2024-02-02 02:52:22,491 Epoch 4526: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 02:52:22,491 EPOCH 4527
2024-02-02 02:52:36,265 Epoch 4527: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 02:52:36,265 EPOCH 4528
2024-02-02 02:52:50,277 Epoch 4528: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 02:52:50,278 EPOCH 4529
2024-02-02 02:53:04,156 Epoch 4529: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 02:53:04,157 EPOCH 4530
2024-02-02 02:53:18,236 Epoch 4530: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 02:53:18,237 EPOCH 4531
2024-02-02 02:53:32,237 Epoch 4531: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 02:53:32,238 EPOCH 4532
2024-02-02 02:53:46,231 Epoch 4532: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 02:53:46,232 EPOCH 4533
2024-02-02 02:54:00,188 Epoch 4533: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 02:54:00,189 EPOCH 4534
2024-02-02 02:54:04,675 [Epoch: 4534 Step: 00040800] Batch Recognition Loss:   0.000163 => Gls Tokens per Sec:      658 || Batch Translation Loss:   0.020763 => Txt Tokens per Sec:     1935 || Lr: 0.000050
2024-02-02 02:54:14,259 Epoch 4534: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 02:54:14,260 EPOCH 4535
2024-02-02 02:54:28,061 Epoch 4535: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 02:54:28,061 EPOCH 4536
2024-02-02 02:54:41,868 Epoch 4536: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 02:54:41,869 EPOCH 4537
2024-02-02 02:54:55,709 Epoch 4537: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 02:54:55,710 EPOCH 4538
2024-02-02 02:55:09,555 Epoch 4538: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 02:55:09,555 EPOCH 4539
2024-02-02 02:55:23,427 Epoch 4539: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 02:55:23,428 EPOCH 4540
2024-02-02 02:55:37,082 Epoch 4540: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 02:55:37,083 EPOCH 4541
2024-02-02 02:55:51,035 Epoch 4541: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 02:55:51,036 EPOCH 4542
2024-02-02 02:56:05,133 Epoch 4542: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 02:56:05,133 EPOCH 4543
2024-02-02 02:56:19,151 Epoch 4543: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:56:19,152 EPOCH 4544
2024-02-02 02:56:33,069 Epoch 4544: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:56:33,070 EPOCH 4545
2024-02-02 02:56:37,605 [Epoch: 4545 Step: 00040900] Batch Recognition Loss:   0.000095 => Gls Tokens per Sec:      933 || Batch Translation Loss:   0.010987 => Txt Tokens per Sec:     2488 || Lr: 0.000050
2024-02-02 02:56:46,992 Epoch 4545: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:56:46,992 EPOCH 4546
2024-02-02 02:57:00,972 Epoch 4546: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:57:00,972 EPOCH 4547
2024-02-02 02:57:14,861 Epoch 4547: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 02:57:14,862 EPOCH 4548
2024-02-02 02:57:28,578 Epoch 4548: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 02:57:28,579 EPOCH 4549
2024-02-02 02:57:42,682 Epoch 4549: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 02:57:42,683 EPOCH 4550
2024-02-02 02:57:56,653 Epoch 4550: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:57:56,653 EPOCH 4551
2024-02-02 02:58:10,620 Epoch 4551: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:58:10,620 EPOCH 4552
2024-02-02 02:58:24,664 Epoch 4552: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:58:24,665 EPOCH 4553
2024-02-02 02:58:38,437 Epoch 4553: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:58:38,438 EPOCH 4554
2024-02-02 02:58:52,318 Epoch 4554: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 02:58:52,318 EPOCH 4555
2024-02-02 02:59:06,224 Epoch 4555: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 02:59:06,224 EPOCH 4556
2024-02-02 02:59:17,037 [Epoch: 4556 Step: 00041000] Batch Recognition Loss:   0.000114 => Gls Tokens per Sec:      510 || Batch Translation Loss:   0.022978 => Txt Tokens per Sec:     1426 || Lr: 0.000050
2024-02-02 02:59:20,325 Epoch 4556: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 02:59:20,326 EPOCH 4557
2024-02-02 02:59:33,673 Epoch 4557: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 02:59:33,673 EPOCH 4558
2024-02-02 02:59:47,946 Epoch 4558: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 02:59:47,947 EPOCH 4559
2024-02-02 03:00:01,963 Epoch 4559: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 03:00:01,964 EPOCH 4560
2024-02-02 03:00:15,848 Epoch 4560: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 03:00:15,849 EPOCH 4561
2024-02-02 03:00:29,828 Epoch 4561: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 03:00:29,829 EPOCH 4562
2024-02-02 03:00:43,697 Epoch 4562: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 03:00:43,698 EPOCH 4563
2024-02-02 03:00:57,660 Epoch 4563: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 03:00:57,661 EPOCH 4564
2024-02-02 03:01:11,560 Epoch 4564: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 03:01:11,561 EPOCH 4565
2024-02-02 03:01:25,423 Epoch 4565: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 03:01:25,424 EPOCH 4566
2024-02-02 03:01:39,231 Epoch 4566: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 03:01:39,232 EPOCH 4567
2024-02-02 03:01:45,876 [Epoch: 4567 Step: 00041100] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     1156 || Batch Translation Loss:   0.024394 => Txt Tokens per Sec:     3112 || Lr: 0.000050
2024-02-02 03:01:53,227 Epoch 4567: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 03:01:53,227 EPOCH 4568
2024-02-02 03:02:07,117 Epoch 4568: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 03:02:07,117 EPOCH 4569
2024-02-02 03:02:21,065 Epoch 4569: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-02 03:02:21,065 EPOCH 4570
2024-02-02 03:02:35,145 Epoch 4570: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.18 
2024-02-02 03:02:35,146 EPOCH 4571
2024-02-02 03:02:48,669 Epoch 4571: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-02 03:02:48,670 EPOCH 4572
2024-02-02 03:03:02,757 Epoch 4572: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.84 
2024-02-02 03:03:02,757 EPOCH 4573
2024-02-02 03:03:16,662 Epoch 4573: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.96 
2024-02-02 03:03:16,662 EPOCH 4574
2024-02-02 03:03:30,595 Epoch 4574: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.83 
2024-02-02 03:03:30,596 EPOCH 4575
2024-02-02 03:03:44,965 Epoch 4575: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-02 03:03:44,966 EPOCH 4576
2024-02-02 03:03:58,823 Epoch 4576: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 03:03:58,824 EPOCH 4577
2024-02-02 03:04:12,532 Epoch 4577: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 03:04:12,533 EPOCH 4578
2024-02-02 03:04:19,560 [Epoch: 4578 Step: 00041200] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     1275 || Batch Translation Loss:   0.014413 => Txt Tokens per Sec:     3363 || Lr: 0.000050
2024-02-02 03:04:26,502 Epoch 4578: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 03:04:26,503 EPOCH 4579
2024-02-02 03:04:40,247 Epoch 4579: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 03:04:40,247 EPOCH 4580
2024-02-02 03:04:54,034 Epoch 4580: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 03:04:54,034 EPOCH 4581
2024-02-02 03:05:07,887 Epoch 4581: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 03:05:07,888 EPOCH 4582
2024-02-02 03:05:21,812 Epoch 4582: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 03:05:21,813 EPOCH 4583
2024-02-02 03:05:35,626 Epoch 4583: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 03:05:35,627 EPOCH 4584
2024-02-02 03:05:49,550 Epoch 4584: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 03:05:49,550 EPOCH 4585
2024-02-02 03:06:03,341 Epoch 4585: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 03:06:03,342 EPOCH 4586
2024-02-02 03:06:17,271 Epoch 4586: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:06:17,271 EPOCH 4587
2024-02-02 03:06:31,294 Epoch 4587: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:06:31,295 EPOCH 4588
2024-02-02 03:06:45,216 Epoch 4588: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 03:06:45,216 EPOCH 4589
2024-02-02 03:06:56,071 [Epoch: 4589 Step: 00041300] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:      862 || Batch Translation Loss:   0.029619 => Txt Tokens per Sec:     2340 || Lr: 0.000050
2024-02-02 03:06:59,167 Epoch 4589: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 03:06:59,168 EPOCH 4590
2024-02-02 03:07:13,069 Epoch 4590: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:07:13,070 EPOCH 4591
2024-02-02 03:07:26,906 Epoch 4591: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:07:26,907 EPOCH 4592
2024-02-02 03:07:40,984 Epoch 4592: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:07:40,984 EPOCH 4593
2024-02-02 03:07:54,982 Epoch 4593: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:07:54,983 EPOCH 4594
2024-02-02 03:08:09,025 Epoch 4594: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:08:09,026 EPOCH 4595
2024-02-02 03:08:22,696 Epoch 4595: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:08:22,696 EPOCH 4596
2024-02-02 03:08:36,431 Epoch 4596: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:08:36,432 EPOCH 4597
2024-02-02 03:08:50,486 Epoch 4597: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:08:50,487 EPOCH 4598
2024-02-02 03:09:04,456 Epoch 4598: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:09:04,456 EPOCH 4599
2024-02-02 03:09:18,428 Epoch 4599: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:09:18,428 EPOCH 4600
2024-02-02 03:09:32,305 [Epoch: 4600 Step: 00041400] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:      766 || Batch Translation Loss:   0.015744 => Txt Tokens per Sec:     2127 || Lr: 0.000050
2024-02-02 03:09:32,305 Epoch 4600: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:09:32,306 EPOCH 4601
2024-02-02 03:09:46,255 Epoch 4601: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 03:09:46,255 EPOCH 4602
2024-02-02 03:10:00,353 Epoch 4602: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:10:00,354 EPOCH 4603
2024-02-02 03:10:13,978 Epoch 4603: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 03:10:13,978 EPOCH 4604
2024-02-02 03:10:27,903 Epoch 4604: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:10:27,904 EPOCH 4605
2024-02-02 03:10:41,873 Epoch 4605: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:10:41,874 EPOCH 4606
2024-02-02 03:10:55,869 Epoch 4606: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:10:55,870 EPOCH 4607
2024-02-02 03:11:09,760 Epoch 4607: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:11:09,760 EPOCH 4608
2024-02-02 03:11:23,683 Epoch 4608: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:11:23,683 EPOCH 4609
2024-02-02 03:11:37,310 Epoch 4609: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:11:37,311 EPOCH 4610
2024-02-02 03:11:51,402 Epoch 4610: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:11:51,403 EPOCH 4611
2024-02-02 03:12:05,264 Epoch 4611: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:12:05,265 EPOCH 4612
2024-02-02 03:12:05,746 [Epoch: 4612 Step: 00041500] Batch Recognition Loss:   0.000099 => Gls Tokens per Sec:     2667 || Batch Translation Loss:   0.012467 => Txt Tokens per Sec:     6687 || Lr: 0.000050
2024-02-02 03:12:19,438 Epoch 4612: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:12:19,439 EPOCH 4613
2024-02-02 03:12:33,503 Epoch 4613: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:12:33,504 EPOCH 4614
2024-02-02 03:12:47,551 Epoch 4614: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 03:12:47,552 EPOCH 4615
2024-02-02 03:13:01,257 Epoch 4615: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:13:01,257 EPOCH 4616
2024-02-02 03:13:15,177 Epoch 4616: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:13:15,177 EPOCH 4617
2024-02-02 03:13:28,950 Epoch 4617: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:13:28,950 EPOCH 4618
2024-02-02 03:13:42,856 Epoch 4618: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:13:42,857 EPOCH 4619
2024-02-02 03:13:56,714 Epoch 4619: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:13:56,715 EPOCH 4620
2024-02-02 03:14:10,294 Epoch 4620: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:14:10,295 EPOCH 4621
2024-02-02 03:14:24,259 Epoch 4621: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:14:24,259 EPOCH 4622
2024-02-02 03:14:38,191 Epoch 4622: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:14:38,191 EPOCH 4623
2024-02-02 03:14:43,440 [Epoch: 4623 Step: 00041600] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:      488 || Batch Translation Loss:   0.019033 => Txt Tokens per Sec:     1588 || Lr: 0.000050
2024-02-02 03:14:52,136 Epoch 4623: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:14:52,137 EPOCH 4624
2024-02-02 03:15:06,027 Epoch 4624: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 03:15:06,028 EPOCH 4625
2024-02-02 03:15:19,966 Epoch 4625: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:15:19,966 EPOCH 4626
2024-02-02 03:15:34,011 Epoch 4626: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:15:34,011 EPOCH 4627
2024-02-02 03:15:47,649 Epoch 4627: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:15:47,649 EPOCH 4628
2024-02-02 03:16:01,635 Epoch 4628: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:16:01,636 EPOCH 4629
2024-02-02 03:16:15,584 Epoch 4629: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:16:15,585 EPOCH 4630
2024-02-02 03:16:29,364 Epoch 4630: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:16:29,365 EPOCH 4631
2024-02-02 03:16:43,432 Epoch 4631: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:16:43,432 EPOCH 4632
2024-02-02 03:16:57,420 Epoch 4632: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:16:57,421 EPOCH 4633
2024-02-02 03:17:11,153 Epoch 4633: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:17:11,154 EPOCH 4634
2024-02-02 03:17:15,483 [Epoch: 4634 Step: 00041700] Batch Recognition Loss:   0.000110 => Gls Tokens per Sec:      887 || Batch Translation Loss:   0.012907 => Txt Tokens per Sec:     2632 || Lr: 0.000050
2024-02-02 03:17:25,077 Epoch 4634: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:17:25,077 EPOCH 4635
2024-02-02 03:17:39,033 Epoch 4635: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 03:17:39,034 EPOCH 4636
2024-02-02 03:17:52,793 Epoch 4636: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:17:52,793 EPOCH 4637
2024-02-02 03:18:06,541 Epoch 4637: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:18:06,542 EPOCH 4638
2024-02-02 03:18:20,597 Epoch 4638: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:18:20,598 EPOCH 4639
2024-02-02 03:18:34,686 Epoch 4639: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:18:34,686 EPOCH 4640
2024-02-02 03:18:48,346 Epoch 4640: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:18:48,346 EPOCH 4641
2024-02-02 03:19:02,330 Epoch 4641: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:19:02,330 EPOCH 4642
2024-02-02 03:19:16,241 Epoch 4642: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:19:16,242 EPOCH 4643
2024-02-02 03:19:30,326 Epoch 4643: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:19:30,326 EPOCH 4644
2024-02-02 03:19:44,026 Epoch 4644: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:19:44,027 EPOCH 4645
2024-02-02 03:19:54,753 [Epoch: 4645 Step: 00041800] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:      394 || Batch Translation Loss:   0.005972 => Txt Tokens per Sec:     1256 || Lr: 0.000050
2024-02-02 03:19:58,078 Epoch 4645: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:19:58,079 EPOCH 4646
2024-02-02 03:20:11,984 Epoch 4646: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:20:11,985 EPOCH 4647
2024-02-02 03:20:25,903 Epoch 4647: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:20:25,903 EPOCH 4648
2024-02-02 03:20:40,048 Epoch 4648: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:20:40,049 EPOCH 4649
2024-02-02 03:20:54,055 Epoch 4649: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:20:54,056 EPOCH 4650
2024-02-02 03:21:07,864 Epoch 4650: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:21:07,864 EPOCH 4651
2024-02-02 03:21:22,034 Epoch 4651: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:21:22,035 EPOCH 4652
2024-02-02 03:21:35,890 Epoch 4652: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:21:35,890 EPOCH 4653
2024-02-02 03:21:49,546 Epoch 4653: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:21:49,546 EPOCH 4654
2024-02-02 03:22:03,438 Epoch 4654: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:22:03,438 EPOCH 4655
2024-02-02 03:22:17,302 Epoch 4655: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:22:17,302 EPOCH 4656
2024-02-02 03:22:24,782 [Epoch: 4656 Step: 00041900] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:      737 || Batch Translation Loss:   0.014591 => Txt Tokens per Sec:     1860 || Lr: 0.000050
2024-02-02 03:22:31,136 Epoch 4656: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:22:31,137 EPOCH 4657
2024-02-02 03:22:45,039 Epoch 4657: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:22:45,040 EPOCH 4658
2024-02-02 03:22:58,918 Epoch 4658: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:22:58,918 EPOCH 4659
2024-02-02 03:23:12,834 Epoch 4659: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:23:12,835 EPOCH 4660
2024-02-02 03:23:26,789 Epoch 4660: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:23:26,789 EPOCH 4661
2024-02-02 03:23:40,702 Epoch 4661: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:23:40,702 EPOCH 4662
2024-02-02 03:23:54,900 Epoch 4662: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:23:54,900 EPOCH 4663
2024-02-02 03:24:08,922 Epoch 4663: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:24:08,922 EPOCH 4664
2024-02-02 03:24:22,721 Epoch 4664: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:24:22,722 EPOCH 4665
2024-02-02 03:24:36,571 Epoch 4665: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:24:36,571 EPOCH 4666
2024-02-02 03:24:50,459 Epoch 4666: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:24:50,460 EPOCH 4667
2024-02-02 03:25:00,607 [Epoch: 4667 Step: 00042000] Batch Recognition Loss:   0.000108 => Gls Tokens per Sec:      669 || Batch Translation Loss:   0.013010 => Txt Tokens per Sec:     1903 || Lr: 0.000050
2024-02-02 03:25:19,900 Validation result at epoch 4667, step    42000: duration: 19.2930s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00033	Translation Loss: 103435.53125	PPL: 31282.01367
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.58	(BLEU-1: 10.44,	BLEU-2: 3.16,	BLEU-3: 1.27,	BLEU-4: 0.58)
	CHRF 16.70	ROUGE 8.83
2024-02-02 03:25:19,901 Logging Recognition and Translation Outputs
2024-02-02 03:25:19,903 ========================================================================================================================
2024-02-02 03:25:19,903 Logging Sequence: 81_8.00
2024-02-02 03:25:19,903 	Gloss Reference :	A B+C+D+E
2024-02-02 03:25:19,903 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 03:25:19,904 	Gloss Alignment :	         
2024-02-02 03:25:19,904 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 03:25:19,905 	Text Reference  :	have been involved in  a     huge    controversy in       connection to    real estate developer amrapali group since     last 7       years
2024-02-02 03:25:19,906 	Text Hypothesis :	**** **** ******** the woman alleged that        danushka on         media if   even   train     and      has   completed the  cricket kit  
2024-02-02 03:25:19,906 	Text Alignment  :	D    D    D        S   S     S       S           S        S          S     S    S      S         S        S     S         S    S       S    
2024-02-02 03:25:19,906 ========================================================================================================================
2024-02-02 03:25:19,906 Logging Sequence: 148_239.00
2024-02-02 03:25:19,906 	Gloss Reference :	A B+C+D+E
2024-02-02 03:25:19,906 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 03:25:19,907 	Gloss Alignment :	         
2024-02-02 03:25:19,907 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 03:25:19,909 	Text Reference  :	**** ***** ******* *** *** ** ** ******** the  ground staff were very  happy and    thanked the  bowler for his kind gesture   
2024-02-02 03:25:19,909 	Text Hypothesis :	test match between rcb and 50 30 november 2022 was    a     huge round of    hardik pandya  away he     is  his **** girlfriend
2024-02-02 03:25:19,909 	Text Alignment  :	I    I     I       I   I   I  I  I        S    S      S     S    S     S     S      S       S    S      S       D    S         
2024-02-02 03:25:19,909 ========================================================================================================================
2024-02-02 03:25:19,909 Logging Sequence: 165_8.00
2024-02-02 03:25:19,909 	Gloss Reference :	A B+C+D+E
2024-02-02 03:25:19,910 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 03:25:19,910 	Gloss Alignment :	         
2024-02-02 03:25:19,910 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 03:25:19,910 	Text Reference  :	however many don't believe in     it  it    varies among  people
2024-02-02 03:25:19,911 	Text Hypothesis :	******* **** ***** they    played 194 balls and    pandya won   
2024-02-02 03:25:19,911 	Text Alignment  :	D       D    D     S       S      S   S     S      S      S     
2024-02-02 03:25:19,911 ========================================================================================================================
2024-02-02 03:25:19,911 Logging Sequence: 93_93.00
2024-02-02 03:25:19,911 	Gloss Reference :	A B+C+D+E
2024-02-02 03:25:19,911 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 03:25:19,911 	Gloss Alignment :	         
2024-02-02 03:25:19,912 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 03:25:19,912 	Text Reference  :	*** ********** rooney was at        the        club   as       well
2024-02-02 03:25:19,912 	Text Hypothesis :	and federation runs   to  england's manchester united football team
2024-02-02 03:25:19,912 	Text Alignment  :	I   I          S      S   S         S          S      S        S   
2024-02-02 03:25:19,913 ========================================================================================================================
2024-02-02 03:25:19,913 Logging Sequence: 96_129.00
2024-02-02 03:25:19,913 	Gloss Reference :	A B+C+D+E
2024-02-02 03:25:19,913 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 03:25:19,913 	Gloss Alignment :	         
2024-02-02 03:25:19,913 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 03:25:19,914 	Text Reference  :	***** *** viewers   were very stressed
2024-02-02 03:25:19,914 	Text Hypothesis :	while the remaining can  win  said    
2024-02-02 03:25:19,914 	Text Alignment  :	I     I   S         S    S    S       
2024-02-02 03:25:19,914 ========================================================================================================================
2024-02-02 03:25:23,850 Epoch 4667: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:25:23,851 EPOCH 4668
2024-02-02 03:25:37,737 Epoch 4668: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:25:37,737 EPOCH 4669
2024-02-02 03:25:51,723 Epoch 4669: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:25:51,724 EPOCH 4670
2024-02-02 03:26:05,749 Epoch 4670: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:26:05,750 EPOCH 4671
2024-02-02 03:26:19,621 Epoch 4671: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:26:19,622 EPOCH 4672
2024-02-02 03:26:33,491 Epoch 4672: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:26:33,492 EPOCH 4673
2024-02-02 03:26:47,394 Epoch 4673: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:26:47,394 EPOCH 4674
2024-02-02 03:27:01,271 Epoch 4674: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 03:27:01,271 EPOCH 4675
2024-02-02 03:27:15,223 Epoch 4675: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 03:27:15,224 EPOCH 4676
2024-02-02 03:27:29,156 Epoch 4676: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 03:27:29,156 EPOCH 4677
2024-02-02 03:27:42,811 Epoch 4677: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 03:27:42,811 EPOCH 4678
2024-02-02 03:27:53,213 [Epoch: 4678 Step: 00042100] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:      776 || Batch Translation Loss:   0.022449 => Txt Tokens per Sec:     2107 || Lr: 0.000050
2024-02-02 03:27:56,949 Epoch 4678: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 03:27:56,949 EPOCH 4679
2024-02-02 03:28:10,653 Epoch 4679: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 03:28:10,653 EPOCH 4680
2024-02-02 03:28:24,624 Epoch 4680: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 03:28:24,624 EPOCH 4681
2024-02-02 03:28:38,461 Epoch 4681: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 03:28:38,461 EPOCH 4682
2024-02-02 03:28:52,360 Epoch 4682: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 03:28:52,360 EPOCH 4683
2024-02-02 03:29:06,298 Epoch 4683: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 03:29:06,299 EPOCH 4684
2024-02-02 03:29:20,309 Epoch 4684: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 03:29:20,309 EPOCH 4685
2024-02-02 03:29:34,337 Epoch 4685: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 03:29:34,338 EPOCH 4686
2024-02-02 03:29:48,392 Epoch 4686: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 03:29:48,392 EPOCH 4687
2024-02-02 03:30:02,218 Epoch 4687: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 03:30:02,218 EPOCH 4688
2024-02-02 03:30:16,033 Epoch 4688: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-02 03:30:16,034 EPOCH 4689
2024-02-02 03:30:26,857 [Epoch: 4689 Step: 00042200] Batch Recognition Loss:   0.000668 => Gls Tokens per Sec:      864 || Batch Translation Loss:   0.092784 => Txt Tokens per Sec:     2350 || Lr: 0.000050
2024-02-02 03:30:30,081 Epoch 4689: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-02 03:30:30,081 EPOCH 4690
2024-02-02 03:30:44,138 Epoch 4690: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 03:30:44,139 EPOCH 4691
2024-02-02 03:30:58,076 Epoch 4691: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 03:30:58,077 EPOCH 4692
2024-02-02 03:31:12,138 Epoch 4692: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-02 03:31:12,139 EPOCH 4693
2024-02-02 03:31:26,002 Epoch 4693: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 03:31:26,002 EPOCH 4694
2024-02-02 03:31:40,019 Epoch 4694: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 03:31:40,020 EPOCH 4695
2024-02-02 03:31:53,909 Epoch 4695: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 03:31:53,910 EPOCH 4696
2024-02-02 03:32:07,720 Epoch 4696: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 03:32:07,721 EPOCH 4697
2024-02-02 03:32:21,599 Epoch 4697: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 03:32:21,599 EPOCH 4698
2024-02-02 03:32:35,486 Epoch 4698: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 03:32:35,486 EPOCH 4699
2024-02-02 03:32:49,123 Epoch 4699: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 03:32:49,124 EPOCH 4700
2024-02-02 03:33:02,843 [Epoch: 4700 Step: 00042300] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:      775 || Batch Translation Loss:   0.028242 => Txt Tokens per Sec:     2151 || Lr: 0.000050
2024-02-02 03:33:02,843 Epoch 4700: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 03:33:02,843 EPOCH 4701
2024-02-02 03:33:16,661 Epoch 4701: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 03:33:16,661 EPOCH 4702
2024-02-02 03:33:30,679 Epoch 4702: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 03:33:30,680 EPOCH 4703
2024-02-02 03:33:44,766 Epoch 4703: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 03:33:44,767 EPOCH 4704
2024-02-02 03:33:58,579 Epoch 4704: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 03:33:58,580 EPOCH 4705
2024-02-02 03:34:12,496 Epoch 4705: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 03:34:12,496 EPOCH 4706
2024-02-02 03:34:26,539 Epoch 4706: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 03:34:26,540 EPOCH 4707
2024-02-02 03:34:40,426 Epoch 4707: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 03:34:40,427 EPOCH 4708
2024-02-02 03:34:54,634 Epoch 4708: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 03:34:54,634 EPOCH 4709
2024-02-02 03:35:08,559 Epoch 4709: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 03:35:08,560 EPOCH 4710
2024-02-02 03:35:22,667 Epoch 4710: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 03:35:22,668 EPOCH 4711
2024-02-02 03:35:36,800 Epoch 4711: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 03:35:36,801 EPOCH 4712
2024-02-02 03:35:37,191 [Epoch: 4712 Step: 00042400] Batch Recognition Loss:   0.000121 => Gls Tokens per Sec:     3282 || Batch Translation Loss:   0.014769 => Txt Tokens per Sec:     8277 || Lr: 0.000050
2024-02-02 03:35:50,528 Epoch 4712: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 03:35:50,529 EPOCH 4713
2024-02-02 03:36:04,469 Epoch 4713: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 03:36:04,469 EPOCH 4714
2024-02-02 03:36:18,375 Epoch 4714: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 03:36:18,375 EPOCH 4715
2024-02-02 03:36:32,258 Epoch 4715: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 03:36:32,258 EPOCH 4716
2024-02-02 03:36:46,451 Epoch 4716: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 03:36:46,452 EPOCH 4717
2024-02-02 03:37:00,316 Epoch 4717: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:37:00,317 EPOCH 4718
2024-02-02 03:37:14,264 Epoch 4718: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:37:14,265 EPOCH 4719
2024-02-02 03:37:28,077 Epoch 4719: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:37:28,078 EPOCH 4720
2024-02-02 03:37:42,133 Epoch 4720: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:37:42,134 EPOCH 4721
2024-02-02 03:37:55,887 Epoch 4721: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 03:37:55,887 EPOCH 4722
2024-02-02 03:38:09,848 Epoch 4722: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:38:09,849 EPOCH 4723
2024-02-02 03:38:10,723 [Epoch: 4723 Step: 00042500] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:     2935 || Batch Translation Loss:   0.016522 => Txt Tokens per Sec:     7709 || Lr: 0.000050
2024-02-02 03:38:23,676 Epoch 4723: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 03:38:23,677 EPOCH 4724
2024-02-02 03:38:37,971 Epoch 4724: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 03:38:37,971 EPOCH 4725
2024-02-02 03:38:51,948 Epoch 4725: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 03:38:51,948 EPOCH 4726
2024-02-02 03:39:05,807 Epoch 4726: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 03:39:05,807 EPOCH 4727
2024-02-02 03:39:19,447 Epoch 4727: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 03:39:19,447 EPOCH 4728
2024-02-02 03:39:33,416 Epoch 4728: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 03:39:33,417 EPOCH 4729
2024-02-02 03:39:47,437 Epoch 4729: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 03:39:47,438 EPOCH 4730
2024-02-02 03:40:01,239 Epoch 4730: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 03:40:01,239 EPOCH 4731
2024-02-02 03:40:15,373 Epoch 4731: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 03:40:15,373 EPOCH 4732
2024-02-02 03:40:29,130 Epoch 4732: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 03:40:29,130 EPOCH 4733
2024-02-02 03:40:42,808 Epoch 4733: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 03:40:42,808 EPOCH 4734
2024-02-02 03:40:46,638 [Epoch: 4734 Step: 00042600] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:      770 || Batch Translation Loss:   0.016054 => Txt Tokens per Sec:     1733 || Lr: 0.000050
2024-02-02 03:40:56,564 Epoch 4734: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 03:40:56,565 EPOCH 4735
2024-02-02 03:41:10,511 Epoch 4735: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 03:41:10,512 EPOCH 4736
2024-02-02 03:41:24,292 Epoch 4736: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 03:41:24,293 EPOCH 4737
2024-02-02 03:41:38,101 Epoch 4737: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 03:41:38,102 EPOCH 4738
2024-02-02 03:41:52,155 Epoch 4738: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 03:41:52,155 EPOCH 4739
2024-02-02 03:42:06,112 Epoch 4739: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 03:42:06,113 EPOCH 4740
2024-02-02 03:42:19,863 Epoch 4740: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 03:42:19,864 EPOCH 4741
2024-02-02 03:42:33,865 Epoch 4741: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 03:42:33,865 EPOCH 4742
2024-02-02 03:42:47,634 Epoch 4742: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 03:42:47,634 EPOCH 4743
2024-02-02 03:43:01,448 Epoch 4743: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 03:43:01,449 EPOCH 4744
2024-02-02 03:43:15,305 Epoch 4744: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 03:43:15,305 EPOCH 4745
2024-02-02 03:43:20,052 [Epoch: 4745 Step: 00042700] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:     1079 || Batch Translation Loss:   0.016616 => Txt Tokens per Sec:     2827 || Lr: 0.000050
2024-02-02 03:43:29,138 Epoch 4745: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 03:43:29,139 EPOCH 4746
2024-02-02 03:43:43,339 Epoch 4746: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 03:43:43,340 EPOCH 4747
2024-02-02 03:43:57,194 Epoch 4747: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 03:43:57,195 EPOCH 4748
2024-02-02 03:44:10,977 Epoch 4748: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 03:44:10,978 EPOCH 4749
2024-02-02 03:44:25,073 Epoch 4749: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:44:25,073 EPOCH 4750
2024-02-02 03:44:38,811 Epoch 4750: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:44:38,811 EPOCH 4751
2024-02-02 03:44:52,917 Epoch 4751: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:44:52,917 EPOCH 4752
2024-02-02 03:45:06,637 Epoch 4752: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:45:06,637 EPOCH 4753
2024-02-02 03:45:20,865 Epoch 4753: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:45:20,865 EPOCH 4754
2024-02-02 03:45:34,633 Epoch 4754: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:45:34,633 EPOCH 4755
2024-02-02 03:45:48,565 Epoch 4755: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:45:48,566 EPOCH 4756
2024-02-02 03:45:58,181 [Epoch: 4756 Step: 00042800] Batch Recognition Loss:   0.000102 => Gls Tokens per Sec:      573 || Batch Translation Loss:   0.010847 => Txt Tokens per Sec:     1558 || Lr: 0.000050
2024-02-02 03:46:02,827 Epoch 4756: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:46:02,828 EPOCH 4757
2024-02-02 03:46:16,630 Epoch 4757: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:46:16,630 EPOCH 4758
2024-02-02 03:46:30,530 Epoch 4758: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:46:30,530 EPOCH 4759
2024-02-02 03:46:44,534 Epoch 4759: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:46:44,534 EPOCH 4760
2024-02-02 03:46:58,350 Epoch 4760: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:46:58,351 EPOCH 4761
2024-02-02 03:47:12,169 Epoch 4761: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:47:12,170 EPOCH 4762
2024-02-02 03:47:26,118 Epoch 4762: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:47:26,118 EPOCH 4763
2024-02-02 03:47:40,152 Epoch 4763: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:47:40,153 EPOCH 4764
2024-02-02 03:47:54,104 Epoch 4764: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:47:54,105 EPOCH 4765
2024-02-02 03:48:08,166 Epoch 4765: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:48:08,167 EPOCH 4766
2024-02-02 03:48:22,213 Epoch 4766: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:48:22,213 EPOCH 4767
2024-02-02 03:48:29,436 [Epoch: 4767 Step: 00042900] Batch Recognition Loss:   0.000098 => Gls Tokens per Sec:     1063 || Batch Translation Loss:   0.011240 => Txt Tokens per Sec:     2918 || Lr: 0.000050
2024-02-02 03:48:36,121 Epoch 4767: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:48:36,122 EPOCH 4768
2024-02-02 03:48:49,767 Epoch 4768: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:48:49,767 EPOCH 4769
2024-02-02 03:49:04,025 Epoch 4769: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:49:04,026 EPOCH 4770
2024-02-02 03:49:17,762 Epoch 4770: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:49:17,763 EPOCH 4771
2024-02-02 03:49:31,677 Epoch 4771: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:49:31,677 EPOCH 4772
2024-02-02 03:49:45,535 Epoch 4772: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:49:45,535 EPOCH 4773
2024-02-02 03:49:59,776 Epoch 4773: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:49:59,776 EPOCH 4774
2024-02-02 03:50:13,887 Epoch 4774: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:50:13,887 EPOCH 4775
2024-02-02 03:50:27,395 Epoch 4775: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:50:27,396 EPOCH 4776
2024-02-02 03:50:41,238 Epoch 4776: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:50:41,238 EPOCH 4777
2024-02-02 03:50:54,916 Epoch 4777: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:50:54,917 EPOCH 4778
2024-02-02 03:51:04,891 [Epoch: 4778 Step: 00043000] Batch Recognition Loss:   0.000107 => Gls Tokens per Sec:      898 || Batch Translation Loss:   0.013332 => Txt Tokens per Sec:     2443 || Lr: 0.000050
2024-02-02 03:51:08,775 Epoch 4778: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:51:08,775 EPOCH 4779
2024-02-02 03:51:22,834 Epoch 4779: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:51:22,834 EPOCH 4780
2024-02-02 03:51:36,038 Epoch 4780: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 03:51:36,038 EPOCH 4781
2024-02-02 03:51:49,945 Epoch 4781: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 03:51:49,946 EPOCH 4782
2024-02-02 03:52:03,677 Epoch 4782: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 03:52:03,678 EPOCH 4783
2024-02-02 03:52:17,963 Epoch 4783: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 03:52:17,964 EPOCH 4784
2024-02-02 03:52:31,991 Epoch 4784: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 03:52:31,991 EPOCH 4785
2024-02-02 03:52:45,604 Epoch 4785: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 03:52:45,604 EPOCH 4786
2024-02-02 03:52:59,794 Epoch 4786: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 03:52:59,795 EPOCH 4787
2024-02-02 03:53:13,791 Epoch 4787: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 03:53:13,791 EPOCH 4788
2024-02-02 03:53:27,587 Epoch 4788: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 03:53:27,587 EPOCH 4789
2024-02-02 03:53:38,452 [Epoch: 4789 Step: 00043100] Batch Recognition Loss:   0.000131 => Gls Tokens per Sec:      861 || Batch Translation Loss:   0.015477 => Txt Tokens per Sec:     2339 || Lr: 0.000050
2024-02-02 03:53:41,608 Epoch 4789: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:53:41,608 EPOCH 4790
2024-02-02 03:53:55,370 Epoch 4790: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:53:55,371 EPOCH 4791
2024-02-02 03:54:09,301 Epoch 4791: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:54:09,302 EPOCH 4792
2024-02-02 03:54:23,219 Epoch 4792: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:54:23,219 EPOCH 4793
2024-02-02 03:54:37,094 Epoch 4793: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:54:37,094 EPOCH 4794
2024-02-02 03:54:50,771 Epoch 4794: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 03:54:50,772 EPOCH 4795
2024-02-02 03:55:04,749 Epoch 4795: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 03:55:04,750 EPOCH 4796
2024-02-02 03:55:18,657 Epoch 4796: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:55:18,658 EPOCH 4797
2024-02-02 03:55:32,254 Epoch 4797: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 03:55:32,255 EPOCH 4798
2024-02-02 03:55:46,094 Epoch 4798: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 03:55:46,094 EPOCH 4799
2024-02-02 03:56:00,022 Epoch 4799: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 03:56:00,023 EPOCH 4800
2024-02-02 03:56:13,867 [Epoch: 4800 Step: 00043200] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:      768 || Batch Translation Loss:   0.008650 => Txt Tokens per Sec:     2132 || Lr: 0.000050
2024-02-02 03:56:13,868 Epoch 4800: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 03:56:13,868 EPOCH 4801
2024-02-02 03:56:27,810 Epoch 4801: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 03:56:27,811 EPOCH 4802
2024-02-02 03:56:41,681 Epoch 4802: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 03:56:41,682 EPOCH 4803
2024-02-02 03:56:55,502 Epoch 4803: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 03:56:55,503 EPOCH 4804
2024-02-02 03:57:09,415 Epoch 4804: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 03:57:09,416 EPOCH 4805
2024-02-02 03:57:23,472 Epoch 4805: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 03:57:23,473 EPOCH 4806
2024-02-02 03:57:37,237 Epoch 4806: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 03:57:37,238 EPOCH 4807
2024-02-02 03:57:51,763 Epoch 4807: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 03:57:51,764 EPOCH 4808
2024-02-02 03:58:05,653 Epoch 4808: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 03:58:05,653 EPOCH 4809
2024-02-02 03:58:19,534 Epoch 4809: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 03:58:19,535 EPOCH 4810
2024-02-02 03:58:33,287 Epoch 4810: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 03:58:33,287 EPOCH 4811
2024-02-02 03:58:46,807 Epoch 4811: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 03:58:46,808 EPOCH 4812
2024-02-02 03:58:47,345 [Epoch: 4812 Step: 00043300] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:     2385 || Batch Translation Loss:   0.021480 => Txt Tokens per Sec:     7019 || Lr: 0.000050
2024-02-02 03:59:00,398 Epoch 4812: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 03:59:00,399 EPOCH 4813
2024-02-02 03:59:14,498 Epoch 4813: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 03:59:14,499 EPOCH 4814
2024-02-02 03:59:28,468 Epoch 4814: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 03:59:28,468 EPOCH 4815
2024-02-02 03:59:42,687 Epoch 4815: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 03:59:42,688 EPOCH 4816
2024-02-02 03:59:56,747 Epoch 4816: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 03:59:56,748 EPOCH 4817
2024-02-02 04:00:10,634 Epoch 4817: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 04:00:10,634 EPOCH 4818
2024-02-02 04:00:24,448 Epoch 4818: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 04:00:24,449 EPOCH 4819
2024-02-02 04:00:38,521 Epoch 4819: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 04:00:38,521 EPOCH 4820
2024-02-02 04:00:52,746 Epoch 4820: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 04:00:52,746 EPOCH 4821
2024-02-02 04:01:06,745 Epoch 4821: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 04:01:06,746 EPOCH 4822
2024-02-02 04:01:20,581 Epoch 4822: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 04:01:20,582 EPOCH 4823
2024-02-02 04:01:26,944 [Epoch: 4823 Step: 00043400] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:      262 || Batch Translation Loss:   0.008167 => Txt Tokens per Sec:      862 || Lr: 0.000050
2024-02-02 04:01:34,456 Epoch 4823: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 04:01:34,456 EPOCH 4824
2024-02-02 04:01:48,184 Epoch 4824: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 04:01:48,185 EPOCH 4825
2024-02-02 04:02:02,110 Epoch 4825: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 04:02:02,111 EPOCH 4826
2024-02-02 04:02:16,034 Epoch 4826: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 04:02:16,034 EPOCH 4827
2024-02-02 04:02:30,139 Epoch 4827: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 04:02:30,140 EPOCH 4828
2024-02-02 04:02:44,066 Epoch 4828: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 04:02:44,067 EPOCH 4829
2024-02-02 04:02:57,957 Epoch 4829: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 04:02:57,957 EPOCH 4830
2024-02-02 04:03:11,938 Epoch 4830: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 04:03:11,938 EPOCH 4831
2024-02-02 04:03:25,924 Epoch 4831: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 04:03:25,925 EPOCH 4832
2024-02-02 04:03:39,862 Epoch 4832: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 04:03:39,862 EPOCH 4833
2024-02-02 04:03:53,836 Epoch 4833: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 04:03:53,837 EPOCH 4834
2024-02-02 04:03:58,260 [Epoch: 4834 Step: 00043500] Batch Recognition Loss:   0.000116 => Gls Tokens per Sec:      667 || Batch Translation Loss:   0.014545 => Txt Tokens per Sec:     1961 || Lr: 0.000050
2024-02-02 04:04:07,751 Epoch 4834: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 04:04:07,752 EPOCH 4835
2024-02-02 04:04:21,760 Epoch 4835: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 04:04:21,761 EPOCH 4836
2024-02-02 04:04:35,675 Epoch 4836: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 04:04:35,676 EPOCH 4837
2024-02-02 04:04:49,660 Epoch 4837: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 04:04:49,661 EPOCH 4838
2024-02-02 04:05:03,669 Epoch 4838: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 04:05:03,670 EPOCH 4839
2024-02-02 04:05:17,578 Epoch 4839: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 04:05:17,578 EPOCH 4840
2024-02-02 04:05:31,426 Epoch 4840: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 04:05:31,427 EPOCH 4841
2024-02-02 04:05:45,428 Epoch 4841: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 04:05:45,429 EPOCH 4842
2024-02-02 04:05:59,352 Epoch 4842: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 04:05:59,352 EPOCH 4843
2024-02-02 04:06:13,362 Epoch 4843: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 04:06:13,362 EPOCH 4844
2024-02-02 04:06:27,349 Epoch 4844: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 04:06:27,349 EPOCH 4845
2024-02-02 04:06:28,796 [Epoch: 4845 Step: 00043600] Batch Recognition Loss:   0.000112 => Gls Tokens per Sec:     3541 || Batch Translation Loss:   0.014658 => Txt Tokens per Sec:     8409 || Lr: 0.000050
2024-02-02 04:06:41,249 Epoch 4845: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 04:06:41,250 EPOCH 4846
2024-02-02 04:06:55,085 Epoch 4846: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 04:06:55,086 EPOCH 4847
2024-02-02 04:07:08,861 Epoch 4847: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 04:07:08,862 EPOCH 4848
2024-02-02 04:07:22,723 Epoch 4848: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 04:07:22,723 EPOCH 4849
2024-02-02 04:07:36,603 Epoch 4849: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 04:07:36,604 EPOCH 4850
2024-02-02 04:07:50,697 Epoch 4850: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 04:07:50,697 EPOCH 4851
2024-02-02 04:08:04,521 Epoch 4851: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 04:08:04,521 EPOCH 4852
2024-02-02 04:08:17,994 Epoch 4852: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 04:08:17,994 EPOCH 4853
2024-02-02 04:08:32,106 Epoch 4853: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 04:08:32,106 EPOCH 4854
2024-02-02 04:08:45,968 Epoch 4854: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 04:08:45,968 EPOCH 4855
2024-02-02 04:09:00,003 Epoch 4855: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 04:09:00,003 EPOCH 4856
2024-02-02 04:09:09,548 [Epoch: 4856 Step: 00043700] Batch Recognition Loss:   0.000118 => Gls Tokens per Sec:      577 || Batch Translation Loss:   0.015084 => Txt Tokens per Sec:     1699 || Lr: 0.000050
2024-02-02 04:09:13,866 Epoch 4856: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 04:09:13,867 EPOCH 4857
2024-02-02 04:09:27,603 Epoch 4857: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 04:09:27,603 EPOCH 4858
2024-02-02 04:09:41,372 Epoch 4858: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 04:09:41,372 EPOCH 4859
2024-02-02 04:09:55,345 Epoch 4859: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 04:09:55,346 EPOCH 4860
2024-02-02 04:10:09,188 Epoch 4860: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 04:10:09,188 EPOCH 4861
2024-02-02 04:10:23,169 Epoch 4861: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 04:10:23,170 EPOCH 4862
2024-02-02 04:10:37,004 Epoch 4862: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 04:10:37,005 EPOCH 4863
2024-02-02 04:10:50,847 Epoch 4863: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 04:10:50,847 EPOCH 4864
2024-02-02 04:11:04,659 Epoch 4864: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 04:11:04,659 EPOCH 4865
2024-02-02 04:11:18,636 Epoch 4865: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 04:11:18,637 EPOCH 4866
2024-02-02 04:11:32,445 Epoch 4866: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 04:11:32,446 EPOCH 4867
2024-02-02 04:11:41,074 [Epoch: 4867 Step: 00043800] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:      890 || Batch Translation Loss:   0.019228 => Txt Tokens per Sec:     2433 || Lr: 0.000050
2024-02-02 04:11:46,561 Epoch 4867: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 04:11:46,562 EPOCH 4868
2024-02-02 04:12:00,434 Epoch 4868: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 04:12:00,435 EPOCH 4869
2024-02-02 04:12:14,646 Epoch 4869: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 04:12:14,647 EPOCH 4870
2024-02-02 04:12:28,456 Epoch 4870: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 04:12:28,457 EPOCH 4871
2024-02-02 04:12:42,322 Epoch 4871: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 04:12:42,323 EPOCH 4872
2024-02-02 04:12:56,497 Epoch 4872: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 04:12:56,497 EPOCH 4873
2024-02-02 04:13:10,963 Epoch 4873: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 04:13:10,964 EPOCH 4874
2024-02-02 04:13:25,011 Epoch 4874: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 04:13:25,011 EPOCH 4875
2024-02-02 04:13:38,680 Epoch 4875: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 04:13:38,680 EPOCH 4876
2024-02-02 04:13:52,502 Epoch 4876: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 04:13:52,502 EPOCH 4877
2024-02-02 04:14:06,588 Epoch 4877: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 04:14:06,588 EPOCH 4878
2024-02-02 04:14:15,022 [Epoch: 4878 Step: 00043900] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:      957 || Batch Translation Loss:   0.019989 => Txt Tokens per Sec:     2510 || Lr: 0.000050
2024-02-02 04:14:20,345 Epoch 4878: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 04:14:20,345 EPOCH 4879
2024-02-02 04:14:34,315 Epoch 4879: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 04:14:34,316 EPOCH 4880
2024-02-02 04:14:47,987 Epoch 4880: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 04:14:47,987 EPOCH 4881
2024-02-02 04:15:02,038 Epoch 4881: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 04:15:02,039 EPOCH 4882
2024-02-02 04:15:15,740 Epoch 4882: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 04:15:15,741 EPOCH 4883
2024-02-02 04:15:29,829 Epoch 4883: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 04:15:29,829 EPOCH 4884
2024-02-02 04:15:43,648 Epoch 4884: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 04:15:43,649 EPOCH 4885
2024-02-02 04:15:57,731 Epoch 4885: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 04:15:57,731 EPOCH 4886
2024-02-02 04:16:11,660 Epoch 4886: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 04:16:11,660 EPOCH 4887
2024-02-02 04:16:25,688 Epoch 4887: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 04:16:25,689 EPOCH 4888
2024-02-02 04:16:39,442 Epoch 4888: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 04:16:39,442 EPOCH 4889
2024-02-02 04:16:53,242 [Epoch: 4889 Step: 00044000] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:      678 || Batch Translation Loss:   0.022601 => Txt Tokens per Sec:     1908 || Lr: 0.000050
2024-02-02 04:17:12,824 Validation result at epoch 4889, step    44000: duration: 19.5806s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00061	Translation Loss: 105562.34375	PPL: 38701.32812
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.53	(BLEU-1: 10.39,	BLEU-2: 3.00,	BLEU-3: 1.11,	BLEU-4: 0.53)
	CHRF 16.51	ROUGE 8.97
2024-02-02 04:17:12,826 Logging Recognition and Translation Outputs
2024-02-02 04:17:12,826 ========================================================================================================================
2024-02-02 04:17:12,827 Logging Sequence: 117_29.00
2024-02-02 04:17:12,827 	Gloss Reference :	A B+C+D+E
2024-02-02 04:17:12,828 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 04:17:12,828 	Gloss Alignment :	         
2024-02-02 04:17:12,828 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 04:17:12,829 	Text Reference  :	however england was unable to reach the target they   were all   out  lost   by  66     runs
2024-02-02 04:17:12,829 	Text Hypothesis :	******* ******* *** ****** ** ***** *** ****** krunal and  rahul took charge and scored 3175
2024-02-02 04:17:12,829 	Text Alignment  :	D       D       D   D      D  D     D   D      S      S    S     S    S      S   S      S   
2024-02-02 04:17:12,829 ========================================================================================================================
2024-02-02 04:17:12,829 Logging Sequence: 84_176.00
2024-02-02 04:17:12,829 	Gloss Reference :	A B+C+D+E
2024-02-02 04:17:12,829 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 04:17:12,830 	Gloss Alignment :	         
2024-02-02 04:17:12,830 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 04:17:12,831 	Text Reference  :	**** germany's nancy faeser who attended the game    in  doha     against japan said
2024-02-02 04:17:12,831 	Text Hypothesis :	when yuvraj    singh into   one day      pm  support and arshdeep with    the   team
2024-02-02 04:17:12,831 	Text Alignment  :	I    S         S     S      S   S        S   S       S   S        S       S     S   
2024-02-02 04:17:12,831 ========================================================================================================================
2024-02-02 04:17:12,831 Logging Sequence: 172_98.00
2024-02-02 04:17:12,831 	Gloss Reference :	A B+C+D+E
2024-02-02 04:17:12,832 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 04:17:12,832 	Gloss Alignment :	         
2024-02-02 04:17:12,832 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 04:17:12,833 	Text Reference  :	******* since    700   pm  it       kept raining the intensity plunged around 915 pm        
2024-02-02 04:17:12,833 	Text Hypothesis :	however arshdeep match the narendra modi stadium the ********* final   before the tournament
2024-02-02 04:17:12,833 	Text Alignment  :	I       S        S     S   S        S    S           D         S       S      S   S         
2024-02-02 04:17:12,833 ========================================================================================================================
2024-02-02 04:17:12,833 Logging Sequence: 135_92.00
2024-02-02 04:17:12,833 	Gloss Reference :	A B+C+D+E
2024-02-02 04:17:12,833 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 04:17:12,834 	Gloss Alignment :	         
2024-02-02 04:17:12,834 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 04:17:12,835 	Text Reference  :	she wrote that half had already been  raised by   the     family's online fundraiser
2024-02-02 04:17:12,835 	Text Hypothesis :	*** ***** **** **** it  is      world cup    that because of       her    sample    
2024-02-02 04:17:12,835 	Text Alignment  :	D   D     D    D    S   S       S     S      S    S       S        S      S         
2024-02-02 04:17:12,835 ========================================================================================================================
2024-02-02 04:17:12,835 Logging Sequence: 180_332.00
2024-02-02 04:17:12,835 	Gloss Reference :	A B+C+D+E
2024-02-02 04:17:12,835 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 04:17:12,835 	Gloss Alignment :	         
2024-02-02 04:17:12,836 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 04:17:12,836 	Text Reference  :	did i eat roti made of shilajit that   i     got  energy to assault so     many girls
2024-02-02 04:17:12,837 	Text Hypothesis :	*** * *** **** **** ** for      taking india gave birth  to ******* donate as   well 
2024-02-02 04:17:12,837 	Text Alignment  :	D   D D   D    D    D  S        S      S     S    S         D       S      S    S    
2024-02-02 04:17:12,837 ========================================================================================================================
2024-02-02 04:17:13,352 Epoch 4889: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 04:17:13,352 EPOCH 4890
2024-02-02 04:17:27,206 Epoch 4890: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 04:17:27,206 EPOCH 4891
2024-02-02 04:17:41,111 Epoch 4891: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 04:17:41,111 EPOCH 4892
2024-02-02 04:17:55,166 Epoch 4892: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 04:17:55,167 EPOCH 4893
2024-02-02 04:18:09,046 Epoch 4893: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 04:18:09,046 EPOCH 4894
2024-02-02 04:18:22,787 Epoch 4894: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 04:18:22,788 EPOCH 4895
2024-02-02 04:18:36,984 Epoch 4895: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 04:18:36,984 EPOCH 4896
2024-02-02 04:18:50,796 Epoch 4896: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 04:18:50,796 EPOCH 4897
2024-02-02 04:19:04,691 Epoch 4897: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 04:19:04,692 EPOCH 4898
2024-02-02 04:19:18,131 Epoch 4898: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 04:19:18,132 EPOCH 4899
2024-02-02 04:19:32,029 Epoch 4899: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 04:19:32,030 EPOCH 4900
2024-02-02 04:19:46,063 [Epoch: 4900 Step: 00044100] Batch Recognition Loss:   0.000106 => Gls Tokens per Sec:      757 || Batch Translation Loss:   0.011832 => Txt Tokens per Sec:     2103 || Lr: 0.000050
2024-02-02 04:19:46,064 Epoch 4900: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 04:19:46,064 EPOCH 4901
2024-02-02 04:20:00,171 Epoch 4901: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 04:20:00,172 EPOCH 4902
2024-02-02 04:20:14,069 Epoch 4902: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 04:20:14,069 EPOCH 4903
2024-02-02 04:20:27,868 Epoch 4903: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 04:20:27,868 EPOCH 4904
2024-02-02 04:20:41,725 Epoch 4904: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 04:20:41,726 EPOCH 4905
2024-02-02 04:20:55,533 Epoch 4905: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 04:20:55,533 EPOCH 4906
2024-02-02 04:21:09,338 Epoch 4906: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 04:21:09,339 EPOCH 4907
2024-02-02 04:21:23,358 Epoch 4907: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 04:21:23,359 EPOCH 4908
2024-02-02 04:21:37,452 Epoch 4908: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 04:21:37,452 EPOCH 4909
2024-02-02 04:21:51,215 Epoch 4909: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 04:21:51,216 EPOCH 4910
2024-02-02 04:22:05,369 Epoch 4910: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 04:22:05,369 EPOCH 4911
2024-02-02 04:22:19,135 Epoch 4911: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 04:22:19,136 EPOCH 4912
2024-02-02 04:22:23,002 [Epoch: 4912 Step: 00044200] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:      331 || Batch Translation Loss:   0.017675 => Txt Tokens per Sec:     1162 || Lr: 0.000050
2024-02-02 04:22:33,217 Epoch 4912: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 04:22:33,218 EPOCH 4913
2024-02-02 04:22:46,948 Epoch 4913: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 04:22:46,949 EPOCH 4914
2024-02-02 04:23:01,029 Epoch 4914: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 04:23:01,030 EPOCH 4915
2024-02-02 04:23:15,058 Epoch 4915: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 04:23:15,059 EPOCH 4916
2024-02-02 04:23:28,924 Epoch 4916: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 04:23:28,925 EPOCH 4917
2024-02-02 04:23:42,909 Epoch 4917: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 04:23:42,910 EPOCH 4918
2024-02-02 04:23:56,834 Epoch 4918: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 04:23:56,835 EPOCH 4919
2024-02-02 04:24:10,826 Epoch 4919: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 04:24:10,826 EPOCH 4920
2024-02-02 04:24:24,691 Epoch 4920: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 04:24:24,692 EPOCH 4921
2024-02-02 04:24:38,685 Epoch 4921: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 04:24:38,686 EPOCH 4922
2024-02-02 04:24:52,366 Epoch 4922: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 04:24:52,367 EPOCH 4923
2024-02-02 04:24:53,158 [Epoch: 4923 Step: 00044300] Batch Recognition Loss:   0.000103 => Gls Tokens per Sec:     3245 || Batch Translation Loss:   0.010620 => Txt Tokens per Sec:     7359 || Lr: 0.000050
2024-02-02 04:25:06,357 Epoch 4923: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 04:25:06,358 EPOCH 4924
2024-02-02 04:25:20,336 Epoch 4924: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 04:25:20,336 EPOCH 4925
2024-02-02 04:25:34,066 Epoch 4925: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 04:25:34,067 EPOCH 4926
2024-02-02 04:25:47,914 Epoch 4926: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 04:25:47,914 EPOCH 4927
2024-02-02 04:26:01,903 Epoch 4927: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 04:26:01,903 EPOCH 4928
2024-02-02 04:26:15,785 Epoch 4928: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 04:26:15,785 EPOCH 4929
2024-02-02 04:26:29,541 Epoch 4929: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 04:26:29,542 EPOCH 4930
2024-02-02 04:26:43,561 Epoch 4930: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 04:26:43,561 EPOCH 4931
2024-02-02 04:26:57,396 Epoch 4931: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 04:26:57,397 EPOCH 4932
2024-02-02 04:27:11,463 Epoch 4932: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 04:27:11,463 EPOCH 4933
2024-02-02 04:27:25,312 Epoch 4933: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 04:27:25,312 EPOCH 4934
2024-02-02 04:27:29,068 [Epoch: 4934 Step: 00044400] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:     1023 || Batch Translation Loss:   0.019868 => Txt Tokens per Sec:     2644 || Lr: 0.000050
2024-02-02 04:27:39,210 Epoch 4934: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 04:27:39,211 EPOCH 4935
2024-02-02 04:27:52,948 Epoch 4935: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 04:27:52,949 EPOCH 4936
2024-02-02 04:28:07,136 Epoch 4936: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 04:28:07,137 EPOCH 4937
2024-02-02 04:28:21,160 Epoch 4937: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 04:28:21,161 EPOCH 4938
2024-02-02 04:28:35,236 Epoch 4938: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 04:28:35,236 EPOCH 4939
2024-02-02 04:28:49,245 Epoch 4939: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 04:28:49,246 EPOCH 4940
2024-02-02 04:29:03,007 Epoch 4940: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 04:29:03,008 EPOCH 4941
2024-02-02 04:29:16,826 Epoch 4941: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 04:29:16,826 EPOCH 4942
2024-02-02 04:29:30,608 Epoch 4942: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 04:29:30,609 EPOCH 4943
2024-02-02 04:29:44,663 Epoch 4943: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 04:29:44,664 EPOCH 4944
2024-02-02 04:29:58,538 Epoch 4944: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 04:29:58,539 EPOCH 4945
2024-02-02 04:30:04,824 [Epoch: 4945 Step: 00044500] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:      815 || Batch Translation Loss:   0.017808 => Txt Tokens per Sec:     2394 || Lr: 0.000050
2024-02-02 04:30:12,435 Epoch 4945: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 04:30:12,436 EPOCH 4946
2024-02-02 04:30:26,171 Epoch 4946: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 04:30:26,172 EPOCH 4947
2024-02-02 04:30:40,142 Epoch 4947: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 04:30:40,143 EPOCH 4948
2024-02-02 04:30:54,054 Epoch 4948: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 04:30:54,055 EPOCH 4949
2024-02-02 04:31:08,040 Epoch 4949: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 04:31:08,041 EPOCH 4950
2024-02-02 04:31:21,811 Epoch 4950: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 04:31:21,812 EPOCH 4951
2024-02-02 04:31:35,634 Epoch 4951: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 04:31:35,634 EPOCH 4952
2024-02-02 04:31:49,637 Epoch 4952: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 04:31:49,638 EPOCH 4953
2024-02-02 04:32:03,369 Epoch 4953: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 04:32:03,370 EPOCH 4954
2024-02-02 04:32:17,394 Epoch 4954: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 04:32:17,395 EPOCH 4955
2024-02-02 04:32:31,235 Epoch 4955: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 04:32:31,236 EPOCH 4956
2024-02-02 04:32:37,832 [Epoch: 4956 Step: 00044600] Batch Recognition Loss:   0.000096 => Gls Tokens per Sec:      835 || Batch Translation Loss:   0.009896 => Txt Tokens per Sec:     2383 || Lr: 0.000050
2024-02-02 04:32:45,236 Epoch 4956: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 04:32:45,236 EPOCH 4957
2024-02-02 04:32:58,930 Epoch 4957: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 04:32:58,930 EPOCH 4958
2024-02-02 04:33:13,016 Epoch 4958: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 04:33:13,017 EPOCH 4959
2024-02-02 04:33:26,903 Epoch 4959: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 04:33:26,903 EPOCH 4960
2024-02-02 04:33:40,835 Epoch 4960: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 04:33:40,835 EPOCH 4961
2024-02-02 04:33:54,955 Epoch 4961: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 04:33:54,955 EPOCH 4962
2024-02-02 04:34:08,753 Epoch 4962: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 04:34:08,754 EPOCH 4963
2024-02-02 04:34:22,465 Epoch 4963: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 04:34:22,465 EPOCH 4964
2024-02-02 04:34:36,188 Epoch 4964: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 04:34:36,189 EPOCH 4965
2024-02-02 04:34:50,200 Epoch 4965: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 04:34:50,201 EPOCH 4966
2024-02-02 04:35:04,195 Epoch 4966: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 04:35:04,196 EPOCH 4967
2024-02-02 04:35:14,294 [Epoch: 4967 Step: 00044700] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:      672 || Batch Translation Loss:   0.018978 => Txt Tokens per Sec:     1999 || Lr: 0.000050
2024-02-02 04:35:18,075 Epoch 4967: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 04:35:18,076 EPOCH 4968
2024-02-02 04:35:31,969 Epoch 4968: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 04:35:31,970 EPOCH 4969
2024-02-02 04:35:45,844 Epoch 4969: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 04:35:45,844 EPOCH 4970
2024-02-02 04:35:59,719 Epoch 4970: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 04:35:59,719 EPOCH 4971
2024-02-02 04:36:13,458 Epoch 4971: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 04:36:13,459 EPOCH 4972
2024-02-02 04:36:27,135 Epoch 4972: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 04:36:27,136 EPOCH 4973
2024-02-02 04:36:40,826 Epoch 4973: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 04:36:40,827 EPOCH 4974
2024-02-02 04:36:54,607 Epoch 4974: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 04:36:54,608 EPOCH 4975
2024-02-02 04:37:08,642 Epoch 4975: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 04:37:08,643 EPOCH 4976
2024-02-02 04:37:22,648 Epoch 4976: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 04:37:22,649 EPOCH 4977
2024-02-02 04:37:36,581 Epoch 4977: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 04:37:36,582 EPOCH 4978
2024-02-02 04:37:45,714 [Epoch: 4978 Step: 00044800] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:      884 || Batch Translation Loss:   0.026282 => Txt Tokens per Sec:     2358 || Lr: 0.000050
2024-02-02 04:37:50,496 Epoch 4978: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 04:37:50,496 EPOCH 4979
2024-02-02 04:38:04,539 Epoch 4979: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 04:38:04,540 EPOCH 4980
2024-02-02 04:38:18,735 Epoch 4980: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 04:38:18,735 EPOCH 4981
2024-02-02 04:38:32,658 Epoch 4981: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 04:38:32,659 EPOCH 4982
2024-02-02 04:38:46,624 Epoch 4982: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 04:38:46,625 EPOCH 4983
2024-02-02 04:39:00,237 Epoch 4983: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 04:39:00,237 EPOCH 4984
2024-02-02 04:39:14,329 Epoch 4984: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 04:39:14,330 EPOCH 4985
2024-02-02 04:39:28,478 Epoch 4985: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 04:39:28,479 EPOCH 4986
2024-02-02 04:39:42,474 Epoch 4986: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 04:39:42,475 EPOCH 4987
2024-02-02 04:39:56,305 Epoch 4987: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 04:39:56,305 EPOCH 4988
2024-02-02 04:40:10,323 Epoch 4988: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 04:40:10,323 EPOCH 4989
2024-02-02 04:40:22,471 [Epoch: 4989 Step: 00044900] Batch Recognition Loss:   0.000284 => Gls Tokens per Sec:      770 || Batch Translation Loss:   0.051931 => Txt Tokens per Sec:     2113 || Lr: 0.000050
2024-02-02 04:40:24,195 Epoch 4989: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 04:40:24,196 EPOCH 4990
2024-02-02 04:40:38,297 Epoch 4990: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 04:40:38,298 EPOCH 4991
2024-02-02 04:40:52,293 Epoch 4991: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 04:40:52,294 EPOCH 4992
2024-02-02 04:41:06,170 Epoch 4992: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 04:41:06,171 EPOCH 4993
2024-02-02 04:41:20,009 Epoch 4993: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 04:41:20,009 EPOCH 4994
2024-02-02 04:41:34,056 Epoch 4994: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 04:41:34,056 EPOCH 4995
2024-02-02 04:41:48,090 Epoch 4995: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 04:41:48,090 EPOCH 4996
2024-02-02 04:42:01,862 Epoch 4996: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 04:42:01,863 EPOCH 4997
2024-02-02 04:42:15,841 Epoch 4997: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 04:42:15,842 EPOCH 4998
2024-02-02 04:42:29,581 Epoch 4998: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 04:42:29,581 EPOCH 4999
2024-02-02 04:42:43,448 Epoch 4999: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 04:42:43,449 EPOCH 5000
2024-02-02 04:42:57,658 [Epoch: 5000 Step: 00045000] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:      748 || Batch Translation Loss:   0.024396 => Txt Tokens per Sec:     2077 || Lr: 0.000050
2024-02-02 04:42:57,659 Epoch 5000: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 04:42:57,659 EPOCH 5001
2024-02-02 04:43:11,296 Epoch 5001: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 04:43:11,296 EPOCH 5002
2024-02-02 04:43:25,253 Epoch 5002: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 04:43:25,253 EPOCH 5003
2024-02-02 04:43:39,183 Epoch 5003: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 04:43:39,183 EPOCH 5004
2024-02-02 04:43:52,980 Epoch 5004: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 04:43:52,980 EPOCH 5005
2024-02-02 04:44:07,003 Epoch 5005: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 04:44:07,003 EPOCH 5006
2024-02-02 04:44:20,958 Epoch 5006: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 04:44:20,958 EPOCH 5007
2024-02-02 04:44:34,821 Epoch 5007: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 04:44:34,821 EPOCH 5008
2024-02-02 04:44:48,591 Epoch 5008: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 04:44:48,592 EPOCH 5009
2024-02-02 04:45:02,610 Epoch 5009: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 04:45:02,611 EPOCH 5010
2024-02-02 04:45:16,490 Epoch 5010: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 04:45:16,490 EPOCH 5011
2024-02-02 04:45:30,195 Epoch 5011: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 04:45:30,196 EPOCH 5012
2024-02-02 04:45:33,275 [Epoch: 5012 Step: 00045100] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:      416 || Batch Translation Loss:   0.019787 => Txt Tokens per Sec:     1331 || Lr: 0.000050
2024-02-02 04:45:43,982 Epoch 5012: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 04:45:43,983 EPOCH 5013
2024-02-02 04:45:58,064 Epoch 5013: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 04:45:58,065 EPOCH 5014
2024-02-02 04:46:12,017 Epoch 5014: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 04:46:12,017 EPOCH 5015
2024-02-02 04:46:25,753 Epoch 5015: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 04:46:25,754 EPOCH 5016
2024-02-02 04:46:39,810 Epoch 5016: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 04:46:39,810 EPOCH 5017
2024-02-02 04:46:53,571 Epoch 5017: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 04:46:53,572 EPOCH 5018
2024-02-02 04:47:07,609 Epoch 5018: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 04:47:07,610 EPOCH 5019
2024-02-02 04:47:21,456 Epoch 5019: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 04:47:21,457 EPOCH 5020
2024-02-02 04:47:35,254 Epoch 5020: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 04:47:35,254 EPOCH 5021
2024-02-02 04:47:49,320 Epoch 5021: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 04:47:49,320 EPOCH 5022
2024-02-02 04:48:02,931 Epoch 5022: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 04:48:02,931 EPOCH 5023
2024-02-02 04:48:06,604 [Epoch: 5023 Step: 00045200] Batch Recognition Loss:   0.000101 => Gls Tokens per Sec:      455 || Batch Translation Loss:   0.009776 => Txt Tokens per Sec:     1000 || Lr: 0.000050
2024-02-02 04:48:17,014 Epoch 5023: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 04:48:17,014 EPOCH 5024
2024-02-02 04:48:31,029 Epoch 5024: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 04:48:31,029 EPOCH 5025
2024-02-02 04:48:44,979 Epoch 5025: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 04:48:44,980 EPOCH 5026
2024-02-02 04:48:58,927 Epoch 5026: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 04:48:58,928 EPOCH 5027
2024-02-02 04:49:12,837 Epoch 5027: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 04:49:12,838 EPOCH 5028
2024-02-02 04:49:26,646 Epoch 5028: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 04:49:26,647 EPOCH 5029
2024-02-02 04:49:40,606 Epoch 5029: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 04:49:40,606 EPOCH 5030
2024-02-02 04:49:54,972 Epoch 5030: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 04:49:54,972 EPOCH 5031
2024-02-02 04:50:08,759 Epoch 5031: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 04:50:08,759 EPOCH 5032
2024-02-02 04:50:22,704 Epoch 5032: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 04:50:22,705 EPOCH 5033
2024-02-02 04:50:36,618 Epoch 5033: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 04:50:36,618 EPOCH 5034
2024-02-02 04:50:43,590 [Epoch: 5034 Step: 00045300] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:      551 || Batch Translation Loss:   0.023852 => Txt Tokens per Sec:     1687 || Lr: 0.000050
2024-02-02 04:50:50,421 Epoch 5034: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 04:50:50,422 EPOCH 5035
2024-02-02 04:51:04,023 Epoch 5035: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 04:51:04,024 EPOCH 5036
2024-02-02 04:51:17,796 Epoch 5036: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 04:51:17,796 EPOCH 5037
2024-02-02 04:51:31,768 Epoch 5037: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 04:51:31,768 EPOCH 5038
2024-02-02 04:51:45,318 Epoch 5038: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 04:51:45,318 EPOCH 5039
2024-02-02 04:51:59,336 Epoch 5039: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 04:51:59,337 EPOCH 5040
2024-02-02 04:52:12,913 Epoch 5040: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 04:52:12,914 EPOCH 5041
2024-02-02 04:52:26,845 Epoch 5041: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 04:52:26,845 EPOCH 5042
2024-02-02 04:52:40,596 Epoch 5042: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 04:52:40,596 EPOCH 5043
2024-02-02 04:52:54,609 Epoch 5043: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 04:52:54,610 EPOCH 5044
2024-02-02 04:53:08,848 Epoch 5044: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 04:53:08,849 EPOCH 5045
2024-02-02 04:53:14,818 [Epoch: 5045 Step: 00045400] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:      709 || Batch Translation Loss:   0.007004 => Txt Tokens per Sec:     1891 || Lr: 0.000050
2024-02-02 04:53:22,792 Epoch 5045: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 04:53:22,792 EPOCH 5046
2024-02-02 04:53:36,851 Epoch 5046: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 04:53:36,851 EPOCH 5047
2024-02-02 04:53:50,787 Epoch 5047: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 04:53:50,787 EPOCH 5048
2024-02-02 04:54:04,686 Epoch 5048: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 04:54:04,687 EPOCH 5049
2024-02-02 04:54:18,655 Epoch 5049: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 04:54:18,655 EPOCH 5050
2024-02-02 04:54:32,655 Epoch 5050: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 04:54:32,656 EPOCH 5051
2024-02-02 04:54:46,441 Epoch 5051: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 04:54:46,442 EPOCH 5052
2024-02-02 04:55:00,553 Epoch 5052: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 04:55:00,554 EPOCH 5053
2024-02-02 04:55:14,385 Epoch 5053: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 04:55:14,386 EPOCH 5054
2024-02-02 04:55:28,460 Epoch 5054: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 04:55:28,460 EPOCH 5055
2024-02-02 04:55:42,095 Epoch 5055: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 04:55:42,096 EPOCH 5056
2024-02-02 04:55:48,350 [Epoch: 5056 Step: 00045500] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:     1024 || Batch Translation Loss:   0.014766 => Txt Tokens per Sec:     2792 || Lr: 0.000050
2024-02-02 04:55:56,157 Epoch 5056: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 04:55:56,157 EPOCH 5057
2024-02-02 04:56:10,202 Epoch 5057: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 04:56:10,203 EPOCH 5058
2024-02-02 04:56:24,054 Epoch 5058: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 04:56:24,054 EPOCH 5059
2024-02-02 04:56:37,653 Epoch 5059: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 04:56:37,654 EPOCH 5060
2024-02-02 04:56:51,419 Epoch 5060: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 04:56:51,419 EPOCH 5061
2024-02-02 04:57:05,618 Epoch 5061: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 04:57:05,619 EPOCH 5062
2024-02-02 04:57:19,552 Epoch 5062: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 04:57:19,553 EPOCH 5063
2024-02-02 04:57:33,115 Epoch 5063: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 04:57:33,116 EPOCH 5064
2024-02-02 04:57:47,113 Epoch 5064: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 04:57:47,114 EPOCH 5065
2024-02-02 04:58:01,085 Epoch 5065: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 04:58:01,086 EPOCH 5066
2024-02-02 04:58:15,396 Epoch 5066: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 04:58:15,396 EPOCH 5067
2024-02-02 04:58:23,690 [Epoch: 5067 Step: 00045600] Batch Recognition Loss:   0.000092 => Gls Tokens per Sec:      819 || Batch Translation Loss:   0.009615 => Txt Tokens per Sec:     2282 || Lr: 0.000050
2024-02-02 04:58:29,562 Epoch 5067: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 04:58:29,562 EPOCH 5068
2024-02-02 04:58:43,392 Epoch 5068: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 04:58:43,392 EPOCH 5069
2024-02-02 04:58:57,296 Epoch 5069: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 04:58:57,296 EPOCH 5070
2024-02-02 04:59:11,160 Epoch 5070: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 04:59:11,160 EPOCH 5071
2024-02-02 04:59:25,113 Epoch 5071: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 04:59:25,114 EPOCH 5072
2024-02-02 04:59:39,093 Epoch 5072: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 04:59:39,093 EPOCH 5073
2024-02-02 04:59:53,011 Epoch 5073: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 04:59:53,012 EPOCH 5074
2024-02-02 05:00:07,135 Epoch 5074: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 05:00:07,136 EPOCH 5075
2024-02-02 05:00:21,126 Epoch 5075: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 05:00:21,126 EPOCH 5076
2024-02-02 05:00:35,216 Epoch 5076: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 05:00:35,217 EPOCH 5077
2024-02-02 05:00:49,099 Epoch 5077: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 05:00:49,100 EPOCH 5078
2024-02-02 05:00:58,985 [Epoch: 5078 Step: 00045700] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:      906 || Batch Translation Loss:   0.019494 => Txt Tokens per Sec:     2464 || Lr: 0.000050
2024-02-02 05:01:02,925 Epoch 5078: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 05:01:02,925 EPOCH 5079
2024-02-02 05:01:16,682 Epoch 5079: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 05:01:16,683 EPOCH 5080
2024-02-02 05:01:30,609 Epoch 5080: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 05:01:30,610 EPOCH 5081
2024-02-02 05:01:44,377 Epoch 5081: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 05:01:44,378 EPOCH 5082
2024-02-02 05:01:58,275 Epoch 5082: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 05:01:58,276 EPOCH 5083
2024-02-02 05:02:12,299 Epoch 5083: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 05:02:12,300 EPOCH 5084
2024-02-02 05:02:26,187 Epoch 5084: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 05:02:26,188 EPOCH 5085
2024-02-02 05:02:40,090 Epoch 5085: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 05:02:40,090 EPOCH 5086
2024-02-02 05:02:54,142 Epoch 5086: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 05:02:54,143 EPOCH 5087
2024-02-02 05:03:08,128 Epoch 5087: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 05:03:08,129 EPOCH 5088
2024-02-02 05:03:21,832 Epoch 5088: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 05:03:21,833 EPOCH 5089
2024-02-02 05:03:33,977 [Epoch: 5089 Step: 00045800] Batch Recognition Loss:   0.000336 => Gls Tokens per Sec:      770 || Batch Translation Loss:   0.622410 => Txt Tokens per Sec:     2110 || Lr: 0.000050
2024-02-02 05:03:35,676 Epoch 5089: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.34 
2024-02-02 05:03:35,676 EPOCH 5090
2024-02-02 05:03:49,745 Epoch 5090: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.63 
2024-02-02 05:03:49,746 EPOCH 5091
2024-02-02 05:04:03,678 Epoch 5091: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-02 05:04:03,679 EPOCH 5092
2024-02-02 05:04:17,649 Epoch 5092: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-02 05:04:17,650 EPOCH 5093
2024-02-02 05:04:31,388 Epoch 5093: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-02 05:04:31,389 EPOCH 5094
2024-02-02 05:04:45,597 Epoch 5094: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-02 05:04:45,597 EPOCH 5095
2024-02-02 05:04:59,459 Epoch 5095: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 05:04:59,460 EPOCH 5096
2024-02-02 05:05:13,487 Epoch 5096: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 05:05:13,487 EPOCH 5097
2024-02-02 05:05:27,495 Epoch 5097: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 05:05:27,495 EPOCH 5098
2024-02-02 05:05:41,244 Epoch 5098: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 05:05:41,245 EPOCH 5099
2024-02-02 05:05:55,154 Epoch 5099: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 05:05:55,155 EPOCH 5100
2024-02-02 05:06:09,087 [Epoch: 5100 Step: 00045900] Batch Recognition Loss:   0.000302 => Gls Tokens per Sec:      763 || Batch Translation Loss:   0.028658 => Txt Tokens per Sec:     2118 || Lr: 0.000050
2024-02-02 05:06:09,088 Epoch 5100: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 05:06:09,088 EPOCH 5101
2024-02-02 05:06:23,072 Epoch 5101: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 05:06:23,073 EPOCH 5102
2024-02-02 05:06:36,994 Epoch 5102: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 05:06:36,995 EPOCH 5103
2024-02-02 05:06:50,945 Epoch 5103: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 05:06:50,945 EPOCH 5104
2024-02-02 05:07:05,077 Epoch 5104: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 05:07:05,077 EPOCH 5105
2024-02-02 05:07:18,828 Epoch 5105: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 05:07:18,828 EPOCH 5106
2024-02-02 05:07:32,780 Epoch 5106: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 05:07:32,781 EPOCH 5107
2024-02-02 05:07:46,665 Epoch 5107: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 05:07:46,666 EPOCH 5108
2024-02-02 05:08:00,544 Epoch 5108: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 05:08:00,545 EPOCH 5109
2024-02-02 05:08:14,561 Epoch 5109: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 05:08:14,562 EPOCH 5110
2024-02-02 05:08:28,643 Epoch 5110: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 05:08:28,643 EPOCH 5111
2024-02-02 05:08:42,653 Epoch 5111: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 05:08:42,653 EPOCH 5112
2024-02-02 05:08:43,351 [Epoch: 5112 Step: 00046000] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:     1837 || Batch Translation Loss:   0.017071 => Txt Tokens per Sec:     5393 || Lr: 0.000050
2024-02-02 05:09:02,470 Validation result at epoch 5112, step    46000: duration: 19.1195s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00063	Translation Loss: 105109.58594	PPL: 36987.00781
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.50	(BLEU-1: 10.52,	BLEU-2: 3.18,	BLEU-3: 1.12,	BLEU-4: 0.50)
	CHRF 16.69	ROUGE 9.01
2024-02-02 05:09:02,471 Logging Recognition and Translation Outputs
2024-02-02 05:09:02,472 ========================================================================================================================
2024-02-02 05:09:02,472 Logging Sequence: 126_121.00
2024-02-02 05:09:02,472 	Gloss Reference :	A B+C+D+E
2024-02-02 05:09:02,472 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 05:09:02,472 	Gloss Alignment :	         
2024-02-02 05:09:02,473 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 05:09:02,473 	Text Reference  :	**** **** * everyone was very    happy by  his victory
2024-02-02 05:09:02,473 	Text Hypothesis :	high such a video    of  decided to    win a   it     
2024-02-02 05:09:02,473 	Text Alignment  :	I    I    I S        S   S       S     S   S   S      
2024-02-02 05:09:02,474 ========================================================================================================================
2024-02-02 05:09:02,474 Logging Sequence: 73_79.00
2024-02-02 05:09:02,474 	Gloss Reference :	A B+C+D+E
2024-02-02 05:09:02,474 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 05:09:02,474 	Gloss Alignment :	         
2024-02-02 05:09:02,474 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 05:09:02,476 	Text Reference  :	raina resturant has food  from the     rich  spices of   north india to   the   aromatic curries of south    india  
2024-02-02 05:09:02,476 	Text Hypothesis :	***** ********* *** there were chicken kebab pani   puri sev   puri  aloo chaat etc      it      is absolute rubbish
2024-02-02 05:09:02,476 	Text Alignment  :	D     D         D   S     S    S       S     S      S    S     S     S    S     S        S       S  S        S      
2024-02-02 05:09:02,476 ========================================================================================================================
2024-02-02 05:09:02,476 Logging Sequence: 95_152.00
2024-02-02 05:09:02,477 	Gloss Reference :	A B+C+D+E
2024-02-02 05:09:02,477 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 05:09:02,477 	Gloss Alignment :	         
2024-02-02 05:09:02,477 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 05:09:02,477 	Text Reference  :	******** *** *** ***** *** ******* ** how strange
2024-02-02 05:09:02,477 	Text Hypothesis :	pakistan are the score was granted by the reason 
2024-02-02 05:09:02,477 	Text Alignment  :	I        I   I   I     I   I       I  S   S      
2024-02-02 05:09:02,478 ========================================================================================================================
2024-02-02 05:09:02,478 Logging Sequence: 135_39.00
2024-02-02 05:09:02,478 	Gloss Reference :	A B+C+D+E
2024-02-02 05:09:02,478 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 05:09:02,478 	Gloss Alignment :	         
2024-02-02 05:09:02,478 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 05:09:02,479 	Text Reference  :	who needs to  travel from poland  to       stanford university in  california
2024-02-02 05:09:02,479 	Text Hypothesis :	but it    was able   to   restart training for      saving     his life      
2024-02-02 05:09:02,479 	Text Alignment  :	S   S     S   S      S    S       S        S        S          S   S         
2024-02-02 05:09:02,479 ========================================================================================================================
2024-02-02 05:09:02,480 Logging Sequence: 87_2.00
2024-02-02 05:09:02,480 	Gloss Reference :	A B+C+D+E
2024-02-02 05:09:02,480 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 05:09:02,480 	Gloss Alignment :	         
2024-02-02 05:09:02,480 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 05:09:02,482 	Text Reference  :	cricketer gautam gambhir's jealousy against ms    dhoni and virat kohli has   been increasing day by     day  
2024-02-02 05:09:02,482 	Text Hypothesis :	support   the    camera    captured their   fifth video and the   same  since the  same       has called dhoni
2024-02-02 05:09:02,482 	Text Alignment  :	S         S      S         S        S       S     S         S     S     S     S    S          S   S      S    
2024-02-02 05:09:02,482 ========================================================================================================================
2024-02-02 05:09:15,911 Epoch 5112: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:09:15,911 EPOCH 5113
2024-02-02 05:09:29,885 Epoch 5113: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:09:29,886 EPOCH 5114
2024-02-02 05:09:43,717 Epoch 5114: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:09:43,717 EPOCH 5115
2024-02-02 05:09:57,556 Epoch 5115: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 05:09:57,556 EPOCH 5116
2024-02-02 05:10:11,552 Epoch 5116: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:10:11,552 EPOCH 5117
2024-02-02 05:10:25,556 Epoch 5117: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:10:25,556 EPOCH 5118
2024-02-02 05:10:39,298 Epoch 5118: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:10:39,299 EPOCH 5119
2024-02-02 05:10:53,208 Epoch 5119: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 05:10:53,209 EPOCH 5120
2024-02-02 05:11:07,540 Epoch 5120: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:11:07,540 EPOCH 5121
2024-02-02 05:11:21,585 Epoch 5121: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:11:21,585 EPOCH 5122
2024-02-02 05:11:35,493 Epoch 5122: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:11:35,493 EPOCH 5123
2024-02-02 05:11:39,608 [Epoch: 5123 Step: 00046100] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:      622 || Batch Translation Loss:   0.017222 => Txt Tokens per Sec:     1869 || Lr: 0.000050
2024-02-02 05:11:49,553 Epoch 5123: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:11:49,554 EPOCH 5124
2024-02-02 05:12:03,395 Epoch 5124: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:12:03,396 EPOCH 5125
2024-02-02 05:12:17,326 Epoch 5125: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:12:17,327 EPOCH 5126
2024-02-02 05:12:31,282 Epoch 5126: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:12:31,283 EPOCH 5127
2024-02-02 05:12:45,033 Epoch 5127: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:12:45,033 EPOCH 5128
2024-02-02 05:12:58,955 Epoch 5128: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 05:12:58,956 EPOCH 5129
2024-02-02 05:13:12,769 Epoch 5129: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:13:12,770 EPOCH 5130
2024-02-02 05:13:26,875 Epoch 5130: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 05:13:26,876 EPOCH 5131
2024-02-02 05:13:40,344 Epoch 5131: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 05:13:40,345 EPOCH 5132
2024-02-02 05:13:54,356 Epoch 5132: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 05:13:54,356 EPOCH 5133
2024-02-02 05:14:08,420 Epoch 5133: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:14:08,421 EPOCH 5134
2024-02-02 05:14:16,826 [Epoch: 5134 Step: 00046200] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:      457 || Batch Translation Loss:   0.016089 => Txt Tokens per Sec:     1478 || Lr: 0.000050
2024-02-02 05:14:22,353 Epoch 5134: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:14:22,353 EPOCH 5135
2024-02-02 05:14:35,973 Epoch 5135: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:14:35,974 EPOCH 5136
2024-02-02 05:14:50,091 Epoch 5136: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:14:50,091 EPOCH 5137
2024-02-02 05:15:04,118 Epoch 5137: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:15:04,118 EPOCH 5138
2024-02-02 05:15:17,909 Epoch 5138: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:15:17,909 EPOCH 5139
2024-02-02 05:15:31,812 Epoch 5139: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:15:31,813 EPOCH 5140
2024-02-02 05:15:45,647 Epoch 5140: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:15:45,648 EPOCH 5141
2024-02-02 05:15:59,331 Epoch 5141: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:15:59,331 EPOCH 5142
2024-02-02 05:16:13,399 Epoch 5142: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:16:13,399 EPOCH 5143
2024-02-02 05:16:27,471 Epoch 5143: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:16:27,472 EPOCH 5144
2024-02-02 05:16:41,363 Epoch 5144: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:16:41,364 EPOCH 5145
2024-02-02 05:16:44,372 [Epoch: 5145 Step: 00046300] Batch Recognition Loss:   0.000093 => Gls Tokens per Sec:     1703 || Batch Translation Loss:   0.006126 => Txt Tokens per Sec:     4353 || Lr: 0.000050
2024-02-02 05:16:55,308 Epoch 5145: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 05:16:55,309 EPOCH 5146
2024-02-02 05:17:09,186 Epoch 5146: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:17:09,186 EPOCH 5147
2024-02-02 05:17:23,245 Epoch 5147: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:17:23,245 EPOCH 5148
2024-02-02 05:17:37,169 Epoch 5148: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:17:37,170 EPOCH 5149
2024-02-02 05:17:51,323 Epoch 5149: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:17:51,323 EPOCH 5150
2024-02-02 05:18:05,200 Epoch 5150: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:18:05,200 EPOCH 5151
2024-02-02 05:18:19,317 Epoch 5151: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:18:19,318 EPOCH 5152
2024-02-02 05:18:33,156 Epoch 5152: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 05:18:33,156 EPOCH 5153
2024-02-02 05:18:47,107 Epoch 5153: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 05:18:47,108 EPOCH 5154
2024-02-02 05:19:01,029 Epoch 5154: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:19:01,030 EPOCH 5155
2024-02-02 05:19:14,994 Epoch 5155: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:19:14,994 EPOCH 5156
2024-02-02 05:19:24,477 [Epoch: 5156 Step: 00046400] Batch Recognition Loss:   0.000115 => Gls Tokens per Sec:      581 || Batch Translation Loss:   0.006054 => Txt Tokens per Sec:     1708 || Lr: 0.000050
2024-02-02 05:19:28,876 Epoch 5156: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:19:28,877 EPOCH 5157
2024-02-02 05:19:42,752 Epoch 5157: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:19:42,752 EPOCH 5158
2024-02-02 05:19:56,615 Epoch 5158: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 05:19:56,616 EPOCH 5159
2024-02-02 05:20:10,121 Epoch 5159: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:20:10,122 EPOCH 5160
2024-02-02 05:20:24,330 Epoch 5160: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:20:24,331 EPOCH 5161
2024-02-02 05:20:38,040 Epoch 5161: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:20:38,040 EPOCH 5162
2024-02-02 05:20:51,976 Epoch 5162: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:20:51,976 EPOCH 5163
2024-02-02 05:21:05,753 Epoch 5163: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 05:21:05,754 EPOCH 5164
2024-02-02 05:21:19,096 Epoch 5164: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:21:19,096 EPOCH 5165
2024-02-02 05:21:33,377 Epoch 5165: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:21:33,377 EPOCH 5166
2024-02-02 05:21:47,257 Epoch 5166: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 05:21:47,258 EPOCH 5167
2024-02-02 05:21:59,747 [Epoch: 5167 Step: 00046500] Batch Recognition Loss:   0.000094 => Gls Tokens per Sec:      544 || Batch Translation Loss:   0.010211 => Txt Tokens per Sec:     1549 || Lr: 0.000050
2024-02-02 05:22:01,102 Epoch 5167: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 05:22:01,102 EPOCH 5168
2024-02-02 05:22:14,956 Epoch 5168: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:22:14,957 EPOCH 5169
2024-02-02 05:22:29,406 Epoch 5169: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 05:22:29,406 EPOCH 5170
2024-02-02 05:22:43,238 Epoch 5170: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 05:22:43,238 EPOCH 5171
2024-02-02 05:22:57,176 Epoch 5171: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 05:22:57,176 EPOCH 5172
2024-02-02 05:23:11,040 Epoch 5172: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 05:23:11,040 EPOCH 5173
2024-02-02 05:23:24,856 Epoch 5173: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 05:23:24,856 EPOCH 5174
2024-02-02 05:23:38,911 Epoch 5174: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 05:23:38,912 EPOCH 5175
2024-02-02 05:23:52,593 Epoch 5175: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 05:23:52,593 EPOCH 5176
2024-02-02 05:24:06,828 Epoch 5176: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 05:24:06,829 EPOCH 5177
2024-02-02 05:24:20,775 Epoch 5177: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 05:24:20,776 EPOCH 5178
2024-02-02 05:24:30,710 [Epoch: 5178 Step: 00046600] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:      812 || Batch Translation Loss:   0.008291 => Txt Tokens per Sec:     2291 || Lr: 0.000050
2024-02-02 05:24:34,657 Epoch 5178: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 05:24:34,657 EPOCH 5179
2024-02-02 05:24:48,573 Epoch 5179: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 05:24:48,574 EPOCH 5180
2024-02-02 05:25:02,508 Epoch 5180: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:25:02,508 EPOCH 5181
2024-02-02 05:25:16,451 Epoch 5181: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:25:16,452 EPOCH 5182
2024-02-02 05:25:30,310 Epoch 5182: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:25:30,310 EPOCH 5183
2024-02-02 05:25:44,512 Epoch 5183: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:25:44,513 EPOCH 5184
2024-02-02 05:25:58,448 Epoch 5184: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:25:58,449 EPOCH 5185
2024-02-02 05:26:12,570 Epoch 5185: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:26:12,571 EPOCH 5186
2024-02-02 05:26:26,461 Epoch 5186: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:26:26,462 EPOCH 5187
2024-02-02 05:26:40,118 Epoch 5187: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:26:40,119 EPOCH 5188
2024-02-02 05:26:53,986 Epoch 5188: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 05:26:53,987 EPOCH 5189
2024-02-02 05:27:06,500 [Epoch: 5189 Step: 00046700] Batch Recognition Loss:   0.000108 => Gls Tokens per Sec:      747 || Batch Translation Loss:   0.079800 => Txt Tokens per Sec:     2050 || Lr: 0.000050
2024-02-02 05:27:08,214 Epoch 5189: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 05:27:08,214 EPOCH 5190
2024-02-02 05:27:21,883 Epoch 5190: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 05:27:21,883 EPOCH 5191
2024-02-02 05:27:35,845 Epoch 5191: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 05:27:35,845 EPOCH 5192
2024-02-02 05:27:49,665 Epoch 5192: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 05:27:49,666 EPOCH 5193
2024-02-02 05:28:03,670 Epoch 5193: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 05:28:03,670 EPOCH 5194
2024-02-02 05:28:17,419 Epoch 5194: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 05:28:17,419 EPOCH 5195
2024-02-02 05:28:31,552 Epoch 5195: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 05:28:31,552 EPOCH 5196
2024-02-02 05:28:45,173 Epoch 5196: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 05:28:45,174 EPOCH 5197
2024-02-02 05:28:59,178 Epoch 5197: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 05:28:59,179 EPOCH 5198
2024-02-02 05:29:13,154 Epoch 5198: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 05:29:13,155 EPOCH 5199
2024-02-02 05:29:27,183 Epoch 5199: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 05:29:27,183 EPOCH 5200
2024-02-02 05:29:40,956 [Epoch: 5200 Step: 00046800] Batch Recognition Loss:   0.000151 => Gls Tokens per Sec:      772 || Batch Translation Loss:   0.020826 => Txt Tokens per Sec:     2143 || Lr: 0.000050
2024-02-02 05:29:40,957 Epoch 5200: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 05:29:40,957 EPOCH 5201
2024-02-02 05:29:54,879 Epoch 5201: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 05:29:54,880 EPOCH 5202
2024-02-02 05:30:08,983 Epoch 5202: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 05:30:08,984 EPOCH 5203
2024-02-02 05:30:22,942 Epoch 5203: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 05:30:22,942 EPOCH 5204
2024-02-02 05:30:37,066 Epoch 5204: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 05:30:37,066 EPOCH 5205
2024-02-02 05:30:50,908 Epoch 5205: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 05:30:50,909 EPOCH 5206
2024-02-02 05:31:04,787 Epoch 5206: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 05:31:04,788 EPOCH 5207
2024-02-02 05:31:18,637 Epoch 5207: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 05:31:18,637 EPOCH 5208
2024-02-02 05:31:32,657 Epoch 5208: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 05:31:32,657 EPOCH 5209
2024-02-02 05:31:46,665 Epoch 5209: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:31:46,666 EPOCH 5210
2024-02-02 05:32:00,665 Epoch 5210: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:32:00,666 EPOCH 5211
2024-02-02 05:32:14,601 Epoch 5211: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:32:14,601 EPOCH 5212
2024-02-02 05:32:14,973 [Epoch: 5212 Step: 00046900] Batch Recognition Loss:   0.000110 => Gls Tokens per Sec:     3450 || Batch Translation Loss:   0.012353 => Txt Tokens per Sec:     8744 || Lr: 0.000050
2024-02-02 05:32:28,367 Epoch 5212: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 05:32:28,367 EPOCH 5213
2024-02-02 05:32:42,551 Epoch 5213: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 05:32:42,552 EPOCH 5214
2024-02-02 05:32:56,519 Epoch 5214: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:32:56,520 EPOCH 5215
2024-02-02 05:33:10,828 Epoch 5215: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:33:10,829 EPOCH 5216
2024-02-02 05:33:24,577 Epoch 5216: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:33:24,578 EPOCH 5217
2024-02-02 05:33:38,610 Epoch 5217: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:33:38,610 EPOCH 5218
2024-02-02 05:33:52,257 Epoch 5218: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:33:52,258 EPOCH 5219
2024-02-02 05:34:06,093 Epoch 5219: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:34:06,094 EPOCH 5220
2024-02-02 05:34:19,904 Epoch 5220: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 05:34:19,905 EPOCH 5221
2024-02-02 05:34:33,775 Epoch 5221: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 05:34:33,776 EPOCH 5222
2024-02-02 05:34:47,203 Epoch 5222: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 05:34:47,204 EPOCH 5223
2024-02-02 05:34:47,959 [Epoch: 5223 Step: 00047000] Batch Recognition Loss:   0.000094 => Gls Tokens per Sec:     3395 || Batch Translation Loss:   0.012006 => Txt Tokens per Sec:     6918 || Lr: 0.000050
2024-02-02 05:35:01,275 Epoch 5223: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 05:35:01,276 EPOCH 5224
2024-02-02 05:35:15,002 Epoch 5224: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:35:15,003 EPOCH 5225
2024-02-02 05:35:28,917 Epoch 5225: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 05:35:28,917 EPOCH 5226
2024-02-02 05:35:42,675 Epoch 5226: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 05:35:42,675 EPOCH 5227
2024-02-02 05:35:56,837 Epoch 5227: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 05:35:56,838 EPOCH 5228
2024-02-02 05:36:10,725 Epoch 5228: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 05:36:10,726 EPOCH 5229
2024-02-02 05:36:24,636 Epoch 5229: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 05:36:24,637 EPOCH 5230
2024-02-02 05:36:38,443 Epoch 5230: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 05:36:38,443 EPOCH 5231
2024-02-02 05:36:52,447 Epoch 5231: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:36:52,447 EPOCH 5232
2024-02-02 05:37:06,538 Epoch 5232: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:37:06,538 EPOCH 5233
2024-02-02 05:37:20,237 Epoch 5233: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 05:37:20,238 EPOCH 5234
2024-02-02 05:37:24,550 [Epoch: 5234 Step: 00047100] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:      891 || Batch Translation Loss:   0.020336 => Txt Tokens per Sec:     2606 || Lr: 0.000050
2024-02-02 05:37:33,805 Epoch 5234: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 05:37:33,806 EPOCH 5235
2024-02-02 05:37:47,916 Epoch 5235: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:37:47,916 EPOCH 5236
2024-02-02 05:38:01,879 Epoch 5236: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 05:38:01,879 EPOCH 5237
2024-02-02 05:38:15,937 Epoch 5237: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 05:38:15,938 EPOCH 5238
2024-02-02 05:38:29,682 Epoch 5238: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 05:38:29,683 EPOCH 5239
2024-02-02 05:38:43,516 Epoch 5239: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 05:38:43,517 EPOCH 5240
2024-02-02 05:38:57,765 Epoch 5240: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 05:38:57,765 EPOCH 5241
2024-02-02 05:39:11,569 Epoch 5241: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 05:39:11,570 EPOCH 5242
2024-02-02 05:39:25,489 Epoch 5242: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 05:39:25,489 EPOCH 5243
2024-02-02 05:39:39,615 Epoch 5243: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 05:39:39,616 EPOCH 5244
2024-02-02 05:39:53,620 Epoch 5244: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 05:39:53,621 EPOCH 5245
2024-02-02 05:39:55,279 [Epoch: 5245 Step: 00047200] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:     3089 || Batch Translation Loss:   0.011112 => Txt Tokens per Sec:     8088 || Lr: 0.000050
2024-02-02 05:40:07,445 Epoch 5245: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 05:40:07,446 EPOCH 5246
2024-02-02 05:40:21,341 Epoch 5246: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 05:40:21,342 EPOCH 5247
2024-02-02 05:40:35,519 Epoch 5247: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 05:40:35,520 EPOCH 5248
2024-02-02 05:40:49,373 Epoch 5248: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 05:40:49,374 EPOCH 5249
2024-02-02 05:41:03,639 Epoch 5249: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 05:41:03,639 EPOCH 5250
2024-02-02 05:41:17,465 Epoch 5250: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 05:41:17,465 EPOCH 5251
2024-02-02 05:41:31,061 Epoch 5251: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 05:41:31,062 EPOCH 5252
2024-02-02 05:41:45,097 Epoch 5252: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 05:41:45,098 EPOCH 5253
2024-02-02 05:41:58,957 Epoch 5253: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 05:41:58,958 EPOCH 5254
2024-02-02 05:42:12,932 Epoch 5254: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 05:42:12,932 EPOCH 5255
2024-02-02 05:42:26,565 Epoch 5255: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 05:42:26,566 EPOCH 5256
2024-02-02 05:42:33,081 [Epoch: 5256 Step: 00047300] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:      846 || Batch Translation Loss:   0.030841 => Txt Tokens per Sec:     2269 || Lr: 0.000050
2024-02-02 05:42:40,647 Epoch 5256: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 05:42:40,648 EPOCH 5257
2024-02-02 05:42:54,641 Epoch 5257: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 05:42:54,642 EPOCH 5258
2024-02-02 05:43:08,577 Epoch 5258: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 05:43:08,577 EPOCH 5259
2024-02-02 05:43:22,549 Epoch 5259: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 05:43:22,549 EPOCH 5260
2024-02-02 05:43:36,707 Epoch 5260: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-02 05:43:36,707 EPOCH 5261
2024-02-02 05:43:50,589 Epoch 5261: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 05:43:50,590 EPOCH 5262
2024-02-02 05:44:04,633 Epoch 5262: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 05:44:04,633 EPOCH 5263
2024-02-02 05:44:18,766 Epoch 5263: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 05:44:18,766 EPOCH 5264
2024-02-02 05:44:32,764 Epoch 5264: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 05:44:32,764 EPOCH 5265
2024-02-02 05:44:46,740 Epoch 5265: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 05:44:46,741 EPOCH 5266
2024-02-02 05:45:00,780 Epoch 5266: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 05:45:00,781 EPOCH 5267
2024-02-02 05:45:10,502 [Epoch: 5267 Step: 00047400] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:      790 || Batch Translation Loss:   0.022608 => Txt Tokens per Sec:     2151 || Lr: 0.000050
2024-02-02 05:45:15,090 Epoch 5267: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 05:45:15,091 EPOCH 5268
2024-02-02 05:45:29,150 Epoch 5268: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 05:45:29,151 EPOCH 5269
2024-02-02 05:45:42,908 Epoch 5269: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 05:45:42,908 EPOCH 5270
2024-02-02 05:45:56,958 Epoch 5270: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 05:45:56,959 EPOCH 5271
2024-02-02 05:46:10,914 Epoch 5271: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 05:46:10,915 EPOCH 5272
2024-02-02 05:46:24,777 Epoch 5272: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 05:46:24,778 EPOCH 5273
2024-02-02 05:46:38,861 Epoch 5273: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 05:46:38,862 EPOCH 5274
2024-02-02 05:46:53,003 Epoch 5274: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 05:46:53,003 EPOCH 5275
2024-02-02 05:47:06,930 Epoch 5275: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 05:47:06,931 EPOCH 5276
2024-02-02 05:47:20,565 Epoch 5276: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 05:47:20,566 EPOCH 5277
2024-02-02 05:47:34,252 Epoch 5277: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 05:47:34,253 EPOCH 5278
2024-02-02 05:47:46,142 [Epoch: 5278 Step: 00047500] Batch Recognition Loss:   0.000234 => Gls Tokens per Sec:      679 || Batch Translation Loss:   0.030425 => Txt Tokens per Sec:     1962 || Lr: 0.000050
2024-02-02 05:47:48,208 Epoch 5278: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 05:47:48,209 EPOCH 5279
2024-02-02 05:48:02,189 Epoch 5279: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 05:48:02,189 EPOCH 5280
2024-02-02 05:48:16,086 Epoch 5280: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 05:48:16,086 EPOCH 5281
2024-02-02 05:48:29,971 Epoch 5281: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 05:48:29,972 EPOCH 5282
2024-02-02 05:48:43,903 Epoch 5282: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 05:48:43,904 EPOCH 5283
2024-02-02 05:48:57,666 Epoch 5283: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 05:48:57,667 EPOCH 5284
2024-02-02 05:49:11,624 Epoch 5284: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 05:49:11,625 EPOCH 5285
2024-02-02 05:49:25,645 Epoch 5285: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 05:49:25,645 EPOCH 5286
2024-02-02 05:49:39,275 Epoch 5286: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 05:49:39,275 EPOCH 5287
2024-02-02 05:49:52,894 Epoch 5287: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:49:52,894 EPOCH 5288
2024-02-02 05:50:06,867 Epoch 5288: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:50:06,867 EPOCH 5289
2024-02-02 05:50:20,394 [Epoch: 5289 Step: 00047600] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:      691 || Batch Translation Loss:   0.007660 => Txt Tokens per Sec:     2013 || Lr: 0.000050
2024-02-02 05:50:20,675 Epoch 5289: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 05:50:20,675 EPOCH 5290
2024-02-02 05:50:34,644 Epoch 5290: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 05:50:34,645 EPOCH 5291
2024-02-02 05:50:48,470 Epoch 5291: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 05:50:48,471 EPOCH 5292
2024-02-02 05:51:02,379 Epoch 5292: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:51:02,379 EPOCH 5293
2024-02-02 05:51:16,110 Epoch 5293: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:51:16,111 EPOCH 5294
2024-02-02 05:51:29,897 Epoch 5294: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:51:29,898 EPOCH 5295
2024-02-02 05:51:43,764 Epoch 5295: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:51:43,765 EPOCH 5296
2024-02-02 05:51:57,447 Epoch 5296: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:51:57,448 EPOCH 5297
2024-02-02 05:52:11,580 Epoch 5297: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:52:11,581 EPOCH 5298
2024-02-02 05:52:25,378 Epoch 5298: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:52:25,379 EPOCH 5299
2024-02-02 05:52:39,521 Epoch 5299: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:52:39,521 EPOCH 5300
2024-02-02 05:52:53,336 [Epoch: 5300 Step: 00047700] Batch Recognition Loss:   0.000085 => Gls Tokens per Sec:      770 || Batch Translation Loss:   0.009651 => Txt Tokens per Sec:     2136 || Lr: 0.000050
2024-02-02 05:52:53,336 Epoch 5300: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:52:53,336 EPOCH 5301
2024-02-02 05:53:07,385 Epoch 5301: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:53:07,386 EPOCH 5302
2024-02-02 05:53:21,536 Epoch 5302: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:53:21,537 EPOCH 5303
2024-02-02 05:53:35,274 Epoch 5303: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:53:35,275 EPOCH 5304
2024-02-02 05:53:49,327 Epoch 5304: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 05:53:49,328 EPOCH 5305
2024-02-02 05:54:03,497 Epoch 5305: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:54:03,498 EPOCH 5306
2024-02-02 05:54:17,242 Epoch 5306: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:54:17,242 EPOCH 5307
2024-02-02 05:54:31,489 Epoch 5307: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:54:31,490 EPOCH 5308
2024-02-02 05:54:45,396 Epoch 5308: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:54:45,396 EPOCH 5309
2024-02-02 05:54:59,310 Epoch 5309: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 05:54:59,311 EPOCH 5310
2024-02-02 05:55:13,152 Epoch 5310: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 05:55:13,153 EPOCH 5311
2024-02-02 05:55:27,055 Epoch 5311: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 05:55:27,056 EPOCH 5312
2024-02-02 05:55:27,621 [Epoch: 5312 Step: 00047800] Batch Recognition Loss:   0.000099 => Gls Tokens per Sec:     2269 || Batch Translation Loss:   0.010175 => Txt Tokens per Sec:     6255 || Lr: 0.000050
2024-02-02 05:55:41,236 Epoch 5312: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 05:55:41,237 EPOCH 5313
2024-02-02 05:55:55,224 Epoch 5313: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 05:55:55,225 EPOCH 5314
2024-02-02 05:56:08,973 Epoch 5314: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 05:56:08,973 EPOCH 5315
2024-02-02 05:56:22,924 Epoch 5315: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 05:56:22,925 EPOCH 5316
2024-02-02 05:56:36,838 Epoch 5316: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:56:36,839 EPOCH 5317
2024-02-02 05:56:50,986 Epoch 5317: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:56:50,987 EPOCH 5318
2024-02-02 05:57:04,818 Epoch 5318: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:57:04,818 EPOCH 5319
2024-02-02 05:57:18,704 Epoch 5319: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:57:18,705 EPOCH 5320
2024-02-02 05:57:32,680 Epoch 5320: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 05:57:32,681 EPOCH 5321
2024-02-02 05:57:46,290 Epoch 5321: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 05:57:46,291 EPOCH 5322
2024-02-02 05:58:00,220 Epoch 5322: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 05:58:00,221 EPOCH 5323
2024-02-02 05:58:07,335 [Epoch: 5323 Step: 00047900] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:      235 || Batch Translation Loss:   0.006947 => Txt Tokens per Sec:      826 || Lr: 0.000050
2024-02-02 05:58:14,208 Epoch 5323: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 05:58:14,208 EPOCH 5324
2024-02-02 05:58:27,554 Epoch 5324: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 05:58:27,555 EPOCH 5325
2024-02-02 05:58:41,572 Epoch 5325: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 05:58:41,572 EPOCH 5326
2024-02-02 05:58:55,493 Epoch 5326: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 05:58:55,493 EPOCH 5327
2024-02-02 05:59:09,667 Epoch 5327: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 05:59:09,668 EPOCH 5328
2024-02-02 05:59:23,609 Epoch 5328: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 05:59:23,610 EPOCH 5329
2024-02-02 05:59:37,459 Epoch 5329: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 05:59:37,460 EPOCH 5330
2024-02-02 05:59:51,259 Epoch 5330: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 05:59:51,260 EPOCH 5331
2024-02-02 06:00:05,476 Epoch 5331: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 06:00:05,477 EPOCH 5332
2024-02-02 06:00:19,332 Epoch 5332: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 06:00:19,332 EPOCH 5333
2024-02-02 06:00:33,068 Epoch 5333: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:00:33,069 EPOCH 5334
2024-02-02 06:00:34,087 [Epoch: 5334 Step: 00048000] Batch Recognition Loss:   0.000089 => Gls Tokens per Sec:     3778 || Batch Translation Loss:   0.006847 => Txt Tokens per Sec:     8586 || Lr: 0.000050
2024-02-02 06:00:52,924 Validation result at epoch 5334, step    48000: duration: 18.8371s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00036	Translation Loss: 106898.40625	PPL: 44237.51953
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.50	(BLEU-1: 11.04,	BLEU-2: 3.33,	BLEU-3: 1.16,	BLEU-4: 0.50)
	CHRF 17.00	ROUGE 9.38
2024-02-02 06:00:52,926 Logging Recognition and Translation Outputs
2024-02-02 06:00:52,926 ========================================================================================================================
2024-02-02 06:00:52,927 Logging Sequence: 88_159.00
2024-02-02 06:00:52,927 	Gloss Reference :	A B+C+D+E
2024-02-02 06:00:52,927 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 06:00:52,927 	Gloss Alignment :	         
2024-02-02 06:00:52,927 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 06:00:52,928 	Text Reference  :	however he  often  comes to    the town to meet his    relatives
2024-02-02 06:00:52,928 	Text Hypothesis :	******* the police never filed the **** ** fir  sushil kumar    
2024-02-02 06:00:52,928 	Text Alignment  :	D       S   S      S     S         D    D  S    S      S        
2024-02-02 06:00:52,928 ========================================================================================================================
2024-02-02 06:00:52,929 Logging Sequence: 180_53.00
2024-02-02 06:00:52,929 	Gloss Reference :	A B+C+D+E
2024-02-02 06:00:52,929 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 06:00:52,929 	Gloss Alignment :	         
2024-02-02 06:00:52,929 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 06:00:52,930 	Text Reference  :	******* ** the ******** ***** **** **** ** ** ******* ** protest is  against singh again
2024-02-02 06:00:52,930 	Text Hypothesis :	because of the olympics since they have to be decided to be      has landed  in    india
2024-02-02 06:00:52,930 	Text Alignment  :	I       I      I        I     I    I    I  I  I       I  S       S   S       S     S    
2024-02-02 06:00:52,930 ========================================================================================================================
2024-02-02 06:00:52,931 Logging Sequence: 163_30.00
2024-02-02 06:00:52,931 	Gloss Reference :	A B+C+D+E
2024-02-02 06:00:52,931 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 06:00:52,931 	Gloss Alignment :	         
2024-02-02 06:00:52,931 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 06:00:52,932 	Text Reference  :	**** **** ***** ******* they never permitted anyone to       reveal her face
2024-02-02 06:00:52,932 	Text Hypothesis :	that time usman khawaja who  is    virat     kohli  involved from   the team
2024-02-02 06:00:52,932 	Text Alignment  :	I    I    I     I       S    S     S         S      S        S      S   S   
2024-02-02 06:00:52,932 ========================================================================================================================
2024-02-02 06:00:52,932 Logging Sequence: 51_110.00
2024-02-02 06:00:52,933 	Gloss Reference :	A B+C+D+E
2024-02-02 06:00:52,933 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 06:00:52,933 	Gloss Alignment :	         
2024-02-02 06:00:52,933 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 06:00:52,933 	Text Reference  :	the aussies were very happy with    their victory
2024-02-02 06:00:52,934 	Text Hypothesis :	*** he      took a    new   zealand in    india  
2024-02-02 06:00:52,934 	Text Alignment  :	D   S       S    S    S     S       S     S      
2024-02-02 06:00:52,934 ========================================================================================================================
2024-02-02 06:00:52,934 Logging Sequence: 70_249.00
2024-02-02 06:00:52,934 	Gloss Reference :	A B+C+D+E
2024-02-02 06:00:52,934 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 06:00:52,934 	Gloss Alignment :	         
2024-02-02 06:00:52,935 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 06:00:52,935 	Text Reference  :	******* **** ** have a       look at   this    video   
2024-02-02 06:00:52,935 	Text Hypothesis :	however then on 16th october 2022 very nothing happened
2024-02-02 06:00:52,935 	Text Alignment  :	I       I    I  S    S       S    S    S       S       
2024-02-02 06:00:52,935 ========================================================================================================================
2024-02-02 06:01:05,938 Epoch 5334: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:01:05,938 EPOCH 5335
2024-02-02 06:01:20,105 Epoch 5335: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:01:20,106 EPOCH 5336
2024-02-02 06:01:34,185 Epoch 5336: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:01:34,186 EPOCH 5337
2024-02-02 06:01:48,096 Epoch 5337: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:01:48,097 EPOCH 5338
2024-02-02 06:02:02,141 Epoch 5338: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:02:02,141 EPOCH 5339
2024-02-02 06:02:16,128 Epoch 5339: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:02:16,129 EPOCH 5340
2024-02-02 06:02:30,025 Epoch 5340: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:02:30,025 EPOCH 5341
2024-02-02 06:02:44,497 Epoch 5341: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 06:02:44,497 EPOCH 5342
2024-02-02 06:02:58,474 Epoch 5342: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:02:58,475 EPOCH 5343
2024-02-02 06:03:12,367 Epoch 5343: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:03:12,368 EPOCH 5344
2024-02-02 06:03:26,527 Epoch 5344: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:03:26,527 EPOCH 5345
2024-02-02 06:03:31,006 [Epoch: 5345 Step: 00048100] Batch Recognition Loss:   0.000112 => Gls Tokens per Sec:     1143 || Batch Translation Loss:   0.012738 => Txt Tokens per Sec:     3118 || Lr: 0.000050
2024-02-02 06:03:40,458 Epoch 5345: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:03:40,458 EPOCH 5346
2024-02-02 06:03:54,176 Epoch 5346: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:03:54,177 EPOCH 5347
2024-02-02 06:04:08,036 Epoch 5347: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:04:08,036 EPOCH 5348
2024-02-02 06:04:21,956 Epoch 5348: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 06:04:21,956 EPOCH 5349
2024-02-02 06:04:35,957 Epoch 5349: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 06:04:35,958 EPOCH 5350
2024-02-02 06:04:49,714 Epoch 5350: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 06:04:49,715 EPOCH 5351
2024-02-02 06:05:03,659 Epoch 5351: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 06:05:03,660 EPOCH 5352
2024-02-02 06:05:17,711 Epoch 5352: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 06:05:17,712 EPOCH 5353
2024-02-02 06:05:31,607 Epoch 5353: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 06:05:31,608 EPOCH 5354
2024-02-02 06:05:45,757 Epoch 5354: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 06:05:45,758 EPOCH 5355
2024-02-02 06:05:59,686 Epoch 5355: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 06:05:59,687 EPOCH 5356
2024-02-02 06:06:07,514 [Epoch: 5356 Step: 00048200] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:      704 || Batch Translation Loss:   0.008594 => Txt Tokens per Sec:     1967 || Lr: 0.000050
2024-02-02 06:06:13,631 Epoch 5356: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 06:06:13,631 EPOCH 5357
2024-02-02 06:06:27,397 Epoch 5357: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 06:06:27,398 EPOCH 5358
2024-02-02 06:06:41,533 Epoch 5358: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:06:41,533 EPOCH 5359
2024-02-02 06:06:55,548 Epoch 5359: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:06:55,548 EPOCH 5360
2024-02-02 06:07:09,622 Epoch 5360: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:07:09,623 EPOCH 5361
2024-02-02 06:07:23,778 Epoch 5361: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 06:07:23,778 EPOCH 5362
2024-02-02 06:07:37,589 Epoch 5362: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 06:07:37,590 EPOCH 5363
2024-02-02 06:07:51,357 Epoch 5363: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 06:07:51,358 EPOCH 5364
2024-02-02 06:08:05,327 Epoch 5364: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 06:08:05,328 EPOCH 5365
2024-02-02 06:08:19,450 Epoch 5365: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 06:08:19,451 EPOCH 5366
2024-02-02 06:08:33,081 Epoch 5366: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 06:08:33,081 EPOCH 5367
2024-02-02 06:08:44,715 [Epoch: 5367 Step: 00048300] Batch Recognition Loss:   0.000152 => Gls Tokens per Sec:      584 || Batch Translation Loss:   0.015398 => Txt Tokens per Sec:     1648 || Lr: 0.000050
2024-02-02 06:08:47,168 Epoch 5367: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 06:08:47,169 EPOCH 5368
2024-02-02 06:09:01,022 Epoch 5368: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 06:09:01,023 EPOCH 5369
2024-02-02 06:09:14,908 Epoch 5369: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 06:09:14,909 EPOCH 5370
2024-02-02 06:09:28,893 Epoch 5370: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 06:09:28,893 EPOCH 5371
2024-02-02 06:09:42,518 Epoch 5371: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 06:09:42,519 EPOCH 5372
2024-02-02 06:09:56,619 Epoch 5372: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 06:09:56,620 EPOCH 5373
2024-02-02 06:10:10,577 Epoch 5373: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 06:10:10,578 EPOCH 5374
2024-02-02 06:10:24,299 Epoch 5374: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 06:10:24,299 EPOCH 5375
2024-02-02 06:10:38,098 Epoch 5375: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 06:10:38,098 EPOCH 5376
2024-02-02 06:10:52,088 Epoch 5376: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:10:52,088 EPOCH 5377
2024-02-02 06:11:06,152 Epoch 5377: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 06:11:06,152 EPOCH 5378
2024-02-02 06:11:18,032 [Epoch: 5378 Step: 00048400] Batch Recognition Loss:   0.000166 => Gls Tokens per Sec:      679 || Batch Translation Loss:   0.007950 => Txt Tokens per Sec:     1971 || Lr: 0.000050
2024-02-02 06:11:20,163 Epoch 5378: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 06:11:20,163 EPOCH 5379
2024-02-02 06:11:33,968 Epoch 5379: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 06:11:33,968 EPOCH 5380
2024-02-02 06:11:48,086 Epoch 5380: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 06:11:48,086 EPOCH 5381
2024-02-02 06:12:02,036 Epoch 5381: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 06:12:02,036 EPOCH 5382
2024-02-02 06:12:15,961 Epoch 5382: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 06:12:15,961 EPOCH 5383
2024-02-02 06:12:30,031 Epoch 5383: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 06:12:30,031 EPOCH 5384
2024-02-02 06:12:43,752 Epoch 5384: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 06:12:43,753 EPOCH 5385
2024-02-02 06:12:57,570 Epoch 5385: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 06:12:57,571 EPOCH 5386
2024-02-02 06:13:11,062 Epoch 5386: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 06:13:11,063 EPOCH 5387
2024-02-02 06:13:24,821 Epoch 5387: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 06:13:24,821 EPOCH 5388
2024-02-02 06:13:38,811 Epoch 5388: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 06:13:38,812 EPOCH 5389
2024-02-02 06:13:50,884 [Epoch: 5389 Step: 00048500] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:      775 || Batch Translation Loss:   0.033529 => Txt Tokens per Sec:     2126 || Lr: 0.000050
2024-02-02 06:13:52,612 Epoch 5389: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-02 06:13:52,612 EPOCH 5390
2024-02-02 06:14:06,390 Epoch 5390: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.32 
2024-02-02 06:14:06,390 EPOCH 5391
2024-02-02 06:14:20,230 Epoch 5391: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-02 06:14:20,230 EPOCH 5392
2024-02-02 06:14:34,269 Epoch 5392: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-02 06:14:34,270 EPOCH 5393
2024-02-02 06:14:48,284 Epoch 5393: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-02 06:14:48,285 EPOCH 5394
2024-02-02 06:15:02,369 Epoch 5394: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-02 06:15:02,370 EPOCH 5395
2024-02-02 06:15:16,218 Epoch 5395: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 06:15:16,218 EPOCH 5396
2024-02-02 06:15:30,007 Epoch 5396: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 06:15:30,007 EPOCH 5397
2024-02-02 06:15:44,059 Epoch 5397: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 06:15:44,060 EPOCH 5398
2024-02-02 06:15:57,662 Epoch 5398: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 06:15:57,662 EPOCH 5399
2024-02-02 06:16:11,721 Epoch 5399: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 06:16:11,721 EPOCH 5400
2024-02-02 06:16:25,710 [Epoch: 5400 Step: 00048600] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:      760 || Batch Translation Loss:   0.030921 => Txt Tokens per Sec:     2110 || Lr: 0.000050
2024-02-02 06:16:25,710 Epoch 5400: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 06:16:25,710 EPOCH 5401
2024-02-02 06:16:39,611 Epoch 5401: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 06:16:39,612 EPOCH 5402
2024-02-02 06:16:53,346 Epoch 5402: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 06:16:53,346 EPOCH 5403
2024-02-02 06:17:07,372 Epoch 5403: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 06:17:07,373 EPOCH 5404
2024-02-02 06:17:21,092 Epoch 5404: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 06:17:21,092 EPOCH 5405
2024-02-02 06:17:35,043 Epoch 5405: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 06:17:35,044 EPOCH 5406
2024-02-02 06:17:49,141 Epoch 5406: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 06:17:49,142 EPOCH 5407
2024-02-02 06:18:03,260 Epoch 5407: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 06:18:03,260 EPOCH 5408
2024-02-02 06:18:17,185 Epoch 5408: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 06:18:17,186 EPOCH 5409
2024-02-02 06:18:31,000 Epoch 5409: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 06:18:31,000 EPOCH 5410
2024-02-02 06:18:45,237 Epoch 5410: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 06:18:45,237 EPOCH 5411
2024-02-02 06:18:59,295 Epoch 5411: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 06:18:59,295 EPOCH 5412
2024-02-02 06:18:59,858 [Epoch: 5412 Step: 00048700] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:     2278 || Batch Translation Loss:   0.014743 => Txt Tokens per Sec:     6665 || Lr: 0.000050
2024-02-02 06:19:12,966 Epoch 5412: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 06:19:12,967 EPOCH 5413
2024-02-02 06:19:27,069 Epoch 5413: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:19:27,069 EPOCH 5414
2024-02-02 06:19:41,108 Epoch 5414: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:19:41,109 EPOCH 5415
2024-02-02 06:19:54,538 Epoch 5415: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:19:54,538 EPOCH 5416
2024-02-02 06:20:08,447 Epoch 5416: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:20:08,447 EPOCH 5417
2024-02-02 06:20:22,292 Epoch 5417: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:20:22,293 EPOCH 5418
2024-02-02 06:20:36,201 Epoch 5418: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:20:36,201 EPOCH 5419
2024-02-02 06:20:50,368 Epoch 5419: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:20:50,369 EPOCH 5420
2024-02-02 06:21:04,185 Epoch 5420: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:21:04,186 EPOCH 5421
2024-02-02 06:21:18,223 Epoch 5421: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:21:18,224 EPOCH 5422
2024-02-02 06:21:32,020 Epoch 5422: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:21:32,020 EPOCH 5423
2024-02-02 06:21:37,374 [Epoch: 5423 Step: 00048800] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:      478 || Batch Translation Loss:   0.020452 => Txt Tokens per Sec:     1559 || Lr: 0.000050
2024-02-02 06:21:45,914 Epoch 5423: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:21:45,915 EPOCH 5424
2024-02-02 06:21:59,752 Epoch 5424: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:21:59,752 EPOCH 5425
2024-02-02 06:22:13,634 Epoch 5425: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:22:13,634 EPOCH 5426
2024-02-02 06:22:27,516 Epoch 5426: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 06:22:27,516 EPOCH 5427
2024-02-02 06:22:41,415 Epoch 5427: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:22:41,415 EPOCH 5428
2024-02-02 06:22:55,281 Epoch 5428: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:22:55,282 EPOCH 5429
2024-02-02 06:23:09,049 Epoch 5429: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:23:09,050 EPOCH 5430
2024-02-02 06:23:22,697 Epoch 5430: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:23:22,698 EPOCH 5431
2024-02-02 06:23:36,604 Epoch 5431: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:23:36,604 EPOCH 5432
2024-02-02 06:23:50,421 Epoch 5432: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:23:50,422 EPOCH 5433
2024-02-02 06:24:04,177 Epoch 5433: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:24:04,178 EPOCH 5434
2024-02-02 06:24:06,818 [Epoch: 5434 Step: 00048900] Batch Recognition Loss:   0.000093 => Gls Tokens per Sec:     1455 || Batch Translation Loss:   0.006506 => Txt Tokens per Sec:     3727 || Lr: 0.000050
2024-02-02 06:24:18,036 Epoch 5434: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:24:18,037 EPOCH 5435
2024-02-02 06:24:31,611 Epoch 5435: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:24:31,612 EPOCH 5436
2024-02-02 06:24:45,749 Epoch 5436: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:24:45,750 EPOCH 5437
2024-02-02 06:24:59,990 Epoch 5437: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:24:59,990 EPOCH 5438
2024-02-02 06:25:14,262 Epoch 5438: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:25:14,262 EPOCH 5439
2024-02-02 06:25:32,514 Epoch 5439: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:25:32,515 EPOCH 5440
2024-02-02 06:25:46,535 Epoch 5440: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:25:46,536 EPOCH 5441
2024-02-02 06:26:00,808 Epoch 5441: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 06:26:00,808 EPOCH 5442
2024-02-02 06:26:14,854 Epoch 5442: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 06:26:14,855 EPOCH 5443
2024-02-02 06:26:28,887 Epoch 5443: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 06:26:28,888 EPOCH 5444
2024-02-02 06:26:42,879 Epoch 5444: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:26:42,880 EPOCH 5445
2024-02-02 06:26:45,832 [Epoch: 5445 Step: 00049000] Batch Recognition Loss:   0.000112 => Gls Tokens per Sec:     1736 || Batch Translation Loss:   0.021067 => Txt Tokens per Sec:     4779 || Lr: 0.000050
2024-02-02 06:26:56,802 Epoch 5445: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:26:56,803 EPOCH 5446
2024-02-02 06:27:10,729 Epoch 5446: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 06:27:10,729 EPOCH 5447
2024-02-02 06:27:24,830 Epoch 5447: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:27:24,830 EPOCH 5448
2024-02-02 06:27:38,723 Epoch 5448: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:27:38,723 EPOCH 5449
2024-02-02 06:27:52,691 Epoch 5449: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:27:52,692 EPOCH 5450
2024-02-02 06:28:06,490 Epoch 5450: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 06:28:06,490 EPOCH 5451
2024-02-02 06:28:20,648 Epoch 5451: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:28:20,649 EPOCH 5452
2024-02-02 06:28:34,724 Epoch 5452: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:28:34,725 EPOCH 5453
2024-02-02 06:28:48,696 Epoch 5453: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:28:48,697 EPOCH 5454
2024-02-02 06:29:02,639 Epoch 5454: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 06:29:02,640 EPOCH 5455
2024-02-02 06:29:16,368 Epoch 5455: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 06:29:16,369 EPOCH 5456
2024-02-02 06:29:25,703 [Epoch: 5456 Step: 00049100] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:      590 || Batch Translation Loss:   0.021515 => Txt Tokens per Sec:     1604 || Lr: 0.000050
2024-02-02 06:29:30,538 Epoch 5456: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 06:29:30,538 EPOCH 5457
2024-02-02 06:29:44,320 Epoch 5457: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:29:44,321 EPOCH 5458
2024-02-02 06:29:58,105 Epoch 5458: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:29:58,105 EPOCH 5459
2024-02-02 06:30:12,035 Epoch 5459: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:30:12,035 EPOCH 5460
2024-02-02 06:30:26,033 Epoch 5460: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:30:26,034 EPOCH 5461
2024-02-02 06:30:39,630 Epoch 5461: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 06:30:39,630 EPOCH 5462
2024-02-02 06:30:53,622 Epoch 5462: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 06:30:53,622 EPOCH 5463
2024-02-02 06:31:07,750 Epoch 5463: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:31:07,750 EPOCH 5464
2024-02-02 06:31:21,513 Epoch 5464: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 06:31:21,514 EPOCH 5465
2024-02-02 06:31:35,672 Epoch 5465: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:31:35,673 EPOCH 5466
2024-02-02 06:31:49,494 Epoch 5466: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:31:49,495 EPOCH 5467
2024-02-02 06:31:57,418 [Epoch: 5467 Step: 00049200] Batch Recognition Loss:   0.000105 => Gls Tokens per Sec:      857 || Batch Translation Loss:   0.013287 => Txt Tokens per Sec:     2205 || Lr: 0.000050
2024-02-02 06:32:03,415 Epoch 5467: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:32:03,415 EPOCH 5468
2024-02-02 06:32:17,097 Epoch 5468: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:32:17,098 EPOCH 5469
2024-02-02 06:32:30,952 Epoch 5469: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 06:32:30,952 EPOCH 5470
2024-02-02 06:32:44,952 Epoch 5470: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:32:44,953 EPOCH 5471
2024-02-02 06:32:58,812 Epoch 5471: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:32:58,813 EPOCH 5472
2024-02-02 06:33:12,955 Epoch 5472: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:33:12,955 EPOCH 5473
2024-02-02 06:33:26,771 Epoch 5473: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:33:26,772 EPOCH 5474
2024-02-02 06:33:40,727 Epoch 5474: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 06:33:40,728 EPOCH 5475
2024-02-02 06:33:54,461 Epoch 5475: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:33:54,462 EPOCH 5476
2024-02-02 06:34:08,083 Epoch 5476: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 06:34:08,084 EPOCH 5477
2024-02-02 06:34:21,917 Epoch 5477: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 06:34:21,918 EPOCH 5478
2024-02-02 06:34:31,499 [Epoch: 5478 Step: 00049300] Batch Recognition Loss:   0.000086 => Gls Tokens per Sec:      842 || Batch Translation Loss:   0.007010 => Txt Tokens per Sec:     2217 || Lr: 0.000050
2024-02-02 06:34:35,834 Epoch 5478: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:34:35,835 EPOCH 5479
2024-02-02 06:34:49,593 Epoch 5479: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:34:49,593 EPOCH 5480
2024-02-02 06:35:03,552 Epoch 5480: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:35:03,552 EPOCH 5481
2024-02-02 06:35:17,231 Epoch 5481: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:35:17,232 EPOCH 5482
2024-02-02 06:35:31,322 Epoch 5482: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 06:35:31,323 EPOCH 5483
2024-02-02 06:35:45,256 Epoch 5483: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 06:35:45,257 EPOCH 5484
2024-02-02 06:35:59,090 Epoch 5484: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 06:35:59,090 EPOCH 5485
2024-02-02 06:36:13,143 Epoch 5485: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 06:36:13,144 EPOCH 5486
2024-02-02 06:36:27,099 Epoch 5486: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 06:36:27,100 EPOCH 5487
2024-02-02 06:36:40,962 Epoch 5487: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 06:36:40,962 EPOCH 5488
2024-02-02 06:36:54,864 Epoch 5488: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 06:36:54,864 EPOCH 5489
2024-02-02 06:37:05,311 [Epoch: 5489 Step: 00049400] Batch Recognition Loss:   0.000151 => Gls Tokens per Sec:      895 || Batch Translation Loss:   0.038059 => Txt Tokens per Sec:     2396 || Lr: 0.000050
2024-02-02 06:37:08,930 Epoch 5489: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 06:37:08,931 EPOCH 5490
2024-02-02 06:37:23,066 Epoch 5490: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 06:37:23,067 EPOCH 5491
2024-02-02 06:37:36,740 Epoch 5491: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 06:37:36,741 EPOCH 5492
2024-02-02 06:37:50,550 Epoch 5492: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 06:37:50,551 EPOCH 5493
2024-02-02 06:38:04,220 Epoch 5493: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 06:38:04,221 EPOCH 5494
2024-02-02 06:38:18,114 Epoch 5494: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 06:38:18,114 EPOCH 5495
2024-02-02 06:38:32,088 Epoch 5495: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 06:38:32,089 EPOCH 5496
2024-02-02 06:38:45,903 Epoch 5496: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 06:38:45,904 EPOCH 5497
2024-02-02 06:38:59,670 Epoch 5497: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 06:38:59,670 EPOCH 5498
2024-02-02 06:39:13,593 Epoch 5498: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 06:39:13,593 EPOCH 5499
2024-02-02 06:39:27,456 Epoch 5499: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 06:39:27,457 EPOCH 5500
2024-02-02 06:39:41,212 [Epoch: 5500 Step: 00049500] Batch Recognition Loss:   0.000120 => Gls Tokens per Sec:      773 || Batch Translation Loss:   0.020435 => Txt Tokens per Sec:     2146 || Lr: 0.000050
2024-02-02 06:39:41,213 Epoch 5500: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 06:39:41,213 EPOCH 5501
2024-02-02 06:39:55,313 Epoch 5501: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 06:39:55,314 EPOCH 5502
2024-02-02 06:40:09,344 Epoch 5502: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 06:40:09,345 EPOCH 5503
2024-02-02 06:40:23,364 Epoch 5503: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 06:40:23,365 EPOCH 5504
2024-02-02 06:40:37,177 Epoch 5504: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 06:40:37,178 EPOCH 5505
2024-02-02 06:40:51,162 Epoch 5505: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 06:40:51,163 EPOCH 5506
2024-02-02 06:41:04,956 Epoch 5506: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.94 
2024-02-02 06:41:04,957 EPOCH 5507
2024-02-02 06:41:18,923 Epoch 5507: Total Training Recognition Loss 0.00  Total Training Translation Loss 5.29 
2024-02-02 06:41:18,924 EPOCH 5508
2024-02-02 06:41:32,971 Epoch 5508: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.07 
2024-02-02 06:41:32,972 EPOCH 5509
2024-02-02 06:41:46,840 Epoch 5509: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.50 
2024-02-02 06:41:46,841 EPOCH 5510
2024-02-02 06:42:00,744 Epoch 5510: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.54 
2024-02-02 06:42:00,744 EPOCH 5511
2024-02-02 06:42:14,570 Epoch 5511: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-02 06:42:14,571 EPOCH 5512
2024-02-02 06:42:14,796 [Epoch: 5512 Step: 00049600] Batch Recognition Loss:   0.000323 => Gls Tokens per Sec:     5714 || Batch Translation Loss:   0.067144 => Txt Tokens per Sec:    10000 || Lr: 0.000050
2024-02-02 06:42:28,268 Epoch 5512: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 06:42:28,268 EPOCH 5513
2024-02-02 06:42:42,235 Epoch 5513: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 06:42:42,236 EPOCH 5514
2024-02-02 06:42:56,104 Epoch 5514: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 06:42:56,104 EPOCH 5515
2024-02-02 06:43:09,839 Epoch 5515: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 06:43:09,839 EPOCH 5516
2024-02-02 06:43:23,432 Epoch 5516: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 06:43:23,433 EPOCH 5517
2024-02-02 06:43:37,241 Epoch 5517: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 06:43:37,242 EPOCH 5518
2024-02-02 06:43:50,876 Epoch 5518: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 06:43:50,876 EPOCH 5519
2024-02-02 06:44:04,888 Epoch 5519: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 06:44:04,889 EPOCH 5520
2024-02-02 06:44:18,673 Epoch 5520: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 06:44:18,674 EPOCH 5521
2024-02-02 06:44:32,592 Epoch 5521: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 06:44:32,593 EPOCH 5522
2024-02-02 06:44:46,400 Epoch 5522: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 06:44:46,400 EPOCH 5523
2024-02-02 06:44:49,752 [Epoch: 5523 Step: 00049700] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:      764 || Batch Translation Loss:   0.011805 => Txt Tokens per Sec:     2087 || Lr: 0.000050
2024-02-02 06:45:00,204 Epoch 5523: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 06:45:00,205 EPOCH 5524
2024-02-02 06:45:14,032 Epoch 5524: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 06:45:14,032 EPOCH 5525
2024-02-02 06:45:28,095 Epoch 5525: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 06:45:28,095 EPOCH 5526
2024-02-02 06:45:41,973 Epoch 5526: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 06:45:41,974 EPOCH 5527
2024-02-02 06:45:55,996 Epoch 5527: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 06:45:55,996 EPOCH 5528
2024-02-02 06:46:09,862 Epoch 5528: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 06:46:09,862 EPOCH 5529
2024-02-02 06:46:23,826 Epoch 5529: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:46:23,826 EPOCH 5530
2024-02-02 06:46:37,618 Epoch 5530: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:46:37,619 EPOCH 5531
2024-02-02 06:46:51,639 Epoch 5531: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:46:51,639 EPOCH 5532
2024-02-02 06:47:05,513 Epoch 5532: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:47:05,513 EPOCH 5533
2024-02-02 06:47:19,196 Epoch 5533: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:47:19,196 EPOCH 5534
2024-02-02 06:47:23,423 [Epoch: 5534 Step: 00049800] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:      698 || Batch Translation Loss:   0.013600 => Txt Tokens per Sec:     1915 || Lr: 0.000050
2024-02-02 06:47:32,895 Epoch 5534: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:47:32,896 EPOCH 5535
2024-02-02 06:47:46,823 Epoch 5535: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:47:46,823 EPOCH 5536
2024-02-02 06:48:00,652 Epoch 5536: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:48:00,652 EPOCH 5537
2024-02-02 06:48:14,578 Epoch 5537: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:48:14,578 EPOCH 5538
2024-02-02 06:48:28,721 Epoch 5538: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:48:28,721 EPOCH 5539
2024-02-02 06:48:42,411 Epoch 5539: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:48:42,412 EPOCH 5540
2024-02-02 06:48:56,280 Epoch 5540: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:48:56,281 EPOCH 5541
2024-02-02 06:49:10,196 Epoch 5541: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:49:10,196 EPOCH 5542
2024-02-02 06:49:23,750 Epoch 5542: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:49:23,751 EPOCH 5543
2024-02-02 06:49:37,628 Epoch 5543: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:49:37,628 EPOCH 5544
2024-02-02 06:49:51,661 Epoch 5544: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:49:51,662 EPOCH 5545
2024-02-02 06:49:56,536 [Epoch: 5545 Step: 00049900] Batch Recognition Loss:   0.000111 => Gls Tokens per Sec:      868 || Batch Translation Loss:   0.011345 => Txt Tokens per Sec:     2425 || Lr: 0.000050
2024-02-02 06:50:05,672 Epoch 5545: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 06:50:05,673 EPOCH 5546
2024-02-02 06:50:19,438 Epoch 5546: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 06:50:19,439 EPOCH 5547
2024-02-02 06:50:33,361 Epoch 5547: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:50:33,362 EPOCH 5548
2024-02-02 06:50:47,218 Epoch 5548: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:50:47,218 EPOCH 5549
2024-02-02 06:51:01,121 Epoch 5549: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:51:01,122 EPOCH 5550
2024-02-02 06:51:15,234 Epoch 5550: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:51:15,235 EPOCH 5551
2024-02-02 06:51:29,181 Epoch 5551: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:51:29,181 EPOCH 5552
2024-02-02 06:51:43,160 Epoch 5552: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:51:43,161 EPOCH 5553
2024-02-02 06:51:57,243 Epoch 5553: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:51:57,243 EPOCH 5554
2024-02-02 06:52:11,212 Epoch 5554: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:52:11,212 EPOCH 5555
2024-02-02 06:52:25,004 Epoch 5555: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 06:52:25,005 EPOCH 5556
2024-02-02 06:52:30,400 [Epoch: 5556 Step: 00050000] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     1186 || Batch Translation Loss:   0.015502 => Txt Tokens per Sec:     3136 || Lr: 0.000050
2024-02-02 06:52:50,813 Validation result at epoch 5556, step    50000: duration: 20.4113s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00040	Translation Loss: 106799.76562	PPL: 43803.01953
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.49	(BLEU-1: 10.39,	BLEU-2: 3.10,	BLEU-3: 1.11,	BLEU-4: 0.49)
	CHRF 16.63	ROUGE 8.74
2024-02-02 06:52:50,814 Logging Recognition and Translation Outputs
2024-02-02 06:52:50,814 ========================================================================================================================
2024-02-02 06:52:50,815 Logging Sequence: 59_58.00
2024-02-02 06:52:50,816 	Gloss Reference :	A B+C+D+E
2024-02-02 06:52:50,816 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 06:52:50,816 	Gloss Alignment :	         
2024-02-02 06:52:50,816 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 06:52:50,817 	Text Reference  :	** to    fix the damage they did not have a       lot of  time      
2024-02-02 06:52:50,817 	Text Hypothesis :	23 years old the ****** **** *** *** **** players who are devastated
2024-02-02 06:52:50,817 	Text Alignment  :	I  S     S       D      D    D   D   D    S       S   S   S         
2024-02-02 06:52:50,817 ========================================================================================================================
2024-02-02 06:52:50,817 Logging Sequence: 165_2.00
2024-02-02 06:52:50,818 	Gloss Reference :	A B+C+D+E
2024-02-02 06:52:50,818 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 06:52:50,818 	Gloss Alignment :	         
2024-02-02 06:52:50,818 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 06:52:50,819 	Text Reference  :	many people believe in  superstitions and think it        brings  good luck   and bad luck 
2024-02-02 06:52:50,819 	Text Hypothesis :	**** in     india   won the           ipl to    celebrate without any  number of  20  overs
2024-02-02 06:52:50,820 	Text Alignment  :	D    S      S       S   S             S   S     S         S       S    S      S   S   S    
2024-02-02 06:52:50,820 ========================================================================================================================
2024-02-02 06:52:50,820 Logging Sequence: 58_147.00
2024-02-02 06:52:50,820 	Gloss Reference :	A B+C+D+E
2024-02-02 06:52:50,820 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 06:52:50,820 	Gloss Alignment :	         
2024-02-02 06:52:50,820 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 06:52:50,821 	Text Reference  :	the women's cricket team grabbed gold by   beating sri lanka in    the finals what a historic win  
2024-02-02 06:52:50,821 	Text Hypothesis :	*** ******* ******* **** let     me   tell you     and know  about the ****** **** * asian    games
2024-02-02 06:52:50,822 	Text Alignment  :	D   D       D       D    S       S    S    S       S   S     S         D      D    D S        S    
2024-02-02 06:52:50,822 ========================================================================================================================
2024-02-02 06:52:50,822 Logging Sequence: 81_139.00
2024-02-02 06:52:50,822 	Gloss Reference :	A B+C+D+E
2024-02-02 06:52:50,822 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 06:52:50,822 	Gloss Alignment :	         
2024-02-02 06:52:50,822 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 06:52:50,824 	Text Reference  :	in 2017 the case was filed first in delhi high    court by rhiti sports   management on       behalf of     dhoni       
2024-02-02 06:52:50,824 	Text Hypothesis :	** now  the **** *** ***** ***** ** ***** supreme court ** ***** canceled the        builder' real   estate registration
2024-02-02 06:52:50,824 	Text Alignment  :	D  S        D    D   D     D     D  D     S             D  D     S        S          S        S      S      S           
2024-02-02 06:52:50,824 ========================================================================================================================
2024-02-02 06:52:50,824 Logging Sequence: 125_72.00
2024-02-02 06:52:50,824 	Gloss Reference :	A B+C+D+E
2024-02-02 06:52:50,824 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 06:52:50,824 	Gloss Alignment :	         
2024-02-02 06:52:50,825 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 06:52:50,826 	Text Reference  :	some      said the pakistani javelineer had milicious intentions of tampering with     the       javelin out of     jealousy
2024-02-02 06:52:50,826 	Text Hypothesis :	yesterday on   the ********* ********** *** ********* ********** ** ********* incident triggered outrage on  social media   
2024-02-02 06:52:50,826 	Text Alignment  :	S         S        D         D          D   D         D          D  D         S        S         S       S   S      S       
2024-02-02 06:52:50,826 ========================================================================================================================
2024-02-02 06:52:59,461 Epoch 5556: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:52:59,462 EPOCH 5557
2024-02-02 06:53:13,045 Epoch 5557: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 06:53:13,045 EPOCH 5558
2024-02-02 06:53:26,979 Epoch 5558: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 06:53:26,980 EPOCH 5559
2024-02-02 06:53:41,123 Epoch 5559: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 06:53:41,123 EPOCH 5560
2024-02-02 06:53:54,768 Epoch 5560: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 06:53:54,769 EPOCH 5561
2024-02-02 06:54:08,801 Epoch 5561: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:54:08,801 EPOCH 5562
2024-02-02 06:54:22,840 Epoch 5562: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:54:22,840 EPOCH 5563
2024-02-02 06:54:36,722 Epoch 5563: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 06:54:36,722 EPOCH 5564
2024-02-02 06:54:50,228 Epoch 5564: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:54:50,228 EPOCH 5565
2024-02-02 06:55:04,290 Epoch 5565: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 06:55:04,291 EPOCH 5566
2024-02-02 06:55:18,282 Epoch 5566: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 06:55:18,283 EPOCH 5567
2024-02-02 06:55:26,609 [Epoch: 5567 Step: 00050100] Batch Recognition Loss:   0.000131 => Gls Tokens per Sec:      815 || Batch Translation Loss:   0.012794 => Txt Tokens per Sec:     2267 || Lr: 0.000050
2024-02-02 06:55:32,204 Epoch 5567: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 06:55:32,205 EPOCH 5568
2024-02-02 06:55:46,169 Epoch 5568: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 06:55:46,170 EPOCH 5569
2024-02-02 06:56:00,191 Epoch 5569: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 06:56:00,191 EPOCH 5570
2024-02-02 06:56:14,070 Epoch 5570: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:56:14,071 EPOCH 5571
2024-02-02 06:56:28,038 Epoch 5571: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 06:56:28,038 EPOCH 5572
2024-02-02 06:56:41,980 Epoch 5572: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 06:56:41,980 EPOCH 5573
2024-02-02 06:56:55,829 Epoch 5573: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 06:56:55,830 EPOCH 5574
2024-02-02 06:57:09,755 Epoch 5574: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 06:57:09,756 EPOCH 5575
2024-02-02 06:57:23,777 Epoch 5575: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 06:57:23,778 EPOCH 5576
2024-02-02 06:57:37,701 Epoch 5576: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 06:57:37,702 EPOCH 5577
2024-02-02 06:57:51,383 Epoch 5577: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 06:57:51,383 EPOCH 5578
2024-02-02 06:58:04,394 [Epoch: 5578 Step: 00050200] Batch Recognition Loss:   0.000100 => Gls Tokens per Sec:      620 || Batch Translation Loss:   0.007548 => Txt Tokens per Sec:     1734 || Lr: 0.000050
2024-02-02 06:58:05,390 Epoch 5578: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:58:05,390 EPOCH 5579
2024-02-02 06:58:19,497 Epoch 5579: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:58:19,497 EPOCH 5580
2024-02-02 06:58:33,182 Epoch 5580: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:58:33,183 EPOCH 5581
2024-02-02 06:58:47,162 Epoch 5581: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:58:47,162 EPOCH 5582
2024-02-02 06:59:01,104 Epoch 5582: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 06:59:01,104 EPOCH 5583
2024-02-02 06:59:15,014 Epoch 5583: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:59:15,015 EPOCH 5584
2024-02-02 06:59:28,328 Epoch 5584: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 06:59:28,328 EPOCH 5585
2024-02-02 06:59:42,281 Epoch 5585: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 06:59:42,281 EPOCH 5586
2024-02-02 06:59:56,010 Epoch 5586: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 06:59:56,010 EPOCH 5587
2024-02-02 07:00:09,996 Epoch 5587: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:00:09,996 EPOCH 5588
2024-02-02 07:00:24,295 Epoch 5588: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:00:24,295 EPOCH 5589
2024-02-02 07:00:34,606 [Epoch: 5589 Step: 00050300] Batch Recognition Loss:   0.000096 => Gls Tokens per Sec:      993 || Batch Translation Loss:   0.007706 => Txt Tokens per Sec:     2727 || Lr: 0.000050
2024-02-02 07:00:37,928 Epoch 5589: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:00:37,929 EPOCH 5590
2024-02-02 07:00:51,788 Epoch 5590: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:00:51,788 EPOCH 5591
2024-02-02 07:01:05,464 Epoch 5591: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:01:05,465 EPOCH 5592
2024-02-02 07:01:19,875 Epoch 5592: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:01:19,876 EPOCH 5593
2024-02-02 07:01:33,916 Epoch 5593: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:01:33,916 EPOCH 5594
2024-02-02 07:01:48,042 Epoch 5594: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:01:48,043 EPOCH 5595
2024-02-02 07:02:02,153 Epoch 5595: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:02:02,154 EPOCH 5596
2024-02-02 07:02:16,341 Epoch 5596: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:02:16,342 EPOCH 5597
2024-02-02 07:02:30,314 Epoch 5597: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:02:30,314 EPOCH 5598
2024-02-02 07:02:44,074 Epoch 5598: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 07:02:44,074 EPOCH 5599
2024-02-02 07:02:58,279 Epoch 5599: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 07:02:58,280 EPOCH 5600
2024-02-02 07:03:12,319 [Epoch: 5600 Step: 00050400] Batch Recognition Loss:   0.000102 => Gls Tokens per Sec:      757 || Batch Translation Loss:   0.046024 => Txt Tokens per Sec:     2102 || Lr: 0.000050
2024-02-02 07:03:12,319 Epoch 5600: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 07:03:12,319 EPOCH 5601
2024-02-02 07:03:25,679 Epoch 5601: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 07:03:25,679 EPOCH 5602
2024-02-02 07:03:40,105 Epoch 5602: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 07:03:40,106 EPOCH 5603
2024-02-02 07:03:54,187 Epoch 5603: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 07:03:54,187 EPOCH 5604
2024-02-02 07:04:08,136 Epoch 5604: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 07:04:08,136 EPOCH 5605
2024-02-02 07:04:21,982 Epoch 5605: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 07:04:21,982 EPOCH 5606
2024-02-02 07:04:36,021 Epoch 5606: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 07:04:36,022 EPOCH 5607
2024-02-02 07:04:49,935 Epoch 5607: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 07:04:49,936 EPOCH 5608
2024-02-02 07:05:03,702 Epoch 5608: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 07:05:03,702 EPOCH 5609
2024-02-02 07:05:17,609 Epoch 5609: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 07:05:17,610 EPOCH 5610
2024-02-02 07:05:31,657 Epoch 5610: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 07:05:31,658 EPOCH 5611
2024-02-02 07:05:45,554 Epoch 5611: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 07:05:45,555 EPOCH 5612
2024-02-02 07:05:48,651 [Epoch: 5612 Step: 00050500] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:      413 || Batch Translation Loss:   0.022541 => Txt Tokens per Sec:     1325 || Lr: 0.000050
2024-02-02 07:05:59,416 Epoch 5612: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 07:05:59,416 EPOCH 5613
2024-02-02 07:06:13,210 Epoch 5613: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 07:06:13,210 EPOCH 5614
2024-02-02 07:06:26,989 Epoch 5614: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 07:06:26,990 EPOCH 5615
2024-02-02 07:06:40,967 Epoch 5615: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 07:06:40,968 EPOCH 5616
2024-02-02 07:06:54,850 Epoch 5616: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:06:54,850 EPOCH 5617
2024-02-02 07:07:08,724 Epoch 5617: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 07:07:08,725 EPOCH 5618
2024-02-02 07:07:22,336 Epoch 5618: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:07:22,336 EPOCH 5619
2024-02-02 07:07:36,164 Epoch 5619: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:07:36,165 EPOCH 5620
2024-02-02 07:07:49,728 Epoch 5620: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:07:49,729 EPOCH 5621
2024-02-02 07:08:03,987 Epoch 5621: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:08:03,988 EPOCH 5622
2024-02-02 07:08:17,805 Epoch 5622: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:08:17,806 EPOCH 5623
2024-02-02 07:08:21,677 [Epoch: 5623 Step: 00050600] Batch Recognition Loss:   0.000100 => Gls Tokens per Sec:      661 || Batch Translation Loss:   0.006402 => Txt Tokens per Sec:     1744 || Lr: 0.000050
2024-02-02 07:08:31,746 Epoch 5623: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:08:31,746 EPOCH 5624
2024-02-02 07:08:45,545 Epoch 5624: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:08:45,545 EPOCH 5625
2024-02-02 07:08:59,392 Epoch 5625: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:08:59,393 EPOCH 5626
2024-02-02 07:09:13,367 Epoch 5626: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:09:13,368 EPOCH 5627
2024-02-02 07:09:26,800 Epoch 5627: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:09:26,800 EPOCH 5628
2024-02-02 07:09:40,731 Epoch 5628: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:09:40,732 EPOCH 5629
2024-02-02 07:09:54,703 Epoch 5629: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:09:54,703 EPOCH 5630
2024-02-02 07:10:08,406 Epoch 5630: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 07:10:08,407 EPOCH 5631
2024-02-02 07:10:22,421 Epoch 5631: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:10:22,422 EPOCH 5632
2024-02-02 07:10:36,172 Epoch 5632: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 07:10:36,172 EPOCH 5633
2024-02-02 07:10:50,070 Epoch 5633: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 07:10:50,070 EPOCH 5634
2024-02-02 07:10:54,428 [Epoch: 5634 Step: 00050700] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:      881 || Batch Translation Loss:   0.019749 => Txt Tokens per Sec:     2363 || Lr: 0.000050
2024-02-02 07:11:03,931 Epoch 5634: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:11:03,932 EPOCH 5635
2024-02-02 07:11:17,943 Epoch 5635: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:11:17,943 EPOCH 5636
2024-02-02 07:11:31,779 Epoch 5636: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:11:31,780 EPOCH 5637
2024-02-02 07:11:45,841 Epoch 5637: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:11:45,841 EPOCH 5638
2024-02-02 07:11:59,851 Epoch 5638: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:11:59,852 EPOCH 5639
2024-02-02 07:12:13,319 Epoch 5639: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:12:13,320 EPOCH 5640
2024-02-02 07:12:27,308 Epoch 5640: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:12:27,308 EPOCH 5641
2024-02-02 07:12:41,540 Epoch 5641: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:12:41,540 EPOCH 5642
2024-02-02 07:12:55,486 Epoch 5642: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:12:55,487 EPOCH 5643
2024-02-02 07:13:09,227 Epoch 5643: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:13:09,228 EPOCH 5644
2024-02-02 07:13:23,125 Epoch 5644: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:13:23,126 EPOCH 5645
2024-02-02 07:13:27,006 [Epoch: 5645 Step: 00050800] Batch Recognition Loss:   0.000091 => Gls Tokens per Sec:     1320 || Batch Translation Loss:   0.011205 => Txt Tokens per Sec:     3360 || Lr: 0.000050
2024-02-02 07:13:36,903 Epoch 5645: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 07:13:36,903 EPOCH 5646
2024-02-02 07:13:50,800 Epoch 5646: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 07:13:50,801 EPOCH 5647
2024-02-02 07:14:04,705 Epoch 5647: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 07:14:04,706 EPOCH 5648
2024-02-02 07:14:18,490 Epoch 5648: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 07:14:18,490 EPOCH 5649
2024-02-02 07:14:32,504 Epoch 5649: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 07:14:32,505 EPOCH 5650
2024-02-02 07:14:46,510 Epoch 5650: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 07:14:46,511 EPOCH 5651
2024-02-02 07:15:00,081 Epoch 5651: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 07:15:00,082 EPOCH 5652
2024-02-02 07:15:14,033 Epoch 5652: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 07:15:14,033 EPOCH 5653
2024-02-02 07:15:28,049 Epoch 5653: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 07:15:28,049 EPOCH 5654
2024-02-02 07:15:41,786 Epoch 5654: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 07:15:41,787 EPOCH 5655
2024-02-02 07:15:55,805 Epoch 5655: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:15:55,805 EPOCH 5656
2024-02-02 07:16:00,499 [Epoch: 5656 Step: 00050900] Batch Recognition Loss:   0.000102 => Gls Tokens per Sec:     1364 || Batch Translation Loss:   0.006632 => Txt Tokens per Sec:     3456 || Lr: 0.000050
2024-02-02 07:16:09,553 Epoch 5656: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:16:09,554 EPOCH 5657
2024-02-02 07:16:23,473 Epoch 5657: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 07:16:23,473 EPOCH 5658
2024-02-02 07:16:37,429 Epoch 5658: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 07:16:37,430 EPOCH 5659
2024-02-02 07:16:51,186 Epoch 5659: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 07:16:51,186 EPOCH 5660
2024-02-02 07:17:05,091 Epoch 5660: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 07:17:05,091 EPOCH 5661
2024-02-02 07:17:19,155 Epoch 5661: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 07:17:19,156 EPOCH 5662
2024-02-02 07:17:32,896 Epoch 5662: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 07:17:32,896 EPOCH 5663
2024-02-02 07:17:46,658 Epoch 5663: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 07:17:46,659 EPOCH 5664
2024-02-02 07:18:00,738 Epoch 5664: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:18:00,739 EPOCH 5665
2024-02-02 07:18:14,743 Epoch 5665: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 07:18:14,744 EPOCH 5666
2024-02-02 07:18:28,778 Epoch 5666: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:18:28,779 EPOCH 5667
2024-02-02 07:18:35,574 [Epoch: 5667 Step: 00051000] Batch Recognition Loss:   0.000106 => Gls Tokens per Sec:      999 || Batch Translation Loss:   0.012034 => Txt Tokens per Sec:     2753 || Lr: 0.000050
2024-02-02 07:18:43,034 Epoch 5667: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:18:43,035 EPOCH 5668
2024-02-02 07:18:57,052 Epoch 5668: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:18:57,052 EPOCH 5669
2024-02-02 07:19:11,141 Epoch 5669: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:19:11,141 EPOCH 5670
2024-02-02 07:19:24,843 Epoch 5670: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:19:24,844 EPOCH 5671
2024-02-02 07:19:38,751 Epoch 5671: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:19:38,751 EPOCH 5672
2024-02-02 07:19:52,597 Epoch 5672: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:19:52,598 EPOCH 5673
2024-02-02 07:20:06,959 Epoch 5673: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:20:06,960 EPOCH 5674
2024-02-02 07:20:20,815 Epoch 5674: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:20:20,816 EPOCH 5675
2024-02-02 07:20:34,656 Epoch 5675: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:20:34,657 EPOCH 5676
2024-02-02 07:20:48,607 Epoch 5676: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:20:48,608 EPOCH 5677
2024-02-02 07:21:02,352 Epoch 5677: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:21:02,353 EPOCH 5678
2024-02-02 07:21:11,270 [Epoch: 5678 Step: 00051100] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:      905 || Batch Translation Loss:   0.008350 => Txt Tokens per Sec:     2417 || Lr: 0.000050
2024-02-02 07:21:16,075 Epoch 5678: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 07:21:16,076 EPOCH 5679
2024-02-02 07:21:30,058 Epoch 5679: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 07:21:30,058 EPOCH 5680
2024-02-02 07:21:44,131 Epoch 5680: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:21:44,132 EPOCH 5681
2024-02-02 07:21:57,987 Epoch 5681: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 07:21:57,988 EPOCH 5682
2024-02-02 07:22:11,783 Epoch 5682: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:22:11,784 EPOCH 5683
2024-02-02 07:22:25,846 Epoch 5683: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 07:22:25,847 EPOCH 5684
2024-02-02 07:22:39,617 Epoch 5684: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:22:39,617 EPOCH 5685
2024-02-02 07:22:53,143 Epoch 5685: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 07:22:53,144 EPOCH 5686
2024-02-02 07:23:06,966 Epoch 5686: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:23:06,966 EPOCH 5687
2024-02-02 07:23:20,850 Epoch 5687: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:23:20,851 EPOCH 5688
2024-02-02 07:23:34,805 Epoch 5688: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:23:34,805 EPOCH 5689
2024-02-02 07:23:48,002 [Epoch: 5689 Step: 00051200] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:      709 || Batch Translation Loss:   0.017099 => Txt Tokens per Sec:     1954 || Lr: 0.000050
2024-02-02 07:23:48,748 Epoch 5689: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:23:48,748 EPOCH 5690
2024-02-02 07:24:02,629 Epoch 5690: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:24:02,630 EPOCH 5691
2024-02-02 07:24:16,421 Epoch 5691: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 07:24:16,422 EPOCH 5692
2024-02-02 07:24:30,091 Epoch 5692: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:24:30,091 EPOCH 5693
2024-02-02 07:24:44,176 Epoch 5693: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 07:24:44,176 EPOCH 5694
2024-02-02 07:24:57,851 Epoch 5694: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 07:24:57,852 EPOCH 5695
2024-02-02 07:25:11,864 Epoch 5695: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 07:25:11,865 EPOCH 5696
2024-02-02 07:25:25,747 Epoch 5696: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 07:25:25,747 EPOCH 5697
2024-02-02 07:25:39,782 Epoch 5697: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 07:25:39,782 EPOCH 5698
2024-02-02 07:25:53,762 Epoch 5698: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 07:25:53,762 EPOCH 5699
2024-02-02 07:26:07,570 Epoch 5699: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 07:26:07,570 EPOCH 5700
2024-02-02 07:26:21,567 [Epoch: 5700 Step: 00051300] Batch Recognition Loss:   0.000096 => Gls Tokens per Sec:      760 || Batch Translation Loss:   0.010061 => Txt Tokens per Sec:     2108 || Lr: 0.000050
2024-02-02 07:26:21,568 Epoch 5700: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 07:26:21,568 EPOCH 5701
2024-02-02 07:26:35,353 Epoch 5701: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 07:26:35,354 EPOCH 5702
2024-02-02 07:26:49,226 Epoch 5702: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 07:26:49,226 EPOCH 5703
2024-02-02 07:27:03,250 Epoch 5703: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 07:27:03,250 EPOCH 5704
2024-02-02 07:27:16,950 Epoch 5704: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 07:27:16,950 EPOCH 5705
2024-02-02 07:27:30,349 Epoch 5705: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 07:27:30,350 EPOCH 5706
2024-02-02 07:27:44,159 Epoch 5706: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 07:27:44,159 EPOCH 5707
2024-02-02 07:27:58,078 Epoch 5707: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 07:27:58,078 EPOCH 5708
2024-02-02 07:28:11,780 Epoch 5708: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-02 07:28:11,780 EPOCH 5709
2024-02-02 07:28:25,395 Epoch 5709: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 07:28:25,395 EPOCH 5710
2024-02-02 07:28:39,196 Epoch 5710: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.00 
2024-02-02 07:28:39,197 EPOCH 5711
2024-02-02 07:28:53,294 Epoch 5711: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-02 07:28:53,295 EPOCH 5712
2024-02-02 07:28:53,608 [Epoch: 5712 Step: 00051400] Batch Recognition Loss:   0.000346 => Gls Tokens per Sec:     4103 || Batch Translation Loss:   0.053159 => Txt Tokens per Sec:     9423 || Lr: 0.000050
2024-02-02 07:29:07,170 Epoch 5712: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.48 
2024-02-02 07:29:07,170 EPOCH 5713
2024-02-02 07:29:21,009 Epoch 5713: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-02 07:29:21,010 EPOCH 5714
2024-02-02 07:29:34,876 Epoch 5714: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-02 07:29:34,876 EPOCH 5715
2024-02-02 07:29:48,495 Epoch 5715: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-02 07:29:48,496 EPOCH 5716
2024-02-02 07:30:02,565 Epoch 5716: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-02 07:30:02,566 EPOCH 5717
2024-02-02 07:30:16,445 Epoch 5717: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 07:30:16,446 EPOCH 5718
2024-02-02 07:30:30,159 Epoch 5718: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 07:30:30,159 EPOCH 5719
2024-02-02 07:30:44,157 Epoch 5719: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 07:30:44,157 EPOCH 5720
2024-02-02 07:30:57,904 Epoch 5720: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 07:30:57,905 EPOCH 5721
2024-02-02 07:31:12,027 Epoch 5721: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 07:31:12,027 EPOCH 5722
2024-02-02 07:31:25,744 Epoch 5722: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 07:31:25,744 EPOCH 5723
2024-02-02 07:31:30,327 [Epoch: 5723 Step: 00051500] Batch Recognition Loss:   0.000278 => Gls Tokens per Sec:      559 || Batch Translation Loss:   0.019206 => Txt Tokens per Sec:     1737 || Lr: 0.000050
2024-02-02 07:31:39,582 Epoch 5723: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 07:31:39,583 EPOCH 5724
2024-02-02 07:31:53,587 Epoch 5724: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 07:31:53,587 EPOCH 5725
2024-02-02 07:32:07,075 Epoch 5725: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 07:32:07,076 EPOCH 5726
2024-02-02 07:32:21,230 Epoch 5726: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 07:32:21,230 EPOCH 5727
2024-02-02 07:32:35,610 Epoch 5727: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 07:32:35,610 EPOCH 5728
2024-02-02 07:32:49,445 Epoch 5728: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 07:32:49,445 EPOCH 5729
2024-02-02 07:33:03,150 Epoch 5729: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 07:33:03,151 EPOCH 5730
2024-02-02 07:33:17,281 Epoch 5730: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 07:33:17,282 EPOCH 5731
2024-02-02 07:33:31,402 Epoch 5731: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 07:33:31,403 EPOCH 5732
2024-02-02 07:33:45,452 Epoch 5732: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:33:45,452 EPOCH 5733
2024-02-02 07:33:59,154 Epoch 5733: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:33:59,155 EPOCH 5734
2024-02-02 07:34:00,083 [Epoch: 5734 Step: 00051600] Batch Recognition Loss:   0.000111 => Gls Tokens per Sec:     4138 || Batch Translation Loss:   0.006296 => Txt Tokens per Sec:     9044 || Lr: 0.000050
2024-02-02 07:34:13,192 Epoch 5734: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:34:13,192 EPOCH 5735
2024-02-02 07:34:27,038 Epoch 5735: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:34:27,039 EPOCH 5736
2024-02-02 07:34:41,037 Epoch 5736: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:34:41,038 EPOCH 5737
2024-02-02 07:34:54,956 Epoch 5737: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:34:54,957 EPOCH 5738
2024-02-02 07:35:08,819 Epoch 5738: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:35:08,819 EPOCH 5739
2024-02-02 07:35:22,789 Epoch 5739: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:35:22,789 EPOCH 5740
2024-02-02 07:35:36,689 Epoch 5740: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:35:36,689 EPOCH 5741
2024-02-02 07:35:50,385 Epoch 5741: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:35:50,385 EPOCH 5742
2024-02-02 07:36:04,253 Epoch 5742: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:36:04,253 EPOCH 5743
2024-02-02 07:36:17,996 Epoch 5743: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:36:17,996 EPOCH 5744
2024-02-02 07:36:32,003 Epoch 5744: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:36:32,004 EPOCH 5745
2024-02-02 07:36:36,779 [Epoch: 5745 Step: 00051700] Batch Recognition Loss:   0.000094 => Gls Tokens per Sec:     1073 || Batch Translation Loss:   0.006977 => Txt Tokens per Sec:     2815 || Lr: 0.000050
2024-02-02 07:36:45,744 Epoch 5745: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:36:45,744 EPOCH 5746
2024-02-02 07:36:59,764 Epoch 5746: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:36:59,765 EPOCH 5747
2024-02-02 07:37:13,425 Epoch 5747: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:37:13,425 EPOCH 5748
2024-02-02 07:37:27,214 Epoch 5748: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:37:27,215 EPOCH 5749
2024-02-02 07:37:40,967 Epoch 5749: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:37:40,967 EPOCH 5750
2024-02-02 07:37:54,711 Epoch 5750: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:37:54,711 EPOCH 5751
2024-02-02 07:38:08,886 Epoch 5751: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:38:08,887 EPOCH 5752
2024-02-02 07:38:23,000 Epoch 5752: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:38:23,000 EPOCH 5753
2024-02-02 07:38:36,667 Epoch 5753: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:38:36,668 EPOCH 5754
2024-02-02 07:38:50,417 Epoch 5754: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:38:50,417 EPOCH 5755
2024-02-02 07:39:04,338 Epoch 5755: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:39:04,339 EPOCH 5756
2024-02-02 07:39:12,250 [Epoch: 5756 Step: 00051800] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:      697 || Batch Translation Loss:   0.016418 => Txt Tokens per Sec:     1848 || Lr: 0.000050
2024-02-02 07:39:18,053 Epoch 5756: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:39:18,053 EPOCH 5757
2024-02-02 07:39:32,030 Epoch 5757: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:39:32,031 EPOCH 5758
2024-02-02 07:39:46,098 Epoch 5758: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:39:46,099 EPOCH 5759
2024-02-02 07:40:00,080 Epoch 5759: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:40:00,081 EPOCH 5760
2024-02-02 07:40:13,838 Epoch 5760: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:40:13,839 EPOCH 5761
2024-02-02 07:40:27,802 Epoch 5761: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:40:27,803 EPOCH 5762
2024-02-02 07:40:41,650 Epoch 5762: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-02 07:40:41,651 EPOCH 5763
2024-02-02 07:40:55,932 Epoch 5763: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:40:55,933 EPOCH 5764
2024-02-02 07:41:09,923 Epoch 5764: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:41:09,923 EPOCH 5765
2024-02-02 07:41:23,799 Epoch 5765: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:41:23,799 EPOCH 5766
2024-02-02 07:41:37,584 Epoch 5766: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:41:37,585 EPOCH 5767
2024-02-02 07:41:46,046 [Epoch: 5767 Step: 00051900] Batch Recognition Loss:   0.000086 => Gls Tokens per Sec:      803 || Batch Translation Loss:   0.009739 => Txt Tokens per Sec:     2103 || Lr: 0.000050
2024-02-02 07:41:51,594 Epoch 5767: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:41:51,595 EPOCH 5768
2024-02-02 07:42:05,483 Epoch 5768: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-02 07:42:05,483 EPOCH 5769
2024-02-02 07:42:19,345 Epoch 5769: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:42:19,346 EPOCH 5770
2024-02-02 07:42:33,476 Epoch 5770: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:42:33,476 EPOCH 5771
2024-02-02 07:42:47,491 Epoch 5771: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:42:47,491 EPOCH 5772
2024-02-02 07:43:01,378 Epoch 5772: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:43:01,378 EPOCH 5773
2024-02-02 07:43:15,414 Epoch 5773: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:43:15,415 EPOCH 5774
2024-02-02 07:43:29,161 Epoch 5774: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:43:29,162 EPOCH 5775
2024-02-02 07:43:42,800 Epoch 5775: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:43:42,801 EPOCH 5776
2024-02-02 07:43:56,753 Epoch 5776: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:43:56,753 EPOCH 5777
2024-02-02 07:44:10,766 Epoch 5777: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:44:10,767 EPOCH 5778
2024-02-02 07:44:17,837 [Epoch: 5778 Step: 00052000] Batch Recognition Loss:   0.000101 => Gls Tokens per Sec:     1267 || Batch Translation Loss:   0.011611 => Txt Tokens per Sec:     3340 || Lr: 0.000050
2024-02-02 07:44:37,445 Validation result at epoch 5778, step    52000: duration: 19.6077s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00031	Translation Loss: 106646.02344	PPL: 43134.28516
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.58	(BLEU-1: 11.18,	BLEU-2: 3.37,	BLEU-3: 1.26,	BLEU-4: 0.58)
	CHRF 17.18	ROUGE 9.30
2024-02-02 07:44:37,447 Logging Recognition and Translation Outputs
2024-02-02 07:44:37,447 ========================================================================================================================
2024-02-02 07:44:37,447 Logging Sequence: 87_229.00
2024-02-02 07:44:37,448 	Gloss Reference :	A B+C+D+E
2024-02-02 07:44:37,448 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 07:44:37,448 	Gloss Alignment :	         
2024-02-02 07:44:37,448 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 07:44:37,449 	Text Reference  :	** ***** *** it  was  not  against dhoni or  kohli
2024-02-02 07:44:37,449 	Text Hypothesis :	in india has now they have used    to    sri lanka
2024-02-02 07:44:37,449 	Text Alignment  :	I  I     I   S   S    S    S       S     S   S    
2024-02-02 07:44:37,449 ========================================================================================================================
2024-02-02 07:44:37,450 Logging Sequence: 134_153.00
2024-02-02 07:44:37,450 	Gloss Reference :	A B+C+D+E
2024-02-02 07:44:37,450 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 07:44:37,450 	Gloss Alignment :	         
2024-02-02 07:44:37,450 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 07:44:37,452 	Text Reference  :	pm modi in his interaction said that deaf athletes must fight for their goals and       never give up    despite  the    losses
2024-02-02 07:44:37,452 	Text Hypothesis :	** **** ** *** *********** **** **** **** ******** **** ***** *** it    was   diagnosed with  a    prime minister anurag thakur
2024-02-02 07:44:37,452 	Text Alignment  :	D  D    D  D   D           D    D    D    D        D    D     D   S     S     S         S     S    S     S        S      S     
2024-02-02 07:44:37,453 ========================================================================================================================
2024-02-02 07:44:37,453 Logging Sequence: 137_155.00
2024-02-02 07:44:37,453 	Gloss Reference :	A B+C+D+E
2024-02-02 07:44:37,453 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 07:44:37,453 	Gloss Alignment :	         
2024-02-02 07:44:37,454 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 07:44:37,455 	Text Reference  :	** **** ******** an   extremely high      tax    named as  sin  tax will  be  applied
2024-02-02 07:44:37,455 	Text Hypothesis :	on 12th november 2022 india     champions trophy he    was held in  qatar and 2022   
2024-02-02 07:44:37,455 	Text Alignment  :	I  I    I        S    S         S         S      S     S   S    S   S     S   S      
2024-02-02 07:44:37,455 ========================================================================================================================
2024-02-02 07:44:37,456 Logging Sequence: 59_18.00
2024-02-02 07:44:37,456 	Gloss Reference :	A B+C+D+E
2024-02-02 07:44:37,456 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 07:44:37,456 	Gloss Alignment :	         
2024-02-02 07:44:37,457 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 07:44:37,458 	Text Reference  :	*** *** 27-year-old jessica fox    from    australia won  a    bronze a        gold medal     in   canoeing
2024-02-02 07:44:37,458 	Text Hypothesis :	all the streets     of      london english football  fans were seen   fighting like hooligans with cops    
2024-02-02 07:44:37,458 	Text Alignment  :	I   I   S           S       S      S       S         S    S    S      S        S    S         S    S       
2024-02-02 07:44:37,459 ========================================================================================================================
2024-02-02 07:44:37,459 Logging Sequence: 173_103.00
2024-02-02 07:44:37,459 	Gloss Reference :	A B+C+D+E
2024-02-02 07:44:37,459 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 07:44:37,459 	Gloss Alignment :	         
2024-02-02 07:44:37,460 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 07:44:37,460 	Text Reference  :	** these rumours are absolutely rubbish
2024-02-02 07:44:37,460 	Text Hypothesis :	in the   end     of  the        month  
2024-02-02 07:44:37,460 	Text Alignment  :	I  S     S       S   S          S      
2024-02-02 07:44:37,460 ========================================================================================================================
2024-02-02 07:44:44,918 Epoch 5778: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:44:44,918 EPOCH 5779
2024-02-02 07:44:58,749 Epoch 5779: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:44:58,749 EPOCH 5780
2024-02-02 07:45:12,678 Epoch 5780: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:45:12,679 EPOCH 5781
2024-02-02 07:45:26,510 Epoch 5781: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:45:26,511 EPOCH 5782
2024-02-02 07:45:40,149 Epoch 5782: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:45:40,150 EPOCH 5783
2024-02-02 07:45:54,262 Epoch 5783: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:45:54,262 EPOCH 5784
2024-02-02 07:46:08,164 Epoch 5784: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-02 07:46:08,165 EPOCH 5785
2024-02-02 07:46:22,358 Epoch 5785: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:46:22,359 EPOCH 5786
2024-02-02 07:46:36,134 Epoch 5786: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:46:36,135 EPOCH 5787
2024-02-02 07:46:50,146 Epoch 5787: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:46:50,147 EPOCH 5788
2024-02-02 07:47:04,026 Epoch 5788: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:47:04,027 EPOCH 5789
2024-02-02 07:47:17,332 [Epoch: 5789 Step: 00052100] Batch Recognition Loss:   0.000106 => Gls Tokens per Sec:      703 || Batch Translation Loss:   0.021124 => Txt Tokens per Sec:     2047 || Lr: 0.000050
2024-02-02 07:47:17,667 Epoch 5789: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 07:47:17,667 EPOCH 5790
2024-02-02 07:47:31,790 Epoch 5790: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:47:31,791 EPOCH 5791
2024-02-02 07:47:45,527 Epoch 5791: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 07:47:45,528 EPOCH 5792
2024-02-02 07:47:59,547 Epoch 5792: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 07:47:59,547 EPOCH 5793
2024-02-02 07:48:13,442 Epoch 5793: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 07:48:13,442 EPOCH 5794
2024-02-02 07:48:27,332 Epoch 5794: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:48:27,332 EPOCH 5795
2024-02-02 07:48:41,174 Epoch 5795: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 07:48:41,175 EPOCH 5796
2024-02-02 07:48:55,193 Epoch 5796: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 07:48:55,193 EPOCH 5797
2024-02-02 07:49:09,078 Epoch 5797: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 07:49:09,079 EPOCH 5798
2024-02-02 07:49:23,071 Epoch 5798: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 07:49:23,072 EPOCH 5799
2024-02-02 07:49:36,798 Epoch 5799: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:49:36,799 EPOCH 5800
2024-02-02 07:49:50,741 [Epoch: 5800 Step: 00052200] Batch Recognition Loss:   0.000093 => Gls Tokens per Sec:      763 || Batch Translation Loss:   0.009196 => Txt Tokens per Sec:     2117 || Lr: 0.000050
2024-02-02 07:49:50,742 Epoch 5800: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:49:50,742 EPOCH 5801
2024-02-02 07:50:04,453 Epoch 5801: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:50:04,453 EPOCH 5802
2024-02-02 07:50:18,379 Epoch 5802: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:50:18,380 EPOCH 5803
2024-02-02 07:50:32,624 Epoch 5803: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:50:32,625 EPOCH 5804
2024-02-02 07:50:46,653 Epoch 5804: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 07:50:46,654 EPOCH 5805
2024-02-02 07:51:00,251 Epoch 5805: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:51:00,251 EPOCH 5806
2024-02-02 07:51:14,196 Epoch 5806: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 07:51:14,196 EPOCH 5807
2024-02-02 07:51:28,120 Epoch 5807: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:51:28,121 EPOCH 5808
2024-02-02 07:51:42,204 Epoch 5808: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:51:42,205 EPOCH 5809
2024-02-02 07:51:56,060 Epoch 5809: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 07:51:56,061 EPOCH 5810
2024-02-02 07:52:10,163 Epoch 5810: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:52:10,164 EPOCH 5811
2024-02-02 07:52:24,136 Epoch 5811: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:52:24,136 EPOCH 5812
2024-02-02 07:52:27,675 [Epoch: 5812 Step: 00052300] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:      362 || Batch Translation Loss:   0.016308 => Txt Tokens per Sec:     1270 || Lr: 0.000050
2024-02-02 07:52:38,046 Epoch 5812: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:52:38,046 EPOCH 5813
2024-02-02 07:52:51,830 Epoch 5813: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:52:51,830 EPOCH 5814
2024-02-02 07:53:05,573 Epoch 5814: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 07:53:05,574 EPOCH 5815
2024-02-02 07:53:19,585 Epoch 5815: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 07:53:19,586 EPOCH 5816
2024-02-02 07:53:33,489 Epoch 5816: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 07:53:33,489 EPOCH 5817
2024-02-02 07:53:47,477 Epoch 5817: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 07:53:47,477 EPOCH 5818
2024-02-02 07:54:01,279 Epoch 5818: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 07:54:01,279 EPOCH 5819
2024-02-02 07:54:15,298 Epoch 5819: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 07:54:15,299 EPOCH 5820
2024-02-02 07:54:28,969 Epoch 5820: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 07:54:28,970 EPOCH 5821
2024-02-02 07:54:42,985 Epoch 5821: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 07:54:42,985 EPOCH 5822
2024-02-02 07:54:57,076 Epoch 5822: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 07:54:57,076 EPOCH 5823
2024-02-02 07:55:00,576 [Epoch: 5823 Step: 00052400] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:      477 || Batch Translation Loss:   0.018444 => Txt Tokens per Sec:     1050 || Lr: 0.000050
2024-02-02 07:55:11,012 Epoch 5823: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 07:55:11,013 EPOCH 5824
2024-02-02 07:55:25,080 Epoch 5824: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-02 07:55:25,081 EPOCH 5825
2024-02-02 07:55:38,991 Epoch 5825: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.99 
2024-02-02 07:55:38,992 EPOCH 5826
2024-02-02 07:55:52,776 Epoch 5826: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.28 
2024-02-02 07:55:52,776 EPOCH 5827
2024-02-02 07:56:06,908 Epoch 5827: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.62 
2024-02-02 07:56:06,908 EPOCH 5828
2024-02-02 07:56:20,970 Epoch 5828: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-02 07:56:20,971 EPOCH 5829
2024-02-02 07:56:34,691 Epoch 5829: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-02 07:56:34,692 EPOCH 5830
2024-02-02 07:56:48,663 Epoch 5830: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-02 07:56:48,664 EPOCH 5831
2024-02-02 07:57:02,621 Epoch 5831: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-02 07:57:02,622 EPOCH 5832
2024-02-02 07:57:16,532 Epoch 5832: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 07:57:16,533 EPOCH 5833
2024-02-02 07:57:30,427 Epoch 5833: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 07:57:30,427 EPOCH 5834
2024-02-02 07:57:35,861 [Epoch: 5834 Step: 00052500] Batch Recognition Loss:   0.000349 => Gls Tokens per Sec:      543 || Batch Translation Loss:   0.029851 => Txt Tokens per Sec:     1552 || Lr: 0.000050
2024-02-02 07:57:44,408 Epoch 5834: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 07:57:44,408 EPOCH 5835
2024-02-02 07:57:58,178 Epoch 5835: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 07:57:58,178 EPOCH 5836
2024-02-02 07:58:12,115 Epoch 5836: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 07:58:12,115 EPOCH 5837
2024-02-02 07:58:26,085 Epoch 5837: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 07:58:26,086 EPOCH 5838
2024-02-02 07:58:39,890 Epoch 5838: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 07:58:39,891 EPOCH 5839
2024-02-02 07:58:54,062 Epoch 5839: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 07:58:54,062 EPOCH 5840
2024-02-02 07:59:07,896 Epoch 5840: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 07:59:07,896 EPOCH 5841
2024-02-02 07:59:21,655 Epoch 5841: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 07:59:21,655 EPOCH 5842
2024-02-02 07:59:35,800 Epoch 5842: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 07:59:35,800 EPOCH 5843
2024-02-02 07:59:49,682 Epoch 5843: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 07:59:49,683 EPOCH 5844
2024-02-02 08:00:03,260 Epoch 5844: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 08:00:03,261 EPOCH 5845
2024-02-02 08:00:08,123 [Epoch: 5845 Step: 00052600] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:     1053 || Batch Translation Loss:   0.010960 => Txt Tokens per Sec:     2915 || Lr: 0.000050
2024-02-02 08:00:17,148 Epoch 5845: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 08:00:17,149 EPOCH 5846
2024-02-02 08:00:31,162 Epoch 5846: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 08:00:31,162 EPOCH 5847
2024-02-02 08:00:44,986 Epoch 5847: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 08:00:44,987 EPOCH 5848
2024-02-02 08:00:58,833 Epoch 5848: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 08:00:58,834 EPOCH 5849
2024-02-02 08:01:12,862 Epoch 5849: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 08:01:12,863 EPOCH 5850
2024-02-02 08:01:26,814 Epoch 5850: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 08:01:26,815 EPOCH 5851
2024-02-02 08:01:40,510 Epoch 5851: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 08:01:40,510 EPOCH 5852
2024-02-02 08:01:54,432 Epoch 5852: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 08:01:54,433 EPOCH 5853
2024-02-02 08:02:08,218 Epoch 5853: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 08:02:08,219 EPOCH 5854
2024-02-02 08:02:22,218 Epoch 5854: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 08:02:22,219 EPOCH 5855
2024-02-02 08:02:36,153 Epoch 5855: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 08:02:36,154 EPOCH 5856
2024-02-02 08:02:42,227 [Epoch: 5856 Step: 00052700] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:      907 || Batch Translation Loss:   0.019493 => Txt Tokens per Sec:     2457 || Lr: 0.000050
2024-02-02 08:02:49,822 Epoch 5856: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 08:02:49,823 EPOCH 5857
2024-02-02 08:03:03,956 Epoch 5857: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 08:03:03,957 EPOCH 5858
2024-02-02 08:03:17,846 Epoch 5858: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 08:03:17,847 EPOCH 5859
2024-02-02 08:03:31,431 Epoch 5859: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 08:03:31,432 EPOCH 5860
2024-02-02 08:03:45,265 Epoch 5860: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 08:03:45,266 EPOCH 5861
2024-02-02 08:03:59,128 Epoch 5861: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 08:03:59,129 EPOCH 5862
2024-02-02 08:04:13,090 Epoch 5862: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 08:04:13,090 EPOCH 5863
2024-02-02 08:04:27,185 Epoch 5863: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 08:04:27,185 EPOCH 5864
2024-02-02 08:04:41,130 Epoch 5864: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 08:04:41,131 EPOCH 5865
2024-02-02 08:04:54,987 Epoch 5865: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 08:04:54,987 EPOCH 5866
2024-02-02 08:05:08,544 Epoch 5866: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 08:05:08,545 EPOCH 5867
2024-02-02 08:05:16,971 [Epoch: 5867 Step: 00052800] Batch Recognition Loss:   0.000079 => Gls Tokens per Sec:      912 || Batch Translation Loss:   0.009158 => Txt Tokens per Sec:     2431 || Lr: 0.000050
2024-02-02 08:05:22,575 Epoch 5867: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 08:05:22,575 EPOCH 5868
2024-02-02 08:05:36,797 Epoch 5868: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 08:05:36,798 EPOCH 5869
2024-02-02 08:05:50,751 Epoch 5869: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 08:05:50,751 EPOCH 5870
2024-02-02 08:06:04,777 Epoch 5870: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 08:06:04,778 EPOCH 5871
2024-02-02 08:06:18,700 Epoch 5871: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 08:06:18,701 EPOCH 5872
2024-02-02 08:06:32,442 Epoch 5872: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 08:06:32,442 EPOCH 5873
2024-02-02 08:06:46,133 Epoch 5873: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 08:06:46,134 EPOCH 5874
2024-02-02 08:06:59,935 Epoch 5874: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 08:06:59,936 EPOCH 5875
2024-02-02 08:07:13,850 Epoch 5875: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 08:07:13,851 EPOCH 5876
2024-02-02 08:07:27,956 Epoch 5876: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 08:07:27,957 EPOCH 5877
2024-02-02 08:07:41,761 Epoch 5877: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 08:07:41,761 EPOCH 5878
2024-02-02 08:07:54,932 [Epoch: 5878 Step: 00052900] Batch Recognition Loss:   0.000104 => Gls Tokens per Sec:      613 || Batch Translation Loss:   0.010959 => Txt Tokens per Sec:     1776 || Lr: 0.000050
2024-02-02 08:07:55,646 Epoch 5878: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 08:07:55,646 EPOCH 5879
2024-02-02 08:08:09,704 Epoch 5879: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 08:08:09,705 EPOCH 5880
2024-02-02 08:08:23,530 Epoch 5880: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 08:08:23,531 EPOCH 5881
2024-02-02 08:08:37,302 Epoch 5881: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 08:08:37,302 EPOCH 5882
2024-02-02 08:08:51,315 Epoch 5882: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-02 08:08:51,316 EPOCH 5883
2024-02-02 08:09:05,272 Epoch 5883: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 08:09:05,272 EPOCH 5884
2024-02-02 08:09:19,109 Epoch 5884: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 08:09:19,110 EPOCH 5885
2024-02-02 08:09:33,404 Epoch 5885: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 08:09:33,405 EPOCH 5886
2024-02-02 08:09:47,308 Epoch 5886: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 08:09:47,309 EPOCH 5887
2024-02-02 08:10:01,269 Epoch 5887: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 08:10:01,270 EPOCH 5888
2024-02-02 08:10:15,056 Epoch 5888: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 08:10:15,056 EPOCH 5889
2024-02-02 08:10:28,448 [Epoch: 5889 Step: 00053000] Batch Recognition Loss:   0.000095 => Gls Tokens per Sec:      698 || Batch Translation Loss:   0.014682 => Txt Tokens per Sec:     2035 || Lr: 0.000050
2024-02-02 08:10:28,691 Epoch 5889: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 08:10:28,691 EPOCH 5890
2024-02-02 08:10:42,547 Epoch 5890: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 08:10:42,548 EPOCH 5891
2024-02-02 08:10:56,179 Epoch 5891: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 08:10:56,180 EPOCH 5892
2024-02-02 08:11:10,095 Epoch 5892: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 08:11:10,096 EPOCH 5893
2024-02-02 08:11:24,141 Epoch 5893: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 08:11:24,142 EPOCH 5894
2024-02-02 08:11:38,139 Epoch 5894: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 08:11:38,140 EPOCH 5895
2024-02-02 08:11:52,083 Epoch 5895: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 08:11:52,084 EPOCH 5896
2024-02-02 08:12:05,887 Epoch 5896: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 08:12:05,888 EPOCH 5897
2024-02-02 08:12:19,767 Epoch 5897: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 08:12:19,768 EPOCH 5898
2024-02-02 08:12:33,620 Epoch 5898: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 08:12:33,620 EPOCH 5899
2024-02-02 08:12:47,562 Epoch 5899: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 08:12:47,563 EPOCH 5900
2024-02-02 08:13:01,444 [Epoch: 5900 Step: 00053100] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:      766 || Batch Translation Loss:   0.022140 => Txt Tokens per Sec:     2126 || Lr: 0.000050
2024-02-02 08:13:01,445 Epoch 5900: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 08:13:01,445 EPOCH 5901
2024-02-02 08:13:15,477 Epoch 5901: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 08:13:15,477 EPOCH 5902
2024-02-02 08:13:29,348 Epoch 5902: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 08:13:29,348 EPOCH 5903
2024-02-02 08:13:43,054 Epoch 5903: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 08:13:43,055 EPOCH 5904
2024-02-02 08:13:56,581 Epoch 5904: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 08:13:56,582 EPOCH 5905
2024-02-02 08:14:10,625 Epoch 5905: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 08:14:10,625 EPOCH 5906
2024-02-02 08:14:24,565 Epoch 5906: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 08:14:24,566 EPOCH 5907
2024-02-02 08:14:38,391 Epoch 5907: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 08:14:38,391 EPOCH 5908
2024-02-02 08:14:52,199 Epoch 5908: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 08:14:52,200 EPOCH 5909
2024-02-02 08:15:06,181 Epoch 5909: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 08:15:06,182 EPOCH 5910
2024-02-02 08:15:20,208 Epoch 5910: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 08:15:20,209 EPOCH 5911
2024-02-02 08:15:34,171 Epoch 5911: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 08:15:34,172 EPOCH 5912
2024-02-02 08:15:34,691 [Epoch: 5912 Step: 00053200] Batch Recognition Loss:   0.000105 => Gls Tokens per Sec:     2471 || Batch Translation Loss:   0.014436 => Txt Tokens per Sec:     6778 || Lr: 0.000050
2024-02-02 08:15:47,917 Epoch 5912: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 08:15:47,918 EPOCH 5913
2024-02-02 08:16:01,711 Epoch 5913: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 08:16:01,711 EPOCH 5914
2024-02-02 08:16:15,699 Epoch 5914: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 08:16:15,700 EPOCH 5915
2024-02-02 08:16:29,550 Epoch 5915: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 08:16:29,550 EPOCH 5916
2024-02-02 08:16:43,155 Epoch 5916: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 08:16:43,156 EPOCH 5917
2024-02-02 08:16:57,146 Epoch 5917: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 08:16:57,146 EPOCH 5918
2024-02-02 08:17:10,891 Epoch 5918: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 08:17:10,891 EPOCH 5919
2024-02-02 08:17:24,614 Epoch 5919: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 08:17:24,615 EPOCH 5920
2024-02-02 08:17:38,352 Epoch 5920: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 08:17:38,353 EPOCH 5921
2024-02-02 08:17:52,314 Epoch 5921: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 08:17:52,315 EPOCH 5922
2024-02-02 08:18:06,148 Epoch 5922: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 08:18:06,149 EPOCH 5923
2024-02-02 08:18:12,510 [Epoch: 5923 Step: 00053300] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:      263 || Batch Translation Loss:   0.007395 => Txt Tokens per Sec:      864 || Lr: 0.000050
2024-02-02 08:18:19,957 Epoch 5923: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 08:18:19,958 EPOCH 5924
2024-02-02 08:18:33,849 Epoch 5924: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 08:18:33,850 EPOCH 5925
2024-02-02 08:18:47,743 Epoch 5925: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 08:18:47,743 EPOCH 5926
2024-02-02 08:19:01,744 Epoch 5926: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 08:19:01,745 EPOCH 5927
2024-02-02 08:19:15,627 Epoch 5927: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 08:19:15,627 EPOCH 5928
2024-02-02 08:19:29,434 Epoch 5928: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 08:19:29,434 EPOCH 5929
2024-02-02 08:19:43,267 Epoch 5929: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 08:19:43,268 EPOCH 5930
2024-02-02 08:19:57,092 Epoch 5930: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 08:19:57,093 EPOCH 5931
2024-02-02 08:20:11,006 Epoch 5931: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 08:20:11,007 EPOCH 5932
2024-02-02 08:20:25,006 Epoch 5932: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 08:20:25,007 EPOCH 5933
2024-02-02 08:20:39,294 Epoch 5933: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 08:20:39,294 EPOCH 5934
2024-02-02 08:20:43,335 [Epoch: 5934 Step: 00053400] Batch Recognition Loss:   0.000096 => Gls Tokens per Sec:      730 || Batch Translation Loss:   0.011744 => Txt Tokens per Sec:     1703 || Lr: 0.000050
2024-02-02 08:20:53,420 Epoch 5934: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 08:20:53,420 EPOCH 5935
2024-02-02 08:21:07,309 Epoch 5935: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 08:21:07,309 EPOCH 5936
2024-02-02 08:21:21,795 Epoch 5936: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 08:21:21,796 EPOCH 5937
2024-02-02 08:21:35,756 Epoch 5937: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 08:21:35,757 EPOCH 5938
2024-02-02 08:21:49,541 Epoch 5938: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 08:21:49,542 EPOCH 5939
2024-02-02 08:22:03,636 Epoch 5939: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 08:22:03,636 EPOCH 5940
2024-02-02 08:22:17,275 Epoch 5940: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 08:22:17,276 EPOCH 5941
2024-02-02 08:22:31,242 Epoch 5941: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-02 08:22:31,243 EPOCH 5942
2024-02-02 08:22:45,096 Epoch 5942: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 08:22:45,096 EPOCH 5943
2024-02-02 08:22:58,892 Epoch 5943: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-02 08:22:58,892 EPOCH 5944
2024-02-02 08:23:12,976 Epoch 5944: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 08:23:12,976 EPOCH 5945
2024-02-02 08:23:17,750 [Epoch: 5945 Step: 00053500] Batch Recognition Loss:   0.000097 => Gls Tokens per Sec:      886 || Batch Translation Loss:   0.011389 => Txt Tokens per Sec:     2477 || Lr: 0.000050
2024-02-02 08:23:26,817 Epoch 5945: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 08:23:26,817 EPOCH 5946
2024-02-02 08:23:40,883 Epoch 5946: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 08:23:40,883 EPOCH 5947
2024-02-02 08:23:54,956 Epoch 5947: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 08:23:54,957 EPOCH 5948
2024-02-02 08:24:08,690 Epoch 5948: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 08:24:08,691 EPOCH 5949
2024-02-02 08:24:22,493 Epoch 5949: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 08:24:22,494 EPOCH 5950
2024-02-02 08:24:36,274 Epoch 5950: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 08:24:36,275 EPOCH 5951
2024-02-02 08:24:49,890 Epoch 5951: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 08:24:49,891 EPOCH 5952
2024-02-02 08:25:03,358 Epoch 5952: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 08:25:03,359 EPOCH 5953
2024-02-02 08:25:17,402 Epoch 5953: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 08:25:17,402 EPOCH 5954
2024-02-02 08:25:31,498 Epoch 5954: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 08:25:31,499 EPOCH 5955
2024-02-02 08:25:45,387 Epoch 5955: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 08:25:45,387 EPOCH 5956
2024-02-02 08:25:51,864 [Epoch: 5956 Step: 00053600] Batch Recognition Loss:   0.000102 => Gls Tokens per Sec:      851 || Batch Translation Loss:   0.013716 => Txt Tokens per Sec:     2384 || Lr: 0.000050
2024-02-02 08:25:59,157 Epoch 5956: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 08:25:59,157 EPOCH 5957
2024-02-02 08:26:13,141 Epoch 5957: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 08:26:13,142 EPOCH 5958
2024-02-02 08:26:27,222 Epoch 5958: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 08:26:27,222 EPOCH 5959
2024-02-02 08:26:41,002 Epoch 5959: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 08:26:41,002 EPOCH 5960
2024-02-02 08:26:54,754 Epoch 5960: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 08:26:54,754 EPOCH 5961
2024-02-02 08:27:08,627 Epoch 5961: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 08:27:08,627 EPOCH 5962
2024-02-02 08:27:22,453 Epoch 5962: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-02 08:27:22,453 EPOCH 5963
2024-02-02 08:27:36,370 Epoch 5963: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 08:27:36,370 EPOCH 5964
2024-02-02 08:27:50,303 Epoch 5964: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 08:27:50,303 EPOCH 5965
2024-02-02 08:28:04,069 Epoch 5965: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 08:28:04,069 EPOCH 5966
2024-02-02 08:28:18,182 Epoch 5966: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 08:28:18,182 EPOCH 5967
2024-02-02 08:28:24,924 [Epoch: 5967 Step: 00053700] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:     1007 || Batch Translation Loss:   0.040631 => Txt Tokens per Sec:     2770 || Lr: 0.000050
2024-02-02 08:28:32,031 Epoch 5967: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 08:28:32,032 EPOCH 5968
2024-02-02 08:28:46,369 Epoch 5968: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 08:28:46,369 EPOCH 5969
2024-02-02 08:29:00,222 Epoch 5969: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 08:29:00,223 EPOCH 5970
2024-02-02 08:29:14,160 Epoch 5970: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 08:29:14,161 EPOCH 5971
2024-02-02 08:29:27,959 Epoch 5971: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 08:29:27,960 EPOCH 5972
2024-02-02 08:29:41,869 Epoch 5972: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 08:29:41,869 EPOCH 5973
2024-02-02 08:29:55,604 Epoch 5973: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 08:29:55,605 EPOCH 5974
2024-02-02 08:30:09,798 Epoch 5974: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 08:30:09,799 EPOCH 5975
2024-02-02 08:30:23,518 Epoch 5975: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 08:30:23,519 EPOCH 5976
2024-02-02 08:30:37,475 Epoch 5976: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 08:30:37,476 EPOCH 5977
2024-02-02 08:30:51,422 Epoch 5977: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 08:30:51,423 EPOCH 5978
2024-02-02 08:31:01,287 [Epoch: 5978 Step: 00053800] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:      818 || Batch Translation Loss:   0.021429 => Txt Tokens per Sec:     2237 || Lr: 0.000050
2024-02-02 08:31:05,364 Epoch 5978: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 08:31:05,365 EPOCH 5979
2024-02-02 08:31:19,219 Epoch 5979: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 08:31:19,220 EPOCH 5980
2024-02-02 08:31:33,069 Epoch 5980: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 08:31:33,069 EPOCH 5981
2024-02-02 08:31:46,907 Epoch 5981: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 08:31:46,907 EPOCH 5982
2024-02-02 08:32:00,808 Epoch 5982: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 08:32:00,809 EPOCH 5983
2024-02-02 08:32:14,912 Epoch 5983: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 08:32:14,913 EPOCH 5984
2024-02-02 08:32:28,718 Epoch 5984: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 08:32:28,718 EPOCH 5985
2024-02-02 08:32:42,511 Epoch 5985: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 08:32:42,511 EPOCH 5986
2024-02-02 08:32:56,439 Epoch 5986: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 08:32:56,440 EPOCH 5987
2024-02-02 08:33:10,258 Epoch 5987: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-02 08:33:10,258 EPOCH 5988
2024-02-02 08:33:24,080 Epoch 5988: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-02 08:33:24,080 EPOCH 5989
2024-02-02 08:33:37,587 [Epoch: 5989 Step: 00053900] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:      692 || Batch Translation Loss:   0.015537 => Txt Tokens per Sec:     1924 || Lr: 0.000050
2024-02-02 08:33:38,037 Epoch 5989: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-02 08:33:38,037 EPOCH 5990
2024-02-02 08:33:51,778 Epoch 5990: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 08:33:51,779 EPOCH 5991
2024-02-02 08:34:05,750 Epoch 5991: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 08:34:05,751 EPOCH 5992
2024-02-02 08:34:19,962 Epoch 5992: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 08:34:19,963 EPOCH 5993
2024-02-02 08:34:33,792 Epoch 5993: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 08:34:33,792 EPOCH 5994
2024-02-02 08:34:47,521 Epoch 5994: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-02 08:34:47,522 EPOCH 5995
2024-02-02 08:35:01,557 Epoch 5995: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 08:35:01,557 EPOCH 5996
2024-02-02 08:35:15,501 Epoch 5996: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 08:35:15,501 EPOCH 5997
2024-02-02 08:35:29,231 Epoch 5997: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 08:35:29,232 EPOCH 5998
2024-02-02 08:35:43,358 Epoch 5998: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 08:35:43,358 EPOCH 5999
2024-02-02 08:35:57,312 Epoch 5999: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 08:35:57,312 EPOCH 6000
2024-02-02 08:36:11,245 [Epoch: 6000 Step: 00054000] Batch Recognition Loss:   0.000208 => Gls Tokens per Sec:      763 || Batch Translation Loss:   0.025632 => Txt Tokens per Sec:     2118 || Lr: 0.000050
2024-02-02 08:36:30,555 Validation result at epoch 6000, step    54000: duration: 19.3092s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00065	Translation Loss: 108155.46094	PPL: 50167.46484
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.68	(BLEU-1: 10.81,	BLEU-2: 3.30,	BLEU-3: 1.36,	BLEU-4: 0.68)
	CHRF 16.86	ROUGE 9.03
2024-02-02 08:36:30,556 Logging Recognition and Translation Outputs
2024-02-02 08:36:30,557 ========================================================================================================================
2024-02-02 08:36:30,557 Logging Sequence: 130_139.00
2024-02-02 08:36:30,557 	Gloss Reference :	A B+C+D+E
2024-02-02 08:36:30,558 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 08:36:30,558 	Gloss Alignment :	         
2024-02-02 08:36:30,558 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 08:36:30,561 	Text Reference  :	he shared a ***** picture of   a   little pouch he knit for his olympic gold medal with uk flag on one side  and     japanese flag    on the ******* other
2024-02-02 08:36:30,561 	Text Hypothesis :	he played a diver but     what the age    old   he **** *** *** ******* **** ***** **** ** **** ** *** still removed it       because of the beijing china
2024-02-02 08:36:30,561 	Text Alignment  :	   S        I     S       S    S   S      S        D    D   D   D       D    D     D    D  D    D  D   S     S       S        S       S      I       S    
2024-02-02 08:36:30,561 ========================================================================================================================
2024-02-02 08:36:30,561 Logging Sequence: 148_155.00
2024-02-02 08:36:30,562 	Gloss Reference :	A B+C+D+E
2024-02-02 08:36:30,562 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 08:36:30,562 	Gloss Alignment :	         
2024-02-02 08:36:30,562 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 08:36:30,564 	Text Reference  :	india *** won the **** *** ****** ** **** ***** match with 263     balls remaining and without losing any wicket
2024-02-02 08:36:30,564 	Text Hypothesis :	india has won the toss and choose to bowl first time  in   however now   pakistan  or  playing in     20  overs 
2024-02-02 08:36:30,564 	Text Alignment  :	      I           I    I   I      I  I    I     S     S    S       S     S         S   S       S      S   S     
2024-02-02 08:36:30,564 ========================================================================================================================
2024-02-02 08:36:30,564 Logging Sequence: 126_99.00
2024-02-02 08:36:30,564 	Gloss Reference :	A B+C+D+E
2024-02-02 08:36:30,565 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 08:36:30,565 	Gloss Alignment :	         
2024-02-02 08:36:30,565 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 08:36:30,565 	Text Reference  :	he        dedicated the medal to sprinter milkha singh
2024-02-02 08:36:30,565 	Text Hypothesis :	yesterday on        the age   of 6        crore  whoa 
2024-02-02 08:36:30,566 	Text Alignment  :	S         S             S     S  S        S      S    
2024-02-02 08:36:30,566 ========================================================================================================================
2024-02-02 08:36:30,566 Logging Sequence: 149_77.00
2024-02-02 08:36:30,566 	Gloss Reference :	A B+C+D+E
2024-02-02 08:36:30,566 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 08:36:30,566 	Gloss Alignment :	         
2024-02-02 08:36:30,567 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 08:36:30,568 	Text Reference  :	and arrested danushka for   alleged sexual assault of  a   29  year old    woman whose name has not ***** been disclosed
2024-02-02 08:36:30,568 	Text Hypothesis :	*** ******** ******** since there   was    no      one who has no   longer india as    he   is  not loved to   play     
2024-02-02 08:36:30,569 	Text Alignment  :	D   D        D        S     S       S      S       S   S   S   S    S      S     S     S    S       I     S    S        
2024-02-02 08:36:30,569 ========================================================================================================================
2024-02-02 08:36:30,569 Logging Sequence: 168_15.00
2024-02-02 08:36:30,569 	Gloss Reference :	A B+C+D+E
2024-02-02 08:36:30,569 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 08:36:30,569 	Gloss Alignment :	         
2024-02-02 08:36:30,569 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 08:36:30,570 	Text Reference  :	when in public the   couple are    always approached for  photographys and autographs
2024-02-02 08:36:30,570 	Text Hypothesis :	**** ** ****** india and    people were   present    that in           the league    
2024-02-02 08:36:30,571 	Text Alignment  :	D    D  D      S     S      S      S      S          S    S            S   S         
2024-02-02 08:36:30,571 ========================================================================================================================
2024-02-02 08:36:30,575 Training ended since there were no improvements inthe last learning rate step: 0.000050
2024-02-02 08:36:30,576 Best validation result at step     2000:   1.14 eval_metric.
2024-02-02 08:37:00,981 ------------------------------------------------------------
2024-02-02 08:37:00,982 [DEV] partition [RECOGNITION] experiment [BW]: 1
2024-02-02 08:37:27,551 finished in 26.5686s 
2024-02-02 08:37:27,552 ************************************************************
2024-02-02 08:37:27,552 [DEV] partition [RECOGNITION] results:
	New Best CTC Decode Beam Size: 1
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
2024-02-02 08:37:27,552 ************************************************************
2024-02-02 08:37:27,553 ------------------------------------------------------------
2024-02-02 08:37:27,553 [DEV] partition [RECOGNITION] experiment [BW]: 2
2024-02-02 08:37:50,639 finished in 23.0864s 
2024-02-02 08:37:50,640 ------------------------------------------------------------
2024-02-02 08:37:50,640 [DEV] partition [RECOGNITION] experiment [BW]: 3
2024-02-02 08:38:10,858 finished in 20.2177s 
2024-02-02 08:38:10,859 ------------------------------------------------------------
2024-02-02 08:38:10,859 [DEV] partition [RECOGNITION] experiment [BW]: 4
2024-02-02 08:38:30,031 finished in 19.1725s 
2024-02-02 08:38:30,032 ------------------------------------------------------------
2024-02-02 08:38:30,032 [DEV] partition [RECOGNITION] experiment [BW]: 5
2024-02-02 08:38:49,001 finished in 18.9689s 
2024-02-02 08:38:49,002 ------------------------------------------------------------
2024-02-02 08:38:49,002 [DEV] partition [RECOGNITION] experiment [BW]: 6
2024-02-02 08:39:07,629 finished in 18.6257s 
2024-02-02 08:39:07,629 ------------------------------------------------------------
2024-02-02 08:39:07,629 [DEV] partition [RECOGNITION] experiment [BW]: 7
2024-02-02 08:39:26,387 finished in 18.7581s 
2024-02-02 08:39:26,388 ------------------------------------------------------------
2024-02-02 08:39:26,388 [DEV] partition [RECOGNITION] experiment [BW]: 8
2024-02-02 08:39:45,156 finished in 18.7682s 
2024-02-02 08:39:45,157 ------------------------------------------------------------
2024-02-02 08:39:45,157 [DEV] partition [RECOGNITION] experiment [BW]: 9
2024-02-02 08:40:03,927 finished in 18.7695s 
2024-02-02 08:40:03,928 ------------------------------------------------------------
2024-02-02 08:40:03,928 [DEV] partition [RECOGNITION] experiment [BW]: 10
2024-02-02 08:40:23,083 finished in 19.1555s 
2024-02-02 08:40:23,084 ============================================================
2024-02-02 08:40:42,361 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 1 and Alpha: -1
	BLEU-4 1.14	(BLEU-1: 12.71,	BLEU-2: 4.82,	BLEU-3: 2.14,	BLEU-4: 1.14)
	CHRF 17.87	ROUGE 11.04
2024-02-02 08:40:42,362 ------------------------------------------------------------
2024-02-02 08:51:43,508 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 5 and Alpha: 1
	BLEU-4 1.20	(BLEU-1: 11.73,	BLEU-2: 4.67,	BLEU-3: 2.23,	BLEU-4: 1.20)
	CHRF 16.88	ROUGE 10.20
2024-02-02 08:51:43,509 ------------------------------------------------------------
2024-02-02 08:55:16,669 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 6 and Alpha: 1
	BLEU-4 1.24	(BLEU-1: 11.77,	BLEU-2: 4.69,	BLEU-3: 2.30,	BLEU-4: 1.24)
	CHRF 16.84	ROUGE 10.16
2024-02-02 08:55:16,670 ------------------------------------------------------------
2024-02-02 08:59:35,395 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 7 and Alpha: 1
	BLEU-4 1.33	(BLEU-1: 11.69,	BLEU-2: 4.73,	BLEU-3: 2.36,	BLEU-4: 1.33)
	CHRF 16.93	ROUGE 9.96
2024-02-02 08:59:35,395 ------------------------------------------------------------
2024-02-02 09:07:19,977 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 8 and Alpha: 1
	BLEU-4 1.37	(BLEU-1: 11.78,	BLEU-2: 4.76,	BLEU-3: 2.41,	BLEU-4: 1.37)
	CHRF 16.91	ROUGE 10.01
2024-02-02 09:07:19,977 ------------------------------------------------------------
2024-02-02 09:28:52,178 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 10 and Alpha: 1
	BLEU-4 1.37	(BLEU-1: 11.66,	BLEU-2: 4.59,	BLEU-3: 2.33,	BLEU-4: 1.37)
	CHRF 16.69	ROUGE 9.87
2024-02-02 09:28:52,179 ------------------------------------------------------------
2024-02-02 09:32:51,485 ************************************************************
2024-02-02 09:32:51,485 [DEV] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 1
	Best Translation Beam Size: 10 and Alpha: 1
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 1.37	(BLEU-1: 11.66,	BLEU-2: 4.59,	BLEU-3: 2.33,	BLEU-4: 1.37)
	CHRF 16.69	ROUGE 9.87
2024-02-02 09:32:51,486 ************************************************************
2024-02-02 09:33:43,915 [TEST] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 1
	Best Translation Beam Size: 10 and Alpha: 1
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 1.02	(BLEU-1: 10.88,	BLEU-2: 4.13,	BLEU-3: 1.92,	BLEU-4: 1.02)
	CHRF 16.40	ROUGE 9.33
2024-02-02 09:33:43,916 ************************************************************
