2024-02-04 07:17:06,922 Hello! This is Joey-NMT.
2024-02-04 07:17:06,937 Total params: 25639944
2024-02-04 07:17:06,937 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'gloss_output_layer.bias', 'gloss_output_layer.weight', 'sgn_embed.ln.bias', 'sgn_embed.ln.weight', 'sgn_embed.norm.norm.bias', 'sgn_embed.norm.norm.weight', 'txt_embed.norm.norm.bias', 'txt_embed.norm.norm.weight']
2024-02-04 07:17:07,909 cfg.name                           : sign_experiment
2024-02-04 07:17:07,909 cfg.data.data_path                 : ./data/Sports_dataset/9/
2024-02-04 07:17:07,909 cfg.data.version                   : phoenix_2014_trans
2024-02-04 07:17:07,909 cfg.data.sgn                       : sign
2024-02-04 07:17:07,909 cfg.data.txt                       : text
2024-02-04 07:17:07,910 cfg.data.gls                       : gloss
2024-02-04 07:17:07,910 cfg.data.train                     : excel_data.train
2024-02-04 07:17:07,910 cfg.data.dev                       : excel_data.dev
2024-02-04 07:17:07,910 cfg.data.test                      : excel_data.test
2024-02-04 07:17:07,910 cfg.data.feature_size              : 2560
2024-02-04 07:17:07,910 cfg.data.level                     : word
2024-02-04 07:17:07,910 cfg.data.txt_lowercase             : True
2024-02-04 07:17:07,910 cfg.data.max_sent_length           : 500
2024-02-04 07:17:07,911 cfg.data.random_train_subset       : -1
2024-02-04 07:17:07,911 cfg.data.random_dev_subset         : -1
2024-02-04 07:17:07,911 cfg.testing.recognition_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-04 07:17:07,911 cfg.testing.translation_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-04 07:17:07,911 cfg.testing.translation_beam_alphas : [-1, 0, 1, 2, 3, 4, 5]
2024-02-04 07:17:07,911 cfg.training.reset_best_ckpt       : False
2024-02-04 07:17:07,912 cfg.training.reset_scheduler       : False
2024-02-04 07:17:07,912 cfg.training.reset_optimizer       : False
2024-02-04 07:17:07,912 cfg.training.random_seed           : 42
2024-02-04 07:17:07,912 cfg.training.model_dir             : ./sign_sample_model/fold9/32head/64batch
2024-02-04 07:17:07,912 cfg.training.recognition_loss_weight : 1.0
2024-02-04 07:17:07,912 cfg.training.translation_loss_weight : 1.0
2024-02-04 07:17:07,912 cfg.training.eval_metric           : bleu
2024-02-04 07:17:07,912 cfg.training.optimizer             : adam
2024-02-04 07:17:07,913 cfg.training.learning_rate         : 0.0001
2024-02-04 07:17:07,913 cfg.training.batch_size            : 64
2024-02-04 07:17:07,913 cfg.training.num_valid_log         : 5
2024-02-04 07:17:07,913 cfg.training.epochs                : 50000
2024-02-04 07:17:07,913 cfg.training.early_stopping_metric : eval_metric
2024-02-04 07:17:07,913 cfg.training.batch_type            : sentence
2024-02-04 07:17:07,913 cfg.training.translation_normalization : batch
2024-02-04 07:17:07,914 cfg.training.eval_recognition_beam_size : 1
2024-02-04 07:17:07,914 cfg.training.eval_translation_beam_size : 1
2024-02-04 07:17:07,914 cfg.training.eval_translation_beam_alpha : -1
2024-02-04 07:17:07,914 cfg.training.overwrite             : True
2024-02-04 07:17:07,914 cfg.training.shuffle               : True
2024-02-04 07:17:07,914 cfg.training.use_cuda              : True
2024-02-04 07:17:07,914 cfg.training.translation_max_output_length : 40
2024-02-04 07:17:07,915 cfg.training.keep_last_ckpts       : 1
2024-02-04 07:17:07,915 cfg.training.batch_multiplier      : 1
2024-02-04 07:17:07,915 cfg.training.logging_freq          : 100
2024-02-04 07:17:07,915 cfg.training.validation_freq       : 2000
2024-02-04 07:17:07,915 cfg.training.betas                 : [0.9, 0.998]
2024-02-04 07:17:07,915 cfg.training.scheduling            : plateau
2024-02-04 07:17:07,915 cfg.training.learning_rate_min     : 1e-08
2024-02-04 07:17:07,915 cfg.training.weight_decay          : 0.0001
2024-02-04 07:17:07,916 cfg.training.patience              : 12
2024-02-04 07:17:07,916 cfg.training.decrease_factor       : 0.5
2024-02-04 07:17:07,916 cfg.training.label_smoothing       : 0.0
2024-02-04 07:17:07,916 cfg.model.initializer              : xavier
2024-02-04 07:17:07,916 cfg.model.bias_initializer         : zeros
2024-02-04 07:17:07,916 cfg.model.init_gain                : 1.0
2024-02-04 07:17:07,916 cfg.model.embed_initializer        : xavier
2024-02-04 07:17:07,917 cfg.model.embed_init_gain          : 1.0
2024-02-04 07:17:07,917 cfg.model.tied_softmax             : True
2024-02-04 07:17:07,917 cfg.model.encoder.type             : transformer
2024-02-04 07:17:07,917 cfg.model.encoder.num_layers       : 3
2024-02-04 07:17:07,917 cfg.model.encoder.num_heads        : 32
2024-02-04 07:17:07,917 cfg.model.encoder.embeddings.embedding_dim : 512
2024-02-04 07:17:07,917 cfg.model.encoder.embeddings.scale : False
2024-02-04 07:17:07,917 cfg.model.encoder.embeddings.dropout : 0.1
2024-02-04 07:17:07,918 cfg.model.encoder.embeddings.norm_type : batch
2024-02-04 07:17:07,918 cfg.model.encoder.embeddings.activation_type : softsign
2024-02-04 07:17:07,918 cfg.model.encoder.hidden_size      : 512
2024-02-04 07:17:07,918 cfg.model.encoder.ff_size          : 2048
2024-02-04 07:17:07,918 cfg.model.encoder.dropout          : 0.1
2024-02-04 07:17:07,918 cfg.model.decoder.type             : transformer
2024-02-04 07:17:07,918 cfg.model.decoder.num_layers       : 3
2024-02-04 07:17:07,918 cfg.model.decoder.num_heads        : 32
2024-02-04 07:17:07,918 cfg.model.decoder.embeddings.embedding_dim : 512
2024-02-04 07:17:07,919 cfg.model.decoder.embeddings.scale : False
2024-02-04 07:17:07,919 cfg.model.decoder.embeddings.dropout : 0.1
2024-02-04 07:17:07,919 cfg.model.decoder.embeddings.norm_type : batch
2024-02-04 07:17:07,919 cfg.model.decoder.embeddings.activation_type : softsign
2024-02-04 07:17:07,919 cfg.model.decoder.hidden_size      : 512
2024-02-04 07:17:07,919 cfg.model.decoder.ff_size          : 2048
2024-02-04 07:17:07,919 cfg.model.decoder.dropout          : 0.1
2024-02-04 07:17:07,919 Data set sizes: 
	train 2126,
	valid 708,
	test 706
2024-02-04 07:17:07,920 First training example:
	[GLS] A B C D E
	[TXT] although new zealand was disappointed to faltered at the finals against australia they did well throughout the tournament
2024-02-04 07:17:07,920 First 10 words (gls): (0) <si> (1) <unk> (2) <pad> (3) A (4) B (5) C (6) D (7) E
2024-02-04 07:17:07,920 First 10 words (txt): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) and (6) to (7) a (8) in (9) of
2024-02-04 07:17:07,920 Number of unique glosses (types): 8
2024-02-04 07:17:07,920 Number of unique words (types): 4397
2024-02-04 07:17:07,920 SignModel(
	encoder=TransformerEncoder(num_layers=3, num_heads=32),
	decoder=TransformerDecoder(num_layers=3, num_heads=32),
	sgn_embed=SpatialEmbeddings(embedding_dim=512, input_size=2560),
	txt_embed=Embeddings(embedding_dim=512, vocab_size=4397))
2024-02-04 07:17:07,924 EPOCH 1
2024-02-04 07:17:13,324 Epoch   1: Total Training Recognition Loss 147.87  Total Training Translation Loss 3460.04 
2024-02-04 07:17:13,324 EPOCH 2
2024-02-04 07:17:18,040 Epoch   2: Total Training Recognition Loss 21.36  Total Training Translation Loss 3099.74 
2024-02-04 07:17:18,040 EPOCH 3
2024-02-04 07:17:22,574 [Epoch: 003 Step: 00000100] Batch Recognition Loss:   0.104173 => Gls Tokens per Sec:     2204 || Batch Translation Loss:  66.278923 => Txt Tokens per Sec:     6082 || Lr: 0.000100
2024-02-04 07:17:22,881 Epoch   3: Total Training Recognition Loss 5.65  Total Training Translation Loss 3014.45 
2024-02-04 07:17:22,882 EPOCH 4
2024-02-04 07:17:27,645 Epoch   4: Total Training Recognition Loss 1.52  Total Training Translation Loss 2980.93 
2024-02-04 07:17:27,646 EPOCH 5
2024-02-04 07:17:32,549 Epoch   5: Total Training Recognition Loss 0.72  Total Training Translation Loss 2927.88 
2024-02-04 07:17:32,549 EPOCH 6
2024-02-04 07:17:36,155 [Epoch: 006 Step: 00000200] Batch Recognition Loss:   0.008316 => Gls Tokens per Sec:     2593 || Batch Translation Loss:  77.712624 => Txt Tokens per Sec:     7235 || Lr: 0.000100
2024-02-04 07:17:36,751 Epoch   6: Total Training Recognition Loss 0.45  Total Training Translation Loss 2848.58 
2024-02-04 07:17:36,751 EPOCH 7
2024-02-04 07:17:41,604 Epoch   7: Total Training Recognition Loss 0.35  Total Training Translation Loss 2763.02 
2024-02-04 07:17:41,605 EPOCH 8
2024-02-04 07:17:45,968 Epoch   8: Total Training Recognition Loss 0.32  Total Training Translation Loss 2695.15 
2024-02-04 07:17:45,969 EPOCH 9
2024-02-04 07:17:49,965 [Epoch: 009 Step: 00000300] Batch Recognition Loss:   0.009363 => Gls Tokens per Sec:     2243 || Batch Translation Loss:  48.849171 => Txt Tokens per Sec:     6154 || Lr: 0.000100
2024-02-04 07:17:51,034 Epoch   9: Total Training Recognition Loss 0.28  Total Training Translation Loss 2600.96 
2024-02-04 07:17:51,034 EPOCH 10
2024-02-04 07:17:55,437 Epoch  10: Total Training Recognition Loss 0.27  Total Training Translation Loss 2521.89 
2024-02-04 07:17:55,437 EPOCH 11
2024-02-04 07:18:00,277 Epoch  11: Total Training Recognition Loss 0.26  Total Training Translation Loss 2434.19 
2024-02-04 07:18:00,277 EPOCH 12
2024-02-04 07:18:03,390 [Epoch: 012 Step: 00000400] Batch Recognition Loss:   0.005877 => Gls Tokens per Sec:     2595 || Batch Translation Loss:  65.686958 => Txt Tokens per Sec:     7061 || Lr: 0.000100
2024-02-04 07:18:04,532 Epoch  12: Total Training Recognition Loss 0.24  Total Training Translation Loss 2363.63 
2024-02-04 07:18:04,533 EPOCH 13
2024-02-04 07:18:09,175 Epoch  13: Total Training Recognition Loss 0.24  Total Training Translation Loss 2288.89 
2024-02-04 07:18:09,175 EPOCH 14
2024-02-04 07:18:13,670 Epoch  14: Total Training Recognition Loss 0.27  Total Training Translation Loss 2214.64 
2024-02-04 07:18:13,670 EPOCH 15
2024-02-04 07:18:16,967 [Epoch: 015 Step: 00000500] Batch Recognition Loss:   0.005315 => Gls Tokens per Sec:     2330 || Batch Translation Loss:  56.101425 => Txt Tokens per Sec:     6381 || Lr: 0.000100
2024-02-04 07:18:18,458 Epoch  15: Total Training Recognition Loss 0.28  Total Training Translation Loss 2144.75 
2024-02-04 07:18:18,459 EPOCH 16
2024-02-04 07:18:23,429 Epoch  16: Total Training Recognition Loss 0.25  Total Training Translation Loss 2087.27 
2024-02-04 07:18:23,429 EPOCH 17
2024-02-04 07:18:27,663 Epoch  17: Total Training Recognition Loss 0.33  Total Training Translation Loss 2023.11 
2024-02-04 07:18:27,663 EPOCH 18
2024-02-04 07:18:30,476 [Epoch: 018 Step: 00000600] Batch Recognition Loss:   0.018723 => Gls Tokens per Sec:     2415 || Batch Translation Loss:  81.908287 => Txt Tokens per Sec:     6911 || Lr: 0.000100
2024-02-04 07:18:31,832 Epoch  18: Total Training Recognition Loss 0.35  Total Training Translation Loss 1972.92 
2024-02-04 07:18:31,833 EPOCH 19
2024-02-04 07:18:36,714 Epoch  19: Total Training Recognition Loss 0.36  Total Training Translation Loss 1903.10 
2024-02-04 07:18:36,714 EPOCH 20
2024-02-04 07:18:41,171 Epoch  20: Total Training Recognition Loss 0.38  Total Training Translation Loss 1843.85 
2024-02-04 07:18:41,172 EPOCH 21
2024-02-04 07:18:44,081 [Epoch: 021 Step: 00000700] Batch Recognition Loss:   0.010045 => Gls Tokens per Sec:     2116 || Batch Translation Loss:  38.808155 => Txt Tokens per Sec:     6061 || Lr: 0.000100
2024-02-04 07:18:45,935 Epoch  21: Total Training Recognition Loss 0.40  Total Training Translation Loss 1782.81 
2024-02-04 07:18:45,935 EPOCH 22
2024-02-04 07:18:50,491 Epoch  22: Total Training Recognition Loss 0.43  Total Training Translation Loss 1733.28 
2024-02-04 07:18:50,492 EPOCH 23
2024-02-04 07:18:55,153 Epoch  23: Total Training Recognition Loss 0.43  Total Training Translation Loss 1682.19 
2024-02-04 07:18:55,153 EPOCH 24
2024-02-04 07:18:57,542 [Epoch: 024 Step: 00000800] Batch Recognition Loss:   0.005965 => Gls Tokens per Sec:     2309 || Batch Translation Loss:  51.508545 => Txt Tokens per Sec:     6388 || Lr: 0.000100
2024-02-04 07:18:59,841 Epoch  24: Total Training Recognition Loss 0.47  Total Training Translation Loss 1620.23 
2024-02-04 07:18:59,841 EPOCH 25
2024-02-04 07:19:04,639 Epoch  25: Total Training Recognition Loss 0.50  Total Training Translation Loss 1562.56 
2024-02-04 07:19:04,640 EPOCH 26
2024-02-04 07:19:09,391 Epoch  26: Total Training Recognition Loss 0.54  Total Training Translation Loss 1518.01 
2024-02-04 07:19:09,392 EPOCH 27
2024-02-04 07:19:11,596 [Epoch: 027 Step: 00000900] Batch Recognition Loss:   0.013489 => Gls Tokens per Sec:     2211 || Batch Translation Loss:  42.443436 => Txt Tokens per Sec:     6232 || Lr: 0.000100
2024-02-04 07:19:14,285 Epoch  27: Total Training Recognition Loss 0.53  Total Training Translation Loss 1473.98 
2024-02-04 07:19:14,285 EPOCH 28
2024-02-04 07:19:19,109 Epoch  28: Total Training Recognition Loss 0.55  Total Training Translation Loss 1426.44 
2024-02-04 07:19:19,110 EPOCH 29
2024-02-04 07:19:23,394 Epoch  29: Total Training Recognition Loss 0.55  Total Training Translation Loss 1370.52 
2024-02-04 07:19:23,395 EPOCH 30
2024-02-04 07:19:25,365 [Epoch: 030 Step: 00001000] Batch Recognition Loss:   0.023104 => Gls Tokens per Sec:     2276 || Batch Translation Loss:  16.354052 => Txt Tokens per Sec:     6111 || Lr: 0.000100
2024-02-04 07:19:28,276 Epoch  30: Total Training Recognition Loss 0.63  Total Training Translation Loss 1313.77 
2024-02-04 07:19:28,276 EPOCH 31
2024-02-04 07:19:32,352 Epoch  31: Total Training Recognition Loss 0.59  Total Training Translation Loss 1265.50 
2024-02-04 07:19:32,352 EPOCH 32
2024-02-04 07:19:37,247 Epoch  32: Total Training Recognition Loss 0.64  Total Training Translation Loss 1217.81 
2024-02-04 07:19:37,248 EPOCH 33
2024-02-04 07:19:38,696 [Epoch: 033 Step: 00001100] Batch Recognition Loss:   0.020683 => Gls Tokens per Sec:     2481 || Batch Translation Loss:  46.974258 => Txt Tokens per Sec:     6654 || Lr: 0.000100
2024-02-04 07:19:42,178 Epoch  33: Total Training Recognition Loss 0.62  Total Training Translation Loss 1172.79 
2024-02-04 07:19:42,179 EPOCH 34
2024-02-04 07:19:46,720 Epoch  34: Total Training Recognition Loss 0.65  Total Training Translation Loss 1133.89 
2024-02-04 07:19:46,721 EPOCH 35
2024-02-04 07:19:51,398 Epoch  35: Total Training Recognition Loss 0.74  Total Training Translation Loss 1100.27 
2024-02-04 07:19:51,399 EPOCH 36
2024-02-04 07:19:53,012 [Epoch: 036 Step: 00001200] Batch Recognition Loss:   0.025542 => Gls Tokens per Sec:     1830 || Batch Translation Loss:  37.407822 => Txt Tokens per Sec:     5213 || Lr: 0.000100
2024-02-04 07:19:56,223 Epoch  36: Total Training Recognition Loss 0.70  Total Training Translation Loss 1057.16 
2024-02-04 07:19:56,223 EPOCH 37
2024-02-04 07:20:00,443 Epoch  37: Total Training Recognition Loss 0.64  Total Training Translation Loss 999.14 
2024-02-04 07:20:00,444 EPOCH 38
2024-02-04 07:20:05,413 Epoch  38: Total Training Recognition Loss 0.66  Total Training Translation Loss 955.14 
2024-02-04 07:20:05,414 EPOCH 39
2024-02-04 07:20:06,337 [Epoch: 039 Step: 00001300] Batch Recognition Loss:   0.017941 => Gls Tokens per Sec:     2780 || Batch Translation Loss:  26.978214 => Txt Tokens per Sec:     7541 || Lr: 0.000100
2024-02-04 07:20:09,889 Epoch  39: Total Training Recognition Loss 0.66  Total Training Translation Loss 910.41 
2024-02-04 07:20:09,889 EPOCH 40
2024-02-04 07:20:14,660 Epoch  40: Total Training Recognition Loss 0.74  Total Training Translation Loss 897.68 
2024-02-04 07:20:14,660 EPOCH 41
2024-02-04 07:20:19,193 Epoch  41: Total Training Recognition Loss 0.69  Total Training Translation Loss 840.61 
2024-02-04 07:20:19,193 EPOCH 42
2024-02-04 07:20:20,218 [Epoch: 042 Step: 00001400] Batch Recognition Loss:   0.016090 => Gls Tokens per Sec:     1875 || Batch Translation Loss:  17.032543 => Txt Tokens per Sec:     4764 || Lr: 0.000100
2024-02-04 07:20:23,834 Epoch  42: Total Training Recognition Loss 0.67  Total Training Translation Loss 790.03 
2024-02-04 07:20:23,834 EPOCH 43
2024-02-04 07:20:28,543 Epoch  43: Total Training Recognition Loss 0.64  Total Training Translation Loss 751.79 
2024-02-04 07:20:28,544 EPOCH 44
2024-02-04 07:20:33,105 Epoch  44: Total Training Recognition Loss 0.65  Total Training Translation Loss 725.11 
2024-02-04 07:20:33,105 EPOCH 45
2024-02-04 07:20:33,599 [Epoch: 045 Step: 00001500] Batch Recognition Loss:   0.029316 => Gls Tokens per Sec:     2088 || Batch Translation Loss:  23.276892 => Txt Tokens per Sec:     5548 || Lr: 0.000100
2024-02-04 07:20:37,913 Epoch  45: Total Training Recognition Loss 0.74  Total Training Translation Loss 693.76 
2024-02-04 07:20:37,914 EPOCH 46
2024-02-04 07:20:42,334 Epoch  46: Total Training Recognition Loss 0.67  Total Training Translation Loss 648.54 
2024-02-04 07:20:42,334 EPOCH 47
2024-02-04 07:20:47,316 Epoch  47: Total Training Recognition Loss 0.64  Total Training Translation Loss 618.99 
2024-02-04 07:20:47,317 EPOCH 48
2024-02-04 07:20:47,558 [Epoch: 048 Step: 00001600] Batch Recognition Loss:   0.018253 => Gls Tokens per Sec:     2667 || Batch Translation Loss:  13.151278 => Txt Tokens per Sec:     7287 || Lr: 0.000100
2024-02-04 07:20:51,730 Epoch  48: Total Training Recognition Loss 0.65  Total Training Translation Loss 586.77 
2024-02-04 07:20:51,730 EPOCH 49
2024-02-04 07:20:56,753 Epoch  49: Total Training Recognition Loss 0.64  Total Training Translation Loss 550.73 
2024-02-04 07:20:56,754 EPOCH 50
2024-02-04 07:21:01,306 [Epoch: 050 Step: 00001700] Batch Recognition Loss:   0.016756 => Gls Tokens per Sec:     2336 || Batch Translation Loss:  19.043316 => Txt Tokens per Sec:     6484 || Lr: 0.000100
2024-02-04 07:21:01,307 Epoch  50: Total Training Recognition Loss 0.61  Total Training Translation Loss 523.31 
2024-02-04 07:21:01,307 EPOCH 51
2024-02-04 07:21:06,173 Epoch  51: Total Training Recognition Loss 0.63  Total Training Translation Loss 503.84 
2024-02-04 07:21:06,174 EPOCH 52
2024-02-04 07:21:10,537 Epoch  52: Total Training Recognition Loss 0.59  Total Training Translation Loss 464.70 
2024-02-04 07:21:10,538 EPOCH 53
2024-02-04 07:21:15,161 [Epoch: 053 Step: 00001800] Batch Recognition Loss:   0.013881 => Gls Tokens per Sec:     2162 || Batch Translation Loss:   7.896895 => Txt Tokens per Sec:     6018 || Lr: 0.000100
2024-02-04 07:21:15,406 Epoch  53: Total Training Recognition Loss 0.59  Total Training Translation Loss 443.46 
2024-02-04 07:21:15,406 EPOCH 54
2024-02-04 07:21:19,913 Epoch  54: Total Training Recognition Loss 0.62  Total Training Translation Loss 422.43 
2024-02-04 07:21:19,914 EPOCH 55
2024-02-04 07:21:24,592 Epoch  55: Total Training Recognition Loss 0.59  Total Training Translation Loss 399.28 
2024-02-04 07:21:24,592 EPOCH 56
2024-02-04 07:21:28,791 [Epoch: 056 Step: 00001900] Batch Recognition Loss:   0.014878 => Gls Tokens per Sec:     2227 || Batch Translation Loss:  10.049990 => Txt Tokens per Sec:     6335 || Lr: 0.000100
2024-02-04 07:21:29,208 Epoch  56: Total Training Recognition Loss 0.56  Total Training Translation Loss 374.00 
2024-02-04 07:21:29,208 EPOCH 57
2024-02-04 07:21:33,756 Epoch  57: Total Training Recognition Loss 0.54  Total Training Translation Loss 351.18 
2024-02-04 07:21:33,756 EPOCH 58
2024-02-04 07:21:38,532 Epoch  58: Total Training Recognition Loss 0.53  Total Training Translation Loss 331.81 
2024-02-04 07:21:38,533 EPOCH 59
2024-02-04 07:21:42,109 [Epoch: 059 Step: 00002000] Batch Recognition Loss:   0.011722 => Gls Tokens per Sec:     2506 || Batch Translation Loss:  10.888443 => Txt Tokens per Sec:     6876 || Lr: 0.000100
2024-02-04 07:21:51,023 Hooray! New best validation result [eval_metric]!
2024-02-04 07:21:51,024 Saving new checkpoint.
2024-02-04 07:21:51,267 Validation result at epoch  59, step     2000: duration: 9.1580s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.04326	Translation Loss: 66587.10938	PPL: 783.19000
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.98	(BLEU-1: 13.32,	BLEU-2: 4.80,	BLEU-3: 2.02,	BLEU-4: 0.98)
	CHRF 17.56	ROUGE 11.25
2024-02-04 07:21:51,267 Logging Recognition and Translation Outputs
2024-02-04 07:21:51,268 ========================================================================================================================
2024-02-04 07:21:51,268 Logging Sequence: 182_115.00
2024-02-04 07:21:51,268 	Gloss Reference :	A B+C+D+E
2024-02-04 07:21:51,268 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:21:51,268 	Gloss Alignment :	         
2024-02-04 07:21:51,268 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:21:51,270 	Text Reference  :	fans are unclear whether yuvraj will  be   returning to    play  test match odi     or     in  t20   leagues from february 2022 
2024-02-04 07:21:51,271 	Text Hypothesis :	you  are ******* ******* ****** aware that lionel    messi messi made the   matches across the world cup     on   the      world
2024-02-04 07:21:51,271 	Text Alignment  :	S        D       D       D      S     S    S         S     S     S    S     S       S      S   S     S       S    S        S    
2024-02-04 07:21:51,271 ========================================================================================================================
2024-02-04 07:21:51,271 Logging Sequence: 140_120.00
2024-02-04 07:21:51,271 	Gloss Reference :	A B+C+D+E
2024-02-04 07:21:51,271 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:21:51,271 	Gloss Alignment :	         
2024-02-04 07:21:51,271 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:21:51,274 	Text Reference  :	but why so       it   is  because pant is      a   talented player and it    will help      encouraging the youth of uttarakhand toward sports
2024-02-04 07:21:51,274 	Text Hypothesis :	*** he  believed that his bag     will qualify for the      world  cup which is   currently at          the ***** ** most        loved  old   
2024-02-04 07:21:51,274 	Text Alignment  :	D   S   S        S    S   S       S    S       S   S        S      S   S     S    S         S               D     D  S           S      S     
2024-02-04 07:21:51,274 ========================================================================================================================
2024-02-04 07:21:51,274 Logging Sequence: 85_36.00
2024-02-04 07:21:51,274 	Gloss Reference :	A B+C+D+E
2024-02-04 07:21:51,275 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:21:51,275 	Gloss Alignment :	         
2024-02-04 07:21:51,275 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:21:51,276 	Text Reference  :	symonds has scored 2 centuries in 26   tests that he *** **** *** played for his country
2024-02-04 07:21:51,276 	Text Hypothesis :	******* *** ****** * i         am very sad   that he has made the game   for 8   crores 
2024-02-04 07:21:51,276 	Text Alignment  :	D       D   D      D S         S  S    S             I   I    I   S          S   S      
2024-02-04 07:21:51,276 ========================================================================================================================
2024-02-04 07:21:51,276 Logging Sequence: 164_100.00
2024-02-04 07:21:51,277 	Gloss Reference :	A B+C+D+E
2024-02-04 07:21:51,277 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:21:51,277 	Gloss Alignment :	         
2024-02-04 07:21:51,277 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:21:51,279 	Text Reference  :	* the   tv rights for broadcasting ipl  matches in  india  for the next 5 years went to star  india for   rs 23575 crore
2024-02-04 07:21:51,279 	Text Hypothesis :	a total of our    our indian       team beat    its rights for the **** * ***** **** ** world cup   match in the   world
2024-02-04 07:21:51,279 	Text Alignment  :	I S     S  S      S   S            S    S       S   S              D    D D     D    D  S     S     S     S  S     S    
2024-02-04 07:21:51,280 ========================================================================================================================
2024-02-04 07:21:51,280 Logging Sequence: 76_79.00
2024-02-04 07:21:51,280 	Gloss Reference :	A B+C+D+E
2024-02-04 07:21:51,280 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:21:51,280 	Gloss Alignment :	         
2024-02-04 07:21:51,280 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:21:51,281 	Text Reference  :	** ** ****** speaking to   ani csk   ceo kasi viswanathan said   
2024-02-04 07:21:51,281 	Text Hypothesis :	it is played by       this ipl there was a    huge        problem
2024-02-04 07:21:51,281 	Text Alignment  :	I  I  I      S        S    S   S     S   S    S           S      
2024-02-04 07:21:51,281 ========================================================================================================================
2024-02-04 07:21:52,238 Epoch  59: Total Training Recognition Loss 0.52  Total Training Translation Loss 305.33 
2024-02-04 07:21:52,238 EPOCH 60
2024-02-04 07:21:57,200 Epoch  60: Total Training Recognition Loss 0.49  Total Training Translation Loss 284.94 
2024-02-04 07:21:57,201 EPOCH 61
2024-02-04 07:22:01,667 Epoch  61: Total Training Recognition Loss 0.46  Total Training Translation Loss 268.17 
2024-02-04 07:22:01,667 EPOCH 62
2024-02-04 07:22:05,452 [Epoch: 062 Step: 00002100] Batch Recognition Loss:   0.011180 => Gls Tokens per Sec:     2133 || Batch Translation Loss:   5.218978 => Txt Tokens per Sec:     5847 || Lr: 0.000100
2024-02-04 07:22:06,529 Epoch  62: Total Training Recognition Loss 0.45  Total Training Translation Loss 251.62 
2024-02-04 07:22:06,529 EPOCH 63
2024-02-04 07:22:10,884 Epoch  63: Total Training Recognition Loss 0.43  Total Training Translation Loss 231.10 
2024-02-04 07:22:10,885 EPOCH 64
2024-02-04 07:22:15,806 Epoch  64: Total Training Recognition Loss 0.42  Total Training Translation Loss 219.79 
2024-02-04 07:22:15,807 EPOCH 65
2024-02-04 07:22:19,003 [Epoch: 065 Step: 00002200] Batch Recognition Loss:   0.014414 => Gls Tokens per Sec:     2325 || Batch Translation Loss:   1.075178 => Txt Tokens per Sec:     6703 || Lr: 0.000100
2024-02-04 07:22:20,135 Epoch  65: Total Training Recognition Loss 0.43  Total Training Translation Loss 215.92 
2024-02-04 07:22:20,135 EPOCH 66
2024-02-04 07:22:25,080 Epoch  66: Total Training Recognition Loss 0.42  Total Training Translation Loss 202.98 
2024-02-04 07:22:25,080 EPOCH 67
2024-02-04 07:22:29,554 Epoch  67: Total Training Recognition Loss 0.40  Total Training Translation Loss 184.86 
2024-02-04 07:22:29,555 EPOCH 68
2024-02-04 07:22:32,942 [Epoch: 068 Step: 00002300] Batch Recognition Loss:   0.010312 => Gls Tokens per Sec:     2005 || Batch Translation Loss:   3.398639 => Txt Tokens per Sec:     5612 || Lr: 0.000100
2024-02-04 07:22:34,458 Epoch  68: Total Training Recognition Loss 0.38  Total Training Translation Loss 168.56 
2024-02-04 07:22:34,458 EPOCH 69
2024-02-04 07:22:39,265 Epoch  69: Total Training Recognition Loss 0.36  Total Training Translation Loss 161.70 
2024-02-04 07:22:39,266 EPOCH 70
2024-02-04 07:22:43,607 Epoch  70: Total Training Recognition Loss 0.35  Total Training Translation Loss 151.09 
2024-02-04 07:22:43,608 EPOCH 71
2024-02-04 07:22:46,228 [Epoch: 071 Step: 00002400] Batch Recognition Loss:   0.010983 => Gls Tokens per Sec:     2347 || Batch Translation Loss:   5.572219 => Txt Tokens per Sec:     6471 || Lr: 0.000100
2024-02-04 07:22:48,351 Epoch  71: Total Training Recognition Loss 0.31  Total Training Translation Loss 140.83 
2024-02-04 07:22:48,352 EPOCH 72
2024-02-04 07:22:52,795 Epoch  72: Total Training Recognition Loss 0.33  Total Training Translation Loss 132.25 
2024-02-04 07:22:52,796 EPOCH 73
2024-02-04 07:22:57,654 Epoch  73: Total Training Recognition Loss 0.30  Total Training Translation Loss 123.12 
2024-02-04 07:22:57,655 EPOCH 74
2024-02-04 07:22:59,982 [Epoch: 074 Step: 00002500] Batch Recognition Loss:   0.006599 => Gls Tokens per Sec:     2369 || Batch Translation Loss:   3.404981 => Txt Tokens per Sec:     6616 || Lr: 0.000100
2024-02-04 07:23:02,037 Epoch  74: Total Training Recognition Loss 0.27  Total Training Translation Loss 117.62 
2024-02-04 07:23:02,037 EPOCH 75
2024-02-04 07:23:07,001 Epoch  75: Total Training Recognition Loss 0.30  Total Training Translation Loss 116.42 
2024-02-04 07:23:07,001 EPOCH 76
2024-02-04 07:23:11,403 Epoch  76: Total Training Recognition Loss 0.28  Total Training Translation Loss 107.27 
2024-02-04 07:23:11,404 EPOCH 77
2024-02-04 07:23:13,662 [Epoch: 077 Step: 00002600] Batch Recognition Loss:   0.005704 => Gls Tokens per Sec:     2157 || Batch Translation Loss:   2.567907 => Txt Tokens per Sec:     5894 || Lr: 0.000100
2024-02-04 07:23:16,267 Epoch  77: Total Training Recognition Loss 0.26  Total Training Translation Loss 98.74 
2024-02-04 07:23:16,267 EPOCH 78
2024-02-04 07:23:20,709 Epoch  78: Total Training Recognition Loss 0.24  Total Training Translation Loss 93.83 
2024-02-04 07:23:20,710 EPOCH 79
2024-02-04 07:23:25,412 Epoch  79: Total Training Recognition Loss 0.24  Total Training Translation Loss 90.16 
2024-02-04 07:23:25,412 EPOCH 80
2024-02-04 07:23:27,107 [Epoch: 080 Step: 00002700] Batch Recognition Loss:   0.006687 => Gls Tokens per Sec:     2498 || Batch Translation Loss:   2.113507 => Txt Tokens per Sec:     6751 || Lr: 0.000100
2024-02-04 07:23:29,989 Epoch  80: Total Training Recognition Loss 0.22  Total Training Translation Loss 83.68 
2024-02-04 07:23:29,989 EPOCH 81
2024-02-04 07:23:34,837 Epoch  81: Total Training Recognition Loss 0.22  Total Training Translation Loss 79.94 
2024-02-04 07:23:34,837 EPOCH 82
2024-02-04 07:23:39,646 Epoch  82: Total Training Recognition Loss 0.21  Total Training Translation Loss 76.63 
2024-02-04 07:23:39,647 EPOCH 83
2024-02-04 07:23:41,329 [Epoch: 083 Step: 00002800] Batch Recognition Loss:   0.007660 => Gls Tokens per Sec:     2286 || Batch Translation Loss:   2.648875 => Txt Tokens per Sec:     6580 || Lr: 0.000100
2024-02-04 07:23:44,357 Epoch  83: Total Training Recognition Loss 0.20  Total Training Translation Loss 72.95 
2024-02-04 07:23:44,357 EPOCH 84
2024-02-04 07:23:49,203 Epoch  84: Total Training Recognition Loss 0.19  Total Training Translation Loss 70.50 
2024-02-04 07:23:49,203 EPOCH 85
2024-02-04 07:23:54,167 Epoch  85: Total Training Recognition Loss 0.20  Total Training Translation Loss 69.65 
2024-02-04 07:23:54,168 EPOCH 86
2024-02-04 07:23:55,990 [Epoch: 086 Step: 00002900] Batch Recognition Loss:   0.004776 => Gls Tokens per Sec:     1757 || Batch Translation Loss:   1.678555 => Txt Tokens per Sec:     5277 || Lr: 0.000100
2024-02-04 07:23:58,996 Epoch  86: Total Training Recognition Loss 0.20  Total Training Translation Loss 66.65 
2024-02-04 07:23:58,996 EPOCH 87
2024-02-04 07:24:03,858 Epoch  87: Total Training Recognition Loss 0.17  Total Training Translation Loss 63.02 
2024-02-04 07:24:03,859 EPOCH 88
2024-02-04 07:24:08,303 Epoch  88: Total Training Recognition Loss 0.17  Total Training Translation Loss 58.70 
2024-02-04 07:24:08,304 EPOCH 89
2024-02-04 07:24:09,128 [Epoch: 089 Step: 00003000] Batch Recognition Loss:   0.003708 => Gls Tokens per Sec:     2809 || Batch Translation Loss:   1.910485 => Txt Tokens per Sec:     7332 || Lr: 0.000100
2024-02-04 07:24:12,951 Epoch  89: Total Training Recognition Loss 0.17  Total Training Translation Loss 56.40 
2024-02-04 07:24:12,951 EPOCH 90
2024-02-04 07:24:17,484 Epoch  90: Total Training Recognition Loss 0.17  Total Training Translation Loss 55.39 
2024-02-04 07:24:17,485 EPOCH 91
2024-02-04 07:24:22,301 Epoch  91: Total Training Recognition Loss 0.15  Total Training Translation Loss 52.31 
2024-02-04 07:24:22,301 EPOCH 92
2024-02-04 07:24:22,846 [Epoch: 092 Step: 00003100] Batch Recognition Loss:   0.004132 => Gls Tokens per Sec:     3534 || Batch Translation Loss:   1.670498 => Txt Tokens per Sec:     8290 || Lr: 0.000100
2024-02-04 07:24:26,653 Epoch  92: Total Training Recognition Loss 0.15  Total Training Translation Loss 50.41 
2024-02-04 07:24:26,653 EPOCH 93
2024-02-04 07:24:31,482 Epoch  93: Total Training Recognition Loss 0.15  Total Training Translation Loss 47.82 
2024-02-04 07:24:31,483 EPOCH 94
2024-02-04 07:24:35,832 Epoch  94: Total Training Recognition Loss 0.15  Total Training Translation Loss 45.58 
2024-02-04 07:24:35,833 EPOCH 95
2024-02-04 07:24:36,734 [Epoch: 095 Step: 00003200] Batch Recognition Loss:   0.003369 => Gls Tokens per Sec:     1424 || Batch Translation Loss:   1.428355 => Txt Tokens per Sec:     4475 || Lr: 0.000100
2024-02-04 07:24:40,763 Epoch  95: Total Training Recognition Loss 0.15  Total Training Translation Loss 45.98 
2024-02-04 07:24:40,763 EPOCH 96
2024-02-04 07:24:45,255 Epoch  96: Total Training Recognition Loss 0.15  Total Training Translation Loss 50.15 
2024-02-04 07:24:45,255 EPOCH 97
2024-02-04 07:24:50,218 Epoch  97: Total Training Recognition Loss 0.15  Total Training Translation Loss 46.44 
2024-02-04 07:24:50,219 EPOCH 98
2024-02-04 07:24:50,471 [Epoch: 098 Step: 00003300] Batch Recognition Loss:   0.003792 => Gls Tokens per Sec:     2550 || Batch Translation Loss:   1.110312 => Txt Tokens per Sec:     6968 || Lr: 0.000100
2024-02-04 07:24:55,025 Epoch  98: Total Training Recognition Loss 0.14  Total Training Translation Loss 43.37 
2024-02-04 07:24:55,025 EPOCH 99
2024-02-04 07:24:59,753 Epoch  99: Total Training Recognition Loss 0.13  Total Training Translation Loss 41.56 
2024-02-04 07:24:59,754 EPOCH 100
2024-02-04 07:25:04,558 [Epoch: 100 Step: 00003400] Batch Recognition Loss:   0.006435 => Gls Tokens per Sec:     2214 || Batch Translation Loss:   1.555442 => Txt Tokens per Sec:     6146 || Lr: 0.000100
2024-02-04 07:25:04,558 Epoch 100: Total Training Recognition Loss 0.13  Total Training Translation Loss 37.68 
2024-02-04 07:25:04,558 EPOCH 101
2024-02-04 07:25:09,135 Epoch 101: Total Training Recognition Loss 0.13  Total Training Translation Loss 36.70 
2024-02-04 07:25:09,136 EPOCH 102
2024-02-04 07:25:13,978 Epoch 102: Total Training Recognition Loss 0.12  Total Training Translation Loss 36.50 
2024-02-04 07:25:13,979 EPOCH 103
2024-02-04 07:25:18,349 [Epoch: 103 Step: 00003500] Batch Recognition Loss:   0.005017 => Gls Tokens per Sec:     2287 || Batch Translation Loss:   1.155789 => Txt Tokens per Sec:     6284 || Lr: 0.000100
2024-02-04 07:25:18,688 Epoch 103: Total Training Recognition Loss 0.11  Total Training Translation Loss 34.31 
2024-02-04 07:25:18,689 EPOCH 104
2024-02-04 07:25:23,656 Epoch 104: Total Training Recognition Loss 0.13  Total Training Translation Loss 33.99 
2024-02-04 07:25:23,656 EPOCH 105
2024-02-04 07:25:28,415 Epoch 105: Total Training Recognition Loss 0.12  Total Training Translation Loss 31.96 
2024-02-04 07:25:28,416 EPOCH 106
2024-02-04 07:25:32,494 [Epoch: 106 Step: 00003600] Batch Recognition Loss:   0.002102 => Gls Tokens per Sec:     2293 || Batch Translation Loss:   0.763301 => Txt Tokens per Sec:     6351 || Lr: 0.000100
2024-02-04 07:25:33,269 Epoch 106: Total Training Recognition Loss 0.12  Total Training Translation Loss 32.15 
2024-02-04 07:25:33,269 EPOCH 107
2024-02-04 07:25:38,240 Epoch 107: Total Training Recognition Loss 0.10  Total Training Translation Loss 28.88 
2024-02-04 07:25:38,240 EPOCH 108
2024-02-04 07:25:43,159 Epoch 108: Total Training Recognition Loss 0.11  Total Training Translation Loss 29.13 
2024-02-04 07:25:43,160 EPOCH 109
2024-02-04 07:25:47,071 [Epoch: 109 Step: 00003700] Batch Recognition Loss:   0.002151 => Gls Tokens per Sec:     2228 || Batch Translation Loss:   0.750402 => Txt Tokens per Sec:     6237 || Lr: 0.000100
2024-02-04 07:25:48,081 Epoch 109: Total Training Recognition Loss 0.09  Total Training Translation Loss 28.05 
2024-02-04 07:25:48,081 EPOCH 110
2024-02-04 07:25:52,880 Epoch 110: Total Training Recognition Loss 0.09  Total Training Translation Loss 28.05 
2024-02-04 07:25:52,881 EPOCH 111
2024-02-04 07:25:57,682 Epoch 111: Total Training Recognition Loss 0.10  Total Training Translation Loss 31.04 
2024-02-04 07:25:57,683 EPOCH 112
2024-02-04 07:26:01,343 [Epoch: 112 Step: 00003800] Batch Recognition Loss:   0.002336 => Gls Tokens per Sec:     2206 || Batch Translation Loss:   0.793339 => Txt Tokens per Sec:     6142 || Lr: 0.000100
2024-02-04 07:26:02,391 Epoch 112: Total Training Recognition Loss 0.09  Total Training Translation Loss 27.95 
2024-02-04 07:26:02,391 EPOCH 113
2024-02-04 07:26:07,205 Epoch 113: Total Training Recognition Loss 0.09  Total Training Translation Loss 25.25 
2024-02-04 07:26:07,206 EPOCH 114
2024-02-04 07:26:11,947 Epoch 114: Total Training Recognition Loss 0.09  Total Training Translation Loss 25.25 
2024-02-04 07:26:11,947 EPOCH 115
2024-02-04 07:26:15,657 [Epoch: 115 Step: 00003900] Batch Recognition Loss:   0.002340 => Gls Tokens per Sec:     2003 || Batch Translation Loss:   1.039244 => Txt Tokens per Sec:     5800 || Lr: 0.000100
2024-02-04 07:26:16,796 Epoch 115: Total Training Recognition Loss 0.08  Total Training Translation Loss 24.81 
2024-02-04 07:26:16,796 EPOCH 116
2024-02-04 07:26:20,999 Epoch 116: Total Training Recognition Loss 0.07  Total Training Translation Loss 24.34 
2024-02-04 07:26:21,000 EPOCH 117
2024-02-04 07:26:25,893 Epoch 117: Total Training Recognition Loss 0.08  Total Training Translation Loss 22.96 
2024-02-04 07:26:25,894 EPOCH 118
2024-02-04 07:26:28,699 [Epoch: 118 Step: 00004000] Batch Recognition Loss:   0.001677 => Gls Tokens per Sec:     2511 || Batch Translation Loss:   0.621000 => Txt Tokens per Sec:     6870 || Lr: 0.000100
2024-02-04 07:26:37,116 Validation result at epoch 118, step     4000: duration: 8.4170s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00486	Translation Loss: 78728.27344	PPL: 2639.49316
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.87	(BLEU-1: 13.19,	BLEU-2: 4.32,	BLEU-3: 1.79,	BLEU-4: 0.87)
	CHRF 17.61	ROUGE 11.33
2024-02-04 07:26:37,117 Logging Recognition and Translation Outputs
2024-02-04 07:26:37,117 ========================================================================================================================
2024-02-04 07:26:37,117 Logging Sequence: 133_173.00
2024-02-04 07:26:37,118 	Gloss Reference :	A B+C+D+E
2024-02-04 07:26:37,118 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:26:37,118 	Gloss Alignment :	         
2024-02-04 07:26:37,118 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:26:37,120 	Text Reference  :	according to sources the  leaders of   the two countries are set to  join  the commentary panel as  well   
2024-02-04 07:26:37,120 	Text Hypothesis :	********* ** pm      modi along   with the *** ********* *** *** ipl there is  one        of    his arrival
2024-02-04 07:26:37,120 	Text Alignment  :	D         D  S       S    S       S        D   D         D   D   S   S     S   S          S     S   S      
2024-02-04 07:26:37,120 ========================================================================================================================
2024-02-04 07:26:37,120 Logging Sequence: 83_33.00
2024-02-04 07:26:37,120 	Gloss Reference :	A B+C+D+E
2024-02-04 07:26:37,120 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:26:37,121 	Gloss Alignment :	         
2024-02-04 07:26:37,121 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:26:37,122 	Text Reference  :	*** a       football *********** ******* ** match lasts   for two equal halves of *** 45      minutes
2024-02-04 07:26:37,122 	Text Hypothesis :	the denmark football association tweeted by 15    minutes and at  the   reason of the denmark team   
2024-02-04 07:26:37,122 	Text Alignment  :	I   S                I           I       I  S     S       S   S   S     S         I   S       S      
2024-02-04 07:26:37,122 ========================================================================================================================
2024-02-04 07:26:37,122 Logging Sequence: 68_147.00
2024-02-04 07:26:37,123 	Gloss Reference :	A B+C+D+E
2024-02-04 07:26:37,123 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:26:37,123 	Gloss Alignment :	         
2024-02-04 07:26:37,123 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:26:37,125 	Text Reference  :	***** ****** remember the 2007 t20     world cup amid a     lot    of    sledging by   english players
2024-02-04 07:26:37,125 	Text Hypothesis :	while bumrah scored   29  runs himself in    the over while stuart broad gave     away 6       balls  
2024-02-04 07:26:37,125 	Text Alignment  :	I     I      S        S   S    S       S     S   S    S     S      S     S        S    S       S      
2024-02-04 07:26:37,125 ========================================================================================================================
2024-02-04 07:26:37,125 Logging Sequence: 165_8.00
2024-02-04 07:26:37,125 	Gloss Reference :	A B+C+D+E
2024-02-04 07:26:37,125 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:26:37,126 	Gloss Alignment :	         
2024-02-04 07:26:37,126 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:26:37,127 	Text Reference  :	** ** however many        don't   believe in  it    it         varies among people
2024-02-04 07:26:37,127 	Text Hypothesis :	he is an      interesting history of      his guru' photograph in     the   world 
2024-02-04 07:26:37,127 	Text Alignment  :	I  I  S       S           S       S       S   S     S          S      S     S     
2024-02-04 07:26:37,127 ========================================================================================================================
2024-02-04 07:26:37,127 Logging Sequence: 119_71.00
2024-02-04 07:26:37,127 	Gloss Reference :	A B+C+D+E
2024-02-04 07:26:37,127 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:26:37,128 	Gloss Alignment :	         
2024-02-04 07:26:37,128 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:26:37,129 	Text Reference  :	the special gold devices  have     each  player' names and    jersey numbers next    to ** the camera
2024-02-04 07:26:37,129 	Text Hypothesis :	the ******* **** iphones' combined worth is      eur   175000 which  roughly amounts to rs 173 crore 
2024-02-04 07:26:37,129 	Text Alignment  :	    D       D    S        S        S     S       S     S      S      S       S          I  S   S     
2024-02-04 07:26:37,129 ========================================================================================================================
2024-02-04 07:26:38,838 Epoch 118: Total Training Recognition Loss 0.08  Total Training Translation Loss 21.94 
2024-02-04 07:26:38,838 EPOCH 119
2024-02-04 07:26:43,835 Epoch 119: Total Training Recognition Loss 0.08  Total Training Translation Loss 20.22 
2024-02-04 07:26:43,835 EPOCH 120
2024-02-04 07:26:48,138 Epoch 120: Total Training Recognition Loss 0.08  Total Training Translation Loss 25.16 
2024-02-04 07:26:48,138 EPOCH 121
2024-02-04 07:26:51,351 [Epoch: 121 Step: 00004100] Batch Recognition Loss:   0.003598 => Gls Tokens per Sec:     1994 || Batch Translation Loss:   0.641467 => Txt Tokens per Sec:     5767 || Lr: 0.000100
2024-02-04 07:26:53,064 Epoch 121: Total Training Recognition Loss 0.08  Total Training Translation Loss 23.26 
2024-02-04 07:26:53,064 EPOCH 122
2024-02-04 07:26:57,385 Epoch 122: Total Training Recognition Loss 0.09  Total Training Translation Loss 24.18 
2024-02-04 07:26:57,385 EPOCH 123
2024-02-04 07:27:02,257 Epoch 123: Total Training Recognition Loss 0.08  Total Training Translation Loss 25.02 
2024-02-04 07:27:02,257 EPOCH 124
2024-02-04 07:27:04,219 [Epoch: 124 Step: 00004200] Batch Recognition Loss:   0.002028 => Gls Tokens per Sec:     2810 || Batch Translation Loss:   0.690389 => Txt Tokens per Sec:     7384 || Lr: 0.000100
2024-02-04 07:27:06,752 Epoch 124: Total Training Recognition Loss 0.07  Total Training Translation Loss 23.68 
2024-02-04 07:27:06,753 EPOCH 125
2024-02-04 07:27:11,477 Epoch 125: Total Training Recognition Loss 0.08  Total Training Translation Loss 22.60 
2024-02-04 07:27:11,477 EPOCH 126
2024-02-04 07:27:16,112 Epoch 126: Total Training Recognition Loss 0.08  Total Training Translation Loss 21.99 
2024-02-04 07:27:16,113 EPOCH 127
2024-02-04 07:27:18,566 [Epoch: 127 Step: 00004300] Batch Recognition Loss:   0.001554 => Gls Tokens per Sec:     1987 || Batch Translation Loss:   0.421406 => Txt Tokens per Sec:     5858 || Lr: 0.000100
2024-02-04 07:27:20,655 Epoch 127: Total Training Recognition Loss 0.08  Total Training Translation Loss 20.78 
2024-02-04 07:27:20,656 EPOCH 128
2024-02-04 07:27:25,338 Epoch 128: Total Training Recognition Loss 0.07  Total Training Translation Loss 19.77 
2024-02-04 07:27:25,339 EPOCH 129
2024-02-04 07:27:29,800 Epoch 129: Total Training Recognition Loss 0.07  Total Training Translation Loss 18.09 
2024-02-04 07:27:29,801 EPOCH 130
2024-02-04 07:27:31,840 [Epoch: 130 Step: 00004400] Batch Recognition Loss:   0.001847 => Gls Tokens per Sec:     2197 || Batch Translation Loss:   0.582094 => Txt Tokens per Sec:     6282 || Lr: 0.000100
2024-02-04 07:27:34,647 Epoch 130: Total Training Recognition Loss 0.06  Total Training Translation Loss 17.43 
2024-02-04 07:27:34,647 EPOCH 131
2024-02-04 07:27:38,954 Epoch 131: Total Training Recognition Loss 0.06  Total Training Translation Loss 17.06 
2024-02-04 07:27:38,954 EPOCH 132
2024-02-04 07:27:43,835 Epoch 132: Total Training Recognition Loss 0.06  Total Training Translation Loss 15.88 
2024-02-04 07:27:43,836 EPOCH 133
2024-02-04 07:27:45,258 [Epoch: 133 Step: 00004500] Batch Recognition Loss:   0.001373 => Gls Tokens per Sec:     2703 || Batch Translation Loss:   0.307354 => Txt Tokens per Sec:     7465 || Lr: 0.000100
2024-02-04 07:27:48,160 Epoch 133: Total Training Recognition Loss 0.06  Total Training Translation Loss 15.54 
2024-02-04 07:27:48,161 EPOCH 134
2024-02-04 07:27:53,061 Epoch 134: Total Training Recognition Loss 0.06  Total Training Translation Loss 14.76 
2024-02-04 07:27:53,062 EPOCH 135
2024-02-04 07:27:57,553 Epoch 135: Total Training Recognition Loss 0.06  Total Training Translation Loss 14.74 
2024-02-04 07:27:57,554 EPOCH 136
2024-02-04 07:27:59,005 [Epoch: 136 Step: 00004600] Batch Recognition Loss:   0.001363 => Gls Tokens per Sec:     2207 || Batch Translation Loss:   0.397804 => Txt Tokens per Sec:     6506 || Lr: 0.000100
2024-02-04 07:28:02,286 Epoch 136: Total Training Recognition Loss 0.06  Total Training Translation Loss 15.43 
2024-02-04 07:28:02,287 EPOCH 137
2024-02-04 07:28:06,793 Epoch 137: Total Training Recognition Loss 0.05  Total Training Translation Loss 15.57 
2024-02-04 07:28:06,794 EPOCH 138
2024-02-04 07:28:11,445 Epoch 138: Total Training Recognition Loss 0.06  Total Training Translation Loss 16.13 
2024-02-04 07:28:11,445 EPOCH 139
2024-02-04 07:28:12,654 [Epoch: 139 Step: 00004700] Batch Recognition Loss:   0.001190 => Gls Tokens per Sec:     2118 || Batch Translation Loss:   0.379198 => Txt Tokens per Sec:     6160 || Lr: 0.000100
2024-02-04 07:28:16,135 Epoch 139: Total Training Recognition Loss 0.05  Total Training Translation Loss 15.47 
2024-02-04 07:28:16,135 EPOCH 140
2024-02-04 07:28:20,752 Epoch 140: Total Training Recognition Loss 0.05  Total Training Translation Loss 19.10 
2024-02-04 07:28:20,753 EPOCH 141
2024-02-04 07:28:25,550 Epoch 141: Total Training Recognition Loss 0.06  Total Training Translation Loss 20.24 
2024-02-04 07:28:25,551 EPOCH 142
2024-02-04 07:28:26,412 [Epoch: 142 Step: 00004800] Batch Recognition Loss:   0.002785 => Gls Tokens per Sec:     2231 || Batch Translation Loss:   0.704378 => Txt Tokens per Sec:     6632 || Lr: 0.000100
2024-02-04 07:28:29,868 Epoch 142: Total Training Recognition Loss 0.07  Total Training Translation Loss 20.88 
2024-02-04 07:28:29,868 EPOCH 143
2024-02-04 07:28:34,799 Epoch 143: Total Training Recognition Loss 0.07  Total Training Translation Loss 17.94 
2024-02-04 07:28:34,800 EPOCH 144
2024-02-04 07:28:39,182 Epoch 144: Total Training Recognition Loss 0.07  Total Training Translation Loss 19.85 
2024-02-04 07:28:39,183 EPOCH 145
2024-02-04 07:28:39,935 [Epoch: 145 Step: 00004900] Batch Recognition Loss:   0.002018 => Gls Tokens per Sec:     1706 || Batch Translation Loss:   0.386585 => Txt Tokens per Sec:     4891 || Lr: 0.000100
2024-02-04 07:28:44,134 Epoch 145: Total Training Recognition Loss 0.07  Total Training Translation Loss 19.79 
2024-02-04 07:28:44,134 EPOCH 146
2024-02-04 07:28:48,560 Epoch 146: Total Training Recognition Loss 0.06  Total Training Translation Loss 16.43 
2024-02-04 07:28:48,560 EPOCH 147
2024-02-04 07:28:53,346 Epoch 147: Total Training Recognition Loss 0.05  Total Training Translation Loss 13.08 
2024-02-04 07:28:53,346 EPOCH 148
2024-02-04 07:28:53,669 [Epoch: 148 Step: 00005000] Batch Recognition Loss:   0.001253 => Gls Tokens per Sec:     1985 || Batch Translation Loss:   0.478046 => Txt Tokens per Sec:     6356 || Lr: 0.000100
2024-02-04 07:28:57,841 Epoch 148: Total Training Recognition Loss 0.05  Total Training Translation Loss 11.04 
2024-02-04 07:28:57,841 EPOCH 149
2024-02-04 07:29:02,490 Epoch 149: Total Training Recognition Loss 0.05  Total Training Translation Loss 11.08 
2024-02-04 07:29:02,490 EPOCH 150
2024-02-04 07:29:07,111 [Epoch: 150 Step: 00005100] Batch Recognition Loss:   0.001091 => Gls Tokens per Sec:     2301 || Batch Translation Loss:   0.144646 => Txt Tokens per Sec:     6387 || Lr: 0.000100
2024-02-04 07:29:07,112 Epoch 150: Total Training Recognition Loss 0.04  Total Training Translation Loss 11.02 
2024-02-04 07:29:07,112 EPOCH 151
2024-02-04 07:29:11,625 Epoch 151: Total Training Recognition Loss 0.04  Total Training Translation Loss 9.95 
2024-02-04 07:29:11,626 EPOCH 152
2024-02-04 07:29:16,473 Epoch 152: Total Training Recognition Loss 0.04  Total Training Translation Loss 9.98 
2024-02-04 07:29:16,473 EPOCH 153
2024-02-04 07:29:20,604 [Epoch: 153 Step: 00005200] Batch Recognition Loss:   0.001554 => Gls Tokens per Sec:     2419 || Batch Translation Loss:   0.335881 => Txt Tokens per Sec:     6754 || Lr: 0.000100
2024-02-04 07:29:20,766 Epoch 153: Total Training Recognition Loss 0.04  Total Training Translation Loss 10.63 
2024-02-04 07:29:20,766 EPOCH 154
2024-02-04 07:29:25,318 Epoch 154: Total Training Recognition Loss 0.04  Total Training Translation Loss 12.19 
2024-02-04 07:29:25,319 EPOCH 155
2024-02-04 07:29:30,103 Epoch 155: Total Training Recognition Loss 0.04  Total Training Translation Loss 13.26 
2024-02-04 07:29:30,103 EPOCH 156
2024-02-04 07:29:34,337 [Epoch: 156 Step: 00005300] Batch Recognition Loss:   0.000836 => Gls Tokens per Sec:     2209 || Batch Translation Loss:   0.411376 => Txt Tokens per Sec:     6136 || Lr: 0.000100
2024-02-04 07:29:34,854 Epoch 156: Total Training Recognition Loss 0.04  Total Training Translation Loss 11.00 
2024-02-04 07:29:34,854 EPOCH 157
2024-02-04 07:29:39,609 Epoch 157: Total Training Recognition Loss 0.04  Total Training Translation Loss 10.14 
2024-02-04 07:29:39,610 EPOCH 158
2024-02-04 07:29:44,359 Epoch 158: Total Training Recognition Loss 0.04  Total Training Translation Loss 9.67 
2024-02-04 07:29:44,359 EPOCH 159
2024-02-04 07:29:47,914 [Epoch: 159 Step: 00005400] Batch Recognition Loss:   0.001293 => Gls Tokens per Sec:     2451 || Batch Translation Loss:   0.296963 => Txt Tokens per Sec:     6788 || Lr: 0.000100
2024-02-04 07:29:48,581 Epoch 159: Total Training Recognition Loss 0.04  Total Training Translation Loss 9.18 
2024-02-04 07:29:48,581 EPOCH 160
2024-02-04 07:29:53,279 Epoch 160: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.74 
2024-02-04 07:29:53,280 EPOCH 161
2024-02-04 07:29:57,729 Epoch 161: Total Training Recognition Loss 0.04  Total Training Translation Loss 8.57 
2024-02-04 07:29:57,729 EPOCH 162
2024-02-04 07:30:01,579 [Epoch: 162 Step: 00005500] Batch Recognition Loss:   0.001095 => Gls Tokens per Sec:     2162 || Batch Translation Loss:   0.363213 => Txt Tokens per Sec:     6047 || Lr: 0.000100
2024-02-04 07:30:02,610 Epoch 162: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.95 
2024-02-04 07:30:02,610 EPOCH 163
2024-02-04 07:30:06,924 Epoch 163: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.78 
2024-02-04 07:30:06,924 EPOCH 164
2024-02-04 07:30:11,417 Epoch 164: Total Training Recognition Loss 0.04  Total Training Translation Loss 12.94 
2024-02-04 07:30:11,418 EPOCH 165
2024-02-04 07:30:14,792 [Epoch: 165 Step: 00005600] Batch Recognition Loss:   0.001425 => Gls Tokens per Sec:     2204 || Batch Translation Loss:   1.313636 => Txt Tokens per Sec:     6080 || Lr: 0.000100
2024-02-04 07:30:16,053 Epoch 165: Total Training Recognition Loss 0.08  Total Training Translation Loss 28.45 
2024-02-04 07:30:16,053 EPOCH 166
2024-02-04 07:30:20,708 Epoch 166: Total Training Recognition Loss 0.09  Total Training Translation Loss 24.70 
2024-02-04 07:30:20,709 EPOCH 167
2024-02-04 07:30:25,468 Epoch 167: Total Training Recognition Loss 0.05  Total Training Translation Loss 14.27 
2024-02-04 07:30:25,469 EPOCH 168
2024-02-04 07:30:28,576 [Epoch: 168 Step: 00005700] Batch Recognition Loss:   0.002230 => Gls Tokens per Sec:     2187 || Batch Translation Loss:   0.320123 => Txt Tokens per Sec:     6094 || Lr: 0.000100
2024-02-04 07:30:30,282 Epoch 168: Total Training Recognition Loss 0.05  Total Training Translation Loss 10.70 
2024-02-04 07:30:30,282 EPOCH 169
2024-02-04 07:30:35,155 Epoch 169: Total Training Recognition Loss 0.04  Total Training Translation Loss 9.27 
2024-02-04 07:30:35,156 EPOCH 170
2024-02-04 07:30:40,092 Epoch 170: Total Training Recognition Loss 0.04  Total Training Translation Loss 12.26 
2024-02-04 07:30:40,093 EPOCH 171
2024-02-04 07:30:42,552 [Epoch: 171 Step: 00005800] Batch Recognition Loss:   0.000502 => Gls Tokens per Sec:     2604 || Batch Translation Loss:   0.194207 => Txt Tokens per Sec:     6922 || Lr: 0.000100
2024-02-04 07:30:44,954 Epoch 171: Total Training Recognition Loss 0.04  Total Training Translation Loss 10.36 
2024-02-04 07:30:44,954 EPOCH 172
2024-02-04 07:30:49,740 Epoch 172: Total Training Recognition Loss 0.04  Total Training Translation Loss 8.90 
2024-02-04 07:30:49,740 EPOCH 173
2024-02-04 07:30:54,603 Epoch 173: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.06 
2024-02-04 07:30:54,604 EPOCH 174
2024-02-04 07:30:57,149 [Epoch: 174 Step: 00005900] Batch Recognition Loss:   0.000597 => Gls Tokens per Sec:     2264 || Batch Translation Loss:   0.271188 => Txt Tokens per Sec:     6411 || Lr: 0.000100
2024-02-04 07:30:58,994 Epoch 174: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.29 
2024-02-04 07:30:58,995 EPOCH 175
2024-02-04 07:31:03,685 Epoch 175: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.06 
2024-02-04 07:31:03,686 EPOCH 176
2024-02-04 07:31:08,164 Epoch 176: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.95 
2024-02-04 07:31:08,164 EPOCH 177
2024-02-04 07:31:10,132 [Epoch: 177 Step: 00006000] Batch Recognition Loss:   0.000484 => Gls Tokens per Sec:     2602 || Batch Translation Loss:   0.179204 => Txt Tokens per Sec:     6725 || Lr: 0.000100
2024-02-04 07:31:18,166 Validation result at epoch 177, step     6000: duration: 8.0328s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00132	Translation Loss: 85519.70312	PPL: 5208.05371
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.81	(BLEU-1: 11.89,	BLEU-2: 3.91,	BLEU-3: 1.70,	BLEU-4: 0.81)
	CHRF 17.49	ROUGE 10.58
2024-02-04 07:31:18,167 Logging Recognition and Translation Outputs
2024-02-04 07:31:18,167 ========================================================================================================================
2024-02-04 07:31:18,167 Logging Sequence: 89_111.00
2024-02-04 07:31:18,168 	Gloss Reference :	A B+C+D+E
2024-02-04 07:31:18,168 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:31:18,168 	Gloss Alignment :	         
2024-02-04 07:31:18,168 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:31:18,169 	Text Reference  :	however selectors never   selected me  for   the *** ***** *** *** ** ****** *** ** ** ***** ** ** team  
2024-02-04 07:31:18,169 	Text Hypothesis :	******* the       spinner is       not touch the t20 world cup due to retire but he is going on to retire
2024-02-04 07:31:18,169 	Text Alignment  :	D       S         S       S        S   S         I   I     I   I   I  I      I   I  I  I     I  I  S     
2024-02-04 07:31:18,169 ========================================================================================================================
2024-02-04 07:31:18,170 Logging Sequence: 137_23.00
2024-02-04 07:31:18,170 	Gloss Reference :	A B+C+D+E
2024-02-04 07:31:18,170 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:31:18,170 	Gloss Alignment :	         
2024-02-04 07:31:18,170 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:31:18,171 	Text Reference  :	fan from around the world are *** in      qatar for the fifa world cup    
2024-02-04 07:31:18,171 	Text Hypothesis :	the fans across the world are not allowed to    see the **** ***** stadium
2024-02-04 07:31:18,171 	Text Alignment  :	S   S    S                    I   S       S     S       D    D     S      
2024-02-04 07:31:18,171 ========================================================================================================================
2024-02-04 07:31:18,172 Logging Sequence: 128_145.00
2024-02-04 07:31:18,172 	Gloss Reference :	A B+C+D+E
2024-02-04 07:31:18,172 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:31:18,172 	Gloss Alignment :	         
2024-02-04 07:31:18,172 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:31:18,173 	Text Reference  :	icc also uploaded a   video of the same *** ** ** ***** *** ****** ***
2024-02-04 07:31:18,173 	Text Hypothesis :	*** the  ban      was not   in the same but he is about the entire man
2024-02-04 07:31:18,173 	Text Alignment  :	D   S    S        S   S     S           I   I  I  I     I   I      I  
2024-02-04 07:31:18,173 ========================================================================================================================
2024-02-04 07:31:18,173 Logging Sequence: 165_192.00
2024-02-04 07:31:18,174 	Gloss Reference :	A B+C+D+E
2024-02-04 07:31:18,174 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:31:18,174 	Gloss Alignment :	         
2024-02-04 07:31:18,174 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:31:18,175 	Text Reference  :	** ** ***** **** *** ****** 3    ravichandran ashwin believes that his   bag is   lucky
2024-02-04 07:31:18,175 	Text Hypothesis :	he is happy with the indian team won          the    toss     and  chose to  give it   
2024-02-04 07:31:18,175 	Text Alignment  :	I  I  I     I    I   I      S    S            S      S        S    S     S   S    S    
2024-02-04 07:31:18,175 ========================================================================================================================
2024-02-04 07:31:18,175 Logging Sequence: 180_494.00
2024-02-04 07:31:18,176 	Gloss Reference :	A B+C+D+E
2024-02-04 07:31:18,176 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:31:18,176 	Gloss Alignment :	         
2024-02-04 07:31:18,176 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:31:18,177 	Text Reference  :	the     women wrestlers spoke angrily against the  police and the controversy in front of     the        media    
2024-02-04 07:31:18,177 	Text Hypothesis :	however an    wrestlers ***** ******* ******* said it     is  not permitted   in ***** sexual harassment complaint
2024-02-04 07:31:18,177 	Text Alignment  :	S       S               D     D       D       S    S      S   S   S              D     S      S          S        
2024-02-04 07:31:18,177 ========================================================================================================================
2024-02-04 07:31:21,257 Epoch 177: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.01 
2024-02-04 07:31:21,257 EPOCH 178
2024-02-04 07:31:25,961 Epoch 178: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.08 
2024-02-04 07:31:25,961 EPOCH 179
2024-02-04 07:31:30,602 Epoch 179: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.55 
2024-02-04 07:31:30,603 EPOCH 180
2024-02-04 07:31:32,472 [Epoch: 180 Step: 00006100] Batch Recognition Loss:   0.000444 => Gls Tokens per Sec:     2264 || Batch Translation Loss:   0.196149 => Txt Tokens per Sec:     6183 || Lr: 0.000100
2024-02-04 07:31:35,139 Epoch 180: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.15 
2024-02-04 07:31:35,139 EPOCH 181
2024-02-04 07:31:39,915 Epoch 181: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.10 
2024-02-04 07:31:39,916 EPOCH 182
2024-02-04 07:31:44,324 Epoch 182: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.25 
2024-02-04 07:31:44,324 EPOCH 183
2024-02-04 07:31:45,954 [Epoch: 183 Step: 00006200] Batch Recognition Loss:   0.000504 => Gls Tokens per Sec:     2204 || Batch Translation Loss:   0.375396 => Txt Tokens per Sec:     6218 || Lr: 0.000100
2024-02-04 07:31:49,223 Epoch 183: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.11 
2024-02-04 07:31:49,224 EPOCH 184
2024-02-04 07:31:53,600 Epoch 184: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.72 
2024-02-04 07:31:53,601 EPOCH 185
2024-02-04 07:31:58,469 Epoch 185: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.40 
2024-02-04 07:31:58,470 EPOCH 186
2024-02-04 07:31:59,952 [Epoch: 186 Step: 00006300] Batch Recognition Loss:   0.000840 => Gls Tokens per Sec:     2161 || Batch Translation Loss:   0.259017 => Txt Tokens per Sec:     6268 || Lr: 0.000100
2024-02-04 07:32:02,895 Epoch 186: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.46 
2024-02-04 07:32:02,896 EPOCH 187
2024-02-04 07:32:07,790 Epoch 187: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.98 
2024-02-04 07:32:07,790 EPOCH 188
2024-02-04 07:32:12,295 Epoch 188: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.39 
2024-02-04 07:32:12,295 EPOCH 189
2024-02-04 07:32:13,458 [Epoch: 189 Step: 00006400] Batch Recognition Loss:   0.000721 => Gls Tokens per Sec:     2204 || Batch Translation Loss:   0.213899 => Txt Tokens per Sec:     5874 || Lr: 0.000100
2024-02-04 07:32:16,944 Epoch 189: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.48 
2024-02-04 07:32:16,944 EPOCH 190
2024-02-04 07:32:21,549 Epoch 190: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.14 
2024-02-04 07:32:21,549 EPOCH 191
2024-02-04 07:32:26,090 Epoch 191: Total Training Recognition Loss 0.02  Total Training Translation Loss 9.05 
2024-02-04 07:32:26,091 EPOCH 192
2024-02-04 07:32:26,841 [Epoch: 192 Step: 00006500] Batch Recognition Loss:   0.000608 => Gls Tokens per Sec:     2560 || Batch Translation Loss:   0.157399 => Txt Tokens per Sec:     7651 || Lr: 0.000100
2024-02-04 07:32:30,497 Epoch 192: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.15 
2024-02-04 07:32:30,498 EPOCH 193
2024-02-04 07:32:35,277 Epoch 193: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.26 
2024-02-04 07:32:35,277 EPOCH 194
2024-02-04 07:32:39,765 Epoch 194: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.62 
2024-02-04 07:32:39,766 EPOCH 195
2024-02-04 07:32:40,534 [Epoch: 195 Step: 00006600] Batch Recognition Loss:   0.000471 => Gls Tokens per Sec:     1669 || Batch Translation Loss:   0.324087 => Txt Tokens per Sec:     4759 || Lr: 0.000100
2024-02-04 07:32:44,465 Epoch 195: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.00 
2024-02-04 07:32:44,465 EPOCH 196
2024-02-04 07:32:49,143 Epoch 196: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.02 
2024-02-04 07:32:49,143 EPOCH 197
2024-02-04 07:32:53,667 Epoch 197: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.00 
2024-02-04 07:32:53,667 EPOCH 198
2024-02-04 07:32:53,848 [Epoch: 198 Step: 00006700] Batch Recognition Loss:   0.000586 => Gls Tokens per Sec:     3556 || Batch Translation Loss:   0.233555 => Txt Tokens per Sec:     9100 || Lr: 0.000100
2024-02-04 07:32:58,670 Epoch 198: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.65 
2024-02-04 07:32:58,670 EPOCH 199
2024-02-04 07:33:03,401 Epoch 199: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.25 
2024-02-04 07:33:03,401 EPOCH 200
2024-02-04 07:33:07,839 [Epoch: 200 Step: 00006800] Batch Recognition Loss:   0.001257 => Gls Tokens per Sec:     2396 || Batch Translation Loss:   0.188765 => Txt Tokens per Sec:     6651 || Lr: 0.000100
2024-02-04 07:33:07,840 Epoch 200: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.10 
2024-02-04 07:33:07,840 EPOCH 201
2024-02-04 07:33:12,512 Epoch 201: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.49 
2024-02-04 07:33:12,512 EPOCH 202
2024-02-04 07:33:17,085 Epoch 202: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.42 
2024-02-04 07:33:17,086 EPOCH 203
2024-02-04 07:33:21,321 [Epoch: 203 Step: 00006900] Batch Recognition Loss:   0.000283 => Gls Tokens per Sec:     2360 || Batch Translation Loss:   0.205044 => Txt Tokens per Sec:     6445 || Lr: 0.000100
2024-02-04 07:33:21,670 Epoch 203: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.63 
2024-02-04 07:33:21,670 EPOCH 204
2024-02-04 07:33:26,372 Epoch 204: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.24 
2024-02-04 07:33:26,373 EPOCH 205
2024-02-04 07:33:30,767 Epoch 205: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.84 
2024-02-04 07:33:30,767 EPOCH 206
2024-02-04 07:33:34,440 [Epoch: 206 Step: 00007000] Batch Recognition Loss:   0.001108 => Gls Tokens per Sec:     2546 || Batch Translation Loss:   0.282624 => Txt Tokens per Sec:     7095 || Lr: 0.000100
2024-02-04 07:33:34,829 Epoch 206: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.02 
2024-02-04 07:33:34,829 EPOCH 207
2024-02-04 07:33:38,940 Epoch 207: Total Training Recognition Loss 0.02  Total Training Translation Loss 9.84 
2024-02-04 07:33:38,940 EPOCH 208
2024-02-04 07:33:42,979 Epoch 208: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.21 
2024-02-04 07:33:42,980 EPOCH 209
2024-02-04 07:33:46,940 [Epoch: 209 Step: 00007100] Batch Recognition Loss:   0.000646 => Gls Tokens per Sec:     2201 || Batch Translation Loss:   0.243974 => Txt Tokens per Sec:     6040 || Lr: 0.000100
2024-02-04 07:33:47,846 Epoch 209: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.34 
2024-02-04 07:33:47,846 EPOCH 210
2024-02-04 07:33:52,371 Epoch 210: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.83 
2024-02-04 07:33:52,371 EPOCH 211
2024-02-04 07:33:57,148 Epoch 211: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.24 
2024-02-04 07:33:57,148 EPOCH 212
2024-02-04 07:34:00,317 [Epoch: 212 Step: 00007200] Batch Recognition Loss:   0.000428 => Gls Tokens per Sec:     2547 || Batch Translation Loss:   0.318888 => Txt Tokens per Sec:     7089 || Lr: 0.000100
2024-02-04 07:34:01,227 Epoch 212: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.89 
2024-02-04 07:34:01,227 EPOCH 213
2024-02-04 07:34:06,046 Epoch 213: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.30 
2024-02-04 07:34:06,046 EPOCH 214
2024-02-04 07:34:10,370 Epoch 214: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.31 
2024-02-04 07:34:10,370 EPOCH 215
2024-02-04 07:34:13,551 [Epoch: 215 Step: 00007300] Batch Recognition Loss:   0.000968 => Gls Tokens per Sec:     2337 || Batch Translation Loss:   0.224741 => Txt Tokens per Sec:     6319 || Lr: 0.000100
2024-02-04 07:34:15,264 Epoch 215: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.80 
2024-02-04 07:34:15,265 EPOCH 216
2024-02-04 07:34:19,641 Epoch 216: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.49 
2024-02-04 07:34:19,642 EPOCH 217
2024-02-04 07:34:24,528 Epoch 217: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.61 
2024-02-04 07:34:24,528 EPOCH 218
2024-02-04 07:34:27,009 [Epoch: 218 Step: 00007400] Batch Recognition Loss:   0.000276 => Gls Tokens per Sec:     2737 || Batch Translation Loss:   0.104782 => Txt Tokens per Sec:     7542 || Lr: 0.000100
2024-02-04 07:34:28,853 Epoch 218: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.64 
2024-02-04 07:34:28,854 EPOCH 219
2024-02-04 07:34:33,739 Epoch 219: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.27 
2024-02-04 07:34:33,740 EPOCH 220
2024-02-04 07:34:38,196 Epoch 220: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.10 
2024-02-04 07:34:38,197 EPOCH 221
2024-02-04 07:34:41,079 [Epoch: 221 Step: 00007500] Batch Recognition Loss:   0.000753 => Gls Tokens per Sec:     2135 || Batch Translation Loss:   0.140823 => Txt Tokens per Sec:     5869 || Lr: 0.000100
2024-02-04 07:34:42,933 Epoch 221: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.32 
2024-02-04 07:34:42,933 EPOCH 222
2024-02-04 07:34:47,037 Epoch 222: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.34 
2024-02-04 07:34:47,038 EPOCH 223
2024-02-04 07:34:51,063 Epoch 223: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.74 
2024-02-04 07:34:51,064 EPOCH 224
2024-02-04 07:34:53,896 [Epoch: 224 Step: 00007600] Batch Recognition Loss:   0.000327 => Gls Tokens per Sec:     2035 || Batch Translation Loss:   0.072251 => Txt Tokens per Sec:     5703 || Lr: 0.000100
2024-02-04 07:34:55,935 Epoch 224: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.87 
2024-02-04 07:34:55,936 EPOCH 225
2024-02-04 07:35:00,344 Epoch 225: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.62 
2024-02-04 07:35:00,344 EPOCH 226
2024-02-04 07:35:05,235 Epoch 226: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.83 
2024-02-04 07:35:05,236 EPOCH 227
2024-02-04 07:35:07,079 [Epoch: 227 Step: 00007700] Batch Recognition Loss:   0.000340 => Gls Tokens per Sec:     2779 || Batch Translation Loss:   0.151813 => Txt Tokens per Sec:     7703 || Lr: 0.000100
2024-02-04 07:35:09,704 Epoch 227: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.18 
2024-02-04 07:35:09,704 EPOCH 228
2024-02-04 07:35:14,405 Epoch 228: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.73 
2024-02-04 07:35:14,405 EPOCH 229
2024-02-04 07:35:18,991 Epoch 229: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.76 
2024-02-04 07:35:18,992 EPOCH 230
2024-02-04 07:35:20,956 [Epoch: 230 Step: 00007800] Batch Recognition Loss:   0.000455 => Gls Tokens per Sec:     2283 || Batch Translation Loss:   0.457464 => Txt Tokens per Sec:     6579 || Lr: 0.000100
2024-02-04 07:35:23,627 Epoch 230: Total Training Recognition Loss 0.02  Total Training Translation Loss 11.34 
2024-02-04 07:35:23,627 EPOCH 231
2024-02-04 07:35:28,384 Epoch 231: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.85 
2024-02-04 07:35:28,385 EPOCH 232
2024-02-04 07:35:32,885 Epoch 232: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.51 
2024-02-04 07:35:32,885 EPOCH 233
2024-02-04 07:35:34,490 [Epoch: 233 Step: 00007900] Batch Recognition Loss:   0.001049 => Gls Tokens per Sec:     2237 || Batch Translation Loss:   0.111574 => Txt Tokens per Sec:     6579 || Lr: 0.000100
2024-02-04 07:35:37,034 Epoch 233: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.66 
2024-02-04 07:35:37,034 EPOCH 234
2024-02-04 07:35:41,256 Epoch 234: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.63 
2024-02-04 07:35:41,257 EPOCH 235
2024-02-04 07:35:46,174 Epoch 235: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.26 
2024-02-04 07:35:46,175 EPOCH 236
2024-02-04 07:35:47,253 [Epoch: 236 Step: 00008000] Batch Recognition Loss:   0.001098 => Gls Tokens per Sec:     2973 || Batch Translation Loss:   0.419294 => Txt Tokens per Sec:     7656 || Lr: 0.000100
2024-02-04 07:35:55,484 Validation result at epoch 236, step     8000: duration: 8.2310s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00202	Translation Loss: 90072.08594	PPL: 8213.36816
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.92	(BLEU-1: 10.74,	BLEU-2: 3.83,	BLEU-3: 1.70,	BLEU-4: 0.92)
	CHRF 16.59	ROUGE 9.76
2024-02-04 07:35:55,485 Logging Recognition and Translation Outputs
2024-02-04 07:35:55,486 ========================================================================================================================
2024-02-04 07:35:55,486 Logging Sequence: 88_57.00
2024-02-04 07:35:55,486 	Gloss Reference :	A B+C+D+E
2024-02-04 07:35:55,486 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:35:55,487 	Gloss Alignment :	         
2024-02-04 07:35:55,487 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:35:55,488 	Text Reference  :	which   stated  messi we're  waiting for  you     to   come here      you  will be finished when    you come
2024-02-04 07:35:55,489 	Text Hypothesis :	notably rosario has   become the     most violent city in   argentina with 250  to 300      murders in  2022
2024-02-04 07:35:55,489 	Text Alignment  :	S       S       S     S      S       S    S       S    S    S         S    S    S  S        S       S   S   
2024-02-04 07:35:55,489 ========================================================================================================================
2024-02-04 07:35:55,489 Logging Sequence: 171_142.00
2024-02-04 07:35:55,489 	Gloss Reference :	A B+C+D+E
2024-02-04 07:35:55,489 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:35:55,489 	Gloss Alignment :	         
2024-02-04 07:35:55,490 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:35:55,491 	Text Reference  :	***** **** *** ********* this decision on   dhoni made     a *** ******** significant impact  as      pathirana claimed two tough  wickets
2024-02-04 07:35:55,491 	Text Hypothesis :	since then the wrestlers left the      same time  spending a new agencies were        playing against each      other   at  jantar mantar 
2024-02-04 07:35:55,492 	Text Alignment  :	I     I    I   I         S    S        S    S     S          I   I        S           S       S       S         S       S   S      S      
2024-02-04 07:35:55,492 ========================================================================================================================
2024-02-04 07:35:55,492 Logging Sequence: 125_207.00
2024-02-04 07:35:55,492 	Gloss Reference :	A B+C+D+E
2024-02-04 07:35:55,492 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:35:55,492 	Gloss Alignment :	         
2024-02-04 07:35:55,492 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:35:55,493 	Text Reference  :	he had not practised since he returned and he   had          also fallen sick   
2024-02-04 07:35:55,493 	Text Hypothesis :	** *** *** ********* ***** ** neeraj   was very disappointed by   his    brother
2024-02-04 07:35:55,493 	Text Alignment  :	D  D   D   D         D     D  S        S   S    S            S    S      S      
2024-02-04 07:35:55,493 ========================================================================================================================
2024-02-04 07:35:55,493 Logging Sequence: 68_230.00
2024-02-04 07:35:55,494 	Gloss Reference :	A B+C+D+E
2024-02-04 07:35:55,494 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:35:55,494 	Gloss Alignment :	         
2024-02-04 07:35:55,494 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:35:55,495 	Text Reference  :	let us know what you  think in   the comments below
2024-02-04 07:35:55,495 	Text Hypothesis :	*** ** they also sent to    pick up  against  pant 
2024-02-04 07:35:55,495 	Text Alignment  :	D   D  S    S    S    S     S    S   S        S    
2024-02-04 07:35:55,495 ========================================================================================================================
2024-02-04 07:35:55,495 Logging Sequence: 126_82.00
2024-02-04 07:35:55,495 	Gloss Reference :	A B+C+D+E
2024-02-04 07:35:55,495 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:35:55,495 	Gloss Alignment :	         
2024-02-04 07:35:55,496 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:35:55,497 	Text Reference  :	neeraj also dedicated his gold medal  to ********* **** ** former indian olympians who came close to winning *** medals
2024-02-04 07:35:55,497 	Text Hypothesis :	he     also dedicated *** the  medals to olympians like pt usha   and    others    who came close to winning the medals
2024-02-04 07:35:55,498 	Text Alignment  :	S                     D   S    S         I         I    I  S      S      S                                   I         
2024-02-04 07:35:55,498 ========================================================================================================================
2024-02-04 07:35:59,040 Epoch 236: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.33 
2024-02-04 07:35:59,041 EPOCH 237
2024-02-04 07:36:03,931 Epoch 237: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.30 
2024-02-04 07:36:03,932 EPOCH 238
2024-02-04 07:36:08,306 Epoch 238: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.46 
2024-02-04 07:36:08,306 EPOCH 239
2024-02-04 07:36:09,428 [Epoch: 239 Step: 00008100] Batch Recognition Loss:   0.000382 => Gls Tokens per Sec:     2284 || Batch Translation Loss:   0.079129 => Txt Tokens per Sec:     6051 || Lr: 0.000100
2024-02-04 07:36:13,070 Epoch 239: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.51 
2024-02-04 07:36:13,071 EPOCH 240
2024-02-04 07:36:17,849 Epoch 240: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.28 
2024-02-04 07:36:17,850 EPOCH 241
2024-02-04 07:36:22,655 Epoch 241: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.29 
2024-02-04 07:36:22,656 EPOCH 242
2024-02-04 07:36:23,290 [Epoch: 242 Step: 00008200] Batch Recognition Loss:   0.000507 => Gls Tokens per Sec:     3033 || Batch Translation Loss:   0.175211 => Txt Tokens per Sec:     7299 || Lr: 0.000100
2024-02-04 07:36:27,489 Epoch 242: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.20 
2024-02-04 07:36:27,490 EPOCH 243
2024-02-04 07:36:32,395 Epoch 243: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.66 
2024-02-04 07:36:32,395 EPOCH 244
2024-02-04 07:36:37,301 Epoch 244: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.07 
2024-02-04 07:36:37,301 EPOCH 245
2024-02-04 07:36:37,995 [Epoch: 245 Step: 00008300] Batch Recognition Loss:   0.000581 => Gls Tokens per Sec:     1847 || Batch Translation Loss:   0.119350 => Txt Tokens per Sec:     5589 || Lr: 0.000100
2024-02-04 07:36:42,165 Epoch 245: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.23 
2024-02-04 07:36:42,166 EPOCH 246
2024-02-04 07:36:46,962 Epoch 246: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.12 
2024-02-04 07:36:46,963 EPOCH 247
2024-02-04 07:36:51,676 Epoch 247: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.44 
2024-02-04 07:36:51,677 EPOCH 248
2024-02-04 07:36:51,939 [Epoch: 248 Step: 00008400] Batch Recognition Loss:   0.000429 => Gls Tokens per Sec:     2452 || Batch Translation Loss:   0.090073 => Txt Tokens per Sec:     7682 || Lr: 0.000100
2024-02-04 07:36:56,455 Epoch 248: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.41 
2024-02-04 07:36:56,456 EPOCH 249
2024-02-04 07:37:00,944 Epoch 249: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.94 
2024-02-04 07:37:00,945 EPOCH 250
2024-02-04 07:37:05,883 [Epoch: 250 Step: 00008500] Batch Recognition Loss:   0.000293 => Gls Tokens per Sec:     2153 || Batch Translation Loss:   0.030954 => Txt Tokens per Sec:     5977 || Lr: 0.000100
2024-02-04 07:37:05,883 Epoch 250: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.32 
2024-02-04 07:37:05,883 EPOCH 251
2024-02-04 07:37:10,704 Epoch 251: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.30 
2024-02-04 07:37:10,704 EPOCH 252
2024-02-04 07:37:15,597 Epoch 252: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.09 
2024-02-04 07:37:15,598 EPOCH 253
2024-02-04 07:37:20,215 [Epoch: 253 Step: 00008600] Batch Recognition Loss:   0.000369 => Gls Tokens per Sec:     2165 || Batch Translation Loss:   0.213785 => Txt Tokens per Sec:     6016 || Lr: 0.000100
2024-02-04 07:37:20,523 Epoch 253: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.63 
2024-02-04 07:37:20,523 EPOCH 254
2024-02-04 07:37:25,386 Epoch 254: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.99 
2024-02-04 07:37:25,386 EPOCH 255
2024-02-04 07:37:30,181 Epoch 255: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.87 
2024-02-04 07:37:30,182 EPOCH 256
2024-02-04 07:37:34,135 [Epoch: 256 Step: 00008700] Batch Recognition Loss:   0.000620 => Gls Tokens per Sec:     2429 || Batch Translation Loss:   0.150347 => Txt Tokens per Sec:     6626 || Lr: 0.000100
2024-02-04 07:37:34,982 Epoch 256: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.99 
2024-02-04 07:37:34,983 EPOCH 257
2024-02-04 07:37:39,753 Epoch 257: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.87 
2024-02-04 07:37:39,754 EPOCH 258
2024-02-04 07:37:44,047 Epoch 258: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.68 
2024-02-04 07:37:44,047 EPOCH 259
2024-02-04 07:37:48,165 [Epoch: 259 Step: 00008800] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     2116 || Batch Translation Loss:   0.109291 => Txt Tokens per Sec:     5976 || Lr: 0.000100
2024-02-04 07:37:48,846 Epoch 259: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.25 
2024-02-04 07:37:48,846 EPOCH 260
2024-02-04 07:37:53,369 Epoch 260: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.80 
2024-02-04 07:37:53,370 EPOCH 261
2024-02-04 07:37:58,257 Epoch 261: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.72 
2024-02-04 07:37:58,257 EPOCH 262
2024-02-04 07:38:01,509 [Epoch: 262 Step: 00008900] Batch Recognition Loss:   0.000247 => Gls Tokens per Sec:     2482 || Batch Translation Loss:   0.061088 => Txt Tokens per Sec:     6932 || Lr: 0.000100
2024-02-04 07:38:02,407 Epoch 262: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.89 
2024-02-04 07:38:02,408 EPOCH 263
2024-02-04 07:38:07,155 Epoch 263: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.07 
2024-02-04 07:38:07,156 EPOCH 264
2024-02-04 07:38:11,595 Epoch 264: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.74 
2024-02-04 07:38:11,596 EPOCH 265
2024-02-04 07:38:14,944 [Epoch: 265 Step: 00009000] Batch Recognition Loss:   0.000326 => Gls Tokens per Sec:     2221 || Batch Translation Loss:   0.226491 => Txt Tokens per Sec:     6108 || Lr: 0.000100
2024-02-04 07:38:16,444 Epoch 265: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.57 
2024-02-04 07:38:16,444 EPOCH 266
2024-02-04 07:38:20,805 Epoch 266: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.00 
2024-02-04 07:38:20,806 EPOCH 267
2024-02-04 07:38:25,787 Epoch 267: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.11 
2024-02-04 07:38:25,787 EPOCH 268
2024-02-04 07:38:28,588 [Epoch: 268 Step: 00009100] Batch Recognition Loss:   0.000210 => Gls Tokens per Sec:     2425 || Batch Translation Loss:   0.101374 => Txt Tokens per Sec:     6901 || Lr: 0.000100
2024-02-04 07:38:30,002 Epoch 268: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.49 
2024-02-04 07:38:30,002 EPOCH 269
2024-02-04 07:38:34,781 Epoch 269: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.48 
2024-02-04 07:38:34,781 EPOCH 270
2024-02-04 07:38:39,188 Epoch 270: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.42 
2024-02-04 07:38:39,188 EPOCH 271
2024-02-04 07:38:42,237 [Epoch: 271 Step: 00009200] Batch Recognition Loss:   0.001253 => Gls Tokens per Sec:     2018 || Batch Translation Loss:   0.145108 => Txt Tokens per Sec:     5724 || Lr: 0.000100
2024-02-04 07:38:44,101 Epoch 271: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.77 
2024-02-04 07:38:44,101 EPOCH 272
2024-02-04 07:38:48,432 Epoch 272: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.59 
2024-02-04 07:38:48,433 EPOCH 273
2024-02-04 07:38:53,344 Epoch 273: Total Training Recognition Loss 0.02  Total Training Translation Loss 10.74 
2024-02-04 07:38:53,344 EPOCH 274
2024-02-04 07:38:55,695 [Epoch: 274 Step: 00009300] Batch Recognition Loss:   0.000533 => Gls Tokens per Sec:     2346 || Batch Translation Loss:   0.463026 => Txt Tokens per Sec:     6629 || Lr: 0.000100
2024-02-04 07:38:57,760 Epoch 274: Total Training Recognition Loss 0.03  Total Training Translation Loss 11.65 
2024-02-04 07:38:57,760 EPOCH 275
2024-02-04 07:39:02,631 Epoch 275: Total Training Recognition Loss 0.04  Total Training Translation Loss 9.32 
2024-02-04 07:39:02,631 EPOCH 276
2024-02-04 07:39:07,125 Epoch 276: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.57 
2024-02-04 07:39:07,126 EPOCH 277
2024-02-04 07:39:09,303 [Epoch: 277 Step: 00009400] Batch Recognition Loss:   0.000566 => Gls Tokens per Sec:     2353 || Batch Translation Loss:   0.163891 => Txt Tokens per Sec:     6599 || Lr: 0.000100
2024-02-04 07:39:11,828 Epoch 277: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.59 
2024-02-04 07:39:11,828 EPOCH 278
2024-02-04 07:39:16,366 Epoch 278: Total Training Recognition Loss 0.09  Total Training Translation Loss 8.67 
2024-02-04 07:39:16,367 EPOCH 279
2024-02-04 07:39:20,978 Epoch 279: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.69 
2024-02-04 07:39:20,978 EPOCH 280
2024-02-04 07:39:22,940 [Epoch: 280 Step: 00009500] Batch Recognition Loss:   0.000515 => Gls Tokens per Sec:     2157 || Batch Translation Loss:   0.130096 => Txt Tokens per Sec:     6339 || Lr: 0.000100
2024-02-04 07:39:25,699 Epoch 280: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.22 
2024-02-04 07:39:25,699 EPOCH 281
2024-02-04 07:39:30,123 Epoch 281: Total Training Recognition Loss 0.01  Total Training Translation Loss 6.81 
2024-02-04 07:39:30,123 EPOCH 282
2024-02-04 07:39:34,930 Epoch 282: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.02 
2024-02-04 07:39:34,930 EPOCH 283
2024-02-04 07:39:36,390 [Epoch: 283 Step: 00009600] Batch Recognition Loss:   0.000622 => Gls Tokens per Sec:     2465 || Batch Translation Loss:   0.330013 => Txt Tokens per Sec:     6702 || Lr: 0.000100
2024-02-04 07:39:39,261 Epoch 283: Total Training Recognition Loss 0.01  Total Training Translation Loss 7.39 
2024-02-04 07:39:39,261 EPOCH 284
2024-02-04 07:39:44,185 Epoch 284: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.93 
2024-02-04 07:39:44,186 EPOCH 285
2024-02-04 07:39:48,591 Epoch 285: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.96 
2024-02-04 07:39:48,592 EPOCH 286
2024-02-04 07:39:50,374 [Epoch: 286 Step: 00009700] Batch Recognition Loss:   0.000619 => Gls Tokens per Sec:     1658 || Batch Translation Loss:   0.231258 => Txt Tokens per Sec:     4688 || Lr: 0.000100
2024-02-04 07:39:53,471 Epoch 286: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.26 
2024-02-04 07:39:53,471 EPOCH 287
2024-02-04 07:39:57,891 Epoch 287: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.93 
2024-02-04 07:39:57,891 EPOCH 288
2024-02-04 07:40:02,669 Epoch 288: Total Training Recognition Loss 0.10  Total Training Translation Loss 13.36 
2024-02-04 07:40:02,669 EPOCH 289
2024-02-04 07:40:03,935 [Epoch: 289 Step: 00009800] Batch Recognition Loss:   0.000374 => Gls Tokens per Sec:     2023 || Batch Translation Loss:   0.319923 => Txt Tokens per Sec:     6167 || Lr: 0.000100
2024-02-04 07:40:07,156 Epoch 289: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.70 
2024-02-04 07:40:07,157 EPOCH 290
2024-02-04 07:40:12,137 Epoch 290: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.93 
2024-02-04 07:40:12,138 EPOCH 291
2024-02-04 07:40:16,445 Epoch 291: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.93 
2024-02-04 07:40:16,445 EPOCH 292
2024-02-04 07:40:17,503 [Epoch: 292 Step: 00009900] Batch Recognition Loss:   0.000608 => Gls Tokens per Sec:     1580 || Batch Translation Loss:   0.229777 => Txt Tokens per Sec:     4805 || Lr: 0.000100
2024-02-04 07:40:21,363 Epoch 292: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.14 
2024-02-04 07:40:21,363 EPOCH 293
2024-02-04 07:40:25,767 Epoch 293: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.98 
2024-02-04 07:40:25,767 EPOCH 294
2024-02-04 07:40:30,647 Epoch 294: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.64 
2024-02-04 07:40:30,647 EPOCH 295
2024-02-04 07:40:31,092 [Epoch: 295 Step: 00010000] Batch Recognition Loss:   0.000336 => Gls Tokens per Sec:     2883 || Batch Translation Loss:   0.055200 => Txt Tokens per Sec:     8266 || Lr: 0.000100
2024-02-04 07:40:39,474 Hooray! New best validation result [eval_metric]!
2024-02-04 07:40:39,475 Saving new checkpoint.
2024-02-04 07:40:39,745 Validation result at epoch 295, step    10000: duration: 8.6528s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00089	Translation Loss: 91787.36719	PPL: 9751.40234
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 1.17	(BLEU-1: 11.33,	BLEU-2: 4.18,	BLEU-3: 2.06,	BLEU-4: 1.17)
	CHRF 17.52	ROUGE 9.83
2024-02-04 07:40:39,746 Logging Recognition and Translation Outputs
2024-02-04 07:40:39,746 ========================================================================================================================
2024-02-04 07:40:39,746 Logging Sequence: 159_139.00
2024-02-04 07:40:39,747 	Gloss Reference :	A B+C+D+E
2024-02-04 07:40:39,747 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:40:39,747 	Gloss Alignment :	         
2024-02-04 07:40:39,747 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:40:39,748 	Text Reference  :	***** **** **** he took time and     finally was ready for the asia cup where he      scored the century
2024-02-04 07:40:39,748 	Text Hypothesis :	dhoni said that he **** felt playing cricket was ***** *** *** **** *** not   present at     the man    
2024-02-04 07:40:39,749 	Text Alignment  :	I     I    I       D    S    S       S           D     D   D   D    D   S     S       S          S      
2024-02-04 07:40:39,749 ========================================================================================================================
2024-02-04 07:40:39,749 Logging Sequence: 159_159.00
2024-02-04 07:40:39,749 	Gloss Reference :	A B+C+D+E
2024-02-04 07:40:39,749 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:40:39,749 	Gloss Alignment :	         
2024-02-04 07:40:39,749 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:40:39,752 	Text Reference  :	he    said it       wasn't easy   the mind has to be focussed and he is glad that he is   back in form with   the asia     cup  century
2024-02-04 07:40:39,752 	Text Hypothesis :	kohli had  revealed that   before the **** *** ** ** ******** *** ** ** **** **** ** ball for  a  sai  before the incident went viral  
2024-02-04 07:40:39,752 	Text Alignment  :	S     S    S        S      S          D    D   D  D  D        D   D  D  D    D    D  S    S    S  S    S          S        S    S      
2024-02-04 07:40:39,752 ========================================================================================================================
2024-02-04 07:40:39,752 Logging Sequence: 103_8.00
2024-02-04 07:40:39,752 	Gloss Reference :	A B+C+D+E
2024-02-04 07:40:39,752 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:40:39,752 	Gloss Alignment :	         
2024-02-04 07:40:39,753 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:40:39,753 	Text Reference  :	were         going on  in birmingham england from 28th july  to        8th   august 2022 
2024-02-04 07:40:39,754 	Text Hypothesis :	commonwealth games cwg in ********** ******* **** **** which australia every 4      years
2024-02-04 07:40:39,754 	Text Alignment  :	S            S     S      D          D       D    D    S     S         S     S      S    
2024-02-04 07:40:39,754 ========================================================================================================================
2024-02-04 07:40:39,754 Logging Sequence: 164_546.00
2024-02-04 07:40:39,754 	Gloss Reference :	A B+C+D+E
2024-02-04 07:40:39,754 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:40:39,754 	Gloss Alignment :	         
2024-02-04 07:40:39,754 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:40:39,755 	Text Reference  :	******* * ** * ******* reliance has turned out     to be the ******** *** strongest company
2024-02-04 07:40:39,755 	Text Hypothesis :	package c is a special category of  mumbai indians to ** the audience was for       drinks 
2024-02-04 07:40:39,756 	Text Alignment  :	I       I I  I I       S        S   S      S          D      I        I   S         S      
2024-02-04 07:40:39,756 ========================================================================================================================
2024-02-04 07:40:39,756 Logging Sequence: 132_173.00
2024-02-04 07:40:39,756 	Gloss Reference :	A B+C+D+E
2024-02-04 07:40:39,756 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:40:39,756 	Gloss Alignment :	         
2024-02-04 07:40:39,756 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:40:39,757 	Text Reference  :	**** ********* ****** ******* *** ** ***** *** **** **** ** **** ****** *** ******* usman is   australia' first muslim player
2024-02-04 07:40:39,757 	Text Hypothesis :	bcci president sourav ganguly and kl rahul had said that he then asking the captain and   then going      on    his    face  
2024-02-04 07:40:39,757 	Text Alignment  :	I    I         I      I       I   I  I     I   I    I    I  I    I      I   I       S     S    S          S     S      S     
2024-02-04 07:40:39,758 ========================================================================================================================
2024-02-04 07:40:44,241 Epoch 295: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.94 
2024-02-04 07:40:44,243 EPOCH 296
2024-02-04 07:40:48,993 Epoch 296: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.98 
2024-02-04 07:40:48,993 EPOCH 297
2024-02-04 07:40:53,476 Epoch 297: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.01 
2024-02-04 07:40:53,476 EPOCH 298
2024-02-04 07:40:53,686 [Epoch: 298 Step: 00010100] Batch Recognition Loss:   0.000481 => Gls Tokens per Sec:     1875 || Batch Translation Loss:   0.012475 => Txt Tokens per Sec:     4990 || Lr: 0.000100
2024-02-04 07:40:58,119 Epoch 298: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.19 
2024-02-04 07:40:58,119 EPOCH 299
2024-02-04 07:41:02,827 Epoch 299: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.82 
2024-02-04 07:41:02,827 EPOCH 300
2024-02-04 07:41:07,334 [Epoch: 300 Step: 00010200] Batch Recognition Loss:   0.000209 => Gls Tokens per Sec:     2359 || Batch Translation Loss:   0.045971 => Txt Tokens per Sec:     6549 || Lr: 0.000100
2024-02-04 07:41:07,334 Epoch 300: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.99 
2024-02-04 07:41:07,334 EPOCH 301
2024-02-04 07:41:12,097 Epoch 301: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.59 
2024-02-04 07:41:12,097 EPOCH 302
2024-02-04 07:41:16,475 Epoch 302: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.19 
2024-02-04 07:41:16,475 EPOCH 303
2024-02-04 07:41:20,782 [Epoch: 303 Step: 00010300] Batch Recognition Loss:   0.000267 => Gls Tokens per Sec:     2320 || Batch Translation Loss:   0.060948 => Txt Tokens per Sec:     6448 || Lr: 0.000100
2024-02-04 07:41:21,010 Epoch 303: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.25 
2024-02-04 07:41:21,010 EPOCH 304
2024-02-04 07:41:25,692 Epoch 304: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.54 
2024-02-04 07:41:25,693 EPOCH 305
2024-02-04 07:41:30,328 Epoch 305: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.81 
2024-02-04 07:41:30,329 EPOCH 306
2024-02-04 07:41:34,244 [Epoch: 306 Step: 00010400] Batch Recognition Loss:   0.000401 => Gls Tokens per Sec:     2389 || Batch Translation Loss:   0.053449 => Txt Tokens per Sec:     6588 || Lr: 0.000100
2024-02-04 07:41:34,824 Epoch 306: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.94 
2024-02-04 07:41:34,824 EPOCH 307
2024-02-04 07:41:39,201 Epoch 307: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.21 
2024-02-04 07:41:39,202 EPOCH 308
2024-02-04 07:41:44,098 Epoch 308: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.51 
2024-02-04 07:41:44,098 EPOCH 309
2024-02-04 07:41:47,674 [Epoch: 309 Step: 00010500] Batch Recognition Loss:   0.000291 => Gls Tokens per Sec:     2437 || Batch Translation Loss:   0.215559 => Txt Tokens per Sec:     6960 || Lr: 0.000100
2024-02-04 07:41:48,253 Epoch 309: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.03 
2024-02-04 07:41:48,254 EPOCH 310
2024-02-04 07:41:53,135 Epoch 310: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.43 
2024-02-04 07:41:53,136 EPOCH 311
2024-02-04 07:41:57,445 Epoch 311: Total Training Recognition Loss 0.01  Total Training Translation Loss 7.10 
2024-02-04 07:41:57,445 EPOCH 312
2024-02-04 07:42:01,481 [Epoch: 312 Step: 00010600] Batch Recognition Loss:   0.000460 => Gls Tokens per Sec:     2000 || Batch Translation Loss:   0.553466 => Txt Tokens per Sec:     5640 || Lr: 0.000100
2024-02-04 07:42:02,447 Epoch 312: Total Training Recognition Loss 0.02  Total Training Translation Loss 8.15 
2024-02-04 07:42:02,448 EPOCH 313
2024-02-04 07:42:06,690 Epoch 313: Total Training Recognition Loss 0.02  Total Training Translation Loss 9.81 
2024-02-04 07:42:06,690 EPOCH 314
2024-02-04 07:42:10,928 Epoch 314: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.52 
2024-02-04 07:42:10,929 EPOCH 315
2024-02-04 07:42:14,433 [Epoch: 315 Step: 00010700] Batch Recognition Loss:   0.000420 => Gls Tokens per Sec:     2122 || Batch Translation Loss:   0.149969 => Txt Tokens per Sec:     5799 || Lr: 0.000100
2024-02-04 07:42:15,872 Epoch 315: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.60 
2024-02-04 07:42:15,873 EPOCH 316
2024-02-04 07:42:20,069 Epoch 316: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.68 
2024-02-04 07:42:20,070 EPOCH 317
2024-02-04 07:42:24,923 Epoch 317: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.83 
2024-02-04 07:42:24,924 EPOCH 318
2024-02-04 07:42:27,683 [Epoch: 318 Step: 00010800] Batch Recognition Loss:   0.000300 => Gls Tokens per Sec:     2461 || Batch Translation Loss:   0.076557 => Txt Tokens per Sec:     6643 || Lr: 0.000100
2024-02-04 07:42:29,204 Epoch 318: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.35 
2024-02-04 07:42:29,205 EPOCH 319
2024-02-04 07:42:33,252 Epoch 319: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.49 
2024-02-04 07:42:33,252 EPOCH 320
2024-02-04 07:42:37,775 Epoch 320: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.30 
2024-02-04 07:42:37,775 EPOCH 321
2024-02-04 07:42:40,557 [Epoch: 321 Step: 00010900] Batch Recognition Loss:   0.000290 => Gls Tokens per Sec:     2211 || Batch Translation Loss:   0.120389 => Txt Tokens per Sec:     6252 || Lr: 0.000100
2024-02-04 07:42:42,361 Epoch 321: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.17 
2024-02-04 07:42:42,361 EPOCH 322
2024-02-04 07:42:47,036 Epoch 322: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.04 
2024-02-04 07:42:47,037 EPOCH 323
2024-02-04 07:42:51,528 Epoch 323: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.08 
2024-02-04 07:42:51,528 EPOCH 324
2024-02-04 07:42:54,379 [Epoch: 324 Step: 00011000] Batch Recognition Loss:   0.000400 => Gls Tokens per Sec:     1933 || Batch Translation Loss:   0.087606 => Txt Tokens per Sec:     5656 || Lr: 0.000100
2024-02-04 07:42:56,319 Epoch 324: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.24 
2024-02-04 07:42:56,319 EPOCH 325
2024-02-04 07:43:00,673 Epoch 325: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.17 
2024-02-04 07:43:00,673 EPOCH 326
2024-02-04 07:43:05,535 Epoch 326: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.97 
2024-02-04 07:43:05,535 EPOCH 327
2024-02-04 07:43:07,627 [Epoch: 327 Step: 00011100] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     2449 || Batch Translation Loss:   0.040313 => Txt Tokens per Sec:     6902 || Lr: 0.000100
2024-02-04 07:43:09,874 Epoch 327: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.23 
2024-02-04 07:43:09,875 EPOCH 328
2024-02-04 07:43:14,806 Epoch 328: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.22 
2024-02-04 07:43:14,806 EPOCH 329
2024-02-04 07:43:19,212 Epoch 329: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.81 
2024-02-04 07:43:19,212 EPOCH 330
2024-02-04 07:43:21,569 [Epoch: 330 Step: 00011200] Batch Recognition Loss:   0.000283 => Gls Tokens per Sec:     1901 || Batch Translation Loss:   0.076971 => Txt Tokens per Sec:     5443 || Lr: 0.000100
2024-02-04 07:43:24,045 Epoch 330: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.66 
2024-02-04 07:43:24,045 EPOCH 331
2024-02-04 07:43:28,502 Epoch 331: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.82 
2024-02-04 07:43:28,503 EPOCH 332
2024-02-04 07:43:33,171 Epoch 332: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.17 
2024-02-04 07:43:33,171 EPOCH 333
2024-02-04 07:43:34,412 [Epoch: 333 Step: 00011300] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:     3099 || Batch Translation Loss:   0.050949 => Txt Tokens per Sec:     8064 || Lr: 0.000100
2024-02-04 07:43:37,744 Epoch 333: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.15 
2024-02-04 07:43:37,745 EPOCH 334
2024-02-04 07:43:42,284 Epoch 334: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.18 
2024-02-04 07:43:42,284 EPOCH 335
2024-02-04 07:43:46,554 Epoch 335: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.73 
2024-02-04 07:43:46,555 EPOCH 336
2024-02-04 07:43:47,832 [Epoch: 336 Step: 00011400] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:     2314 || Batch Translation Loss:   0.308856 => Txt Tokens per Sec:     5922 || Lr: 0.000100
2024-02-04 07:43:51,421 Epoch 336: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.95 
2024-02-04 07:43:51,421 EPOCH 337
2024-02-04 07:43:55,877 Epoch 337: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.69 
2024-02-04 07:43:55,877 EPOCH 338
2024-02-04 07:44:00,561 Epoch 338: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.21 
2024-02-04 07:44:00,562 EPOCH 339
2024-02-04 07:44:01,445 [Epoch: 339 Step: 00011500] Batch Recognition Loss:   0.000408 => Gls Tokens per Sec:     2899 || Batch Translation Loss:   0.103807 => Txt Tokens per Sec:     8189 || Lr: 0.000100
2024-02-04 07:44:05,167 Epoch 339: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.05 
2024-02-04 07:44:05,167 EPOCH 340
2024-02-04 07:44:09,688 Epoch 340: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.12 
2024-02-04 07:44:09,688 EPOCH 341
2024-02-04 07:44:14,437 Epoch 341: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.80 
2024-02-04 07:44:14,438 EPOCH 342
2024-02-04 07:44:15,117 [Epoch: 342 Step: 00011600] Batch Recognition Loss:   0.000620 => Gls Tokens per Sec:     2832 || Batch Translation Loss:   0.231398 => Txt Tokens per Sec:     7514 || Lr: 0.000100
2024-02-04 07:44:18,853 Epoch 342: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.77 
2024-02-04 07:44:18,853 EPOCH 343
2024-02-04 07:44:23,687 Epoch 343: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.60 
2024-02-04 07:44:23,688 EPOCH 344
2024-02-04 07:44:28,001 Epoch 344: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.03 
2024-02-04 07:44:28,001 EPOCH 345
2024-02-04 07:44:28,434 [Epoch: 345 Step: 00011700] Batch Recognition Loss:   0.000278 => Gls Tokens per Sec:     2963 || Batch Translation Loss:   0.123845 => Txt Tokens per Sec:     8440 || Lr: 0.000100
2024-02-04 07:44:32,768 Epoch 345: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.01 
2024-02-04 07:44:32,768 EPOCH 346
2024-02-04 07:44:37,100 Epoch 346: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.11 
2024-02-04 07:44:37,100 EPOCH 347
2024-02-04 07:44:42,023 Epoch 347: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.59 
2024-02-04 07:44:42,024 EPOCH 348
2024-02-04 07:44:42,199 [Epoch: 348 Step: 00011800] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     3699 || Batch Translation Loss:   0.124470 => Txt Tokens per Sec:     9786 || Lr: 0.000100
2024-02-04 07:44:46,997 Epoch 348: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.62 
2024-02-04 07:44:46,997 EPOCH 349
2024-02-04 07:44:51,792 Epoch 349: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.87 
2024-02-04 07:44:51,793 EPOCH 350
2024-02-04 07:44:56,649 [Epoch: 350 Step: 00011900] Batch Recognition Loss:   0.000343 => Gls Tokens per Sec:     2190 || Batch Translation Loss:   0.076486 => Txt Tokens per Sec:     6079 || Lr: 0.000100
2024-02-04 07:44:56,649 Epoch 350: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.94 
2024-02-04 07:44:56,650 EPOCH 351
2024-02-04 07:45:01,454 Epoch 351: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.79 
2024-02-04 07:45:01,455 EPOCH 352
2024-02-04 07:45:06,274 Epoch 352: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.08 
2024-02-04 07:45:06,274 EPOCH 353
2024-02-04 07:45:10,663 [Epoch: 353 Step: 00012000] Batch Recognition Loss:   0.000220 => Gls Tokens per Sec:     2276 || Batch Translation Loss:   0.033403 => Txt Tokens per Sec:     6258 || Lr: 0.000100
2024-02-04 07:45:19,570 Validation result at epoch 353, step    12000: duration: 8.9061s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00060	Translation Loss: 93047.62500	PPL: 11062.10156
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.88	(BLEU-1: 10.78,	BLEU-2: 3.71,	BLEU-3: 1.66,	BLEU-4: 0.88)
	CHRF 17.40	ROUGE 9.44
2024-02-04 07:45:19,572 Logging Recognition and Translation Outputs
2024-02-04 07:45:19,572 ========================================================================================================================
2024-02-04 07:45:19,572 Logging Sequence: 177_50.00
2024-02-04 07:45:19,572 	Gloss Reference :	A B+C+D+E
2024-02-04 07:45:19,572 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:45:19,572 	Gloss Alignment :	         
2024-02-04 07:45:19,572 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:45:19,573 	Text Reference  :	a similar reward of  rs        50000  was    announced for          information against his associate ajay kumar 
2024-02-04 07:45:19,573 	Text Hypothesis :	* ******* ****** but rajasthan royals issued a         non-bailable warrant     against *** ********* **** sushil
2024-02-04 07:45:19,574 	Text Alignment  :	D D       D      S   S         S      S      S         S            S                   D   D         D    S     
2024-02-04 07:45:19,574 ========================================================================================================================
2024-02-04 07:45:19,574 Logging Sequence: 122_86.00
2024-02-04 07:45:19,574 	Gloss Reference :	A B+C+D+E
2024-02-04 07:45:19,574 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:45:19,574 	Gloss Alignment :	         
2024-02-04 07:45:19,574 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:45:19,575 	Text Reference  :	after winning chanu spoke to the media and ****** said
2024-02-04 07:45:19,575 	Text Hypothesis :	i     am      very  hard  to *** train and scored 3175
2024-02-04 07:45:19,575 	Text Alignment  :	S     S       S     S        D   S         I      S   
2024-02-04 07:45:19,575 ========================================================================================================================
2024-02-04 07:45:19,575 Logging Sequence: 165_27.00
2024-02-04 07:45:19,576 	Gloss Reference :	A B+C+D+E
2024-02-04 07:45:19,576 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:45:19,576 	Gloss Alignment :	         
2024-02-04 07:45:19,576 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:45:19,577 	Text Reference  :	so then they change their      routes some people believe in  this while some don't    
2024-02-04 07:45:19,577 	Text Hypothesis :	** **** it   is     disgusting that   if   a      huge    fan has  been  very difficult
2024-02-04 07:45:19,577 	Text Alignment  :	D  D    S    S      S          S      S    S      S       S   S    S     S    S        
2024-02-04 07:45:19,578 ========================================================================================================================
2024-02-04 07:45:19,578 Logging Sequence: 70_65.00
2024-02-04 07:45:19,578 	Gloss Reference :	A B+C+D+E
2024-02-04 07:45:19,578 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:45:19,578 	Gloss Alignment :	         
2024-02-04 07:45:19,578 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:45:19,579 	Text Reference  :	during the press conference a table was placed in front   of the ******* ******** *** media    
2024-02-04 07:45:19,579 	Text Hypothesis :	****** *** ***** ********** * ***** *** this   is because of the various sponsors for marketing
2024-02-04 07:45:19,579 	Text Alignment  :	D      D   D     D          D D     D   S      S  S              I       I        I   S        
2024-02-04 07:45:19,579 ========================================================================================================================
2024-02-04 07:45:19,579 Logging Sequence: 149_65.00
2024-02-04 07:45:19,580 	Gloss Reference :	A B+C+D+E
2024-02-04 07:45:19,580 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:45:19,580 	Gloss Alignment :	         
2024-02-04 07:45:19,580 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:45:19,582 	Text Reference  :	at 6am on 6th november 2022  the     police reached  sri lankan team's      hotel in  sydney  australia's central business district cbd 
2024-02-04 07:45:19,582 	Text Hypothesis :	** *** ** *** the      woman alleged that   danushka had sexual intercourse with  her without her         consent which    means    rape
2024-02-04 07:45:19,582 	Text Alignment  :	D  D   D  D   S        S     S       S      S        S   S      S           S     S   S       S           S       S        S        S   
2024-02-04 07:45:19,582 ========================================================================================================================
2024-02-04 07:45:19,992 Epoch 353: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.61 
2024-02-04 07:45:19,992 EPOCH 354
2024-02-04 07:45:25,038 Epoch 354: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.48 
2024-02-04 07:45:25,039 EPOCH 355
2024-02-04 07:45:29,464 Epoch 355: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.13 
2024-02-04 07:45:29,465 EPOCH 356
2024-02-04 07:45:33,777 [Epoch: 356 Step: 00012100] Batch Recognition Loss:   0.000337 => Gls Tokens per Sec:     2169 || Batch Translation Loss:   0.071550 => Txt Tokens per Sec:     6000 || Lr: 0.000100
2024-02-04 07:45:34,352 Epoch 356: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.46 
2024-02-04 07:45:34,352 EPOCH 357
2024-02-04 07:45:38,738 Epoch 357: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.95 
2024-02-04 07:45:38,738 EPOCH 358
2024-02-04 07:45:43,524 Epoch 358: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.94 
2024-02-04 07:45:43,525 EPOCH 359
2024-02-04 07:45:47,230 [Epoch: 359 Step: 00012200] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     2352 || Batch Translation Loss:   0.033091 => Txt Tokens per Sec:     6579 || Lr: 0.000100
2024-02-04 07:45:48,035 Epoch 359: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.73 
2024-02-04 07:45:48,035 EPOCH 360
2024-02-04 07:45:52,717 Epoch 360: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.32 
2024-02-04 07:45:52,717 EPOCH 361
2024-02-04 07:45:56,902 Epoch 361: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.57 
2024-02-04 07:45:56,902 EPOCH 362
2024-02-04 07:46:00,731 [Epoch: 362 Step: 00012300] Batch Recognition Loss:   0.000259 => Gls Tokens per Sec:     2174 || Batch Translation Loss:   0.208105 => Txt Tokens per Sec:     5990 || Lr: 0.000100
2024-02-04 07:46:01,815 Epoch 362: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.30 
2024-02-04 07:46:01,815 EPOCH 363
2024-02-04 07:46:06,316 Epoch 363: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.29 
2024-02-04 07:46:06,316 EPOCH 364
2024-02-04 07:46:11,022 Epoch 364: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.50 
2024-02-04 07:46:11,022 EPOCH 365
2024-02-04 07:46:14,000 [Epoch: 365 Step: 00012400] Batch Recognition Loss:   0.000325 => Gls Tokens per Sec:     2496 || Batch Translation Loss:   0.093925 => Txt Tokens per Sec:     6684 || Lr: 0.000100
2024-02-04 07:46:15,575 Epoch 365: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.89 
2024-02-04 07:46:15,575 EPOCH 366
2024-02-04 07:46:20,172 Epoch 366: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.71 
2024-02-04 07:46:20,172 EPOCH 367
2024-02-04 07:46:24,838 Epoch 367: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.11 
2024-02-04 07:46:24,839 EPOCH 368
2024-02-04 07:46:27,512 [Epoch: 368 Step: 00012500] Batch Recognition Loss:   0.000491 => Gls Tokens per Sec:     2541 || Batch Translation Loss:   0.065190 => Txt Tokens per Sec:     6968 || Lr: 0.000100
2024-02-04 07:46:29,270 Epoch 368: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.07 
2024-02-04 07:46:29,271 EPOCH 369
2024-02-04 07:46:34,054 Epoch 369: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.50 
2024-02-04 07:46:34,055 EPOCH 370
2024-02-04 07:46:38,437 Epoch 370: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.03 
2024-02-04 07:46:38,437 EPOCH 371
2024-02-04 07:46:41,552 [Epoch: 371 Step: 00012600] Batch Recognition Loss:   0.000251 => Gls Tokens per Sec:     1975 || Batch Translation Loss:   0.118819 => Txt Tokens per Sec:     5634 || Lr: 0.000100
2024-02-04 07:46:43,308 Epoch 371: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.52 
2024-02-04 07:46:43,308 EPOCH 372
2024-02-04 07:46:47,602 Epoch 372: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.68 
2024-02-04 07:46:47,603 EPOCH 373
2024-02-04 07:46:52,550 Epoch 373: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.48 
2024-02-04 07:46:52,551 EPOCH 374
2024-02-04 07:46:54,531 [Epoch: 374 Step: 00012700] Batch Recognition Loss:   0.000259 => Gls Tokens per Sec:     2784 || Batch Translation Loss:   0.105803 => Txt Tokens per Sec:     7518 || Lr: 0.000100
2024-02-04 07:46:56,933 Epoch 374: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.79 
2024-02-04 07:46:56,934 EPOCH 375
2024-02-04 07:47:01,773 Epoch 375: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.10 
2024-02-04 07:47:01,774 EPOCH 376
2024-02-04 07:47:06,191 Epoch 376: Total Training Recognition Loss 0.02  Total Training Translation Loss 9.55 
2024-02-04 07:47:06,192 EPOCH 377
2024-02-04 07:47:08,851 [Epoch: 377 Step: 00012800] Batch Recognition Loss:   0.001049 => Gls Tokens per Sec:     1832 || Batch Translation Loss:   0.305884 => Txt Tokens per Sec:     5472 || Lr: 0.000100
2024-02-04 07:47:10,918 Epoch 377: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.50 
2024-02-04 07:47:10,918 EPOCH 378
2024-02-04 07:47:15,285 Epoch 378: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.38 
2024-02-04 07:47:15,285 EPOCH 379
2024-02-04 07:47:20,006 Epoch 379: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.32 
2024-02-04 07:47:20,007 EPOCH 380
2024-02-04 07:47:21,793 [Epoch: 380 Step: 00012900] Batch Recognition Loss:   0.000891 => Gls Tokens per Sec:     2370 || Batch Translation Loss:   0.166611 => Txt Tokens per Sec:     6913 || Lr: 0.000100
2024-02-04 07:47:24,536 Epoch 380: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.42 
2024-02-04 07:47:24,536 EPOCH 381
2024-02-04 07:47:29,245 Epoch 381: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.88 
2024-02-04 07:47:29,245 EPOCH 382
2024-02-04 07:47:33,844 Epoch 382: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.54 
2024-02-04 07:47:33,844 EPOCH 383
2024-02-04 07:47:35,112 [Epoch: 383 Step: 00013000] Batch Recognition Loss:   0.000266 => Gls Tokens per Sec:     3031 || Batch Translation Loss:   0.030539 => Txt Tokens per Sec:     7579 || Lr: 0.000100
2024-02-04 07:47:38,376 Epoch 383: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.29 
2024-02-04 07:47:38,376 EPOCH 384
2024-02-04 07:47:43,146 Epoch 384: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.10 
2024-02-04 07:47:43,147 EPOCH 385
2024-02-04 07:47:47,612 Epoch 385: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.58 
2024-02-04 07:47:47,613 EPOCH 386
2024-02-04 07:47:49,502 [Epoch: 386 Step: 00013100] Batch Recognition Loss:   0.000433 => Gls Tokens per Sec:     1693 || Batch Translation Loss:   0.129882 => Txt Tokens per Sec:     5067 || Lr: 0.000100
2024-02-04 07:47:52,450 Epoch 386: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.16 
2024-02-04 07:47:52,451 EPOCH 387
2024-02-04 07:47:56,723 Epoch 387: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.07 
2024-02-04 07:47:56,723 EPOCH 388
2024-02-04 07:48:01,342 Epoch 388: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.89 
2024-02-04 07:48:01,342 EPOCH 389
2024-02-04 07:48:03,070 [Epoch: 389 Step: 00013200] Batch Recognition Loss:   0.000500 => Gls Tokens per Sec:     1483 || Batch Translation Loss:   0.093834 => Txt Tokens per Sec:     4362 || Lr: 0.000100
2024-02-04 07:48:06,585 Epoch 389: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.61 
2024-02-04 07:48:06,585 EPOCH 390
2024-02-04 07:48:11,083 Epoch 390: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.32 
2024-02-04 07:48:11,084 EPOCH 391
2024-02-04 07:48:15,865 Epoch 391: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.14 
2024-02-04 07:48:15,865 EPOCH 392
2024-02-04 07:48:16,482 [Epoch: 392 Step: 00013300] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:     3117 || Batch Translation Loss:   0.055241 => Txt Tokens per Sec:     8255 || Lr: 0.000100
2024-02-04 07:48:20,315 Epoch 392: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.79 
2024-02-04 07:48:20,316 EPOCH 393
2024-02-04 07:48:24,963 Epoch 393: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.78 
2024-02-04 07:48:24,963 EPOCH 394
2024-02-04 07:48:29,574 Epoch 394: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.97 
2024-02-04 07:48:29,574 EPOCH 395
2024-02-04 07:48:30,292 [Epoch: 395 Step: 00013400] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:     1787 || Batch Translation Loss:   0.050061 => Txt Tokens per Sec:     4736 || Lr: 0.000100
2024-02-04 07:48:34,141 Epoch 395: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.69 
2024-02-04 07:48:34,141 EPOCH 396
2024-02-04 07:48:38,852 Epoch 396: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.09 
2024-02-04 07:48:38,853 EPOCH 397
2024-02-04 07:48:43,316 Epoch 397: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.61 
2024-02-04 07:48:43,316 EPOCH 398
2024-02-04 07:48:43,808 [Epoch: 398 Step: 00013500] Batch Recognition Loss:   0.000390 => Gls Tokens per Sec:     1304 || Batch Translation Loss:   0.040828 => Txt Tokens per Sec:     4538 || Lr: 0.000100
2024-02-04 07:48:48,132 Epoch 398: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.17 
2024-02-04 07:48:48,133 EPOCH 399
2024-02-04 07:48:52,409 Epoch 399: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.29 
2024-02-04 07:48:52,409 EPOCH 400
2024-02-04 07:48:57,409 [Epoch: 400 Step: 00013600] Batch Recognition Loss:   0.000595 => Gls Tokens per Sec:     2126 || Batch Translation Loss:   0.011950 => Txt Tokens per Sec:     5903 || Lr: 0.000100
2024-02-04 07:48:57,410 Epoch 400: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.78 
2024-02-04 07:48:57,410 EPOCH 401
2024-02-04 07:49:01,748 Epoch 401: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.26 
2024-02-04 07:49:01,748 EPOCH 402
2024-02-04 07:49:06,631 Epoch 402: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.56 
2024-02-04 07:49:06,632 EPOCH 403
2024-02-04 07:49:10,489 [Epoch: 403 Step: 00013700] Batch Recognition Loss:   0.000261 => Gls Tokens per Sec:     2656 || Batch Translation Loss:   0.052609 => Txt Tokens per Sec:     7299 || Lr: 0.000100
2024-02-04 07:49:10,801 Epoch 403: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.69 
2024-02-04 07:49:10,801 EPOCH 404
2024-02-04 07:49:15,692 Epoch 404: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.87 
2024-02-04 07:49:15,693 EPOCH 405
2024-02-04 07:49:20,238 Epoch 405: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-04 07:49:20,239 EPOCH 406
2024-02-04 07:49:24,694 [Epoch: 406 Step: 00013800] Batch Recognition Loss:   0.000262 => Gls Tokens per Sec:     2099 || Batch Translation Loss:   0.032082 => Txt Tokens per Sec:     5858 || Lr: 0.000100
2024-02-04 07:49:25,133 Epoch 406: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.65 
2024-02-04 07:49:25,133 EPOCH 407
2024-02-04 07:49:29,189 Epoch 407: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.19 
2024-02-04 07:49:29,189 EPOCH 408
2024-02-04 07:49:33,966 Epoch 408: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.74 
2024-02-04 07:49:33,967 EPOCH 409
2024-02-04 07:49:37,909 [Epoch: 409 Step: 00013900] Batch Recognition Loss:   0.000413 => Gls Tokens per Sec:     2210 || Batch Translation Loss:   0.103959 => Txt Tokens per Sec:     6070 || Lr: 0.000100
2024-02-04 07:49:38,771 Epoch 409: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.35 
2024-02-04 07:49:38,771 EPOCH 410
2024-02-04 07:49:43,579 Epoch 410: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.25 
2024-02-04 07:49:43,580 EPOCH 411
2024-02-04 07:49:48,584 Epoch 411: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.85 
2024-02-04 07:49:48,585 EPOCH 412
2024-02-04 07:49:52,350 [Epoch: 412 Step: 00014000] Batch Recognition Loss:   0.000270 => Gls Tokens per Sec:     2144 || Batch Translation Loss:   0.049406 => Txt Tokens per Sec:     6029 || Lr: 0.000100
2024-02-04 07:50:00,932 Validation result at epoch 412, step    14000: duration: 8.5822s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00062	Translation Loss: 94365.78906	PPL: 12621.91211
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.77	(BLEU-1: 10.75,	BLEU-2: 3.35,	BLEU-3: 1.45,	BLEU-4: 0.77)
	CHRF 16.67	ROUGE 9.34
2024-02-04 07:50:00,933 Logging Recognition and Translation Outputs
2024-02-04 07:50:00,933 ========================================================================================================================
2024-02-04 07:50:00,934 Logging Sequence: 141_40.00
2024-02-04 07:50:00,934 	Gloss Reference :	A B+C+D+E
2024-02-04 07:50:00,935 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:50:00,935 	Gloss Alignment :	         
2024-02-04 07:50:00,935 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:50:00,936 	Text Reference  :	got infected with covid-19 he   was     quarantined and  could not        take part in the warmup match   
2024-02-04 07:50:00,936 	Text Hypothesis :	*** and      told him      that mirabai who         gave a     five-match the  2000 on the ****** olympics
2024-02-04 07:50:00,936 	Text Alignment  :	D   S        S    S        S    S       S           S    S     S          S    S    S      D      S       
2024-02-04 07:50:00,937 ========================================================================================================================
2024-02-04 07:50:00,937 Logging Sequence: 117_37.00
2024-02-04 07:50:00,937 	Gloss Reference :	A B+C+D+E
2024-02-04 07:50:00,937 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:50:00,937 	Gloss Alignment :	         
2024-02-04 07:50:00,937 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:50:00,938 	Text Reference  :	****** shikhar dhawan put   up   a    wonderful performance scoring 98     runs  
2024-02-04 07:50:00,938 	Text Hypothesis :	krunal pandya  and    rahul took part of        pant        and     hardik pandya
2024-02-04 07:50:00,938 	Text Alignment  :	I      S       S      S     S    S    S         S           S       S      S     
2024-02-04 07:50:00,938 ========================================================================================================================
2024-02-04 07:50:00,939 Logging Sequence: 64_13.00
2024-02-04 07:50:00,939 	Gloss Reference :	A B+C+D+E
2024-02-04 07:50:00,939 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:50:00,939 	Gloss Alignment :	         
2024-02-04 07:50:00,939 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:50:00,940 	Text Reference  :	arrangements were made to move all  the    ipl    matches to the wankhede stadium in   mumbai
2024-02-04 07:50:00,940 	Text Hypothesis :	************ **** **** ** and  then deepak chahar not     to *** ******** play    very well  
2024-02-04 07:50:00,940 	Text Alignment  :	D            D    D    D  S    S    S      S      S          D   D        S       S    S     
2024-02-04 07:50:00,940 ========================================================================================================================
2024-02-04 07:50:00,940 Logging Sequence: 98_121.00
2024-02-04 07:50:00,941 	Gloss Reference :	A B+C+D+E
2024-02-04 07:50:00,941 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:50:00,941 	Gloss Alignment :	         
2024-02-04 07:50:00,941 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:50:00,942 	Text Reference  :	so then england   legends and     bangladesh legends were added to    the tournament
2024-02-04 07:50:00,942 	Text Hypothesis :	** **** according to      various news       reports were ***** taken the tickets   
2024-02-04 07:50:00,942 	Text Alignment  :	D  D    S         S       S       S          S            D     S         S         
2024-02-04 07:50:00,942 ========================================================================================================================
2024-02-04 07:50:00,942 Logging Sequence: 179_414.00
2024-02-04 07:50:00,942 	Gloss Reference :	A B+C+D+E
2024-02-04 07:50:00,943 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:50:00,943 	Gloss Alignment :	         
2024-02-04 07:50:00,943 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:50:00,944 	Text Reference  :	we      could not        travel to   delhi as there was  a     lockdown in   our home town haryana 
2024-02-04 07:50:00,944 	Text Hypothesis :	however zabka federation said   that he    is a     fine other vehicles that she is   the  passport
2024-02-04 07:50:00,945 	Text Alignment  :	S       S     S          S      S    S     S  S     S    S     S        S    S   S    S    S       
2024-02-04 07:50:00,945 ========================================================================================================================
2024-02-04 07:50:01,878 Epoch 412: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.98 
2024-02-04 07:50:01,879 EPOCH 413
2024-02-04 07:50:06,804 Epoch 413: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.79 
2024-02-04 07:50:06,804 EPOCH 414
2024-02-04 07:50:11,505 Epoch 414: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.35 
2024-02-04 07:50:11,505 EPOCH 415
2024-02-04 07:50:14,857 [Epoch: 415 Step: 00014100] Batch Recognition Loss:   0.000450 => Gls Tokens per Sec:     2218 || Batch Translation Loss:   0.121707 => Txt Tokens per Sec:     6085 || Lr: 0.000100
2024-02-04 07:50:16,227 Epoch 415: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.93 
2024-02-04 07:50:16,227 EPOCH 416
2024-02-04 07:50:20,759 Epoch 416: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.92 
2024-02-04 07:50:20,759 EPOCH 417
2024-02-04 07:50:25,402 Epoch 417: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.49 
2024-02-04 07:50:25,403 EPOCH 418
2024-02-04 07:50:28,466 [Epoch: 418 Step: 00014200] Batch Recognition Loss:   0.000633 => Gls Tokens per Sec:     2300 || Batch Translation Loss:   0.155702 => Txt Tokens per Sec:     6445 || Lr: 0.000100
2024-02-04 07:50:29,825 Epoch 418: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.36 
2024-02-04 07:50:29,826 EPOCH 419
2024-02-04 07:50:34,684 Epoch 419: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.29 
2024-02-04 07:50:34,684 EPOCH 420
2024-02-04 07:50:38,967 Epoch 420: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.29 
2024-02-04 07:50:38,967 EPOCH 421
2024-02-04 07:50:41,816 [Epoch: 421 Step: 00014300] Batch Recognition Loss:   0.000555 => Gls Tokens per Sec:     2159 || Batch Translation Loss:   0.392785 => Txt Tokens per Sec:     6064 || Lr: 0.000100
2024-02-04 07:50:43,685 Epoch 421: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.20 
2024-02-04 07:50:43,685 EPOCH 422
2024-02-04 07:50:48,118 Epoch 422: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.15 
2024-02-04 07:50:48,118 EPOCH 423
2024-02-04 07:50:52,929 Epoch 423: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.91 
2024-02-04 07:50:52,930 EPOCH 424
2024-02-04 07:50:55,144 [Epoch: 424 Step: 00014400] Batch Recognition Loss:   0.000680 => Gls Tokens per Sec:     2603 || Batch Translation Loss:   0.057762 => Txt Tokens per Sec:     7304 || Lr: 0.000100
2024-02-04 07:50:57,231 Epoch 424: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.74 
2024-02-04 07:50:57,231 EPOCH 425
2024-02-04 07:51:02,288 Epoch 425: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.35 
2024-02-04 07:51:02,289 EPOCH 426
2024-02-04 07:51:07,207 Epoch 426: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.85 
2024-02-04 07:51:07,208 EPOCH 427
2024-02-04 07:51:09,269 [Epoch: 427 Step: 00014500] Batch Recognition Loss:   0.000230 => Gls Tokens per Sec:     2486 || Batch Translation Loss:   0.083314 => Txt Tokens per Sec:     7189 || Lr: 0.000100
2024-02-04 07:51:11,843 Epoch 427: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.76 
2024-02-04 07:51:11,843 EPOCH 428
2024-02-04 07:51:15,881 Epoch 428: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.30 
2024-02-04 07:51:15,881 EPOCH 429
2024-02-04 07:51:20,798 Epoch 429: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.44 
2024-02-04 07:51:20,798 EPOCH 430
2024-02-04 07:51:22,530 [Epoch: 430 Step: 00014600] Batch Recognition Loss:   0.000263 => Gls Tokens per Sec:     2589 || Batch Translation Loss:   0.024820 => Txt Tokens per Sec:     7163 || Lr: 0.000100
2024-02-04 07:51:25,192 Epoch 430: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.56 
2024-02-04 07:51:25,193 EPOCH 431
2024-02-04 07:51:30,101 Epoch 431: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.98 
2024-02-04 07:51:30,101 EPOCH 432
2024-02-04 07:51:34,560 Epoch 432: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.49 
2024-02-04 07:51:34,560 EPOCH 433
2024-02-04 07:51:36,576 [Epoch: 433 Step: 00014700] Batch Recognition Loss:   0.000258 => Gls Tokens per Sec:     1908 || Batch Translation Loss:   0.073308 => Txt Tokens per Sec:     5578 || Lr: 0.000100
2024-02-04 07:51:39,420 Epoch 433: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.37 
2024-02-04 07:51:39,420 EPOCH 434
2024-02-04 07:51:43,948 Epoch 434: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.41 
2024-02-04 07:51:43,949 EPOCH 435
2024-02-04 07:51:48,576 Epoch 435: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-04 07:51:48,576 EPOCH 436
2024-02-04 07:51:49,903 [Epoch: 436 Step: 00014800] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     2414 || Batch Translation Loss:   0.088242 => Txt Tokens per Sec:     6684 || Lr: 0.000100
2024-02-04 07:51:52,895 Epoch 436: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.53 
2024-02-04 07:51:52,896 EPOCH 437
2024-02-04 07:51:57,736 Epoch 437: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.42 
2024-02-04 07:51:57,736 EPOCH 438
2024-02-04 07:52:02,241 Epoch 438: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.41 
2024-02-04 07:52:02,242 EPOCH 439
2024-02-04 07:52:03,166 [Epoch: 439 Step: 00014900] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:     2774 || Batch Translation Loss:   0.057359 => Txt Tokens per Sec:     7300 || Lr: 0.000100
2024-02-04 07:52:07,027 Epoch 439: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.31 
2024-02-04 07:52:07,027 EPOCH 440
2024-02-04 07:52:11,577 Epoch 440: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.15 
2024-02-04 07:52:11,577 EPOCH 441
2024-02-04 07:52:16,091 Epoch 441: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.01 
2024-02-04 07:52:16,091 EPOCH 442
2024-02-04 07:52:16,692 [Epoch: 442 Step: 00015000] Batch Recognition Loss:   0.000241 => Gls Tokens per Sec:     3200 || Batch Translation Loss:   0.043436 => Txt Tokens per Sec:     8218 || Lr: 0.000100
2024-02-04 07:52:20,764 Epoch 442: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.92 
2024-02-04 07:52:20,765 EPOCH 443
2024-02-04 07:52:25,308 Epoch 443: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.06 
2024-02-04 07:52:25,309 EPOCH 444
2024-02-04 07:52:30,116 Epoch 444: Total Training Recognition Loss 0.01  Total Training Translation Loss 7.73 
2024-02-04 07:52:30,116 EPOCH 445
2024-02-04 07:52:30,509 [Epoch: 445 Step: 00015100] Batch Recognition Loss:   0.000360 => Gls Tokens per Sec:     3265 || Batch Translation Loss:   0.170931 => Txt Tokens per Sec:     7903 || Lr: 0.000100
2024-02-04 07:52:34,466 Epoch 445: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.55 
2024-02-04 07:52:34,466 EPOCH 446
2024-02-04 07:52:39,123 Epoch 446: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.07 
2024-02-04 07:52:39,123 EPOCH 447
2024-02-04 07:52:43,789 Epoch 447: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.45 
2024-02-04 07:52:43,790 EPOCH 448
2024-02-04 07:52:44,107 [Epoch: 448 Step: 00015200] Batch Recognition Loss:   0.000412 => Gls Tokens per Sec:     2025 || Batch Translation Loss:   0.044069 => Txt Tokens per Sec:     6437 || Lr: 0.000100
2024-02-04 07:52:48,549 Epoch 448: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.31 
2024-02-04 07:52:48,549 EPOCH 449
2024-02-04 07:52:52,945 Epoch 449: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.41 
2024-02-04 07:52:52,945 EPOCH 450
2024-02-04 07:52:57,473 [Epoch: 450 Step: 00015300] Batch Recognition Loss:   0.000320 => Gls Tokens per Sec:     2348 || Batch Translation Loss:   0.032065 => Txt Tokens per Sec:     6518 || Lr: 0.000100
2024-02-04 07:52:57,473 Epoch 450: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-04 07:52:57,473 EPOCH 451
2024-02-04 07:53:02,085 Epoch 451: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-04 07:53:02,085 EPOCH 452
2024-02-04 07:53:06,704 Epoch 452: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.56 
2024-02-04 07:53:06,704 EPOCH 453
2024-02-04 07:53:11,406 [Epoch: 453 Step: 00015400] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     2126 || Batch Translation Loss:   0.017670 => Txt Tokens per Sec:     5877 || Lr: 0.000100
2024-02-04 07:53:11,714 Epoch 453: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.63 
2024-02-04 07:53:11,714 EPOCH 454
2024-02-04 07:53:16,038 Epoch 454: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-04 07:53:16,038 EPOCH 455
2024-02-04 07:53:20,941 Epoch 455: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.75 
2024-02-04 07:53:20,941 EPOCH 456
2024-02-04 07:53:24,688 [Epoch: 456 Step: 00015500] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     2497 || Batch Translation Loss:   0.037628 => Txt Tokens per Sec:     6869 || Lr: 0.000100
2024-02-04 07:53:25,414 Epoch 456: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.70 
2024-02-04 07:53:25,415 EPOCH 457
2024-02-04 07:53:30,304 Epoch 457: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.90 
2024-02-04 07:53:30,304 EPOCH 458
2024-02-04 07:53:35,299 Epoch 458: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.36 
2024-02-04 07:53:35,300 EPOCH 459
2024-02-04 07:53:38,567 [Epoch: 459 Step: 00015600] Batch Recognition Loss:   0.000265 => Gls Tokens per Sec:     2744 || Batch Translation Loss:   0.045313 => Txt Tokens per Sec:     7485 || Lr: 0.000100
2024-02-04 07:53:39,552 Epoch 459: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.06 
2024-02-04 07:53:39,552 EPOCH 460
2024-02-04 07:53:43,563 Epoch 460: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.27 
2024-02-04 07:53:43,563 EPOCH 461
2024-02-04 07:53:47,637 Epoch 461: Total Training Recognition Loss 0.01  Total Training Translation Loss 6.67 
2024-02-04 07:53:47,638 EPOCH 462
2024-02-04 07:53:51,400 [Epoch: 462 Step: 00015700] Batch Recognition Loss:   0.000422 => Gls Tokens per Sec:     2212 || Batch Translation Loss:   0.079226 => Txt Tokens per Sec:     6063 || Lr: 0.000100
2024-02-04 07:53:52,528 Epoch 462: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.81 
2024-02-04 07:53:52,528 EPOCH 463
2024-02-04 07:53:56,808 Epoch 463: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.51 
2024-02-04 07:53:56,809 EPOCH 464
2024-02-04 07:54:01,958 Epoch 464: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.24 
2024-02-04 07:54:01,959 EPOCH 465
2024-02-04 07:54:05,181 [Epoch: 465 Step: 00015800] Batch Recognition Loss:   0.000496 => Gls Tokens per Sec:     2307 || Batch Translation Loss:   0.084826 => Txt Tokens per Sec:     6301 || Lr: 0.000100
2024-02-04 07:54:06,848 Epoch 465: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.98 
2024-02-04 07:54:06,848 EPOCH 466
2024-02-04 07:54:11,707 Epoch 466: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.70 
2024-02-04 07:54:11,707 EPOCH 467
2024-02-04 07:54:16,495 Epoch 467: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.07 
2024-02-04 07:54:16,496 EPOCH 468
2024-02-04 07:54:19,589 [Epoch: 468 Step: 00015900] Batch Recognition Loss:   0.000251 => Gls Tokens per Sec:     2196 || Batch Translation Loss:   0.023604 => Txt Tokens per Sec:     6390 || Lr: 0.000100
2024-02-04 07:54:20,781 Epoch 468: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.54 
2024-02-04 07:54:20,781 EPOCH 469
2024-02-04 07:54:25,198 Epoch 469: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.08 
2024-02-04 07:54:25,198 EPOCH 470
2024-02-04 07:54:29,937 Epoch 470: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.06 
2024-02-04 07:54:29,937 EPOCH 471
2024-02-04 07:54:32,200 [Epoch: 471 Step: 00016000] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     2829 || Batch Translation Loss:   0.017090 => Txt Tokens per Sec:     7508 || Lr: 0.000100
2024-02-04 07:54:41,034 Validation result at epoch 471, step    16000: duration: 8.8340s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00047	Translation Loss: 94308.07031	PPL: 12549.21289
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.79	(BLEU-1: 11.52,	BLEU-2: 3.67,	BLEU-3: 1.51,	BLEU-4: 0.79)
	CHRF 17.45	ROUGE 9.56
2024-02-04 07:54:41,035 Logging Recognition and Translation Outputs
2024-02-04 07:54:41,036 ========================================================================================================================
2024-02-04 07:54:41,036 Logging Sequence: 147_132.00
2024-02-04 07:54:41,036 	Gloss Reference :	A B+C+D+E
2024-02-04 07:54:41,036 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:54:41,037 	Gloss Alignment :	         
2024-02-04 07:54:41,037 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:54:41,038 	Text Reference  :	i    can not **** ** **** earlier i   used   to ***** have fun in     gymnastics
2024-02-04 07:54:41,038 	Text Hypothesis :	they do  not want to risk this    and wanted to focus on   her mental health    
2024-02-04 07:54:41,038 	Text Alignment  :	S    S       I    I  I    S       S   S         I     S    S   S      S         
2024-02-04 07:54:41,038 ========================================================================================================================
2024-02-04 07:54:41,038 Logging Sequence: 116_162.00
2024-02-04 07:54:41,038 	Gloss Reference :	A B+C+D+E
2024-02-04 07:54:41,039 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:54:41,039 	Gloss Alignment :	         
2024-02-04 07:54:41,039 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:54:41,041 	Text Reference  :	***** turned out   the     video  was shared on    social media by   a         staff at the ******* *** ******** **** * hotel
2024-02-04 07:54:41,041 	Text Hypothesis :	after this   match gambhir issued a   video  along with   two   12th september 2023  at the stadium and pakistan what a man  
2024-02-04 07:54:41,041 	Text Alignment  :	I     S      S     S       S      S   S      S     S      S     S    S         S            I       I   I        I    I S    
2024-02-04 07:54:41,041 ========================================================================================================================
2024-02-04 07:54:41,041 Logging Sequence: 73_79.00
2024-02-04 07:54:41,041 	Gloss Reference :	A B+C+D+E
2024-02-04 07:54:41,041 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:54:41,042 	Gloss Alignment :	         
2024-02-04 07:54:41,042 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:54:41,043 	Text Reference  :	raina resturant has food from the  rich  spices of    north india   to  the aromatic curries of south  india  
2024-02-04 07:54:41,043 	Text Hypothesis :	***** ********* *** **** on   23rd march 2023   raina loves playing for the ******** ******* ** police station
2024-02-04 07:54:41,043 	Text Alignment  :	D     D         D   D    S    S    S     S      S     S     S       S       D        D       D  S      S      
2024-02-04 07:54:41,043 ========================================================================================================================
2024-02-04 07:54:41,044 Logging Sequence: 165_523.00
2024-02-04 07:54:41,044 	Gloss Reference :	A B+C+D+E
2024-02-04 07:54:41,044 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:54:41,044 	Gloss Alignment :	         
2024-02-04 07:54:41,044 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:54:41,045 	Text Reference  :	**** as   he   believed that his  team might     lose if   he   takes off  his batting pads 
2024-02-04 07:54:41,046 	Text Hypothesis :	when they were batting  and  well to   celebrate the  2011 when the   team won the     match
2024-02-04 07:54:41,046 	Text Alignment  :	I    S    S    S        S    S    S    S         S    S    S    S     S    S   S       S    
2024-02-04 07:54:41,046 ========================================================================================================================
2024-02-04 07:54:41,046 Logging Sequence: 125_72.00
2024-02-04 07:54:41,046 	Gloss Reference :	A B+C+D+E
2024-02-04 07:54:41,046 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:54:41,047 	Gloss Alignment :	         
2024-02-04 07:54:41,047 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:54:41,047 	Text Reference  :	some said the pakistani javelineer had milicious intentions of tampering with the  javelin      out of    jealousy  
2024-02-04 07:54:41,048 	Text Hypothesis :	**** **** *** ********* ********** *** ********* ********** ** neeraj    was  very disappointed by  these statements
2024-02-04 07:54:41,048 	Text Alignment  :	D    D    D   D         D          D   D         D          D  S         S    S    S            S   S     S         
2024-02-04 07:54:41,048 ========================================================================================================================
2024-02-04 07:54:43,167 Epoch 471: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.95 
2024-02-04 07:54:43,167 EPOCH 472
2024-02-04 07:54:48,210 Epoch 472: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.87 
2024-02-04 07:54:48,211 EPOCH 473
2024-02-04 07:54:52,427 Epoch 473: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.98 
2024-02-04 07:54:52,427 EPOCH 474
2024-02-04 07:54:54,546 [Epoch: 474 Step: 00016100] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     2720 || Batch Translation Loss:   0.064946 => Txt Tokens per Sec:     7440 || Lr: 0.000100
2024-02-04 07:54:56,948 Epoch 474: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.57 
2024-02-04 07:54:56,949 EPOCH 475
2024-02-04 07:55:01,545 Epoch 475: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.89 
2024-02-04 07:55:01,546 EPOCH 476
2024-02-04 07:55:06,163 Epoch 476: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.38 
2024-02-04 07:55:06,164 EPOCH 477
2024-02-04 07:55:08,381 [Epoch: 477 Step: 00016200] Batch Recognition Loss:   0.000318 => Gls Tokens per Sec:     2310 || Batch Translation Loss:   0.035686 => Txt Tokens per Sec:     6427 || Lr: 0.000100
2024-02-04 07:55:10,673 Epoch 477: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.75 
2024-02-04 07:55:10,674 EPOCH 478
2024-02-04 07:55:15,490 Epoch 478: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.60 
2024-02-04 07:55:15,491 EPOCH 479
2024-02-04 07:55:19,845 Epoch 479: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.20 
2024-02-04 07:55:19,845 EPOCH 480
2024-02-04 07:55:22,094 [Epoch: 480 Step: 00016300] Batch Recognition Loss:   0.000445 => Gls Tokens per Sec:     1994 || Batch Translation Loss:   0.212164 => Txt Tokens per Sec:     5590 || Lr: 0.000100
2024-02-04 07:55:24,696 Epoch 480: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.01 
2024-02-04 07:55:24,697 EPOCH 481
2024-02-04 07:55:29,036 Epoch 481: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.37 
2024-02-04 07:55:29,037 EPOCH 482
2024-02-04 07:55:33,124 Epoch 482: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.53 
2024-02-04 07:55:33,124 EPOCH 483
2024-02-04 07:55:34,262 [Epoch: 483 Step: 00016400] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     3156 || Batch Translation Loss:   0.105821 => Txt Tokens per Sec:     8261 || Lr: 0.000100
2024-02-04 07:55:37,865 Epoch 483: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.33 
2024-02-04 07:55:37,866 EPOCH 484
2024-02-04 07:55:42,289 Epoch 484: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.90 
2024-02-04 07:55:42,289 EPOCH 485
2024-02-04 07:55:47,081 Epoch 485: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.11 
2024-02-04 07:55:47,081 EPOCH 486
2024-02-04 07:55:48,443 [Epoch: 486 Step: 00016500] Batch Recognition Loss:   0.000278 => Gls Tokens per Sec:     2351 || Batch Translation Loss:   0.143452 => Txt Tokens per Sec:     6580 || Lr: 0.000100
2024-02-04 07:55:51,410 Epoch 486: Total Training Recognition Loss 0.01  Total Training Translation Loss 6.14 
2024-02-04 07:55:51,411 EPOCH 487
2024-02-04 07:55:56,581 Epoch 487: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.51 
2024-02-04 07:55:56,581 EPOCH 488
2024-02-04 07:56:01,057 Epoch 488: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.56 
2024-02-04 07:56:01,057 EPOCH 489
2024-02-04 07:56:01,890 [Epoch: 489 Step: 00016600] Batch Recognition Loss:   0.000361 => Gls Tokens per Sec:     3080 || Batch Translation Loss:   0.093457 => Txt Tokens per Sec:     8364 || Lr: 0.000100
2024-02-04 07:56:05,768 Epoch 489: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.74 
2024-02-04 07:56:05,768 EPOCH 490
2024-02-04 07:56:10,240 Epoch 490: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.97 
2024-02-04 07:56:10,240 EPOCH 491
2024-02-04 07:56:15,014 Epoch 491: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.84 
2024-02-04 07:56:15,016 EPOCH 492
2024-02-04 07:56:15,979 [Epoch: 492 Step: 00016700] Batch Recognition Loss:   0.000337 => Gls Tokens per Sec:     1994 || Batch Translation Loss:   0.076990 => Txt Tokens per Sec:     5405 || Lr: 0.000100
2024-02-04 07:56:19,379 Epoch 492: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.91 
2024-02-04 07:56:19,379 EPOCH 493
2024-02-04 07:56:24,261 Epoch 493: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.80 
2024-02-04 07:56:24,262 EPOCH 494
2024-02-04 07:56:28,650 Epoch 494: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.44 
2024-02-04 07:56:28,651 EPOCH 495
2024-02-04 07:56:29,165 [Epoch: 495 Step: 00016800] Batch Recognition Loss:   0.000351 => Gls Tokens per Sec:     2498 || Batch Translation Loss:   0.036186 => Txt Tokens per Sec:     6943 || Lr: 0.000100
2024-02-04 07:56:33,524 Epoch 495: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.92 
2024-02-04 07:56:33,524 EPOCH 496
2024-02-04 07:56:37,750 Epoch 496: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.20 
2024-02-04 07:56:37,751 EPOCH 497
2024-02-04 07:56:42,641 Epoch 497: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.17 
2024-02-04 07:56:42,641 EPOCH 498
2024-02-04 07:56:42,961 [Epoch: 498 Step: 00016900] Batch Recognition Loss:   0.000391 => Gls Tokens per Sec:     2010 || Batch Translation Loss:   0.051554 => Txt Tokens per Sec:     7050 || Lr: 0.000100
2024-02-04 07:56:47,014 Epoch 498: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.54 
2024-02-04 07:56:47,015 EPOCH 499
2024-02-04 07:56:51,772 Epoch 499: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.44 
2024-02-04 07:56:51,772 EPOCH 500
2024-02-04 07:56:56,215 [Epoch: 500 Step: 00017000] Batch Recognition Loss:   0.000247 => Gls Tokens per Sec:     2393 || Batch Translation Loss:   0.034771 => Txt Tokens per Sec:     6643 || Lr: 0.000100
2024-02-04 07:56:56,215 Epoch 500: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.54 
2024-02-04 07:56:56,216 EPOCH 501
2024-02-04 07:57:00,948 Epoch 501: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.52 
2024-02-04 07:57:00,949 EPOCH 502
2024-02-04 07:57:05,493 Epoch 502: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.36 
2024-02-04 07:57:05,494 EPOCH 503
2024-02-04 07:57:09,841 [Epoch: 503 Step: 00017100] Batch Recognition Loss:   0.000316 => Gls Tokens per Sec:     2299 || Batch Translation Loss:   0.056226 => Txt Tokens per Sec:     6398 || Lr: 0.000100
2024-02-04 07:57:10,084 Epoch 503: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.28 
2024-02-04 07:57:10,084 EPOCH 504
2024-02-04 07:57:14,195 Epoch 504: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.77 
2024-02-04 07:57:14,195 EPOCH 505
2024-02-04 07:57:18,957 Epoch 505: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.60 
2024-02-04 07:57:18,957 EPOCH 506
2024-02-04 07:57:22,780 [Epoch: 506 Step: 00017200] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:     2446 || Batch Translation Loss:   0.084116 => Txt Tokens per Sec:     6702 || Lr: 0.000100
2024-02-04 07:57:23,345 Epoch 506: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.86 
2024-02-04 07:57:23,345 EPOCH 507
2024-02-04 07:57:28,130 Epoch 507: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.92 
2024-02-04 07:57:28,131 EPOCH 508
2024-02-04 07:57:32,510 Epoch 508: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.38 
2024-02-04 07:57:32,511 EPOCH 509
2024-02-04 07:57:36,405 [Epoch: 509 Step: 00017300] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     2236 || Batch Translation Loss:   0.024373 => Txt Tokens per Sec:     6060 || Lr: 0.000100
2024-02-04 07:57:37,312 Epoch 509: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.73 
2024-02-04 07:57:37,312 EPOCH 510
2024-02-04 07:57:41,689 Epoch 510: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.19 
2024-02-04 07:57:41,689 EPOCH 511
2024-02-04 07:57:46,601 Epoch 511: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.62 
2024-02-04 07:57:46,601 EPOCH 512
2024-02-04 07:57:49,808 [Epoch: 512 Step: 00017400] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     2518 || Batch Translation Loss:   0.058082 => Txt Tokens per Sec:     6945 || Lr: 0.000100
2024-02-04 07:57:50,997 Epoch 512: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-04 07:57:50,997 EPOCH 513
2024-02-04 07:57:55,861 Epoch 513: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.36 
2024-02-04 07:57:55,862 EPOCH 514
2024-02-04 07:57:59,932 Epoch 514: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.53 
2024-02-04 07:57:59,932 EPOCH 515
2024-02-04 07:58:03,470 [Epoch: 515 Step: 00017500] Batch Recognition Loss:   0.000241 => Gls Tokens per Sec:     2101 || Batch Translation Loss:   0.018347 => Txt Tokens per Sec:     5855 || Lr: 0.000100
2024-02-04 07:58:04,926 Epoch 515: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.69 
2024-02-04 07:58:04,926 EPOCH 516
2024-02-04 07:58:09,366 Epoch 516: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.88 
2024-02-04 07:58:09,367 EPOCH 517
2024-02-04 07:58:14,247 Epoch 517: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.35 
2024-02-04 07:58:14,248 EPOCH 518
2024-02-04 07:58:16,631 [Epoch: 518 Step: 00017600] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     2852 || Batch Translation Loss:   0.020468 => Txt Tokens per Sec:     7612 || Lr: 0.000100
2024-02-04 07:58:18,715 Epoch 518: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.91 
2024-02-04 07:58:18,715 EPOCH 519
2024-02-04 07:58:23,416 Epoch 519: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.96 
2024-02-04 07:58:23,416 EPOCH 520
2024-02-04 07:58:27,999 Epoch 520: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.81 
2024-02-04 07:58:28,000 EPOCH 521
2024-02-04 07:58:30,916 [Epoch: 521 Step: 00017700] Batch Recognition Loss:   0.000393 => Gls Tokens per Sec:     2110 || Batch Translation Loss:   0.075475 => Txt Tokens per Sec:     6081 || Lr: 0.000100
2024-02-04 07:58:32,657 Epoch 521: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.12 
2024-02-04 07:58:32,657 EPOCH 522
2024-02-04 07:58:37,299 Epoch 522: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.05 
2024-02-04 07:58:37,299 EPOCH 523
2024-02-04 07:58:41,804 Epoch 523: Total Training Recognition Loss 0.03  Total Training Translation Loss 19.10 
2024-02-04 07:58:41,805 EPOCH 524
2024-02-04 07:58:44,048 [Epoch: 524 Step: 00017800] Batch Recognition Loss:   0.000667 => Gls Tokens per Sec:     2570 || Batch Translation Loss:   0.357947 => Txt Tokens per Sec:     6705 || Lr: 0.000100
2024-02-04 07:58:46,688 Epoch 524: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.24 
2024-02-04 07:58:46,688 EPOCH 525
2024-02-04 07:58:51,086 Epoch 525: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.31 
2024-02-04 07:58:51,087 EPOCH 526
2024-02-04 07:58:55,991 Epoch 526: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.89 
2024-02-04 07:58:55,992 EPOCH 527
2024-02-04 07:58:57,860 [Epoch: 527 Step: 00017900] Batch Recognition Loss:   0.000290 => Gls Tokens per Sec:     2607 || Batch Translation Loss:   0.048030 => Txt Tokens per Sec:     7283 || Lr: 0.000100
2024-02-04 07:59:00,347 Epoch 527: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.97 
2024-02-04 07:59:00,347 EPOCH 528
2024-02-04 07:59:05,229 Epoch 528: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.79 
2024-02-04 07:59:05,230 EPOCH 529
2024-02-04 07:59:09,807 Epoch 529: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.84 
2024-02-04 07:59:09,808 EPOCH 530
2024-02-04 07:59:11,915 [Epoch: 530 Step: 00018000] Batch Recognition Loss:   0.000257 => Gls Tokens per Sec:     2128 || Batch Translation Loss:   0.079618 => Txt Tokens per Sec:     6210 || Lr: 0.000100
2024-02-04 07:59:20,175 Validation result at epoch 530, step    18000: duration: 8.2598s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00059	Translation Loss: 94243.93750	PPL: 12468.93945
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.79	(BLEU-1: 11.26,	BLEU-2: 3.89,	BLEU-3: 1.65,	BLEU-4: 0.79)
	CHRF 17.36	ROUGE 10.00
2024-02-04 07:59:20,176 Logging Recognition and Translation Outputs
2024-02-04 07:59:20,176 ========================================================================================================================
2024-02-04 07:59:20,176 Logging Sequence: 155_119.00
2024-02-04 07:59:20,176 	Gloss Reference :	A B+C+D+E
2024-02-04 07:59:20,177 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:59:20,177 	Gloss Alignment :	         
2024-02-04 07:59:20,177 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:59:20,179 	Text Reference  :	*** a       report    said   that the ***** ******* ***** ***** ** taliban wanted icc    to ** replace the afghan flag with its own 
2024-02-04 07:59:20,180 	Text Hypothesis :	and taliban considers itself as   the match between their match as they    would  decide to go on      the afghan **** **** *** team
2024-02-04 07:59:20,180 	Text Alignment  :	I   S       S         S      S        I     I       I     I     I  S       S      S         I  S                  D    D    D   S   
2024-02-04 07:59:20,180 ========================================================================================================================
2024-02-04 07:59:20,180 Logging Sequence: 153_43.00
2024-02-04 07:59:20,180 	Gloss Reference :	A B+C+D+E
2024-02-04 07:59:20,180 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:59:20,180 	Gloss Alignment :	         
2024-02-04 07:59:20,181 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:59:20,181 	Text Reference  :	******* these runs   were all because of     hardik  pandya    and  virat kohli
2024-02-04 07:59:20,182 	Text Hypothesis :	however the   venues took now were    played against rajasthan lost the   match
2024-02-04 07:59:20,182 	Text Alignment  :	I       S     S      S    S   S       S      S       S         S    S     S    
2024-02-04 07:59:20,182 ========================================================================================================================
2024-02-04 07:59:20,182 Logging Sequence: 150_35.00
2024-02-04 07:59:20,182 	Gloss Reference :	A B+C+D+E
2024-02-04 07:59:20,182 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:59:20,182 	Gloss Alignment :	         
2024-02-04 07:59:20,182 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:59:20,183 	Text Reference  :	*** ***** ** ** wow india football team is      really  strong
2024-02-04 07:59:20,183 	Text Hypothesis :	the match is no one to    keep     with another similar way   
2024-02-04 07:59:20,183 	Text Alignment  :	I   I     I  I  S   S     S        S    S       S       S     
2024-02-04 07:59:20,183 ========================================================================================================================
2024-02-04 07:59:20,183 Logging Sequence: 146_154.00
2024-02-04 07:59:20,184 	Gloss Reference :	A B+C+D+E
2024-02-04 07:59:20,184 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:59:20,184 	Gloss Alignment :	         
2024-02-04 07:59:20,184 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:59:20,186 	Text Reference  :	bwf said  that testing protocols have   been implemented to  ensure the health and ** safety of all participants
2024-02-04 07:59:20,186 	Text Hypothesis :	the ashes is   a       special   series of   afghanistan and won    the toss   and is going  on 9th of          
2024-02-04 07:59:20,186 	Text Alignment  :	S   S     S    S       S         S      S    S           S   S          S          I  S      S  S   S           
2024-02-04 07:59:20,186 ========================================================================================================================
2024-02-04 07:59:20,186 Logging Sequence: 76_79.00
2024-02-04 07:59:20,186 	Gloss Reference :	A B+C+D+E
2024-02-04 07:59:20,187 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 07:59:20,187 	Gloss Alignment :	         
2024-02-04 07:59:20,187 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 07:59:20,187 	Text Reference  :	** **** speaking to   ani   csk ceo kasi  viswanathan said  
2024-02-04 07:59:20,188 	Text Hypothesis :	on 13th february 2023 there was a   match between     mumbai
2024-02-04 07:59:20,188 	Text Alignment  :	I  I    S        S    S     S   S   S     S           S     
2024-02-04 07:59:20,188 ========================================================================================================================
2024-02-04 07:59:22,887 Epoch 530: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.53 
2024-02-04 07:59:22,887 EPOCH 531
2024-02-04 07:59:27,542 Epoch 531: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.49 
2024-02-04 07:59:27,543 EPOCH 532
2024-02-04 07:59:32,223 Epoch 532: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-04 07:59:32,223 EPOCH 533
2024-02-04 07:59:33,434 [Epoch: 533 Step: 00018100] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     3174 || Batch Translation Loss:   0.035071 => Txt Tokens per Sec:     8474 || Lr: 0.000100
2024-02-04 07:59:36,891 Epoch 533: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-04 07:59:36,892 EPOCH 534
2024-02-04 07:59:41,814 Epoch 534: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.86 
2024-02-04 07:59:41,815 EPOCH 535
2024-02-04 07:59:46,163 Epoch 535: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-04 07:59:46,164 EPOCH 536
2024-02-04 07:59:47,448 [Epoch: 536 Step: 00018200] Batch Recognition Loss:   0.000278 => Gls Tokens per Sec:     2495 || Batch Translation Loss:   0.020118 => Txt Tokens per Sec:     6426 || Lr: 0.000100
2024-02-04 07:59:51,052 Epoch 536: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.15 
2024-02-04 07:59:51,052 EPOCH 537
2024-02-04 07:59:55,557 Epoch 537: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.54 
2024-02-04 07:59:55,557 EPOCH 538
2024-02-04 08:00:00,292 Epoch 538: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.10 
2024-02-04 08:00:00,292 EPOCH 539
2024-02-04 08:00:00,967 [Epoch: 539 Step: 00018300] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:     3798 || Batch Translation Loss:   0.081076 => Txt Tokens per Sec:     8733 || Lr: 0.000100
2024-02-04 08:00:04,835 Epoch 539: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.11 
2024-02-04 08:00:04,835 EPOCH 540
2024-02-04 08:00:09,477 Epoch 540: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.01 
2024-02-04 08:00:09,477 EPOCH 541
2024-02-04 08:00:14,220 Epoch 541: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.12 
2024-02-04 08:00:14,220 EPOCH 542
2024-02-04 08:00:14,954 [Epoch: 542 Step: 00018400] Batch Recognition Loss:   0.000313 => Gls Tokens per Sec:     2623 || Batch Translation Loss:   0.167130 => Txt Tokens per Sec:     7143 || Lr: 0.000100
2024-02-04 08:00:18,655 Epoch 542: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.29 
2024-02-04 08:00:18,655 EPOCH 543
2024-02-04 08:00:23,655 Epoch 543: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.86 
2024-02-04 08:00:23,655 EPOCH 544
2024-02-04 08:00:27,913 Epoch 544: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.51 
2024-02-04 08:00:27,914 EPOCH 545
2024-02-04 08:00:28,388 [Epoch: 545 Step: 00018500] Batch Recognition Loss:   0.000272 => Gls Tokens per Sec:     2712 || Batch Translation Loss:   0.053622 => Txt Tokens per Sec:     7898 || Lr: 0.000100
2024-02-04 08:00:32,447 Epoch 545: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.79 
2024-02-04 08:00:32,448 EPOCH 546
2024-02-04 08:00:37,102 Epoch 546: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.25 
2024-02-04 08:00:37,102 EPOCH 547
2024-02-04 08:00:41,794 Epoch 547: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.34 
2024-02-04 08:00:41,795 EPOCH 548
2024-02-04 08:00:41,956 [Epoch: 548 Step: 00018600] Batch Recognition Loss:   0.000280 => Gls Tokens per Sec:     4000 || Batch Translation Loss:   0.090858 => Txt Tokens per Sec:     8300 || Lr: 0.000100
2024-02-04 08:00:46,262 Epoch 548: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.99 
2024-02-04 08:00:46,262 EPOCH 549
2024-02-04 08:00:51,078 Epoch 549: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.10 
2024-02-04 08:00:51,078 EPOCH 550
2024-02-04 08:00:55,414 [Epoch: 550 Step: 00018700] Batch Recognition Loss:   0.000325 => Gls Tokens per Sec:     2452 || Batch Translation Loss:   0.074239 => Txt Tokens per Sec:     6807 || Lr: 0.000100
2024-02-04 08:00:55,414 Epoch 550: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.04 
2024-02-04 08:00:55,414 EPOCH 551
2024-02-04 08:01:00,333 Epoch 551: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.17 
2024-02-04 08:01:00,334 EPOCH 552
2024-02-04 08:01:04,935 Epoch 552: Total Training Recognition Loss 0.01  Total Training Translation Loss 6.59 
2024-02-04 08:01:04,935 EPOCH 553
2024-02-04 08:01:09,469 [Epoch: 553 Step: 00018800] Batch Recognition Loss:   0.000501 => Gls Tokens per Sec:     2204 || Batch Translation Loss:   0.093363 => Txt Tokens per Sec:     6159 || Lr: 0.000100
2024-02-04 08:01:09,705 Epoch 553: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.27 
2024-02-04 08:01:09,705 EPOCH 554
2024-02-04 08:01:13,849 Epoch 554: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.24 
2024-02-04 08:01:13,849 EPOCH 555
2024-02-04 08:01:18,679 Epoch 555: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.74 
2024-02-04 08:01:18,679 EPOCH 556
2024-02-04 08:01:22,677 [Epoch: 556 Step: 00018900] Batch Recognition Loss:   0.000238 => Gls Tokens per Sec:     2402 || Batch Translation Loss:   0.042502 => Txt Tokens per Sec:     6753 || Lr: 0.000100
2024-02-04 08:01:23,163 Epoch 556: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.61 
2024-02-04 08:01:23,163 EPOCH 557
2024-02-04 08:01:28,081 Epoch 557: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.43 
2024-02-04 08:01:28,082 EPOCH 558
2024-02-04 08:01:32,714 Epoch 558: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.61 
2024-02-04 08:01:32,715 EPOCH 559
2024-02-04 08:01:36,596 [Epoch: 559 Step: 00019000] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:     2245 || Batch Translation Loss:   0.035588 => Txt Tokens per Sec:     6247 || Lr: 0.000100
2024-02-04 08:01:37,351 Epoch 559: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.50 
2024-02-04 08:01:37,352 EPOCH 560
2024-02-04 08:01:41,571 Epoch 560: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-04 08:01:41,572 EPOCH 561
2024-02-04 08:01:46,566 Epoch 561: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.37 
2024-02-04 08:01:46,566 EPOCH 562
2024-02-04 08:01:50,481 [Epoch: 562 Step: 00019100] Batch Recognition Loss:   0.000267 => Gls Tokens per Sec:     2062 || Batch Translation Loss:   0.086170 => Txt Tokens per Sec:     5759 || Lr: 0.000100
2024-02-04 08:01:51,486 Epoch 562: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.77 
2024-02-04 08:01:51,486 EPOCH 563
2024-02-04 08:01:56,133 Epoch 563: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.01 
2024-02-04 08:01:56,133 EPOCH 564
2024-02-04 08:02:00,690 Epoch 564: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.81 
2024-02-04 08:02:00,691 EPOCH 565
2024-02-04 08:02:03,925 [Epoch: 565 Step: 00019200] Batch Recognition Loss:   0.000129 => Gls Tokens per Sec:     2298 || Batch Translation Loss:   0.038697 => Txt Tokens per Sec:     6340 || Lr: 0.000100
2024-02-04 08:02:05,368 Epoch 565: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.01 
2024-02-04 08:02:05,368 EPOCH 566
2024-02-04 08:02:09,975 Epoch 566: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.69 
2024-02-04 08:02:09,975 EPOCH 567
2024-02-04 08:02:14,460 Epoch 567: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-04 08:02:14,460 EPOCH 568
2024-02-04 08:02:17,201 [Epoch: 568 Step: 00019300] Batch Recognition Loss:   0.000239 => Gls Tokens per Sec:     2570 || Batch Translation Loss:   0.037699 => Txt Tokens per Sec:     6957 || Lr: 0.000100
2024-02-04 08:02:19,176 Epoch 568: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.62 
2024-02-04 08:02:19,176 EPOCH 569
2024-02-04 08:02:24,027 Epoch 569: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.16 
2024-02-04 08:02:24,028 EPOCH 570
2024-02-04 08:02:28,883 Epoch 570: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.59 
2024-02-04 08:02:28,884 EPOCH 571
2024-02-04 08:02:31,252 [Epoch: 571 Step: 00019400] Batch Recognition Loss:   0.000169 => Gls Tokens per Sec:     2598 || Batch Translation Loss:   0.025725 => Txt Tokens per Sec:     7020 || Lr: 0.000100
2024-02-04 08:02:33,073 Epoch 571: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.28 
2024-02-04 08:02:33,073 EPOCH 572
2024-02-04 08:02:37,172 Epoch 572: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-04 08:02:37,172 EPOCH 573
2024-02-04 08:02:41,241 Epoch 573: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-04 08:02:41,241 EPOCH 574
2024-02-04 08:02:43,434 [Epoch: 574 Step: 00019500] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     2513 || Batch Translation Loss:   0.013275 => Txt Tokens per Sec:     6845 || Lr: 0.000100
2024-02-04 08:02:45,884 Epoch 574: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-04 08:02:45,885 EPOCH 575
2024-02-04 08:02:50,579 Epoch 575: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.12 
2024-02-04 08:02:50,579 EPOCH 576
2024-02-04 08:02:55,262 Epoch 576: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.01 
2024-02-04 08:02:55,263 EPOCH 577
2024-02-04 08:02:57,080 [Epoch: 577 Step: 00019600] Batch Recognition Loss:   0.000292 => Gls Tokens per Sec:     2682 || Batch Translation Loss:   0.117192 => Txt Tokens per Sec:     7173 || Lr: 0.000100
2024-02-04 08:02:59,760 Epoch 577: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.82 
2024-02-04 08:02:59,760 EPOCH 578
2024-02-04 08:03:04,607 Epoch 578: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.68 
2024-02-04 08:03:04,608 EPOCH 579
2024-02-04 08:03:08,991 Epoch 579: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.95 
2024-02-04 08:03:08,992 EPOCH 580
2024-02-04 08:03:11,462 [Epoch: 580 Step: 00019700] Batch Recognition Loss:   0.000330 => Gls Tokens per Sec:     1814 || Batch Translation Loss:   0.053208 => Txt Tokens per Sec:     5559 || Lr: 0.000100
2024-02-04 08:03:14,029 Epoch 580: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.40 
2024-02-04 08:03:14,029 EPOCH 581
2024-02-04 08:03:18,312 Epoch 581: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.02 
2024-02-04 08:03:18,312 EPOCH 582
2024-02-04 08:03:23,251 Epoch 582: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.43 
2024-02-04 08:03:23,252 EPOCH 583
2024-02-04 08:03:24,896 [Epoch: 583 Step: 00019800] Batch Recognition Loss:   0.000493 => Gls Tokens per Sec:     2337 || Batch Translation Loss:   0.275981 => Txt Tokens per Sec:     6751 || Lr: 0.000100
2024-02-04 08:03:27,715 Epoch 583: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.89 
2024-02-04 08:03:27,716 EPOCH 584
2024-02-04 08:03:32,632 Epoch 584: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.72 
2024-02-04 08:03:32,633 EPOCH 585
2024-02-04 08:03:37,138 Epoch 585: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.89 
2024-02-04 08:03:37,139 EPOCH 586
2024-02-04 08:03:38,583 [Epoch: 586 Step: 00019900] Batch Recognition Loss:   0.000383 => Gls Tokens per Sec:     2218 || Batch Translation Loss:   0.119780 => Txt Tokens per Sec:     6350 || Lr: 0.000100
2024-02-04 08:03:41,918 Epoch 586: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.55 
2024-02-04 08:03:41,918 EPOCH 587
2024-02-04 08:03:46,472 Epoch 587: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.26 
2024-02-04 08:03:46,472 EPOCH 588
2024-02-04 08:03:51,131 Epoch 588: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.53 
2024-02-04 08:03:51,132 EPOCH 589
2024-02-04 08:03:51,933 [Epoch: 589 Step: 00020000] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     3203 || Batch Translation Loss:   0.026481 => Txt Tokens per Sec:     8547 || Lr: 0.000100
2024-02-04 08:04:00,515 Validation result at epoch 589, step    20000: duration: 8.5825s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00070	Translation Loss: 94814.90625	PPL: 13202.11719
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.90	(BLEU-1: 10.69,	BLEU-2: 3.43,	BLEU-3: 1.62,	BLEU-4: 0.90)
	CHRF 17.06	ROUGE 9.31
2024-02-04 08:04:00,516 Logging Recognition and Translation Outputs
2024-02-04 08:04:00,516 ========================================================================================================================
2024-02-04 08:04:00,516 Logging Sequence: 174_121.00
2024-02-04 08:04:00,517 	Gloss Reference :	A B+C+D+E
2024-02-04 08:04:00,517 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:04:00,517 	Gloss Alignment :	         
2024-02-04 08:04:00,518 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:04:00,519 	Text Reference  :	******* **** there was a strong competition and a difficult auction for       the 5      franchise owners
2024-02-04 08:04:00,519 	Text Hypothesis :	however here there was * ****** *********** *** a huge      fan     following in  mumbai and       shobit
2024-02-04 08:04:00,519 	Text Alignment  :	I       I              D D      D           D     S         S       S         S   S      S         S     
2024-02-04 08:04:00,519 ========================================================================================================================
2024-02-04 08:04:00,519 Logging Sequence: 170_24.00
2024-02-04 08:04:00,520 	Gloss Reference :	A B+C+D+E
2024-02-04 08:04:00,520 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:04:00,520 	Gloss Alignment :	         
2024-02-04 08:04:00,520 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:04:00,521 	Text Reference  :	let me tell you about it
2024-02-04 08:04:00,521 	Text Hypothesis :	let me tell you about it
2024-02-04 08:04:00,521 	Text Alignment  :	                        
2024-02-04 08:04:00,521 ========================================================================================================================
2024-02-04 08:04:00,521 Logging Sequence: 73_79.00
2024-02-04 08:04:00,521 	Gloss Reference :	A B+C+D+E
2024-02-04 08:04:00,522 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:04:00,522 	Gloss Alignment :	         
2024-02-04 08:04:00,522 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:04:00,523 	Text Reference  :	raina resturant has food from the  rich  spices of    north india   to the aromatic curries of south india  
2024-02-04 08:04:00,523 	Text Hypothesis :	***** ********* *** **** on   23rd march 2023   raina loves playing in the ******** ******* ** ***** stadium
2024-02-04 08:04:00,523 	Text Alignment  :	D     D         D   D    S    S    S     S      S     S     S       S      D        D       D  D     S      
2024-02-04 08:04:00,523 ========================================================================================================================
2024-02-04 08:04:00,524 Logging Sequence: 140_2.00
2024-02-04 08:04:00,524 	Gloss Reference :	A B+C+D+E
2024-02-04 08:04:00,524 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:04:00,524 	Gloss Alignment :	         
2024-02-04 08:04:00,524 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:04:00,525 	Text Reference  :	***** *** **** indian batsman-wicket keeper rishabh pant has   outstanding skills in  cricket
2024-02-04 08:04:00,525 	Text Hypothesis :	dhoni has also become the            first  time    when kohli was         given  the reason 
2024-02-04 08:04:00,525 	Text Alignment  :	I     I   I    S      S              S      S       S    S     S           S      S   S      
2024-02-04 08:04:00,525 ========================================================================================================================
2024-02-04 08:04:00,526 Logging Sequence: 81_470.00
2024-02-04 08:04:00,526 	Gloss Reference :	A B+C+D+E
2024-02-04 08:04:00,526 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:04:00,526 	Gloss Alignment :	         
2024-02-04 08:04:00,526 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:04:00,527 	Text Reference  :	or you don't know       if you do      let   us know in         the comments
2024-02-04 08:04:00,527 	Text Hypothesis :	** *** ***** arbitrator is a   supreme court of the  tournament was said    
2024-02-04 08:04:00,527 	Text Alignment  :	D  D   D     S          S  S   S       S     S  S    S          S   S       
2024-02-04 08:04:00,527 ========================================================================================================================
2024-02-04 08:04:04,475 Epoch 589: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-04 08:04:04,476 EPOCH 590
2024-02-04 08:04:09,388 Epoch 590: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-04 08:04:09,389 EPOCH 591
2024-02-04 08:04:13,794 Epoch 591: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-04 08:04:13,795 EPOCH 592
2024-02-04 08:04:14,749 [Epoch: 592 Step: 00020100] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:     2015 || Batch Translation Loss:   0.029243 => Txt Tokens per Sec:     5795 || Lr: 0.000100
2024-02-04 08:04:18,758 Epoch 592: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-04 08:04:18,758 EPOCH 593
2024-02-04 08:04:23,166 Epoch 593: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-04 08:04:23,167 EPOCH 594
2024-02-04 08:04:28,062 Epoch 594: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-04 08:04:28,063 EPOCH 595
2024-02-04 08:04:28,725 [Epoch: 595 Step: 00020200] Batch Recognition Loss:   0.000232 => Gls Tokens per Sec:     1938 || Batch Translation Loss:   0.108399 => Txt Tokens per Sec:     6259 || Lr: 0.000100
2024-02-04 08:04:32,119 Epoch 595: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.42 
2024-02-04 08:04:32,119 EPOCH 596
2024-02-04 08:04:37,049 Epoch 596: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.99 
2024-02-04 08:04:37,050 EPOCH 597
2024-02-04 08:04:41,394 Epoch 597: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.76 
2024-02-04 08:04:41,394 EPOCH 598
2024-02-04 08:04:41,636 [Epoch: 598 Step: 00020300] Batch Recognition Loss:   0.000397 => Gls Tokens per Sec:     2656 || Batch Translation Loss:   0.090315 => Txt Tokens per Sec:     7813 || Lr: 0.000100
2024-02-04 08:04:45,479 Epoch 598: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.89 
2024-02-04 08:04:45,479 EPOCH 599
2024-02-04 08:04:50,165 Epoch 599: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.22 
2024-02-04 08:04:50,166 EPOCH 600
2024-02-04 08:04:54,732 [Epoch: 600 Step: 00020400] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     2329 || Batch Translation Loss:   0.102966 => Txt Tokens per Sec:     6466 || Lr: 0.000100
2024-02-04 08:04:54,732 Epoch 600: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.01 
2024-02-04 08:04:54,732 EPOCH 601
2024-02-04 08:04:59,493 Epoch 601: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.98 
2024-02-04 08:04:59,493 EPOCH 602
2024-02-04 08:05:03,915 Epoch 602: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.34 
2024-02-04 08:05:03,915 EPOCH 603
2024-02-04 08:05:08,569 [Epoch: 603 Step: 00020500] Batch Recognition Loss:   0.000422 => Gls Tokens per Sec:     2201 || Batch Translation Loss:   0.070940 => Txt Tokens per Sec:     6104 || Lr: 0.000100
2024-02-04 08:05:08,821 Epoch 603: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.24 
2024-02-04 08:05:08,821 EPOCH 604
2024-02-04 08:05:13,142 Epoch 604: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.37 
2024-02-04 08:05:13,143 EPOCH 605
2024-02-04 08:05:18,156 Epoch 605: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.01 
2024-02-04 08:05:18,157 EPOCH 606
2024-02-04 08:05:22,451 [Epoch: 606 Step: 00020600] Batch Recognition Loss:   0.000285 => Gls Tokens per Sec:     2178 || Batch Translation Loss:   0.152807 => Txt Tokens per Sec:     6105 || Lr: 0.000100
2024-02-04 08:05:22,874 Epoch 606: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.98 
2024-02-04 08:05:22,874 EPOCH 607
2024-02-04 08:05:27,721 Epoch 607: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.32 
2024-02-04 08:05:27,722 EPOCH 608
2024-02-04 08:05:31,895 Epoch 608: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.37 
2024-02-04 08:05:31,895 EPOCH 609
2024-02-04 08:05:35,777 [Epoch: 609 Step: 00020700] Batch Recognition Loss:   0.000374 => Gls Tokens per Sec:     2245 || Batch Translation Loss:   0.068227 => Txt Tokens per Sec:     6109 || Lr: 0.000100
2024-02-04 08:05:36,747 Epoch 609: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.29 
2024-02-04 08:05:36,747 EPOCH 610
2024-02-04 08:05:41,031 Epoch 610: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.12 
2024-02-04 08:05:41,031 EPOCH 611
2024-02-04 08:05:45,519 Epoch 611: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.28 
2024-02-04 08:05:45,520 EPOCH 612
2024-02-04 08:05:49,409 [Epoch: 612 Step: 00020800] Batch Recognition Loss:   0.000474 => Gls Tokens per Sec:     2140 || Batch Translation Loss:   0.085922 => Txt Tokens per Sec:     6097 || Lr: 0.000100
2024-02-04 08:05:50,252 Epoch 612: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.95 
2024-02-04 08:05:50,252 EPOCH 613
2024-02-04 08:05:54,876 Epoch 613: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.71 
2024-02-04 08:05:54,876 EPOCH 614
2024-02-04 08:05:59,546 Epoch 614: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.66 
2024-02-04 08:05:59,547 EPOCH 615
2024-02-04 08:06:03,065 [Epoch: 615 Step: 00020900] Batch Recognition Loss:   0.000221 => Gls Tokens per Sec:     2113 || Batch Translation Loss:   0.062222 => Txt Tokens per Sec:     6057 || Lr: 0.000100
2024-02-04 08:06:04,241 Epoch 615: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.34 
2024-02-04 08:06:04,242 EPOCH 616
2024-02-04 08:06:08,668 Epoch 616: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.37 
2024-02-04 08:06:08,668 EPOCH 617
2024-02-04 08:06:13,467 Epoch 617: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.96 
2024-02-04 08:06:13,468 EPOCH 618
2024-02-04 08:06:16,059 [Epoch: 618 Step: 00021000] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:     2718 || Batch Translation Loss:   0.044988 => Txt Tokens per Sec:     7399 || Lr: 0.000100
2024-02-04 08:06:17,862 Epoch 618: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.75 
2024-02-04 08:06:17,863 EPOCH 619
2024-02-04 08:06:22,719 Epoch 619: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.21 
2024-02-04 08:06:22,720 EPOCH 620
2024-02-04 08:06:27,103 Epoch 620: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-04 08:06:27,104 EPOCH 621
2024-02-04 08:06:29,993 [Epoch: 621 Step: 00021100] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:     2130 || Batch Translation Loss:   0.033400 => Txt Tokens per Sec:     5893 || Lr: 0.000100
2024-02-04 08:06:31,995 Epoch 621: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-04 08:06:31,995 EPOCH 622
2024-02-04 08:06:36,446 Epoch 622: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-04 08:06:36,447 EPOCH 623
2024-02-04 08:06:41,255 Epoch 623: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-04 08:06:41,255 EPOCH 624
2024-02-04 08:06:43,587 [Epoch: 624 Step: 00021200] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     2471 || Batch Translation Loss:   0.043136 => Txt Tokens per Sec:     6611 || Lr: 0.000100
2024-02-04 08:06:46,035 Epoch 624: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.49 
2024-02-04 08:06:46,035 EPOCH 625
2024-02-04 08:06:50,911 Epoch 625: Total Training Recognition Loss 0.06  Total Training Translation Loss 12.66 
2024-02-04 08:06:50,912 EPOCH 626
2024-02-04 08:06:55,543 Epoch 626: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.42 
2024-02-04 08:06:55,544 EPOCH 627
2024-02-04 08:06:57,581 [Epoch: 627 Step: 00021300] Batch Recognition Loss:   0.000350 => Gls Tokens per Sec:     2514 || Batch Translation Loss:   0.084459 => Txt Tokens per Sec:     6715 || Lr: 0.000100
2024-02-04 08:07:00,390 Epoch 627: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.25 
2024-02-04 08:07:00,390 EPOCH 628
2024-02-04 08:07:04,723 Epoch 628: Total Training Recognition Loss 0.02  Total Training Translation Loss 11.26 
2024-02-04 08:07:04,723 EPOCH 629
2024-02-04 08:07:09,588 Epoch 629: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.91 
2024-02-04 08:07:09,589 EPOCH 630
2024-02-04 08:07:10,922 [Epoch: 630 Step: 00021400] Batch Recognition Loss:   0.000672 => Gls Tokens per Sec:     3360 || Batch Translation Loss:   0.073058 => Txt Tokens per Sec:     8636 || Lr: 0.000100
2024-02-04 08:07:14,017 Epoch 630: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.11 
2024-02-04 08:07:14,018 EPOCH 631
2024-02-04 08:07:18,814 Epoch 631: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.18 
2024-02-04 08:07:18,814 EPOCH 632
2024-02-04 08:07:23,222 Epoch 632: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.50 
2024-02-04 08:07:23,223 EPOCH 633
2024-02-04 08:07:25,098 [Epoch: 633 Step: 00021500] Batch Recognition Loss:   0.000304 => Gls Tokens per Sec:     2049 || Batch Translation Loss:   0.037853 => Txt Tokens per Sec:     6174 || Lr: 0.000100
2024-02-04 08:07:28,054 Epoch 633: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.57 
2024-02-04 08:07:28,055 EPOCH 634
2024-02-04 08:07:32,655 Epoch 634: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.82 
2024-02-04 08:07:32,656 EPOCH 635
2024-02-04 08:07:37,222 Epoch 635: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.32 
2024-02-04 08:07:37,222 EPOCH 636
2024-02-04 08:07:38,674 [Epoch: 636 Step: 00021600] Batch Recognition Loss:   0.000243 => Gls Tokens per Sec:     2205 || Batch Translation Loss:   0.027313 => Txt Tokens per Sec:     6398 || Lr: 0.000100
2024-02-04 08:07:41,851 Epoch 636: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-04 08:07:41,852 EPOCH 637
2024-02-04 08:07:46,448 Epoch 637: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-04 08:07:46,448 EPOCH 638
2024-02-04 08:07:51,285 Epoch 638: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-04 08:07:51,286 EPOCH 639
2024-02-04 08:07:52,176 [Epoch: 639 Step: 00021700] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:     2879 || Batch Translation Loss:   0.026720 => Txt Tokens per Sec:     7398 || Lr: 0.000100
2024-02-04 08:07:55,722 Epoch 639: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-04 08:07:55,722 EPOCH 640
2024-02-04 08:08:00,617 Epoch 640: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-04 08:08:00,617 EPOCH 641
2024-02-04 08:08:04,941 Epoch 641: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-04 08:08:04,941 EPOCH 642
2024-02-04 08:08:05,777 [Epoch: 642 Step: 00021800] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     2301 || Batch Translation Loss:   0.019125 => Txt Tokens per Sec:     6203 || Lr: 0.000100
2024-02-04 08:08:09,980 Epoch 642: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-04 08:08:09,980 EPOCH 643
2024-02-04 08:08:14,427 Epoch 643: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-04 08:08:14,428 EPOCH 644
2024-02-04 08:08:19,217 Epoch 644: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-04 08:08:19,217 EPOCH 645
2024-02-04 08:08:19,693 [Epoch: 645 Step: 00021900] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:     2698 || Batch Translation Loss:   0.012566 => Txt Tokens per Sec:     7844 || Lr: 0.000100
2024-02-04 08:08:23,780 Epoch 645: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-04 08:08:23,780 EPOCH 646
2024-02-04 08:08:28,464 Epoch 646: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-04 08:08:28,464 EPOCH 647
2024-02-04 08:08:32,672 Epoch 647: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-04 08:08:32,673 EPOCH 648
2024-02-04 08:08:32,931 [Epoch: 648 Step: 00022000] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     2487 || Batch Translation Loss:   0.069469 => Txt Tokens per Sec:     7385 || Lr: 0.000100
2024-02-04 08:08:41,411 Validation result at epoch 648, step    22000: duration: 8.4796s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00044	Translation Loss: 93554.91406	PPL: 11638.16797
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 1.10	(BLEU-1: 10.91,	BLEU-2: 3.85,	BLEU-3: 1.86,	BLEU-4: 1.10)
	CHRF 17.06	ROUGE 9.63
2024-02-04 08:08:41,412 Logging Recognition and Translation Outputs
2024-02-04 08:08:41,413 ========================================================================================================================
2024-02-04 08:08:41,415 Logging Sequence: 146_56.00
2024-02-04 08:08:41,415 	Gloss Reference :	A B+C+D+E
2024-02-04 08:08:41,416 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:08:41,416 	Gloss Alignment :	         
2024-02-04 08:08:41,416 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:08:41,418 	Text Reference  :	when the players go back to the hotel  as   per     rules all          of them have to     undergo rtpcr test for    covid-19 everyday
2024-02-04 08:08:41,418 	Text Hypothesis :	**** *** ******* ** **** ** the indian team follows a     superstition of **** **** saying we      will  lose before any      match   
2024-02-04 08:08:41,418 	Text Alignment  :	D    D   D       D  D    D      S      S    S       S     S               D    D    S      S       S     S    S      S        S       
2024-02-04 08:08:41,418 ========================================================================================================================
2024-02-04 08:08:41,418 Logging Sequence: 118_338.00
2024-02-04 08:08:41,419 	Gloss Reference :	A B+C+D+E
2024-02-04 08:08:41,419 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:08:41,419 	Gloss Alignment :	         
2024-02-04 08:08:41,419 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:08:41,419 	Text Reference  :	this is    why    even    messi wore it  
2024-02-04 08:08:41,420 	Text Hypothesis :	**** their maiden victory was   in   1978
2024-02-04 08:08:41,420 	Text Alignment  :	D    S     S      S       S     S    S   
2024-02-04 08:08:41,420 ========================================================================================================================
2024-02-04 08:08:41,420 Logging Sequence: 66_61.00
2024-02-04 08:08:41,420 	Gloss Reference :	A B+C+D+E
2024-02-04 08:08:41,420 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:08:41,420 	Gloss Alignment :	         
2024-02-04 08:08:41,421 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:08:41,421 	Text Reference  :	instead of returning back to his homeland because of   his  injury
2024-02-04 08:08:41,421 	Text Hypothesis :	******* ** she       led  to *** ******** a       slow over rate  
2024-02-04 08:08:41,421 	Text Alignment  :	D       D  S         S       D   D        S       S    S    S     
2024-02-04 08:08:41,422 ========================================================================================================================
2024-02-04 08:08:41,422 Logging Sequence: 81_278.00
2024-02-04 08:08:41,422 	Gloss Reference :	A B+C+D+E
2024-02-04 08:08:41,422 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:08:41,422 	Gloss Alignment :	         
2024-02-04 08:08:41,422 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:08:41,425 	Text Reference  :	of this amrapali group paid rs    3570 crore the   remaining rs 652  crore was paid  by amrapali sapphire developers a subsidiary of amrapali group
2024-02-04 08:08:41,425 	Text Hypothesis :	** **** ******** then  in   april 2021 ms    dhoni scored    35 runs so    far total of amrapali mahi     developers a subsidiary of amrapali group
2024-02-04 08:08:41,425 	Text Alignment  :	D  D    D        S     S    S     S    S     S     S         S  S    S     S   S     S           S                                                 
2024-02-04 08:08:41,425 ========================================================================================================================
2024-02-04 08:08:41,426 Logging Sequence: 162_125.00
2024-02-04 08:08:41,426 	Gloss Reference :	A B+C+D+E
2024-02-04 08:08:41,426 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:08:41,426 	Gloss Alignment :	         
2024-02-04 08:08:41,426 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:08:41,427 	Text Reference  :	in response to      this kohli received many hate    comments on social    media    
2024-02-04 08:08:41,427 	Text Hypothesis :	** ******** ronaldo has  also  become   a    violent city     in ahmedabad wonderful
2024-02-04 08:08:41,427 	Text Alignment  :	D  D        S       S    S     S        S    S       S        S  S         S        
2024-02-04 08:08:41,427 ========================================================================================================================
2024-02-04 08:08:46,123 Epoch 648: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.36 
2024-02-04 08:08:46,123 EPOCH 649
2024-02-04 08:08:50,402 Epoch 649: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.11 
2024-02-04 08:08:50,403 EPOCH 650
2024-02-04 08:08:55,292 [Epoch: 650 Step: 00022100] Batch Recognition Loss:   0.000157 => Gls Tokens per Sec:     2175 || Batch Translation Loss:   0.051350 => Txt Tokens per Sec:     6038 || Lr: 0.000100
2024-02-04 08:08:55,292 Epoch 650: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.16 
2024-02-04 08:08:55,292 EPOCH 651
2024-02-04 08:09:00,016 Epoch 651: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-04 08:09:00,017 EPOCH 652
2024-02-04 08:09:04,521 Epoch 652: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.49 
2024-02-04 08:09:04,521 EPOCH 653
2024-02-04 08:09:08,379 [Epoch: 653 Step: 00022200] Batch Recognition Loss:   0.000241 => Gls Tokens per Sec:     2591 || Batch Translation Loss:   0.069217 => Txt Tokens per Sec:     7165 || Lr: 0.000100
2024-02-04 08:09:08,690 Epoch 653: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.34 
2024-02-04 08:09:08,690 EPOCH 654
2024-02-04 08:09:13,613 Epoch 654: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.22 
2024-02-04 08:09:13,614 EPOCH 655
2024-02-04 08:09:17,877 Epoch 655: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.17 
2024-02-04 08:09:17,877 EPOCH 656
2024-02-04 08:09:22,174 [Epoch: 656 Step: 00022300] Batch Recognition Loss:   0.000228 => Gls Tokens per Sec:     2235 || Batch Translation Loss:   0.080434 => Txt Tokens per Sec:     6219 || Lr: 0.000100
2024-02-04 08:09:22,781 Epoch 656: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.00 
2024-02-04 08:09:22,781 EPOCH 657
2024-02-04 08:09:27,256 Epoch 657: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.67 
2024-02-04 08:09:27,257 EPOCH 658
2024-02-04 08:09:32,105 Epoch 658: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.92 
2024-02-04 08:09:32,105 EPOCH 659
2024-02-04 08:09:35,998 [Epoch: 659 Step: 00022400] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     2303 || Batch Translation Loss:   0.033885 => Txt Tokens per Sec:     6337 || Lr: 0.000100
2024-02-04 08:09:36,875 Epoch 659: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.32 
2024-02-04 08:09:36,875 EPOCH 660
2024-02-04 08:09:41,297 Epoch 660: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.56 
2024-02-04 08:09:41,297 EPOCH 661
2024-02-04 08:09:46,037 Epoch 661: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.58 
2024-02-04 08:09:46,038 EPOCH 662
2024-02-04 08:09:49,532 [Epoch: 662 Step: 00022500] Batch Recognition Loss:   0.000466 => Gls Tokens per Sec:     2382 || Batch Translation Loss:   0.093980 => Txt Tokens per Sec:     6525 || Lr: 0.000100
2024-02-04 08:09:50,531 Epoch 662: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.37 
2024-02-04 08:09:50,531 EPOCH 663
2024-02-04 08:09:55,503 Epoch 663: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.85 
2024-02-04 08:09:55,504 EPOCH 664
2024-02-04 08:09:59,702 Epoch 664: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.60 
2024-02-04 08:09:59,702 EPOCH 665
2024-02-04 08:10:02,618 [Epoch: 665 Step: 00022600] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     2550 || Batch Translation Loss:   0.066966 => Txt Tokens per Sec:     7073 || Lr: 0.000100
2024-02-04 08:10:04,340 Epoch 665: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.36 
2024-02-04 08:10:04,340 EPOCH 666
2024-02-04 08:10:08,904 Epoch 666: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.18 
2024-02-04 08:10:08,904 EPOCH 667
2024-02-04 08:10:13,595 Epoch 667: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.97 
2024-02-04 08:10:13,595 EPOCH 668
2024-02-04 08:10:15,997 [Epoch: 668 Step: 00022700] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:     2828 || Batch Translation Loss:   0.028996 => Txt Tokens per Sec:     7377 || Lr: 0.000100
2024-02-04 08:10:18,058 Epoch 668: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.57 
2024-02-04 08:10:18,058 EPOCH 669
2024-02-04 08:10:22,880 Epoch 669: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.72 
2024-02-04 08:10:22,880 EPOCH 670
2024-02-04 08:10:27,287 Epoch 670: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.71 
2024-02-04 08:10:27,287 EPOCH 671
2024-02-04 08:10:30,233 [Epoch: 671 Step: 00022800] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     2089 || Batch Translation Loss:   0.040408 => Txt Tokens per Sec:     5884 || Lr: 0.000100
2024-02-04 08:10:32,250 Epoch 671: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.80 
2024-02-04 08:10:32,251 EPOCH 672
2024-02-04 08:10:36,563 Epoch 672: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.51 
2024-02-04 08:10:36,564 EPOCH 673
2024-02-04 08:10:41,478 Epoch 673: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.06 
2024-02-04 08:10:41,479 EPOCH 674
2024-02-04 08:10:43,643 [Epoch: 674 Step: 00022900] Batch Recognition Loss:   0.000322 => Gls Tokens per Sec:     2546 || Batch Translation Loss:   0.076619 => Txt Tokens per Sec:     6988 || Lr: 0.000100
2024-02-04 08:10:45,832 Epoch 674: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.67 
2024-02-04 08:10:45,833 EPOCH 675
2024-02-04 08:10:50,659 Epoch 675: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.21 
2024-02-04 08:10:50,660 EPOCH 676
2024-02-04 08:10:55,147 Epoch 676: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.31 
2024-02-04 08:10:55,148 EPOCH 677
2024-02-04 08:10:57,387 [Epoch: 677 Step: 00023000] Batch Recognition Loss:   0.000301 => Gls Tokens per Sec:     2176 || Batch Translation Loss:   0.149342 => Txt Tokens per Sec:     6163 || Lr: 0.000100
2024-02-04 08:10:59,801 Epoch 677: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.82 
2024-02-04 08:10:59,801 EPOCH 678
2024-02-04 08:11:04,429 Epoch 678: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.60 
2024-02-04 08:11:04,429 EPOCH 679
2024-02-04 08:11:09,015 Epoch 679: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.06 
2024-02-04 08:11:09,015 EPOCH 680
2024-02-04 08:11:10,689 [Epoch: 680 Step: 00023100] Batch Recognition Loss:   0.000245 => Gls Tokens per Sec:     2678 || Batch Translation Loss:   0.023545 => Txt Tokens per Sec:     7413 || Lr: 0.000100
2024-02-04 08:11:13,825 Epoch 680: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.55 
2024-02-04 08:11:13,826 EPOCH 681
2024-02-04 08:11:18,258 Epoch 681: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.05 
2024-02-04 08:11:18,258 EPOCH 682
2024-02-04 08:11:22,960 Epoch 682: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.83 
2024-02-04 08:11:22,960 EPOCH 683
2024-02-04 08:11:24,543 [Epoch: 683 Step: 00023200] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:     2427 || Batch Translation Loss:   0.044937 => Txt Tokens per Sec:     6708 || Lr: 0.000100
2024-02-04 08:11:27,409 Epoch 683: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.80 
2024-02-04 08:11:27,410 EPOCH 684
2024-02-04 08:11:32,146 Epoch 684: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.64 
2024-02-04 08:11:32,146 EPOCH 685
2024-02-04 08:11:36,584 Epoch 685: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.56 
2024-02-04 08:11:36,584 EPOCH 686
2024-02-04 08:11:37,675 [Epoch: 686 Step: 00023300] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:     2936 || Batch Translation Loss:   0.110859 => Txt Tokens per Sec:     7727 || Lr: 0.000100
2024-02-04 08:11:41,491 Epoch 686: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.00 
2024-02-04 08:11:41,491 EPOCH 687
2024-02-04 08:11:45,838 Epoch 687: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.78 
2024-02-04 08:11:45,838 EPOCH 688
2024-02-04 08:11:50,840 Epoch 688: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.33 
2024-02-04 08:11:50,840 EPOCH 689
2024-02-04 08:11:51,816 [Epoch: 689 Step: 00023400] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     2628 || Batch Translation Loss:   0.060317 => Txt Tokens per Sec:     7302 || Lr: 0.000100
2024-02-04 08:11:55,139 Epoch 689: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.38 
2024-02-04 08:11:55,140 EPOCH 690
2024-02-04 08:12:00,132 Epoch 690: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.11 
2024-02-04 08:12:00,133 EPOCH 691
2024-02-04 08:12:04,724 Epoch 691: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-04 08:12:04,724 EPOCH 692
2024-02-04 08:12:05,734 [Epoch: 692 Step: 00023500] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:     1656 || Batch Translation Loss:   0.004598 => Txt Tokens per Sec:     4695 || Lr: 0.000100
2024-02-04 08:12:09,639 Epoch 692: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-04 08:12:09,639 EPOCH 693
2024-02-04 08:12:14,454 Epoch 693: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-04 08:12:14,455 EPOCH 694
2024-02-04 08:12:18,949 Epoch 694: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-04 08:12:18,950 EPOCH 695
2024-02-04 08:12:19,352 [Epoch: 695 Step: 00023600] Batch Recognition Loss:   0.000291 => Gls Tokens per Sec:     3192 || Batch Translation Loss:   0.019044 => Txt Tokens per Sec:     8519 || Lr: 0.000100
2024-02-04 08:12:23,601 Epoch 695: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-04 08:12:23,601 EPOCH 696
2024-02-04 08:12:28,187 Epoch 696: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-04 08:12:28,187 EPOCH 697
2024-02-04 08:12:32,679 Epoch 697: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.30 
2024-02-04 08:12:32,680 EPOCH 698
2024-02-04 08:12:33,085 [Epoch: 698 Step: 00023700] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     1586 || Batch Translation Loss:   0.029708 => Txt Tokens per Sec:     5292 || Lr: 0.000100
2024-02-04 08:12:37,329 Epoch 698: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.06 
2024-02-04 08:12:37,329 EPOCH 699
2024-02-04 08:12:42,013 Epoch 699: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.76 
2024-02-04 08:12:42,014 EPOCH 700
2024-02-04 08:12:46,507 [Epoch: 700 Step: 00023800] Batch Recognition Loss:   0.000387 => Gls Tokens per Sec:     2366 || Batch Translation Loss:   0.070325 => Txt Tokens per Sec:     6568 || Lr: 0.000100
2024-02-04 08:12:46,508 Epoch 700: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.88 
2024-02-04 08:12:46,508 EPOCH 701
2024-02-04 08:12:51,286 Epoch 701: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.47 
2024-02-04 08:12:51,287 EPOCH 702
2024-02-04 08:12:55,679 Epoch 702: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.34 
2024-02-04 08:12:55,679 EPOCH 703
2024-02-04 08:13:00,245 [Epoch: 703 Step: 00023900] Batch Recognition Loss:   0.000471 => Gls Tokens per Sec:     2188 || Batch Translation Loss:   0.080362 => Txt Tokens per Sec:     6121 || Lr: 0.000100
2024-02-04 08:13:00,505 Epoch 703: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.32 
2024-02-04 08:13:00,505 EPOCH 704
2024-02-04 08:13:04,870 Epoch 704: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.74 
2024-02-04 08:13:04,870 EPOCH 705
2024-02-04 08:13:09,597 Epoch 705: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.67 
2024-02-04 08:13:09,598 EPOCH 706
2024-02-04 08:13:13,330 [Epoch: 706 Step: 00024000] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:     2506 || Batch Translation Loss:   0.024879 => Txt Tokens per Sec:     6799 || Lr: 0.000100
2024-02-04 08:13:21,780 Validation result at epoch 706, step    24000: duration: 8.4500s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00074	Translation Loss: 94128.24219	PPL: 12325.41113
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.74	(BLEU-1: 10.19,	BLEU-2: 3.15,	BLEU-3: 1.39,	BLEU-4: 0.74)
	CHRF 17.07	ROUGE 9.04
2024-02-04 08:13:21,781 Logging Recognition and Translation Outputs
2024-02-04 08:13:21,781 ========================================================================================================================
2024-02-04 08:13:21,782 Logging Sequence: 169_165.00
2024-02-04 08:13:21,782 	Gloss Reference :	A B+C+D+E
2024-02-04 08:13:21,782 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:13:21,783 	Gloss Alignment :	         
2024-02-04 08:13:21,783 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:13:21,785 	Text Reference  :	the indian government was   outraged by       the  incident and   these changes were undone by    wikipedia 
2024-02-04 08:13:21,785 	Text Hypothesis :	*** ****** ********** kohli has      revealed that mahendra singh dhoni was     the  most   loved footballer
2024-02-04 08:13:21,785 	Text Alignment  :	D   D      D          S     S        S        S    S        S     S     S       S    S      S     S         
2024-02-04 08:13:21,786 ========================================================================================================================
2024-02-04 08:13:21,786 Logging Sequence: 175_60.00
2024-02-04 08:13:21,786 	Gloss Reference :	A B+C+D+E
2024-02-04 08:13:21,786 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:13:21,786 	Gloss Alignment :	         
2024-02-04 08:13:21,786 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:13:21,787 	Text Reference  :	that is how india bagged 9   medals in the youth    tournament
2024-02-04 08:13:21,787 	Text Hypothesis :	**** ** *** as    per    the rules  of the original way       
2024-02-04 08:13:21,787 	Text Alignment  :	D    D  D   S     S      S   S      S      S        S         
2024-02-04 08:13:21,787 ========================================================================================================================
2024-02-04 08:13:21,787 Logging Sequence: 61_255.00
2024-02-04 08:13:21,788 	Gloss Reference :	A B+C+D+E
2024-02-04 08:13:21,788 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:13:21,788 	Gloss Alignment :	         
2024-02-04 08:13:21,788 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:13:21,789 	Text Reference  :	** in   2011    we  decided to    marry  and informed our families
2024-02-04 08:13:21,789 	Text Hypothesis :	so many tickets can watch   other number on  3rd      may 2023    
2024-02-04 08:13:21,789 	Text Alignment  :	I  S    S       S   S       S     S      S   S        S   S       
2024-02-04 08:13:21,789 ========================================================================================================================
2024-02-04 08:13:21,789 Logging Sequence: 173_39.00
2024-02-04 08:13:21,789 	Gloss Reference :	A B+C+D+E
2024-02-04 08:13:21,789 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:13:21,790 	Gloss Alignment :	         
2024-02-04 08:13:21,790 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:13:21,791 	Text Reference  :	***** ****** *** *** kohli   will step down  as india' captain
2024-02-04 08:13:21,791 	Text Hypothesis :	since sharma was the captain he   was  fined rs 12     lakh   
2024-02-04 08:13:21,791 	Text Alignment  :	I     I      I   I   S       S    S    S     S  S      S      
2024-02-04 08:13:21,791 ========================================================================================================================
2024-02-04 08:13:21,791 Logging Sequence: 172_82.00
2024-02-04 08:13:21,791 	Gloss Reference :	A B+C+D+E
2024-02-04 08:13:21,791 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:13:21,792 	Gloss Alignment :	         
2024-02-04 08:13:21,792 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:13:21,793 	Text Reference  :	you all know that the toss was about to start at 700 pm but it started raining at around 630 pm      
2024-02-04 08:13:21,793 	Text Hypothesis :	*** she said that *** **** *** ***** ** ***** ** *** ** *** ** ******* ******* he has    5   children
2024-02-04 08:13:21,793 	Text Alignment  :	D   S   S         D   D    D   D     D  D     D  D   D  D   D  D       D       S  S      S   S       
2024-02-04 08:13:21,793 ========================================================================================================================
2024-02-04 08:13:22,585 Epoch 706: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.66 
2024-02-04 08:13:22,585 EPOCH 707
2024-02-04 08:13:27,556 Epoch 707: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.94 
2024-02-04 08:13:27,556 EPOCH 708
2024-02-04 08:13:31,908 Epoch 708: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.62 
2024-02-04 08:13:31,908 EPOCH 709
2024-02-04 08:13:35,751 [Epoch: 709 Step: 00024100] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     2266 || Batch Translation Loss:   0.020491 => Txt Tokens per Sec:     6169 || Lr: 0.000100
2024-02-04 08:13:36,812 Epoch 709: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-04 08:13:36,812 EPOCH 710
2024-02-04 08:13:41,221 Epoch 710: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-04 08:13:41,222 EPOCH 711
2024-02-04 08:13:46,095 Epoch 711: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.16 
2024-02-04 08:13:46,096 EPOCH 712
2024-02-04 08:13:49,324 [Epoch: 712 Step: 00024200] Batch Recognition Loss:   0.000250 => Gls Tokens per Sec:     2501 || Batch Translation Loss:   0.030996 => Txt Tokens per Sec:     7006 || Lr: 0.000100
2024-02-04 08:13:50,471 Epoch 712: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-04 08:13:50,471 EPOCH 713
2024-02-04 08:13:55,318 Epoch 713: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-04 08:13:55,318 EPOCH 714
2024-02-04 08:13:59,795 Epoch 714: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.19 
2024-02-04 08:13:59,796 EPOCH 715
2024-02-04 08:14:03,021 [Epoch: 715 Step: 00024300] Batch Recognition Loss:   0.000232 => Gls Tokens per Sec:     2304 || Batch Translation Loss:   0.086004 => Txt Tokens per Sec:     6240 || Lr: 0.000100
2024-02-04 08:14:04,850 Epoch 715: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.89 
2024-02-04 08:14:04,850 EPOCH 716
2024-02-04 08:14:09,115 Epoch 716: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.75 
2024-02-04 08:14:09,115 EPOCH 717
2024-02-04 08:14:14,018 Epoch 717: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.09 
2024-02-04 08:14:14,019 EPOCH 718
2024-02-04 08:14:16,782 [Epoch: 718 Step: 00024400] Batch Recognition Loss:   0.000113 => Gls Tokens per Sec:     2458 || Batch Translation Loss:   0.080639 => Txt Tokens per Sec:     6985 || Lr: 0.000100
2024-02-04 08:14:18,386 Epoch 718: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.24 
2024-02-04 08:14:18,387 EPOCH 719
2024-02-04 08:14:23,304 Epoch 719: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.04 
2024-02-04 08:14:23,304 EPOCH 720
2024-02-04 08:14:27,731 Epoch 720: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.21 
2024-02-04 08:14:27,732 EPOCH 721
2024-02-04 08:14:30,950 [Epoch: 721 Step: 00024500] Batch Recognition Loss:   0.000253 => Gls Tokens per Sec:     1911 || Batch Translation Loss:   0.044478 => Txt Tokens per Sec:     5706 || Lr: 0.000100
2024-02-04 08:14:32,513 Epoch 721: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.12 
2024-02-04 08:14:32,513 EPOCH 722
2024-02-04 08:14:36,537 Epoch 722: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.94 
2024-02-04 08:14:36,537 EPOCH 723
2024-02-04 08:14:41,444 Epoch 723: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.18 
2024-02-04 08:14:41,444 EPOCH 724
2024-02-04 08:14:43,670 [Epoch: 724 Step: 00024600] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     2590 || Batch Translation Loss:   0.067970 => Txt Tokens per Sec:     7117 || Lr: 0.000100
2024-02-04 08:14:45,737 Epoch 724: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.30 
2024-02-04 08:14:45,738 EPOCH 725
2024-02-04 08:14:49,790 Epoch 725: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.07 
2024-02-04 08:14:49,790 EPOCH 726
2024-02-04 08:14:54,397 Epoch 726: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.45 
2024-02-04 08:14:54,398 EPOCH 727
2024-02-04 08:14:56,452 [Epoch: 727 Step: 00024700] Batch Recognition Loss:   0.000272 => Gls Tokens per Sec:     2494 || Batch Translation Loss:   0.211821 => Txt Tokens per Sec:     6795 || Lr: 0.000100
2024-02-04 08:14:58,973 Epoch 727: Total Training Recognition Loss 0.01  Total Training Translation Loss 7.32 
2024-02-04 08:14:58,973 EPOCH 728
2024-02-04 08:15:03,679 Epoch 728: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.78 
2024-02-04 08:15:03,680 EPOCH 729
2024-02-04 08:15:08,154 Epoch 729: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.78 
2024-02-04 08:15:08,155 EPOCH 730
2024-02-04 08:15:10,291 [Epoch: 730 Step: 00024800] Batch Recognition Loss:   0.000323 => Gls Tokens per Sec:     2098 || Batch Translation Loss:   0.023488 => Txt Tokens per Sec:     5961 || Lr: 0.000100
2024-02-04 08:15:12,936 Epoch 730: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.38 
2024-02-04 08:15:12,936 EPOCH 731
2024-02-04 08:15:17,212 Epoch 731: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.70 
2024-02-04 08:15:17,212 EPOCH 732
2024-02-04 08:15:22,092 Epoch 732: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.93 
2024-02-04 08:15:22,093 EPOCH 733
2024-02-04 08:15:23,625 [Epoch: 733 Step: 00024900] Batch Recognition Loss:   0.000276 => Gls Tokens per Sec:     2508 || Batch Translation Loss:   0.041069 => Txt Tokens per Sec:     7020 || Lr: 0.000100
2024-02-04 08:15:26,411 Epoch 733: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.95 
2024-02-04 08:15:26,411 EPOCH 734
2024-02-04 08:15:31,394 Epoch 734: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.63 
2024-02-04 08:15:31,394 EPOCH 735
2024-02-04 08:15:35,600 Epoch 735: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-04 08:15:35,600 EPOCH 736
2024-02-04 08:15:36,640 [Epoch: 736 Step: 00025000] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:     3080 || Batch Translation Loss:   0.050040 => Txt Tokens per Sec:     8210 || Lr: 0.000100
2024-02-04 08:15:40,337 Epoch 736: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-04 08:15:40,337 EPOCH 737
2024-02-04 08:15:44,741 Epoch 737: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-04 08:15:44,742 EPOCH 738
2024-02-04 08:15:49,699 Epoch 738: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-04 08:15:49,700 EPOCH 739
2024-02-04 08:15:50,635 [Epoch: 739 Step: 00025100] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:     2472 || Batch Translation Loss:   0.017567 => Txt Tokens per Sec:     6758 || Lr: 0.000100
2024-02-04 08:15:53,933 Epoch 739: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-04 08:15:53,933 EPOCH 740
2024-02-04 08:15:58,539 Epoch 740: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-04 08:15:58,540 EPOCH 741
2024-02-04 08:16:03,123 Epoch 741: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-04 08:16:03,123 EPOCH 742
2024-02-04 08:16:03,635 [Epoch: 742 Step: 00025200] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:     3757 || Batch Translation Loss:   0.017237 => Txt Tokens per Sec:     8769 || Lr: 0.000100
2024-02-04 08:16:07,465 Epoch 742: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-04 08:16:07,466 EPOCH 743
2024-02-04 08:16:12,239 Epoch 743: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.43 
2024-02-04 08:16:12,239 EPOCH 744
2024-02-04 08:16:16,834 Epoch 744: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.29 
2024-02-04 08:16:16,834 EPOCH 745
2024-02-04 08:16:17,446 [Epoch: 745 Step: 00025300] Batch Recognition Loss:   0.000327 => Gls Tokens per Sec:     2095 || Batch Translation Loss:   0.030134 => Txt Tokens per Sec:     5473 || Lr: 0.000100
2024-02-04 08:16:21,449 Epoch 745: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.37 
2024-02-04 08:16:21,449 EPOCH 746
2024-02-04 08:16:26,021 Epoch 746: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.49 
2024-02-04 08:16:26,022 EPOCH 747
2024-02-04 08:16:30,626 Epoch 747: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.91 
2024-02-04 08:16:30,626 EPOCH 748
2024-02-04 08:16:30,859 [Epoch: 748 Step: 00025400] Batch Recognition Loss:   0.000242 => Gls Tokens per Sec:     2771 || Batch Translation Loss:   0.101854 => Txt Tokens per Sec:     7243 || Lr: 0.000100
2024-02-04 08:16:35,347 Epoch 748: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.99 
2024-02-04 08:16:35,348 EPOCH 749
2024-02-04 08:16:39,859 Epoch 749: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.42 
2024-02-04 08:16:39,859 EPOCH 750
2024-02-04 08:16:44,323 [Epoch: 750 Step: 00025500] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:     2382 || Batch Translation Loss:   0.034553 => Txt Tokens per Sec:     6613 || Lr: 0.000100
2024-02-04 08:16:44,323 Epoch 750: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.45 
2024-02-04 08:16:44,323 EPOCH 751
2024-02-04 08:16:48,965 Epoch 751: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.39 
2024-02-04 08:16:48,965 EPOCH 752
2024-02-04 08:16:53,660 Epoch 752: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.61 
2024-02-04 08:16:53,660 EPOCH 753
2024-02-04 08:16:57,840 [Epoch: 753 Step: 00025600] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     2391 || Batch Translation Loss:   0.038776 => Txt Tokens per Sec:     6551 || Lr: 0.000100
2024-02-04 08:16:58,217 Epoch 753: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.66 
2024-02-04 08:16:58,217 EPOCH 754
2024-02-04 08:17:02,952 Epoch 754: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.85 
2024-02-04 08:17:02,953 EPOCH 755
2024-02-04 08:17:07,410 Epoch 755: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.33 
2024-02-04 08:17:07,410 EPOCH 756
2024-02-04 08:17:11,445 [Epoch: 756 Step: 00025700] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     2380 || Batch Translation Loss:   0.041930 => Txt Tokens per Sec:     6702 || Lr: 0.000100
2024-02-04 08:17:11,869 Epoch 756: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.99 
2024-02-04 08:17:11,870 EPOCH 757
2024-02-04 08:17:16,569 Epoch 757: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.53 
2024-02-04 08:17:16,569 EPOCH 758
2024-02-04 08:17:21,255 Epoch 758: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.96 
2024-02-04 08:17:21,255 EPOCH 759
2024-02-04 08:17:25,182 [Epoch: 759 Step: 00025800] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     2283 || Batch Translation Loss:   0.032939 => Txt Tokens per Sec:     6435 || Lr: 0.000100
2024-02-04 08:17:25,796 Epoch 759: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.59 
2024-02-04 08:17:25,796 EPOCH 760
2024-02-04 08:17:30,516 Epoch 760: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.17 
2024-02-04 08:17:30,516 EPOCH 761
2024-02-04 08:17:34,951 Epoch 761: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.95 
2024-02-04 08:17:34,951 EPOCH 762
2024-02-04 08:17:38,679 [Epoch: 762 Step: 00025900] Batch Recognition Loss:   0.000293 => Gls Tokens per Sec:     2165 || Batch Translation Loss:   0.068062 => Txt Tokens per Sec:     5972 || Lr: 0.000100
2024-02-04 08:17:39,781 Epoch 762: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.20 
2024-02-04 08:17:39,781 EPOCH 763
2024-02-04 08:17:44,138 Epoch 763: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.80 
2024-02-04 08:17:44,138 EPOCH 764
2024-02-04 08:17:49,103 Epoch 764: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.90 
2024-02-04 08:17:49,104 EPOCH 765
2024-02-04 08:17:52,216 [Epoch: 765 Step: 00026000] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:     2388 || Batch Translation Loss:   0.691252 => Txt Tokens per Sec:     6665 || Lr: 0.000100
2024-02-04 08:18:00,956 Validation result at epoch 765, step    26000: duration: 8.7401s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00079	Translation Loss: 93606.57812	PPL: 11698.49219
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.65	(BLEU-1: 10.76,	BLEU-2: 3.27,	BLEU-3: 1.37,	BLEU-4: 0.65)
	CHRF 17.49	ROUGE 9.20
2024-02-04 08:18:00,957 Logging Recognition and Translation Outputs
2024-02-04 08:18:00,957 ========================================================================================================================
2024-02-04 08:18:00,957 Logging Sequence: 130_139.00
2024-02-04 08:18:00,958 	Gloss Reference :	A B+C+D+E
2024-02-04 08:18:00,958 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:18:00,959 	Gloss Alignment :	         
2024-02-04 08:18:00,960 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:18:00,962 	Text Reference  :	he shared a picture of  a little pouch he knit for his olympic gold   medal with uk flag on  one  side   and **** japanese flag on      the     other
2024-02-04 08:18:00,962 	Text Hypothesis :	he ****** * ******* won a ****** ***** ** **** *** *** ******* bronze medal **** ** at   the 2012 london and 2016 rio      de   janeiro olympic games
2024-02-04 08:18:00,963 	Text Alignment  :	   D      D D       S     D      D     D  D    D   D   D       S            D    D  S    S   S    S          I    S        S    S       S       S    
2024-02-04 08:18:00,963 ========================================================================================================================
2024-02-04 08:18:00,963 Logging Sequence: 72_194.00
2024-02-04 08:18:00,963 	Gloss Reference :	A B+C+D+E
2024-02-04 08:18:00,963 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:18:00,963 	Gloss Alignment :	         
2024-02-04 08:18:00,963 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:18:00,964 	Text Reference  :	shah told her to do what she  wants and      filed a police complaint against her  
2024-02-04 08:18:00,965 	Text Hypothesis :	**** **** *** ** ** they have been  shocking for   5 or     going     on      there
2024-02-04 08:18:00,965 	Text Alignment  :	D    D    D   D  D  S    S    S     S        S     S S      S         S       S    
2024-02-04 08:18:00,965 ========================================================================================================================
2024-02-04 08:18:00,965 Logging Sequence: 69_177.00
2024-02-04 08:18:00,965 	Gloss Reference :	A B+C+D+E
2024-02-04 08:18:00,965 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:18:00,965 	Gloss Alignment :	         
2024-02-04 08:18:00,965 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:18:00,967 	Text Reference  :	he said 'i will continue playing i   know   it's about   time i    retire i     also    have   a       knee    condition
2024-02-04 08:18:00,967 	Text Hypothesis :	** **** ** **** ******** when    the stumps were waiting to   play the    match between mumbai indians indians mi       
2024-02-04 08:18:00,967 	Text Alignment  :	D  D    D  D    D        S       S   S      S    S       S    S    S      S     S       S      S       S       S        
2024-02-04 08:18:00,967 ========================================================================================================================
2024-02-04 08:18:00,968 Logging Sequence: 95_118.00
2024-02-04 08:18:00,968 	Gloss Reference :	A B+C+D+E
2024-02-04 08:18:00,968 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:18:00,968 	Gloss Alignment :	         
2024-02-04 08:18:00,968 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:18:00,969 	Text Reference  :	*** **** ** **** *** *** the game  was stopped strangely due    to excessive sunlight
2024-02-04 08:18:00,969 	Text Hypothesis :	and post my cool are all the pitch and hope    he        failed to ********* bat     
2024-02-04 08:18:00,969 	Text Alignment  :	I   I    I  I    I   I       S     S   S       S         S         D         S       
2024-02-04 08:18:00,969 ========================================================================================================================
2024-02-04 08:18:00,969 Logging Sequence: 112_8.00
2024-02-04 08:18:00,970 	Gloss Reference :	A B+C+D+E
2024-02-04 08:18:00,970 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:18:00,970 	Gloss Alignment :	         
2024-02-04 08:18:00,970 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:18:00,972 	Text Reference  :	before there were 8   teams such  as         mumbai indians delhi  capitals punjab     kings etc  and now there will be 10 teams in 2022
2024-02-04 08:18:00,972 	Text Hypothesis :	****** ***** **** the rpsg  group previously owned  the     rising pune     supergiant in    2016 and *** ***** **** ** ** ***** ** 2017
2024-02-04 08:18:00,972 	Text Alignment  :	D      D     D    S   S     S     S          S      S       S      S        S          S     S        D   D     D    D  D  D     D  S   
2024-02-04 08:18:00,972 ========================================================================================================================
2024-02-04 08:18:02,238 Epoch 765: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.29 
2024-02-04 08:18:02,239 EPOCH 766
2024-02-04 08:18:06,914 Epoch 766: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.73 
2024-02-04 08:18:06,914 EPOCH 767
2024-02-04 08:18:11,005 Epoch 767: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.24 
2024-02-04 08:18:11,005 EPOCH 768
2024-02-04 08:18:14,106 [Epoch: 768 Step: 00026100] Batch Recognition Loss:   0.000266 => Gls Tokens per Sec:     2271 || Batch Translation Loss:   0.032617 => Txt Tokens per Sec:     6181 || Lr: 0.000100
2024-02-04 08:18:15,777 Epoch 768: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.14 
2024-02-04 08:18:15,777 EPOCH 769
2024-02-04 08:18:20,131 Epoch 769: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.41 
2024-02-04 08:18:20,132 EPOCH 770
2024-02-04 08:18:25,048 Epoch 770: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.01 
2024-02-04 08:18:25,049 EPOCH 771
2024-02-04 08:18:27,803 [Epoch: 771 Step: 00026200] Batch Recognition Loss:   0.000488 => Gls Tokens per Sec:     2235 || Batch Translation Loss:   0.406856 => Txt Tokens per Sec:     6333 || Lr: 0.000100
2024-02-04 08:18:29,319 Epoch 771: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.58 
2024-02-04 08:18:29,319 EPOCH 772
2024-02-04 08:18:34,440 Epoch 772: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.52 
2024-02-04 08:18:34,440 EPOCH 773
2024-02-04 08:18:39,175 Epoch 773: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.77 
2024-02-04 08:18:39,176 EPOCH 774
2024-02-04 08:18:41,694 [Epoch: 774 Step: 00026300] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     2189 || Batch Translation Loss:   0.036375 => Txt Tokens per Sec:     6075 || Lr: 0.000100
2024-02-04 08:18:43,982 Epoch 774: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.12 
2024-02-04 08:18:43,982 EPOCH 775
2024-02-04 08:18:48,260 Epoch 775: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.44 
2024-02-04 08:18:48,261 EPOCH 776
2024-02-04 08:18:52,772 Epoch 776: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.72 
2024-02-04 08:18:52,772 EPOCH 777
2024-02-04 08:18:55,165 [Epoch: 777 Step: 00026400] Batch Recognition Loss:   0.000304 => Gls Tokens per Sec:     2037 || Batch Translation Loss:   0.050428 => Txt Tokens per Sec:     5870 || Lr: 0.000100
2024-02-04 08:18:57,499 Epoch 777: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.38 
2024-02-04 08:18:57,499 EPOCH 778
2024-02-04 08:19:02,195 Epoch 778: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-04 08:19:02,195 EPOCH 779
2024-02-04 08:19:06,695 Epoch 779: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-04 08:19:06,695 EPOCH 780
2024-02-04 08:19:08,728 [Epoch: 780 Step: 00026500] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:     2081 || Batch Translation Loss:   0.032166 => Txt Tokens per Sec:     5987 || Lr: 0.000100
2024-02-04 08:19:11,424 Epoch 780: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.30 
2024-02-04 08:19:11,425 EPOCH 781
2024-02-04 08:19:15,895 Epoch 781: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-04 08:19:15,896 EPOCH 782
2024-02-04 08:19:20,803 Epoch 782: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.15 
2024-02-04 08:19:20,804 EPOCH 783
2024-02-04 08:19:22,315 [Epoch: 783 Step: 00026600] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     2544 || Batch Translation Loss:   0.023431 => Txt Tokens per Sec:     7210 || Lr: 0.000100
2024-02-04 08:19:25,175 Epoch 783: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-04 08:19:25,175 EPOCH 784
2024-02-04 08:19:30,289 Epoch 784: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.02 
2024-02-04 08:19:30,290 EPOCH 785
2024-02-04 08:19:34,826 Epoch 785: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.54 
2024-02-04 08:19:34,826 EPOCH 786
2024-02-04 08:19:35,899 [Epoch: 786 Step: 00026700] Batch Recognition Loss:   0.000323 => Gls Tokens per Sec:     2752 || Batch Translation Loss:   0.029749 => Txt Tokens per Sec:     7216 || Lr: 0.000100
2024-02-04 08:19:39,563 Epoch 786: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.50 
2024-02-04 08:19:39,563 EPOCH 787
2024-02-04 08:19:44,060 Epoch 787: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.36 
2024-02-04 08:19:44,060 EPOCH 788
2024-02-04 08:19:48,893 Epoch 788: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.90 
2024-02-04 08:19:48,893 EPOCH 789
2024-02-04 08:19:49,843 [Epoch: 789 Step: 00026800] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:     2702 || Batch Translation Loss:   0.054392 => Txt Tokens per Sec:     7647 || Lr: 0.000100
2024-02-04 08:19:53,268 Epoch 789: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.33 
2024-02-04 08:19:53,268 EPOCH 790
2024-02-04 08:19:58,109 Epoch 790: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-04 08:19:58,109 EPOCH 791
2024-02-04 08:20:02,410 Epoch 791: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-04 08:20:02,410 EPOCH 792
2024-02-04 08:20:02,988 [Epoch: 792 Step: 00026900] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     3325 || Batch Translation Loss:   0.015748 => Txt Tokens per Sec:     8139 || Lr: 0.000100
2024-02-04 08:20:06,532 Epoch 792: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.73 
2024-02-04 08:20:06,533 EPOCH 793
2024-02-04 08:20:11,288 Epoch 793: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.29 
2024-02-04 08:20:11,289 EPOCH 794
2024-02-04 08:20:15,690 Epoch 794: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.54 
2024-02-04 08:20:15,690 EPOCH 795
2024-02-04 08:20:16,075 [Epoch: 795 Step: 00027000] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:     3335 || Batch Translation Loss:   0.048285 => Txt Tokens per Sec:     8122 || Lr: 0.000100
2024-02-04 08:20:20,520 Epoch 795: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.71 
2024-02-04 08:20:20,521 EPOCH 796
2024-02-04 08:20:24,874 Epoch 796: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.61 
2024-02-04 08:20:24,874 EPOCH 797
2024-02-04 08:20:29,856 Epoch 797: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.71 
2024-02-04 08:20:29,857 EPOCH 798
2024-02-04 08:20:30,042 [Epoch: 798 Step: 00027100] Batch Recognition Loss:   0.000239 => Gls Tokens per Sec:     3478 || Batch Translation Loss:   0.054436 => Txt Tokens per Sec:     8967 || Lr: 0.000100
2024-02-04 08:20:34,260 Epoch 798: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.43 
2024-02-04 08:20:34,260 EPOCH 799
2024-02-04 08:20:39,080 Epoch 799: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.34 
2024-02-04 08:20:39,081 EPOCH 800
2024-02-04 08:20:43,543 [Epoch: 800 Step: 00027200] Batch Recognition Loss:   0.000408 => Gls Tokens per Sec:     2383 || Batch Translation Loss:   0.114465 => Txt Tokens per Sec:     6616 || Lr: 0.000100
2024-02-04 08:20:43,544 Epoch 800: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.66 
2024-02-04 08:20:43,544 EPOCH 801
2024-02-04 08:20:48,289 Epoch 801: Total Training Recognition Loss 0.05  Total Training Translation Loss 13.22 
2024-02-04 08:20:48,289 EPOCH 802
2024-02-04 08:20:52,821 Epoch 802: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.65 
2024-02-04 08:20:52,821 EPOCH 803
2024-02-04 08:20:57,307 [Epoch: 803 Step: 00027300] Batch Recognition Loss:   0.000343 => Gls Tokens per Sec:     2228 || Batch Translation Loss:   0.075315 => Txt Tokens per Sec:     6221 || Lr: 0.000100
2024-02-04 08:20:57,510 Epoch 803: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.40 
2024-02-04 08:20:57,510 EPOCH 804
2024-02-04 08:21:02,242 Epoch 804: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.80 
2024-02-04 08:21:02,243 EPOCH 805
2024-02-04 08:21:06,560 Epoch 805: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.88 
2024-02-04 08:21:06,560 EPOCH 806
2024-02-04 08:21:10,220 [Epoch: 806 Step: 00027400] Batch Recognition Loss:   0.000289 => Gls Tokens per Sec:     2555 || Batch Translation Loss:   0.014520 => Txt Tokens per Sec:     7124 || Lr: 0.000100
2024-02-04 08:21:10,704 Epoch 806: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-04 08:21:10,704 EPOCH 807
2024-02-04 08:21:15,429 Epoch 807: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-04 08:21:15,430 EPOCH 808
2024-02-04 08:21:19,902 Epoch 808: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-04 08:21:19,902 EPOCH 809
2024-02-04 08:21:23,645 [Epoch: 809 Step: 00027500] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     2395 || Batch Translation Loss:   0.019383 => Txt Tokens per Sec:     6711 || Lr: 0.000100
2024-02-04 08:21:24,441 Epoch 809: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-04 08:21:24,441 EPOCH 810
2024-02-04 08:21:29,076 Epoch 810: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-04 08:21:29,076 EPOCH 811
2024-02-04 08:21:33,867 Epoch 811: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-04 08:21:33,867 EPOCH 812
2024-02-04 08:21:37,270 [Epoch: 812 Step: 00027600] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     2372 || Batch Translation Loss:   0.014855 => Txt Tokens per Sec:     6503 || Lr: 0.000100
2024-02-04 08:21:38,315 Epoch 812: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-04 08:21:38,316 EPOCH 813
2024-02-04 08:21:42,870 Epoch 813: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-04 08:21:42,871 EPOCH 814
2024-02-04 08:21:47,476 Epoch 814: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-04 08:21:47,476 EPOCH 815
2024-02-04 08:21:50,560 [Epoch: 815 Step: 00027700] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     2411 || Batch Translation Loss:   0.012601 => Txt Tokens per Sec:     6667 || Lr: 0.000100
2024-02-04 08:21:52,118 Epoch 815: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-04 08:21:52,118 EPOCH 816
2024-02-04 08:21:56,613 Epoch 816: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-04 08:21:56,613 EPOCH 817
2024-02-04 08:22:01,201 Epoch 817: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-04 08:22:01,202 EPOCH 818
2024-02-04 08:22:04,201 [Epoch: 818 Step: 00027800] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     2265 || Batch Translation Loss:   0.015921 => Txt Tokens per Sec:     6362 || Lr: 0.000100
2024-02-04 08:22:05,810 Epoch 818: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-04 08:22:05,810 EPOCH 819
2024-02-04 08:22:10,579 Epoch 819: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-04 08:22:10,580 EPOCH 820
2024-02-04 08:22:15,167 Epoch 820: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.48 
2024-02-04 08:22:15,167 EPOCH 821
2024-02-04 08:22:17,940 [Epoch: 821 Step: 00027900] Batch Recognition Loss:   0.000618 => Gls Tokens per Sec:     2309 || Batch Translation Loss:   0.125289 => Txt Tokens per Sec:     6477 || Lr: 0.000100
2024-02-04 08:22:19,837 Epoch 821: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.34 
2024-02-04 08:22:19,837 EPOCH 822
2024-02-04 08:22:24,380 Epoch 822: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.15 
2024-02-04 08:22:24,380 EPOCH 823
2024-02-04 08:22:28,743 Epoch 823: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.04 
2024-02-04 08:22:28,743 EPOCH 824
2024-02-04 08:22:30,920 [Epoch: 824 Step: 00028000] Batch Recognition Loss:   0.000226 => Gls Tokens per Sec:     2532 || Batch Translation Loss:   0.056396 => Txt Tokens per Sec:     6636 || Lr: 0.000100
2024-02-04 08:22:39,501 Validation result at epoch 824, step    28000: duration: 8.5810s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00077	Translation Loss: 91738.74219	PPL: 9704.07031
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.74	(BLEU-1: 10.16,	BLEU-2: 3.18,	BLEU-3: 1.39,	BLEU-4: 0.74)
	CHRF 16.97	ROUGE 8.86
2024-02-04 08:22:39,502 Logging Recognition and Translation Outputs
2024-02-04 08:22:39,502 ========================================================================================================================
2024-02-04 08:22:39,502 Logging Sequence: 67_98.00
2024-02-04 08:22:39,503 	Gloss Reference :	A B+C+D+E
2024-02-04 08:22:39,503 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:22:39,503 	Gloss Alignment :	         
2024-02-04 08:22:39,503 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:22:39,505 	Text Reference  :	it saddens me to   see people suffering and  dying   due to      lack   of ****** ******** *** ***** ** ****** ** *** oxygen     
2024-02-04 08:22:39,505 	Text Hypothesis :	** i       am sure you all    must      have enjoyed the current season of indian wrestler but would be played in the semi-finals
2024-02-04 08:22:39,505 	Text Alignment  :	D  S       S  S    S   S      S         S    S       S   S       S         I      I        I   I     I  I      I  I   S          
2024-02-04 08:22:39,505 ========================================================================================================================
2024-02-04 08:22:39,505 Logging Sequence: 157_83.00
2024-02-04 08:22:39,506 	Gloss Reference :	A B+C+D+E
2024-02-04 08:22:39,506 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:22:39,506 	Gloss Alignment :	         
2024-02-04 08:22:39,506 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:22:39,508 	Text Reference  :	also when you eat sandwich at        a  streetside hawker or   stall the   sandwich maker     will      first apply     butter with a       knife 
2024-02-04 08:22:39,508 	Text Hypothesis :	**** **** *** *** however  yesterday on 28th       june   2023 virat kohli said     ganguly's statement is    extremely fit    and  gujarat giants
2024-02-04 08:22:39,508 	Text Alignment  :	D    D    D   D   S        S         S  S          S      S    S     S     S        S         S         S     S         S      S    S       S     
2024-02-04 08:22:39,509 ========================================================================================================================
2024-02-04 08:22:39,509 Logging Sequence: 76_35.00
2024-02-04 08:22:39,509 	Gloss Reference :	A B+C+D+E
2024-02-04 08:22:39,509 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:22:39,509 	Gloss Alignment :	         
2024-02-04 08:22:39,509 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:22:39,510 	Text Reference  :	*** bcci   president sourav ganguly along with   board secretary jay     shah
2024-02-04 08:22:39,510 	Text Hypothesis :	the indian premiere  league ipl     the   indian team  is        leading 2-1 
2024-02-04 08:22:39,510 	Text Alignment  :	I   S      S         S      S       S     S      S     S         S       S   
2024-02-04 08:22:39,510 ========================================================================================================================
2024-02-04 08:22:39,511 Logging Sequence: 139_180.00
2024-02-04 08:22:39,511 	Gloss Reference :	A B+C+D+E
2024-02-04 08:22:39,511 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:22:39,511 	Gloss Alignment :	         
2024-02-04 08:22:39,511 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:22:39,512 	Text Reference  :	* *** **** netherlands also  faced similar riots 
2024-02-04 08:22:39,512 	Text Hypothesis :	a few days went        viral on    the     matter
2024-02-04 08:22:39,512 	Text Alignment  :	I I   I    S           S     S     S       S     
2024-02-04 08:22:39,512 ========================================================================================================================
2024-02-04 08:22:39,512 Logging Sequence: 98_87.00
2024-02-04 08:22:39,512 	Gloss Reference :	A B+C+D+E
2024-02-04 08:22:39,512 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:22:39,512 	Gloss Alignment :	         
2024-02-04 08:22:39,513 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:22:39,514 	Text Reference  :	*** instead of starting afresh in  2021 the  organizers opted to ** resume     with the previous edition   
2024-02-04 08:22:39,514 	Text Hypothesis :	the coach   of ******** ****** the 10   team will       have  to be applicable in   the ******** tournament
2024-02-04 08:22:39,514 	Text Alignment  :	I   S          D        D      S   S    S    S          S        I  S          S        D        S         
2024-02-04 08:22:39,514 ========================================================================================================================
2024-02-04 08:22:42,194 Epoch 824: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.92 
2024-02-04 08:22:42,194 EPOCH 825
2024-02-04 08:22:46,542 Epoch 825: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.79 
2024-02-04 08:22:46,542 EPOCH 826
2024-02-04 08:22:50,663 Epoch 826: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.40 
2024-02-04 08:22:50,664 EPOCH 827
2024-02-04 08:22:53,382 [Epoch: 827 Step: 00028100] Batch Recognition Loss:   0.000328 => Gls Tokens per Sec:     1792 || Batch Translation Loss:   0.069688 => Txt Tokens per Sec:     5313 || Lr: 0.000100
2024-02-04 08:22:55,560 Epoch 827: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.44 
2024-02-04 08:22:55,560 EPOCH 828
2024-02-04 08:22:59,818 Epoch 828: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.61 
2024-02-04 08:22:59,819 EPOCH 829
2024-02-04 08:23:04,820 Epoch 829: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.00 
2024-02-04 08:23:04,820 EPOCH 830
2024-02-04 08:23:06,330 [Epoch: 830 Step: 00028200] Batch Recognition Loss:   0.000361 => Gls Tokens per Sec:     2804 || Batch Translation Loss:   0.162608 => Txt Tokens per Sec:     7694 || Lr: 0.000100
2024-02-04 08:23:09,175 Epoch 830: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.77 
2024-02-04 08:23:09,175 EPOCH 831
2024-02-04 08:23:14,019 Epoch 831: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-04 08:23:14,019 EPOCH 832
2024-02-04 08:23:18,524 Epoch 832: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-04 08:23:18,524 EPOCH 833
2024-02-04 08:23:19,891 [Epoch: 833 Step: 00028300] Batch Recognition Loss:   0.000213 => Gls Tokens per Sec:     2811 || Batch Translation Loss:   0.015839 => Txt Tokens per Sec:     7493 || Lr: 0.000100
2024-02-04 08:23:23,212 Epoch 833: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-04 08:23:23,212 EPOCH 834
2024-02-04 08:23:27,750 Epoch 834: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-04 08:23:27,751 EPOCH 835
2024-02-04 08:23:32,761 Epoch 835: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-04 08:23:32,762 EPOCH 836
2024-02-04 08:23:33,874 [Epoch: 836 Step: 00028400] Batch Recognition Loss:   0.000115 => Gls Tokens per Sec:     2879 || Batch Translation Loss:   0.016583 => Txt Tokens per Sec:     7662 || Lr: 0.000100
2024-02-04 08:23:37,053 Epoch 836: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-04 08:23:37,054 EPOCH 837
2024-02-04 08:23:41,990 Epoch 837: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-04 08:23:41,990 EPOCH 838
2024-02-04 08:23:46,354 Epoch 838: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-04 08:23:46,355 EPOCH 839
2024-02-04 08:23:47,437 [Epoch: 839 Step: 00028500] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     2373 || Batch Translation Loss:   0.014824 => Txt Tokens per Sec:     6500 || Lr: 0.000100
2024-02-04 08:23:51,180 Epoch 839: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-04 08:23:51,180 EPOCH 840
2024-02-04 08:23:55,726 Epoch 840: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.56 
2024-02-04 08:23:55,727 EPOCH 841
2024-02-04 08:24:00,430 Epoch 841: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.46 
2024-02-04 08:24:00,431 EPOCH 842
2024-02-04 08:24:01,498 [Epoch: 842 Step: 00028600] Batch Recognition Loss:   0.000152 => Gls Tokens per Sec:     1801 || Batch Translation Loss:   0.023346 => Txt Tokens per Sec:     5508 || Lr: 0.000100
2024-02-04 08:24:04,964 Epoch 842: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-04 08:24:04,964 EPOCH 843
2024-02-04 08:24:09,614 Epoch 843: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.55 
2024-02-04 08:24:09,614 EPOCH 844
2024-02-04 08:24:14,295 Epoch 844: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.61 
2024-02-04 08:24:14,295 EPOCH 845
2024-02-04 08:24:14,830 [Epoch: 845 Step: 00028700] Batch Recognition Loss:   0.000676 => Gls Tokens per Sec:     2405 || Batch Translation Loss:   0.112870 => Txt Tokens per Sec:     6256 || Lr: 0.000100
2024-02-04 08:24:18,811 Epoch 845: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.19 
2024-02-04 08:24:18,811 EPOCH 846
2024-02-04 08:24:23,708 Epoch 846: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.98 
2024-02-04 08:24:23,708 EPOCH 847
2024-02-04 08:24:27,953 Epoch 847: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.71 
2024-02-04 08:24:27,954 EPOCH 848
2024-02-04 08:24:28,170 [Epoch: 848 Step: 00028800] Batch Recognition Loss:   0.000323 => Gls Tokens per Sec:     2971 || Batch Translation Loss:   0.116725 => Txt Tokens per Sec:     8255 || Lr: 0.000100
2024-02-04 08:24:31,997 Epoch 848: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.83 
2024-02-04 08:24:31,997 EPOCH 849
2024-02-04 08:24:36,098 Epoch 849: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.78 
2024-02-04 08:24:36,098 EPOCH 850
2024-02-04 08:24:40,176 [Epoch: 850 Step: 00028900] Batch Recognition Loss:   0.000304 => Gls Tokens per Sec:     2607 || Batch Translation Loss:   0.054911 => Txt Tokens per Sec:     7238 || Lr: 0.000100
2024-02-04 08:24:40,176 Epoch 850: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.61 
2024-02-04 08:24:40,177 EPOCH 851
2024-02-04 08:24:44,265 Epoch 851: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.87 
2024-02-04 08:24:44,265 EPOCH 852
2024-02-04 08:24:48,408 Epoch 852: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-04 08:24:48,408 EPOCH 853
2024-02-04 08:24:53,140 [Epoch: 853 Step: 00029000] Batch Recognition Loss:   0.000257 => Gls Tokens per Sec:     2111 || Batch Translation Loss:   0.026804 => Txt Tokens per Sec:     5885 || Lr: 0.000100
2024-02-04 08:24:53,355 Epoch 853: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-04 08:24:53,355 EPOCH 854
2024-02-04 08:24:57,781 Epoch 854: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-04 08:24:57,781 EPOCH 855
2024-02-04 08:25:01,874 Epoch 855: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-04 08:25:01,874 EPOCH 856
2024-02-04 08:25:05,550 [Epoch: 856 Step: 00029100] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     2544 || Batch Translation Loss:   0.093337 => Txt Tokens per Sec:     7045 || Lr: 0.000100
2024-02-04 08:25:05,974 Epoch 856: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.46 
2024-02-04 08:25:05,974 EPOCH 857
2024-02-04 08:25:10,038 Epoch 857: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.77 
2024-02-04 08:25:10,038 EPOCH 858
2024-02-04 08:25:15,056 Epoch 858: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.71 
2024-02-04 08:25:15,057 EPOCH 859
2024-02-04 08:25:18,686 [Epoch: 859 Step: 00029200] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     2400 || Batch Translation Loss:   0.041486 => Txt Tokens per Sec:     6492 || Lr: 0.000100
2024-02-04 08:25:19,697 Epoch 859: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.16 
2024-02-04 08:25:19,697 EPOCH 860
2024-02-04 08:25:24,320 Epoch 860: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.46 
2024-02-04 08:25:24,320 EPOCH 861
2024-02-04 08:25:28,539 Epoch 861: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.50 
2024-02-04 08:25:28,540 EPOCH 862
2024-02-04 08:25:32,397 [Epoch: 862 Step: 00029300] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:     2093 || Batch Translation Loss:   0.150684 => Txt Tokens per Sec:     5921 || Lr: 0.000100
2024-02-04 08:25:33,383 Epoch 862: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.62 
2024-02-04 08:25:33,383 EPOCH 863
2024-02-04 08:25:37,764 Epoch 863: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.94 
2024-02-04 08:25:37,765 EPOCH 864
2024-02-04 08:25:42,565 Epoch 864: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.30 
2024-02-04 08:25:42,565 EPOCH 865
2024-02-04 08:25:45,321 [Epoch: 865 Step: 00029400] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     2697 || Batch Translation Loss:   0.027050 => Txt Tokens per Sec:     7211 || Lr: 0.000100
2024-02-04 08:25:47,201 Epoch 865: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-04 08:25:47,201 EPOCH 866
2024-02-04 08:25:51,838 Epoch 866: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-04 08:25:51,838 EPOCH 867
2024-02-04 08:25:56,491 Epoch 867: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.00 
2024-02-04 08:25:56,492 EPOCH 868
2024-02-04 08:25:59,336 [Epoch: 868 Step: 00029500] Batch Recognition Loss:   0.000252 => Gls Tokens per Sec:     2476 || Batch Translation Loss:   0.054760 => Txt Tokens per Sec:     6771 || Lr: 0.000100
2024-02-04 08:26:00,964 Epoch 868: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.06 
2024-02-04 08:26:00,964 EPOCH 869
2024-02-04 08:26:05,637 Epoch 869: Total Training Recognition Loss 0.01  Total Training Translation Loss 8.54 
2024-02-04 08:26:05,638 EPOCH 870
2024-02-04 08:26:10,143 Epoch 870: Total Training Recognition Loss 0.02  Total Training Translation Loss 9.73 
2024-02-04 08:26:10,143 EPOCH 871
2024-02-04 08:26:13,183 [Epoch: 871 Step: 00029600] Batch Recognition Loss:   0.000547 => Gls Tokens per Sec:     2106 || Batch Translation Loss:   0.098532 => Txt Tokens per Sec:     5945 || Lr: 0.000100
2024-02-04 08:26:14,973 Epoch 871: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.58 
2024-02-04 08:26:14,973 EPOCH 872
2024-02-04 08:26:19,316 Epoch 872: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.71 
2024-02-04 08:26:19,317 EPOCH 873
2024-02-04 08:26:24,256 Epoch 873: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.82 
2024-02-04 08:26:24,256 EPOCH 874
2024-02-04 08:26:26,561 [Epoch: 874 Step: 00029700] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:     2391 || Batch Translation Loss:   0.175917 => Txt Tokens per Sec:     6451 || Lr: 0.000100
2024-02-04 08:26:28,578 Epoch 874: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.74 
2024-02-04 08:26:28,579 EPOCH 875
2024-02-04 08:26:33,526 Epoch 875: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.62 
2024-02-04 08:26:33,527 EPOCH 876
2024-02-04 08:26:37,938 Epoch 876: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.19 
2024-02-04 08:26:37,938 EPOCH 877
2024-02-04 08:26:40,314 [Epoch: 877 Step: 00029800] Batch Recognition Loss:   0.000399 => Gls Tokens per Sec:     2051 || Batch Translation Loss:   0.029155 => Txt Tokens per Sec:     5684 || Lr: 0.000100
2024-02-04 08:26:42,731 Epoch 877: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.87 
2024-02-04 08:26:42,731 EPOCH 878
2024-02-04 08:26:47,219 Epoch 878: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.89 
2024-02-04 08:26:47,220 EPOCH 879
2024-02-04 08:26:51,919 Epoch 879: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-04 08:26:51,920 EPOCH 880
2024-02-04 08:26:53,268 [Epoch: 880 Step: 00029900] Batch Recognition Loss:   0.000202 => Gls Tokens per Sec:     3322 || Batch Translation Loss:   0.033173 => Txt Tokens per Sec:     8570 || Lr: 0.000100
2024-02-04 08:26:56,514 Epoch 880: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-04 08:26:56,514 EPOCH 881
2024-02-04 08:27:01,104 Epoch 881: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-04 08:27:01,104 EPOCH 882
2024-02-04 08:27:05,769 Epoch 882: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-04 08:27:05,770 EPOCH 883
2024-02-04 08:27:07,153 [Epoch: 883 Step: 00030000] Batch Recognition Loss:   0.000152 => Gls Tokens per Sec:     2781 || Batch Translation Loss:   0.014770 => Txt Tokens per Sec:     7417 || Lr: 0.000100
2024-02-04 08:27:15,685 Validation result at epoch 883, step    30000: duration: 8.5321s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00060	Translation Loss: 94254.43750	PPL: 12482.03809
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.81	(BLEU-1: 10.37,	BLEU-2: 3.47,	BLEU-3: 1.55,	BLEU-4: 0.81)
	CHRF 16.73	ROUGE 8.99
2024-02-04 08:27:15,686 Logging Recognition and Translation Outputs
2024-02-04 08:27:15,687 ========================================================================================================================
2024-02-04 08:27:15,687 Logging Sequence: 165_502.00
2024-02-04 08:27:15,687 	Gloss Reference :	A B+C+D+E
2024-02-04 08:27:15,687 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:27:15,688 	Gloss Alignment :	         
2024-02-04 08:27:15,688 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:27:15,689 	Text Reference  :	tendulkar would sit in the pavilion wearing both his   batting    pads even  after he got out   
2024-02-04 08:27:15,689 	Text Hypothesis :	********* ***** *** ** but many     people  use  their birthdates in   their email id and finals
2024-02-04 08:27:15,689 	Text Alignment  :	D         D     D   D  S   S        S       S    S     S          S    S     S     S  S   S     
2024-02-04 08:27:15,689 ========================================================================================================================
2024-02-04 08:27:15,690 Logging Sequence: 127_57.00
2024-02-04 08:27:15,690 	Gloss Reference :	A B+C+D+E
2024-02-04 08:27:15,690 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:27:15,690 	Gloss Alignment :	         
2024-02-04 08:27:15,690 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:27:15,692 	Text Reference  :	till date india had won only 2  medals    at the championships which like the    olympics is   the highest   level championship
2024-02-04 08:27:15,692 	Text Hypothesis :	**** **** ***** the an  odi  or scheduled in the ************* ***** same colony he       held in  australia new   zealand     
2024-02-04 08:27:15,692 	Text Alignment  :	D    D    D     S   S   S    S  S         S      D             D     S    S      S        S    S   S         S     S           
2024-02-04 08:27:15,692 ========================================================================================================================
2024-02-04 08:27:15,692 Logging Sequence: 169_10.00
2024-02-04 08:27:15,693 	Gloss Reference :	A B+C+D+E
2024-02-04 08:27:15,693 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:27:15,693 	Gloss Alignment :	         
2024-02-04 08:27:15,693 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:27:15,695 	Text Reference  :	the  18th     over was bowled by      ravi    bishnoi with   khushdil shah       and asif ali on the ******* crease    
2024-02-04 08:27:15,695 	Text Hypothesis :	when arshdeep lost the ball   leaving skipper rohit   sharma the      spectators and **** *** ** the viewers frustrated
2024-02-04 08:27:15,695 	Text Alignment  :	S    S        S    S   S      S       S       S       S      S        S              D    D   D      I       S         
2024-02-04 08:27:15,695 ========================================================================================================================
2024-02-04 08:27:15,695 Logging Sequence: 64_89.00
2024-02-04 08:27:15,695 	Gloss Reference :	A B+C+D+E
2024-02-04 08:27:15,695 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:27:15,696 	Gloss Alignment :	         
2024-02-04 08:27:15,696 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:27:15,697 	Text Reference  :	but this can  not go on amidst the rising cases human lives    need to   be safeguarded
2024-02-04 08:27:15,697 	Text Hypothesis :	*** ipl  will not ** ** ****** *** ****** ***** be    possible in   june as well       
2024-02-04 08:27:15,697 	Text Alignment  :	D   S    S        D  D  D      D   D      D     S     S        S    S    S  S          
2024-02-04 08:27:15,697 ========================================================================================================================
2024-02-04 08:27:15,697 Logging Sequence: 166_261.00
2024-02-04 08:27:15,697 	Gloss Reference :	A B+C+D+E
2024-02-04 08:27:15,698 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:27:15,698 	Gloss Alignment :	         
2024-02-04 08:27:15,698 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:27:15,699 	Text Reference  :	***** ** *** ****** ** for     all organizational matters   and the schedule
2024-02-04 08:27:15,699 	Text Hypothesis :	based on the number of matches we  are            spreading out of  them    
2024-02-04 08:27:15,699 	Text Alignment  :	I     I  I   I      I  S       S   S              S         S   S   S       
2024-02-04 08:27:15,699 ========================================================================================================================
2024-02-04 08:27:19,081 Epoch 883: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-04 08:27:19,081 EPOCH 884
2024-02-04 08:27:23,642 Epoch 884: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-04 08:27:23,642 EPOCH 885
2024-02-04 08:27:28,494 Epoch 885: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-04 08:27:28,494 EPOCH 886
2024-02-04 08:27:29,517 [Epoch: 886 Step: 00030100] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     2886 || Batch Translation Loss:   0.015892 => Txt Tokens per Sec:     7568 || Lr: 0.000100
2024-02-04 08:27:32,923 Epoch 886: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-04 08:27:32,924 EPOCH 887
2024-02-04 08:27:37,700 Epoch 887: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-04 08:27:37,700 EPOCH 888
2024-02-04 08:27:42,218 Epoch 888: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-04 08:27:42,219 EPOCH 889
2024-02-04 08:27:43,611 [Epoch: 889 Step: 00030200] Batch Recognition Loss:   0.000114 => Gls Tokens per Sec:     1841 || Batch Translation Loss:   0.018879 => Txt Tokens per Sec:     5469 || Lr: 0.000100
2024-02-04 08:27:46,899 Epoch 889: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-04 08:27:46,899 EPOCH 890
2024-02-04 08:27:51,591 Epoch 890: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-04 08:27:51,591 EPOCH 891
2024-02-04 08:27:56,130 Epoch 891: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-04 08:27:56,130 EPOCH 892
2024-02-04 08:27:56,839 [Epoch: 892 Step: 00030300] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     2710 || Batch Translation Loss:   0.016242 => Txt Tokens per Sec:     7159 || Lr: 0.000100
2024-02-04 08:28:01,119 Epoch 892: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-04 08:28:01,120 EPOCH 893
2024-02-04 08:28:05,498 Epoch 893: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-04 08:28:05,498 EPOCH 894
2024-02-04 08:28:10,255 Epoch 894: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-04 08:28:10,255 EPOCH 895
2024-02-04 08:28:10,775 [Epoch: 895 Step: 00030400] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     1988 || Batch Translation Loss:   0.043696 => Txt Tokens per Sec:     5630 || Lr: 0.000100
2024-02-04 08:28:14,774 Epoch 895: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-04 08:28:14,774 EPOCH 896
2024-02-04 08:28:19,555 Epoch 896: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-04 08:28:19,556 EPOCH 897
2024-02-04 08:28:23,888 Epoch 897: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.48 
2024-02-04 08:28:23,888 EPOCH 898
2024-02-04 08:28:24,118 [Epoch: 898 Step: 00030500] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     2803 || Batch Translation Loss:   0.027977 => Txt Tokens per Sec:     8373 || Lr: 0.000100
2024-02-04 08:28:28,649 Epoch 898: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-04 08:28:28,650 EPOCH 899
2024-02-04 08:28:33,066 Epoch 899: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.20 
2024-02-04 08:28:33,066 EPOCH 900
2024-02-04 08:28:37,949 [Epoch: 900 Step: 00030600] Batch Recognition Loss:   0.000432 => Gls Tokens per Sec:     2177 || Batch Translation Loss:   0.159497 => Txt Tokens per Sec:     6044 || Lr: 0.000100
2024-02-04 08:28:37,949 Epoch 900: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.44 
2024-02-04 08:28:37,950 EPOCH 901
2024-02-04 08:28:42,297 Epoch 901: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.55 
2024-02-04 08:28:42,298 EPOCH 902
2024-02-04 08:28:47,266 Epoch 902: Total Training Recognition Loss 0.02  Total Training Translation Loss 11.43 
2024-02-04 08:28:47,267 EPOCH 903
2024-02-04 08:28:51,951 [Epoch: 903 Step: 00030700] Batch Recognition Loss:   0.000420 => Gls Tokens per Sec:     2134 || Batch Translation Loss:   0.190574 => Txt Tokens per Sec:     5882 || Lr: 0.000100
2024-02-04 08:28:52,248 Epoch 903: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.12 
2024-02-04 08:28:52,248 EPOCH 904
2024-02-04 08:28:56,940 Epoch 904: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.93 
2024-02-04 08:28:56,940 EPOCH 905
2024-02-04 08:29:01,402 Epoch 905: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.43 
2024-02-04 08:29:01,402 EPOCH 906
2024-02-04 08:29:05,719 [Epoch: 906 Step: 00030800] Batch Recognition Loss:   0.000425 => Gls Tokens per Sec:     2225 || Batch Translation Loss:   0.062522 => Txt Tokens per Sec:     6299 || Lr: 0.000100
2024-02-04 08:29:06,140 Epoch 906: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.27 
2024-02-04 08:29:06,140 EPOCH 907
2024-02-04 08:29:10,470 Epoch 907: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.43 
2024-02-04 08:29:10,471 EPOCH 908
2024-02-04 08:29:15,257 Epoch 908: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.27 
2024-02-04 08:29:15,257 EPOCH 909
2024-02-04 08:29:18,817 [Epoch: 909 Step: 00030900] Batch Recognition Loss:   0.000367 => Gls Tokens per Sec:     2519 || Batch Translation Loss:   0.038209 => Txt Tokens per Sec:     6864 || Lr: 0.000100
2024-02-04 08:29:19,823 Epoch 909: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-04 08:29:19,823 EPOCH 910
2024-02-04 08:29:24,480 Epoch 910: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-04 08:29:24,480 EPOCH 911
2024-02-04 08:29:28,806 Epoch 911: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-04 08:29:28,807 EPOCH 912
2024-02-04 08:29:32,341 [Epoch: 912 Step: 00031000] Batch Recognition Loss:   0.000354 => Gls Tokens per Sec:     2285 || Batch Translation Loss:   0.020313 => Txt Tokens per Sec:     6295 || Lr: 0.000100
2024-02-04 08:29:33,605 Epoch 912: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-04 08:29:33,605 EPOCH 913
2024-02-04 08:29:38,147 Epoch 913: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-04 08:29:38,147 EPOCH 914
2024-02-04 08:29:42,829 Epoch 914: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-04 08:29:42,830 EPOCH 915
2024-02-04 08:29:46,141 [Epoch: 915 Step: 00031100] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:     2245 || Batch Translation Loss:   0.018051 => Txt Tokens per Sec:     6271 || Lr: 0.000100
2024-02-04 08:29:47,494 Epoch 915: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-04 08:29:47,494 EPOCH 916
2024-02-04 08:29:51,980 Epoch 916: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-04 08:29:51,980 EPOCH 917
2024-02-04 08:29:56,668 Epoch 917: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-04 08:29:56,669 EPOCH 918
2024-02-04 08:29:59,327 [Epoch: 918 Step: 00031200] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:     2555 || Batch Translation Loss:   0.023792 => Txt Tokens per Sec:     6673 || Lr: 0.000100
2024-02-04 08:30:01,507 Epoch 918: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-04 08:30:01,507 EPOCH 919
2024-02-04 08:30:06,299 Epoch 919: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-04 08:30:06,300 EPOCH 920
2024-02-04 08:30:11,304 Epoch 920: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-04 08:30:11,305 EPOCH 921
2024-02-04 08:30:13,984 [Epoch: 921 Step: 00031300] Batch Recognition Loss:   0.000121 => Gls Tokens per Sec:     2390 || Batch Translation Loss:   0.027279 => Txt Tokens per Sec:     6584 || Lr: 0.000100
2024-02-04 08:30:16,025 Epoch 921: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.41 
2024-02-04 08:30:16,025 EPOCH 922
2024-02-04 08:30:20,289 Epoch 922: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.16 
2024-02-04 08:30:20,290 EPOCH 923
2024-02-04 08:30:25,165 Epoch 923: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.52 
2024-02-04 08:30:25,165 EPOCH 924
2024-02-04 08:30:27,356 [Epoch: 924 Step: 00031400] Batch Recognition Loss:   0.000293 => Gls Tokens per Sec:     2631 || Batch Translation Loss:   0.054924 => Txt Tokens per Sec:     7262 || Lr: 0.000100
2024-02-04 08:30:29,831 Epoch 924: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.80 
2024-02-04 08:30:29,832 EPOCH 925
2024-02-04 08:30:34,409 Epoch 925: Total Training Recognition Loss 0.05  Total Training Translation Loss 6.86 
2024-02-04 08:30:34,409 EPOCH 926
2024-02-04 08:30:38,612 Epoch 926: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.53 
2024-02-04 08:30:38,613 EPOCH 927
2024-02-04 08:30:41,092 [Epoch: 927 Step: 00031500] Batch Recognition Loss:   0.000413 => Gls Tokens per Sec:     1965 || Batch Translation Loss:   0.077042 => Txt Tokens per Sec:     5475 || Lr: 0.000100
2024-02-04 08:30:43,515 Epoch 927: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.81 
2024-02-04 08:30:43,515 EPOCH 928
2024-02-04 08:30:47,867 Epoch 928: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-04 08:30:47,868 EPOCH 929
2024-02-04 08:30:52,771 Epoch 929: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-04 08:30:52,772 EPOCH 930
2024-02-04 08:30:54,572 [Epoch: 930 Step: 00031600] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:     2354 || Batch Translation Loss:   0.033707 => Txt Tokens per Sec:     6815 || Lr: 0.000100
2024-02-04 08:30:57,197 Epoch 930: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-04 08:30:57,197 EPOCH 931
2024-02-04 08:31:01,923 Epoch 931: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-04 08:31:01,924 EPOCH 932
2024-02-04 08:31:06,438 Epoch 932: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.11 
2024-02-04 08:31:06,439 EPOCH 933
2024-02-04 08:31:08,147 [Epoch: 933 Step: 00031700] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     2102 || Batch Translation Loss:   0.022789 => Txt Tokens per Sec:     5880 || Lr: 0.000100
2024-02-04 08:31:11,259 Epoch 933: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-04 08:31:11,260 EPOCH 934
2024-02-04 08:31:16,092 Epoch 934: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-04 08:31:16,092 EPOCH 935
2024-02-04 08:31:20,769 Epoch 935: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-04 08:31:20,770 EPOCH 936
2024-02-04 08:31:22,218 [Epoch: 936 Step: 00031800] Batch Recognition Loss:   0.000192 => Gls Tokens per Sec:     2213 || Batch Translation Loss:   0.032787 => Txt Tokens per Sec:     6242 || Lr: 0.000100
2024-02-04 08:31:25,639 Epoch 936: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-04 08:31:25,640 EPOCH 937
2024-02-04 08:31:29,819 Epoch 937: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-04 08:31:29,819 EPOCH 938
2024-02-04 08:31:34,662 Epoch 938: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-04 08:31:34,662 EPOCH 939
2024-02-04 08:31:35,668 [Epoch: 939 Step: 00031900] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:     2301 || Batch Translation Loss:   0.022004 => Txt Tokens per Sec:     6139 || Lr: 0.000100
2024-02-04 08:31:39,388 Epoch 939: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.58 
2024-02-04 08:31:39,388 EPOCH 940
2024-02-04 08:31:43,768 Epoch 940: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-04 08:31:43,769 EPOCH 941
2024-02-04 08:31:48,515 Epoch 941: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-04 08:31:48,516 EPOCH 942
2024-02-04 08:31:49,368 [Epoch: 942 Step: 00032000] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     2256 || Batch Translation Loss:   0.051339 => Txt Tokens per Sec:     7042 || Lr: 0.000100
2024-02-04 08:31:58,103 Validation result at epoch 942, step    32000: duration: 8.7344s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00065	Translation Loss: 94033.11719	PPL: 12208.63477
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.57	(BLEU-1: 10.58,	BLEU-2: 3.11,	BLEU-3: 1.23,	BLEU-4: 0.57)
	CHRF 16.85	ROUGE 8.94
2024-02-04 08:31:58,104 Logging Recognition and Translation Outputs
2024-02-04 08:31:58,105 ========================================================================================================================
2024-02-04 08:31:58,105 Logging Sequence: 86_11.00
2024-02-04 08:31:58,105 	Gloss Reference :	A B+C+D+E
2024-02-04 08:31:58,105 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:31:58,106 	Gloss Alignment :	         
2024-02-04 08:31:58,106 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:31:58,106 	Text Reference  :	he was 66 years     old   
2024-02-04 08:31:58,106 	Text Hypothesis :	he was a  brilliant player
2024-02-04 08:31:58,106 	Text Alignment  :	       S  S         S     
2024-02-04 08:31:58,107 ========================================================================================================================
2024-02-04 08:31:58,107 Logging Sequence: 67_16.00
2024-02-04 08:31:58,107 	Gloss Reference :	A B+C+D+E
2024-02-04 08:31:58,107 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:31:58,107 	Gloss Alignment :	         
2024-02-04 08:31:58,107 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:31:58,108 	Text Reference  :	***** *** **** to      help   india's fight against  the     covid-19 pandemic
2024-02-04 08:31:58,108 	Text Hypothesis :	there are many batsmen posted a       huge  argument between their    dhoni   
2024-02-04 08:31:58,108 	Text Alignment  :	I     I   I    S       S      S       S     S        S       S        S       
2024-02-04 08:31:58,108 ========================================================================================================================
2024-02-04 08:31:58,108 Logging Sequence: 69_177.00
2024-02-04 08:31:58,109 	Gloss Reference :	A B+C+D+E
2024-02-04 08:31:58,109 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:31:58,109 	Gloss Alignment :	         
2024-02-04 08:31:58,109 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:31:58,110 	Text Reference  :	he said 'i will continue playing i know it's about time i   retire i   also have a     knee    condition
2024-02-04 08:31:58,110 	Text Hypothesis :	** **** ** **** ******** ******* * **** when csk   came out to     bat ipl  the  first against england  
2024-02-04 08:31:58,110 	Text Alignment  :	D  D    D  D    D        D       D D    S    S     S    S   S      S   S    S    S     S       S        
2024-02-04 08:31:58,111 ========================================================================================================================
2024-02-04 08:31:58,111 Logging Sequence: 165_615.00
2024-02-04 08:31:58,111 	Gloss Reference :	A B+C+D+E
2024-02-04 08:31:58,111 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:31:58,111 	Gloss Alignment :	         
2024-02-04 08:31:58,111 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:31:58,112 	Text Reference  :	** ** ********* ***** *** we  defeated pakistan too   
2024-02-04 08:31:58,112 	Text Hypothesis :	it is currently known for the age      of       people
2024-02-04 08:31:58,112 	Text Alignment  :	I  I  I         I     I   S   S        S        S     
2024-02-04 08:31:58,112 ========================================================================================================================
2024-02-04 08:31:58,112 Logging Sequence: 61_5.00
2024-02-04 08:31:58,112 	Gloss Reference :	A B+C+D+E
2024-02-04 08:31:58,112 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:31:58,112 	Gloss Alignment :	         
2024-02-04 08:31:58,113 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:31:58,114 	Text Reference  :	they    rivalry is ** *** seen the **** most   during india pakistan cricket matches
2024-02-04 08:31:58,114 	Text Hypothesis :	however there   is no one day  the huge demand to     see   him      as      well   
2024-02-04 08:31:58,114 	Text Alignment  :	S       S          I  I   S        I    S      S      S     S        S       S      
2024-02-04 08:31:58,114 ========================================================================================================================
2024-02-04 08:32:02,165 Epoch 942: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.38 
2024-02-04 08:32:02,166 EPOCH 943
2024-02-04 08:32:06,770 Epoch 943: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.34 
2024-02-04 08:32:06,770 EPOCH 944
2024-02-04 08:32:11,437 Epoch 944: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.20 
2024-02-04 08:32:11,437 EPOCH 945
2024-02-04 08:32:11,948 [Epoch: 945 Step: 00032100] Batch Recognition Loss:   0.000297 => Gls Tokens per Sec:     2514 || Batch Translation Loss:   0.034466 => Txt Tokens per Sec:     7066 || Lr: 0.000100
2024-02-04 08:32:15,994 Epoch 945: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.42 
2024-02-04 08:32:15,995 EPOCH 946
2024-02-04 08:32:20,740 Epoch 946: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.21 
2024-02-04 08:32:20,740 EPOCH 947
2024-02-04 08:32:25,144 Epoch 947: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.98 
2024-02-04 08:32:25,144 EPOCH 948
2024-02-04 08:32:25,476 [Epoch: 948 Step: 00032200] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     1178 || Batch Translation Loss:   0.007364 => Txt Tokens per Sec:     4191 || Lr: 0.000100
2024-02-04 08:32:30,097 Epoch 948: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.20 
2024-02-04 08:32:30,097 EPOCH 949
2024-02-04 08:32:34,353 Epoch 949: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.15 
2024-02-04 08:32:34,354 EPOCH 950
2024-02-04 08:32:39,043 [Epoch: 950 Step: 00032300] Batch Recognition Loss:   0.000452 => Gls Tokens per Sec:     2268 || Batch Translation Loss:   0.112594 => Txt Tokens per Sec:     6296 || Lr: 0.000100
2024-02-04 08:32:39,043 Epoch 950: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.24 
2024-02-04 08:32:39,044 EPOCH 951
2024-02-04 08:32:43,584 Epoch 951: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.88 
2024-02-04 08:32:43,584 EPOCH 952
2024-02-04 08:32:48,420 Epoch 952: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.16 
2024-02-04 08:32:48,420 EPOCH 953
2024-02-04 08:32:52,656 [Epoch: 953 Step: 00032400] Batch Recognition Loss:   0.000632 => Gls Tokens per Sec:     2359 || Batch Translation Loss:   0.144703 => Txt Tokens per Sec:     6625 || Lr: 0.000100
2024-02-04 08:32:52,810 Epoch 953: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.42 
2024-02-04 08:32:52,810 EPOCH 954
2024-02-04 08:32:56,950 Epoch 954: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.76 
2024-02-04 08:32:56,951 EPOCH 955
2024-02-04 08:33:01,021 Epoch 955: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.36 
2024-02-04 08:33:01,022 EPOCH 956
2024-02-04 08:33:04,308 [Epoch: 956 Step: 00032500] Batch Recognition Loss:   0.000234 => Gls Tokens per Sec:     2846 || Batch Translation Loss:   0.049563 => Txt Tokens per Sec:     7787 || Lr: 0.000100
2024-02-04 08:33:05,064 Epoch 956: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-04 08:33:05,064 EPOCH 957
2024-02-04 08:33:09,818 Epoch 957: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-04 08:33:09,818 EPOCH 958
2024-02-04 08:33:14,215 Epoch 958: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-04 08:33:14,216 EPOCH 959
2024-02-04 08:33:18,250 [Epoch: 959 Step: 00032600] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:     2222 || Batch Translation Loss:   0.021562 => Txt Tokens per Sec:     6080 || Lr: 0.000100
2024-02-04 08:33:19,090 Epoch 959: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-04 08:33:19,091 EPOCH 960
2024-02-04 08:33:23,471 Epoch 960: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-04 08:33:23,472 EPOCH 961
2024-02-04 08:33:28,488 Epoch 961: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-04 08:33:28,488 EPOCH 962
2024-02-04 08:33:31,726 [Epoch: 962 Step: 00032700] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:     2571 || Batch Translation Loss:   0.015324 => Txt Tokens per Sec:     7296 || Lr: 0.000100
2024-02-04 08:33:32,708 Epoch 962: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-04 08:33:32,708 EPOCH 963
2024-02-04 08:33:37,537 Epoch 963: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-04 08:33:37,538 EPOCH 964
2024-02-04 08:33:41,887 Epoch 964: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-04 08:33:41,887 EPOCH 965
2024-02-04 08:33:45,312 [Epoch: 965 Step: 00032800] Batch Recognition Loss:   0.000209 => Gls Tokens per Sec:     2171 || Batch Translation Loss:   0.015949 => Txt Tokens per Sec:     5946 || Lr: 0.000100
2024-02-04 08:33:46,776 Epoch 965: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-04 08:33:46,777 EPOCH 966
2024-02-04 08:33:51,114 Epoch 966: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-04 08:33:51,115 EPOCH 967
2024-02-04 08:33:55,906 Epoch 967: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-04 08:33:55,906 EPOCH 968
2024-02-04 08:33:59,025 [Epoch: 968 Step: 00032900] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:     2177 || Batch Translation Loss:   0.015222 => Txt Tokens per Sec:     6183 || Lr: 0.000100
2024-02-04 08:34:00,342 Epoch 968: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-04 08:34:00,342 EPOCH 969
2024-02-04 08:34:05,102 Epoch 969: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-04 08:34:05,103 EPOCH 970
2024-02-04 08:34:09,493 Epoch 970: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.43 
2024-02-04 08:34:09,493 EPOCH 971
2024-02-04 08:34:11,893 [Epoch: 971 Step: 00033000] Batch Recognition Loss:   0.000232 => Gls Tokens per Sec:     2564 || Batch Translation Loss:   0.045884 => Txt Tokens per Sec:     7349 || Lr: 0.000100
2024-02-04 08:34:13,623 Epoch 971: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-04 08:34:13,623 EPOCH 972
2024-02-04 08:34:18,291 Epoch 972: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-04 08:34:18,292 EPOCH 973
2024-02-04 08:34:22,853 Epoch 973: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.55 
2024-02-04 08:34:22,853 EPOCH 974
2024-02-04 08:34:25,288 [Epoch: 974 Step: 00033100] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:     2264 || Batch Translation Loss:   0.118450 => Txt Tokens per Sec:     6266 || Lr: 0.000100
2024-02-04 08:34:27,503 Epoch 974: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.81 
2024-02-04 08:34:27,504 EPOCH 975
2024-02-04 08:34:31,979 Epoch 975: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.37 
2024-02-04 08:34:31,979 EPOCH 976
2024-02-04 08:34:36,746 Epoch 976: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.00 
2024-02-04 08:34:36,746 EPOCH 977
2024-02-04 08:34:38,409 [Epoch: 977 Step: 00033200] Batch Recognition Loss:   0.000271 => Gls Tokens per Sec:     3081 || Batch Translation Loss:   0.038014 => Txt Tokens per Sec:     8132 || Lr: 0.000100
2024-02-04 08:34:41,173 Epoch 977: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.44 
2024-02-04 08:34:41,173 EPOCH 978
2024-02-04 08:34:46,041 Epoch 978: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.21 
2024-02-04 08:34:46,042 EPOCH 979
2024-02-04 08:34:50,269 Epoch 979: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.13 
2024-02-04 08:34:50,269 EPOCH 980
2024-02-04 08:34:52,029 [Epoch: 980 Step: 00033300] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:     2404 || Batch Translation Loss:   0.089218 => Txt Tokens per Sec:     6663 || Lr: 0.000100
2024-02-04 08:34:55,137 Epoch 980: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.07 
2024-02-04 08:34:55,138 EPOCH 981
2024-02-04 08:34:59,538 Epoch 981: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.94 
2024-02-04 08:34:59,538 EPOCH 982
2024-02-04 08:35:04,438 Epoch 982: Total Training Recognition Loss 0.01  Total Training Translation Loss 6.84 
2024-02-04 08:35:04,439 EPOCH 983
2024-02-04 08:35:05,989 [Epoch: 983 Step: 00033400] Batch Recognition Loss:   0.000640 => Gls Tokens per Sec:     2317 || Batch Translation Loss:   0.116355 => Txt Tokens per Sec:     6181 || Lr: 0.000100
2024-02-04 08:35:08,820 Epoch 983: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.64 
2024-02-04 08:35:08,821 EPOCH 984
2024-02-04 08:35:13,718 Epoch 984: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.56 
2024-02-04 08:35:13,719 EPOCH 985
2024-02-04 08:35:18,092 Epoch 985: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.13 
2024-02-04 08:35:18,092 EPOCH 986
2024-02-04 08:35:19,643 [Epoch: 986 Step: 00033500] Batch Recognition Loss:   0.000489 => Gls Tokens per Sec:     2064 || Batch Translation Loss:   0.056707 => Txt Tokens per Sec:     5685 || Lr: 0.000100
2024-02-04 08:35:22,997 Epoch 986: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.52 
2024-02-04 08:35:22,997 EPOCH 987
2024-02-04 08:35:27,495 Epoch 987: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-04 08:35:27,496 EPOCH 988
2024-02-04 08:35:32,237 Epoch 988: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-04 08:35:32,237 EPOCH 989
2024-02-04 08:35:33,338 [Epoch: 989 Step: 00033600] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     2328 || Batch Translation Loss:   0.018092 => Txt Tokens per Sec:     6989 || Lr: 0.000100
2024-02-04 08:35:36,938 Epoch 989: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-04 08:35:36,938 EPOCH 990
2024-02-04 08:35:41,421 Epoch 990: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-04 08:35:41,421 EPOCH 991
2024-02-04 08:35:45,659 Epoch 991: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-04 08:35:45,660 EPOCH 992
2024-02-04 08:35:46,482 [Epoch: 992 Step: 00033700] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:     2341 || Batch Translation Loss:   0.015112 => Txt Tokens per Sec:     6439 || Lr: 0.000100
2024-02-04 08:35:50,589 Epoch 992: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-04 08:35:50,590 EPOCH 993
2024-02-04 08:35:54,911 Epoch 993: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-04 08:35:54,912 EPOCH 994
2024-02-04 08:35:59,654 Epoch 994: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-04 08:35:59,654 EPOCH 995
2024-02-04 08:36:00,028 [Epoch: 995 Step: 00033800] Batch Recognition Loss:   0.000125 => Gls Tokens per Sec:     3425 || Batch Translation Loss:   0.010706 => Txt Tokens per Sec:     8205 || Lr: 0.000100
2024-02-04 08:36:03,867 Epoch 995: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-04 08:36:03,867 EPOCH 996
2024-02-04 08:36:08,729 Epoch 996: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-04 08:36:08,730 EPOCH 997
2024-02-04 08:36:13,206 Epoch 997: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-04 08:36:13,206 EPOCH 998
2024-02-04 08:36:13,443 [Epoch: 998 Step: 00033900] Batch Recognition Loss:   0.000114 => Gls Tokens per Sec:     2712 || Batch Translation Loss:   0.393584 => Txt Tokens per Sec:     6470 || Lr: 0.000100
2024-02-04 08:36:17,944 Epoch 998: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-04 08:36:17,944 EPOCH 999
2024-02-04 08:36:22,373 Epoch 999: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.31 
2024-02-04 08:36:22,374 EPOCH 1000
2024-02-04 08:36:27,129 [Epoch: 1000 Step: 00034000] Batch Recognition Loss:   0.000416 => Gls Tokens per Sec:     2236 || Batch Translation Loss:   0.011191 => Txt Tokens per Sec:     6207 || Lr: 0.000100
2024-02-04 08:36:35,590 Validation result at epoch 1000, step    34000: duration: 8.4606s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00076	Translation Loss: 94438.60938	PPL: 12714.22461
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.83	(BLEU-1: 10.57,	BLEU-2: 3.24,	BLEU-3: 1.48,	BLEU-4: 0.83)
	CHRF 16.68	ROUGE 9.17
2024-02-04 08:36:35,591 Logging Recognition and Translation Outputs
2024-02-04 08:36:35,591 ========================================================================================================================
2024-02-04 08:36:35,591 Logging Sequence: 92_199.00
2024-02-04 08:36:35,592 	Gloss Reference :	A B+C+D+E
2024-02-04 08:36:35,592 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:36:35,592 	Gloss Alignment :	         
2024-02-04 08:36:35,592 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:36:35,593 	Text Reference  :	*** ******** *** people on     social media said that 
2024-02-04 08:36:35,593 	Text Hypothesis :	and shocking are the    second team   won   the  match
2024-02-04 08:36:35,593 	Text Alignment  :	I   I        I   S      S      S      S     S    S    
2024-02-04 08:36:35,593 ========================================================================================================================
2024-02-04 08:36:35,593 Logging Sequence: 109_64.00
2024-02-04 08:36:35,593 	Gloss Reference :	A B+C+D+E
2024-02-04 08:36:35,594 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:36:35,594 	Gloss Alignment :	         
2024-02-04 08:36:35,594 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:36:35,595 	Text Reference  :	** the **** 2 players as well as    the ***** entire kkr team have        been    quarantined
2024-02-04 08:36:35,595 	Text Hypothesis :	in the next 9 months  of the  match the super kings  ceo kasi viswanathan bowling coach      
2024-02-04 08:36:35,596 	Text Alignment  :	I      I    S S       S  S    S         I     S      S   S    S           S       S          
2024-02-04 08:36:35,596 ========================================================================================================================
2024-02-04 08:36:35,596 Logging Sequence: 84_108.00
2024-02-04 08:36:35,596 	Gloss Reference :	A B+C+D+E
2024-02-04 08:36:35,596 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:36:35,596 	Gloss Alignment :	         
2024-02-04 08:36:35,596 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:36:35,597 	Text Reference  :	so in order to show their protest they covered their mouth in the photos which then went viral
2024-02-04 08:36:35,597 	Text Hypothesis :	** ** ***** ** **** ***** ******* **** ******* ***** ***** ** the ****** ***** news went viral
2024-02-04 08:36:35,597 	Text Alignment  :	D  D  D     D  D    D     D       D    D       D     D     D      D      D     S              
2024-02-04 08:36:35,597 ========================================================================================================================
2024-02-04 08:36:35,597 Logging Sequence: 115_24.00
2024-02-04 08:36:35,598 	Gloss Reference :	A B+C+D+E
2024-02-04 08:36:35,598 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:36:35,598 	Gloss Alignment :	         
2024-02-04 08:36:35,598 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:36:35,599 	Text Reference  :	** bumrah  also did not   participate in   the 5      match t20 series  
2024-02-04 08:36:35,599 	Text Hypothesis :	as neither of   the teams could       when he  always at    the ceremony
2024-02-04 08:36:35,599 	Text Alignment  :	I  S       S    S   S     S           S    S   S      S     S   S       
2024-02-04 08:36:35,599 ========================================================================================================================
2024-02-04 08:36:35,599 Logging Sequence: 96_129.00
2024-02-04 08:36:35,600 	Gloss Reference :	A B+C+D+E
2024-02-04 08:36:35,600 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:36:35,600 	Gloss Alignment :	         
2024-02-04 08:36:35,600 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:36:35,600 	Text Reference  :	******** ***** viewers were    very  stressed
2024-02-04 08:36:35,600 	Text Hypothesis :	everyone hoped that    england would win     
2024-02-04 08:36:35,601 	Text Alignment  :	I        I     S       S       S     S       
2024-02-04 08:36:35,601 ========================================================================================================================
2024-02-04 08:36:35,604 Epoch 1000: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.80 
2024-02-04 08:36:35,604 EPOCH 1001
2024-02-04 08:36:40,657 Epoch 1001: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.29 
2024-02-04 08:36:40,658 EPOCH 1002
2024-02-04 08:36:45,192 Epoch 1002: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.70 
2024-02-04 08:36:45,192 EPOCH 1003
2024-02-04 08:36:49,589 [Epoch: 1003 Step: 00034100] Batch Recognition Loss:   0.000319 => Gls Tokens per Sec:     2273 || Batch Translation Loss:   0.030676 => Txt Tokens per Sec:     6257 || Lr: 0.000100
2024-02-04 08:36:49,918 Epoch 1003: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.34 
2024-02-04 08:36:49,918 EPOCH 1004
2024-02-04 08:36:54,293 Epoch 1004: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.15 
2024-02-04 08:36:54,293 EPOCH 1005
2024-02-04 08:36:59,077 Epoch 1005: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.48 
2024-02-04 08:36:59,077 EPOCH 1006
2024-02-04 08:37:02,851 [Epoch: 1006 Step: 00034200] Batch Recognition Loss:   0.000309 => Gls Tokens per Sec:     2478 || Batch Translation Loss:   0.193701 => Txt Tokens per Sec:     6950 || Lr: 0.000100
2024-02-04 08:37:03,527 Epoch 1006: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.26 
2024-02-04 08:37:03,527 EPOCH 1007
2024-02-04 08:37:08,521 Epoch 1007: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.51 
2024-02-04 08:37:08,522 EPOCH 1008
2024-02-04 08:37:12,933 Epoch 1008: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.25 
2024-02-04 08:37:12,934 EPOCH 1009
2024-02-04 08:37:17,252 [Epoch: 1009 Step: 00034300] Batch Recognition Loss:   0.000280 => Gls Tokens per Sec:     2076 || Batch Translation Loss:   0.126895 => Txt Tokens per Sec:     5977 || Lr: 0.000100
2024-02-04 08:37:17,846 Epoch 1009: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.44 
2024-02-04 08:37:17,847 EPOCH 1010
2024-02-04 08:37:22,018 Epoch 1010: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.42 
2024-02-04 08:37:22,018 EPOCH 1011
2024-02-04 08:37:26,907 Epoch 1011: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.36 
2024-02-04 08:37:26,907 EPOCH 1012
2024-02-04 08:37:30,113 [Epoch: 1012 Step: 00034400] Batch Recognition Loss:   0.000323 => Gls Tokens per Sec:     2518 || Batch Translation Loss:   0.056600 => Txt Tokens per Sec:     6859 || Lr: 0.000100
2024-02-04 08:37:31,222 Epoch 1012: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.26 
2024-02-04 08:37:31,222 EPOCH 1013
2024-02-04 08:37:36,131 Epoch 1013: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.49 
2024-02-04 08:37:36,132 EPOCH 1014
2024-02-04 08:37:40,463 Epoch 1014: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.78 
2024-02-04 08:37:40,464 EPOCH 1015
2024-02-04 08:37:44,260 [Epoch: 1015 Step: 00034500] Batch Recognition Loss:   0.000615 => Gls Tokens per Sec:     2024 || Batch Translation Loss:   0.257035 => Txt Tokens per Sec:     5650 || Lr: 0.000100
2024-02-04 08:37:45,593 Epoch 1015: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.35 
2024-02-04 08:37:45,594 EPOCH 1016
2024-02-04 08:37:50,157 Epoch 1016: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.57 
2024-02-04 08:37:50,157 EPOCH 1017
2024-02-04 08:37:54,925 Epoch 1017: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.27 
2024-02-04 08:37:54,926 EPOCH 1018
2024-02-04 08:37:57,715 [Epoch: 1018 Step: 00034600] Batch Recognition Loss:   0.000364 => Gls Tokens per Sec:     2525 || Batch Translation Loss:   0.025360 => Txt Tokens per Sec:     6920 || Lr: 0.000100
2024-02-04 08:37:59,261 Epoch 1018: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-04 08:37:59,262 EPOCH 1019
2024-02-04 08:38:04,149 Epoch 1019: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-04 08:38:04,149 EPOCH 1020
2024-02-04 08:38:08,500 Epoch 1020: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-04 08:38:08,500 EPOCH 1021
2024-02-04 08:38:11,540 [Epoch: 1021 Step: 00034700] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:     2024 || Batch Translation Loss:   0.031250 => Txt Tokens per Sec:     5605 || Lr: 0.000100
2024-02-04 08:38:13,438 Epoch 1021: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-04 08:38:13,439 EPOCH 1022
2024-02-04 08:38:17,818 Epoch 1022: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-04 08:38:17,819 EPOCH 1023
2024-02-04 08:38:22,673 Epoch 1023: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-04 08:38:22,673 EPOCH 1024
2024-02-04 08:38:25,002 [Epoch: 1024 Step: 00034800] Batch Recognition Loss:   0.000157 => Gls Tokens per Sec:     2366 || Batch Translation Loss:   0.017335 => Txt Tokens per Sec:     7017 || Lr: 0.000100
2024-02-04 08:38:27,084 Epoch 1024: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-04 08:38:27,085 EPOCH 1025
2024-02-04 08:38:31,902 Epoch 1025: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-04 08:38:31,902 EPOCH 1026
2024-02-04 08:38:35,991 Epoch 1026: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-04 08:38:35,991 EPOCH 1027
2024-02-04 08:38:38,110 [Epoch: 1027 Step: 00034900] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:     2299 || Batch Translation Loss:   0.014527 => Txt Tokens per Sec:     6116 || Lr: 0.000100
2024-02-04 08:38:40,870 Epoch 1027: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-04 08:38:40,870 EPOCH 1028
2024-02-04 08:38:45,177 Epoch 1028: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-04 08:38:45,177 EPOCH 1029
2024-02-04 08:38:50,096 Epoch 1029: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.51 
2024-02-04 08:38:50,096 EPOCH 1030
2024-02-04 08:38:51,529 [Epoch: 1030 Step: 00035000] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     2956 || Batch Translation Loss:   0.012478 => Txt Tokens per Sec:     7722 || Lr: 0.000100
2024-02-04 08:38:54,462 Epoch 1030: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.65 
2024-02-04 08:38:54,463 EPOCH 1031
2024-02-04 08:38:59,380 Epoch 1031: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.34 
2024-02-04 08:38:59,380 EPOCH 1032
2024-02-04 08:39:03,765 Epoch 1032: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-04 08:39:03,766 EPOCH 1033
2024-02-04 08:39:05,466 [Epoch: 1033 Step: 00035100] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:     2112 || Batch Translation Loss:   0.025281 => Txt Tokens per Sec:     5964 || Lr: 0.000100
2024-02-04 08:39:08,468 Epoch 1033: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.11 
2024-02-04 08:39:08,468 EPOCH 1034
2024-02-04 08:39:12,993 Epoch 1034: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.69 
2024-02-04 08:39:12,994 EPOCH 1035
2024-02-04 08:39:17,613 Epoch 1035: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.40 
2024-02-04 08:39:17,613 EPOCH 1036
2024-02-04 08:39:18,752 [Epoch: 1036 Step: 00035200] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     2811 || Batch Translation Loss:   0.110828 => Txt Tokens per Sec:     7293 || Lr: 0.000100
2024-02-04 08:39:22,271 Epoch 1036: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.39 
2024-02-04 08:39:22,272 EPOCH 1037
2024-02-04 08:39:26,832 Epoch 1037: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-04 08:39:26,832 EPOCH 1038
2024-02-04 08:39:31,488 Epoch 1038: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-04 08:39:31,488 EPOCH 1039
2024-02-04 08:39:32,485 [Epoch: 1039 Step: 00035300] Batch Recognition Loss:   0.000191 => Gls Tokens per Sec:     2324 || Batch Translation Loss:   0.014781 => Txt Tokens per Sec:     6337 || Lr: 0.000100
2024-02-04 08:39:35,971 Epoch 1039: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-04 08:39:35,971 EPOCH 1040
2024-02-04 08:39:40,866 Epoch 1040: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-04 08:39:40,867 EPOCH 1041
2024-02-04 08:39:45,196 Epoch 1041: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.51 
2024-02-04 08:39:45,196 EPOCH 1042
2024-02-04 08:39:45,836 [Epoch: 1042 Step: 00035400] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:     3008 || Batch Translation Loss:   0.101372 => Txt Tokens per Sec:     8085 || Lr: 0.000100
2024-02-04 08:39:49,947 Epoch 1042: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.45 
2024-02-04 08:39:49,948 EPOCH 1043
2024-02-04 08:39:54,386 Epoch 1043: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.80 
2024-02-04 08:39:54,387 EPOCH 1044
2024-02-04 08:39:59,215 Epoch 1044: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.76 
2024-02-04 08:39:59,216 EPOCH 1045
2024-02-04 08:39:59,594 [Epoch: 1045 Step: 00035500] Batch Recognition Loss:   0.000246 => Gls Tokens per Sec:     3395 || Batch Translation Loss:   0.066583 => Txt Tokens per Sec:     8297 || Lr: 0.000100
2024-02-04 08:40:03,616 Epoch 1045: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.47 
2024-02-04 08:40:03,616 EPOCH 1046
2024-02-04 08:40:08,511 Epoch 1046: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.06 
2024-02-04 08:40:08,512 EPOCH 1047
2024-02-04 08:40:12,847 Epoch 1047: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.46 
2024-02-04 08:40:12,847 EPOCH 1048
2024-02-04 08:40:13,038 [Epoch: 1048 Step: 00035600] Batch Recognition Loss:   0.000451 => Gls Tokens per Sec:     3368 || Batch Translation Loss:   0.288279 => Txt Tokens per Sec:     7837 || Lr: 0.000100
2024-02-04 08:40:18,081 Epoch 1048: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.59 
2024-02-04 08:40:18,081 EPOCH 1049
2024-02-04 08:40:22,585 Epoch 1049: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.92 
2024-02-04 08:40:22,585 EPOCH 1050
2024-02-04 08:40:27,354 [Epoch: 1050 Step: 00035700] Batch Recognition Loss:   0.000234 => Gls Tokens per Sec:     2229 || Batch Translation Loss:   0.044146 => Txt Tokens per Sec:     6189 || Lr: 0.000100
2024-02-04 08:40:27,355 Epoch 1050: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.54 
2024-02-04 08:40:27,355 EPOCH 1051
2024-02-04 08:40:31,756 Epoch 1051: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.90 
2024-02-04 08:40:31,757 EPOCH 1052
2024-02-04 08:40:36,541 Epoch 1052: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-04 08:40:36,542 EPOCH 1053
2024-02-04 08:40:40,461 [Epoch: 1053 Step: 00035800] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     2551 || Batch Translation Loss:   0.019336 => Txt Tokens per Sec:     7078 || Lr: 0.000100
2024-02-04 08:40:40,918 Epoch 1053: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-04 08:40:40,918 EPOCH 1054
2024-02-04 08:40:45,794 Epoch 1054: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-04 08:40:45,795 EPOCH 1055
2024-02-04 08:40:50,042 Epoch 1055: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-04 08:40:50,042 EPOCH 1056
2024-02-04 08:40:54,505 [Epoch: 1056 Step: 00035900] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     2096 || Batch Translation Loss:   0.025427 => Txt Tokens per Sec:     5851 || Lr: 0.000100
2024-02-04 08:40:54,950 Epoch 1056: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.48 
2024-02-04 08:40:54,950 EPOCH 1057
2024-02-04 08:40:59,236 Epoch 1057: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-04 08:40:59,236 EPOCH 1058
2024-02-04 08:41:04,167 Epoch 1058: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-04 08:41:04,168 EPOCH 1059
2024-02-04 08:41:07,891 [Epoch: 1059 Step: 00036000] Batch Recognition Loss:   0.000209 => Gls Tokens per Sec:     2340 || Batch Translation Loss:   0.023091 => Txt Tokens per Sec:     6708 || Lr: 0.000100
2024-02-04 08:41:16,395 Validation result at epoch 1059, step    36000: duration: 8.5032s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00088	Translation Loss: 94728.30469	PPL: 13088.20117
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.83	(BLEU-1: 11.57,	BLEU-2: 3.69,	BLEU-3: 1.56,	BLEU-4: 0.83)
	CHRF 17.24	ROUGE 9.70
2024-02-04 08:41:16,396 Logging Recognition and Translation Outputs
2024-02-04 08:41:16,396 ========================================================================================================================
2024-02-04 08:41:16,396 Logging Sequence: 78_198.00
2024-02-04 08:41:16,396 	Gloss Reference :	A B+C+D+E
2024-02-04 08:41:16,397 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:41:16,397 	Gloss Alignment :	         
2024-02-04 08:41:16,397 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:41:16,398 	Text Reference  :	******* **** ** **** *** ****** ** ****** *** they    have been         flooded with congratulations comments
2024-02-04 08:41:16,398 	Text Hypothesis :	england were to host the finals of cities for players who  disassociate himself from the             builder 
2024-02-04 08:41:16,398 	Text Alignment  :	I       I    I  I    I   I      I  I      I   S       S    S            S       S    S               S       
2024-02-04 08:41:16,398 ========================================================================================================================
2024-02-04 08:41:16,398 Logging Sequence: 145_216.00
2024-02-04 08:41:16,399 	Gloss Reference :	A B+C+D+E
2024-02-04 08:41:16,399 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:41:16,399 	Gloss Alignment :	         
2024-02-04 08:41:16,399 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:41:16,400 	Text Reference  :	asking him to include sameeha in *** the    world championship as  she was a talented athlete    
2024-02-04 08:41:16,400 	Text Hypothesis :	****** *** ** ******* ******* in his series went  viral        and t20 was a severe   competition
2024-02-04 08:41:16,400 	Text Alignment  :	D      D   D  D       D          I   S      S     S            S   S         S        S          
2024-02-04 08:41:16,401 ========================================================================================================================
2024-02-04 08:41:16,401 Logging Sequence: 70_137.00
2024-02-04 08:41:16,401 	Gloss Reference :	A B+C+D+E
2024-02-04 08:41:16,401 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:41:16,401 	Gloss Alignment :	         
2024-02-04 08:41:16,401 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:41:16,403 	Text Reference  :	the     small gesture appeared to    encourage people to    drink water  instead of   aerated drinks 
2024-02-04 08:41:16,403 	Text Hypothesis :	however even  though  the      tokyo olympics  are    angry at    jantar mantar  with thier   demands
2024-02-04 08:41:16,403 	Text Alignment  :	S       S     S       S        S     S         S      S     S     S      S       S    S       S      
2024-02-04 08:41:16,403 ========================================================================================================================
2024-02-04 08:41:16,403 Logging Sequence: 119_20.00
2024-02-04 08:41:16,403 	Gloss Reference :	A B+C+D+E
2024-02-04 08:41:16,403 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:41:16,403 	Gloss Alignment :	         
2024-02-04 08:41:16,404 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:41:16,405 	Text Reference  :	messi intended to gift something to       all      the   players and the    staff to      special to celebrate the moment
2024-02-04 08:41:16,405 	Text Hypothesis :	***** ******** ** **** the       iphones' combined worth is      eur 175000 which roughly amounts to rs        173 crore 
2024-02-04 08:41:16,405 	Text Alignment  :	D     D        D  D    S         S        S        S     S       S   S      S     S       S          S         S   S     
2024-02-04 08:41:16,405 ========================================================================================================================
2024-02-04 08:41:16,406 Logging Sequence: 106_15.00
2024-02-04 08:41:16,406 	Gloss Reference :	A B+C+D+E
2024-02-04 08:41:16,406 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:41:16,406 	Gloss Alignment :	         
2024-02-04 08:41:16,406 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:41:16,407 	Text Reference  :	but what about women's cricket   earlier we   never spoke about it **** ******** ** ** ********
2024-02-04 08:41:16,408 	Text Hypothesis :	*** and  then  the     wrestlers were    left the   field and   it will continue to be argument
2024-02-04 08:41:16,408 	Text Alignment  :	D   S    S     S       S         S       S    S     S     S        I    I        I  I  I       
2024-02-04 08:41:16,408 ========================================================================================================================
2024-02-04 08:41:17,118 Epoch 1059: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.22 
2024-02-04 08:41:17,118 EPOCH 1060
2024-02-04 08:41:22,131 Epoch 1060: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-04 08:41:22,131 EPOCH 1061
2024-02-04 08:41:26,487 Epoch 1061: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-04 08:41:26,487 EPOCH 1062
2024-02-04 08:41:30,131 [Epoch: 1062 Step: 00036100] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     2284 || Batch Translation Loss:   0.013852 => Txt Tokens per Sec:     6415 || Lr: 0.000050
2024-02-04 08:41:31,319 Epoch 1062: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-04 08:41:31,319 EPOCH 1063
2024-02-04 08:41:35,790 Epoch 1063: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-04 08:41:35,790 EPOCH 1064
2024-02-04 08:41:40,575 Epoch 1064: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-04 08:41:40,575 EPOCH 1065
2024-02-04 08:41:43,470 [Epoch: 1065 Step: 00036200] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     2654 || Batch Translation Loss:   0.009996 => Txt Tokens per Sec:     7353 || Lr: 0.000050
2024-02-04 08:41:45,222 Epoch 1065: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-04 08:41:45,222 EPOCH 1066
2024-02-04 08:41:50,292 Epoch 1066: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-04 08:41:50,293 EPOCH 1067
2024-02-04 08:41:54,751 Epoch 1067: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-04 08:41:54,751 EPOCH 1068
2024-02-04 08:41:57,976 [Epoch: 1068 Step: 00036300] Batch Recognition Loss:   0.000163 => Gls Tokens per Sec:     2106 || Batch Translation Loss:   0.008762 => Txt Tokens per Sec:     5834 || Lr: 0.000050
2024-02-04 08:41:59,688 Epoch 1068: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-04 08:41:59,689 EPOCH 1069
2024-02-04 08:42:03,936 Epoch 1069: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-04 08:42:03,937 EPOCH 1070
2024-02-04 08:42:08,937 Epoch 1070: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-04 08:42:08,938 EPOCH 1071
2024-02-04 08:42:10,998 [Epoch: 1071 Step: 00036400] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:     2987 || Batch Translation Loss:   0.013050 => Txt Tokens per Sec:     7708 || Lr: 0.000050
2024-02-04 08:42:13,229 Epoch 1071: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-04 08:42:13,230 EPOCH 1072
2024-02-04 08:42:18,116 Epoch 1072: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-04 08:42:18,117 EPOCH 1073
2024-02-04 08:42:22,747 Epoch 1073: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-04 08:42:22,748 EPOCH 1074
2024-02-04 08:42:25,262 [Epoch: 1074 Step: 00036500] Batch Recognition Loss:   0.000131 => Gls Tokens per Sec:     2192 || Batch Translation Loss:   0.009381 => Txt Tokens per Sec:     5895 || Lr: 0.000050
2024-02-04 08:42:27,532 Epoch 1074: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-04 08:42:27,533 EPOCH 1075
2024-02-04 08:42:31,879 Epoch 1075: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-04 08:42:31,879 EPOCH 1076
2024-02-04 08:42:36,732 Epoch 1076: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-04 08:42:36,732 EPOCH 1077
2024-02-04 08:42:38,903 [Epoch: 1077 Step: 00036600] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:     2361 || Batch Translation Loss:   0.016628 => Txt Tokens per Sec:     6731 || Lr: 0.000050
2024-02-04 08:42:41,312 Epoch 1077: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-04 08:42:41,313 EPOCH 1078
2024-02-04 08:42:45,902 Epoch 1078: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-04 08:42:45,902 EPOCH 1079
2024-02-04 08:42:50,242 Epoch 1079: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-04 08:42:50,242 EPOCH 1080
2024-02-04 08:42:51,890 [Epoch: 1080 Step: 00036700] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:     2571 || Batch Translation Loss:   0.008519 => Txt Tokens per Sec:     6602 || Lr: 0.000050
2024-02-04 08:42:55,064 Epoch 1080: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-04 08:42:55,064 EPOCH 1081
2024-02-04 08:42:59,564 Epoch 1081: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-04 08:42:59,565 EPOCH 1082
2024-02-04 08:43:04,245 Epoch 1082: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-04 08:43:04,245 EPOCH 1083
2024-02-04 08:43:05,704 [Epoch: 1083 Step: 00036800] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     2462 || Batch Translation Loss:   0.010209 => Txt Tokens per Sec:     6970 || Lr: 0.000050
2024-02-04 08:43:08,826 Epoch 1083: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-04 08:43:08,826 EPOCH 1084
2024-02-04 08:43:13,506 Epoch 1084: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-04 08:43:13,507 EPOCH 1085
2024-02-04 08:43:18,211 Epoch 1085: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-04 08:43:18,211 EPOCH 1086
2024-02-04 08:43:19,736 [Epoch: 1086 Step: 00036900] Batch Recognition Loss:   0.000104 => Gls Tokens per Sec:     2100 || Batch Translation Loss:   0.006538 => Txt Tokens per Sec:     5920 || Lr: 0.000050
2024-02-04 08:43:22,688 Epoch 1086: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-04 08:43:22,688 EPOCH 1087
2024-02-04 08:43:27,515 Epoch 1087: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-04 08:43:27,515 EPOCH 1088
2024-02-04 08:43:32,321 Epoch 1088: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-04 08:43:32,322 EPOCH 1089
2024-02-04 08:43:33,470 [Epoch: 1089 Step: 00037000] Batch Recognition Loss:   0.000122 => Gls Tokens per Sec:     2232 || Batch Translation Loss:   0.012655 => Txt Tokens per Sec:     6009 || Lr: 0.000050
2024-02-04 08:43:37,225 Epoch 1089: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-04 08:43:37,226 EPOCH 1090
2024-02-04 08:43:42,202 Epoch 1090: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-04 08:43:42,203 EPOCH 1091
2024-02-04 08:43:47,010 Epoch 1091: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-04 08:43:47,011 EPOCH 1092
2024-02-04 08:43:47,982 [Epoch: 1092 Step: 00037100] Batch Recognition Loss:   0.000261 => Gls Tokens per Sec:     1978 || Batch Translation Loss:   0.024571 => Txt Tokens per Sec:     5654 || Lr: 0.000050
2024-02-04 08:43:51,938 Epoch 1092: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-04 08:43:51,939 EPOCH 1093
2024-02-04 08:43:56,616 Epoch 1093: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.34 
2024-02-04 08:43:56,616 EPOCH 1094
2024-02-04 08:44:01,434 Epoch 1094: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-04 08:44:01,434 EPOCH 1095
2024-02-04 08:44:02,061 [Epoch: 1095 Step: 00037200] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     2044 || Batch Translation Loss:   0.021891 => Txt Tokens per Sec:     5876 || Lr: 0.000050
2024-02-04 08:44:06,256 Epoch 1095: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-04 08:44:06,257 EPOCH 1096
2024-02-04 08:44:11,008 Epoch 1096: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-04 08:44:11,008 EPOCH 1097
2024-02-04 08:44:15,756 Epoch 1097: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-04 08:44:15,756 EPOCH 1098
2024-02-04 08:44:16,098 [Epoch: 1098 Step: 00037300] Batch Recognition Loss:   0.000242 => Gls Tokens per Sec:     1147 || Batch Translation Loss:   0.004271 => Txt Tokens per Sec:     3262 || Lr: 0.000050
2024-02-04 08:44:20,688 Epoch 1098: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-04 08:44:20,688 EPOCH 1099
2024-02-04 08:44:25,634 Epoch 1099: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-04 08:44:25,635 EPOCH 1100
2024-02-04 08:44:30,472 [Epoch: 1100 Step: 00037400] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:     2198 || Batch Translation Loss:   0.011805 => Txt Tokens per Sec:     6103 || Lr: 0.000050
2024-02-04 08:44:30,472 Epoch 1100: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-04 08:44:30,472 EPOCH 1101
2024-02-04 08:44:35,259 Epoch 1101: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-04 08:44:35,259 EPOCH 1102
2024-02-04 08:44:40,062 Epoch 1102: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-04 08:44:40,062 EPOCH 1103
2024-02-04 08:44:44,599 [Epoch: 1103 Step: 00037500] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:     2202 || Batch Translation Loss:   0.009973 => Txt Tokens per Sec:     6067 || Lr: 0.000050
2024-02-04 08:44:44,906 Epoch 1103: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-04 08:44:44,906 EPOCH 1104
2024-02-04 08:44:49,635 Epoch 1104: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-04 08:44:49,636 EPOCH 1105
2024-02-04 08:44:54,402 Epoch 1105: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-04 08:44:54,402 EPOCH 1106
2024-02-04 08:44:58,045 [Epoch: 1106 Step: 00037600] Batch Recognition Loss:   0.000125 => Gls Tokens per Sec:     2567 || Batch Translation Loss:   0.010032 => Txt Tokens per Sec:     7143 || Lr: 0.000050
2024-02-04 08:44:58,600 Epoch 1106: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-04 08:44:58,600 EPOCH 1107
2024-02-04 08:45:03,648 Epoch 1107: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-04 08:45:03,648 EPOCH 1108
2024-02-04 08:45:08,330 Epoch 1108: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-04 08:45:08,330 EPOCH 1109
2024-02-04 08:45:12,227 [Epoch: 1109 Step: 00037700] Batch Recognition Loss:   0.000113 => Gls Tokens per Sec:     2235 || Batch Translation Loss:   0.010805 => Txt Tokens per Sec:     6297 || Lr: 0.000050
2024-02-04 08:45:12,909 Epoch 1109: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-04 08:45:12,909 EPOCH 1110
2024-02-04 08:45:17,509 Epoch 1110: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-04 08:45:17,509 EPOCH 1111
2024-02-04 08:45:22,022 Epoch 1111: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-04 08:45:22,023 EPOCH 1112
2024-02-04 08:45:25,462 [Epoch: 1112 Step: 00037800] Batch Recognition Loss:   0.000116 => Gls Tokens per Sec:     2420 || Batch Translation Loss:   0.016548 => Txt Tokens per Sec:     6653 || Lr: 0.000050
2024-02-04 08:45:26,685 Epoch 1112: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.88 
2024-02-04 08:45:26,686 EPOCH 1113
2024-02-04 08:45:31,329 Epoch 1113: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.31 
2024-02-04 08:45:31,329 EPOCH 1114
2024-02-04 08:45:35,888 Epoch 1114: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.39 
2024-02-04 08:45:35,888 EPOCH 1115
2024-02-04 08:45:39,273 [Epoch: 1115 Step: 00037900] Batch Recognition Loss:   0.000435 => Gls Tokens per Sec:     2195 || Batch Translation Loss:   0.032601 => Txt Tokens per Sec:     6330 || Lr: 0.000050
2024-02-04 08:45:40,567 Epoch 1115: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.72 
2024-02-04 08:45:40,567 EPOCH 1116
2024-02-04 08:45:45,027 Epoch 1116: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-04 08:45:45,027 EPOCH 1117
2024-02-04 08:45:49,149 Epoch 1117: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-04 08:45:49,149 EPOCH 1118
2024-02-04 08:45:51,623 [Epoch: 1118 Step: 00038000] Batch Recognition Loss:   0.000286 => Gls Tokens per Sec:     2746 || Batch Translation Loss:   0.026204 => Txt Tokens per Sec:     7554 || Lr: 0.000050
2024-02-04 08:46:00,044 Validation result at epoch 1118, step    38000: duration: 8.4201s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00086	Translation Loss: 92344.28906	PPL: 10310.29395
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.55	(BLEU-1: 10.14,	BLEU-2: 2.92,	BLEU-3: 1.18,	BLEU-4: 0.55)
	CHRF 16.77	ROUGE 8.59
2024-02-04 08:46:00,045 Logging Recognition and Translation Outputs
2024-02-04 08:46:00,045 ========================================================================================================================
2024-02-04 08:46:00,045 Logging Sequence: 72_194.00
2024-02-04 08:46:00,046 	Gloss Reference :	A B+C+D+E
2024-02-04 08:46:00,046 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:46:00,046 	Gloss Alignment :	         
2024-02-04 08:46:00,046 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:46:00,047 	Text Reference  :	shah told her to do what she wants and filed a    police complaint against  her    
2024-02-04 08:46:00,047 	Text Hypothesis :	**** **** *** ** ** **** *** ***** *** as    they have   been      shocking reports
2024-02-04 08:46:00,047 	Text Alignment  :	D    D    D   D  D  D    D   D     D   S     S    S      S         S        S      
2024-02-04 08:46:00,048 ========================================================================================================================
2024-02-04 08:46:00,048 Logging Sequence: 108_59.00
2024-02-04 08:46:00,048 	Gloss Reference :	A B+C+D+E
2024-02-04 08:46:00,048 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:46:00,048 	Gloss Alignment :	         
2024-02-04 08:46:00,048 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:46:00,050 	Text Reference  :	ishan kishan remained the biggest buy        of   ipl     as     mumbai indians paid a    whopping rs     1525 crore to  keep him   
2024-02-04 08:46:00,050 	Text Hypothesis :	***** ****** ******** *** ******* shockingly star batsman suresh raina  did     not  find any      takers and  he    was left unsold
2024-02-04 08:46:00,050 	Text Alignment  :	D     D      D        D   D       S          S    S       S      S      S       S    S    S        S      S    S     S   S    S     
2024-02-04 08:46:00,051 ========================================================================================================================
2024-02-04 08:46:00,051 Logging Sequence: 109_10.00
2024-02-04 08:46:00,051 	Gloss Reference :	A B+C+D+E
2024-02-04 08:46:00,051 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:46:00,051 	Gloss Alignment :	         
2024-02-04 08:46:00,051 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:46:00,052 	Text Reference  :	was scheduled to be   played    at     the narendra modi stadium   in     ahmedabad
2024-02-04 08:46:00,052 	Text Hypothesis :	*** ********* ** then rajasthan royals had issued   a    statement saying that     
2024-02-04 08:46:00,052 	Text Alignment  :	D   D         D  S    S         S      S   S        S    S         S      S        
2024-02-04 08:46:00,052 ========================================================================================================================
2024-02-04 08:46:00,052 Logging Sequence: 103_202.00
2024-02-04 08:46:00,053 	Gloss Reference :	A B+C+D+E
2024-02-04 08:46:00,053 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:46:00,053 	Gloss Alignment :	         
2024-02-04 08:46:00,053 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:46:00,054 	Text Reference  :	india in total has won 61           medals including 22           gold medals 16      silver    medals 23     bronze      medals
2024-02-04 08:46:00,054 	Text Hypothesis :	***** ** ***** *** *** commonwealth games  encourage independence from the    british democracy human  rights development etc   
2024-02-04 08:46:00,054 	Text Alignment  :	D     D  D     D   D   S            S      S         S            S    S      S       S         S      S      S           S     
2024-02-04 08:46:00,055 ========================================================================================================================
2024-02-04 08:46:00,055 Logging Sequence: 149_77.00
2024-02-04 08:46:00,055 	Gloss Reference :	A B+C+D+E
2024-02-04 08:46:00,055 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:46:00,055 	Gloss Alignment :	         
2024-02-04 08:46:00,055 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:46:00,057 	Text Reference  :	and arrested danushka for     alleged sexual    assault of  a      29  year old woman whose     name has not    been disclosed
2024-02-04 08:46:00,057 	Text Hypothesis :	*** on       2nd      october 2022    kartikeya met     his family and they get a     tricolour from the number of   matches  
2024-02-04 08:46:00,057 	Text Alignment  :	D   S        S        S       S       S         S       S   S      S   S    S   S     S         S    S   S      S    S        
2024-02-04 08:46:00,057 ========================================================================================================================
2024-02-04 08:46:01,940 Epoch 1118: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.89 
2024-02-04 08:46:01,940 EPOCH 1119
2024-02-04 08:46:06,774 Epoch 1119: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.29 
2024-02-04 08:46:06,775 EPOCH 1120
2024-02-04 08:46:11,271 Epoch 1120: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.50 
2024-02-04 08:46:11,271 EPOCH 1121
2024-02-04 08:46:14,153 [Epoch: 1121 Step: 00038100] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     2134 || Batch Translation Loss:   0.023234 => Txt Tokens per Sec:     6071 || Lr: 0.000050
2024-02-04 08:46:15,998 Epoch 1121: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.75 
2024-02-04 08:46:15,998 EPOCH 1122
2024-02-04 08:46:20,471 Epoch 1122: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.74 
2024-02-04 08:46:20,472 EPOCH 1123
2024-02-04 08:46:25,349 Epoch 1123: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-04 08:46:25,349 EPOCH 1124
2024-02-04 08:46:27,267 [Epoch: 1124 Step: 00038200] Batch Recognition Loss:   0.000191 => Gls Tokens per Sec:     3007 || Batch Translation Loss:   0.018385 => Txt Tokens per Sec:     7993 || Lr: 0.000050
2024-02-04 08:46:29,603 Epoch 1124: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-04 08:46:29,603 EPOCH 1125
2024-02-04 08:46:34,414 Epoch 1125: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-04 08:46:34,414 EPOCH 1126
2024-02-04 08:46:38,786 Epoch 1126: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-04 08:46:38,786 EPOCH 1127
2024-02-04 08:46:40,838 [Epoch: 1127 Step: 00038300] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     2497 || Batch Translation Loss:   0.012423 => Txt Tokens per Sec:     6809 || Lr: 0.000050
2024-02-04 08:46:43,697 Epoch 1127: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-04 08:46:43,698 EPOCH 1128
2024-02-04 08:46:47,996 Epoch 1128: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-04 08:46:47,997 EPOCH 1129
2024-02-04 08:46:53,133 Epoch 1129: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-04 08:46:53,133 EPOCH 1130
2024-02-04 08:46:55,013 [Epoch: 1130 Step: 00038400] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:     2384 || Batch Translation Loss:   0.011147 => Txt Tokens per Sec:     6808 || Lr: 0.000050
2024-02-04 08:46:57,701 Epoch 1130: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-04 08:46:57,701 EPOCH 1131
2024-02-04 08:47:02,449 Epoch 1131: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-04 08:47:02,450 EPOCH 1132
2024-02-04 08:47:06,959 Epoch 1132: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-04 08:47:06,959 EPOCH 1133
2024-02-04 08:47:08,376 [Epoch: 1133 Step: 00038500] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     2712 || Batch Translation Loss:   0.011221 => Txt Tokens per Sec:     7542 || Lr: 0.000050
2024-02-04 08:47:11,761 Epoch 1133: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-04 08:47:11,762 EPOCH 1134
2024-02-04 08:47:16,114 Epoch 1134: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-04 08:47:16,114 EPOCH 1135
2024-02-04 08:47:20,950 Epoch 1135: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-04 08:47:20,951 EPOCH 1136
2024-02-04 08:47:22,421 [Epoch: 1136 Step: 00038600] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:     2178 || Batch Translation Loss:   0.012284 => Txt Tokens per Sec:     6377 || Lr: 0.000050
2024-02-04 08:47:25,290 Epoch 1136: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-04 08:47:25,290 EPOCH 1137
2024-02-04 08:47:30,212 Epoch 1137: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-04 08:47:30,212 EPOCH 1138
2024-02-04 08:47:34,530 Epoch 1138: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-04 08:47:34,531 EPOCH 1139
2024-02-04 08:47:35,711 [Epoch: 1139 Step: 00038700] Batch Recognition Loss:   0.000115 => Gls Tokens per Sec:     2171 || Batch Translation Loss:   0.013026 => Txt Tokens per Sec:     6395 || Lr: 0.000050
2024-02-04 08:47:39,660 Epoch 1139: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-04 08:47:39,661 EPOCH 1140
2024-02-04 08:47:44,153 Epoch 1140: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-04 08:47:44,153 EPOCH 1141
2024-02-04 08:47:48,927 Epoch 1141: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-04 08:47:48,927 EPOCH 1142
2024-02-04 08:47:49,527 [Epoch: 1142 Step: 00038800] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     3209 || Batch Translation Loss:   0.012193 => Txt Tokens per Sec:     8579 || Lr: 0.000050
2024-02-04 08:47:53,396 Epoch 1142: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-04 08:47:53,396 EPOCH 1143
2024-02-04 08:47:58,244 Epoch 1143: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-04 08:47:58,244 EPOCH 1144
2024-02-04 08:48:02,523 Epoch 1144: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-04 08:48:02,523 EPOCH 1145
2024-02-04 08:48:03,114 [Epoch: 1145 Step: 00038900] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     2173 || Batch Translation Loss:   0.013209 => Txt Tokens per Sec:     6214 || Lr: 0.000050
2024-02-04 08:48:07,507 Epoch 1145: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-04 08:48:07,507 EPOCH 1146
2024-02-04 08:48:11,861 Epoch 1146: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-04 08:48:11,861 EPOCH 1147
2024-02-04 08:48:16,758 Epoch 1147: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-04 08:48:16,759 EPOCH 1148
2024-02-04 08:48:17,117 [Epoch: 1148 Step: 00039000] Batch Recognition Loss:   0.000225 => Gls Tokens per Sec:     1793 || Batch Translation Loss:   0.029300 => Txt Tokens per Sec:     5812 || Lr: 0.000050
2024-02-04 08:48:21,280 Epoch 1148: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-04 08:48:21,281 EPOCH 1149
2024-02-04 08:48:26,059 Epoch 1149: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-04 08:48:26,059 EPOCH 1150
2024-02-04 08:48:30,327 [Epoch: 1150 Step: 00039100] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     2491 || Batch Translation Loss:   0.017005 => Txt Tokens per Sec:     6916 || Lr: 0.000050
2024-02-04 08:48:30,328 Epoch 1150: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-04 08:48:30,328 EPOCH 1151
2024-02-04 08:48:35,209 Epoch 1151: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-04 08:48:35,210 EPOCH 1152
2024-02-04 08:48:39,675 Epoch 1152: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-04 08:48:39,675 EPOCH 1153
2024-02-04 08:48:44,268 [Epoch: 1153 Step: 00039200] Batch Recognition Loss:   0.000125 => Gls Tokens per Sec:     2175 || Batch Translation Loss:   0.036461 => Txt Tokens per Sec:     6059 || Lr: 0.000050
2024-02-04 08:48:44,479 Epoch 1153: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-04 08:48:44,480 EPOCH 1154
2024-02-04 08:48:49,018 Epoch 1154: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-04 08:48:49,018 EPOCH 1155
2024-02-04 08:48:53,671 Epoch 1155: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-04 08:48:53,671 EPOCH 1156
2024-02-04 08:48:57,806 [Epoch: 1156 Step: 00039300] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:     2262 || Batch Translation Loss:   0.012915 => Txt Tokens per Sec:     6280 || Lr: 0.000050
2024-02-04 08:48:58,313 Epoch 1156: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-04 08:48:58,313 EPOCH 1157
2024-02-04 08:49:02,878 Epoch 1157: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-04 08:49:02,878 EPOCH 1158
2024-02-04 08:49:07,572 Epoch 1158: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.45 
2024-02-04 08:49:07,573 EPOCH 1159
2024-02-04 08:49:11,150 [Epoch: 1159 Step: 00039400] Batch Recognition Loss:   0.000288 => Gls Tokens per Sec:     2436 || Batch Translation Loss:   0.051249 => Txt Tokens per Sec:     6758 || Lr: 0.000050
2024-02-04 08:49:12,016 Epoch 1159: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.74 
2024-02-04 08:49:12,016 EPOCH 1160
2024-02-04 08:49:16,829 Epoch 1160: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.42 
2024-02-04 08:49:16,829 EPOCH 1161
2024-02-04 08:49:21,208 Epoch 1161: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-04 08:49:21,208 EPOCH 1162
2024-02-04 08:49:25,101 [Epoch: 1162 Step: 00039500] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:     2073 || Batch Translation Loss:   0.014659 => Txt Tokens per Sec:     5775 || Lr: 0.000050
2024-02-04 08:49:26,118 Epoch 1162: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-04 08:49:26,119 EPOCH 1163
2024-02-04 08:49:30,340 Epoch 1163: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-04 08:49:30,340 EPOCH 1164
2024-02-04 08:49:35,293 Epoch 1164: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-04 08:49:35,293 EPOCH 1165
2024-02-04 08:49:38,311 [Epoch: 1165 Step: 00039600] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:     2545 || Batch Translation Loss:   0.025567 => Txt Tokens per Sec:     7081 || Lr: 0.000050
2024-02-04 08:49:39,674 Epoch 1165: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-04 08:49:39,675 EPOCH 1166
2024-02-04 08:49:44,577 Epoch 1166: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-04 08:49:44,577 EPOCH 1167
2024-02-04 08:49:49,066 Epoch 1167: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.11 
2024-02-04 08:49:49,066 EPOCH 1168
2024-02-04 08:49:51,836 [Epoch: 1168 Step: 00039700] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     2452 || Batch Translation Loss:   0.020005 => Txt Tokens per Sec:     6492 || Lr: 0.000050
2024-02-04 08:49:53,802 Epoch 1168: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-04 08:49:53,802 EPOCH 1169
2024-02-04 08:49:58,272 Epoch 1169: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-04 08:49:58,272 EPOCH 1170
2024-02-04 08:50:02,924 Epoch 1170: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-04 08:50:02,924 EPOCH 1171
2024-02-04 08:50:05,434 [Epoch: 1171 Step: 00039800] Batch Recognition Loss:   0.000252 => Gls Tokens per Sec:     2551 || Batch Translation Loss:   0.018601 => Txt Tokens per Sec:     7040 || Lr: 0.000050
2024-02-04 08:50:07,487 Epoch 1171: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-04 08:50:07,488 EPOCH 1172
2024-02-04 08:50:12,462 Epoch 1172: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-04 08:50:12,463 EPOCH 1173
2024-02-04 08:50:16,790 Epoch 1173: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-04 08:50:16,791 EPOCH 1174
2024-02-04 08:50:19,520 [Epoch: 1174 Step: 00039900] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     2020 || Batch Translation Loss:   0.008196 => Txt Tokens per Sec:     5506 || Lr: 0.000050
2024-02-04 08:50:21,648 Epoch 1174: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-04 08:50:21,649 EPOCH 1175
2024-02-04 08:50:26,045 Epoch 1175: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-04 08:50:26,045 EPOCH 1176
2024-02-04 08:50:30,897 Epoch 1176: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-04 08:50:30,897 EPOCH 1177
2024-02-04 08:50:32,685 [Epoch: 1177 Step: 00040000] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     2727 || Batch Translation Loss:   0.018344 => Txt Tokens per Sec:     7354 || Lr: 0.000050
2024-02-04 08:50:41,428 Validation result at epoch 1177, step    40000: duration: 8.7430s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00055	Translation Loss: 93457.42188	PPL: 11525.17773
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.80	(BLEU-1: 10.79,	BLEU-2: 3.39,	BLEU-3: 1.48,	BLEU-4: 0.80)
	CHRF 17.00	ROUGE 9.27
2024-02-04 08:50:41,429 Logging Recognition and Translation Outputs
2024-02-04 08:50:41,430 ========================================================================================================================
2024-02-04 08:50:41,430 Logging Sequence: 123_104.00
2024-02-04 08:50:41,430 	Gloss Reference :	A B+C+D+E
2024-02-04 08:50:41,430 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:50:41,431 	Gloss Alignment :	         
2024-02-04 08:50:41,431 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:50:41,432 	Text Reference  :	the car was presented to the former india cricketer from an  unknown person
2024-02-04 08:50:41,432 	Text Hypothesis :	*** *** now i         am so  my     cool  and       kept the wrong   jersey
2024-02-04 08:50:41,432 	Text Alignment  :	D   D   S   S         S  S   S      S     S         S    S   S       S     
2024-02-04 08:50:41,432 ========================================================================================================================
2024-02-04 08:50:41,432 Logging Sequence: 107_23.00
2024-02-04 08:50:41,432 	Gloss Reference :	A B+C+D+E
2024-02-04 08:50:41,432 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:50:41,433 	Gloss Alignment :	         
2024-02-04 08:50:41,433 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:50:41,433 	Text Reference  :	and viktor lilov who  is also from the  usa    
2024-02-04 08:50:41,433 	Text Hypothesis :	*** hence  the   bcci is **** **** very worried
2024-02-04 08:50:41,433 	Text Alignment  :	D   S      S     S       D    D    S    S      
2024-02-04 08:50:41,434 ========================================================================================================================
2024-02-04 08:50:41,434 Logging Sequence: 134_212.00
2024-02-04 08:50:41,434 	Gloss Reference :	A B+C+D+E
2024-02-04 08:50:41,434 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:50:41,434 	Gloss Alignment :	         
2024-02-04 08:50:41,434 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:50:41,435 	Text Reference  :	*** ******* ** *** ******** **** *** *** dhanush     said that he       practises little yoga
2024-02-04 08:50:41,435 	Text Hypothesis :	and because of the athletes what are the interaction with the  athletes on        their  way 
2024-02-04 08:50:41,435 	Text Alignment  :	I   I       I  I   I        I    I   I   S           S    S    S        S         S      S   
2024-02-04 08:50:41,435 ========================================================================================================================
2024-02-04 08:50:41,436 Logging Sequence: 165_577.00
2024-02-04 08:50:41,436 	Gloss Reference :	A B+C+D+E
2024-02-04 08:50:41,436 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:50:41,436 	Gloss Alignment :	         
2024-02-04 08:50:41,436 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:50:41,437 	Text Reference  :	***** then after 28 years india  won  the *** *** *** ****** world cup again in  2011     
2024-02-04 08:50:41,438 	Text Hypothesis :	dhoni said that  we will  handed over the bag who has warned about a   huge  fan following
2024-02-04 08:50:41,438 	Text Alignment  :	I     S    S     S  S     S      S        I   I   I   I      S     S   S     S   S        
2024-02-04 08:50:41,438 ========================================================================================================================
2024-02-04 08:50:41,438 Logging Sequence: 88_142.00
2024-02-04 08:50:41,438 	Gloss Reference :	A B+C+D+E
2024-02-04 08:50:41,438 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:50:41,438 	Gloss Alignment :	         
2024-02-04 08:50:41,438 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:50:41,439 	Text Reference  :	*** this  is    because the police ***** does not      do anything
2024-02-04 08:50:41,439 	Text Hypothesis :	the mayor added that    the police never does anything to catch   
2024-02-04 08:50:41,439 	Text Alignment  :	I   S     S     S                  I          S        S  S       
2024-02-04 08:50:41,440 ========================================================================================================================
2024-02-04 08:50:44,326 Epoch 1177: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-04 08:50:44,327 EPOCH 1178
2024-02-04 08:50:48,953 Epoch 1178: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-04 08:50:48,954 EPOCH 1179
2024-02-04 08:50:53,490 Epoch 1179: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-04 08:50:53,491 EPOCH 1180
2024-02-04 08:50:55,186 [Epoch: 1180 Step: 00040100] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:     2497 || Batch Translation Loss:   0.012901 => Txt Tokens per Sec:     6889 || Lr: 0.000050
2024-02-04 08:50:58,069 Epoch 1180: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-04 08:50:58,069 EPOCH 1181
2024-02-04 08:51:02,855 Epoch 1181: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-04 08:51:02,856 EPOCH 1182
2024-02-04 08:51:07,360 Epoch 1182: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-04 08:51:07,360 EPOCH 1183
2024-02-04 08:51:08,952 [Epoch: 1183 Step: 00040200] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     2413 || Batch Translation Loss:   0.020048 => Txt Tokens per Sec:     6636 || Lr: 0.000050
2024-02-04 08:51:12,181 Epoch 1183: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-04 08:51:12,182 EPOCH 1184
2024-02-04 08:51:16,596 Epoch 1184: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-04 08:51:16,596 EPOCH 1185
2024-02-04 08:51:21,495 Epoch 1185: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.27 
2024-02-04 08:51:21,495 EPOCH 1186
2024-02-04 08:51:22,864 [Epoch: 1186 Step: 00040300] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:     2342 || Batch Translation Loss:   0.039032 => Txt Tokens per Sec:     6793 || Lr: 0.000050
2024-02-04 08:51:25,918 Epoch 1186: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-04 08:51:25,919 EPOCH 1187
2024-02-04 08:51:30,942 Epoch 1187: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-04 08:51:30,943 EPOCH 1188
2024-02-04 08:51:35,103 Epoch 1188: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.72 
2024-02-04 08:51:35,103 EPOCH 1189
2024-02-04 08:51:36,236 [Epoch: 1189 Step: 00040400] Batch Recognition Loss:   0.000401 => Gls Tokens per Sec:     2042 || Batch Translation Loss:   0.230500 => Txt Tokens per Sec:     5930 || Lr: 0.000050
2024-02-04 08:51:40,078 Epoch 1189: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.71 
2024-02-04 08:51:40,079 EPOCH 1190
2024-02-04 08:51:44,675 Epoch 1190: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.71 
2024-02-04 08:51:44,675 EPOCH 1191
2024-02-04 08:51:49,443 Epoch 1191: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.82 
2024-02-04 08:51:49,444 EPOCH 1192
2024-02-04 08:51:50,010 [Epoch: 1192 Step: 00040500] Batch Recognition Loss:   0.000253 => Gls Tokens per Sec:     3398 || Batch Translation Loss:   0.020495 => Txt Tokens per Sec:     8616 || Lr: 0.000050
2024-02-04 08:51:53,548 Epoch 1192: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-04 08:51:53,548 EPOCH 1193
2024-02-04 08:51:57,975 Epoch 1193: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-04 08:51:57,975 EPOCH 1194
2024-02-04 08:52:02,738 Epoch 1194: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-04 08:52:02,738 EPOCH 1195
2024-02-04 08:52:03,349 [Epoch: 1195 Step: 00040600] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:     2098 || Batch Translation Loss:   0.049194 => Txt Tokens per Sec:     6379 || Lr: 0.000050
2024-02-04 08:52:06,895 Epoch 1195: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-04 08:52:06,896 EPOCH 1196
2024-02-04 08:52:11,747 Epoch 1196: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-04 08:52:11,747 EPOCH 1197
2024-02-04 08:52:16,402 Epoch 1197: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-04 08:52:16,402 EPOCH 1198
2024-02-04 08:52:16,665 [Epoch: 1198 Step: 00040700] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:     2443 || Batch Translation Loss:   0.019113 => Txt Tokens per Sec:     7523 || Lr: 0.000050
2024-02-04 08:52:21,063 Epoch 1198: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-04 08:52:21,063 EPOCH 1199
2024-02-04 08:52:25,585 Epoch 1199: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-04 08:52:25,586 EPOCH 1200
2024-02-04 08:52:30,114 [Epoch: 1200 Step: 00040800] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     2348 || Batch Translation Loss:   0.007526 => Txt Tokens per Sec:     6518 || Lr: 0.000050
2024-02-04 08:52:30,115 Epoch 1200: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-04 08:52:30,115 EPOCH 1201
2024-02-04 08:52:34,874 Epoch 1201: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-04 08:52:34,874 EPOCH 1202
2024-02-04 08:52:39,513 Epoch 1202: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-04 08:52:39,514 EPOCH 1203
2024-02-04 08:52:43,738 [Epoch: 1203 Step: 00040900] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:     2366 || Batch Translation Loss:   0.010815 => Txt Tokens per Sec:     6463 || Lr: 0.000050
2024-02-04 08:52:44,125 Epoch 1203: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-04 08:52:44,126 EPOCH 1204
2024-02-04 08:52:49,008 Epoch 1204: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-04 08:52:49,009 EPOCH 1205
2024-02-04 08:52:53,918 Epoch 1205: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-04 08:52:53,919 EPOCH 1206
2024-02-04 08:52:57,791 [Epoch: 1206 Step: 00041000] Batch Recognition Loss:   0.000125 => Gls Tokens per Sec:     2416 || Batch Translation Loss:   0.013381 => Txt Tokens per Sec:     6751 || Lr: 0.000050
2024-02-04 08:52:58,328 Epoch 1206: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-04 08:52:58,328 EPOCH 1207
2024-02-04 08:53:03,069 Epoch 1207: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-04 08:53:03,069 EPOCH 1208
2024-02-04 08:53:07,553 Epoch 1208: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-04 08:53:07,554 EPOCH 1209
2024-02-04 08:53:11,259 [Epoch: 1209 Step: 00041100] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     2351 || Batch Translation Loss:   0.011346 => Txt Tokens per Sec:     6459 || Lr: 0.000050
2024-02-04 08:53:12,244 Epoch 1209: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-04 08:53:12,244 EPOCH 1210
2024-02-04 08:53:16,616 Epoch 1210: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-04 08:53:16,616 EPOCH 1211
2024-02-04 08:53:21,409 Epoch 1211: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-04 08:53:21,409 EPOCH 1212
2024-02-04 08:53:24,590 [Epoch: 1212 Step: 00041200] Batch Recognition Loss:   0.000129 => Gls Tokens per Sec:     2538 || Batch Translation Loss:   0.016345 => Txt Tokens per Sec:     6935 || Lr: 0.000050
2024-02-04 08:53:25,912 Epoch 1212: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-04 08:53:25,913 EPOCH 1213
2024-02-04 08:53:30,899 Epoch 1213: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-04 08:53:30,900 EPOCH 1214
2024-02-04 08:53:35,106 Epoch 1214: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-04 08:53:35,106 EPOCH 1215
2024-02-04 08:53:38,823 [Epoch: 1215 Step: 00041300] Batch Recognition Loss:   0.000163 => Gls Tokens per Sec:     1999 || Batch Translation Loss:   0.012488 => Txt Tokens per Sec:     5584 || Lr: 0.000050
2024-02-04 08:53:40,039 Epoch 1215: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-04 08:53:40,039 EPOCH 1216
2024-02-04 08:53:44,262 Epoch 1216: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-04 08:53:44,262 EPOCH 1217
2024-02-04 08:53:48,812 Epoch 1217: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-04 08:53:48,813 EPOCH 1218
2024-02-04 08:53:52,004 [Epoch: 1218 Step: 00041400] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     2129 || Batch Translation Loss:   0.013454 => Txt Tokens per Sec:     5957 || Lr: 0.000050
2024-02-04 08:53:53,504 Epoch 1218: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-04 08:53:53,505 EPOCH 1219
2024-02-04 08:53:58,137 Epoch 1219: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-04 08:53:58,138 EPOCH 1220
2024-02-04 08:54:02,685 Epoch 1220: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-04 08:54:02,685 EPOCH 1221
2024-02-04 08:54:04,933 [Epoch: 1221 Step: 00041500] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:     2737 || Batch Translation Loss:   0.012115 => Txt Tokens per Sec:     7247 || Lr: 0.000050
2024-02-04 08:54:07,354 Epoch 1221: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-04 08:54:07,354 EPOCH 1222
2024-02-04 08:54:11,792 Epoch 1222: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-04 08:54:11,792 EPOCH 1223
2024-02-04 08:54:16,668 Epoch 1223: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-04 08:54:16,669 EPOCH 1224
2024-02-04 08:54:19,050 [Epoch: 1224 Step: 00041600] Batch Recognition Loss:   0.000131 => Gls Tokens per Sec:     2420 || Batch Translation Loss:   0.021147 => Txt Tokens per Sec:     6928 || Lr: 0.000050
2024-02-04 08:54:21,062 Epoch 1224: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-04 08:54:21,062 EPOCH 1225
2024-02-04 08:54:25,955 Epoch 1225: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.21 
2024-02-04 08:54:25,955 EPOCH 1226
2024-02-04 08:54:30,249 Epoch 1226: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.80 
2024-02-04 08:54:30,250 EPOCH 1227
2024-02-04 08:54:31,870 [Epoch: 1227 Step: 00041700] Batch Recognition Loss:   0.000516 => Gls Tokens per Sec:     3010 || Batch Translation Loss:   0.567062 => Txt Tokens per Sec:     8037 || Lr: 0.000050
2024-02-04 08:54:34,912 Epoch 1227: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.73 
2024-02-04 08:54:34,913 EPOCH 1228
2024-02-04 08:54:39,429 Epoch 1228: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.07 
2024-02-04 08:54:39,429 EPOCH 1229
2024-02-04 08:54:44,233 Epoch 1229: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.59 
2024-02-04 08:54:44,233 EPOCH 1230
2024-02-04 08:54:46,255 [Epoch: 1230 Step: 00041800] Batch Recognition Loss:   0.000240 => Gls Tokens per Sec:     2093 || Batch Translation Loss:   0.022479 => Txt Tokens per Sec:     5882 || Lr: 0.000050
2024-02-04 08:54:48,610 Epoch 1230: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.30 
2024-02-04 08:54:48,611 EPOCH 1231
2024-02-04 08:54:53,421 Epoch 1231: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-04 08:54:53,422 EPOCH 1232
2024-02-04 08:54:57,727 Epoch 1232: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-04 08:54:57,728 EPOCH 1233
2024-02-04 08:54:59,393 [Epoch: 1233 Step: 00041900] Batch Recognition Loss:   0.000180 => Gls Tokens per Sec:     2308 || Batch Translation Loss:   0.021066 => Txt Tokens per Sec:     6537 || Lr: 0.000050
2024-02-04 08:55:02,492 Epoch 1233: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-04 08:55:02,492 EPOCH 1234
2024-02-04 08:55:06,913 Epoch 1234: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-04 08:55:06,913 EPOCH 1235
2024-02-04 08:55:11,752 Epoch 1235: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-04 08:55:11,753 EPOCH 1236
2024-02-04 08:55:12,985 [Epoch: 1236 Step: 00042000] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     2396 || Batch Translation Loss:   0.010708 => Txt Tokens per Sec:     6262 || Lr: 0.000050
2024-02-04 08:55:21,501 Validation result at epoch 1236, step    42000: duration: 8.5141s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00064	Translation Loss: 93518.77344	PPL: 11596.14551
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.69	(BLEU-1: 10.53,	BLEU-2: 3.25,	BLEU-3: 1.32,	BLEU-4: 0.69)
	CHRF 16.94	ROUGE 8.92
2024-02-04 08:55:21,502 Logging Recognition and Translation Outputs
2024-02-04 08:55:21,502 ========================================================================================================================
2024-02-04 08:55:21,502 Logging Sequence: 81_8.00
2024-02-04 08:55:21,502 	Gloss Reference :	A B+C+D+E
2024-02-04 08:55:21,503 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:55:21,503 	Gloss Alignment :	         
2024-02-04 08:55:21,503 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:55:21,505 	Text Reference  :	have been  involved in   a       huge controversy in             connection to  real estate developer amrapali group  since last 7       years
2024-02-04 08:55:21,505 	Text Hypothesis :	the  venue narendra modi stadium for  the         india-pakistan match      has been kept   the       same     people can   book flights etc  
2024-02-04 08:55:21,505 	Text Alignment  :	S    S     S        S    S       S    S           S              S          S   S    S      S         S        S      S     S    S       S    
2024-02-04 08:55:21,505 ========================================================================================================================
2024-02-04 08:55:21,505 Logging Sequence: 148_239.00
2024-02-04 08:55:21,506 	Gloss Reference :	A B+C+D+E
2024-02-04 08:55:21,506 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:55:21,506 	Gloss Alignment :	         
2024-02-04 08:55:21,506 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:55:21,507 	Text Reference  :	the     ground staff were  very    happy  and    thanked the   bowler for      his kind gesture
2024-02-04 08:55:21,507 	Text Hypothesis :	however virat  kohli actor anushka sharma wiping that    india and    pakistan had a    man    
2024-02-04 08:55:21,507 	Text Alignment  :	S       S      S     S     S       S      S      S       S     S      S        S   S    S      
2024-02-04 08:55:21,508 ========================================================================================================================
2024-02-04 08:55:21,508 Logging Sequence: 165_8.00
2024-02-04 08:55:21,508 	Gloss Reference :	A B+C+D+E
2024-02-04 08:55:21,508 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:55:21,508 	Gloss Alignment :	         
2024-02-04 08:55:21,508 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:55:21,509 	Text Reference  :	however many don't believe in          it  it       varies among people 
2024-02-04 08:55:21,509 	Text Hypothesis :	******* they are   caught  interogated and expelled from   the   stadium
2024-02-04 08:55:21,509 	Text Alignment  :	D       S    S     S       S           S   S        S      S     S      
2024-02-04 08:55:21,509 ========================================================================================================================
2024-02-04 08:55:21,509 Logging Sequence: 93_93.00
2024-02-04 08:55:21,510 	Gloss Reference :	A B+C+D+E
2024-02-04 08:55:21,510 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:55:21,510 	Gloss Alignment :	         
2024-02-04 08:55:21,510 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:55:21,510 	Text Reference  :	rooney was at   the club as well
2024-02-04 08:55:21,510 	Text Hypothesis :	****** he  took a   gift 35 year
2024-02-04 08:55:21,511 	Text Alignment  :	D      S   S    S   S    S  S   
2024-02-04 08:55:21,511 ========================================================================================================================
2024-02-04 08:55:21,511 Logging Sequence: 96_129.00
2024-02-04 08:55:21,511 	Gloss Reference :	A B+C+D+E
2024-02-04 08:55:21,511 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 08:55:21,511 	Gloss Alignment :	         
2024-02-04 08:55:21,511 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 08:55:21,512 	Text Reference  :	***** *** ***** ******** *********** viewers    were very stressed
2024-02-04 08:55:21,512 	Text Hypothesis :	while the other includes afghanistan bangladesh and  sri  lanka   
2024-02-04 08:55:21,512 	Text Alignment  :	I     I   I     I        I           S          S    S    S       
2024-02-04 08:55:21,512 ========================================================================================================================
2024-02-04 08:55:24,911 Epoch 1236: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-04 08:55:24,912 EPOCH 1237
2024-02-04 08:55:29,864 Epoch 1237: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-04 08:55:29,864 EPOCH 1238
2024-02-04 08:55:34,236 Epoch 1238: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-04 08:55:34,236 EPOCH 1239
2024-02-04 08:55:35,746 [Epoch: 1239 Step: 00042100] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:     1698 || Batch Translation Loss:   0.016098 => Txt Tokens per Sec:     5220 || Lr: 0.000050
2024-02-04 08:55:39,154 Epoch 1239: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-04 08:55:39,154 EPOCH 1240
2024-02-04 08:55:43,647 Epoch 1240: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-04 08:55:43,647 EPOCH 1241
2024-02-04 08:55:48,395 Epoch 1241: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-04 08:55:48,395 EPOCH 1242
2024-02-04 08:55:48,910 [Epoch: 1242 Step: 00042200] Batch Recognition Loss:   0.000115 => Gls Tokens per Sec:     3735 || Batch Translation Loss:   0.007796 => Txt Tokens per Sec:     9416 || Lr: 0.000050
2024-02-04 08:55:52,847 Epoch 1242: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-04 08:55:52,848 EPOCH 1243
2024-02-04 08:55:57,545 Epoch 1243: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-04 08:55:57,545 EPOCH 1244
2024-02-04 08:56:02,162 Epoch 1244: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-04 08:56:02,163 EPOCH 1245
2024-02-04 08:56:02,702 [Epoch: 1245 Step: 00042300] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     1914 || Batch Translation Loss:   0.020891 => Txt Tokens per Sec:     5368 || Lr: 0.000050
2024-02-04 08:56:06,751 Epoch 1245: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-04 08:56:06,752 EPOCH 1246
2024-02-04 08:56:11,470 Epoch 1246: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-04 08:56:11,470 EPOCH 1247
2024-02-04 08:56:15,930 Epoch 1247: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-04 08:56:15,931 EPOCH 1248
2024-02-04 08:56:16,150 [Epoch: 1248 Step: 00042400] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     2922 || Batch Translation Loss:   0.018753 => Txt Tokens per Sec:     8945 || Lr: 0.000050
2024-02-04 08:56:20,381 Epoch 1248: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-04 08:56:20,382 EPOCH 1249
2024-02-04 08:56:25,124 Epoch 1249: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-04 08:56:25,124 EPOCH 1250
2024-02-04 08:56:29,600 [Epoch: 1250 Step: 00042500] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     2375 || Batch Translation Loss:   0.030188 => Txt Tokens per Sec:     6594 || Lr: 0.000050
2024-02-04 08:56:29,602 Epoch 1250: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-04 08:56:29,602 EPOCH 1251
2024-02-04 08:56:34,231 Epoch 1251: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.57 
2024-02-04 08:56:34,231 EPOCH 1252
2024-02-04 08:56:38,870 Epoch 1252: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-04 08:56:38,871 EPOCH 1253
2024-02-04 08:56:43,166 [Epoch: 1253 Step: 00042600] Batch Recognition Loss:   0.000293 => Gls Tokens per Sec:     2326 || Batch Translation Loss:   0.048562 => Txt Tokens per Sec:     6466 || Lr: 0.000050
2024-02-04 08:56:43,441 Epoch 1253: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.16 
2024-02-04 08:56:43,442 EPOCH 1254
2024-02-04 08:56:47,899 Epoch 1254: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-04 08:56:47,900 EPOCH 1255
2024-02-04 08:56:52,900 Epoch 1255: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-04 08:56:52,900 EPOCH 1256
2024-02-04 08:56:56,658 [Epoch: 1256 Step: 00042700] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:     2489 || Batch Translation Loss:   0.006356 => Txt Tokens per Sec:     6893 || Lr: 0.000050
2024-02-04 08:56:57,160 Epoch 1256: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-04 08:56:57,160 EPOCH 1257
2024-02-04 08:57:01,835 Epoch 1257: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-04 08:57:01,836 EPOCH 1258
2024-02-04 08:57:06,303 Epoch 1258: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.54 
2024-02-04 08:57:06,304 EPOCH 1259
2024-02-04 08:57:09,890 [Epoch: 1259 Step: 00042800] Batch Recognition Loss:   0.000247 => Gls Tokens per Sec:     2430 || Batch Translation Loss:   0.042277 => Txt Tokens per Sec:     6613 || Lr: 0.000050
2024-02-04 08:57:11,085 Epoch 1259: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.34 
2024-02-04 08:57:11,086 EPOCH 1260
2024-02-04 08:57:15,505 Epoch 1260: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-04 08:57:15,506 EPOCH 1261
2024-02-04 08:57:20,319 Epoch 1261: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-04 08:57:20,320 EPOCH 1262
2024-02-04 08:57:23,855 [Epoch: 1262 Step: 00042900] Batch Recognition Loss:   0.000178 => Gls Tokens per Sec:     2284 || Batch Translation Loss:   0.015210 => Txt Tokens per Sec:     6503 || Lr: 0.000050
2024-02-04 08:57:24,650 Epoch 1262: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-04 08:57:24,650 EPOCH 1263
2024-02-04 08:57:29,660 Epoch 1263: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.11 
2024-02-04 08:57:29,661 EPOCH 1264
2024-02-04 08:57:33,839 Epoch 1264: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-04 08:57:33,839 EPOCH 1265
2024-02-04 08:57:37,225 [Epoch: 1265 Step: 00043000] Batch Recognition Loss:   0.000352 => Gls Tokens per Sec:     2270 || Batch Translation Loss:   0.023952 => Txt Tokens per Sec:     6192 || Lr: 0.000050
2024-02-04 08:57:38,868 Epoch 1265: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-04 08:57:38,869 EPOCH 1266
2024-02-04 08:57:43,684 Epoch 1266: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-04 08:57:43,685 EPOCH 1267
2024-02-04 08:57:48,488 Epoch 1267: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-04 08:57:48,488 EPOCH 1268
2024-02-04 08:57:51,934 [Epoch: 1268 Step: 00043100] Batch Recognition Loss:   0.000252 => Gls Tokens per Sec:     1971 || Batch Translation Loss:   0.023468 => Txt Tokens per Sec:     5760 || Lr: 0.000050
2024-02-04 08:57:53,255 Epoch 1268: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-04 08:57:53,255 EPOCH 1269
2024-02-04 08:57:58,029 Epoch 1269: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-04 08:57:58,029 EPOCH 1270
2024-02-04 08:58:02,260 Epoch 1270: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-04 08:58:02,261 EPOCH 1271
2024-02-04 08:58:05,113 [Epoch: 1271 Step: 00043200] Batch Recognition Loss:   0.000122 => Gls Tokens per Sec:     2245 || Batch Translation Loss:   0.008314 => Txt Tokens per Sec:     6151 || Lr: 0.000050
2024-02-04 08:58:07,087 Epoch 1271: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-04 08:58:07,087 EPOCH 1272
2024-02-04 08:58:11,486 Epoch 1272: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-04 08:58:11,487 EPOCH 1273
2024-02-04 08:58:16,418 Epoch 1273: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-04 08:58:16,418 EPOCH 1274
2024-02-04 08:58:18,629 [Epoch: 1274 Step: 00043300] Batch Recognition Loss:   0.000180 => Gls Tokens per Sec:     2607 || Batch Translation Loss:   0.019786 => Txt Tokens per Sec:     7159 || Lr: 0.000050
2024-02-04 08:58:20,791 Epoch 1274: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-04 08:58:20,792 EPOCH 1275
2024-02-04 08:58:25,688 Epoch 1275: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-04 08:58:25,689 EPOCH 1276
2024-02-04 08:58:30,107 Epoch 1276: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-04 08:58:30,108 EPOCH 1277
2024-02-04 08:58:32,444 [Epoch: 1277 Step: 00043400] Batch Recognition Loss:   0.000208 => Gls Tokens per Sec:     2193 || Batch Translation Loss:   0.021714 => Txt Tokens per Sec:     6097 || Lr: 0.000050
2024-02-04 08:58:34,976 Epoch 1277: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.42 
2024-02-04 08:58:34,976 EPOCH 1278
2024-02-04 08:58:39,105 Epoch 1278: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.22 
2024-02-04 08:58:39,105 EPOCH 1279
2024-02-04 08:58:44,070 Epoch 1279: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-04 08:58:44,070 EPOCH 1280
2024-02-04 08:58:45,938 [Epoch: 1280 Step: 00043500] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:     2399 || Batch Translation Loss:   0.033236 => Txt Tokens per Sec:     6915 || Lr: 0.000050
2024-02-04 08:58:48,500 Epoch 1280: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-04 08:58:48,501 EPOCH 1281
2024-02-04 08:58:53,387 Epoch 1281: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-04 08:58:53,387 EPOCH 1282
2024-02-04 08:58:57,657 Epoch 1282: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.57 
2024-02-04 08:58:57,657 EPOCH 1283
2024-02-04 08:58:59,194 [Epoch: 1283 Step: 00043600] Batch Recognition Loss:   0.000232 => Gls Tokens per Sec:     2339 || Batch Translation Loss:   0.063058 => Txt Tokens per Sec:     6334 || Lr: 0.000050
2024-02-04 08:59:02,529 Epoch 1283: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.16 
2024-02-04 08:59:02,530 EPOCH 1284
2024-02-04 08:59:06,869 Epoch 1284: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.64 
2024-02-04 08:59:06,869 EPOCH 1285
2024-02-04 08:59:11,688 Epoch 1285: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-04 08:59:11,688 EPOCH 1286
2024-02-04 08:59:13,096 [Epoch: 1286 Step: 00043700] Batch Recognition Loss:   0.000345 => Gls Tokens per Sec:     2097 || Batch Translation Loss:   0.031534 => Txt Tokens per Sec:     5941 || Lr: 0.000050
2024-02-04 08:59:16,151 Epoch 1286: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-04 08:59:16,152 EPOCH 1287
2024-02-04 08:59:20,855 Epoch 1287: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-04 08:59:20,855 EPOCH 1288
2024-02-04 08:59:25,481 Epoch 1288: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-04 08:59:25,482 EPOCH 1289
2024-02-04 08:59:26,522 [Epoch: 1289 Step: 00043800] Batch Recognition Loss:   0.000178 => Gls Tokens per Sec:     2224 || Batch Translation Loss:   0.029504 => Txt Tokens per Sec:     6034 || Lr: 0.000050
2024-02-04 08:59:30,077 Epoch 1289: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-04 08:59:30,077 EPOCH 1290
2024-02-04 08:59:34,292 Epoch 1290: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-04 08:59:34,293 EPOCH 1291
2024-02-04 08:59:39,238 Epoch 1291: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-04 08:59:39,238 EPOCH 1292
2024-02-04 08:59:39,965 [Epoch: 1292 Step: 00043900] Batch Recognition Loss:   0.000272 => Gls Tokens per Sec:     2650 || Batch Translation Loss:   0.024986 => Txt Tokens per Sec:     7079 || Lr: 0.000050
2024-02-04 08:59:43,825 Epoch 1292: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-04 08:59:43,826 EPOCH 1293
2024-02-04 08:59:48,407 Epoch 1293: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-04 08:59:48,407 EPOCH 1294
2024-02-04 08:59:52,484 Epoch 1294: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.11 
2024-02-04 08:59:52,484 EPOCH 1295
2024-02-04 08:59:52,963 [Epoch: 1295 Step: 00044000] Batch Recognition Loss:   0.000221 => Gls Tokens per Sec:     2159 || Batch Translation Loss:   0.023526 => Txt Tokens per Sec:     5950 || Lr: 0.000050
2024-02-04 09:00:01,957 Validation result at epoch 1295, step    44000: duration: 8.9936s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00089	Translation Loss: 92938.87500	PPL: 10942.37207
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.85	(BLEU-1: 10.42,	BLEU-2: 3.14,	BLEU-3: 1.50,	BLEU-4: 0.85)
	CHRF 16.81	ROUGE 8.95
2024-02-04 09:00:01,958 Logging Recognition and Translation Outputs
2024-02-04 09:00:01,959 ========================================================================================================================
2024-02-04 09:00:01,959 Logging Sequence: 117_29.00
2024-02-04 09:00:01,960 	Gloss Reference :	A B+C+D+E
2024-02-04 09:00:01,960 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:00:01,960 	Gloss Alignment :	         
2024-02-04 09:00:01,960 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:00:01,961 	Text Reference  :	however england was unable to reach the target they were all out lost by  66    runs   
2024-02-04 09:00:01,961 	Text Hypothesis :	******* ******* *** ****** ** ***** *** sadly  they **** *** *** lost the match england
2024-02-04 09:00:01,962 	Text Alignment  :	D       D       D   D      D  D     D   S           D    D   D        S   S     S      
2024-02-04 09:00:01,962 ========================================================================================================================
2024-02-04 09:00:01,962 Logging Sequence: 84_176.00
2024-02-04 09:00:01,962 	Gloss Reference :	A B+C+D+E
2024-02-04 09:00:01,963 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:00:01,963 	Gloss Alignment :	         
2024-02-04 09:00:01,963 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:00:01,965 	Text Reference  :	** germany's nancy faeser who     attended the  game in  doha       against japan said
2024-02-04 09:00:01,965 	Text Hypothesis :	on 23rd      march 2022   ronaldo was      part of   the tournament without any   team
2024-02-04 09:00:01,965 	Text Alignment  :	I  S         S     S      S       S        S    S    S   S          S       S     S   
2024-02-04 09:00:01,965 ========================================================================================================================
2024-02-04 09:00:01,965 Logging Sequence: 172_98.00
2024-02-04 09:00:01,966 	Gloss Reference :	A B+C+D+E
2024-02-04 09:00:01,966 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:00:01,966 	Gloss Alignment :	         
2024-02-04 09:00:01,966 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:00:01,968 	Text Reference  :	*** ***** since 700 pm ****** *** it      kept raining the intensity plunged around 915 pm  
2024-02-04 09:00:01,968 	Text Hypothesis :	the rains at    630 pm centre was covered with final   of  her       hitting fours  and pune
2024-02-04 09:00:01,969 	Text Alignment  :	I   I     S     S      I      I   S       S    S       S   S         S       S      S   S   
2024-02-04 09:00:01,969 ========================================================================================================================
2024-02-04 09:00:01,969 Logging Sequence: 135_92.00
2024-02-04 09:00:01,969 	Gloss Reference :	A B+C+D+E
2024-02-04 09:00:01,969 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:00:01,970 	Gloss Alignment :	         
2024-02-04 09:00:01,970 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:00:01,972 	Text Reference  :	she wrote  that half    had      already been raised by  the ****** ***** family's online  fundraiser
2024-02-04 09:00:01,972 	Text Hypothesis :	she wanted to   restart training in      2019 and    won the silver medal in       javelin throw     
2024-02-04 09:00:01,972 	Text Alignment  :	    S      S    S       S        S       S    S      S       I      I     S        S       S         
2024-02-04 09:00:01,972 ========================================================================================================================
2024-02-04 09:00:01,972 Logging Sequence: 180_332.00
2024-02-04 09:00:01,973 	Gloss Reference :	A B+C+D+E
2024-02-04 09:00:01,973 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:00:01,973 	Gloss Alignment :	         
2024-02-04 09:00:01,973 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:00:01,974 	Text Reference  :	did i eat roti made of shilajit that i got energy  to  assault  so  many girls
2024-02-04 09:00:01,975 	Text Hypothesis :	*** * *** **** **** ** ******** **** * *** however our families did not  agree
2024-02-04 09:00:01,975 	Text Alignment  :	D   D D   D    D    D  D        D    D D   S       S   S        S   S    S    
2024-02-04 09:00:01,975 ========================================================================================================================
2024-02-04 09:00:06,534 Epoch 1295: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-04 09:00:06,535 EPOCH 1296
2024-02-04 09:00:11,509 Epoch 1296: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-04 09:00:11,510 EPOCH 1297
2024-02-04 09:00:16,152 Epoch 1297: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-04 09:00:16,152 EPOCH 1298
2024-02-04 09:00:16,345 [Epoch: 1298 Step: 00044100] Batch Recognition Loss:   0.000148 => Gls Tokens per Sec:     3342 || Batch Translation Loss:   0.009716 => Txt Tokens per Sec:     8031 || Lr: 0.000050
2024-02-04 09:00:20,196 Epoch 1298: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-04 09:00:20,196 EPOCH 1299
2024-02-04 09:00:24,959 Epoch 1299: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-04 09:00:24,959 EPOCH 1300
2024-02-04 09:00:29,416 [Epoch: 1300 Step: 00044200] Batch Recognition Loss:   0.000230 => Gls Tokens per Sec:     2386 || Batch Translation Loss:   0.065833 => Txt Tokens per Sec:     6622 || Lr: 0.000050
2024-02-04 09:00:29,416 Epoch 1300: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-04 09:00:29,416 EPOCH 1301
2024-02-04 09:00:34,152 Epoch 1301: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.18 
2024-02-04 09:00:34,153 EPOCH 1302
2024-02-04 09:00:38,532 Epoch 1302: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.72 
2024-02-04 09:00:38,532 EPOCH 1303
2024-02-04 09:00:43,175 [Epoch: 1303 Step: 00044300] Batch Recognition Loss:   0.000336 => Gls Tokens per Sec:     2152 || Batch Translation Loss:   0.056957 => Txt Tokens per Sec:     5932 || Lr: 0.000050
2024-02-04 09:00:43,517 Epoch 1303: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-04 09:00:43,517 EPOCH 1304
2024-02-04 09:00:48,172 Epoch 1304: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-04 09:00:48,172 EPOCH 1305
2024-02-04 09:00:52,766 Epoch 1305: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-04 09:00:52,767 EPOCH 1306
2024-02-04 09:00:56,713 [Epoch: 1306 Step: 00044400] Batch Recognition Loss:   0.000189 => Gls Tokens per Sec:     2370 || Batch Translation Loss:   0.020955 => Txt Tokens per Sec:     6462 || Lr: 0.000050
2024-02-04 09:00:57,291 Epoch 1306: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-04 09:00:57,292 EPOCH 1307
2024-02-04 09:01:01,922 Epoch 1307: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-04 09:01:01,923 EPOCH 1308
2024-02-04 09:01:06,446 Epoch 1308: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-04 09:01:06,446 EPOCH 1309
2024-02-04 09:01:10,535 [Epoch: 1309 Step: 00044500] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     2130 || Batch Translation Loss:   0.019479 => Txt Tokens per Sec:     5888 || Lr: 0.000050
2024-02-04 09:01:11,290 Epoch 1309: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-04 09:01:11,291 EPOCH 1310
2024-02-04 09:01:15,578 Epoch 1310: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-04 09:01:15,578 EPOCH 1311
2024-02-04 09:01:19,664 Epoch 1311: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-04 09:01:19,664 EPOCH 1312
2024-02-04 09:01:22,489 [Epoch: 1312 Step: 00044600] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:     2858 || Batch Translation Loss:   0.082831 => Txt Tokens per Sec:     7577 || Lr: 0.000050
2024-02-04 09:01:23,711 Epoch 1312: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-04 09:01:23,712 EPOCH 1313
2024-02-04 09:01:28,620 Epoch 1313: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-04 09:01:28,621 EPOCH 1314
2024-02-04 09:01:32,930 Epoch 1314: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-04 09:01:32,931 EPOCH 1315
2024-02-04 09:01:36,685 [Epoch: 1315 Step: 00044700] Batch Recognition Loss:   0.000320 => Gls Tokens per Sec:     1980 || Batch Translation Loss:   0.003973 => Txt Tokens per Sec:     5583 || Lr: 0.000050
2024-02-04 09:01:37,864 Epoch 1315: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-04 09:01:37,864 EPOCH 1316
2024-02-04 09:01:42,299 Epoch 1316: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-04 09:01:42,300 EPOCH 1317
2024-02-04 09:01:47,115 Epoch 1317: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-04 09:01:47,115 EPOCH 1318
2024-02-04 09:01:49,880 [Epoch: 1318 Step: 00044800] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:     2547 || Batch Translation Loss:   0.015274 => Txt Tokens per Sec:     7109 || Lr: 0.000050
2024-02-04 09:01:51,526 Epoch 1318: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-04 09:01:51,526 EPOCH 1319
2024-02-04 09:01:56,309 Epoch 1319: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-04 09:01:56,309 EPOCH 1320
2024-02-04 09:02:01,055 Epoch 1320: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-04 09:02:01,056 EPOCH 1321
2024-02-04 09:02:03,682 [Epoch: 1321 Step: 00044900] Batch Recognition Loss:   0.000250 => Gls Tokens per Sec:     2438 || Batch Translation Loss:   0.017802 => Txt Tokens per Sec:     6598 || Lr: 0.000050
2024-02-04 09:02:05,488 Epoch 1321: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-04 09:02:05,488 EPOCH 1322
2024-02-04 09:02:09,745 Epoch 1322: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-04 09:02:09,746 EPOCH 1323
2024-02-04 09:02:14,563 Epoch 1323: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-04 09:02:14,563 EPOCH 1324
2024-02-04 09:02:16,820 [Epoch: 1324 Step: 00045000] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     2554 || Batch Translation Loss:   0.021928 => Txt Tokens per Sec:     7066 || Lr: 0.000050
2024-02-04 09:02:18,932 Epoch 1324: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-04 09:02:18,933 EPOCH 1325
2024-02-04 09:02:23,784 Epoch 1325: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-04 09:02:23,784 EPOCH 1326
2024-02-04 09:02:28,282 Epoch 1326: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-04 09:02:28,283 EPOCH 1327
2024-02-04 09:02:30,695 [Epoch: 1327 Step: 00045100] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:     2124 || Batch Translation Loss:   0.020008 => Txt Tokens per Sec:     6142 || Lr: 0.000050
2024-02-04 09:02:32,902 Epoch 1327: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-04 09:02:32,903 EPOCH 1328
2024-02-04 09:02:37,455 Epoch 1328: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.47 
2024-02-04 09:02:37,455 EPOCH 1329
2024-02-04 09:02:42,244 Epoch 1329: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-04 09:02:42,245 EPOCH 1330
2024-02-04 09:02:44,034 [Epoch: 1330 Step: 00045200] Batch Recognition Loss:   0.000157 => Gls Tokens per Sec:     2366 || Batch Translation Loss:   0.016568 => Txt Tokens per Sec:     6460 || Lr: 0.000050
2024-02-04 09:02:47,076 Epoch 1330: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-04 09:02:47,076 EPOCH 1331
2024-02-04 09:02:51,904 Epoch 1331: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-04 09:02:51,905 EPOCH 1332
2024-02-04 09:02:56,651 Epoch 1332: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-04 09:02:56,652 EPOCH 1333
2024-02-04 09:02:57,928 [Epoch: 1333 Step: 00045300] Batch Recognition Loss:   0.000104 => Gls Tokens per Sec:     3012 || Batch Translation Loss:   0.004997 => Txt Tokens per Sec:     7793 || Lr: 0.000050
2024-02-04 09:03:00,844 Epoch 1333: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-04 09:03:00,844 EPOCH 1334
2024-02-04 09:03:05,727 Epoch 1334: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-04 09:03:05,728 EPOCH 1335
2024-02-04 09:03:09,956 Epoch 1335: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-04 09:03:09,956 EPOCH 1336
2024-02-04 09:03:11,847 [Epoch: 1336 Step: 00045400] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:     1695 || Batch Translation Loss:   0.015781 => Txt Tokens per Sec:     5038 || Lr: 0.000050
2024-02-04 09:03:14,956 Epoch 1336: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-04 09:03:14,956 EPOCH 1337
2024-02-04 09:03:19,115 Epoch 1337: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-04 09:03:19,115 EPOCH 1338
2024-02-04 09:03:23,678 Epoch 1338: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-04 09:03:23,679 EPOCH 1339
2024-02-04 09:03:25,060 [Epoch: 1339 Step: 00045500] Batch Recognition Loss:   0.000120 => Gls Tokens per Sec:     1674 || Batch Translation Loss:   0.010392 => Txt Tokens per Sec:     4853 || Lr: 0.000050
2024-02-04 09:03:28,270 Epoch 1339: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-04 09:03:28,270 EPOCH 1340
2024-02-04 09:03:32,512 Epoch 1340: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.23 
2024-02-04 09:03:32,513 EPOCH 1341
2024-02-04 09:03:37,440 Epoch 1341: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.78 
2024-02-04 09:03:37,440 EPOCH 1342
2024-02-04 09:03:38,397 [Epoch: 1342 Step: 00045600] Batch Recognition Loss:   0.000362 => Gls Tokens per Sec:     2007 || Batch Translation Loss:   1.063411 => Txt Tokens per Sec:     5883 || Lr: 0.000050
2024-02-04 09:03:41,553 Epoch 1342: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.57 
2024-02-04 09:03:41,553 EPOCH 1343
2024-02-04 09:03:45,590 Epoch 1343: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.06 
2024-02-04 09:03:45,590 EPOCH 1344
2024-02-04 09:03:49,670 Epoch 1344: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.35 
2024-02-04 09:03:49,671 EPOCH 1345
2024-02-04 09:03:50,271 [Epoch: 1345 Step: 00045700] Batch Recognition Loss:   0.000335 => Gls Tokens per Sec:     2132 || Batch Translation Loss:   0.029852 => Txt Tokens per Sec:     6410 || Lr: 0.000050
2024-02-04 09:03:54,322 Epoch 1345: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-04 09:03:54,322 EPOCH 1346
2024-02-04 09:03:58,879 Epoch 1346: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-04 09:03:58,879 EPOCH 1347
2024-02-04 09:04:03,585 Epoch 1347: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-04 09:04:03,586 EPOCH 1348
2024-02-04 09:04:03,926 [Epoch: 1348 Step: 00045800] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:     1882 || Batch Translation Loss:   0.021343 => Txt Tokens per Sec:     5556 || Lr: 0.000050
2024-02-04 09:04:08,032 Epoch 1348: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-04 09:04:08,032 EPOCH 1349
2024-02-04 09:04:12,257 Epoch 1349: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-04 09:04:12,257 EPOCH 1350
2024-02-04 09:04:17,100 [Epoch: 1350 Step: 00045900] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     2195 || Batch Translation Loss:   0.012972 => Txt Tokens per Sec:     6094 || Lr: 0.000050
2024-02-04 09:04:17,101 Epoch 1350: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-04 09:04:17,101 EPOCH 1351
2024-02-04 09:04:21,507 Epoch 1351: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-04 09:04:21,507 EPOCH 1352
2024-02-04 09:04:26,312 Epoch 1352: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-04 09:04:26,312 EPOCH 1353
2024-02-04 09:04:30,442 [Epoch: 1353 Step: 00046000] Batch Recognition Loss:   0.000110 => Gls Tokens per Sec:     2419 || Batch Translation Loss:   0.010734 => Txt Tokens per Sec:     6686 || Lr: 0.000050
2024-02-04 09:04:39,416 Validation result at epoch 1353, step    46000: duration: 8.9730s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00065	Translation Loss: 94608.59375	PPL: 12932.33887
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.66	(BLEU-1: 9.95,	BLEU-2: 3.01,	BLEU-3: 1.30,	BLEU-4: 0.66)
	CHRF 16.62	ROUGE 8.71
2024-02-04 09:04:39,417 Logging Recognition and Translation Outputs
2024-02-04 09:04:39,417 ========================================================================================================================
2024-02-04 09:04:39,419 Logging Sequence: 126_121.00
2024-02-04 09:04:39,419 	Gloss Reference :	A B+C+D+E
2024-02-04 09:04:39,419 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:04:39,419 	Gloss Alignment :	         
2024-02-04 09:04:39,420 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:04:39,420 	Text Reference  :	everyone was very happy by his victory  
2024-02-04 09:04:39,420 	Text Hypothesis :	why      was **** ***** ** *** wonderful
2024-02-04 09:04:39,420 	Text Alignment  :	S            D    D     D  D   S        
2024-02-04 09:04:39,420 ========================================================================================================================
2024-02-04 09:04:39,420 Logging Sequence: 73_79.00
2024-02-04 09:04:39,421 	Gloss Reference :	A B+C+D+E
2024-02-04 09:04:39,421 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:04:39,421 	Gloss Alignment :	         
2024-02-04 09:04:39,421 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:04:39,423 	Text Reference  :	raina resturant has food from the rich spices of north india to       the aromatic   curries of   south india 
2024-02-04 09:04:39,423 	Text Hypothesis :	***** ********* *** **** **** the **** ****** ** pm    modi  realised his retirement is      from 6     cities
2024-02-04 09:04:39,423 	Text Alignment  :	D     D         D   D    D        D    D      D  S     S     S        S   S          S       S    S     S     
2024-02-04 09:04:39,423 ========================================================================================================================
2024-02-04 09:04:39,423 Logging Sequence: 95_152.00
2024-02-04 09:04:39,423 	Gloss Reference :	A B+C+D+E
2024-02-04 09:04:39,423 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:04:39,424 	Gloss Alignment :	         
2024-02-04 09:04:39,424 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:04:39,424 	Text Reference  :	******** ** ***** *** ****** how strange
2024-02-04 09:04:39,424 	Text Hypothesis :	everyone is about ' reason for being  
2024-02-04 09:04:39,424 	Text Alignment  :	I        I  I     I   I      S   S      
2024-02-04 09:04:39,424 ========================================================================================================================
2024-02-04 09:04:39,424 Logging Sequence: 135_39.00
2024-02-04 09:04:39,425 	Gloss Reference :	A B+C+D+E
2024-02-04 09:04:39,425 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:04:39,425 	Gloss Alignment :	         
2024-02-04 09:04:39,425 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:04:39,426 	Text Reference  :	who needs to travel from poland to       stanford university in      california
2024-02-04 09:04:39,426 	Text Hypothesis :	*** flew  to ****** the  tokyo  olympics left     her        olympic medal     
2024-02-04 09:04:39,426 	Text Alignment  :	D   S        D      S    S      S        S        S          S       S         
2024-02-04 09:04:39,426 ========================================================================================================================
2024-02-04 09:04:39,426 Logging Sequence: 87_2.00
2024-02-04 09:04:39,426 	Gloss Reference :	A B+C+D+E
2024-02-04 09:04:39,426 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:04:39,427 	Gloss Alignment :	         
2024-02-04 09:04:39,427 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:04:39,428 	Text Reference  :	cricketer gautam gambhir's jealousy against ms  dhoni and      virat kohli has       been increasing day     by  day       
2024-02-04 09:04:39,428 	Text Hypothesis :	********* ****** however   due      to      the covid pandemic they  were  postponed and  started    without any spectators
2024-02-04 09:04:39,428 	Text Alignment  :	D         D      S         S        S       S   S     S        S     S     S         S    S          S       S   S         
2024-02-04 09:04:39,428 ========================================================================================================================
2024-02-04 09:04:39,717 Epoch 1353: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-04 09:04:39,717 EPOCH 1354
2024-02-04 09:04:44,686 Epoch 1354: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-04 09:04:44,686 EPOCH 1355
2024-02-04 09:04:48,863 Epoch 1355: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-04 09:04:48,864 EPOCH 1356
2024-02-04 09:04:53,356 [Epoch: 1356 Step: 00046100] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:     2082 || Batch Translation Loss:   0.014021 => Txt Tokens per Sec:     5928 || Lr: 0.000050
2024-02-04 09:04:53,741 Epoch 1356: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-04 09:04:53,741 EPOCH 1357
2024-02-04 09:04:57,952 Epoch 1357: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-04 09:04:57,952 EPOCH 1358
2024-02-04 09:05:02,412 Epoch 1358: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-04 09:05:02,413 EPOCH 1359
2024-02-04 09:05:06,341 [Epoch: 1359 Step: 00046200] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:     2218 || Batch Translation Loss:   0.016398 => Txt Tokens per Sec:     6124 || Lr: 0.000050
2024-02-04 09:05:07,162 Epoch 1359: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-04 09:05:07,162 EPOCH 1360
2024-02-04 09:05:11,916 Epoch 1360: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-04 09:05:11,917 EPOCH 1361
2024-02-04 09:05:16,321 Epoch 1361: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-04 09:05:16,321 EPOCH 1362
2024-02-04 09:05:19,652 [Epoch: 1362 Step: 00046300] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:     2499 || Batch Translation Loss:   0.010072 => Txt Tokens per Sec:     7105 || Lr: 0.000050
2024-02-04 09:05:20,428 Epoch 1362: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-04 09:05:20,428 EPOCH 1363
2024-02-04 09:05:25,397 Epoch 1363: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-04 09:05:25,397 EPOCH 1364
2024-02-04 09:05:29,804 Epoch 1364: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-04 09:05:29,805 EPOCH 1365
2024-02-04 09:05:33,195 [Epoch: 1365 Step: 00046400] Batch Recognition Loss:   0.000163 => Gls Tokens per Sec:     2192 || Batch Translation Loss:   0.014745 => Txt Tokens per Sec:     6087 || Lr: 0.000050
2024-02-04 09:05:34,732 Epoch 1365: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-04 09:05:34,733 EPOCH 1366
2024-02-04 09:05:39,028 Epoch 1366: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-04 09:05:39,028 EPOCH 1367
2024-02-04 09:05:43,854 Epoch 1367: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-04 09:05:43,854 EPOCH 1368
2024-02-04 09:05:46,804 [Epoch: 1368 Step: 00046500] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     2387 || Batch Translation Loss:   0.021776 => Txt Tokens per Sec:     6937 || Lr: 0.000050
2024-02-04 09:05:48,315 Epoch 1368: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-04 09:05:48,315 EPOCH 1369
2024-02-04 09:05:53,054 Epoch 1369: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-04 09:05:53,054 EPOCH 1370
2024-02-04 09:05:57,888 Epoch 1370: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-04 09:05:57,889 EPOCH 1371
2024-02-04 09:06:00,587 [Epoch: 1371 Step: 00046600] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:     2280 || Batch Translation Loss:   0.015796 => Txt Tokens per Sec:     6209 || Lr: 0.000050
2024-02-04 09:06:02,502 Epoch 1371: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-04 09:06:02,502 EPOCH 1372
2024-02-04 09:06:06,925 Epoch 1372: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-04 09:06:06,926 EPOCH 1373
2024-02-04 09:06:11,625 Epoch 1373: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-04 09:06:11,625 EPOCH 1374
2024-02-04 09:06:13,510 [Epoch: 1374 Step: 00046700] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:     2925 || Batch Translation Loss:   0.022744 => Txt Tokens per Sec:     7954 || Lr: 0.000050
2024-02-04 09:06:16,209 Epoch 1374: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-04 09:06:16,210 EPOCH 1375
2024-02-04 09:06:20,832 Epoch 1375: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-04 09:06:20,833 EPOCH 1376
2024-02-04 09:06:25,523 Epoch 1376: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-04 09:06:25,524 EPOCH 1377
2024-02-04 09:06:27,857 [Epoch: 1377 Step: 00046800] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:     2195 || Batch Translation Loss:   0.030577 => Txt Tokens per Sec:     6154 || Lr: 0.000050
2024-02-04 09:06:30,001 Epoch 1377: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-04 09:06:30,001 EPOCH 1378
2024-02-04 09:06:34,766 Epoch 1378: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-04 09:06:34,767 EPOCH 1379
2024-02-04 09:06:39,199 Epoch 1379: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-04 09:06:39,200 EPOCH 1380
2024-02-04 09:06:41,222 [Epoch: 1380 Step: 00046900] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:     2093 || Batch Translation Loss:   0.036524 => Txt Tokens per Sec:     5842 || Lr: 0.000050
2024-02-04 09:06:44,106 Epoch 1380: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-04 09:06:44,107 EPOCH 1381
2024-02-04 09:06:48,371 Epoch 1381: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-04 09:06:48,372 EPOCH 1382
2024-02-04 09:06:53,313 Epoch 1382: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.37 
2024-02-04 09:06:53,314 EPOCH 1383
2024-02-04 09:06:54,776 [Epoch: 1383 Step: 00047000] Batch Recognition Loss:   0.000252 => Gls Tokens per Sec:     2630 || Batch Translation Loss:   0.043078 => Txt Tokens per Sec:     7331 || Lr: 0.000050
2024-02-04 09:06:57,674 Epoch 1383: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.42 
2024-02-04 09:06:57,675 EPOCH 1384
2024-02-04 09:07:02,528 Epoch 1384: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.36 
2024-02-04 09:07:02,528 EPOCH 1385
2024-02-04 09:07:07,014 Epoch 1385: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.15 
2024-02-04 09:07:07,015 EPOCH 1386
2024-02-04 09:07:08,315 [Epoch: 1386 Step: 00047100] Batch Recognition Loss:   0.000208 => Gls Tokens per Sec:     2464 || Batch Translation Loss:   0.104469 => Txt Tokens per Sec:     6528 || Lr: 0.000050
2024-02-04 09:07:11,771 Epoch 1386: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-04 09:07:11,771 EPOCH 1387
2024-02-04 09:07:16,282 Epoch 1387: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-04 09:07:16,283 EPOCH 1388
2024-02-04 09:07:20,950 Epoch 1388: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-04 09:07:20,950 EPOCH 1389
2024-02-04 09:07:21,998 [Epoch: 1389 Step: 00047200] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     2206 || Batch Translation Loss:   0.080952 => Txt Tokens per Sec:     6161 || Lr: 0.000050
2024-02-04 09:07:25,479 Epoch 1389: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.39 
2024-02-04 09:07:25,480 EPOCH 1390
2024-02-04 09:07:30,139 Epoch 1390: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.45 
2024-02-04 09:07:30,140 EPOCH 1391
2024-02-04 09:07:34,825 Epoch 1391: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-04 09:07:34,826 EPOCH 1392
2024-02-04 09:07:35,506 [Epoch: 1392 Step: 00047300] Batch Recognition Loss:   0.000259 => Gls Tokens per Sec:     2828 || Batch Translation Loss:   0.029640 => Txt Tokens per Sec:     7425 || Lr: 0.000050
2024-02-04 09:07:39,324 Epoch 1392: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-04 09:07:39,324 EPOCH 1393
2024-02-04 09:07:44,042 Epoch 1393: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-04 09:07:44,043 EPOCH 1394
2024-02-04 09:07:48,460 Epoch 1394: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-04 09:07:48,460 EPOCH 1395
2024-02-04 09:07:49,060 [Epoch: 1395 Step: 00047400] Batch Recognition Loss:   0.000290 => Gls Tokens per Sec:     2136 || Batch Translation Loss:   0.022481 => Txt Tokens per Sec:     6464 || Lr: 0.000050
2024-02-04 09:07:53,285 Epoch 1395: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-04 09:07:53,286 EPOCH 1396
2024-02-04 09:07:57,610 Epoch 1396: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-04 09:07:57,610 EPOCH 1397
2024-02-04 09:08:02,464 Epoch 1397: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-04 09:08:02,464 EPOCH 1398
2024-02-04 09:08:02,949 [Epoch: 1398 Step: 00047500] Batch Recognition Loss:   0.000259 => Gls Tokens per Sec:     1326 || Batch Translation Loss:   0.034592 => Txt Tokens per Sec:     4022 || Lr: 0.000050
2024-02-04 09:08:06,781 Epoch 1398: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-04 09:08:06,782 EPOCH 1399
2024-02-04 09:08:11,968 Epoch 1399: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-04 09:08:11,969 EPOCH 1400
2024-02-04 09:08:16,394 [Epoch: 1400 Step: 00047600] Batch Recognition Loss:   0.000317 => Gls Tokens per Sec:     2403 || Batch Translation Loss:   0.068684 => Txt Tokens per Sec:     6670 || Lr: 0.000050
2024-02-04 09:08:16,394 Epoch 1400: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.20 
2024-02-04 09:08:16,394 EPOCH 1401
2024-02-04 09:08:21,125 Epoch 1401: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.68 
2024-02-04 09:08:21,126 EPOCH 1402
2024-02-04 09:08:25,643 Epoch 1402: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.65 
2024-02-04 09:08:25,644 EPOCH 1403
2024-02-04 09:08:30,252 [Epoch: 1403 Step: 00047700] Batch Recognition Loss:   0.000362 => Gls Tokens per Sec:     2168 || Batch Translation Loss:   0.038584 => Txt Tokens per Sec:     6070 || Lr: 0.000050
2024-02-04 09:08:30,447 Epoch 1403: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-04 09:08:30,447 EPOCH 1404
2024-02-04 09:08:34,828 Epoch 1404: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-04 09:08:34,829 EPOCH 1405
2024-02-04 09:08:39,721 Epoch 1405: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-04 09:08:39,722 EPOCH 1406
2024-02-04 09:08:43,599 [Epoch: 1406 Step: 00047800] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:     2412 || Batch Translation Loss:   0.021914 => Txt Tokens per Sec:     6799 || Lr: 0.000050
2024-02-04 09:08:43,970 Epoch 1406: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-04 09:08:43,971 EPOCH 1407
2024-02-04 09:08:49,023 Epoch 1407: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-04 09:08:49,024 EPOCH 1408
2024-02-04 09:08:53,341 Epoch 1408: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-04 09:08:53,341 EPOCH 1409
2024-02-04 09:08:57,175 [Epoch: 1409 Step: 00047900] Batch Recognition Loss:   0.000192 => Gls Tokens per Sec:     2273 || Batch Translation Loss:   0.018536 => Txt Tokens per Sec:     6236 || Lr: 0.000050
2024-02-04 09:08:58,232 Epoch 1409: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-04 09:08:58,233 EPOCH 1410
2024-02-04 09:09:02,893 Epoch 1410: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-04 09:09:02,894 EPOCH 1411
2024-02-04 09:09:07,488 Epoch 1411: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-04 09:09:07,488 EPOCH 1412
2024-02-04 09:09:10,574 [Epoch: 1412 Step: 00048000] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:     2616 || Batch Translation Loss:   0.013254 => Txt Tokens per Sec:     7264 || Lr: 0.000050
2024-02-04 09:09:19,058 Validation result at epoch 1412, step    48000: duration: 8.4843s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00064	Translation Loss: 94739.94531	PPL: 13103.45117
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.57	(BLEU-1: 10.14,	BLEU-2: 2.96,	BLEU-3: 1.22,	BLEU-4: 0.57)
	CHRF 16.66	ROUGE 9.01
2024-02-04 09:09:19,059 Logging Recognition and Translation Outputs
2024-02-04 09:09:19,059 ========================================================================================================================
2024-02-04 09:09:19,060 Logging Sequence: 88_159.00
2024-02-04 09:09:19,060 	Gloss Reference :	A B+C+D+E
2024-02-04 09:09:19,061 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:09:19,061 	Gloss Alignment :	         
2024-02-04 09:09:19,062 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:09:19,062 	Text Reference  :	however he often comes to the town to  meet his relatives
2024-02-04 09:09:19,063 	Text Hypothesis :	******* ** ***** ***** ** the ban  was held in  goa      
2024-02-04 09:09:19,063 	Text Alignment  :	D       D  D     D     D      S    S   S    S   S        
2024-02-04 09:09:19,063 ========================================================================================================================
2024-02-04 09:09:19,063 Logging Sequence: 180_53.00
2024-02-04 09:09:19,063 	Gloss Reference :	A B+C+D+E
2024-02-04 09:09:19,063 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:09:19,064 	Gloss Alignment :	         
2024-02-04 09:09:19,064 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:09:19,064 	Text Reference  :	**** ** *** the ***** **** protest is    against singh again   
2024-02-04 09:09:19,064 	Text Hypothesis :	this is not the first time that    kohli and     an    argument
2024-02-04 09:09:19,064 	Text Alignment  :	I    I  I       I     I    S       S     S       S     S       
2024-02-04 09:09:19,065 ========================================================================================================================
2024-02-04 09:09:19,065 Logging Sequence: 163_30.00
2024-02-04 09:09:19,065 	Gloss Reference :	A B+C+D+E
2024-02-04 09:09:19,065 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:09:19,065 	Gloss Alignment :	         
2024-02-04 09:09:19,065 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:09:19,066 	Text Reference  :	*** *** *** ***** *** ****** *** they    never permitted anyone to reveal her     face   
2024-02-04 09:09:19,066 	Text Hypothesis :	the two men fired for around the referee and   going     on     to ****** several reasons
2024-02-04 09:09:19,066 	Text Alignment  :	I   I   I   I     I   I      I   S       S     S         S         D      S       S      
2024-02-04 09:09:19,066 ========================================================================================================================
2024-02-04 09:09:19,067 Logging Sequence: 51_110.00
2024-02-04 09:09:19,067 	Gloss Reference :	A B+C+D+E
2024-02-04 09:09:19,067 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:09:19,067 	Gloss Alignment :	         
2024-02-04 09:09:19,067 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:09:19,068 	Text Reference  :	the aussies were very happy with their      victory
2024-02-04 09:09:19,068 	Text Hypothesis :	*** they    are  from 6     such determined balls  
2024-02-04 09:09:19,068 	Text Alignment  :	D   S       S    S    S     S    S          S      
2024-02-04 09:09:19,068 ========================================================================================================================
2024-02-04 09:09:19,068 Logging Sequence: 70_249.00
2024-02-04 09:09:19,068 	Gloss Reference :	A B+C+D+E
2024-02-04 09:09:19,068 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:09:19,069 	Gloss Alignment :	         
2024-02-04 09:09:19,069 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:09:19,069 	Text Reference  :	** *** *** have a ****** ********* *** *** look    at      this video   
2024-02-04 09:09:19,070 	Text Hypothesis :	on 5th may be   a police complaint and the tickets through the  olympics
2024-02-04 09:09:19,070 	Text Alignment  :	I  I   I   S      I      I         I   I   S       S       S    S       
2024-02-04 09:09:19,070 ========================================================================================================================
2024-02-04 09:09:20,236 Epoch 1412: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-04 09:09:20,237 EPOCH 1413
2024-02-04 09:09:25,254 Epoch 1413: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-04 09:09:25,254 EPOCH 1414
2024-02-04 09:09:29,522 Epoch 1414: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-04 09:09:29,522 EPOCH 1415
2024-02-04 09:09:33,077 [Epoch: 1415 Step: 00048100] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:     2161 || Batch Translation Loss:   0.013170 => Txt Tokens per Sec:     6043 || Lr: 0.000050
2024-02-04 09:09:34,419 Epoch 1415: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-04 09:09:34,420 EPOCH 1416
2024-02-04 09:09:38,760 Epoch 1416: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-04 09:09:38,761 EPOCH 1417
2024-02-04 09:09:43,626 Epoch 1417: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-04 09:09:43,626 EPOCH 1418
2024-02-04 09:09:46,536 [Epoch: 1418 Step: 00048200] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:     2335 || Batch Translation Loss:   0.012978 => Txt Tokens per Sec:     6606 || Lr: 0.000050
2024-02-04 09:09:48,120 Epoch 1418: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-04 09:09:48,120 EPOCH 1419
2024-02-04 09:09:52,856 Epoch 1419: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-04 09:09:52,856 EPOCH 1420
2024-02-04 09:09:57,309 Epoch 1420: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-04 09:09:57,310 EPOCH 1421
2024-02-04 09:09:59,981 [Epoch: 1421 Step: 00048300] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:     2304 || Batch Translation Loss:   0.013262 => Txt Tokens per Sec:     6513 || Lr: 0.000050
2024-02-04 09:10:01,989 Epoch 1421: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-04 09:10:01,989 EPOCH 1422
2024-02-04 09:10:06,746 Epoch 1422: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-04 09:10:06,747 EPOCH 1423
2024-02-04 09:10:11,141 Epoch 1423: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-04 09:10:11,141 EPOCH 1424
2024-02-04 09:10:13,509 [Epoch: 1424 Step: 00048400] Batch Recognition Loss:   0.000294 => Gls Tokens per Sec:     2326 || Batch Translation Loss:   0.023298 => Txt Tokens per Sec:     6449 || Lr: 0.000050
2024-02-04 09:10:15,381 Epoch 1424: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-04 09:10:15,381 EPOCH 1425
2024-02-04 09:10:20,240 Epoch 1425: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.63 
2024-02-04 09:10:20,240 EPOCH 1426
2024-02-04 09:10:24,650 Epoch 1426: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.84 
2024-02-04 09:10:24,651 EPOCH 1427
2024-02-04 09:10:26,765 [Epoch: 1427 Step: 00048500] Batch Recognition Loss:   0.000417 => Gls Tokens per Sec:     2424 || Batch Translation Loss:   0.133204 => Txt Tokens per Sec:     6578 || Lr: 0.000050
2024-02-04 09:10:29,454 Epoch 1427: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.74 
2024-02-04 09:10:29,455 EPOCH 1428
2024-02-04 09:10:33,965 Epoch 1428: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.51 
2024-02-04 09:10:33,965 EPOCH 1429
2024-02-04 09:10:38,625 Epoch 1429: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-04 09:10:38,625 EPOCH 1430
2024-02-04 09:10:40,512 [Epoch: 1430 Step: 00048600] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     2377 || Batch Translation Loss:   0.035413 => Txt Tokens per Sec:     6735 || Lr: 0.000050
2024-02-04 09:10:42,690 Epoch 1430: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-04 09:10:42,690 EPOCH 1431
2024-02-04 09:10:47,573 Epoch 1431: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-04 09:10:47,574 EPOCH 1432
2024-02-04 09:10:51,939 Epoch 1432: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-04 09:10:51,940 EPOCH 1433
2024-02-04 09:10:54,000 [Epoch: 1433 Step: 00048700] Batch Recognition Loss:   0.000209 => Gls Tokens per Sec:     1865 || Batch Translation Loss:   0.027826 => Txt Tokens per Sec:     5180 || Lr: 0.000050
2024-02-04 09:10:56,861 Epoch 1433: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-04 09:10:56,862 EPOCH 1434
2024-02-04 09:11:01,186 Epoch 1434: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-04 09:11:01,187 EPOCH 1435
2024-02-04 09:11:06,242 Epoch 1435: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-04 09:11:06,243 EPOCH 1436
2024-02-04 09:11:07,602 [Epoch: 1436 Step: 00048800] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     2357 || Batch Translation Loss:   0.024986 => Txt Tokens per Sec:     6540 || Lr: 0.000050
2024-02-04 09:11:11,105 Epoch 1436: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-04 09:11:11,105 EPOCH 1437
2024-02-04 09:11:15,813 Epoch 1437: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-04 09:11:15,814 EPOCH 1438
2024-02-04 09:11:20,596 Epoch 1438: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-04 09:11:20,596 EPOCH 1439
2024-02-04 09:11:21,359 [Epoch: 1439 Step: 00048900] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:     3364 || Batch Translation Loss:   0.008724 => Txt Tokens per Sec:     7559 || Lr: 0.000050
2024-02-04 09:11:25,317 Epoch 1439: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-04 09:11:25,317 EPOCH 1440
2024-02-04 09:11:30,088 Epoch 1440: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-04 09:11:30,088 EPOCH 1441
2024-02-04 09:11:34,674 Epoch 1441: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-04 09:11:34,675 EPOCH 1442
2024-02-04 09:11:35,422 [Epoch: 1442 Step: 00049000] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     2237 || Batch Translation Loss:   0.017631 => Txt Tokens per Sec:     6083 || Lr: 0.000050
2024-02-04 09:11:39,552 Epoch 1442: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-04 09:11:39,553 EPOCH 1443
2024-02-04 09:11:44,024 Epoch 1443: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-04 09:11:44,024 EPOCH 1444
2024-02-04 09:11:48,790 Epoch 1444: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-04 09:11:48,791 EPOCH 1445
2024-02-04 09:11:49,241 [Epoch: 1445 Step: 00049100] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:     2293 || Batch Translation Loss:   0.025165 => Txt Tokens per Sec:     6529 || Lr: 0.000050
2024-02-04 09:11:53,663 Epoch 1445: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-04 09:11:53,663 EPOCH 1446
2024-02-04 09:11:58,650 Epoch 1446: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-04 09:11:58,650 EPOCH 1447
2024-02-04 09:12:03,600 Epoch 1447: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-04 09:12:03,600 EPOCH 1448
2024-02-04 09:12:03,823 [Epoch: 1448 Step: 00049200] Batch Recognition Loss:   0.000207 => Gls Tokens per Sec:     1765 || Batch Translation Loss:   0.004822 => Txt Tokens per Sec:     4176 || Lr: 0.000050
2024-02-04 09:12:08,524 Epoch 1448: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-04 09:12:08,525 EPOCH 1449
2024-02-04 09:12:13,543 Epoch 1449: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-04 09:12:13,544 EPOCH 1450
2024-02-04 09:12:18,490 [Epoch: 1450 Step: 00049300] Batch Recognition Loss:   0.000104 => Gls Tokens per Sec:     2150 || Batch Translation Loss:   0.012730 => Txt Tokens per Sec:     5968 || Lr: 0.000050
2024-02-04 09:12:18,490 Epoch 1450: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-04 09:12:18,491 EPOCH 1451
2024-02-04 09:12:23,382 Epoch 1451: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-04 09:12:23,382 EPOCH 1452
2024-02-04 09:12:27,914 Epoch 1452: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-04 09:12:27,914 EPOCH 1453
2024-02-04 09:12:32,027 [Epoch: 1453 Step: 00049400] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:     2430 || Batch Translation Loss:   0.020019 => Txt Tokens per Sec:     6724 || Lr: 0.000050
2024-02-04 09:12:32,411 Epoch 1453: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-04 09:12:32,411 EPOCH 1454
2024-02-04 09:12:37,164 Epoch 1454: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-04 09:12:37,164 EPOCH 1455
2024-02-04 09:12:41,956 Epoch 1455: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-04 09:12:41,957 EPOCH 1456
2024-02-04 09:12:46,026 [Epoch: 1456 Step: 00049500] Batch Recognition Loss:   0.000197 => Gls Tokens per Sec:     2298 || Batch Translation Loss:   0.044116 => Txt Tokens per Sec:     6369 || Lr: 0.000050
2024-02-04 09:12:46,745 Epoch 1456: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.15 
2024-02-04 09:12:46,745 EPOCH 1457
2024-02-04 09:12:51,580 Epoch 1457: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.77 
2024-02-04 09:12:51,581 EPOCH 1458
2024-02-04 09:12:56,363 Epoch 1458: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.89 
2024-02-04 09:12:56,363 EPOCH 1459
2024-02-04 09:12:59,839 [Epoch: 1459 Step: 00049600] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     2506 || Batch Translation Loss:   0.023033 => Txt Tokens per Sec:     7058 || Lr: 0.000050
2024-02-04 09:13:00,581 Epoch 1459: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.67 
2024-02-04 09:13:00,582 EPOCH 1460
2024-02-04 09:13:05,499 Epoch 1460: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.09 
2024-02-04 09:13:05,500 EPOCH 1461
2024-02-04 09:13:09,881 Epoch 1461: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.65 
2024-02-04 09:13:09,882 EPOCH 1462
2024-02-04 09:13:13,868 [Epoch: 1462 Step: 00049700] Batch Recognition Loss:   0.000228 => Gls Tokens per Sec:     2088 || Batch Translation Loss:   0.027958 => Txt Tokens per Sec:     5909 || Lr: 0.000050
2024-02-04 09:13:14,748 Epoch 1462: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.33 
2024-02-04 09:13:14,748 EPOCH 1463
2024-02-04 09:13:19,249 Epoch 1463: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.22 
2024-02-04 09:13:19,250 EPOCH 1464
2024-02-04 09:13:23,914 Epoch 1464: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-04 09:13:23,915 EPOCH 1465
2024-02-04 09:13:26,834 [Epoch: 1465 Step: 00049800] Batch Recognition Loss:   0.000309 => Gls Tokens per Sec:     2631 || Batch Translation Loss:   0.030606 => Txt Tokens per Sec:     7211 || Lr: 0.000050
2024-02-04 09:13:28,522 Epoch 1465: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-04 09:13:28,523 EPOCH 1466
2024-02-04 09:13:33,180 Epoch 1466: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-04 09:13:33,180 EPOCH 1467
2024-02-04 09:13:37,869 Epoch 1467: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-04 09:13:37,870 EPOCH 1468
2024-02-04 09:13:40,471 [Epoch: 1468 Step: 00049900] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:     2612 || Batch Translation Loss:   0.017345 => Txt Tokens per Sec:     6775 || Lr: 0.000050
2024-02-04 09:13:42,391 Epoch 1468: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-04 09:13:42,391 EPOCH 1469
2024-02-04 09:13:46,990 Epoch 1469: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-04 09:13:46,991 EPOCH 1470
2024-02-04 09:13:51,638 Epoch 1470: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-04 09:13:51,638 EPOCH 1471
2024-02-04 09:13:53,996 [Epoch: 1471 Step: 00050000] Batch Recognition Loss:   0.000108 => Gls Tokens per Sec:     2716 || Batch Translation Loss:   0.015099 => Txt Tokens per Sec:     7318 || Lr: 0.000050
2024-02-04 09:14:02,514 Validation result at epoch 1471, step    50000: duration: 8.5176s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00073	Translation Loss: 95564.33594	PPL: 14230.28223
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.66	(BLEU-1: 10.44,	BLEU-2: 3.15,	BLEU-3: 1.32,	BLEU-4: 0.66)
	CHRF 16.76	ROUGE 8.82
2024-02-04 09:14:02,515 Logging Recognition and Translation Outputs
2024-02-04 09:14:02,515 ========================================================================================================================
2024-02-04 09:14:02,516 Logging Sequence: 59_58.00
2024-02-04 09:14:02,516 	Gloss Reference :	A B+C+D+E
2024-02-04 09:14:02,517 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:14:02,517 	Gloss Alignment :	         
2024-02-04 09:14:02,518 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:14:02,519 	Text Reference  :	* ** **** ******** to fix the damage they did    not have a lot of time 
2024-02-04 09:14:02,519 	Text Hypothesis :	i am very exciting to fix *** it     and  missed out on   a *** ** medal
2024-02-04 09:14:02,520 	Text Alignment  :	I I  I    I               D   S      S    S      S   S      D   D  S    
2024-02-04 09:14:02,520 ========================================================================================================================
2024-02-04 09:14:02,520 Logging Sequence: 165_2.00
2024-02-04 09:14:02,520 	Gloss Reference :	A B+C+D+E
2024-02-04 09:14:02,520 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:14:02,520 	Gloss Alignment :	         
2024-02-04 09:14:02,521 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:14:02,522 	Text Reference  :	many people believe in   superstitions and   think it    brings good luck and  bad luck 
2024-02-04 09:14:02,522 	Text Hypothesis :	**** you    all     know that          after the   world cup    will be   held in  qatar
2024-02-04 09:14:02,522 	Text Alignment  :	D    S      S       S    S             S     S     S     S      S    S    S    S   S    
2024-02-04 09:14:02,522 ========================================================================================================================
2024-02-04 09:14:02,522 Logging Sequence: 58_147.00
2024-02-04 09:14:02,522 	Gloss Reference :	A B+C+D+E
2024-02-04 09:14:02,523 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:14:02,523 	Gloss Alignment :	         
2024-02-04 09:14:02,523 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:14:02,524 	Text Reference  :	the women's cricket team grabbed gold by beating sri lanka in   the finals what a   historic      win 
2024-02-04 09:14:02,524 	Text Hypothesis :	*** ******* ******* **** ******* **** ** ******* *** i     want to  tell   you  how disappointing that
2024-02-04 09:14:02,524 	Text Alignment  :	D   D       D       D    D       D    D  D       D   S     S    S   S      S    S   S             S   
2024-02-04 09:14:02,524 ========================================================================================================================
2024-02-04 09:14:02,524 Logging Sequence: 81_139.00
2024-02-04 09:14:02,525 	Gloss Reference :	A B+C+D+E
2024-02-04 09:14:02,525 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:14:02,525 	Gloss Alignment :	         
2024-02-04 09:14:02,525 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:14:02,527 	Text Reference  :	** in  2017      the  case     was filed first in delhi high court by rhiti   sports  management on    behalf of     dhoni
2024-02-04 09:14:02,527 	Text Hypothesis :	on 4th september 2022 everyone was ***** ***** ** ***** **** ***** ** playing against each       other in     former india
2024-02-04 09:14:02,527 	Text Alignment  :	I  S   S         S    S            D     D     D  D     D    D     D  S       S       S          S     S      S      S    
2024-02-04 09:14:02,527 ========================================================================================================================
2024-02-04 09:14:02,527 Logging Sequence: 125_72.00
2024-02-04 09:14:02,527 	Gloss Reference :	A B+C+D+E
2024-02-04 09:14:02,528 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:14:02,528 	Gloss Alignment :	         
2024-02-04 09:14:02,528 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:14:02,529 	Text Reference  :	some said the pakistani javelineer had milicious intentions of tampering with the  javelin      out of    jealousy  
2024-02-04 09:14:02,529 	Text Hypothesis :	**** **** *** ********* ********** *** ********* ********** ** neeraj    was  very disappointed by  these statements
2024-02-04 09:14:02,529 	Text Alignment  :	D    D    D   D         D          D   D         D          D  S         S    S    S            S   S     S         
2024-02-04 09:14:02,529 ========================================================================================================================
2024-02-04 09:14:04,908 Epoch 1471: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-04 09:14:04,909 EPOCH 1472
2024-02-04 09:14:09,394 Epoch 1472: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-04 09:14:09,394 EPOCH 1473
2024-02-04 09:14:14,141 Epoch 1473: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-04 09:14:14,142 EPOCH 1474
2024-02-04 09:14:16,311 [Epoch: 1474 Step: 00050100] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:     2659 || Batch Translation Loss:   0.010121 => Txt Tokens per Sec:     7196 || Lr: 0.000050
2024-02-04 09:14:18,543 Epoch 1474: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-04 09:14:18,544 EPOCH 1475
2024-02-04 09:14:23,394 Epoch 1475: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-04 09:14:23,395 EPOCH 1476
2024-02-04 09:14:27,791 Epoch 1476: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-04 09:14:27,792 EPOCH 1477
2024-02-04 09:14:29,994 [Epoch: 1477 Step: 00050200] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:     2213 || Batch Translation Loss:   0.010002 => Txt Tokens per Sec:     6534 || Lr: 0.000050
2024-02-04 09:14:32,240 Epoch 1477: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-04 09:14:32,241 EPOCH 1478
2024-02-04 09:14:36,950 Epoch 1478: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-04 09:14:36,950 EPOCH 1479
2024-02-04 09:14:41,052 Epoch 1479: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-04 09:14:41,053 EPOCH 1480
2024-02-04 09:14:42,945 [Epoch: 1480 Step: 00050300] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:     2235 || Batch Translation Loss:   0.010674 => Txt Tokens per Sec:     6563 || Lr: 0.000050
2024-02-04 09:14:45,136 Epoch 1480: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-04 09:14:45,136 EPOCH 1481
2024-02-04 09:14:50,013 Epoch 1481: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-04 09:14:50,013 EPOCH 1482
2024-02-04 09:14:54,331 Epoch 1482: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.21 
2024-02-04 09:14:54,332 EPOCH 1483
2024-02-04 09:14:55,913 [Epoch: 1483 Step: 00050400] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:     2431 || Batch Translation Loss:   0.012666 => Txt Tokens per Sec:     6523 || Lr: 0.000050
2024-02-04 09:14:59,278 Epoch 1483: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.47 
2024-02-04 09:14:59,278 EPOCH 1484
2024-02-04 09:15:03,545 Epoch 1484: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.27 
2024-02-04 09:15:03,545 EPOCH 1485
2024-02-04 09:15:08,467 Epoch 1485: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-04 09:15:08,467 EPOCH 1486
2024-02-04 09:15:09,670 [Epoch: 1486 Step: 00050500] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:     2666 || Batch Translation Loss:   0.028035 => Txt Tokens per Sec:     7604 || Lr: 0.000050
2024-02-04 09:15:12,913 Epoch 1486: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.13 
2024-02-04 09:15:12,914 EPOCH 1487
2024-02-04 09:15:17,764 Epoch 1487: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.81 
2024-02-04 09:15:17,764 EPOCH 1488
2024-02-04 09:15:22,240 Epoch 1488: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.46 
2024-02-04 09:15:22,241 EPOCH 1489
2024-02-04 09:15:23,361 [Epoch: 1489 Step: 00050600] Batch Recognition Loss:   0.000278 => Gls Tokens per Sec:     2288 || Batch Translation Loss:   0.040641 => Txt Tokens per Sec:     6771 || Lr: 0.000050
2024-02-04 09:15:26,950 Epoch 1489: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-04 09:15:26,950 EPOCH 1490
2024-02-04 09:15:31,518 Epoch 1490: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-04 09:15:31,519 EPOCH 1491
2024-02-04 09:15:36,209 Epoch 1491: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-04 09:15:36,209 EPOCH 1492
2024-02-04 09:15:37,100 [Epoch: 1492 Step: 00050700] Batch Recognition Loss:   0.000357 => Gls Tokens per Sec:     2160 || Batch Translation Loss:   0.031043 => Txt Tokens per Sec:     6798 || Lr: 0.000050
2024-02-04 09:15:40,970 Epoch 1492: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-04 09:15:40,970 EPOCH 1493
2024-02-04 09:15:45,318 Epoch 1493: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-04 09:15:45,318 EPOCH 1494
2024-02-04 09:15:49,670 Epoch 1494: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-04 09:15:49,671 EPOCH 1495
2024-02-04 09:15:50,374 [Epoch: 1495 Step: 00050800] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     1823 || Batch Translation Loss:   0.024982 => Txt Tokens per Sec:     5640 || Lr: 0.000050
2024-02-04 09:15:54,519 Epoch 1495: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-04 09:15:54,519 EPOCH 1496
2024-02-04 09:15:58,953 Epoch 1496: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-04 09:15:58,953 EPOCH 1497
2024-02-04 09:16:03,635 Epoch 1497: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-04 09:16:03,635 EPOCH 1498
2024-02-04 09:16:03,754 [Epoch: 1498 Step: 00050900] Batch Recognition Loss:   0.000113 => Gls Tokens per Sec:     5424 || Batch Translation Loss:   0.008548 => Txt Tokens per Sec:     9907 || Lr: 0.000050
2024-02-04 09:16:08,172 Epoch 1498: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-04 09:16:08,172 EPOCH 1499
2024-02-04 09:16:12,787 Epoch 1499: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-04 09:16:12,787 EPOCH 1500
2024-02-04 09:16:17,445 [Epoch: 1500 Step: 00051000] Batch Recognition Loss:   0.000178 => Gls Tokens per Sec:     2283 || Batch Translation Loss:   0.014173 => Txt Tokens per Sec:     6337 || Lr: 0.000050
2024-02-04 09:16:17,446 Epoch 1500: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-04 09:16:17,446 EPOCH 1501
2024-02-04 09:16:21,967 Epoch 1501: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-04 09:16:21,967 EPOCH 1502
2024-02-04 09:16:26,457 Epoch 1502: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-04 09:16:26,458 EPOCH 1503
2024-02-04 09:16:30,919 [Epoch: 1503 Step: 00051100] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:     2240 || Batch Translation Loss:   0.041289 => Txt Tokens per Sec:     6293 || Lr: 0.000050
2024-02-04 09:16:31,078 Epoch 1503: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-04 09:16:31,079 EPOCH 1504
2024-02-04 09:16:35,703 Epoch 1504: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-04 09:16:35,704 EPOCH 1505
2024-02-04 09:16:40,264 Epoch 1505: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-04 09:16:40,264 EPOCH 1506
2024-02-04 09:16:44,627 [Epoch: 1506 Step: 00051200] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:     2144 || Batch Translation Loss:   0.016461 => Txt Tokens per Sec:     6064 || Lr: 0.000050
2024-02-04 09:16:45,042 Epoch 1506: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-04 09:16:45,042 EPOCH 1507
2024-02-04 09:16:49,452 Epoch 1507: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-04 09:16:49,452 EPOCH 1508
2024-02-04 09:16:54,300 Epoch 1508: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-04 09:16:54,301 EPOCH 1509
2024-02-04 09:16:57,996 [Epoch: 1509 Step: 00051300] Batch Recognition Loss:   0.000232 => Gls Tokens per Sec:     2358 || Batch Translation Loss:   0.021650 => Txt Tokens per Sec:     6576 || Lr: 0.000050
2024-02-04 09:16:58,679 Epoch 1509: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-04 09:16:58,679 EPOCH 1510
2024-02-04 09:17:03,586 Epoch 1510: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-04 09:17:03,586 EPOCH 1511
2024-02-04 09:17:08,129 Epoch 1511: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-04 09:17:08,130 EPOCH 1512
2024-02-04 09:17:11,807 [Epoch: 1512 Step: 00051400] Batch Recognition Loss:   0.000276 => Gls Tokens per Sec:     2196 || Batch Translation Loss:   0.005332 => Txt Tokens per Sec:     6002 || Lr: 0.000050
2024-02-04 09:17:12,871 Epoch 1512: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-04 09:17:12,871 EPOCH 1513
2024-02-04 09:17:16,930 Epoch 1513: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-04 09:17:16,930 EPOCH 1514
2024-02-04 09:17:21,681 Epoch 1514: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.53 
2024-02-04 09:17:21,681 EPOCH 1515
2024-02-04 09:17:24,968 [Epoch: 1515 Step: 00051500] Batch Recognition Loss:   0.000219 => Gls Tokens per Sec:     2262 || Batch Translation Loss:   0.035958 => Txt Tokens per Sec:     6315 || Lr: 0.000050
2024-02-04 09:17:26,098 Epoch 1515: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.19 
2024-02-04 09:17:26,098 EPOCH 1516
2024-02-04 09:17:30,853 Epoch 1516: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.93 
2024-02-04 09:17:30,854 EPOCH 1517
2024-02-04 09:17:35,217 Epoch 1517: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.00 
2024-02-04 09:17:35,218 EPOCH 1518
2024-02-04 09:17:38,228 [Epoch: 1518 Step: 00051600] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     2256 || Batch Translation Loss:   0.039469 => Txt Tokens per Sec:     6084 || Lr: 0.000050
2024-02-04 09:17:40,084 Epoch 1518: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.70 
2024-02-04 09:17:40,085 EPOCH 1519
2024-02-04 09:17:44,405 Epoch 1519: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.82 
2024-02-04 09:17:44,406 EPOCH 1520
2024-02-04 09:17:49,365 Epoch 1520: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.28 
2024-02-04 09:17:49,365 EPOCH 1521
2024-02-04 09:17:51,522 [Epoch: 1521 Step: 00051700] Batch Recognition Loss:   0.000324 => Gls Tokens per Sec:     2971 || Batch Translation Loss:   0.029788 => Txt Tokens per Sec:     7917 || Lr: 0.000050
2024-02-04 09:17:53,647 Epoch 1521: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.29 
2024-02-04 09:17:53,648 EPOCH 1522
2024-02-04 09:17:58,715 Epoch 1522: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.50 
2024-02-04 09:17:58,715 EPOCH 1523
2024-02-04 09:18:03,642 Epoch 1523: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-04 09:18:03,643 EPOCH 1524
2024-02-04 09:18:05,958 [Epoch: 1524 Step: 00051800] Batch Recognition Loss:   0.000228 => Gls Tokens per Sec:     2489 || Batch Translation Loss:   0.023764 => Txt Tokens per Sec:     6747 || Lr: 0.000050
2024-02-04 09:18:08,455 Epoch 1524: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-04 09:18:08,456 EPOCH 1525
2024-02-04 09:18:13,199 Epoch 1525: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-04 09:18:13,200 EPOCH 1526
2024-02-04 09:18:17,574 Epoch 1526: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-04 09:18:17,574 EPOCH 1527
2024-02-04 09:18:19,420 [Epoch: 1527 Step: 00051900] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:     2775 || Batch Translation Loss:   0.018774 => Txt Tokens per Sec:     7619 || Lr: 0.000050
2024-02-04 09:18:22,227 Epoch 1527: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-04 09:18:22,227 EPOCH 1528
2024-02-04 09:18:26,809 Epoch 1528: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-04 09:18:26,809 EPOCH 1529
2024-02-04 09:18:31,533 Epoch 1529: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-04 09:18:31,534 EPOCH 1530
2024-02-04 09:18:33,463 [Epoch: 1530 Step: 00052000] Batch Recognition Loss:   0.000193 => Gls Tokens per Sec:     2194 || Batch Translation Loss:   0.008419 => Txt Tokens per Sec:     6473 || Lr: 0.000050
2024-02-04 09:18:42,173 Validation result at epoch 1530, step    52000: duration: 8.7105s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00080	Translation Loss: 95792.65625	PPL: 14559.16016
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.56	(BLEU-1: 10.11,	BLEU-2: 2.94,	BLEU-3: 1.13,	BLEU-4: 0.56)
	CHRF 16.56	ROUGE 8.68
2024-02-04 09:18:42,174 Logging Recognition and Translation Outputs
2024-02-04 09:18:42,174 ========================================================================================================================
2024-02-04 09:18:42,174 Logging Sequence: 87_229.00
2024-02-04 09:18:42,175 	Gloss Reference :	A B+C+D+E
2024-02-04 09:18:42,175 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:18:42,175 	Gloss Alignment :	         
2024-02-04 09:18:42,175 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:18:42,176 	Text Reference  :	*** **** it  was   not      against dhoni or      kohli
2024-02-04 09:18:42,176 	Text Hypothesis :	but then the delhi capitals are     being follwed dhoni
2024-02-04 09:18:42,176 	Text Alignment  :	I   I    S   S     S        S       S     S       S    
2024-02-04 09:18:42,176 ========================================================================================================================
2024-02-04 09:18:42,176 Logging Sequence: 134_153.00
2024-02-04 09:18:42,176 	Gloss Reference :	A B+C+D+E
2024-02-04 09:18:42,177 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:18:42,177 	Gloss Alignment :	         
2024-02-04 09:18:42,177 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:18:42,180 	Text Reference  :	pm    modi          in *** *********** **** his  interaction said  that deaf   athletes must fight  for their goals and     never   give up    despite the *** losses  
2024-02-04 09:18:42,180 	Text Hypothesis :	after participating in the deaflympics from 1965 the         india was  always at       the  bottom now they  have  created history by   being on      the 9th position
2024-02-04 09:18:42,180 	Text Alignment  :	S     S                I   I           I    S    S           S     S    S      S        S    S      S   S     S     S       S       S    S     S           I   S       
2024-02-04 09:18:42,180 ========================================================================================================================
2024-02-04 09:18:42,180 Logging Sequence: 137_155.00
2024-02-04 09:18:42,180 	Gloss Reference :	A B+C+D+E
2024-02-04 09:18:42,181 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:18:42,181 	Gloss Alignment :	         
2024-02-04 09:18:42,181 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:18:42,182 	Text Reference  :	** **** ***** **** ** an  extremely high  tax     named as  sin      tax will be applied
2024-02-04 09:18:42,182 	Text Hypothesis :	on 23rd march 2023 at the 1st       match between india and pakistan in  2022 a  cricket
2024-02-04 09:18:42,182 	Text Alignment  :	I  I    I     I    I  S   S         S     S       S     S   S        S   S    S  S      
2024-02-04 09:18:42,182 ========================================================================================================================
2024-02-04 09:18:42,182 Logging Sequence: 59_18.00
2024-02-04 09:18:42,183 	Gloss Reference :	A B+C+D+E
2024-02-04 09:18:42,183 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:18:42,183 	Gloss Alignment :	         
2024-02-04 09:18:42,183 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:18:42,184 	Text Reference  :	**** *** ********** 27-year-old jessica fox      from australia won    a   bronze a       gold medal in  canoeing
2024-02-04 09:18:42,184 	Text Hypothesis :	well the organisers of          the     olympics in   tokyo     handed out 60000  condoms to   all   the athletes
2024-02-04 09:18:42,185 	Text Alignment  :	I    I   I          S           S       S        S    S         S      S   S      S       S    S     S   S       
2024-02-04 09:18:42,185 ========================================================================================================================
2024-02-04 09:18:42,185 Logging Sequence: 173_103.00
2024-02-04 09:18:42,185 	Gloss Reference :	A B+C+D+E
2024-02-04 09:18:42,185 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:18:42,185 	Gloss Alignment :	         
2024-02-04 09:18:42,185 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:18:42,186 	Text Reference  :	** these rumours are absolutely rubbish
2024-02-04 09:18:42,186 	Text Hypothesis :	in ipl   india   is  very       worried
2024-02-04 09:18:42,186 	Text Alignment  :	I  S     S       S   S          S      
2024-02-04 09:18:42,186 ========================================================================================================================
2024-02-04 09:18:44,966 Epoch 1530: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-04 09:18:44,967 EPOCH 1531
2024-02-04 09:18:49,902 Epoch 1531: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-04 09:18:49,902 EPOCH 1532
2024-02-04 09:18:54,139 Epoch 1532: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-04 09:18:54,140 EPOCH 1533
2024-02-04 09:18:55,562 [Epoch: 1533 Step: 00052100] Batch Recognition Loss:   0.000104 => Gls Tokens per Sec:     2705 || Batch Translation Loss:   0.019091 => Txt Tokens per Sec:     6911 || Lr: 0.000050
2024-02-04 09:18:59,096 Epoch 1533: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-04 09:18:59,096 EPOCH 1534
2024-02-04 09:19:03,527 Epoch 1534: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-04 09:19:03,527 EPOCH 1535
2024-02-04 09:19:08,385 Epoch 1535: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-04 09:19:08,386 EPOCH 1536
2024-02-04 09:19:09,579 [Epoch: 1536 Step: 00052200] Batch Recognition Loss:   0.000105 => Gls Tokens per Sec:     2475 || Batch Translation Loss:   0.006704 => Txt Tokens per Sec:     6499 || Lr: 0.000050
2024-02-04 09:19:12,797 Epoch 1536: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-04 09:19:12,797 EPOCH 1537
2024-02-04 09:19:17,512 Epoch 1537: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-04 09:19:17,513 EPOCH 1538
2024-02-04 09:19:21,980 Epoch 1538: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-04 09:19:21,981 EPOCH 1539
2024-02-04 09:19:23,164 [Epoch: 1539 Step: 00052300] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:     1955 || Batch Translation Loss:   0.010117 => Txt Tokens per Sec:     5559 || Lr: 0.000050
2024-02-04 09:19:26,681 Epoch 1539: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-04 09:19:26,681 EPOCH 1540
2024-02-04 09:19:31,216 Epoch 1540: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-04 09:19:31,217 EPOCH 1541
2024-02-04 09:19:35,849 Epoch 1541: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-04 09:19:35,849 EPOCH 1542
2024-02-04 09:19:36,585 [Epoch: 1542 Step: 00052400] Batch Recognition Loss:   0.000178 => Gls Tokens per Sec:     2613 || Batch Translation Loss:   0.015166 => Txt Tokens per Sec:     7781 || Lr: 0.000050
2024-02-04 09:19:40,104 Epoch 1542: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-04 09:19:40,104 EPOCH 1543
2024-02-04 09:19:44,948 Epoch 1543: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-04 09:19:44,949 EPOCH 1544
2024-02-04 09:19:49,402 Epoch 1544: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-04 09:19:49,402 EPOCH 1545
2024-02-04 09:19:49,988 [Epoch: 1545 Step: 00052500] Batch Recognition Loss:   0.000270 => Gls Tokens per Sec:     2188 || Batch Translation Loss:   0.020761 => Txt Tokens per Sec:     6021 || Lr: 0.000050
2024-02-04 09:19:54,098 Epoch 1545: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-04 09:19:54,098 EPOCH 1546
2024-02-04 09:19:58,356 Epoch 1546: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-04 09:19:58,356 EPOCH 1547
2024-02-04 09:20:03,198 Epoch 1547: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-04 09:20:03,198 EPOCH 1548
2024-02-04 09:20:03,449 [Epoch: 1548 Step: 00052600] Batch Recognition Loss:   0.000388 => Gls Tokens per Sec:     2555 || Batch Translation Loss:   0.033364 => Txt Tokens per Sec:     7819 || Lr: 0.000050
2024-02-04 09:20:07,297 Epoch 1548: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-04 09:20:07,297 EPOCH 1549
2024-02-04 09:20:12,247 Epoch 1549: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-04 09:20:12,247 EPOCH 1550
2024-02-04 09:20:16,545 [Epoch: 1550 Step: 00052700] Batch Recognition Loss:   0.000288 => Gls Tokens per Sec:     2474 || Batch Translation Loss:   0.030286 => Txt Tokens per Sec:     6867 || Lr: 0.000050
2024-02-04 09:20:16,546 Epoch 1550: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-04 09:20:16,546 EPOCH 1551
2024-02-04 09:20:21,431 Epoch 1551: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-04 09:20:21,431 EPOCH 1552
2024-02-04 09:20:25,780 Epoch 1552: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.52 
2024-02-04 09:20:25,780 EPOCH 1553
2024-02-04 09:20:30,393 [Epoch: 1553 Step: 00052800] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     2167 || Batch Translation Loss:   0.019065 => Txt Tokens per Sec:     6015 || Lr: 0.000050
2024-02-04 09:20:30,683 Epoch 1553: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.81 
2024-02-04 09:20:30,684 EPOCH 1554
2024-02-04 09:20:35,080 Epoch 1554: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.67 
2024-02-04 09:20:35,081 EPOCH 1555
2024-02-04 09:20:39,879 Epoch 1555: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.76 
2024-02-04 09:20:39,879 EPOCH 1556
2024-02-04 09:20:44,032 [Epoch: 1556 Step: 00052900] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:     2252 || Batch Translation Loss:   0.008740 => Txt Tokens per Sec:     6268 || Lr: 0.000050
2024-02-04 09:20:44,510 Epoch 1556: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-04 09:20:44,511 EPOCH 1557
2024-02-04 09:20:48,993 Epoch 1557: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-04 09:20:48,993 EPOCH 1558
2024-02-04 09:20:53,101 Epoch 1558: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-04 09:20:53,102 EPOCH 1559
2024-02-04 09:20:57,262 [Epoch: 1559 Step: 00053000] Batch Recognition Loss:   0.000209 => Gls Tokens per Sec:     2094 || Batch Translation Loss:   0.031656 => Txt Tokens per Sec:     5764 || Lr: 0.000050
2024-02-04 09:20:58,087 Epoch 1559: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-04 09:20:58,088 EPOCH 1560
2024-02-04 09:21:02,361 Epoch 1560: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-04 09:21:02,361 EPOCH 1561
2024-02-04 09:21:07,255 Epoch 1561: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-04 09:21:07,255 EPOCH 1562
2024-02-04 09:21:10,630 [Epoch: 1562 Step: 00053100] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     2392 || Batch Translation Loss:   0.019052 => Txt Tokens per Sec:     6717 || Lr: 0.000050
2024-02-04 09:21:11,683 Epoch 1562: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-04 09:21:11,683 EPOCH 1563
2024-02-04 09:21:16,491 Epoch 1563: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-04 09:21:16,491 EPOCH 1564
2024-02-04 09:21:21,146 Epoch 1564: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-04 09:21:21,147 EPOCH 1565
2024-02-04 09:21:24,127 [Epoch: 1565 Step: 00053200] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:     2494 || Batch Translation Loss:   0.017735 => Txt Tokens per Sec:     6818 || Lr: 0.000050
2024-02-04 09:21:25,632 Epoch 1565: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-04 09:21:25,632 EPOCH 1566
2024-02-04 09:21:29,677 Epoch 1566: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-04 09:21:29,677 EPOCH 1567
2024-02-04 09:21:34,716 Epoch 1567: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-04 09:21:34,717 EPOCH 1568
2024-02-04 09:21:37,228 [Epoch: 1568 Step: 00053300] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     2705 || Batch Translation Loss:   0.052193 => Txt Tokens per Sec:     7496 || Lr: 0.000050
2024-02-04 09:21:39,086 Epoch 1568: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-04 09:21:39,086 EPOCH 1569
2024-02-04 09:21:43,914 Epoch 1569: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.66 
2024-02-04 09:21:43,914 EPOCH 1570
2024-02-04 09:21:48,371 Epoch 1570: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-04 09:21:48,372 EPOCH 1571
2024-02-04 09:21:51,127 [Epoch: 1571 Step: 00053400] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     2233 || Batch Translation Loss:   0.012650 => Txt Tokens per Sec:     6172 || Lr: 0.000050
2024-02-04 09:21:53,092 Epoch 1571: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-04 09:21:53,092 EPOCH 1572
2024-02-04 09:21:57,637 Epoch 1572: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-04 09:21:57,637 EPOCH 1573
2024-02-04 09:22:02,373 Epoch 1573: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-04 09:22:02,373 EPOCH 1574
2024-02-04 09:22:04,259 [Epoch: 1574 Step: 00053500] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     3055 || Batch Translation Loss:   0.015853 => Txt Tokens per Sec:     8288 || Lr: 0.000050
2024-02-04 09:22:06,951 Epoch 1574: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-04 09:22:06,951 EPOCH 1575
2024-02-04 09:22:11,539 Epoch 1575: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-04 09:22:11,539 EPOCH 1576
2024-02-04 09:22:16,184 Epoch 1576: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-04 09:22:16,185 EPOCH 1577
2024-02-04 09:22:18,555 [Epoch: 1577 Step: 00053600] Batch Recognition Loss:   0.000152 => Gls Tokens per Sec:     2161 || Batch Translation Loss:   0.016842 => Txt Tokens per Sec:     6125 || Lr: 0.000050
2024-02-04 09:22:20,694 Epoch 1577: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-04 09:22:20,694 EPOCH 1578
2024-02-04 09:22:25,509 Epoch 1578: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-04 09:22:25,509 EPOCH 1579
2024-02-04 09:22:29,859 Epoch 1579: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-04 09:22:29,859 EPOCH 1580
2024-02-04 09:22:32,078 [Epoch: 1580 Step: 00053700] Batch Recognition Loss:   0.000152 => Gls Tokens per Sec:     1908 || Batch Translation Loss:   0.013187 => Txt Tokens per Sec:     5686 || Lr: 0.000050
2024-02-04 09:22:34,709 Epoch 1580: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-04 09:22:34,709 EPOCH 1581
2024-02-04 09:22:39,068 Epoch 1581: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-04 09:22:39,068 EPOCH 1582
2024-02-04 09:22:44,006 Epoch 1582: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-04 09:22:44,007 EPOCH 1583
2024-02-04 09:22:45,297 [Epoch: 1583 Step: 00053800] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:     2786 || Batch Translation Loss:   0.027580 => Txt Tokens per Sec:     7210 || Lr: 0.000050
2024-02-04 09:22:48,351 Epoch 1583: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.62 
2024-02-04 09:22:48,351 EPOCH 1584
2024-02-04 09:22:53,223 Epoch 1584: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-04 09:22:53,224 EPOCH 1585
2024-02-04 09:22:58,151 Epoch 1585: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-04 09:22:58,152 EPOCH 1586
2024-02-04 09:22:59,491 [Epoch: 1586 Step: 00053900] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     2391 || Batch Translation Loss:   0.024409 => Txt Tokens per Sec:     6765 || Lr: 0.000050
2024-02-04 09:23:02,623 Epoch 1586: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-04 09:23:02,623 EPOCH 1587
2024-02-04 09:23:07,524 Epoch 1587: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-04 09:23:07,525 EPOCH 1588
2024-02-04 09:23:12,382 Epoch 1588: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.15 
2024-02-04 09:23:12,383 EPOCH 1589
2024-02-04 09:23:13,585 [Epoch: 1589 Step: 00054000] Batch Recognition Loss:   0.000116 => Gls Tokens per Sec:     2132 || Batch Translation Loss:   0.023684 => Txt Tokens per Sec:     6156 || Lr: 0.000050
2024-02-04 09:23:21,914 Validation result at epoch 1589, step    54000: duration: 8.3290s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00100	Translation Loss: 96662.27344	PPL: 15882.90137
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.71	(BLEU-1: 10.38,	BLEU-2: 3.27,	BLEU-3: 1.45,	BLEU-4: 0.71)
	CHRF 16.75	ROUGE 9.28
2024-02-04 09:23:21,915 Logging Recognition and Translation Outputs
2024-02-04 09:23:21,915 ========================================================================================================================
2024-02-04 09:23:21,915 Logging Sequence: 130_139.00
2024-02-04 09:23:21,916 	Gloss Reference :	A B+C+D+E
2024-02-04 09:23:21,916 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:23:21,916 	Gloss Alignment :	         
2024-02-04 09:23:21,916 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:23:21,919 	Text Reference  :	he shared a picture of a little pouch he  knit for his olympic  gold medal with   uk   flag    on  one  side  and japanese flag   on   the   other   
2024-02-04 09:23:21,919 	Text Hypothesis :	** ****** * ******* ** * ****** ***** but when i   am  stepping down as    india' test captain but then viral it  old      images from tokyo olympics
2024-02-04 09:23:21,919 	Text Alignment  :	D  D      D D       D  D D      D     S   S    S   S   S        S    S     S      S    S       S   S    S     S   S        S      S    S     S       
2024-02-04 09:23:21,919 ========================================================================================================================
2024-02-04 09:23:21,920 Logging Sequence: 148_155.00
2024-02-04 09:23:21,920 	Gloss Reference :	A B+C+D+E
2024-02-04 09:23:21,920 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:23:21,920 	Gloss Alignment :	         
2024-02-04 09:23:21,920 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:23:21,922 	Text Reference  :	india *** *** **** * *** won   the match ******* with  263 balls    remaining and  without losing any       wicket   
2024-02-04 09:23:21,922 	Text Hypothesis :	india did not have a t20 world cup match between india and pakistan will      play an      edited yesterday yesterday
2024-02-04 09:23:21,922 	Text Alignment  :	      I   I   I    I I   S     S         I       S     S   S        S         S    S       S      S         S        
2024-02-04 09:23:21,922 ========================================================================================================================
2024-02-04 09:23:21,922 Logging Sequence: 126_99.00
2024-02-04 09:23:21,922 	Gloss Reference :	A B+C+D+E
2024-02-04 09:23:21,922 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:23:21,923 	Gloss Alignment :	         
2024-02-04 09:23:21,923 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:23:21,924 	Text Reference  :	*** he    dedicated the medal   to **** *** * ***** ******** sprinter milkha singh
2024-02-04 09:23:21,924 	Text Hypothesis :	now let's wait      for updates to play had a chief minister n        biren  said 
2024-02-04 09:23:21,924 	Text Alignment  :	I   S     S         S   S          I    I   I I     I        S        S      S    
2024-02-04 09:23:21,924 ========================================================================================================================
2024-02-04 09:23:21,924 Logging Sequence: 149_77.00
2024-02-04 09:23:21,924 	Gloss Reference :	A B+C+D+E
2024-02-04 09:23:21,924 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:23:21,925 	Gloss Alignment :	         
2024-02-04 09:23:21,925 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:23:21,926 	Text Reference  :	and arrested danushka for   alleged sexual assault of       a   29     year        old  woman whose   name has     not   been  disclosed
2024-02-04 09:23:21,927 	Text Hypothesis :	*** ******** a        woman alleged ****** that    danushka had sexual intercourse with her   without her  consent which means rape     
2024-02-04 09:23:21,927 	Text Alignment  :	D   D        S        S             D      S       S        S   S      S           S    S     S       S    S       S     S     S        
2024-02-04 09:23:21,927 ========================================================================================================================
2024-02-04 09:23:21,927 Logging Sequence: 168_15.00
2024-02-04 09:23:21,927 	Gloss Reference :	A B+C+D+E
2024-02-04 09:23:21,927 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:23:21,927 	Gloss Alignment :	         
2024-02-04 09:23:21,927 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:23:21,928 	Text Reference  :	when ***** in     public the couple are  always approached for   photographys and autographs
2024-02-04 09:23:21,929 	Text Hypothesis :	when rohit sharma asked  the indian team was    fined      their names        of  their     
2024-02-04 09:23:21,929 	Text Alignment  :	     I     S      S          S      S    S      S          S     S            S   S         
2024-02-04 09:23:21,929 ========================================================================================================================
2024-02-04 09:23:25,678 Epoch 1589: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.72 
2024-02-04 09:23:25,679 EPOCH 1590
2024-02-04 09:23:30,510 Epoch 1590: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-04 09:23:30,510 EPOCH 1591
2024-02-04 09:23:35,098 Epoch 1591: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-04 09:23:35,099 EPOCH 1592
2024-02-04 09:23:35,952 [Epoch: 1592 Step: 00054100] Batch Recognition Loss:   0.000288 => Gls Tokens per Sec:     2256 || Batch Translation Loss:   0.021683 => Txt Tokens per Sec:     6331 || Lr: 0.000050
2024-02-04 09:23:39,718 Epoch 1592: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-04 09:23:39,718 EPOCH 1593
2024-02-04 09:23:43,756 Epoch 1593: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-04 09:23:43,756 EPOCH 1594
2024-02-04 09:23:48,722 Epoch 1594: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-04 09:23:48,723 EPOCH 1595
2024-02-04 09:23:49,333 [Epoch: 1595 Step: 00054200] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:     2105 || Batch Translation Loss:   0.017916 => Txt Tokens per Sec:     6553 || Lr: 0.000050
2024-02-04 09:23:53,117 Epoch 1595: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-04 09:23:53,117 EPOCH 1596
2024-02-04 09:23:57,968 Epoch 1596: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-04 09:23:57,969 EPOCH 1597
2024-02-04 09:24:02,358 Epoch 1597: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-04 09:24:02,359 EPOCH 1598
2024-02-04 09:24:02,662 [Epoch: 1598 Step: 00054300] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     2119 || Batch Translation Loss:   0.017494 => Txt Tokens per Sec:     5444 || Lr: 0.000050
2024-02-04 09:24:07,199 Epoch 1598: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-04 09:24:07,199 EPOCH 1599
2024-02-04 09:24:11,561 Epoch 1599: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.72 
2024-02-04 09:24:11,562 EPOCH 1600
2024-02-04 09:24:16,314 [Epoch: 1600 Step: 00054400] Batch Recognition Loss:   0.000297 => Gls Tokens per Sec:     2238 || Batch Translation Loss:   0.116074 => Txt Tokens per Sec:     6212 || Lr: 0.000050
2024-02-04 09:24:16,314 Epoch 1600: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.69 
2024-02-04 09:24:16,314 EPOCH 1601
2024-02-04 09:24:20,883 Epoch 1601: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.22 
2024-02-04 09:24:20,884 EPOCH 1602
2024-02-04 09:24:25,565 Epoch 1602: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.49 
2024-02-04 09:24:25,565 EPOCH 1603
2024-02-04 09:24:30,062 [Epoch: 1603 Step: 00054500] Batch Recognition Loss:   0.000412 => Gls Tokens per Sec:     2222 || Batch Translation Loss:   0.029336 => Txt Tokens per Sec:     6184 || Lr: 0.000050
2024-02-04 09:24:30,268 Epoch 1603: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-04 09:24:30,268 EPOCH 1604
2024-02-04 09:24:34,772 Epoch 1604: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-04 09:24:34,773 EPOCH 1605
2024-02-04 09:24:39,075 Epoch 1605: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-04 09:24:39,076 EPOCH 1606
2024-02-04 09:24:43,506 [Epoch: 1606 Step: 00054600] Batch Recognition Loss:   0.000390 => Gls Tokens per Sec:     2111 || Batch Translation Loss:   0.027329 => Txt Tokens per Sec:     5909 || Lr: 0.000050
2024-02-04 09:24:43,940 Epoch 1606: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-04 09:24:43,940 EPOCH 1607
2024-02-04 09:24:48,360 Epoch 1607: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-04 09:24:48,361 EPOCH 1608
2024-02-04 09:24:53,137 Epoch 1608: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-04 09:24:53,137 EPOCH 1609
2024-02-04 09:24:56,897 [Epoch: 1609 Step: 00054700] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     2317 || Batch Translation Loss:   0.015676 => Txt Tokens per Sec:     6385 || Lr: 0.000050
2024-02-04 09:24:57,658 Epoch 1609: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-04 09:24:57,658 EPOCH 1610
2024-02-04 09:25:02,276 Epoch 1610: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-04 09:25:02,276 EPOCH 1611
2024-02-04 09:25:06,874 Epoch 1611: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-04 09:25:06,875 EPOCH 1612
2024-02-04 09:25:10,246 [Epoch: 1612 Step: 00054800] Batch Recognition Loss:   0.000122 => Gls Tokens per Sec:     2395 || Batch Translation Loss:   0.011235 => Txt Tokens per Sec:     6434 || Lr: 0.000050
2024-02-04 09:25:11,528 Epoch 1612: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-04 09:25:11,528 EPOCH 1613
2024-02-04 09:25:15,918 Epoch 1613: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-04 09:25:15,919 EPOCH 1614
2024-02-04 09:25:20,652 Epoch 1614: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-04 09:25:20,652 EPOCH 1615
2024-02-04 09:25:23,545 [Epoch: 1615 Step: 00054900] Batch Recognition Loss:   0.000113 => Gls Tokens per Sec:     2655 || Batch Translation Loss:   0.012189 => Txt Tokens per Sec:     7338 || Lr: 0.000050
2024-02-04 09:25:25,176 Epoch 1615: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-04 09:25:25,177 EPOCH 1616
2024-02-04 09:25:30,066 Epoch 1616: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-04 09:25:30,067 EPOCH 1617
2024-02-04 09:25:34,864 Epoch 1617: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-04 09:25:34,864 EPOCH 1618
2024-02-04 09:25:37,882 [Epoch: 1618 Step: 00055000] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:     2333 || Batch Translation Loss:   0.017972 => Txt Tokens per Sec:     6255 || Lr: 0.000050
2024-02-04 09:25:39,652 Epoch 1618: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-04 09:25:39,652 EPOCH 1619
2024-02-04 09:25:44,436 Epoch 1619: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-04 09:25:44,437 EPOCH 1620
2024-02-04 09:25:48,710 Epoch 1620: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-04 09:25:48,710 EPOCH 1621
2024-02-04 09:25:51,584 [Epoch: 1621 Step: 00055100] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     2141 || Batch Translation Loss:   0.034408 => Txt Tokens per Sec:     5890 || Lr: 0.000050
2024-02-04 09:25:53,561 Epoch 1621: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-04 09:25:53,561 EPOCH 1622
2024-02-04 09:25:57,838 Epoch 1622: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-04 09:25:57,838 EPOCH 1623
2024-02-04 09:26:02,823 Epoch 1623: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.16 
2024-02-04 09:26:02,824 EPOCH 1624
2024-02-04 09:26:05,210 [Epoch: 1624 Step: 00055200] Batch Recognition Loss:   0.000193 => Gls Tokens per Sec:     2415 || Batch Translation Loss:   0.020634 => Txt Tokens per Sec:     6830 || Lr: 0.000050
2024-02-04 09:26:07,125 Epoch 1624: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-04 09:26:07,125 EPOCH 1625
2024-02-04 09:26:12,023 Epoch 1625: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-04 09:26:12,024 EPOCH 1626
2024-02-04 09:26:16,471 Epoch 1626: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-04 09:26:16,472 EPOCH 1627
2024-02-04 09:26:18,719 [Epoch: 1627 Step: 00055300] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:     2279 || Batch Translation Loss:   0.016623 => Txt Tokens per Sec:     6355 || Lr: 0.000050
2024-02-04 09:26:21,268 Epoch 1627: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-04 09:26:21,268 EPOCH 1628
2024-02-04 09:26:25,670 Epoch 1628: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-04 09:26:25,671 EPOCH 1629
2024-02-04 09:26:30,384 Epoch 1629: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-04 09:26:30,385 EPOCH 1630
2024-02-04 09:26:31,872 [Epoch: 1630 Step: 00055400] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:     3015 || Batch Translation Loss:   0.035101 => Txt Tokens per Sec:     8205 || Lr: 0.000050
2024-02-04 09:26:34,965 Epoch 1630: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-04 09:26:34,965 EPOCH 1631
2024-02-04 09:26:39,581 Epoch 1631: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-04 09:26:39,581 EPOCH 1632
2024-02-04 09:26:44,205 Epoch 1632: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-04 09:26:44,206 EPOCH 1633
2024-02-04 09:26:46,262 [Epoch: 1633 Step: 00055500] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:     1747 || Batch Translation Loss:   0.019717 => Txt Tokens per Sec:     5163 || Lr: 0.000050
2024-02-04 09:26:48,686 Epoch 1633: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-04 09:26:48,686 EPOCH 1634
2024-02-04 09:26:53,042 Epoch 1634: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-04 09:26:53,043 EPOCH 1635
2024-02-04 09:26:57,881 Epoch 1635: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-04 09:26:57,881 EPOCH 1636
2024-02-04 09:26:58,886 [Epoch: 1636 Step: 00055600] Batch Recognition Loss:   0.000118 => Gls Tokens per Sec:     3190 || Batch Translation Loss:   0.144585 => Txt Tokens per Sec:     8405 || Lr: 0.000050
2024-02-04 09:27:02,297 Epoch 1636: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-04 09:27:02,297 EPOCH 1637
2024-02-04 09:27:06,987 Epoch 1637: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-04 09:27:06,987 EPOCH 1638
2024-02-04 09:27:11,468 Epoch 1638: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-04 09:27:11,469 EPOCH 1639
2024-02-04 09:27:12,465 [Epoch: 1639 Step: 00055700] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     2573 || Batch Translation Loss:   0.009286 => Txt Tokens per Sec:     6661 || Lr: 0.000050
2024-02-04 09:27:16,083 Epoch 1639: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-04 09:27:16,083 EPOCH 1640
2024-02-04 09:27:20,790 Epoch 1640: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-04 09:27:20,791 EPOCH 1641
2024-02-04 09:27:25,275 Epoch 1641: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.21 
2024-02-04 09:27:25,275 EPOCH 1642
2024-02-04 09:27:25,745 [Epoch: 1642 Step: 00055800] Batch Recognition Loss:   0.000189 => Gls Tokens per Sec:     4093 || Batch Translation Loss:   0.029991 => Txt Tokens per Sec:     9975 || Lr: 0.000050
2024-02-04 09:27:30,007 Epoch 1642: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-04 09:27:30,007 EPOCH 1643
2024-02-04 09:27:34,477 Epoch 1643: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.36 
2024-02-04 09:27:34,478 EPOCH 1644
2024-02-04 09:27:39,271 Epoch 1644: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.10 
2024-02-04 09:27:39,271 EPOCH 1645
2024-02-04 09:27:39,611 [Epoch: 1645 Step: 00055900] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:     3787 || Batch Translation Loss:   0.140248 => Txt Tokens per Sec:     8926 || Lr: 0.000050
2024-02-04 09:27:43,668 Epoch 1645: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.91 
2024-02-04 09:27:43,668 EPOCH 1646
2024-02-04 09:27:48,649 Epoch 1646: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.64 
2024-02-04 09:27:48,649 EPOCH 1647
2024-02-04 09:27:53,213 Epoch 1647: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-04 09:27:53,213 EPOCH 1648
2024-02-04 09:27:53,492 [Epoch: 1648 Step: 00056000] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     2302 || Batch Translation Loss:   0.021430 => Txt Tokens per Sec:     6187 || Lr: 0.000050
2024-02-04 09:28:01,815 Validation result at epoch 1648, step    56000: duration: 8.3230s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00124	Translation Loss: 97382.24219	PPL: 17069.45117
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.51	(BLEU-1: 9.83,	BLEU-2: 2.96,	BLEU-3: 1.16,	BLEU-4: 0.51)
	CHRF 16.42	ROUGE 8.78
2024-02-04 09:28:01,816 Logging Recognition and Translation Outputs
2024-02-04 09:28:01,816 ========================================================================================================================
2024-02-04 09:28:01,816 Logging Sequence: 122_110.00
2024-02-04 09:28:01,816 	Gloss Reference :	A B+C+D+E
2024-02-04 09:28:01,817 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:28:01,817 	Gloss Alignment :	         
2024-02-04 09:28:01,817 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:28:01,818 	Text Reference  :	** *** **** *** now     that i   achieved my      dream and secured a   silver medal
2024-02-04 09:28:01,818 	Text Hypothesis :	if she does not respond then the match    however this  is  1st     may be     held 
2024-02-04 09:28:01,818 	Text Alignment  :	I  I   I    I   S       S    S   S        S       S     S   S       S   S      S    
2024-02-04 09:28:01,818 ========================================================================================================================
2024-02-04 09:28:01,818 Logging Sequence: 161_111.00
2024-02-04 09:28:01,819 	Gloss Reference :	A B+C+D+E
2024-02-04 09:28:01,819 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:28:01,819 	Gloss Alignment :	         
2024-02-04 09:28:01,819 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:28:01,820 	Text Reference  :	his last game as   captain was the     cape town test in  south  africa  in jan 2022   
2024-02-04 09:28:01,820 	Text Hypothesis :	*** **** the  bcci posted  a   picture of   csk  for  the sharma instead of csk captain
2024-02-04 09:28:01,820 	Text Alignment  :	D   D    S    S    S       S   S       S    S    S    S   S      S       S  S   S      
2024-02-04 09:28:01,821 ========================================================================================================================
2024-02-04 09:28:01,821 Logging Sequence: 136_79.00
2024-02-04 09:28:01,821 	Gloss Reference :	A B+C+D+E
2024-02-04 09:28:01,821 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:28:01,821 	Gloss Alignment :	         
2024-02-04 09:28:01,821 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:28:01,822 	Text Reference  :	with this win sindhu became the first indian woman     to ******* win  two    individual olympic medals
2024-02-04 09:28:01,822 	Text Hypothesis :	**** **** *** ****** ****** *** ***** ****** according to rumours that sushil kumar      and     loss  
2024-02-04 09:28:01,822 	Text Alignment  :	D    D    D   D      D      D   D     D      S            I       S    S      S          S       S     
2024-02-04 09:28:01,822 ========================================================================================================================
2024-02-04 09:28:01,823 Logging Sequence: 166_335.00
2024-02-04 09:28:01,823 	Gloss Reference :	A B+C+D+E
2024-02-04 09:28:01,823 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:28:01,823 	Gloss Alignment :	         
2024-02-04 09:28:01,823 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:28:01,824 	Text Reference  :	the  second world test championship is scheduled from          june 2021 to    30  april 2023
2024-02-04 09:28:01,824 	Text Hypothesis :	many of     you   may  believe      in such      superstitions and  many won't its upto  you 
2024-02-04 09:28:01,825 	Text Alignment  :	S    S      S     S    S            S  S         S             S    S    S     S   S     S   
2024-02-04 09:28:01,825 ========================================================================================================================
2024-02-04 09:28:01,825 Logging Sequence: 95_152.00
2024-02-04 09:28:01,825 	Gloss Reference :	A B+C+D+E
2024-02-04 09:28:01,825 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:28:01,825 	Gloss Alignment :	         
2024-02-04 09:28:01,825 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:28:01,826 	Text Reference  :	**** *** ** ** **** ** * how     strange
2024-02-04 09:28:01,826 	Text Hypothesis :	they had to be sent in a similar way    
2024-02-04 09:28:01,826 	Text Alignment  :	I    I   I  I  I    I  I S       S      
2024-02-04 09:28:01,826 ========================================================================================================================
2024-02-04 09:28:06,725 Epoch 1648: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-04 09:28:06,726 EPOCH 1649
2024-02-04 09:28:11,477 Epoch 1649: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-04 09:28:11,478 EPOCH 1650
2024-02-04 09:28:15,526 [Epoch: 1650 Step: 00056100] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     2626 || Batch Translation Loss:   0.023033 => Txt Tokens per Sec:     7290 || Lr: 0.000050
2024-02-04 09:28:15,526 Epoch 1650: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-04 09:28:15,527 EPOCH 1651
2024-02-04 09:28:20,289 Epoch 1651: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-04 09:28:20,289 EPOCH 1652
2024-02-04 09:28:24,635 Epoch 1652: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-04 09:28:24,635 EPOCH 1653
2024-02-04 09:28:28,819 [Epoch: 1653 Step: 00056200] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     2388 || Batch Translation Loss:   0.021234 => Txt Tokens per Sec:     6678 || Lr: 0.000050
2024-02-04 09:28:29,178 Epoch 1653: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-04 09:28:29,178 EPOCH 1654
2024-02-04 09:28:33,776 Epoch 1654: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-04 09:28:33,776 EPOCH 1655
2024-02-04 09:28:38,363 Epoch 1655: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-04 09:28:38,363 EPOCH 1656
2024-02-04 09:28:42,546 [Epoch: 1656 Step: 00056300] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     2236 || Batch Translation Loss:   0.018037 => Txt Tokens per Sec:     6245 || Lr: 0.000050
2024-02-04 09:28:42,933 Epoch 1656: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-04 09:28:42,933 EPOCH 1657
2024-02-04 09:28:47,510 Epoch 1657: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-04 09:28:47,511 EPOCH 1658
2024-02-04 09:28:52,103 Epoch 1658: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-04 09:28:52,103 EPOCH 1659
2024-02-04 09:28:55,735 [Epoch: 1659 Step: 00056400] Batch Recognition Loss:   0.000282 => Gls Tokens per Sec:     2399 || Batch Translation Loss:   0.046896 => Txt Tokens per Sec:     6587 || Lr: 0.000050
2024-02-04 09:28:56,774 Epoch 1659: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-04 09:28:56,774 EPOCH 1660
2024-02-04 09:29:01,347 Epoch 1660: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-04 09:29:01,347 EPOCH 1661
2024-02-04 09:29:06,081 Epoch 1661: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-04 09:29:06,082 EPOCH 1662
2024-02-04 09:29:09,558 [Epoch: 1662 Step: 00056500] Batch Recognition Loss:   0.000109 => Gls Tokens per Sec:     2323 || Batch Translation Loss:   0.016712 => Txt Tokens per Sec:     6437 || Lr: 0.000050
2024-02-04 09:29:10,491 Epoch 1662: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-04 09:29:10,491 EPOCH 1663
2024-02-04 09:29:15,288 Epoch 1663: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-04 09:29:15,288 EPOCH 1664
2024-02-04 09:29:19,644 Epoch 1664: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-04 09:29:19,644 EPOCH 1665
2024-02-04 09:29:23,025 [Epoch: 1665 Step: 00056600] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:     2199 || Batch Translation Loss:   0.021153 => Txt Tokens per Sec:     5886 || Lr: 0.000050
2024-02-04 09:29:24,562 Epoch 1665: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-04 09:29:24,563 EPOCH 1666
2024-02-04 09:29:28,824 Epoch 1666: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-04 09:29:28,824 EPOCH 1667
2024-02-04 09:29:33,786 Epoch 1667: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-04 09:29:33,786 EPOCH 1668
2024-02-04 09:29:36,485 [Epoch: 1668 Step: 00056700] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:     2610 || Batch Translation Loss:   0.059473 => Txt Tokens per Sec:     7180 || Lr: 0.000050
2024-02-04 09:29:38,129 Epoch 1668: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.46 
2024-02-04 09:29:38,130 EPOCH 1669
2024-02-04 09:29:43,044 Epoch 1669: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.38 
2024-02-04 09:29:43,045 EPOCH 1670
2024-02-04 09:29:47,636 Epoch 1670: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.33 
2024-02-04 09:29:47,636 EPOCH 1671
2024-02-04 09:29:50,439 [Epoch: 1671 Step: 00056800] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:     2196 || Batch Translation Loss:   0.015159 => Txt Tokens per Sec:     6152 || Lr: 0.000050
2024-02-04 09:29:52,244 Epoch 1671: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.73 
2024-02-04 09:29:52,244 EPOCH 1672
2024-02-04 09:29:56,262 Epoch 1672: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-04 09:29:56,262 EPOCH 1673
2024-02-04 09:30:01,119 Epoch 1673: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-04 09:30:01,120 EPOCH 1674
2024-02-04 09:30:03,632 [Epoch: 1674 Step: 00056900] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:     2294 || Batch Translation Loss:   0.020346 => Txt Tokens per Sec:     6486 || Lr: 0.000050
2024-02-04 09:30:05,464 Epoch 1674: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-04 09:30:05,464 EPOCH 1675
2024-02-04 09:30:10,405 Epoch 1675: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-04 09:30:10,406 EPOCH 1676
2024-02-04 09:30:14,740 Epoch 1676: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-04 09:30:14,741 EPOCH 1677
2024-02-04 09:30:17,502 [Epoch: 1677 Step: 00057000] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     1855 || Batch Translation Loss:   0.020323 => Txt Tokens per Sec:     5497 || Lr: 0.000050
2024-02-04 09:30:19,619 Epoch 1677: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-04 09:30:19,619 EPOCH 1678
2024-02-04 09:30:23,945 Epoch 1678: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.36 
2024-02-04 09:30:23,946 EPOCH 1679
2024-02-04 09:30:28,826 Epoch 1679: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.16 
2024-02-04 09:30:28,827 EPOCH 1680
2024-02-04 09:30:30,574 [Epoch: 1680 Step: 00057100] Batch Recognition Loss:   0.000284 => Gls Tokens per Sec:     2565 || Batch Translation Loss:   0.029982 => Txt Tokens per Sec:     6931 || Lr: 0.000050
2024-02-04 09:30:33,321 Epoch 1680: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-04 09:30:33,322 EPOCH 1681
2024-02-04 09:30:38,016 Epoch 1681: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-04 09:30:38,016 EPOCH 1682
2024-02-04 09:30:42,530 Epoch 1682: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.11 
2024-02-04 09:30:42,530 EPOCH 1683
2024-02-04 09:30:44,027 [Epoch: 1683 Step: 00057200] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:     2399 || Batch Translation Loss:   0.015542 => Txt Tokens per Sec:     6380 || Lr: 0.000050
2024-02-04 09:30:47,193 Epoch 1683: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-04 09:30:47,193 EPOCH 1684
2024-02-04 09:30:51,809 Epoch 1684: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-04 09:30:51,810 EPOCH 1685
2024-02-04 09:30:56,349 Epoch 1685: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-04 09:30:56,349 EPOCH 1686
2024-02-04 09:30:57,468 [Epoch: 1686 Step: 00057300] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:     2864 || Batch Translation Loss:   0.130840 => Txt Tokens per Sec:     7893 || Lr: 0.000050
2024-02-04 09:31:01,013 Epoch 1686: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-04 09:31:01,013 EPOCH 1687
2024-02-04 09:31:05,847 Epoch 1687: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-04 09:31:05,847 EPOCH 1688
2024-02-04 09:31:10,761 Epoch 1688: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-04 09:31:10,762 EPOCH 1689
2024-02-04 09:31:11,747 [Epoch: 1689 Step: 00057400] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:     2603 || Batch Translation Loss:   0.082656 => Txt Tokens per Sec:     7273 || Lr: 0.000050
2024-02-04 09:31:15,667 Epoch 1689: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-04 09:31:15,668 EPOCH 1690
2024-02-04 09:31:20,446 Epoch 1690: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-04 09:31:20,447 EPOCH 1691
2024-02-04 09:31:24,508 Epoch 1691: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-04 09:31:24,508 EPOCH 1692
2024-02-04 09:31:25,461 [Epoch: 1692 Step: 00057500] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     2016 || Batch Translation Loss:   0.028290 => Txt Tokens per Sec:     6138 || Lr: 0.000050
2024-02-04 09:31:29,306 Epoch 1692: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-04 09:31:29,307 EPOCH 1693
2024-02-04 09:31:34,212 Epoch 1693: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-04 09:31:34,212 EPOCH 1694
2024-02-04 09:31:38,521 Epoch 1694: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-04 09:31:38,521 EPOCH 1695
2024-02-04 09:31:39,143 [Epoch: 1695 Step: 00057600] Batch Recognition Loss:   0.000273 => Gls Tokens per Sec:     2066 || Batch Translation Loss:   0.062294 => Txt Tokens per Sec:     5779 || Lr: 0.000050
2024-02-04 09:31:43,344 Epoch 1695: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-04 09:31:43,345 EPOCH 1696
2024-02-04 09:31:47,846 Epoch 1696: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-04 09:31:47,847 EPOCH 1697
2024-02-04 09:31:52,497 Epoch 1697: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-04 09:31:52,498 EPOCH 1698
2024-02-04 09:31:52,796 [Epoch: 1698 Step: 00057700] Batch Recognition Loss:   0.000281 => Gls Tokens per Sec:     2152 || Batch Translation Loss:   0.042120 => Txt Tokens per Sec:     6594 || Lr: 0.000050
2024-02-04 09:31:57,063 Epoch 1698: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-04 09:31:57,064 EPOCH 1699
2024-02-04 09:32:01,726 Epoch 1699: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-04 09:32:01,726 EPOCH 1700
2024-02-04 09:32:06,589 [Epoch: 1700 Step: 00057800] Batch Recognition Loss:   0.000115 => Gls Tokens per Sec:     2187 || Batch Translation Loss:   0.032619 => Txt Tokens per Sec:     6071 || Lr: 0.000050
2024-02-04 09:32:06,589 Epoch 1700: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-04 09:32:06,589 EPOCH 1701
2024-02-04 09:32:10,897 Epoch 1701: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.50 
2024-02-04 09:32:10,897 EPOCH 1702
2024-02-04 09:32:15,195 Epoch 1702: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.65 
2024-02-04 09:32:15,196 EPOCH 1703
2024-02-04 09:32:19,704 [Epoch: 1703 Step: 00057900] Batch Recognition Loss:   0.000292 => Gls Tokens per Sec:     2216 || Batch Translation Loss:   0.030318 => Txt Tokens per Sec:     6140 || Lr: 0.000050
2024-02-04 09:32:19,960 Epoch 1703: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-04 09:32:19,960 EPOCH 1704
2024-02-04 09:32:24,437 Epoch 1704: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-04 09:32:24,438 EPOCH 1705
2024-02-04 09:32:29,147 Epoch 1705: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-04 09:32:29,147 EPOCH 1706
2024-02-04 09:32:33,125 [Epoch: 1706 Step: 00058000] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:     2351 || Batch Translation Loss:   0.014318 => Txt Tokens per Sec:     6515 || Lr: 0.000050
2024-02-04 09:32:41,497 Validation result at epoch 1706, step    58000: duration: 8.3709s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00144	Translation Loss: 95713.53906	PPL: 14444.34375
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.61	(BLEU-1: 9.97,	BLEU-2: 2.97,	BLEU-3: 1.24,	BLEU-4: 0.61)
	CHRF 16.94	ROUGE 8.67
2024-02-04 09:32:41,498 Logging Recognition and Translation Outputs
2024-02-04 09:32:41,499 ========================================================================================================================
2024-02-04 09:32:41,499 Logging Sequence: 180_138.00
2024-02-04 09:32:41,499 	Gloss Reference :	A B+C+D+E
2024-02-04 09:32:41,499 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:32:41,500 	Gloss Alignment :	         
2024-02-04 09:32:41,500 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:32:41,502 	Text Reference  :	ioa president p t usha constituted a    seven-member panel which included world champions from various   sports to   inquire into       the    allegations
2024-02-04 09:32:41,502 	Text Hypothesis :	*** ********* * * **** they        also demanding    that  the   full     faith in        the  judiciary and    will accpet  assaulting female wrestlers  
2024-02-04 09:32:41,502 	Text Alignment  :	D   D         D D D    S           S    S            S     S     S        S     S         S    S         S      S    S       S          S      S          
2024-02-04 09:32:41,502 ========================================================================================================================
2024-02-04 09:32:41,502 Logging Sequence: 128_189.00
2024-02-04 09:32:41,502 	Gloss Reference :	A B+C+D+E
2024-02-04 09:32:41,503 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:32:41,503 	Gloss Alignment :	         
2024-02-04 09:32:41,503 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:32:41,504 	Text Reference  :	*** *** ***** *** meanwhile some funny incidents happened during the  match
2024-02-04 09:32:41,504 	Text Hypothesis :	now the wrong but whether   to   be    held      in       a      long award
2024-02-04 09:32:41,504 	Text Alignment  :	I   I   I     I   S         S    S     S         S        S      S    S    
2024-02-04 09:32:41,504 ========================================================================================================================
2024-02-04 09:32:41,504 Logging Sequence: 165_523.00
2024-02-04 09:32:41,504 	Gloss Reference :	A B+C+D+E
2024-02-04 09:32:41,504 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:32:41,505 	Gloss Alignment :	         
2024-02-04 09:32:41,505 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:32:41,506 	Text Reference  :	as he believed that his     team    might lose  if   he    takes off his  batting pads
2024-02-04 09:32:41,506 	Text Hypothesis :	** ** when     they started pouring on    dhoni when india won   the fifa world   cup 
2024-02-04 09:32:41,506 	Text Alignment  :	D  D  S        S    S       S       S     S     S    S     S     S   S    S       S   
2024-02-04 09:32:41,506 ========================================================================================================================
2024-02-04 09:32:41,506 Logging Sequence: 145_168.00
2024-02-04 09:32:41,507 	Gloss Reference :	A B+C+D+E
2024-02-04 09:32:41,507 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:32:41,507 	Gloss Alignment :	         
2024-02-04 09:32:41,507 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:32:41,508 	Text Reference  :	the  decision has     devastated sameeha   and her    parents
2024-02-04 09:32:41,508 	Text Hypothesis :	this is       because taliban    overthrew on  social media  
2024-02-04 09:32:41,508 	Text Alignment  :	S    S        S       S          S         S   S      S      
2024-02-04 09:32:41,508 ========================================================================================================================
2024-02-04 09:32:41,508 Logging Sequence: 92_123.00
2024-02-04 09:32:41,508 	Gloss Reference :	A B+C+D+E
2024-02-04 09:32:41,508 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:32:41,509 	Gloss Alignment :	         
2024-02-04 09:32:41,509 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:32:41,510 	Text Reference  :	a heated argument also   took place    between members of    the       family and    the two   men     
2024-02-04 09:32:41,510 	Text Hypothesis :	* ****** ******** people are  confused that    india   never completed to     create a   major argument
2024-02-04 09:32:41,510 	Text Alignment  :	D D      D        S      S    S        S       S       S     S         S      S      S   S     S       
2024-02-04 09:32:41,510 ========================================================================================================================
2024-02-04 09:32:42,076 Epoch 1706: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-04 09:32:42,077 EPOCH 1707
2024-02-04 09:32:46,847 Epoch 1707: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-04 09:32:46,847 EPOCH 1708
2024-02-04 09:32:51,506 Epoch 1708: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-04 09:32:51,506 EPOCH 1709
2024-02-04 09:32:55,480 [Epoch: 1709 Step: 00058100] Batch Recognition Loss:   0.000235 => Gls Tokens per Sec:     2194 || Batch Translation Loss:   0.027974 => Txt Tokens per Sec:     6198 || Lr: 0.000050
2024-02-04 09:32:56,064 Epoch 1709: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-04 09:32:56,064 EPOCH 1710
2024-02-04 09:33:00,570 Epoch 1710: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-04 09:33:00,571 EPOCH 1711
2024-02-04 09:33:05,361 Epoch 1711: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-04 09:33:05,361 EPOCH 1712
2024-02-04 09:33:08,650 [Epoch: 1712 Step: 00058200] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:     2454 || Batch Translation Loss:   0.011424 => Txt Tokens per Sec:     6600 || Lr: 0.000050
2024-02-04 09:33:10,108 Epoch 1712: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-04 09:33:10,108 EPOCH 1713
2024-02-04 09:33:14,506 Epoch 1713: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-04 09:33:14,506 EPOCH 1714
2024-02-04 09:33:19,235 Epoch 1714: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-04 09:33:19,235 EPOCH 1715
2024-02-04 09:33:22,594 [Epoch: 1715 Step: 00058300] Batch Recognition Loss:   0.000122 => Gls Tokens per Sec:     2288 || Batch Translation Loss:   0.015067 => Txt Tokens per Sec:     6467 || Lr: 0.000050
2024-02-04 09:33:23,667 Epoch 1715: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-04 09:33:23,667 EPOCH 1716
2024-02-04 09:33:27,735 Epoch 1716: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-04 09:33:27,735 EPOCH 1717
2024-02-04 09:33:31,820 Epoch 1717: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-04 09:33:31,820 EPOCH 1718
2024-02-04 09:33:34,589 [Epoch: 1718 Step: 00058400] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:     2543 || Batch Translation Loss:   0.010237 => Txt Tokens per Sec:     7069 || Lr: 0.000050
2024-02-04 09:33:36,230 Epoch 1718: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-04 09:33:36,231 EPOCH 1719
2024-02-04 09:33:41,042 Epoch 1719: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-04 09:33:41,042 EPOCH 1720
2024-02-04 09:33:45,071 Epoch 1720: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-04 09:33:45,071 EPOCH 1721
2024-02-04 09:33:47,474 [Epoch: 1721 Step: 00058500] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:     2562 || Batch Translation Loss:   0.011309 => Txt Tokens per Sec:     7062 || Lr: 0.000050
2024-02-04 09:33:49,563 Epoch 1721: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-04 09:33:49,564 EPOCH 1722
2024-02-04 09:33:54,249 Epoch 1722: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.94 
2024-02-04 09:33:54,249 EPOCH 1723
2024-02-04 09:33:58,818 Epoch 1723: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.83 
2024-02-04 09:33:58,819 EPOCH 1724
2024-02-04 09:34:01,400 [Epoch: 1724 Step: 00058600] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     2136 || Batch Translation Loss:   0.022395 => Txt Tokens per Sec:     6041 || Lr: 0.000050
2024-02-04 09:34:03,381 Epoch 1724: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.38 
2024-02-04 09:34:03,381 EPOCH 1725
2024-02-04 09:34:07,754 Epoch 1725: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-04 09:34:07,754 EPOCH 1726
2024-02-04 09:34:12,582 Epoch 1726: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.11 
2024-02-04 09:34:12,583 EPOCH 1727
2024-02-04 09:34:14,642 [Epoch: 1727 Step: 00058700] Batch Recognition Loss:   0.000180 => Gls Tokens per Sec:     2488 || Batch Translation Loss:   0.020765 => Txt Tokens per Sec:     7144 || Lr: 0.000050
2024-02-04 09:34:17,177 Epoch 1727: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-04 09:34:17,177 EPOCH 1728
2024-02-04 09:34:21,858 Epoch 1728: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-04 09:34:21,858 EPOCH 1729
2024-02-04 09:34:25,925 Epoch 1729: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-04 09:34:25,925 EPOCH 1730
2024-02-04 09:34:27,794 [Epoch: 1730 Step: 00058800] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:     2397 || Batch Translation Loss:   0.016163 => Txt Tokens per Sec:     6254 || Lr: 0.000050
2024-02-04 09:34:30,804 Epoch 1730: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-04 09:34:30,804 EPOCH 1731
2024-02-04 09:34:35,179 Epoch 1731: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-04 09:34:35,180 EPOCH 1732
2024-02-04 09:34:40,107 Epoch 1732: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-04 09:34:40,107 EPOCH 1733
2024-02-04 09:34:41,588 [Epoch: 1733 Step: 00058900] Batch Recognition Loss:   0.000239 => Gls Tokens per Sec:     2425 || Batch Translation Loss:   0.010245 => Txt Tokens per Sec:     6765 || Lr: 0.000050
2024-02-04 09:34:44,536 Epoch 1733: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-04 09:34:44,536 EPOCH 1734
2024-02-04 09:34:49,376 Epoch 1734: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-04 09:34:49,377 EPOCH 1735
2024-02-04 09:34:53,772 Epoch 1735: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-04 09:34:53,773 EPOCH 1736
2024-02-04 09:34:55,319 [Epoch: 1736 Step: 00059000] Batch Recognition Loss:   0.000221 => Gls Tokens per Sec:     1909 || Batch Translation Loss:   0.053282 => Txt Tokens per Sec:     5420 || Lr: 0.000050
2024-02-04 09:34:58,666 Epoch 1736: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.71 
2024-02-04 09:34:58,667 EPOCH 1737
2024-02-04 09:35:03,550 Epoch 1737: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.31 
2024-02-04 09:35:03,551 EPOCH 1738
2024-02-04 09:35:07,898 Epoch 1738: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.71 
2024-02-04 09:35:07,898 EPOCH 1739
2024-02-04 09:35:09,042 [Epoch: 1739 Step: 00059100] Batch Recognition Loss:   0.000307 => Gls Tokens per Sec:     2241 || Batch Translation Loss:   0.022053 => Txt Tokens per Sec:     5857 || Lr: 0.000050
2024-02-04 09:35:12,806 Epoch 1739: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.94 
2024-02-04 09:35:12,807 EPOCH 1740
2024-02-04 09:35:17,638 Epoch 1740: Total Training Recognition Loss 0.02  Total Training Translation Loss 11.38 
2024-02-04 09:35:17,638 EPOCH 1741
2024-02-04 09:35:22,527 Epoch 1741: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.54 
2024-02-04 09:35:22,527 EPOCH 1742
2024-02-04 09:35:23,214 [Epoch: 1742 Step: 00059200] Batch Recognition Loss:   0.000521 => Gls Tokens per Sec:     2803 || Batch Translation Loss:   0.058492 => Txt Tokens per Sec:     6559 || Lr: 0.000050
2024-02-04 09:35:27,565 Epoch 1742: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.46 
2024-02-04 09:35:27,565 EPOCH 1743
2024-02-04 09:35:32,219 Epoch 1743: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-04 09:35:32,220 EPOCH 1744
2024-02-04 09:35:36,638 Epoch 1744: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-04 09:35:36,638 EPOCH 1745
2024-02-04 09:35:37,062 [Epoch: 1745 Step: 00059300] Batch Recognition Loss:   0.000221 => Gls Tokens per Sec:     3024 || Batch Translation Loss:   0.013108 => Txt Tokens per Sec:     7921 || Lr: 0.000050
2024-02-04 09:35:41,342 Epoch 1745: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-04 09:35:41,343 EPOCH 1746
2024-02-04 09:35:45,546 Epoch 1746: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-04 09:35:45,547 EPOCH 1747
2024-02-04 09:35:50,446 Epoch 1747: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-04 09:35:50,447 EPOCH 1748
2024-02-04 09:35:50,602 [Epoch: 1748 Step: 00059400] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:     4183 || Batch Translation Loss:   0.014731 => Txt Tokens per Sec:     9719 || Lr: 0.000050
2024-02-04 09:35:54,601 Epoch 1748: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-04 09:35:54,601 EPOCH 1749
2024-02-04 09:35:59,261 Epoch 1749: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-04 09:35:59,261 EPOCH 1750
2024-02-04 09:36:03,797 [Epoch: 1750 Step: 00059500] Batch Recognition Loss:   0.000225 => Gls Tokens per Sec:     2345 || Batch Translation Loss:   0.025988 => Txt Tokens per Sec:     6509 || Lr: 0.000050
2024-02-04 09:36:03,797 Epoch 1750: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-04 09:36:03,797 EPOCH 1751
2024-02-04 09:36:08,552 Epoch 1751: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-04 09:36:08,552 EPOCH 1752
2024-02-04 09:36:12,940 Epoch 1752: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-04 09:36:12,940 EPOCH 1753
2024-02-04 09:36:17,549 [Epoch: 1753 Step: 00059600] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     2168 || Batch Translation Loss:   0.014775 => Txt Tokens per Sec:     5996 || Lr: 0.000050
2024-02-04 09:36:17,788 Epoch 1753: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-04 09:36:17,788 EPOCH 1754
2024-02-04 09:36:22,031 Epoch 1754: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-04 09:36:22,031 EPOCH 1755
2024-02-04 09:36:26,928 Epoch 1755: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-04 09:36:26,929 EPOCH 1756
2024-02-04 09:36:30,787 [Epoch: 1756 Step: 00059700] Batch Recognition Loss:   0.000117 => Gls Tokens per Sec:     2425 || Batch Translation Loss:   0.007687 => Txt Tokens per Sec:     6864 || Lr: 0.000050
2024-02-04 09:36:31,241 Epoch 1756: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-04 09:36:31,241 EPOCH 1757
2024-02-04 09:36:36,178 Epoch 1757: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-04 09:36:36,178 EPOCH 1758
2024-02-04 09:36:40,464 Epoch 1758: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-04 09:36:40,464 EPOCH 1759
2024-02-04 09:36:44,586 [Epoch: 1759 Step: 00059800] Batch Recognition Loss:   0.000112 => Gls Tokens per Sec:     2113 || Batch Translation Loss:   0.009381 => Txt Tokens per Sec:     5944 || Lr: 0.000050
2024-02-04 09:36:45,331 Epoch 1759: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-04 09:36:45,331 EPOCH 1760
2024-02-04 09:36:49,721 Epoch 1760: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-04 09:36:49,722 EPOCH 1761
2024-02-04 09:36:54,506 Epoch 1761: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-04 09:36:54,506 EPOCH 1762
2024-02-04 09:36:57,490 [Epoch: 1762 Step: 00059900] Batch Recognition Loss:   0.000120 => Gls Tokens per Sec:     2706 || Batch Translation Loss:   0.007014 => Txt Tokens per Sec:     7205 || Lr: 0.000050
2024-02-04 09:36:59,007 Epoch 1762: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-04 09:36:59,007 EPOCH 1763
2024-02-04 09:37:03,638 Epoch 1763: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-04 09:37:03,638 EPOCH 1764
2024-02-04 09:37:08,231 Epoch 1764: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-04 09:37:08,232 EPOCH 1765
2024-02-04 09:37:11,273 [Epoch: 1765 Step: 00060000] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     2444 || Batch Translation Loss:   0.013339 => Txt Tokens per Sec:     6675 || Lr: 0.000050
2024-02-04 09:37:19,983 Validation result at epoch 1765, step    60000: duration: 8.7099s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00088	Translation Loss: 96878.85938	PPL: 16230.90820
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.45	(BLEU-1: 9.94,	BLEU-2: 2.79,	BLEU-3: 1.00,	BLEU-4: 0.45)
	CHRF 16.51	ROUGE 8.49
2024-02-04 09:37:19,984 Logging Recognition and Translation Outputs
2024-02-04 09:37:19,985 ========================================================================================================================
2024-02-04 09:37:19,987 Logging Sequence: 179_269.00
2024-02-04 09:37:19,987 	Gloss Reference :	A B+C+D+E
2024-02-04 09:37:19,988 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:37:19,988 	Gloss Alignment :	         
2024-02-04 09:37:19,988 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:37:19,989 	Text Reference  :	** ** the ban would mean she  can't compete in any  national or   other domestic events
2024-02-04 09:37:19,989 	Text Hypothesis :	or up to  a   same  room with top   they    be held as       they would collect  it    
2024-02-04 09:37:19,990 	Text Alignment  :	I  I  S   S   S     S    S    S     S       S  S    S        S    S     S        S     
2024-02-04 09:37:19,990 ========================================================================================================================
2024-02-04 09:37:19,990 Logging Sequence: 94_253.00
2024-02-04 09:37:19,990 	Gloss Reference :	A B+C+D+E
2024-02-04 09:37:19,990 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:37:19,990 	Gloss Alignment :	         
2024-02-04 09:37:19,990 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:37:19,991 	Text Reference  :	however some tickets will be kept aside for physical sale at the stadiums a few days prior    to **** the    match
2024-02-04 09:37:19,992 	Text Hypothesis :	******* **** ******* **** ** **** ***** *** ******** **** if the ******** * *** bcci preponed to 11th august 2023 
2024-02-04 09:37:19,992 	Text Alignment  :	D       D    D       D    D  D    D     D   D        D    S      D        D D   S    S           I    S      S    
2024-02-04 09:37:19,992 ========================================================================================================================
2024-02-04 09:37:19,992 Logging Sequence: 114_201.00
2024-02-04 09:37:19,992 	Gloss Reference :	A B+C+D+E
2024-02-04 09:37:19,992 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:37:19,992 	Gloss Alignment :	         
2024-02-04 09:37:19,992 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:37:19,993 	Text Reference  :	*** **** **** *** **** ** *** **** **** ** this    is    his first time winning the ****** copa     
2024-02-04 09:37:19,994 	Text Hypothesis :	the euro 2020 was held in the year 2021 as doctors could not save  her  from    the deadly infection
2024-02-04 09:37:19,994 	Text Alignment  :	I   I    I    I   I    I  I   I    I    I  S       S     S   S     S    S           I      S        
2024-02-04 09:37:19,994 ========================================================================================================================
2024-02-04 09:37:19,994 Logging Sequence: 118_104.00
2024-02-04 09:37:19,994 	Gloss Reference :	A B+C+D+E
2024-02-04 09:37:19,994 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:37:19,994 	Gloss Alignment :	         
2024-02-04 09:37:19,995 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:37:19,996 	Text Reference  :	*** ** ***** ***** *** kylian mbapp strong     performance in the match was  greatly     appreciated
2024-02-04 09:37:19,996 	Text Hypothesis :	due to qatar dhoni was played a      wrestler's role        in the movie over argentina's goalkeeper 
2024-02-04 09:37:19,996 	Text Alignment  :	I   I  I     I     I   S      S      S          S                  S     S    S           S          
2024-02-04 09:37:19,996 ========================================================================================================================
2024-02-04 09:37:19,996 Logging Sequence: 144_74.00
2024-02-04 09:37:19,996 	Gloss Reference :	A B+C+D+E
2024-02-04 09:37:19,996 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:37:19,996 	Gloss Alignment :	         
2024-02-04 09:37:19,997 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:37:19,997 	Text Reference  :	** ** *** ***** ** *** **** *** *** ** isn't that      amazing 
2024-02-04 09:37:19,997 	Text Hypothesis :	it is not known as the full who was it is    something shocking
2024-02-04 09:37:19,997 	Text Alignment  :	I  I  I   I     I  I   I    I   I   I  S     S         S       
2024-02-04 09:37:19,997 ========================================================================================================================
2024-02-04 09:37:21,586 Epoch 1765: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-04 09:37:21,587 EPOCH 1766
2024-02-04 09:37:26,553 Epoch 1766: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-04 09:37:26,554 EPOCH 1767
2024-02-04 09:37:31,061 Epoch 1767: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-04 09:37:31,061 EPOCH 1768
2024-02-04 09:37:34,450 [Epoch: 1768 Step: 00060100] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     2004 || Batch Translation Loss:   0.032267 => Txt Tokens per Sec:     5783 || Lr: 0.000050
2024-02-04 09:37:35,861 Epoch 1768: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-04 09:37:35,861 EPOCH 1769
2024-02-04 09:37:40,286 Epoch 1769: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-04 09:37:40,287 EPOCH 1770
2024-02-04 09:37:45,163 Epoch 1770: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-04 09:37:45,164 EPOCH 1771
2024-02-04 09:37:47,612 [Epoch: 1771 Step: 00060200] Batch Recognition Loss:   0.000111 => Gls Tokens per Sec:     2513 || Batch Translation Loss:   0.029110 => Txt Tokens per Sec:     7048 || Lr: 0.000050
2024-02-04 09:37:49,465 Epoch 1771: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-04 09:37:49,465 EPOCH 1772
2024-02-04 09:37:54,644 Epoch 1772: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-04 09:37:54,645 EPOCH 1773
2024-02-04 09:37:59,174 Epoch 1773: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-04 09:37:59,174 EPOCH 1774
2024-02-04 09:38:01,613 [Epoch: 1774 Step: 00060300] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     2363 || Batch Translation Loss:   0.059013 => Txt Tokens per Sec:     6691 || Lr: 0.000050
2024-02-04 09:38:03,773 Epoch 1774: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-04 09:38:03,774 EPOCH 1775
2024-02-04 09:38:08,274 Epoch 1775: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-04 09:38:08,274 EPOCH 1776
2024-02-04 09:38:13,227 Epoch 1776: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-04 09:38:13,228 EPOCH 1777
2024-02-04 09:38:14,834 [Epoch: 1777 Step: 00060400] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     3036 || Batch Translation Loss:   0.024391 => Txt Tokens per Sec:     7940 || Lr: 0.000050
2024-02-04 09:38:17,577 Epoch 1777: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-04 09:38:17,577 EPOCH 1778
2024-02-04 09:38:22,059 Epoch 1778: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.29 
2024-02-04 09:38:22,059 EPOCH 1779
2024-02-04 09:38:26,715 Epoch 1779: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-04 09:38:26,715 EPOCH 1780
2024-02-04 09:38:28,269 [Epoch: 1780 Step: 00060500] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:     2724 || Batch Translation Loss:   0.015587 => Txt Tokens per Sec:     7433 || Lr: 0.000050
2024-02-04 09:38:30,779 Epoch 1780: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-04 09:38:30,779 EPOCH 1781
2024-02-04 09:38:34,883 Epoch 1781: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-04 09:38:34,883 EPOCH 1782
2024-02-04 09:38:39,725 Epoch 1782: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-04 09:38:39,725 EPOCH 1783
2024-02-04 09:38:40,903 [Epoch: 1783 Step: 00060600] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:     3263 || Batch Translation Loss:   0.023483 => Txt Tokens per Sec:     8268 || Lr: 0.000050
2024-02-04 09:38:44,010 Epoch 1783: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-04 09:38:44,010 EPOCH 1784
2024-02-04 09:38:48,561 Epoch 1784: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-04 09:38:48,562 EPOCH 1785
2024-02-04 09:38:53,104 Epoch 1785: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-04 09:38:53,104 EPOCH 1786
2024-02-04 09:38:54,573 [Epoch: 1786 Step: 00060700] Batch Recognition Loss:   0.000175 => Gls Tokens per Sec:     2179 || Batch Translation Loss:   0.018976 => Txt Tokens per Sec:     6008 || Lr: 0.000050
2024-02-04 09:38:57,941 Epoch 1786: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.54 
2024-02-04 09:38:57,941 EPOCH 1787
2024-02-04 09:39:02,348 Epoch 1787: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.75 
2024-02-04 09:39:02,348 EPOCH 1788
2024-02-04 09:39:06,643 Epoch 1788: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.21 
2024-02-04 09:39:06,644 EPOCH 1789
2024-02-04 09:39:08,615 [Epoch: 1789 Step: 00060800] Batch Recognition Loss:   0.000266 => Gls Tokens per Sec:     1300 || Batch Translation Loss:   0.029624 => Txt Tokens per Sec:     4219 || Lr: 0.000050
2024-02-04 09:39:11,790 Epoch 1789: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-04 09:39:11,790 EPOCH 1790
2024-02-04 09:39:16,199 Epoch 1790: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.39 
2024-02-04 09:39:16,199 EPOCH 1791
2024-02-04 09:39:20,676 Epoch 1791: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-04 09:39:20,677 EPOCH 1792
2024-02-04 09:39:21,893 [Epoch: 1792 Step: 00060900] Batch Recognition Loss:   0.000250 => Gls Tokens per Sec:     1580 || Batch Translation Loss:   0.038058 => Txt Tokens per Sec:     5077 || Lr: 0.000050
2024-02-04 09:39:25,386 Epoch 1792: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-04 09:39:25,386 EPOCH 1793
2024-02-04 09:39:30,063 Epoch 1793: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.32 
2024-02-04 09:39:30,064 EPOCH 1794
2024-02-04 09:39:34,581 Epoch 1794: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.45 
2024-02-04 09:39:34,581 EPOCH 1795
2024-02-04 09:39:35,195 [Epoch: 1795 Step: 00061000] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:     2088 || Batch Translation Loss:   0.020161 => Txt Tokens per Sec:     6599 || Lr: 0.000050
2024-02-04 09:39:39,288 Epoch 1795: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.85 
2024-02-04 09:39:39,289 EPOCH 1796
2024-02-04 09:39:43,749 Epoch 1796: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.45 
2024-02-04 09:39:43,749 EPOCH 1797
2024-02-04 09:39:48,572 Epoch 1797: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-04 09:39:48,573 EPOCH 1798
2024-02-04 09:39:48,786 [Epoch: 1798 Step: 00061100] Batch Recognition Loss:   0.000334 => Gls Tokens per Sec:     3019 || Batch Translation Loss:   0.042519 => Txt Tokens per Sec:     7575 || Lr: 0.000050
2024-02-04 09:39:52,989 Epoch 1798: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-04 09:39:52,989 EPOCH 1799
2024-02-04 09:39:57,839 Epoch 1799: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-04 09:39:57,840 EPOCH 1800
2024-02-04 09:40:02,186 [Epoch: 1800 Step: 00061200] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:     2446 || Batch Translation Loss:   0.018019 => Txt Tokens per Sec:     6791 || Lr: 0.000050
2024-02-04 09:40:02,187 Epoch 1800: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-04 09:40:02,187 EPOCH 1801
2024-02-04 09:40:07,168 Epoch 1801: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-04 09:40:07,168 EPOCH 1802
2024-02-04 09:40:11,497 Epoch 1802: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-04 09:40:11,498 EPOCH 1803
2024-02-04 09:40:16,232 [Epoch: 1803 Step: 00061300] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:     2163 || Batch Translation Loss:   0.019857 => Txt Tokens per Sec:     6049 || Lr: 0.000050
2024-02-04 09:40:16,455 Epoch 1803: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-04 09:40:16,455 EPOCH 1804
2024-02-04 09:40:21,384 Epoch 1804: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-04 09:40:21,384 EPOCH 1805
2024-02-04 09:40:25,972 Epoch 1805: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-04 09:40:25,972 EPOCH 1806
2024-02-04 09:40:29,732 [Epoch: 1806 Step: 00061400] Batch Recognition Loss:   0.000112 => Gls Tokens per Sec:     2487 || Batch Translation Loss:   0.011091 => Txt Tokens per Sec:     7040 || Lr: 0.000050
2024-02-04 09:40:30,085 Epoch 1806: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-04 09:40:30,086 EPOCH 1807
2024-02-04 09:40:34,116 Epoch 1807: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-04 09:40:34,116 EPOCH 1808
2024-02-04 09:40:38,828 Epoch 1808: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-04 09:40:38,829 EPOCH 1809
2024-02-04 09:40:42,529 [Epoch: 1809 Step: 00061500] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:     2355 || Batch Translation Loss:   0.028472 => Txt Tokens per Sec:     6605 || Lr: 0.000050
2024-02-04 09:40:43,190 Epoch 1809: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-04 09:40:43,190 EPOCH 1810
2024-02-04 09:40:47,403 Epoch 1810: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-04 09:40:47,404 EPOCH 1811
2024-02-04 09:40:52,085 Epoch 1811: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-04 09:40:52,085 EPOCH 1812
2024-02-04 09:40:55,507 [Epoch: 1812 Step: 00061600] Batch Recognition Loss:   0.000281 => Gls Tokens per Sec:     2432 || Batch Translation Loss:   0.025203 => Txt Tokens per Sec:     6738 || Lr: 0.000050
2024-02-04 09:40:56,593 Epoch 1812: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-04 09:40:56,594 EPOCH 1813
2024-02-04 09:41:01,364 Epoch 1813: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-04 09:41:01,365 EPOCH 1814
2024-02-04 09:41:05,789 Epoch 1814: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-04 09:41:05,790 EPOCH 1815
2024-02-04 09:41:08,891 [Epoch: 1815 Step: 00061700] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     2398 || Batch Translation Loss:   0.020157 => Txt Tokens per Sec:     6521 || Lr: 0.000050
2024-02-04 09:41:10,619 Epoch 1815: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-04 09:41:10,619 EPOCH 1816
2024-02-04 09:41:14,952 Epoch 1816: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-04 09:41:14,952 EPOCH 1817
2024-02-04 09:41:19,819 Epoch 1817: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-04 09:41:19,820 EPOCH 1818
2024-02-04 09:41:22,494 [Epoch: 1818 Step: 00061800] Batch Recognition Loss:   0.000192 => Gls Tokens per Sec:     2540 || Batch Translation Loss:   0.026909 => Txt Tokens per Sec:     6998 || Lr: 0.000050
2024-02-04 09:41:24,069 Epoch 1818: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.59 
2024-02-04 09:41:24,070 EPOCH 1819
2024-02-04 09:41:29,021 Epoch 1819: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.64 
2024-02-04 09:41:29,022 EPOCH 1820
2024-02-04 09:41:33,277 Epoch 1820: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.54 
2024-02-04 09:41:33,277 EPOCH 1821
2024-02-04 09:41:35,977 [Epoch: 1821 Step: 00061900] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     2279 || Batch Translation Loss:   0.021484 => Txt Tokens per Sec:     6324 || Lr: 0.000050
2024-02-04 09:41:38,175 Epoch 1821: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.64 
2024-02-04 09:41:38,175 EPOCH 1822
2024-02-04 09:41:42,828 Epoch 1822: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.33 
2024-02-04 09:41:42,828 EPOCH 1823
2024-02-04 09:41:47,584 Epoch 1823: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.38 
2024-02-04 09:41:47,585 EPOCH 1824
2024-02-04 09:41:49,909 [Epoch: 1824 Step: 00062000] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     2373 || Batch Translation Loss:   0.050127 => Txt Tokens per Sec:     6397 || Lr: 0.000050
2024-02-04 09:41:58,420 Validation result at epoch 1824, step    62000: duration: 8.5114s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00131	Translation Loss: 97373.03125	PPL: 17053.73438
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.45	(BLEU-1: 10.41,	BLEU-2: 3.14,	BLEU-3: 1.12,	BLEU-4: 0.45)
	CHRF 16.37	ROUGE 8.84
2024-02-04 09:41:58,421 Logging Recognition and Translation Outputs
2024-02-04 09:41:58,421 ========================================================================================================================
2024-02-04 09:41:58,422 Logging Sequence: 87_52.00
2024-02-04 09:41:58,423 	Gloss Reference :	A B+C+D+E
2024-02-04 09:41:58,423 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:41:58,424 	Gloss Alignment :	         
2024-02-04 09:41:58,424 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:41:58,424 	Text Reference  :	that is when gambhir walked into bat and rescued india with his  brilliant 97  runs         
2024-02-04 09:41:58,425 	Text Hypothesis :	**** ** **** gambhir ****** **** *** *** ******* ***** took over you       how disappointing
2024-02-04 09:41:58,425 	Text Alignment  :	D    D  D            D      D    D   D   D       D     S    S    S         S   S            
2024-02-04 09:41:58,425 ========================================================================================================================
2024-02-04 09:41:58,425 Logging Sequence: 85_2.00
2024-02-04 09:41:58,425 	Gloss Reference :	A B+C+D+E
2024-02-04 09:41:58,425 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:41:58,425 	Gloss Alignment :	         
2024-02-04 09:41:58,426 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:41:58,427 	Text Reference  :	andrew symonds is one of      the finest all    rounders in    the history of australian cricket
2024-02-04 09:41:58,427 	Text Hypothesis :	when   symonds ** was driving the car    rolled multiple times and hit     a  young      player 
2024-02-04 09:41:58,427 	Text Alignment  :	S              D  S   S           S      S      S        S     S   S       S  S          S      
2024-02-04 09:41:58,427 ========================================================================================================================
2024-02-04 09:41:58,427 Logging Sequence: 51_110.00
2024-02-04 09:41:58,427 	Gloss Reference :	A B+C+D+E
2024-02-04 09:41:58,428 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:41:58,428 	Gloss Alignment :	         
2024-02-04 09:41:58,428 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:41:58,428 	Text Reference  :	the aussies were very happy with their victory
2024-02-04 09:41:58,428 	Text Hypothesis :	*** he      took a    run   and  3     wickets
2024-02-04 09:41:58,429 	Text Alignment  :	D   S       S    S    S     S    S     S      
2024-02-04 09:41:58,429 ========================================================================================================================
2024-02-04 09:41:58,429 Logging Sequence: 72_59.00
2024-02-04 09:41:58,429 	Gloss Reference :	A B+C+D+E
2024-02-04 09:41:58,429 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:41:58,429 	Gloss Alignment :	         
2024-02-04 09:41:58,429 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:41:58,430 	Text Reference  :	**** after   that sapna and shobit started arguing and misbehaving with the     cricketer
2024-02-04 09:41:58,430 	Text Hypothesis :	roma balwani ceo  of    the idca   said    'i      am  ecstatic    with india's victory  
2024-02-04 09:41:58,431 	Text Alignment  :	I    S       S    S     S   S      S       S       S   S                S       S        
2024-02-04 09:41:58,431 ========================================================================================================================
2024-02-04 09:41:58,431 Logging Sequence: 122_184.00
2024-02-04 09:41:58,431 	Gloss Reference :	A B+C+D+E
2024-02-04 09:41:58,431 	Gloss Hypothesis:	A B+C+D+E
2024-02-04 09:41:58,431 	Gloss Alignment :	         
2024-02-04 09:41:58,431 	--------------------------------------------------------------------------------------------------------------------
2024-02-04 09:41:58,432 	Text Reference  :	are playing exceptionally well and keeping hopes of further olympic medals alive     
2024-02-04 09:41:58,432 	Text Hypothesis :	*** ******* ************* **** *** there   is    no cure    for     the    tournament
2024-02-04 09:41:58,432 	Text Alignment  :	D   D       D             D    D   S       S     S  S       S       S      S         
2024-02-04 09:41:58,432 ========================================================================================================================
2024-02-04 09:41:58,436 Training ended since there were no improvements inthe last learning rate step: 0.000050
2024-02-04 09:41:58,436 Best validation result at step    10000:   1.17 eval_metric.
2024-02-04 09:42:26,096 ------------------------------------------------------------
2024-02-04 09:42:26,096 [DEV] partition [RECOGNITION] experiment [BW]: 1
2024-02-04 09:42:34,711 finished in 8.6153s 
2024-02-04 09:42:34,712 ************************************************************
2024-02-04 09:42:34,712 [DEV] partition [RECOGNITION] results:
	New Best CTC Decode Beam Size: 1
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
2024-02-04 09:42:34,712 ************************************************************
2024-02-04 09:42:34,712 ------------------------------------------------------------
2024-02-04 09:42:34,712 [DEV] partition [RECOGNITION] experiment [BW]: 2
2024-02-04 09:42:43,113 finished in 8.4010s 
2024-02-04 09:42:43,114 ------------------------------------------------------------
2024-02-04 09:42:43,114 [DEV] partition [RECOGNITION] experiment [BW]: 3
2024-02-04 09:42:51,252 finished in 8.1380s 
2024-02-04 09:42:51,252 ------------------------------------------------------------
2024-02-04 09:42:51,252 [DEV] partition [RECOGNITION] experiment [BW]: 4
2024-02-04 09:42:59,448 finished in 8.1960s 
2024-02-04 09:42:59,448 ------------------------------------------------------------
2024-02-04 09:42:59,449 [DEV] partition [RECOGNITION] experiment [BW]: 5
2024-02-04 09:43:07,725 finished in 8.2770s 
2024-02-04 09:43:07,726 ------------------------------------------------------------
2024-02-04 09:43:07,726 [DEV] partition [RECOGNITION] experiment [BW]: 6
2024-02-04 09:43:15,928 finished in 8.2020s 
2024-02-04 09:43:15,929 ------------------------------------------------------------
2024-02-04 09:43:15,929 [DEV] partition [RECOGNITION] experiment [BW]: 7
2024-02-04 09:43:24,337 finished in 8.4080s 
2024-02-04 09:43:24,338 ------------------------------------------------------------
2024-02-04 09:43:24,338 [DEV] partition [RECOGNITION] experiment [BW]: 8
2024-02-04 09:43:32,714 finished in 8.3753s 
2024-02-04 09:43:32,715 ------------------------------------------------------------
2024-02-04 09:43:32,715 [DEV] partition [RECOGNITION] experiment [BW]: 9
2024-02-04 09:43:41,129 finished in 8.4147s 
2024-02-04 09:43:41,130 ------------------------------------------------------------
2024-02-04 09:43:41,130 [DEV] partition [RECOGNITION] experiment [BW]: 10
2024-02-04 09:43:49,495 finished in 8.3637s 
2024-02-04 09:43:49,495 ============================================================
2024-02-04 09:43:57,340 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 1 and Alpha: -1
	BLEU-4 1.17	(BLEU-1: 11.33,	BLEU-2: 4.18,	BLEU-3: 2.06,	BLEU-4: 1.17)
	CHRF 17.52	ROUGE 9.83
2024-02-04 09:43:57,340 ------------------------------------------------------------
2024-02-04 09:45:14,790 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 2 and Alpha: 1
	BLEU-4 1.17	(BLEU-1: 11.45,	BLEU-2: 4.17,	BLEU-3: 2.02,	BLEU-4: 1.17)
	CHRF 17.23	ROUGE 9.94
2024-02-04 09:45:14,790 ------------------------------------------------------------
2024-02-04 09:45:24,456 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 2 and Alpha: 2
	BLEU-4 1.23	(BLEU-1: 11.68,	BLEU-2: 4.34,	BLEU-3: 2.13,	BLEU-4: 1.23)
	CHRF 17.36	ROUGE 9.93
2024-02-04 09:45:24,457 ------------------------------------------------------------
2024-02-04 09:45:34,112 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 2 and Alpha: 3
	BLEU-4 1.27	(BLEU-1: 11.80,	BLEU-2: 4.39,	BLEU-3: 2.18,	BLEU-4: 1.27)
	CHRF 17.38	ROUGE 9.94
2024-02-04 09:45:34,112 ------------------------------------------------------------
2024-02-04 09:48:07,392 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 4 and Alpha: 3
	BLEU-4 1.29	(BLEU-1: 11.53,	BLEU-2: 4.32,	BLEU-3: 2.19,	BLEU-4: 1.29)
	CHRF 17.32	ROUGE 9.72
2024-02-04 09:48:07,393 ------------------------------------------------------------
2024-02-04 09:48:19,390 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 4 and Alpha: 4
	BLEU-4 1.29	(BLEU-1: 11.56,	BLEU-2: 4.34,	BLEU-3: 2.19,	BLEU-4: 1.29)
	CHRF 17.34	ROUGE 9.71
2024-02-04 09:48:19,390 ------------------------------------------------------------
2024-02-04 09:59:53,263 ************************************************************
2024-02-04 09:59:53,264 [DEV] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 1
	Best Translation Beam Size: 4 and Alpha: 4
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 1.29	(BLEU-1: 11.56,	BLEU-2: 4.34,	BLEU-3: 2.19,	BLEU-4: 1.29)
	CHRF 17.34	ROUGE 9.71
2024-02-04 09:59:53,264 ************************************************************
2024-02-04 10:00:05,208 [TEST] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 1
	Best Translation Beam Size: 4 and Alpha: 4
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.47	(BLEU-1: 10.45,	BLEU-2: 3.18,	BLEU-3: 1.22,	BLEU-4: 0.47)
	CHRF 17.01	ROUGE 8.93
2024-02-04 10:00:05,209 ************************************************************
