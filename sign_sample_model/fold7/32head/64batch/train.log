2024-02-08 17:35:07,436 Hello! This is Joey-NMT.
2024-02-08 17:35:07,444 Total params: 25642504
2024-02-08 17:35:07,444 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'gloss_output_layer.bias', 'gloss_output_layer.weight', 'sgn_embed.ln.bias', 'sgn_embed.ln.weight', 'sgn_embed.norm.norm.bias', 'sgn_embed.norm.norm.weight', 'txt_embed.norm.norm.bias', 'txt_embed.norm.norm.weight']
2024-02-08 17:35:08,466 cfg.name                           : sign_experiment
2024-02-08 17:35:08,466 cfg.data.data_path                 : ./data/Sports_dataset/7/
2024-02-08 17:35:08,467 cfg.data.version                   : phoenix_2014_trans
2024-02-08 17:35:08,467 cfg.data.sgn                       : sign
2024-02-08 17:35:08,467 cfg.data.txt                       : text
2024-02-08 17:35:08,467 cfg.data.gls                       : gloss
2024-02-08 17:35:08,467 cfg.data.train                     : excel_data.train
2024-02-08 17:35:08,467 cfg.data.dev                       : excel_data.dev
2024-02-08 17:35:08,467 cfg.data.test                      : excel_data.test
2024-02-08 17:35:08,467 cfg.data.feature_size              : 2560
2024-02-08 17:35:08,468 cfg.data.level                     : word
2024-02-08 17:35:08,468 cfg.data.txt_lowercase             : True
2024-02-08 17:35:08,468 cfg.data.max_sent_length           : 500
2024-02-08 17:35:08,468 cfg.data.random_train_subset       : -1
2024-02-08 17:35:08,468 cfg.data.random_dev_subset         : -1
2024-02-08 17:35:08,468 cfg.testing.recognition_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-08 17:35:08,468 cfg.testing.translation_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-08 17:35:08,468 cfg.testing.translation_beam_alphas : [-1, 0, 1, 2, 3, 4, 5]
2024-02-08 17:35:08,469 cfg.training.reset_best_ckpt       : False
2024-02-08 17:35:08,469 cfg.training.reset_scheduler       : False
2024-02-08 17:35:08,469 cfg.training.reset_optimizer       : False
2024-02-08 17:35:08,469 cfg.training.random_seed           : 42
2024-02-08 17:35:08,469 cfg.training.model_dir             : ./sign_sample_model/fold7/32head/64batch
2024-02-08 17:35:08,469 cfg.training.recognition_loss_weight : 1.0
2024-02-08 17:35:08,469 cfg.training.translation_loss_weight : 1.0
2024-02-08 17:35:08,469 cfg.training.eval_metric           : bleu
2024-02-08 17:35:08,469 cfg.training.optimizer             : adam
2024-02-08 17:35:08,470 cfg.training.learning_rate         : 0.0001
2024-02-08 17:35:08,470 cfg.training.batch_size            : 64
2024-02-08 17:35:08,470 cfg.training.num_valid_log         : 5
2024-02-08 17:35:08,470 cfg.training.epochs                : 50000
2024-02-08 17:35:08,470 cfg.training.early_stopping_metric : eval_metric
2024-02-08 17:35:08,470 cfg.training.batch_type            : sentence
2024-02-08 17:35:08,471 cfg.training.translation_normalization : batch
2024-02-08 17:35:08,471 cfg.training.eval_recognition_beam_size : 1
2024-02-08 17:35:08,471 cfg.training.eval_translation_beam_size : 1
2024-02-08 17:35:08,471 cfg.training.eval_translation_beam_alpha : -1
2024-02-08 17:35:08,471 cfg.training.overwrite             : True
2024-02-08 17:35:08,471 cfg.training.shuffle               : True
2024-02-08 17:35:08,471 cfg.training.use_cuda              : True
2024-02-08 17:35:08,472 cfg.training.translation_max_output_length : 40
2024-02-08 17:35:08,472 cfg.training.keep_last_ckpts       : 1
2024-02-08 17:35:08,472 cfg.training.batch_multiplier      : 1
2024-02-08 17:35:08,472 cfg.training.logging_freq          : 100
2024-02-08 17:35:08,472 cfg.training.validation_freq       : 2000
2024-02-08 17:35:08,472 cfg.training.betas                 : [0.9, 0.998]
2024-02-08 17:35:08,472 cfg.training.scheduling            : plateau
2024-02-08 17:35:08,472 cfg.training.learning_rate_min     : 1e-08
2024-02-08 17:35:08,473 cfg.training.weight_decay          : 0.0001
2024-02-08 17:35:08,473 cfg.training.patience              : 12
2024-02-08 17:35:08,473 cfg.training.decrease_factor       : 0.5
2024-02-08 17:35:08,473 cfg.training.label_smoothing       : 0.0
2024-02-08 17:35:08,473 cfg.model.initializer              : xavier
2024-02-08 17:35:08,473 cfg.model.bias_initializer         : zeros
2024-02-08 17:35:08,473 cfg.model.init_gain                : 1.0
2024-02-08 17:35:08,473 cfg.model.embed_initializer        : xavier
2024-02-08 17:35:08,474 cfg.model.embed_init_gain          : 1.0
2024-02-08 17:35:08,474 cfg.model.tied_softmax             : True
2024-02-08 17:35:08,474 cfg.model.encoder.type             : transformer
2024-02-08 17:35:08,474 cfg.model.encoder.num_layers       : 3
2024-02-08 17:35:08,474 cfg.model.encoder.num_heads        : 32
2024-02-08 17:35:08,474 cfg.model.encoder.embeddings.embedding_dim : 512
2024-02-08 17:35:08,474 cfg.model.encoder.embeddings.scale : False
2024-02-08 17:35:08,474 cfg.model.encoder.embeddings.dropout : 0.1
2024-02-08 17:35:08,474 cfg.model.encoder.embeddings.norm_type : batch
2024-02-08 17:35:08,475 cfg.model.encoder.embeddings.activation_type : softsign
2024-02-08 17:35:08,475 cfg.model.encoder.hidden_size      : 512
2024-02-08 17:35:08,475 cfg.model.encoder.ff_size          : 2048
2024-02-08 17:35:08,475 cfg.model.encoder.dropout          : 0.1
2024-02-08 17:35:08,475 cfg.model.decoder.type             : transformer
2024-02-08 17:35:08,475 cfg.model.decoder.num_layers       : 3
2024-02-08 17:35:08,475 cfg.model.decoder.num_heads        : 32
2024-02-08 17:35:08,475 cfg.model.decoder.embeddings.embedding_dim : 512
2024-02-08 17:35:08,476 cfg.model.decoder.embeddings.scale : False
2024-02-08 17:35:08,476 cfg.model.decoder.embeddings.dropout : 0.1
2024-02-08 17:35:08,476 cfg.model.decoder.embeddings.norm_type : batch
2024-02-08 17:35:08,476 cfg.model.decoder.embeddings.activation_type : softsign
2024-02-08 17:35:08,476 cfg.model.decoder.hidden_size      : 512
2024-02-08 17:35:08,476 cfg.model.decoder.ff_size          : 2048
2024-02-08 17:35:08,476 cfg.model.decoder.dropout          : 0.1
2024-02-08 17:35:08,476 Data set sizes: 
	train 2124,
	valid 708,
	test 708
2024-02-08 17:35:08,476 First training example:
	[GLS] A B C D E
	[TXT] how did she become a champion
2024-02-08 17:35:08,477 First 10 words (gls): (0) <si> (1) <unk> (2) <pad> (3) A (4) B (5) C (6) D (7) E
2024-02-08 17:35:08,477 First 10 words (txt): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) and (6) to (7) in (8) a (9) of
2024-02-08 17:35:08,477 Number of unique glosses (types): 8
2024-02-08 17:35:08,477 Number of unique words (types): 4402
2024-02-08 17:35:08,477 SignModel(
	encoder=TransformerEncoder(num_layers=3, num_heads=32),
	decoder=TransformerDecoder(num_layers=3, num_heads=32),
	sgn_embed=SpatialEmbeddings(embedding_dim=512, input_size=2560),
	txt_embed=Embeddings(embedding_dim=512, vocab_size=4402))
2024-02-08 17:35:08,481 EPOCH 1
2024-02-08 17:35:14,059 Epoch   1: Total Training Recognition Loss 199.08  Total Training Translation Loss 3444.24 
2024-02-08 17:35:14,060 EPOCH 2
2024-02-08 17:35:18,893 Epoch   2: Total Training Recognition Loss 66.37  Total Training Translation Loss 3092.40 
2024-02-08 17:35:18,893 EPOCH 3
2024-02-08 17:35:23,537 [Epoch: 003 Step: 00000100] Batch Recognition Loss:   1.053872 => Gls Tokens per Sec:     2206 || Batch Translation Loss:  51.455860 => Txt Tokens per Sec:     6192 || Lr: 0.000100
2024-02-08 17:35:23,720 Epoch   3: Total Training Recognition Loss 41.30  Total Training Translation Loss 3007.27 
2024-02-08 17:35:23,720 EPOCH 4
2024-02-08 17:35:28,371 Epoch   4: Total Training Recognition Loss 30.55  Total Training Translation Loss 2979.99 
2024-02-08 17:35:28,371 EPOCH 5
2024-02-08 17:35:33,291 Epoch   5: Total Training Recognition Loss 26.58  Total Training Translation Loss 2932.03 
2024-02-08 17:35:33,291 EPOCH 6
2024-02-08 17:35:37,508 [Epoch: 006 Step: 00000200] Batch Recognition Loss:   0.824959 => Gls Tokens per Sec:     2215 || Batch Translation Loss:  88.531090 => Txt Tokens per Sec:     6214 || Lr: 0.000100
2024-02-08 17:35:37,919 Epoch   6: Total Training Recognition Loss 23.94  Total Training Translation Loss 2867.24 
2024-02-08 17:35:37,919 EPOCH 7
2024-02-08 17:35:43,023 Epoch   7: Total Training Recognition Loss 37.79  Total Training Translation Loss 2784.68 
2024-02-08 17:35:43,024 EPOCH 8
2024-02-08 17:35:47,717 Epoch   8: Total Training Recognition Loss 65.34  Total Training Translation Loss 2701.41 
2024-02-08 17:35:47,717 EPOCH 9
2024-02-08 17:35:51,923 [Epoch: 009 Step: 00000300] Batch Recognition Loss:   0.695411 => Gls Tokens per Sec:     2069 || Batch Translation Loss: 103.506454 => Txt Tokens per Sec:     5794 || Lr: 0.000100
2024-02-08 17:35:52,724 Epoch   9: Total Training Recognition Loss 31.36  Total Training Translation Loss 2629.18 
2024-02-08 17:35:52,724 EPOCH 10
2024-02-08 17:35:57,577 Epoch  10: Total Training Recognition Loss 22.09  Total Training Translation Loss 2549.24 
2024-02-08 17:35:57,578 EPOCH 11
2024-02-08 17:36:02,344 Epoch  11: Total Training Recognition Loss 18.71  Total Training Translation Loss 2464.22 
2024-02-08 17:36:02,344 EPOCH 12
2024-02-08 17:36:06,322 [Epoch: 012 Step: 00000400] Batch Recognition Loss:   0.442147 => Gls Tokens per Sec:     2027 || Batch Translation Loss:  40.034283 => Txt Tokens per Sec:     5748 || Lr: 0.000100
2024-02-08 17:36:07,246 Epoch  12: Total Training Recognition Loss 17.31  Total Training Translation Loss 2387.79 
2024-02-08 17:36:07,246 EPOCH 13
2024-02-08 17:36:11,516 Epoch  13: Total Training Recognition Loss 13.40  Total Training Translation Loss 2321.10 
2024-02-08 17:36:11,516 EPOCH 14
2024-02-08 17:36:16,356 Epoch  14: Total Training Recognition Loss 11.29  Total Training Translation Loss 2265.74 
2024-02-08 17:36:16,356 EPOCH 15
2024-02-08 17:36:19,410 [Epoch: 015 Step: 00000500] Batch Recognition Loss:   0.241104 => Gls Tokens per Sec:     2430 || Batch Translation Loss:  67.428558 => Txt Tokens per Sec:     6567 || Lr: 0.000100
2024-02-08 17:36:20,923 Epoch  15: Total Training Recognition Loss 10.23  Total Training Translation Loss 2190.96 
2024-02-08 17:36:20,924 EPOCH 16
2024-02-08 17:36:25,557 Epoch  16: Total Training Recognition Loss 9.19  Total Training Translation Loss 2133.00 
2024-02-08 17:36:25,557 EPOCH 17
2024-02-08 17:36:30,309 Epoch  17: Total Training Recognition Loss 8.25  Total Training Translation Loss 2071.43 
2024-02-08 17:36:30,309 EPOCH 18
2024-02-08 17:36:32,594 [Epoch: 018 Step: 00000600] Batch Recognition Loss:   0.266354 => Gls Tokens per Sec:     3082 || Batch Translation Loss:  24.121902 => Txt Tokens per Sec:     8109 || Lr: 0.000100
2024-02-08 17:36:34,659 Epoch  18: Total Training Recognition Loss 7.44  Total Training Translation Loss 2004.09 
2024-02-08 17:36:34,659 EPOCH 19
2024-02-08 17:36:39,637 Epoch  19: Total Training Recognition Loss 6.68  Total Training Translation Loss 1951.27 
2024-02-08 17:36:39,637 EPOCH 20
2024-02-08 17:36:43,783 Epoch  20: Total Training Recognition Loss 5.81  Total Training Translation Loss 1884.91 
2024-02-08 17:36:43,783 EPOCH 21
2024-02-08 17:36:46,766 [Epoch: 021 Step: 00000700] Batch Recognition Loss:   0.204490 => Gls Tokens per Sec:     2059 || Batch Translation Loss:  68.844452 => Txt Tokens per Sec:     5785 || Lr: 0.000100
2024-02-08 17:36:48,764 Epoch  21: Total Training Recognition Loss 5.60  Total Training Translation Loss 1865.68 
2024-02-08 17:36:48,765 EPOCH 22
2024-02-08 17:36:53,148 Epoch  22: Total Training Recognition Loss 5.20  Total Training Translation Loss 1791.92 
2024-02-08 17:36:53,148 EPOCH 23
2024-02-08 17:36:57,965 Epoch  23: Total Training Recognition Loss 4.89  Total Training Translation Loss 1742.06 
2024-02-08 17:36:57,965 EPOCH 24
2024-02-08 17:37:00,454 [Epoch: 024 Step: 00000800] Batch Recognition Loss:   0.098793 => Gls Tokens per Sec:     2211 || Batch Translation Loss:  50.491665 => Txt Tokens per Sec:     6028 || Lr: 0.000100
2024-02-08 17:37:02,488 Epoch  24: Total Training Recognition Loss 4.57  Total Training Translation Loss 1691.21 
2024-02-08 17:37:02,488 EPOCH 25
2024-02-08 17:37:07,222 Epoch  25: Total Training Recognition Loss 4.42  Total Training Translation Loss 1633.62 
2024-02-08 17:37:07,222 EPOCH 26
2024-02-08 17:37:11,755 Epoch  26: Total Training Recognition Loss 3.99  Total Training Translation Loss 1576.51 
2024-02-08 17:37:11,755 EPOCH 27
2024-02-08 17:37:13,720 [Epoch: 027 Step: 00000900] Batch Recognition Loss:   0.046470 => Gls Tokens per Sec:     2475 || Batch Translation Loss:  44.001110 => Txt Tokens per Sec:     7062 || Lr: 0.000100
2024-02-08 17:37:16,496 Epoch  27: Total Training Recognition Loss 3.59  Total Training Translation Loss 1534.78 
2024-02-08 17:37:16,497 EPOCH 28
2024-02-08 17:37:21,486 Epoch  28: Total Training Recognition Loss 3.54  Total Training Translation Loss 1505.17 
2024-02-08 17:37:21,487 EPOCH 29
2024-02-08 17:37:26,079 Epoch  29: Total Training Recognition Loss 3.51  Total Training Translation Loss 1446.78 
2024-02-08 17:37:26,079 EPOCH 30
2024-02-08 17:37:27,674 [Epoch: 030 Step: 00001000] Batch Recognition Loss:   0.203078 => Gls Tokens per Sec:     2811 || Batch Translation Loss:  16.697739 => Txt Tokens per Sec:     7477 || Lr: 0.000100
2024-02-08 17:37:30,185 Epoch  30: Total Training Recognition Loss 3.52  Total Training Translation Loss 1403.22 
2024-02-08 17:37:30,185 EPOCH 31
2024-02-08 17:37:34,259 Epoch  31: Total Training Recognition Loss 3.41  Total Training Translation Loss 1350.41 
2024-02-08 17:37:34,259 EPOCH 32
2024-02-08 17:37:38,599 Epoch  32: Total Training Recognition Loss 3.24  Total Training Translation Loss 1302.19 
2024-02-08 17:37:38,600 EPOCH 33
2024-02-08 17:37:40,719 [Epoch: 033 Step: 00001100] Batch Recognition Loss:   0.065149 => Gls Tokens per Sec:     1814 || Batch Translation Loss:  37.496822 => Txt Tokens per Sec:     5119 || Lr: 0.000100
2024-02-08 17:37:43,559 Epoch  33: Total Training Recognition Loss 3.15  Total Training Translation Loss 1285.21 
2024-02-08 17:37:43,559 EPOCH 34
2024-02-08 17:37:47,686 Epoch  34: Total Training Recognition Loss 3.34  Total Training Translation Loss 1258.03 
2024-02-08 17:37:47,686 EPOCH 35
2024-02-08 17:37:52,735 Epoch  35: Total Training Recognition Loss 3.54  Total Training Translation Loss 1191.24 
2024-02-08 17:37:52,735 EPOCH 36
2024-02-08 17:37:53,919 [Epoch: 036 Step: 00001200] Batch Recognition Loss:   0.139176 => Gls Tokens per Sec:     2705 || Batch Translation Loss:  44.087296 => Txt Tokens per Sec:     7327 || Lr: 0.000100
2024-02-08 17:37:57,039 Epoch  36: Total Training Recognition Loss 3.08  Total Training Translation Loss 1143.82 
2024-02-08 17:37:57,039 EPOCH 37
2024-02-08 17:38:01,877 Epoch  37: Total Training Recognition Loss 2.98  Total Training Translation Loss 1085.59 
2024-02-08 17:38:01,878 EPOCH 38
2024-02-08 17:38:06,394 Epoch  38: Total Training Recognition Loss 2.79  Total Training Translation Loss 1040.01 
2024-02-08 17:38:06,394 EPOCH 39
2024-02-08 17:38:07,345 [Epoch: 039 Step: 00001300] Batch Recognition Loss:   0.078342 => Gls Tokens per Sec:     2421 || Batch Translation Loss:  32.576775 => Txt Tokens per Sec:     7016 || Lr: 0.000100
2024-02-08 17:38:11,068 Epoch  39: Total Training Recognition Loss 2.73  Total Training Translation Loss 1000.75 
2024-02-08 17:38:11,069 EPOCH 40
2024-02-08 17:38:15,733 Epoch  40: Total Training Recognition Loss 2.65  Total Training Translation Loss 958.59 
2024-02-08 17:38:15,733 EPOCH 41
2024-02-08 17:38:20,070 Epoch  41: Total Training Recognition Loss 2.88  Total Training Translation Loss 928.24 
2024-02-08 17:38:20,070 EPOCH 42
2024-02-08 17:38:20,964 [Epoch: 042 Step: 00001400] Batch Recognition Loss:   0.090345 => Gls Tokens per Sec:     2150 || Batch Translation Loss:  19.919621 => Txt Tokens per Sec:     5773 || Lr: 0.000100
2024-02-08 17:38:25,052 Epoch  42: Total Training Recognition Loss 2.71  Total Training Translation Loss 880.15 
2024-02-08 17:38:25,052 EPOCH 43
2024-02-08 17:38:29,187 Epoch  43: Total Training Recognition Loss 2.70  Total Training Translation Loss 847.64 
2024-02-08 17:38:29,188 EPOCH 44
2024-02-08 17:38:34,185 Epoch  44: Total Training Recognition Loss 2.71  Total Training Translation Loss 829.63 
2024-02-08 17:38:34,186 EPOCH 45
2024-02-08 17:38:34,568 [Epoch: 045 Step: 00001500] Batch Recognition Loss:   0.032952 => Gls Tokens per Sec:     3351 || Batch Translation Loss:  19.728821 => Txt Tokens per Sec:     8906 || Lr: 0.000100
2024-02-08 17:38:38,471 Epoch  45: Total Training Recognition Loss 2.71  Total Training Translation Loss 786.73 
2024-02-08 17:38:38,471 EPOCH 46
2024-02-08 17:38:43,286 Epoch  46: Total Training Recognition Loss 2.66  Total Training Translation Loss 741.77 
2024-02-08 17:38:43,287 EPOCH 47
2024-02-08 17:38:47,877 Epoch  47: Total Training Recognition Loss 2.62  Total Training Translation Loss 717.74 
2024-02-08 17:38:47,877 EPOCH 48
2024-02-08 17:38:48,098 [Epoch: 048 Step: 00001600] Batch Recognition Loss:   0.032640 => Gls Tokens per Sec:     2909 || Batch Translation Loss:  22.582270 => Txt Tokens per Sec:     8368 || Lr: 0.000100
2024-02-08 17:38:52,521 Epoch  48: Total Training Recognition Loss 2.59  Total Training Translation Loss 682.24 
2024-02-08 17:38:52,522 EPOCH 49
2024-02-08 17:38:57,271 Epoch  49: Total Training Recognition Loss 2.57  Total Training Translation Loss 646.29 
2024-02-08 17:38:57,271 EPOCH 50
2024-02-08 17:39:01,610 [Epoch: 050 Step: 00001700] Batch Recognition Loss:   0.059141 => Gls Tokens per Sec:     2448 || Batch Translation Loss:  20.316795 => Txt Tokens per Sec:     6773 || Lr: 0.000100
2024-02-08 17:39:01,610 Epoch  50: Total Training Recognition Loss 2.50  Total Training Translation Loss 604.57 
2024-02-08 17:39:01,611 EPOCH 51
2024-02-08 17:39:06,569 Epoch  51: Total Training Recognition Loss 2.44  Total Training Translation Loss 574.13 
2024-02-08 17:39:06,570 EPOCH 52
2024-02-08 17:39:10,742 Epoch  52: Total Training Recognition Loss 2.22  Total Training Translation Loss 537.61 
2024-02-08 17:39:10,742 EPOCH 53
2024-02-08 17:39:15,542 [Epoch: 053 Step: 00001800] Batch Recognition Loss:   0.072295 => Gls Tokens per Sec:     2080 || Batch Translation Loss:  10.184749 => Txt Tokens per Sec:     5770 || Lr: 0.000100
2024-02-08 17:39:15,779 Epoch  53: Total Training Recognition Loss 2.41  Total Training Translation Loss 507.91 
2024-02-08 17:39:15,779 EPOCH 54
2024-02-08 17:39:20,046 Epoch  54: Total Training Recognition Loss 2.28  Total Training Translation Loss 475.32 
2024-02-08 17:39:20,047 EPOCH 55
2024-02-08 17:39:24,692 Epoch  55: Total Training Recognition Loss 2.23  Total Training Translation Loss 457.24 
2024-02-08 17:39:24,693 EPOCH 56
2024-02-08 17:39:28,591 [Epoch: 056 Step: 00001900] Batch Recognition Loss:   0.055720 => Gls Tokens per Sec:     2397 || Batch Translation Loss:  19.850578 => Txt Tokens per Sec:     6544 || Lr: 0.000100
2024-02-08 17:39:29,374 Epoch  56: Total Training Recognition Loss 2.27  Total Training Translation Loss 434.14 
2024-02-08 17:39:29,374 EPOCH 57
2024-02-08 17:39:33,455 Epoch  57: Total Training Recognition Loss 2.19  Total Training Translation Loss 418.23 
2024-02-08 17:39:33,455 EPOCH 58
2024-02-08 17:39:38,296 Epoch  58: Total Training Recognition Loss 2.28  Total Training Translation Loss 382.21 
2024-02-08 17:39:38,296 EPOCH 59
2024-02-08 17:39:42,159 [Epoch: 059 Step: 00002000] Batch Recognition Loss:   0.074057 => Gls Tokens per Sec:     2254 || Batch Translation Loss:  14.070908 => Txt Tokens per Sec:     6437 || Lr: 0.000100
2024-02-08 17:39:51,174 Hooray! New best validation result [eval_metric]!
2024-02-08 17:39:51,175 Saving new checkpoint.
2024-02-08 17:39:51,425 Validation result at epoch  59, step     2000: duration: 9.2662s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.67966	Translation Loss: 66935.32031	PPL: 800.71826
	Eval Metric: BLEU
	WER 7.56	(DEL: 0.00,	INS: 0.00,	SUB: 7.56)
	BLEU-4 0.67	(BLEU-1: 12.76,	BLEU-2: 4.33,	BLEU-3: 1.69,	BLEU-4: 0.67)
	CHRF 17.07	ROUGE 10.61
2024-02-08 17:39:51,426 Logging Recognition and Translation Outputs
2024-02-08 17:39:51,426 ========================================================================================================================
2024-02-08 17:39:51,426 Logging Sequence: 165_414.00
2024-02-08 17:39:51,426 	Gloss Reference :	A B+C+D+E
2024-02-08 17:39:51,427 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:39:51,427 	Gloss Alignment :	         
2024-02-08 17:39:51,427 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:39:51,429 	Text Reference  :	he  felt sachin was  lucky so   he  always    gave his ******* sweater to   give it   to   the  umpire 
2024-02-08 17:39:51,429 	Text Hypothesis :	ipl has  now    have a     huge fan following on   his batting but     some some some some some players
2024-02-08 17:39:51,429 	Text Alignment  :	S   S    S      S    S     S    S   S         S        I       S       S    S    S    S    S    S      
2024-02-08 17:39:51,429 ========================================================================================================================
2024-02-08 17:39:51,429 Logging Sequence: 169_268.00
2024-02-08 17:39:51,429 	Gloss Reference :	A B+C+D+E
2024-02-08 17:39:51,429 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:39:51,430 	Gloss Alignment :	         
2024-02-08 17:39:51,430 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:39:51,431 	Text Reference  :	shami supports arshdeep and  many     fans supported him as **** **** **** ***** *** ** ****** well 
2024-02-08 17:39:51,431 	Text Hypothesis :	***** ******** fans     were supposed to   see       him as they have been ruled out on social media
2024-02-08 17:39:51,431 	Text Alignment  :	D     D        S        S    S        S    S                I    I    I    I     I   I  I      S    
2024-02-08 17:39:51,431 ========================================================================================================================
2024-02-08 17:39:51,431 Logging Sequence: 172_15.00
2024-02-08 17:39:51,432 	Gloss Reference :	A B+C+D+E
2024-02-08 17:39:51,432 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:39:51,432 	Gloss Alignment :	         
2024-02-08 17:39:51,432 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:39:51,434 	Text Reference  :	now in the final match   on    28   may 2023    the ***** two teams  were up  against each other at   the  same venue 
2024-02-08 17:39:51,434 	Text Hypothesis :	*** ** *** ***** however after this was stopped the world cup trophy he   was covered with his   bare with his  jersey
2024-02-08 17:39:51,434 	Text Alignment  :	D   D  D   D     S       S     S    S   S           I     S   S      S    S   S       S    S     S    S    S    S     
2024-02-08 17:39:51,435 ========================================================================================================================
2024-02-08 17:39:51,435 Logging Sequence: 96_158.00
2024-02-08 17:39:51,435 	Gloss Reference :	A B+C+D+E    
2024-02-08 17:39:51,435 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-08 17:39:51,435 	Gloss Alignment :	  S          
2024-02-08 17:39:51,435 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:39:51,436 	Text Reference  :	after this   pandya fell     on his knees in *** **** * ***** ** *** **** **** **** disappointment
2024-02-08 17:39:51,436 	Text Hypothesis :	the   couple were   supposed to be  held  in the next 5 years of the next next next next          
2024-02-08 17:39:51,437 	Text Alignment  :	S     S      S      S        S  S   S        I   I    I I     I  I   I    I    I    S             
2024-02-08 17:39:51,437 ========================================================================================================================
2024-02-08 17:39:51,437 Logging Sequence: 152_73.00
2024-02-08 17:39:51,437 	Gloss Reference :	A B+C+D+E
2024-02-08 17:39:51,437 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:39:51,437 	Gloss Alignment :	         
2024-02-08 17:39:51,437 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:39:51,438 	Text Reference  :	******* *** ***** eventually he   too got out           by ******* ***** *** ******** **** shaheen afridi   
2024-02-08 17:39:51,438 	Text Hypothesis :	however the match was        held at  his sportsmanship by shaking hands and pakistan will be      available
2024-02-08 17:39:51,438 	Text Alignment  :	I       I   I     S          S    S   S   S                I       I     I   I        I    S       S        
2024-02-08 17:39:51,439 ========================================================================================================================
2024-02-08 17:39:52,137 Epoch  59: Total Training Recognition Loss 2.05  Total Training Translation Loss 351.06 
2024-02-08 17:39:52,137 EPOCH 60
2024-02-08 17:39:57,101 Epoch  60: Total Training Recognition Loss 2.04  Total Training Translation Loss 345.61 
2024-02-08 17:39:57,101 EPOCH 61
2024-02-08 17:40:01,997 Epoch  61: Total Training Recognition Loss 2.03  Total Training Translation Loss 320.42 
2024-02-08 17:40:01,997 EPOCH 62
2024-02-08 17:40:05,256 [Epoch: 062 Step: 00002100] Batch Recognition Loss:   0.037388 => Gls Tokens per Sec:     2474 || Batch Translation Loss:   9.629452 => Txt Tokens per Sec:     6911 || Lr: 0.000100
2024-02-08 17:40:06,106 Epoch  62: Total Training Recognition Loss 2.02  Total Training Translation Loss 312.82 
2024-02-08 17:40:06,107 EPOCH 63
2024-02-08 17:40:11,169 Epoch  63: Total Training Recognition Loss 1.94  Total Training Translation Loss 292.88 
2024-02-08 17:40:11,170 EPOCH 64
2024-02-08 17:40:15,936 Epoch  64: Total Training Recognition Loss 2.02  Total Training Translation Loss 274.66 
2024-02-08 17:40:15,937 EPOCH 65
2024-02-08 17:40:19,088 [Epoch: 065 Step: 00002200] Batch Recognition Loss:   0.029176 => Gls Tokens per Sec:     2356 || Batch Translation Loss:   5.633029 => Txt Tokens per Sec:     6133 || Lr: 0.000100
2024-02-08 17:40:20,782 Epoch  65: Total Training Recognition Loss 1.86  Total Training Translation Loss 248.06 
2024-02-08 17:40:20,783 EPOCH 66
2024-02-08 17:40:25,021 Epoch  66: Total Training Recognition Loss 1.67  Total Training Translation Loss 227.64 
2024-02-08 17:40:25,022 EPOCH 67
2024-02-08 17:40:29,772 Epoch  67: Total Training Recognition Loss 1.75  Total Training Translation Loss 209.98 
2024-02-08 17:40:29,773 EPOCH 68
2024-02-08 17:40:32,396 [Epoch: 068 Step: 00002300] Batch Recognition Loss:   0.070652 => Gls Tokens per Sec:     2586 || Batch Translation Loss:   2.952607 => Txt Tokens per Sec:     6951 || Lr: 0.000100
2024-02-08 17:40:34,393 Epoch  68: Total Training Recognition Loss 1.78  Total Training Translation Loss 197.22 
2024-02-08 17:40:34,393 EPOCH 69
2024-02-08 17:40:38,917 Epoch  69: Total Training Recognition Loss 1.65  Total Training Translation Loss 187.15 
2024-02-08 17:40:38,917 EPOCH 70
2024-02-08 17:40:43,766 Epoch  70: Total Training Recognition Loss 1.64  Total Training Translation Loss 174.44 
2024-02-08 17:40:43,766 EPOCH 71
2024-02-08 17:40:47,007 [Epoch: 071 Step: 00002400] Batch Recognition Loss:   0.046817 => Gls Tokens per Sec:     1896 || Batch Translation Loss:   5.362587 => Txt Tokens per Sec:     5504 || Lr: 0.000100
2024-02-08 17:40:48,850 Epoch  71: Total Training Recognition Loss 1.59  Total Training Translation Loss 168.98 
2024-02-08 17:40:48,851 EPOCH 72
2024-02-08 17:40:53,550 Epoch  72: Total Training Recognition Loss 1.45  Total Training Translation Loss 153.31 
2024-02-08 17:40:53,550 EPOCH 73
2024-02-08 17:40:58,594 Epoch  73: Total Training Recognition Loss 1.44  Total Training Translation Loss 144.45 
2024-02-08 17:40:58,594 EPOCH 74
2024-02-08 17:41:01,073 [Epoch: 074 Step: 00002500] Batch Recognition Loss:   0.027276 => Gls Tokens per Sec:     2220 || Batch Translation Loss:   5.390568 => Txt Tokens per Sec:     6094 || Lr: 0.000100
2024-02-08 17:41:03,425 Epoch  74: Total Training Recognition Loss 1.38  Total Training Translation Loss 135.70 
2024-02-08 17:41:03,425 EPOCH 75
2024-02-08 17:41:07,549 Epoch  75: Total Training Recognition Loss 1.44  Total Training Translation Loss 131.07 
2024-02-08 17:41:07,549 EPOCH 76
2024-02-08 17:41:12,437 Epoch  76: Total Training Recognition Loss 1.36  Total Training Translation Loss 121.60 
2024-02-08 17:41:12,438 EPOCH 77
2024-02-08 17:41:14,409 [Epoch: 077 Step: 00002600] Batch Recognition Loss:   0.022519 => Gls Tokens per Sec:     2600 || Batch Translation Loss:   3.675611 => Txt Tokens per Sec:     7483 || Lr: 0.000100
2024-02-08 17:41:16,874 Epoch  77: Total Training Recognition Loss 1.23  Total Training Translation Loss 114.25 
2024-02-08 17:41:16,874 EPOCH 78
2024-02-08 17:41:21,643 Epoch  78: Total Training Recognition Loss 1.27  Total Training Translation Loss 108.61 
2024-02-08 17:41:21,643 EPOCH 79
2024-02-08 17:41:26,238 Epoch  79: Total Training Recognition Loss 1.25  Total Training Translation Loss 101.37 
2024-02-08 17:41:26,238 EPOCH 80
2024-02-08 17:41:27,872 [Epoch: 080 Step: 00002700] Batch Recognition Loss:   0.092703 => Gls Tokens per Sec:     2743 || Batch Translation Loss:   0.993097 => Txt Tokens per Sec:     7571 || Lr: 0.000100
2024-02-08 17:41:30,814 Epoch  80: Total Training Recognition Loss 1.22  Total Training Translation Loss 94.02 
2024-02-08 17:41:30,815 EPOCH 81
2024-02-08 17:41:35,618 Epoch  81: Total Training Recognition Loss 1.29  Total Training Translation Loss 91.10 
2024-02-08 17:41:35,619 EPOCH 82
2024-02-08 17:41:39,934 Epoch  82: Total Training Recognition Loss 1.06  Total Training Translation Loss 86.87 
2024-02-08 17:41:39,934 EPOCH 83
2024-02-08 17:41:41,886 [Epoch: 083 Step: 00002800] Batch Recognition Loss:   0.015062 => Gls Tokens per Sec:     1835 || Batch Translation Loss:   2.440210 => Txt Tokens per Sec:     5439 || Lr: 0.000100
2024-02-08 17:41:44,955 Epoch  83: Total Training Recognition Loss 1.19  Total Training Translation Loss 84.00 
2024-02-08 17:41:44,955 EPOCH 84
2024-02-08 17:41:49,125 Epoch  84: Total Training Recognition Loss 1.13  Total Training Translation Loss 80.68 
2024-02-08 17:41:49,125 EPOCH 85
2024-02-08 17:41:54,103 Epoch  85: Total Training Recognition Loss 1.10  Total Training Translation Loss 75.89 
2024-02-08 17:41:54,104 EPOCH 86
2024-02-08 17:41:55,601 [Epoch: 086 Step: 00002900] Batch Recognition Loss:   0.033407 => Gls Tokens per Sec:     2139 || Batch Translation Loss:   2.591701 => Txt Tokens per Sec:     5880 || Lr: 0.000100
2024-02-08 17:41:58,543 Epoch  86: Total Training Recognition Loss 1.03  Total Training Translation Loss 71.91 
2024-02-08 17:41:58,543 EPOCH 87
2024-02-08 17:42:03,070 Epoch  87: Total Training Recognition Loss 1.03  Total Training Translation Loss 67.45 
2024-02-08 17:42:03,071 EPOCH 88
2024-02-08 17:42:07,895 Epoch  88: Total Training Recognition Loss 0.97  Total Training Translation Loss 63.69 
2024-02-08 17:42:07,895 EPOCH 89
2024-02-08 17:42:09,016 [Epoch: 089 Step: 00003000] Batch Recognition Loss:   0.030620 => Gls Tokens per Sec:     2286 || Batch Translation Loss:   2.054914 => Txt Tokens per Sec:     6592 || Lr: 0.000100
2024-02-08 17:42:11,999 Epoch  89: Total Training Recognition Loss 0.98  Total Training Translation Loss 60.84 
2024-02-08 17:42:11,999 EPOCH 90
2024-02-08 17:42:16,964 Epoch  90: Total Training Recognition Loss 0.91  Total Training Translation Loss 58.68 
2024-02-08 17:42:16,964 EPOCH 91
2024-02-08 17:42:21,409 Epoch  91: Total Training Recognition Loss 0.90  Total Training Translation Loss 56.43 
2024-02-08 17:42:21,410 EPOCH 92
2024-02-08 17:42:22,003 [Epoch: 092 Step: 00003100] Batch Recognition Loss:   0.013087 => Gls Tokens per Sec:     3238 || Batch Translation Loss:   1.722710 => Txt Tokens per Sec:     8636 || Lr: 0.000100
2024-02-08 17:42:26,157 Epoch  92: Total Training Recognition Loss 0.84  Total Training Translation Loss 53.56 
2024-02-08 17:42:26,158 EPOCH 93
2024-02-08 17:42:30,865 Epoch  93: Total Training Recognition Loss 0.88  Total Training Translation Loss 51.24 
2024-02-08 17:42:30,866 EPOCH 94
2024-02-08 17:42:35,766 Epoch  94: Total Training Recognition Loss 0.73  Total Training Translation Loss 48.51 
2024-02-08 17:42:35,766 EPOCH 95
2024-02-08 17:42:36,252 [Epoch: 095 Step: 00003200] Batch Recognition Loss:   0.026714 => Gls Tokens per Sec:     2645 || Batch Translation Loss:   0.920087 => Txt Tokens per Sec:     7076 || Lr: 0.000100
2024-02-08 17:42:40,498 Epoch  95: Total Training Recognition Loss 0.82  Total Training Translation Loss 47.02 
2024-02-08 17:42:40,499 EPOCH 96
2024-02-08 17:42:45,357 Epoch  96: Total Training Recognition Loss 0.73  Total Training Translation Loss 46.02 
2024-02-08 17:42:45,357 EPOCH 97
2024-02-08 17:42:50,341 Epoch  97: Total Training Recognition Loss 0.74  Total Training Translation Loss 44.67 
2024-02-08 17:42:50,341 EPOCH 98
2024-02-08 17:42:50,616 [Epoch: 098 Step: 00003300] Batch Recognition Loss:   0.023538 => Gls Tokens per Sec:     2344 || Batch Translation Loss:   1.078201 => Txt Tokens per Sec:     5711 || Lr: 0.000100
2024-02-08 17:42:55,244 Epoch  98: Total Training Recognition Loss 0.81  Total Training Translation Loss 43.67 
2024-02-08 17:42:55,244 EPOCH 99
2024-02-08 17:42:59,389 Epoch  99: Total Training Recognition Loss 0.72  Total Training Translation Loss 40.87 
2024-02-08 17:42:59,389 EPOCH 100
2024-02-08 17:43:03,800 [Epoch: 100 Step: 00003400] Batch Recognition Loss:   0.020821 => Gls Tokens per Sec:     2408 || Batch Translation Loss:   0.893569 => Txt Tokens per Sec:     6662 || Lr: 0.000100
2024-02-08 17:43:03,801 Epoch 100: Total Training Recognition Loss 0.67  Total Training Translation Loss 40.94 
2024-02-08 17:43:03,801 EPOCH 101
2024-02-08 17:43:08,698 Epoch 101: Total Training Recognition Loss 0.68  Total Training Translation Loss 39.23 
2024-02-08 17:43:08,699 EPOCH 102
2024-02-08 17:43:13,693 Epoch 102: Total Training Recognition Loss 0.67  Total Training Translation Loss 37.28 
2024-02-08 17:43:13,693 EPOCH 103
2024-02-08 17:43:18,364 [Epoch: 103 Step: 00003500] Batch Recognition Loss:   0.029991 => Gls Tokens per Sec:     2137 || Batch Translation Loss:   0.526594 => Txt Tokens per Sec:     5927 || Lr: 0.000100
2024-02-08 17:43:18,595 Epoch 103: Total Training Recognition Loss 0.60  Total Training Translation Loss 36.12 
2024-02-08 17:43:18,595 EPOCH 104
2024-02-08 17:43:23,555 Epoch 104: Total Training Recognition Loss 0.69  Total Training Translation Loss 35.16 
2024-02-08 17:43:23,556 EPOCH 105
2024-02-08 17:43:28,486 Epoch 105: Total Training Recognition Loss 0.64  Total Training Translation Loss 33.93 
2024-02-08 17:43:28,486 EPOCH 106
2024-02-08 17:43:32,672 [Epoch: 106 Step: 00003600] Batch Recognition Loss:   0.013351 => Gls Tokens per Sec:     2232 || Batch Translation Loss:   1.180814 => Txt Tokens per Sec:     6148 || Lr: 0.000100
2024-02-08 17:43:33,186 Epoch 106: Total Training Recognition Loss 0.59  Total Training Translation Loss 33.40 
2024-02-08 17:43:33,186 EPOCH 107
2024-02-08 17:43:38,056 Epoch 107: Total Training Recognition Loss 0.58  Total Training Translation Loss 32.44 
2024-02-08 17:43:38,057 EPOCH 108
2024-02-08 17:43:42,640 Epoch 108: Total Training Recognition Loss 0.55  Total Training Translation Loss 30.37 
2024-02-08 17:43:42,641 EPOCH 109
2024-02-08 17:43:46,873 [Epoch: 109 Step: 00003700] Batch Recognition Loss:   0.018822 => Gls Tokens per Sec:     2056 || Batch Translation Loss:   0.134283 => Txt Tokens per Sec:     5805 || Lr: 0.000100
2024-02-08 17:43:47,465 Epoch 109: Total Training Recognition Loss 0.57  Total Training Translation Loss 30.26 
2024-02-08 17:43:47,465 EPOCH 110
2024-02-08 17:43:51,885 Epoch 110: Total Training Recognition Loss 0.51  Total Training Translation Loss 30.24 
2024-02-08 17:43:51,885 EPOCH 111
2024-02-08 17:43:56,437 Epoch 111: Total Training Recognition Loss 0.58  Total Training Translation Loss 27.90 
2024-02-08 17:43:56,437 EPOCH 112
2024-02-08 17:44:00,202 [Epoch: 112 Step: 00003800] Batch Recognition Loss:   0.027075 => Gls Tokens per Sec:     2141 || Batch Translation Loss:   0.291939 => Txt Tokens per Sec:     5989 || Lr: 0.000100
2024-02-08 17:44:01,222 Epoch 112: Total Training Recognition Loss 0.53  Total Training Translation Loss 27.82 
2024-02-08 17:44:01,222 EPOCH 113
2024-02-08 17:44:05,477 Epoch 113: Total Training Recognition Loss 0.45  Total Training Translation Loss 27.52 
2024-02-08 17:44:05,477 EPOCH 114
2024-02-08 17:44:10,540 Epoch 114: Total Training Recognition Loss 0.58  Total Training Translation Loss 25.79 
2024-02-08 17:44:10,541 EPOCH 115
2024-02-08 17:44:13,602 [Epoch: 115 Step: 00003900] Batch Recognition Loss:   0.019903 => Gls Tokens per Sec:     2425 || Batch Translation Loss:   0.567159 => Txt Tokens per Sec:     6601 || Lr: 0.000100
2024-02-08 17:44:14,814 Epoch 115: Total Training Recognition Loss 0.57  Total Training Translation Loss 29.56 
2024-02-08 17:44:14,814 EPOCH 116
2024-02-08 17:44:19,736 Epoch 116: Total Training Recognition Loss 0.52  Total Training Translation Loss 28.49 
2024-02-08 17:44:19,737 EPOCH 117
2024-02-08 17:44:24,588 Epoch 117: Total Training Recognition Loss 0.67  Total Training Translation Loss 29.08 
2024-02-08 17:44:24,589 EPOCH 118
2024-02-08 17:44:27,806 [Epoch: 118 Step: 00004000] Batch Recognition Loss:   0.018193 => Gls Tokens per Sec:     2108 || Batch Translation Loss:   0.570639 => Txt Tokens per Sec:     5687 || Lr: 0.000100
2024-02-08 17:44:36,193 Validation result at epoch 118, step     4000: duration: 8.3860s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.69559	Translation Loss: 80010.64062	PPL: 2955.65112
	Eval Metric: BLEU
	WER 6.07	(DEL: 0.00,	INS: 0.00,	SUB: 6.07)
	BLEU-4 0.54	(BLEU-1: 11.20,	BLEU-2: 3.65,	BLEU-3: 1.24,	BLEU-4: 0.54)
	CHRF 16.21	ROUGE 10.19
2024-02-08 17:44:36,194 Logging Recognition and Translation Outputs
2024-02-08 17:44:36,194 ========================================================================================================================
2024-02-08 17:44:36,194 Logging Sequence: 112_165.00
2024-02-08 17:44:36,195 	Gloss Reference :	A B+C+D+E
2024-02-08 17:44:36,195 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:44:36,195 	Gloss Alignment :	         
2024-02-08 17:44:36,195 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:44:36,196 	Text Reference  :	the narendra modi stadium will be       the home for  the  ahmedabad-based franchise
2024-02-08 17:44:36,196 	Text Hypothesis :	*** this     was  india'  best olympics as  they will lose in              dubai    
2024-02-08 17:44:36,196 	Text Alignment  :	D   S        S    S       S    S        S   S    S    S    S               S        
2024-02-08 17:44:36,197 ========================================================================================================================
2024-02-08 17:44:36,197 Logging Sequence: 176_154.00
2024-02-08 17:44:36,197 	Gloss Reference :	A B+C+D+E
2024-02-08 17:44:36,197 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:44:36,197 	Gloss Alignment :	         
2024-02-08 17:44:36,197 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:44:36,198 	Text Reference  :	* ** dahiya    could   potentially bring home india's second gold medal
2024-02-08 17:44:36,198 	Text Hypothesis :	i am extremely sadened by          her   win  and     lost   the  match
2024-02-08 17:44:36,198 	Text Alignment  :	I I  S         S       S           S     S    S       S      S    S    
2024-02-08 17:44:36,198 ========================================================================================================================
2024-02-08 17:44:36,199 Logging Sequence: 94_2.00
2024-02-08 17:44:36,199 	Gloss Reference :	A B+C+D+E
2024-02-08 17:44:36,199 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:44:36,199 	Gloss Alignment :	         
2024-02-08 17:44:36,199 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:44:36,201 	Text Reference  :	***** *** *** the icc  odi   men'    world cup 2023     will  be     hosted     by           india on  5th   october   2023         
2024-02-08 17:44:36,201 	Text Hypothesis :	india did not win this match between india and pakistan south africa bangladesh participated in    the world athletics championships
2024-02-08 17:44:36,201 	Text Alignment  :	I     I   I   S   S    S     S       S     S   S        S     S      S          S            S     S   S     S         S            
2024-02-08 17:44:36,201 ========================================================================================================================
2024-02-08 17:44:36,202 Logging Sequence: 165_453.00
2024-02-08 17:44:36,202 	Gloss Reference :	A B+C+D+E
2024-02-08 17:44:36,202 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:44:36,202 	Gloss Alignment :	         
2024-02-08 17:44:36,202 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:44:36,203 	Text Reference  :	icc did not  agree    to sehwag' decision of  wearing a  numberless jersey
2024-02-08 17:44:36,203 	Text Hypothesis :	he  has been selected to ******* see      him luck    in the        match 
2024-02-08 17:44:36,203 	Text Alignment  :	S   S   S    S           D       S        S   S       S  S          S     
2024-02-08 17:44:36,204 ========================================================================================================================
2024-02-08 17:44:36,204 Logging Sequence: 139_46.00
2024-02-08 17:44:36,204 	Gloss Reference :	A B+C+D+E
2024-02-08 17:44:36,204 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:44:36,204 	Gloss Alignment :	         
2024-02-08 17:44:36,204 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:44:36,206 	Text Reference  :	everyone thought it would be a one sided match because morocco is an      amateur team and belgium ranks 2nd       in     the world
2024-02-08 17:44:36,206 	Text Hypothesis :	******** ******* ** ***** ** * *** ***** ***** ******* ******* ** another indian  team *** played  in    argentina during the match
2024-02-08 17:44:36,206 	Text Alignment  :	D        D       D  D     D  D D   D     D     D       D       D  S       S            D   S       S     S         S          S    
2024-02-08 17:44:36,206 ========================================================================================================================
2024-02-08 17:44:38,166 Epoch 118: Total Training Recognition Loss 0.64  Total Training Translation Loss 29.45 
2024-02-08 17:44:38,167 EPOCH 119
2024-02-08 17:44:43,030 Epoch 119: Total Training Recognition Loss 0.76  Total Training Translation Loss 31.49 
2024-02-08 17:44:43,030 EPOCH 120
2024-02-08 17:44:47,938 Epoch 120: Total Training Recognition Loss 0.78  Total Training Translation Loss 27.55 
2024-02-08 17:44:47,938 EPOCH 121
2024-02-08 17:44:50,762 [Epoch: 121 Step: 00004100] Batch Recognition Loss:   0.012983 => Gls Tokens per Sec:     2175 || Batch Translation Loss:   1.152667 => Txt Tokens per Sec:     6078 || Lr: 0.000100
2024-02-08 17:44:52,865 Epoch 121: Total Training Recognition Loss 1.13  Total Training Translation Loss 38.05 
2024-02-08 17:44:52,865 EPOCH 122
2024-02-08 17:44:57,814 Epoch 122: Total Training Recognition Loss 0.68  Total Training Translation Loss 32.71 
2024-02-08 17:44:57,815 EPOCH 123
2024-02-08 17:45:02,753 Epoch 123: Total Training Recognition Loss 0.54  Total Training Translation Loss 30.74 
2024-02-08 17:45:02,753 EPOCH 124
2024-02-08 17:45:05,222 [Epoch: 124 Step: 00004200] Batch Recognition Loss:   0.005573 => Gls Tokens per Sec:     2335 || Batch Translation Loss:   0.715268 => Txt Tokens per Sec:     6281 || Lr: 0.000100
2024-02-08 17:45:07,718 Epoch 124: Total Training Recognition Loss 0.55  Total Training Translation Loss 27.64 
2024-02-08 17:45:07,719 EPOCH 125
2024-02-08 17:45:11,950 Epoch 125: Total Training Recognition Loss 0.48  Total Training Translation Loss 27.32 
2024-02-08 17:45:11,950 EPOCH 126
2024-02-08 17:45:16,726 Epoch 126: Total Training Recognition Loss 0.45  Total Training Translation Loss 20.18 
2024-02-08 17:45:16,726 EPOCH 127
2024-02-08 17:45:18,964 [Epoch: 127 Step: 00004300] Batch Recognition Loss:   0.027023 => Gls Tokens per Sec:     2290 || Batch Translation Loss:   0.278682 => Txt Tokens per Sec:     6179 || Lr: 0.000100
2024-02-08 17:45:21,509 Epoch 127: Total Training Recognition Loss 0.44  Total Training Translation Loss 18.14 
2024-02-08 17:45:21,510 EPOCH 128
2024-02-08 17:45:26,391 Epoch 128: Total Training Recognition Loss 0.43  Total Training Translation Loss 19.09 
2024-02-08 17:45:26,392 EPOCH 129
2024-02-08 17:45:30,975 Epoch 129: Total Training Recognition Loss 0.40  Total Training Translation Loss 16.24 
2024-02-08 17:45:30,975 EPOCH 130
2024-02-08 17:45:32,895 [Epoch: 130 Step: 00004400] Batch Recognition Loss:   0.008584 => Gls Tokens per Sec:     2200 || Batch Translation Loss:   0.630196 => Txt Tokens per Sec:     6276 || Lr: 0.000100
2024-02-08 17:45:35,232 Epoch 130: Total Training Recognition Loss 0.39  Total Training Translation Loss 14.88 
2024-02-08 17:45:35,232 EPOCH 131
2024-02-08 17:45:40,152 Epoch 131: Total Training Recognition Loss 0.30  Total Training Translation Loss 13.85 
2024-02-08 17:45:40,152 EPOCH 132
2024-02-08 17:45:44,382 Epoch 132: Total Training Recognition Loss 0.33  Total Training Translation Loss 12.99 
2024-02-08 17:45:44,382 EPOCH 133
2024-02-08 17:45:46,238 [Epoch: 133 Step: 00004500] Batch Recognition Loss:   0.029517 => Gls Tokens per Sec:     2070 || Batch Translation Loss:   0.119193 => Txt Tokens per Sec:     5744 || Lr: 0.000100
2024-02-08 17:45:49,370 Epoch 133: Total Training Recognition Loss 0.34  Total Training Translation Loss 12.68 
2024-02-08 17:45:49,370 EPOCH 134
2024-02-08 17:45:53,762 Epoch 134: Total Training Recognition Loss 0.31  Total Training Translation Loss 13.67 
2024-02-08 17:45:53,762 EPOCH 135
2024-02-08 17:45:58,591 Epoch 135: Total Training Recognition Loss 0.30  Total Training Translation Loss 14.19 
2024-02-08 17:45:58,592 EPOCH 136
2024-02-08 17:45:59,912 [Epoch: 136 Step: 00004600] Batch Recognition Loss:   0.013401 => Gls Tokens per Sec:     2426 || Batch Translation Loss:   0.529784 => Txt Tokens per Sec:     6741 || Lr: 0.000100
2024-02-08 17:46:03,107 Epoch 136: Total Training Recognition Loss 0.27  Total Training Translation Loss 13.88 
2024-02-08 17:46:03,107 EPOCH 137
2024-02-08 17:46:07,226 Epoch 137: Total Training Recognition Loss 0.27  Total Training Translation Loss 13.79 
2024-02-08 17:46:07,226 EPOCH 138
2024-02-08 17:46:11,761 Epoch 138: Total Training Recognition Loss 0.27  Total Training Translation Loss 14.43 
2024-02-08 17:46:11,762 EPOCH 139
2024-02-08 17:46:13,288 [Epoch: 139 Step: 00004700] Batch Recognition Loss:   0.016766 => Gls Tokens per Sec:     1679 || Batch Translation Loss:   0.497621 => Txt Tokens per Sec:     5218 || Lr: 0.000100
2024-02-08 17:46:16,872 Epoch 139: Total Training Recognition Loss 0.33  Total Training Translation Loss 16.56 
2024-02-08 17:46:16,873 EPOCH 140
2024-02-08 17:46:21,321 Epoch 140: Total Training Recognition Loss 0.32  Total Training Translation Loss 15.33 
2024-02-08 17:46:21,321 EPOCH 141
2024-02-08 17:46:25,900 Epoch 141: Total Training Recognition Loss 0.35  Total Training Translation Loss 13.81 
2024-02-08 17:46:25,901 EPOCH 142
2024-02-08 17:46:26,590 [Epoch: 142 Step: 00004800] Batch Recognition Loss:   0.004892 => Gls Tokens per Sec:     2791 || Batch Translation Loss:   0.285721 => Txt Tokens per Sec:     7129 || Lr: 0.000100
2024-02-08 17:46:30,610 Epoch 142: Total Training Recognition Loss 0.33  Total Training Translation Loss 12.32 
2024-02-08 17:46:30,611 EPOCH 143
2024-02-08 17:46:34,974 Epoch 143: Total Training Recognition Loss 0.28  Total Training Translation Loss 12.10 
2024-02-08 17:46:34,975 EPOCH 144
2024-02-08 17:46:39,900 Epoch 144: Total Training Recognition Loss 0.29  Total Training Translation Loss 12.73 
2024-02-08 17:46:39,901 EPOCH 145
2024-02-08 17:46:40,359 [Epoch: 145 Step: 00004900] Batch Recognition Loss:   0.001553 => Gls Tokens per Sec:     2232 || Batch Translation Loss:   0.054739 => Txt Tokens per Sec:     5983 || Lr: 0.000100
2024-02-08 17:46:44,072 Epoch 145: Total Training Recognition Loss 0.29  Total Training Translation Loss 12.69 
2024-02-08 17:46:44,072 EPOCH 146
2024-02-08 17:46:48,841 Epoch 146: Total Training Recognition Loss 0.25  Total Training Translation Loss 12.69 
2024-02-08 17:46:48,841 EPOCH 147
2024-02-08 17:46:53,339 Epoch 147: Total Training Recognition Loss 0.33  Total Training Translation Loss 12.66 
2024-02-08 17:46:53,339 EPOCH 148
2024-02-08 17:46:53,511 [Epoch: 148 Step: 00005000] Batch Recognition Loss:   0.009567 => Gls Tokens per Sec:     3743 || Batch Translation Loss:   0.260944 => Txt Tokens per Sec:     9269 || Lr: 0.000100
2024-02-08 17:46:57,998 Epoch 148: Total Training Recognition Loss 0.52  Total Training Translation Loss 11.62 
2024-02-08 17:46:57,999 EPOCH 149
2024-02-08 17:47:03,090 Epoch 149: Total Training Recognition Loss 0.66  Total Training Translation Loss 12.07 
2024-02-08 17:47:03,091 EPOCH 150
2024-02-08 17:47:07,422 [Epoch: 150 Step: 00005100] Batch Recognition Loss:   0.004319 => Gls Tokens per Sec:     2453 || Batch Translation Loss:   0.417189 => Txt Tokens per Sec:     6785 || Lr: 0.000100
2024-02-08 17:47:07,422 Epoch 150: Total Training Recognition Loss 0.38  Total Training Translation Loss 12.26 
2024-02-08 17:47:07,423 EPOCH 151
2024-02-08 17:47:12,385 Epoch 151: Total Training Recognition Loss 0.33  Total Training Translation Loss 12.61 
2024-02-08 17:47:12,385 EPOCH 152
2024-02-08 17:47:16,806 Epoch 152: Total Training Recognition Loss 0.32  Total Training Translation Loss 12.51 
2024-02-08 17:47:16,806 EPOCH 153
2024-02-08 17:47:21,243 [Epoch: 153 Step: 00005200] Batch Recognition Loss:   0.012167 => Gls Tokens per Sec:     2250 || Batch Translation Loss:   0.274089 => Txt Tokens per Sec:     6206 || Lr: 0.000100
2024-02-08 17:47:21,540 Epoch 153: Total Training Recognition Loss 0.24  Total Training Translation Loss 11.96 
2024-02-08 17:47:21,541 EPOCH 154
2024-02-08 17:47:26,169 Epoch 154: Total Training Recognition Loss 0.28  Total Training Translation Loss 14.03 
2024-02-08 17:47:26,170 EPOCH 155
2024-02-08 17:47:30,264 Epoch 155: Total Training Recognition Loss 0.27  Total Training Translation Loss 14.61 
2024-02-08 17:47:30,264 EPOCH 156
2024-02-08 17:47:34,834 [Epoch: 156 Step: 00005300] Batch Recognition Loss:   0.004552 => Gls Tokens per Sec:     2102 || Batch Translation Loss:   0.614373 => Txt Tokens per Sec:     5889 || Lr: 0.000100
2024-02-08 17:47:35,320 Epoch 156: Total Training Recognition Loss 0.28  Total Training Translation Loss 14.05 
2024-02-08 17:47:35,320 EPOCH 157
2024-02-08 17:47:39,603 Epoch 157: Total Training Recognition Loss 0.27  Total Training Translation Loss 14.46 
2024-02-08 17:47:39,603 EPOCH 158
2024-02-08 17:47:44,553 Epoch 158: Total Training Recognition Loss 0.29  Total Training Translation Loss 15.54 
2024-02-08 17:47:44,553 EPOCH 159
2024-02-08 17:47:47,979 [Epoch: 159 Step: 00005400] Batch Recognition Loss:   0.008576 => Gls Tokens per Sec:     2540 || Batch Translation Loss:   0.206137 => Txt Tokens per Sec:     6941 || Lr: 0.000100
2024-02-08 17:47:48,997 Epoch 159: Total Training Recognition Loss 0.24  Total Training Translation Loss 12.06 
2024-02-08 17:47:48,997 EPOCH 160
2024-02-08 17:47:53,683 Epoch 160: Total Training Recognition Loss 0.27  Total Training Translation Loss 10.99 
2024-02-08 17:47:53,683 EPOCH 161
2024-02-08 17:47:58,333 Epoch 161: Total Training Recognition Loss 0.26  Total Training Translation Loss 10.24 
2024-02-08 17:47:58,333 EPOCH 162
2024-02-08 17:48:01,801 [Epoch: 162 Step: 00005500] Batch Recognition Loss:   0.010116 => Gls Tokens per Sec:     2325 || Batch Translation Loss:   0.150624 => Txt Tokens per Sec:     6550 || Lr: 0.000100
2024-02-08 17:48:02,853 Epoch 162: Total Training Recognition Loss 0.23  Total Training Translation Loss 10.41 
2024-02-08 17:48:02,854 EPOCH 163
2024-02-08 17:48:07,719 Epoch 163: Total Training Recognition Loss 0.29  Total Training Translation Loss 9.66 
2024-02-08 17:48:07,719 EPOCH 164
2024-02-08 17:48:12,269 Epoch 164: Total Training Recognition Loss 0.22  Total Training Translation Loss 8.71 
2024-02-08 17:48:12,270 EPOCH 165
2024-02-08 17:48:16,150 [Epoch: 165 Step: 00005600] Batch Recognition Loss:   0.004464 => Gls Tokens per Sec:     1980 || Batch Translation Loss:   0.310704 => Txt Tokens per Sec:     5691 || Lr: 0.000100
2024-02-08 17:48:17,225 Epoch 165: Total Training Recognition Loss 0.21  Total Training Translation Loss 9.28 
2024-02-08 17:48:17,225 EPOCH 166
2024-02-08 17:48:21,451 Epoch 166: Total Training Recognition Loss 0.23  Total Training Translation Loss 9.46 
2024-02-08 17:48:21,452 EPOCH 167
2024-02-08 17:48:26,163 Epoch 167: Total Training Recognition Loss 0.22  Total Training Translation Loss 10.55 
2024-02-08 17:48:26,164 EPOCH 168
2024-02-08 17:48:29,158 [Epoch: 168 Step: 00005700] Batch Recognition Loss:   0.001338 => Gls Tokens per Sec:     2265 || Batch Translation Loss:   0.266876 => Txt Tokens per Sec:     6080 || Lr: 0.000100
2024-02-08 17:48:30,791 Epoch 168: Total Training Recognition Loss 0.24  Total Training Translation Loss 10.29 
2024-02-08 17:48:30,791 EPOCH 169
2024-02-08 17:48:35,290 Epoch 169: Total Training Recognition Loss 0.23  Total Training Translation Loss 11.07 
2024-02-08 17:48:35,291 EPOCH 170
2024-02-08 17:48:40,129 Epoch 170: Total Training Recognition Loss 0.30  Total Training Translation Loss 15.46 
2024-02-08 17:48:40,130 EPOCH 171
2024-02-08 17:48:42,543 [Epoch: 171 Step: 00005800] Batch Recognition Loss:   0.002778 => Gls Tokens per Sec:     2652 || Batch Translation Loss:   0.467629 => Txt Tokens per Sec:     7229 || Lr: 0.000100
2024-02-08 17:48:44,341 Epoch 171: Total Training Recognition Loss 0.27  Total Training Translation Loss 15.62 
2024-02-08 17:48:44,342 EPOCH 172
2024-02-08 17:48:49,314 Epoch 172: Total Training Recognition Loss 0.26  Total Training Translation Loss 11.93 
2024-02-08 17:48:49,314 EPOCH 173
2024-02-08 17:48:53,563 Epoch 173: Total Training Recognition Loss 0.31  Total Training Translation Loss 10.17 
2024-02-08 17:48:53,563 EPOCH 174
2024-02-08 17:48:55,988 [Epoch: 174 Step: 00005900] Batch Recognition Loss:   0.017565 => Gls Tokens per Sec:     2377 || Batch Translation Loss:   0.227066 => Txt Tokens per Sec:     6623 || Lr: 0.000100
2024-02-08 17:48:58,664 Epoch 174: Total Training Recognition Loss 0.35  Total Training Translation Loss 9.24 
2024-02-08 17:48:58,664 EPOCH 175
2024-02-08 17:49:03,563 Epoch 175: Total Training Recognition Loss 0.26  Total Training Translation Loss 12.07 
2024-02-08 17:49:03,563 EPOCH 176
2024-02-08 17:49:07,685 Epoch 176: Total Training Recognition Loss 0.53  Total Training Translation Loss 19.92 
2024-02-08 17:49:07,685 EPOCH 177
2024-02-08 17:49:10,600 [Epoch: 177 Step: 00006000] Batch Recognition Loss:   0.030910 => Gls Tokens per Sec:     1757 || Batch Translation Loss:   0.478198 => Txt Tokens per Sec:     5154 || Lr: 0.000100
2024-02-08 17:49:19,159 Hooray! New best validation result [eval_metric]!
2024-02-08 17:49:19,160 Saving new checkpoint.
2024-02-08 17:49:19,443 Validation result at epoch 177, step     6000: duration: 8.8428s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.94545	Translation Loss: 84500.06250	PPL: 4627.99219
	Eval Metric: BLEU
	WER 5.86	(DEL: 0.00,	INS: 0.00,	SUB: 5.86)
	BLEU-4 0.80	(BLEU-1: 12.55,	BLEU-2: 4.03,	BLEU-3: 1.68,	BLEU-4: 0.80)
	CHRF 17.78	ROUGE 10.16
2024-02-08 17:49:19,445 Logging Recognition and Translation Outputs
2024-02-08 17:49:19,445 ========================================================================================================================
2024-02-08 17:49:19,445 Logging Sequence: 160_153.00
2024-02-08 17:49:19,446 	Gloss Reference :	A B+C+D+E
2024-02-08 17:49:19,446 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:49:19,446 	Gloss Alignment :	         
2024-02-08 17:49:19,446 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:49:19,448 	Text Reference  :	i have no hard feelings towards rohit sharma and he   will always have my full support   as he  is   my teammate
2024-02-08 17:49:19,448 	Text Hypothesis :	* **** ** **** ******** ******* ***** ****** now they will ****** **** ** be   wondering as you will be applied 
2024-02-08 17:49:19,448 	Text Alignment  :	D D    D  D    D        D       D     D      S   S         D      D    D  S    S            S   S    S  S       
2024-02-08 17:49:19,448 ========================================================================================================================
2024-02-08 17:49:19,448 Logging Sequence: 103_253.00
2024-02-08 17:49:19,448 	Gloss Reference :	A B+C+D+E
2024-02-08 17:49:19,449 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:49:19,449 	Gloss Alignment :	         
2024-02-08 17:49:19,449 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:49:19,449 	Text Reference  :	***** canada    is 3rd with 92   medals  
2024-02-08 17:49:19,449 	Text Hypothesis :	these cricketer is *** so   what happened
2024-02-08 17:49:19,450 	Text Alignment  :	I     S            D   S    S    S       
2024-02-08 17:49:19,450 ========================================================================================================================
2024-02-08 17:49:19,450 Logging Sequence: 155_25.00
2024-02-08 17:49:19,450 	Gloss Reference :	A B+C+D+E
2024-02-08 17:49:19,450 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:49:19,450 	Gloss Alignment :	         
2024-02-08 17:49:19,450 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:49:19,452 	Text Reference  :	****** *** ********* this is  because taliban overthrew the afghan government and took over the  country
2024-02-08 17:49:19,452 	Text Hypothesis :	people are desparate to   see the     taliban wanted    the ****** family     and it   is   very popular
2024-02-08 17:49:19,452 	Text Alignment  :	I      I   I         S    S   S               S             D      S              S    S    S    S      
2024-02-08 17:49:19,452 ========================================================================================================================
2024-02-08 17:49:19,452 Logging Sequence: 81_105.00
2024-02-08 17:49:19,452 	Gloss Reference :	A B+C+D+E
2024-02-08 17:49:19,453 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:49:19,453 	Gloss Alignment :	         
2024-02-08 17:49:19,453 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:49:19,454 	Text Reference  :	dhoni    was tagged in multiple such     posts    as  he   was the brand ambassador
2024-02-08 17:49:19,454 	Text Hypothesis :	everyone was ****** ** stunned  amrapali sapphire has been in  the ***** match     
2024-02-08 17:49:19,454 	Text Alignment  :	S            D      D  S        S        S        S   S    S       D     S         
2024-02-08 17:49:19,454 ========================================================================================================================
2024-02-08 17:49:19,454 Logging Sequence: 105_136.00
2024-02-08 17:49:19,455 	Gloss Reference :	A B+C+D+E
2024-02-08 17:49:19,455 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:49:19,455 	Gloss Alignment :	         
2024-02-08 17:49:19,455 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:49:19,456 	Text Reference  :	** **** *** beating him once    is  my       biggest dream
2024-02-08 17:49:19,456 	Text Hypothesis :	as they had to      be  careful the incident as      well 
2024-02-08 17:49:19,456 	Text Alignment  :	I  I    I   S       S   S       S   S        S       S    
2024-02-08 17:49:19,456 ========================================================================================================================
2024-02-08 17:49:21,641 Epoch 177: Total Training Recognition Loss 0.46  Total Training Translation Loss 12.48 
2024-02-08 17:49:21,642 EPOCH 178
2024-02-08 17:49:26,188 Epoch 178: Total Training Recognition Loss 0.35  Total Training Translation Loss 23.07 
2024-02-08 17:49:26,189 EPOCH 179
2024-02-08 17:49:30,792 Epoch 179: Total Training Recognition Loss 0.34  Total Training Translation Loss 19.06 
2024-02-08 17:49:30,793 EPOCH 180
2024-02-08 17:49:32,217 [Epoch: 180 Step: 00006100] Batch Recognition Loss:   0.022430 => Gls Tokens per Sec:     3148 || Batch Translation Loss:   0.354753 => Txt Tokens per Sec:     7609 || Lr: 0.000100
2024-02-08 17:49:35,587 Epoch 180: Total Training Recognition Loss 0.38  Total Training Translation Loss 17.21 
2024-02-08 17:49:35,587 EPOCH 181
2024-02-08 17:49:39,861 Epoch 181: Total Training Recognition Loss 0.41  Total Training Translation Loss 15.95 
2024-02-08 17:49:39,861 EPOCH 182
2024-02-08 17:49:44,854 Epoch 182: Total Training Recognition Loss 0.27  Total Training Translation Loss 10.22 
2024-02-08 17:49:44,855 EPOCH 183
2024-02-08 17:49:46,410 [Epoch: 183 Step: 00006200] Batch Recognition Loss:   0.002848 => Gls Tokens per Sec:     2471 || Batch Translation Loss:   0.287230 => Txt Tokens per Sec:     6826 || Lr: 0.000100
2024-02-08 17:49:48,990 Epoch 183: Total Training Recognition Loss 0.22  Total Training Translation Loss 7.74 
2024-02-08 17:49:48,990 EPOCH 184
2024-02-08 17:49:53,955 Epoch 184: Total Training Recognition Loss 0.23  Total Training Translation Loss 7.06 
2024-02-08 17:49:53,955 EPOCH 185
2024-02-08 17:49:58,319 Epoch 185: Total Training Recognition Loss 0.14  Total Training Translation Loss 6.01 
2024-02-08 17:49:58,319 EPOCH 186
2024-02-08 17:49:59,487 [Epoch: 186 Step: 00006300] Batch Recognition Loss:   0.001563 => Gls Tokens per Sec:     2744 || Batch Translation Loss:   0.220571 => Txt Tokens per Sec:     7231 || Lr: 0.000100
2024-02-08 17:50:03,329 Epoch 186: Total Training Recognition Loss 0.21  Total Training Translation Loss 4.83 
2024-02-08 17:50:03,330 EPOCH 187
2024-02-08 17:50:07,773 Epoch 187: Total Training Recognition Loss 0.18  Total Training Translation Loss 4.60 
2024-02-08 17:50:07,774 EPOCH 188
2024-02-08 17:50:12,557 Epoch 188: Total Training Recognition Loss 0.18  Total Training Translation Loss 4.55 
2024-02-08 17:50:12,558 EPOCH 189
2024-02-08 17:50:13,423 [Epoch: 189 Step: 00006400] Batch Recognition Loss:   0.003098 => Gls Tokens per Sec:     2662 || Batch Translation Loss:   0.130157 => Txt Tokens per Sec:     6919 || Lr: 0.000100
2024-02-08 17:50:17,084 Epoch 189: Total Training Recognition Loss 0.15  Total Training Translation Loss 4.23 
2024-02-08 17:50:17,084 EPOCH 190
2024-02-08 17:50:21,704 Epoch 190: Total Training Recognition Loss 0.13  Total Training Translation Loss 4.95 
2024-02-08 17:50:21,705 EPOCH 191
2024-02-08 17:50:26,549 Epoch 191: Total Training Recognition Loss 0.17  Total Training Translation Loss 4.86 
2024-02-08 17:50:26,550 EPOCH 192
2024-02-08 17:50:27,208 [Epoch: 192 Step: 00006500] Batch Recognition Loss:   0.006009 => Gls Tokens per Sec:     2922 || Batch Translation Loss:   0.127296 => Txt Tokens per Sec:     7508 || Lr: 0.000100
2024-02-08 17:50:31,405 Epoch 192: Total Training Recognition Loss 0.15  Total Training Translation Loss 5.40 
2024-02-08 17:50:31,405 EPOCH 193
2024-02-08 17:50:36,505 Epoch 193: Total Training Recognition Loss 0.18  Total Training Translation Loss 6.28 
2024-02-08 17:50:36,505 EPOCH 194
2024-02-08 17:50:41,274 Epoch 194: Total Training Recognition Loss 0.16  Total Training Translation Loss 8.23 
2024-02-08 17:50:41,274 EPOCH 195
2024-02-08 17:50:41,748 [Epoch: 195 Step: 00006600] Batch Recognition Loss:   0.001934 => Gls Tokens per Sec:     2156 || Batch Translation Loss:   0.103706 => Txt Tokens per Sec:     6290 || Lr: 0.000100
2024-02-08 17:50:46,287 Epoch 195: Total Training Recognition Loss 0.13  Total Training Translation Loss 6.97 
2024-02-08 17:50:46,288 EPOCH 196
2024-02-08 17:50:50,989 Epoch 196: Total Training Recognition Loss 0.17  Total Training Translation Loss 10.10 
2024-02-08 17:50:50,989 EPOCH 197
2024-02-08 17:50:55,959 Epoch 197: Total Training Recognition Loss 0.18  Total Training Translation Loss 10.21 
2024-02-08 17:50:55,960 EPOCH 198
2024-02-08 17:50:56,211 [Epoch: 198 Step: 00006700] Batch Recognition Loss:   0.002223 => Gls Tokens per Sec:     1520 || Batch Translation Loss:   0.080918 => Txt Tokens per Sec:     4520 || Lr: 0.000100
2024-02-08 17:51:00,830 Epoch 198: Total Training Recognition Loss 0.20  Total Training Translation Loss 7.88 
2024-02-08 17:51:00,831 EPOCH 199
2024-02-08 17:51:05,476 Epoch 199: Total Training Recognition Loss 0.16  Total Training Translation Loss 6.11 
2024-02-08 17:51:05,476 EPOCH 200
2024-02-08 17:51:10,457 [Epoch: 200 Step: 00006800] Batch Recognition Loss:   0.003289 => Gls Tokens per Sec:     2133 || Batch Translation Loss:   0.102797 => Txt Tokens per Sec:     5901 || Lr: 0.000100
2024-02-08 17:51:10,457 Epoch 200: Total Training Recognition Loss 0.16  Total Training Translation Loss 5.43 
2024-02-08 17:51:10,458 EPOCH 201
2024-02-08 17:51:15,111 Epoch 201: Total Training Recognition Loss 0.12  Total Training Translation Loss 5.24 
2024-02-08 17:51:15,111 EPOCH 202
2024-02-08 17:51:20,088 Epoch 202: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.17 
2024-02-08 17:51:20,088 EPOCH 203
2024-02-08 17:51:24,733 [Epoch: 203 Step: 00006900] Batch Recognition Loss:   0.003939 => Gls Tokens per Sec:     2149 || Batch Translation Loss:   0.158372 => Txt Tokens per Sec:     6050 || Lr: 0.000100
2024-02-08 17:51:24,880 Epoch 203: Total Training Recognition Loss 0.14  Total Training Translation Loss 4.49 
2024-02-08 17:51:24,880 EPOCH 204
2024-02-08 17:51:29,659 Epoch 204: Total Training Recognition Loss 0.13  Total Training Translation Loss 4.78 
2024-02-08 17:51:29,659 EPOCH 205
2024-02-08 17:51:34,564 Epoch 205: Total Training Recognition Loss 0.16  Total Training Translation Loss 4.47 
2024-02-08 17:51:34,565 EPOCH 206
2024-02-08 17:51:38,413 [Epoch: 206 Step: 00007000] Batch Recognition Loss:   0.002868 => Gls Tokens per Sec:     2428 || Batch Translation Loss:   0.196340 => Txt Tokens per Sec:     6605 || Lr: 0.000100
2024-02-08 17:51:39,279 Epoch 206: Total Training Recognition Loss 0.15  Total Training Translation Loss 5.02 
2024-02-08 17:51:39,280 EPOCH 207
2024-02-08 17:51:44,128 Epoch 207: Total Training Recognition Loss 0.18  Total Training Translation Loss 6.53 
2024-02-08 17:51:44,128 EPOCH 208
2024-02-08 17:51:48,773 Epoch 208: Total Training Recognition Loss 0.17  Total Training Translation Loss 6.47 
2024-02-08 17:51:48,774 EPOCH 209
2024-02-08 17:51:52,746 [Epoch: 209 Step: 00007100] Batch Recognition Loss:   0.002341 => Gls Tokens per Sec:     2191 || Batch Translation Loss:   0.273485 => Txt Tokens per Sec:     5979 || Lr: 0.000100
2024-02-08 17:51:53,728 Epoch 209: Total Training Recognition Loss 0.15  Total Training Translation Loss 7.43 
2024-02-08 17:51:53,728 EPOCH 210
2024-02-08 17:51:58,138 Epoch 210: Total Training Recognition Loss 0.15  Total Training Translation Loss 7.51 
2024-02-08 17:51:58,138 EPOCH 211
2024-02-08 17:52:02,662 Epoch 211: Total Training Recognition Loss 0.16  Total Training Translation Loss 6.61 
2024-02-08 17:52:02,662 EPOCH 212
2024-02-08 17:52:06,543 [Epoch: 212 Step: 00007200] Batch Recognition Loss:   0.000963 => Gls Tokens per Sec:     2077 || Batch Translation Loss:   0.335921 => Txt Tokens per Sec:     5853 || Lr: 0.000100
2024-02-08 17:52:07,517 Epoch 212: Total Training Recognition Loss 0.14  Total Training Translation Loss 6.77 
2024-02-08 17:52:07,517 EPOCH 213
2024-02-08 17:52:11,654 Epoch 213: Total Training Recognition Loss 0.17  Total Training Translation Loss 7.81 
2024-02-08 17:52:11,654 EPOCH 214
2024-02-08 17:52:16,669 Epoch 214: Total Training Recognition Loss 0.23  Total Training Translation Loss 10.95 
2024-02-08 17:52:16,670 EPOCH 215
2024-02-08 17:52:19,806 [Epoch: 215 Step: 00007300] Batch Recognition Loss:   0.004662 => Gls Tokens per Sec:     2367 || Batch Translation Loss:   0.274567 => Txt Tokens per Sec:     6731 || Lr: 0.000100
2024-02-08 17:52:20,900 Epoch 215: Total Training Recognition Loss 0.45  Total Training Translation Loss 13.42 
2024-02-08 17:52:20,901 EPOCH 216
2024-02-08 17:52:25,773 Epoch 216: Total Training Recognition Loss 0.28  Total Training Translation Loss 11.93 
2024-02-08 17:52:25,773 EPOCH 217
2024-02-08 17:52:30,222 Epoch 217: Total Training Recognition Loss 0.24  Total Training Translation Loss 11.19 
2024-02-08 17:52:30,222 EPOCH 218
2024-02-08 17:52:32,935 [Epoch: 218 Step: 00007400] Batch Recognition Loss:   0.005735 => Gls Tokens per Sec:     2502 || Batch Translation Loss:   0.592747 => Txt Tokens per Sec:     6845 || Lr: 0.000100
2024-02-08 17:52:34,935 Epoch 218: Total Training Recognition Loss 0.29  Total Training Translation Loss 13.64 
2024-02-08 17:52:34,936 EPOCH 219
2024-02-08 17:52:39,607 Epoch 219: Total Training Recognition Loss 0.89  Total Training Translation Loss 12.08 
2024-02-08 17:52:39,607 EPOCH 220
2024-02-08 17:52:44,151 Epoch 220: Total Training Recognition Loss 2.68  Total Training Translation Loss 10.09 
2024-02-08 17:52:44,152 EPOCH 221
2024-02-08 17:52:46,827 [Epoch: 221 Step: 00007500] Batch Recognition Loss:   0.105063 => Gls Tokens per Sec:     2394 || Batch Translation Loss:   0.153845 => Txt Tokens per Sec:     6550 || Lr: 0.000100
2024-02-08 17:52:49,045 Epoch 221: Total Training Recognition Loss 1.69  Total Training Translation Loss 8.56 
2024-02-08 17:52:49,045 EPOCH 222
2024-02-08 17:52:53,193 Epoch 222: Total Training Recognition Loss 0.72  Total Training Translation Loss 9.58 
2024-02-08 17:52:53,194 EPOCH 223
2024-02-08 17:52:58,191 Epoch 223: Total Training Recognition Loss 0.23  Total Training Translation Loss 9.43 
2024-02-08 17:52:58,191 EPOCH 224
2024-02-08 17:53:00,460 [Epoch: 224 Step: 00007600] Batch Recognition Loss:   0.006074 => Gls Tokens per Sec:     2425 || Batch Translation Loss:   0.681477 => Txt Tokens per Sec:     6680 || Lr: 0.000100
2024-02-08 17:53:02,443 Epoch 224: Total Training Recognition Loss 0.18  Total Training Translation Loss 10.26 
2024-02-08 17:53:02,444 EPOCH 225
2024-02-08 17:53:07,319 Epoch 225: Total Training Recognition Loss 0.19  Total Training Translation Loss 13.06 
2024-02-08 17:53:07,319 EPOCH 226
2024-02-08 17:53:11,749 Epoch 226: Total Training Recognition Loss 0.16  Total Training Translation Loss 11.27 
2024-02-08 17:53:11,749 EPOCH 227
2024-02-08 17:53:13,599 [Epoch: 227 Step: 00007700] Batch Recognition Loss:   0.000889 => Gls Tokens per Sec:     2769 || Batch Translation Loss:   0.198661 => Txt Tokens per Sec:     7818 || Lr: 0.000100
2024-02-08 17:53:16,383 Epoch 227: Total Training Recognition Loss 0.23  Total Training Translation Loss 10.78 
2024-02-08 17:53:16,384 EPOCH 228
2024-02-08 17:53:21,229 Epoch 228: Total Training Recognition Loss 0.20  Total Training Translation Loss 9.43 
2024-02-08 17:53:21,230 EPOCH 229
2024-02-08 17:53:26,165 Epoch 229: Total Training Recognition Loss 0.16  Total Training Translation Loss 5.19 
2024-02-08 17:53:26,165 EPOCH 230
2024-02-08 17:53:28,249 [Epoch: 230 Step: 00007800] Batch Recognition Loss:   0.019021 => Gls Tokens per Sec:     2027 || Batch Translation Loss:   0.107881 => Txt Tokens per Sec:     5727 || Lr: 0.000100
2024-02-08 17:53:30,905 Epoch 230: Total Training Recognition Loss 0.17  Total Training Translation Loss 3.40 
2024-02-08 17:53:30,906 EPOCH 231
2024-02-08 17:53:35,937 Epoch 231: Total Training Recognition Loss 0.14  Total Training Translation Loss 3.18 
2024-02-08 17:53:35,937 EPOCH 232
2024-02-08 17:53:40,838 Epoch 232: Total Training Recognition Loss 0.17  Total Training Translation Loss 3.38 
2024-02-08 17:53:40,838 EPOCH 233
2024-02-08 17:53:42,766 [Epoch: 233 Step: 00007900] Batch Recognition Loss:   0.002148 => Gls Tokens per Sec:     1994 || Batch Translation Loss:   0.073495 => Txt Tokens per Sec:     5860 || Lr: 0.000100
2024-02-08 17:53:45,760 Epoch 233: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.55 
2024-02-08 17:53:45,760 EPOCH 234
2024-02-08 17:53:50,442 Epoch 234: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.53 
2024-02-08 17:53:50,443 EPOCH 235
2024-02-08 17:53:55,380 Epoch 235: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.88 
2024-02-08 17:53:55,380 EPOCH 236
2024-02-08 17:53:56,834 [Epoch: 236 Step: 00008000] Batch Recognition Loss:   0.002640 => Gls Tokens per Sec:     2204 || Batch Translation Loss:   0.054774 => Txt Tokens per Sec:     6300 || Lr: 0.000100
2024-02-08 17:54:05,764 Validation result at epoch 236, step     8000: duration: 8.9303s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.66275	Translation Loss: 88913.33594	PPL: 7191.65234
	Eval Metric: BLEU
	WER 4.73	(DEL: 0.00,	INS: 0.00,	SUB: 4.73)
	BLEU-4 0.72	(BLEU-1: 11.98,	BLEU-2: 3.92,	BLEU-3: 1.58,	BLEU-4: 0.72)
	CHRF 17.78	ROUGE 10.07
2024-02-08 17:54:05,765 Logging Recognition and Translation Outputs
2024-02-08 17:54:05,765 ========================================================================================================================
2024-02-08 17:54:05,765 Logging Sequence: 180_236.00
2024-02-08 17:54:05,766 	Gloss Reference :	A B+C+D+E
2024-02-08 17:54:05,766 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:54:05,766 	Gloss Alignment :	         
2024-02-08 17:54:05,766 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:54:05,767 	Text Reference  :	however the  wrestlers returned to the protest site        at jantar mantar with  thier demands  
2024-02-08 17:54:05,767 	Text Hypothesis :	however they were      unable   to *** ******* participate in the    fir    which is    incorrect
2024-02-08 17:54:05,767 	Text Alignment  :	        S    S         S           D   D       S           S  S      S      S     S     S        
2024-02-08 17:54:05,768 ========================================================================================================================
2024-02-08 17:54:05,768 Logging Sequence: 111_154.00
2024-02-08 17:54:05,768 	Gloss Reference :	A B+C+D+E  
2024-02-08 17:54:05,768 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-08 17:54:05,768 	Gloss Alignment :	  S        
2024-02-08 17:54:05,768 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:54:05,770 	Text Reference  :	***** *** due   to       csk's  slow over   rate dhoni   was fined rs 12 lakh *** **** *********
2024-02-08 17:54:05,770 	Text Hypothesis :	after the first instance during a    season the  captain is  fined rs 12 lakh for slow over-rate
2024-02-08 17:54:05,770 	Text Alignment  :	I     I   S     S        S      S    S      S    S       S                    I   I    I        
2024-02-08 17:54:05,770 ========================================================================================================================
2024-02-08 17:54:05,770 Logging Sequence: 118_314.00
2024-02-08 17:54:05,770 	Gloss Reference :	A B+C+D+E
2024-02-08 17:54:05,770 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:54:05,771 	Gloss Alignment :	         
2024-02-08 17:54:05,771 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:54:05,771 	Text Reference  :	** wow  even the president had come to      watch  
2024-02-08 17:54:05,771 	Text Hypothesis :	so here are  the ********* *** **** various winners
2024-02-08 17:54:05,771 	Text Alignment  :	I  S    S        D         D   D    S       S      
2024-02-08 17:54:05,772 ========================================================================================================================
2024-02-08 17:54:05,772 Logging Sequence: 156_197.00
2024-02-08 17:54:05,772 	Gloss Reference :	A B+C+D+E
2024-02-08 17:54:05,772 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:54:05,772 	Gloss Alignment :	         
2024-02-08 17:54:05,772 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:54:05,773 	Text Reference  :	seattle orcas sor is   owned by   many investors including satya nadella microsoft ceo
2024-02-08 17:54:05,773 	Text Hypothesis :	she     did   not want to    book him  off       for       120   lakh    in        ipl
2024-02-08 17:54:05,774 	Text Alignment  :	S       S     S   S    S     S    S    S         S         S     S       S         S  
2024-02-08 17:54:05,774 ========================================================================================================================
2024-02-08 17:54:05,774 Logging Sequence: 183_159.00
2024-02-08 17:54:05,774 	Gloss Reference :	A B+C+D+E
2024-02-08 17:54:05,774 	Gloss Hypothesis:	A B+C+D  
2024-02-08 17:54:05,774 	Gloss Alignment :	  S      
2024-02-08 17:54:05,774 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:54:05,776 	Text Reference  :	however an exception to this is     virat   kohli   and his   wife anushka sharma who refuse to share    images of        their     daughter
2024-02-08 17:54:05,776 	Text Hypothesis :	******* ** ********* ** **** people praised cummins for being a    great   leader who ****** ** respects his    teammate' religious beliefs 
2024-02-08 17:54:05,776 	Text Alignment  :	D       D  D         D  D    S      S       S       S   S     S    S       S          D      D  S        S      S         S         S       
2024-02-08 17:54:05,777 ========================================================================================================================
2024-02-08 17:54:09,011 Epoch 236: Total Training Recognition Loss 0.11  Total Training Translation Loss 2.37 
2024-02-08 17:54:09,012 EPOCH 237
2024-02-08 17:54:13,938 Epoch 237: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.60 
2024-02-08 17:54:13,938 EPOCH 238
2024-02-08 17:54:18,101 Epoch 238: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.98 
2024-02-08 17:54:18,102 EPOCH 239
2024-02-08 17:54:19,492 [Epoch: 239 Step: 00008100] Batch Recognition Loss:   0.001145 => Gls Tokens per Sec:     1843 || Batch Translation Loss:   0.213419 => Txt Tokens per Sec:     5186 || Lr: 0.000100
2024-02-08 17:54:23,140 Epoch 239: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.35 
2024-02-08 17:54:23,141 EPOCH 240
2024-02-08 17:54:27,964 Epoch 240: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.67 
2024-02-08 17:54:27,964 EPOCH 241
2024-02-08 17:54:32,703 Epoch 241: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.38 
2024-02-08 17:54:32,703 EPOCH 242
2024-02-08 17:54:33,567 [Epoch: 242 Step: 00008200] Batch Recognition Loss:   0.002457 => Gls Tokens per Sec:     2227 || Batch Translation Loss:   0.091525 => Txt Tokens per Sec:     6352 || Lr: 0.000100
2024-02-08 17:54:37,446 Epoch 242: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.23 
2024-02-08 17:54:37,446 EPOCH 243
2024-02-08 17:54:42,199 Epoch 243: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.39 
2024-02-08 17:54:42,199 EPOCH 244
2024-02-08 17:54:47,107 Epoch 244: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.66 
2024-02-08 17:54:47,108 EPOCH 245
2024-02-08 17:54:47,998 [Epoch: 245 Step: 00008300] Batch Recognition Loss:   0.003119 => Gls Tokens per Sec:     1440 || Batch Translation Loss:   0.084591 => Txt Tokens per Sec:     4390 || Lr: 0.000100
2024-02-08 17:54:51,953 Epoch 245: Total Training Recognition Loss 0.09  Total Training Translation Loss 4.28 
2024-02-08 17:54:51,954 EPOCH 246
2024-02-08 17:54:56,822 Epoch 246: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.66 
2024-02-08 17:54:56,823 EPOCH 247
2024-02-08 17:55:01,684 Epoch 247: Total Training Recognition Loss 0.12  Total Training Translation Loss 4.72 
2024-02-08 17:55:01,684 EPOCH 248
2024-02-08 17:55:01,942 [Epoch: 248 Step: 00008400] Batch Recognition Loss:   0.005264 => Gls Tokens per Sec:     2500 || Batch Translation Loss:   0.248084 => Txt Tokens per Sec:     6039 || Lr: 0.000100
2024-02-08 17:55:06,658 Epoch 248: Total Training Recognition Loss 0.11  Total Training Translation Loss 6.95 
2024-02-08 17:55:06,659 EPOCH 249
2024-02-08 17:55:11,569 Epoch 249: Total Training Recognition Loss 0.10  Total Training Translation Loss 7.72 
2024-02-08 17:55:11,569 EPOCH 250
2024-02-08 17:55:16,378 [Epoch: 250 Step: 00008500] Batch Recognition Loss:   0.014084 => Gls Tokens per Sec:     2209 || Batch Translation Loss:   0.238172 => Txt Tokens per Sec:     6112 || Lr: 0.000100
2024-02-08 17:55:16,379 Epoch 250: Total Training Recognition Loss 0.13  Total Training Translation Loss 6.21 
2024-02-08 17:55:16,379 EPOCH 251
2024-02-08 17:55:21,178 Epoch 251: Total Training Recognition Loss 0.10  Total Training Translation Loss 6.73 
2024-02-08 17:55:21,179 EPOCH 252
2024-02-08 17:55:25,932 Epoch 252: Total Training Recognition Loss 0.09  Total Training Translation Loss 9.13 
2024-02-08 17:55:25,932 EPOCH 253
2024-02-08 17:55:30,409 [Epoch: 253 Step: 00008600] Batch Recognition Loss:   0.006896 => Gls Tokens per Sec:     2288 || Batch Translation Loss:   0.236075 => Txt Tokens per Sec:     6307 || Lr: 0.000100
2024-02-08 17:55:30,679 Epoch 253: Total Training Recognition Loss 0.22  Total Training Translation Loss 14.65 
2024-02-08 17:55:30,680 EPOCH 254
2024-02-08 17:55:35,461 Epoch 254: Total Training Recognition Loss 0.16  Total Training Translation Loss 9.76 
2024-02-08 17:55:35,462 EPOCH 255
2024-02-08 17:55:40,334 Epoch 255: Total Training Recognition Loss 0.16  Total Training Translation Loss 7.13 
2024-02-08 17:55:40,335 EPOCH 256
2024-02-08 17:55:44,579 [Epoch: 256 Step: 00008700] Batch Recognition Loss:   0.001176 => Gls Tokens per Sec:     2202 || Batch Translation Loss:   0.162933 => Txt Tokens per Sec:     6149 || Lr: 0.000100
2024-02-08 17:55:45,037 Epoch 256: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.42 
2024-02-08 17:55:45,037 EPOCH 257
2024-02-08 17:55:49,813 Epoch 257: Total Training Recognition Loss 0.15  Total Training Translation Loss 4.69 
2024-02-08 17:55:49,814 EPOCH 258
2024-02-08 17:55:54,684 Epoch 258: Total Training Recognition Loss 0.12  Total Training Translation Loss 3.53 
2024-02-08 17:55:54,684 EPOCH 259
2024-02-08 17:55:58,500 [Epoch: 259 Step: 00008800] Batch Recognition Loss:   0.001187 => Gls Tokens per Sec:     2282 || Batch Translation Loss:   0.061971 => Txt Tokens per Sec:     6165 || Lr: 0.000100
2024-02-08 17:55:59,489 Epoch 259: Total Training Recognition Loss 0.12  Total Training Translation Loss 3.62 
2024-02-08 17:55:59,489 EPOCH 260
2024-02-08 17:56:04,280 Epoch 260: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.10 
2024-02-08 17:56:04,280 EPOCH 261
2024-02-08 17:56:09,054 Epoch 261: Total Training Recognition Loss 0.12  Total Training Translation Loss 3.06 
2024-02-08 17:56:09,055 EPOCH 262
2024-02-08 17:56:12,962 [Epoch: 262 Step: 00008900] Batch Recognition Loss:   0.006692 => Gls Tokens per Sec:     2063 || Batch Translation Loss:   0.163340 => Txt Tokens per Sec:     5791 || Lr: 0.000100
2024-02-08 17:56:14,013 Epoch 262: Total Training Recognition Loss 0.12  Total Training Translation Loss 3.12 
2024-02-08 17:56:14,013 EPOCH 263
2024-02-08 17:56:18,889 Epoch 263: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.89 
2024-02-08 17:56:18,890 EPOCH 264
2024-02-08 17:56:23,738 Epoch 264: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.32 
2024-02-08 17:56:23,738 EPOCH 265
2024-02-08 17:56:27,238 [Epoch: 265 Step: 00009000] Batch Recognition Loss:   0.004913 => Gls Tokens per Sec:     2121 || Batch Translation Loss:   0.085572 => Txt Tokens per Sec:     5838 || Lr: 0.000100
2024-02-08 17:56:28,552 Epoch 265: Total Training Recognition Loss 0.09  Total Training Translation Loss 4.10 
2024-02-08 17:56:28,553 EPOCH 266
2024-02-08 17:56:33,586 Epoch 266: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.56 
2024-02-08 17:56:33,587 EPOCH 267
2024-02-08 17:56:38,367 Epoch 267: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.02 
2024-02-08 17:56:38,368 EPOCH 268
2024-02-08 17:56:40,856 [Epoch: 268 Step: 00009100] Batch Recognition Loss:   0.000733 => Gls Tokens per Sec:     2831 || Batch Translation Loss:   0.082514 => Txt Tokens per Sec:     7735 || Lr: 0.000100
2024-02-08 17:56:42,801 Epoch 268: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.93 
2024-02-08 17:56:42,802 EPOCH 269
2024-02-08 17:56:47,808 Epoch 269: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.83 
2024-02-08 17:56:47,808 EPOCH 270
2024-02-08 17:56:52,690 Epoch 270: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.05 
2024-02-08 17:56:52,690 EPOCH 271
2024-02-08 17:56:55,199 [Epoch: 271 Step: 00009200] Batch Recognition Loss:   0.003082 => Gls Tokens per Sec:     2552 || Batch Translation Loss:   0.045610 => Txt Tokens per Sec:     6754 || Lr: 0.000100
2024-02-08 17:56:57,713 Epoch 271: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.96 
2024-02-08 17:56:57,714 EPOCH 272
2024-02-08 17:57:02,648 Epoch 272: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.76 
2024-02-08 17:57:02,649 EPOCH 273
2024-02-08 17:57:07,597 Epoch 273: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.31 
2024-02-08 17:57:07,598 EPOCH 274
2024-02-08 17:57:10,063 [Epoch: 274 Step: 00009300] Batch Recognition Loss:   0.005113 => Gls Tokens per Sec:     2233 || Batch Translation Loss:   0.100868 => Txt Tokens per Sec:     6127 || Lr: 0.000100
2024-02-08 17:57:12,500 Epoch 274: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.18 
2024-02-08 17:57:12,500 EPOCH 275
2024-02-08 17:57:17,366 Epoch 275: Total Training Recognition Loss 0.09  Total Training Translation Loss 4.33 
2024-02-08 17:57:17,367 EPOCH 276
2024-02-08 17:57:22,121 Epoch 276: Total Training Recognition Loss 0.10  Total Training Translation Loss 5.83 
2024-02-08 17:57:22,121 EPOCH 277
2024-02-08 17:57:24,469 [Epoch: 277 Step: 00009400] Batch Recognition Loss:   0.004983 => Gls Tokens per Sec:     2071 || Batch Translation Loss:   0.155210 => Txt Tokens per Sec:     5708 || Lr: 0.000100
2024-02-08 17:57:27,065 Epoch 277: Total Training Recognition Loss 0.09  Total Training Translation Loss 5.54 
2024-02-08 17:57:27,066 EPOCH 278
2024-02-08 17:57:31,909 Epoch 278: Total Training Recognition Loss 0.12  Total Training Translation Loss 5.96 
2024-02-08 17:57:31,909 EPOCH 279
2024-02-08 17:57:36,718 Epoch 279: Total Training Recognition Loss 0.12  Total Training Translation Loss 5.22 
2024-02-08 17:57:36,718 EPOCH 280
2024-02-08 17:57:38,755 [Epoch: 280 Step: 00009500] Batch Recognition Loss:   0.003890 => Gls Tokens per Sec:     2201 || Batch Translation Loss:   0.133290 => Txt Tokens per Sec:     6041 || Lr: 0.000100
2024-02-08 17:57:41,548 Epoch 280: Total Training Recognition Loss 0.09  Total Training Translation Loss 5.93 
2024-02-08 17:57:41,548 EPOCH 281
2024-02-08 17:57:46,386 Epoch 281: Total Training Recognition Loss 0.10  Total Training Translation Loss 6.86 
2024-02-08 17:57:46,386 EPOCH 282
2024-02-08 17:57:51,272 Epoch 282: Total Training Recognition Loss 0.11  Total Training Translation Loss 5.95 
2024-02-08 17:57:51,273 EPOCH 283
2024-02-08 17:57:52,942 [Epoch: 283 Step: 00009600] Batch Recognition Loss:   0.001493 => Gls Tokens per Sec:     2146 || Batch Translation Loss:   0.148634 => Txt Tokens per Sec:     5887 || Lr: 0.000100
2024-02-08 17:57:56,137 Epoch 283: Total Training Recognition Loss 0.09  Total Training Translation Loss 4.19 
2024-02-08 17:57:56,137 EPOCH 284
2024-02-08 17:58:00,938 Epoch 284: Total Training Recognition Loss 0.11  Total Training Translation Loss 4.64 
2024-02-08 17:58:00,939 EPOCH 285
2024-02-08 17:58:05,741 Epoch 285: Total Training Recognition Loss 0.10  Total Training Translation Loss 5.69 
2024-02-08 17:58:05,741 EPOCH 286
2024-02-08 17:58:07,171 [Epoch: 286 Step: 00009700] Batch Recognition Loss:   0.001131 => Gls Tokens per Sec:     2239 || Batch Translation Loss:   0.212662 => Txt Tokens per Sec:     6479 || Lr: 0.000100
2024-02-08 17:58:10,668 Epoch 286: Total Training Recognition Loss 0.09  Total Training Translation Loss 6.02 
2024-02-08 17:58:10,669 EPOCH 287
2024-02-08 17:58:15,716 Epoch 287: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.59 
2024-02-08 17:58:15,717 EPOCH 288
2024-02-08 17:58:20,635 Epoch 288: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.89 
2024-02-08 17:58:20,636 EPOCH 289
2024-02-08 17:58:22,049 [Epoch: 289 Step: 00009800] Batch Recognition Loss:   0.003550 => Gls Tokens per Sec:     1630 || Batch Translation Loss:   0.177015 => Txt Tokens per Sec:     4572 || Lr: 0.000100
2024-02-08 17:58:25,442 Epoch 289: Total Training Recognition Loss 0.12  Total Training Translation Loss 6.46 
2024-02-08 17:58:25,443 EPOCH 290
2024-02-08 17:58:29,503 Epoch 290: Total Training Recognition Loss 0.11  Total Training Translation Loss 15.96 
2024-02-08 17:58:29,503 EPOCH 291
2024-02-08 17:58:34,055 Epoch 291: Total Training Recognition Loss 0.11  Total Training Translation Loss 8.27 
2024-02-08 17:58:34,055 EPOCH 292
2024-02-08 17:58:34,847 [Epoch: 292 Step: 00009900] Batch Recognition Loss:   0.001866 => Gls Tokens per Sec:     2427 || Batch Translation Loss:   0.166385 => Txt Tokens per Sec:     6755 || Lr: 0.000100
2024-02-08 17:58:38,855 Epoch 292: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.46 
2024-02-08 17:58:38,855 EPOCH 293
2024-02-08 17:58:43,617 Epoch 293: Total Training Recognition Loss 0.16  Total Training Translation Loss 4.77 
2024-02-08 17:58:43,617 EPOCH 294
2024-02-08 17:58:48,435 Epoch 294: Total Training Recognition Loss 0.13  Total Training Translation Loss 4.51 
2024-02-08 17:58:48,436 EPOCH 295
2024-02-08 17:58:49,045 [Epoch: 295 Step: 00010000] Batch Recognition Loss:   0.002626 => Gls Tokens per Sec:     2105 || Batch Translation Loss:   0.099777 => Txt Tokens per Sec:     6012 || Lr: 0.000100
2024-02-08 17:58:57,781 Hooray! New best validation result [eval_metric]!
2024-02-08 17:58:57,782 Saving new checkpoint.
2024-02-08 17:58:58,043 Validation result at epoch 295, step    10000: duration: 8.9980s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.87127	Translation Loss: 92395.35938	PPL: 10182.86035
	Eval Metric: BLEU
	WER 5.44	(DEL: 0.00,	INS: 0.00,	SUB: 5.44)
	BLEU-4 0.83	(BLEU-1: 12.53,	BLEU-2: 4.22,	BLEU-3: 1.68,	BLEU-4: 0.83)
	CHRF 17.56	ROUGE 10.45
2024-02-08 17:58:58,044 Logging Recognition and Translation Outputs
2024-02-08 17:58:58,044 ========================================================================================================================
2024-02-08 17:58:58,044 Logging Sequence: 123_147.00
2024-02-08 17:58:58,045 	Gloss Reference :	A B+C+D+E
2024-02-08 17:58:58,045 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:58:58,045 	Gloss Alignment :	         
2024-02-08 17:58:58,045 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:58:58,046 	Text Reference  :	the former captain also owns the pontiac firebird trans am car   worth rs 68                   lakh  
2024-02-08 17:58:58,046 	Text Hypothesis :	*** ****** ******* **** **** *** ******* ******** ***** ** dhoni is    a  once-in-a-generation player
2024-02-08 17:58:58,046 	Text Alignment  :	D   D      D       D    D    D   D       D        D     D  S     S     S  S                    S     
2024-02-08 17:58:58,046 ========================================================================================================================
2024-02-08 17:58:58,046 Logging Sequence: 58_196.00
2024-02-08 17:58:58,046 	Gloss Reference :	A B+C+D+E
2024-02-08 17:58:58,047 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:58:58,047 	Gloss Alignment :	         
2024-02-08 17:58:58,047 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:58:58,048 	Text Reference  :	the talents and skills of our athletes *** ********* ***** **** knows no  bounds    
2024-02-08 17:58:58,048 	Text Hypothesis :	*** while   28  years  of *** athletes are currently being held in    the tournament
2024-02-08 17:58:58,048 	Text Alignment  :	D   S       S   S         D            I   I         I     I    S     S   S         
2024-02-08 17:58:58,048 ========================================================================================================================
2024-02-08 17:58:58,048 Logging Sequence: 168_184.00
2024-02-08 17:58:58,048 	Gloss Reference :	A B+C+D+E
2024-02-08 17:58:58,049 	Gloss Hypothesis:	A B+C+D  
2024-02-08 17:58:58,049 	Gloss Alignment :	  S      
2024-02-08 17:58:58,049 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:58:58,051 	Text Reference  :	people say that we may get a true glimpse of    vamika in *** february   2022  when she     turns 1     year   old    
2024-02-08 17:58:58,051 	Text Hypothesis :	****** *** **** ** *** *** * this rule    would be     in the tournament after the  opening game  where mumbai indians
2024-02-08 17:58:58,051 	Text Alignment  :	D      D   D    D  D   D   D S    S       S     S         I   S          S     S    S       S     S     S      S      
2024-02-08 17:58:58,051 ========================================================================================================================
2024-02-08 17:58:58,051 Logging Sequence: 87_123.00
2024-02-08 17:58:58,051 	Gloss Reference :	A B+C+D+E
2024-02-08 17:58:58,051 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:58:58,051 	Gloss Alignment :	         
2024-02-08 17:58:58,052 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:58:58,053 	Text Reference  :	he said that  he hoped kl    rahul would  be      fit       for  the ******* ******* upcoming world cup     
2024-02-08 17:58:58,053 	Text Hypothesis :	** the  final of the   world cup   trophy however yesterday with the penalty between india    and   pakistan
2024-02-08 17:58:58,053 	Text Alignment  :	D  S    S     S  S     S     S     S      S       S         S        I       I       S        S     S       
2024-02-08 17:58:58,053 ========================================================================================================================
2024-02-08 17:58:58,054 Logging Sequence: 144_154.00
2024-02-08 17:58:58,054 	Gloss Reference :	A B+C+D+E
2024-02-08 17:58:58,054 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 17:58:58,054 	Gloss Alignment :	         
2024-02-08 17:58:58,054 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 17:58:58,056 	Text Reference  :	she also participated in the **** ********** rural olympic games organised in       rajasthan a     few months
2024-02-08 17:58:58,056 	Text Hypothesis :	*** the  final        of the u-19 tournament was   held    on    5th       february between   india and japan 
2024-02-08 17:58:58,056 	Text Alignment  :	D   S    S            S      I    I          S     S       S     S         S        S         S     S   S     
2024-02-08 17:58:58,056 ========================================================================================================================
2024-02-08 17:59:02,477 Epoch 295: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.33 
2024-02-08 17:59:02,477 EPOCH 296
2024-02-08 17:59:07,326 Epoch 296: Total Training Recognition Loss 0.10  Total Training Translation Loss 4.01 
2024-02-08 17:59:07,326 EPOCH 297
2024-02-08 17:59:12,237 Epoch 297: Total Training Recognition Loss 0.09  Total Training Translation Loss 4.98 
2024-02-08 17:59:12,237 EPOCH 298
2024-02-08 17:59:12,509 [Epoch: 298 Step: 00010100] Batch Recognition Loss:   0.000956 => Gls Tokens per Sec:     2370 || Batch Translation Loss:   0.099137 => Txt Tokens per Sec:     6552 || Lr: 0.000100
2024-02-08 17:59:17,067 Epoch 298: Total Training Recognition Loss 0.12  Total Training Translation Loss 4.80 
2024-02-08 17:59:17,068 EPOCH 299
2024-02-08 17:59:21,952 Epoch 299: Total Training Recognition Loss 0.11  Total Training Translation Loss 9.94 
2024-02-08 17:59:21,953 EPOCH 300
2024-02-08 17:59:26,458 [Epoch: 300 Step: 00010200] Batch Recognition Loss:   0.001830 => Gls Tokens per Sec:     2358 || Batch Translation Loss:   0.154651 => Txt Tokens per Sec:     6523 || Lr: 0.000100
2024-02-08 17:59:26,459 Epoch 300: Total Training Recognition Loss 0.13  Total Training Translation Loss 7.47 
2024-02-08 17:59:26,459 EPOCH 301
2024-02-08 17:59:31,419 Epoch 301: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.53 
2024-02-08 17:59:31,419 EPOCH 302
2024-02-08 17:59:36,396 Epoch 302: Total Training Recognition Loss 0.16  Total Training Translation Loss 5.35 
2024-02-08 17:59:36,397 EPOCH 303
2024-02-08 17:59:40,950 [Epoch: 303 Step: 00010300] Batch Recognition Loss:   0.002104 => Gls Tokens per Sec:     2192 || Batch Translation Loss:   0.065618 => Txt Tokens per Sec:     6075 || Lr: 0.000100
2024-02-08 17:59:41,211 Epoch 303: Total Training Recognition Loss 0.11  Total Training Translation Loss 2.97 
2024-02-08 17:59:41,211 EPOCH 304
2024-02-08 17:59:46,121 Epoch 304: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.07 
2024-02-08 17:59:46,121 EPOCH 305
2024-02-08 17:59:50,983 Epoch 305: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.14 
2024-02-08 17:59:50,984 EPOCH 306
2024-02-08 17:59:55,445 [Epoch: 306 Step: 00010400] Batch Recognition Loss:   0.000405 => Gls Tokens per Sec:     2094 || Batch Translation Loss:   0.015633 => Txt Tokens per Sec:     5883 || Lr: 0.000100
2024-02-08 17:59:55,891 Epoch 306: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.73 
2024-02-08 17:59:55,891 EPOCH 307
2024-02-08 18:00:00,784 Epoch 307: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.94 
2024-02-08 18:00:00,785 EPOCH 308
2024-02-08 18:00:05,640 Epoch 308: Total Training Recognition Loss 0.11  Total Training Translation Loss 1.97 
2024-02-08 18:00:05,641 EPOCH 309
2024-02-08 18:00:09,496 [Epoch: 309 Step: 00010500] Batch Recognition Loss:   0.001342 => Gls Tokens per Sec:     2258 || Batch Translation Loss:   0.053714 => Txt Tokens per Sec:     6237 || Lr: 0.000100
2024-02-08 18:00:10,481 Epoch 309: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.59 
2024-02-08 18:00:10,481 EPOCH 310
2024-02-08 18:00:15,276 Epoch 310: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.21 
2024-02-08 18:00:15,277 EPOCH 311
2024-02-08 18:00:20,077 Epoch 311: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.61 
2024-02-08 18:00:20,077 EPOCH 312
2024-02-08 18:00:23,702 [Epoch: 312 Step: 00010600] Batch Recognition Loss:   0.004550 => Gls Tokens per Sec:     2224 || Batch Translation Loss:   0.178906 => Txt Tokens per Sec:     6063 || Lr: 0.000100
2024-02-08 18:00:24,844 Epoch 312: Total Training Recognition Loss 0.14  Total Training Translation Loss 7.06 
2024-02-08 18:00:24,845 EPOCH 313
2024-02-08 18:00:29,549 Epoch 313: Total Training Recognition Loss 0.10  Total Training Translation Loss 7.26 
2024-02-08 18:00:29,549 EPOCH 314
2024-02-08 18:00:34,294 Epoch 314: Total Training Recognition Loss 0.10  Total Training Translation Loss 9.58 
2024-02-08 18:00:34,295 EPOCH 315
2024-02-08 18:00:37,763 [Epoch: 315 Step: 00010700] Batch Recognition Loss:   0.001267 => Gls Tokens per Sec:     2216 || Batch Translation Loss:   0.167631 => Txt Tokens per Sec:     6038 || Lr: 0.000100
2024-02-08 18:00:39,178 Epoch 315: Total Training Recognition Loss 0.08  Total Training Translation Loss 5.42 
2024-02-08 18:00:39,179 EPOCH 316
2024-02-08 18:00:44,094 Epoch 316: Total Training Recognition Loss 0.09  Total Training Translation Loss 5.09 
2024-02-08 18:00:44,095 EPOCH 317
2024-02-08 18:00:48,939 Epoch 317: Total Training Recognition Loss 0.11  Total Training Translation Loss 4.24 
2024-02-08 18:00:48,939 EPOCH 318
2024-02-08 18:00:52,091 [Epoch: 318 Step: 00010800] Batch Recognition Loss:   0.001403 => Gls Tokens per Sec:     2235 || Batch Translation Loss:   0.181332 => Txt Tokens per Sec:     6302 || Lr: 0.000100
2024-02-08 18:00:53,813 Epoch 318: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.62 
2024-02-08 18:00:53,813 EPOCH 319
2024-02-08 18:00:58,760 Epoch 319: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.71 
2024-02-08 18:00:58,761 EPOCH 320
2024-02-08 18:01:03,647 Epoch 320: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.89 
2024-02-08 18:01:03,648 EPOCH 321
2024-02-08 18:01:06,041 [Epoch: 321 Step: 00010900] Batch Recognition Loss:   0.002682 => Gls Tokens per Sec:     2567 || Batch Translation Loss:   0.061749 => Txt Tokens per Sec:     7089 || Lr: 0.000100
2024-02-08 18:01:07,858 Epoch 321: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.21 
2024-02-08 18:01:07,858 EPOCH 322
2024-02-08 18:01:11,927 Epoch 322: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.97 
2024-02-08 18:01:11,927 EPOCH 323
2024-02-08 18:01:16,042 Epoch 323: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.75 
2024-02-08 18:01:16,042 EPOCH 324
2024-02-08 18:01:17,933 [Epoch: 324 Step: 00011000] Batch Recognition Loss:   0.000493 => Gls Tokens per Sec:     3048 || Batch Translation Loss:   0.040743 => Txt Tokens per Sec:     8104 || Lr: 0.000100
2024-02-08 18:01:20,135 Epoch 324: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.78 
2024-02-08 18:01:20,135 EPOCH 325
2024-02-08 18:01:25,211 Epoch 325: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.00 
2024-02-08 18:01:25,211 EPOCH 326
2024-02-08 18:01:30,249 Epoch 326: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.36 
2024-02-08 18:01:30,250 EPOCH 327
2024-02-08 18:01:32,464 [Epoch: 327 Step: 00011100] Batch Recognition Loss:   0.000920 => Gls Tokens per Sec:     2196 || Batch Translation Loss:   0.038316 => Txt Tokens per Sec:     5899 || Lr: 0.000100
2024-02-08 18:01:35,162 Epoch 327: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.35 
2024-02-08 18:01:35,163 EPOCH 328
2024-02-08 18:01:40,052 Epoch 328: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.72 
2024-02-08 18:01:40,053 EPOCH 329
2024-02-08 18:01:44,730 Epoch 329: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.77 
2024-02-08 18:01:44,731 EPOCH 330
2024-02-08 18:01:46,508 [Epoch: 330 Step: 00011200] Batch Recognition Loss:   0.000437 => Gls Tokens per Sec:     2376 || Batch Translation Loss:   0.064678 => Txt Tokens per Sec:     6324 || Lr: 0.000100
2024-02-08 18:01:49,765 Epoch 330: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.10 
2024-02-08 18:01:49,765 EPOCH 331
2024-02-08 18:01:54,432 Epoch 331: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.00 
2024-02-08 18:01:54,432 EPOCH 332
2024-02-08 18:01:59,406 Epoch 332: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.29 
2024-02-08 18:01:59,407 EPOCH 333
2024-02-08 18:02:00,938 [Epoch: 333 Step: 00011300] Batch Recognition Loss:   0.003007 => Gls Tokens per Sec:     2340 || Batch Translation Loss:   0.182867 => Txt Tokens per Sec:     6270 || Lr: 0.000100
2024-02-08 18:02:04,253 Epoch 333: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.99 
2024-02-08 18:02:04,253 EPOCH 334
2024-02-08 18:02:09,040 Epoch 334: Total Training Recognition Loss 0.08  Total Training Translation Loss 6.71 
2024-02-08 18:02:09,041 EPOCH 335
2024-02-08 18:02:13,965 Epoch 335: Total Training Recognition Loss 0.08  Total Training Translation Loss 9.08 
2024-02-08 18:02:13,966 EPOCH 336
2024-02-08 18:02:15,103 [Epoch: 336 Step: 00011400] Batch Recognition Loss:   0.005760 => Gls Tokens per Sec:     2590 || Batch Translation Loss:   2.659433 => Txt Tokens per Sec:     7313 || Lr: 0.000100
2024-02-08 18:02:18,688 Epoch 336: Total Training Recognition Loss 0.12  Total Training Translation Loss 15.61 
2024-02-08 18:02:18,689 EPOCH 337
2024-02-08 18:02:23,494 Epoch 337: Total Training Recognition Loss 0.14  Total Training Translation Loss 10.58 
2024-02-08 18:02:23,494 EPOCH 338
2024-02-08 18:02:28,148 Epoch 338: Total Training Recognition Loss 0.14  Total Training Translation Loss 13.36 
2024-02-08 18:02:28,148 EPOCH 339
2024-02-08 18:02:29,074 [Epoch: 339 Step: 00011500] Batch Recognition Loss:   0.001579 => Gls Tokens per Sec:     2771 || Batch Translation Loss:   0.367171 => Txt Tokens per Sec:     7326 || Lr: 0.000100
2024-02-08 18:02:33,009 Epoch 339: Total Training Recognition Loss 0.26  Total Training Translation Loss 14.35 
2024-02-08 18:02:33,010 EPOCH 340
2024-02-08 18:02:37,652 Epoch 340: Total Training Recognition Loss 0.19  Total Training Translation Loss 10.24 
2024-02-08 18:02:37,652 EPOCH 341
2024-02-08 18:02:42,531 Epoch 341: Total Training Recognition Loss 0.18  Total Training Translation Loss 10.68 
2024-02-08 18:02:42,531 EPOCH 342
2024-02-08 18:02:43,245 [Epoch: 342 Step: 00011600] Batch Recognition Loss:   0.001567 => Gls Tokens per Sec:     2693 || Batch Translation Loss:   0.105730 => Txt Tokens per Sec:     7053 || Lr: 0.000100
2024-02-08 18:02:47,077 Epoch 342: Total Training Recognition Loss 0.17  Total Training Translation Loss 4.46 
2024-02-08 18:02:47,078 EPOCH 343
2024-02-08 18:02:51,286 Epoch 343: Total Training Recognition Loss 0.48  Total Training Translation Loss 2.67 
2024-02-08 18:02:51,286 EPOCH 344
2024-02-08 18:02:56,303 Epoch 344: Total Training Recognition Loss 2.55  Total Training Translation Loss 3.50 
2024-02-08 18:02:56,303 EPOCH 345
2024-02-08 18:02:56,719 [Epoch: 345 Step: 00011700] Batch Recognition Loss:   0.008662 => Gls Tokens per Sec:     3092 || Batch Translation Loss:   0.131739 => Txt Tokens per Sec:     7389 || Lr: 0.000100
2024-02-08 18:03:00,535 Epoch 345: Total Training Recognition Loss 2.54  Total Training Translation Loss 2.85 
2024-02-08 18:03:00,535 EPOCH 346
2024-02-08 18:03:05,504 Epoch 346: Total Training Recognition Loss 1.69  Total Training Translation Loss 2.82 
2024-02-08 18:03:05,505 EPOCH 347
2024-02-08 18:03:09,856 Epoch 347: Total Training Recognition Loss 4.05  Total Training Translation Loss 2.68 
2024-02-08 18:03:09,857 EPOCH 348
2024-02-08 18:03:10,202 [Epoch: 348 Step: 00011800] Batch Recognition Loss:   0.075246 => Gls Tokens per Sec:     1855 || Batch Translation Loss:   0.105343 => Txt Tokens per Sec:     5936 || Lr: 0.000100
2024-02-08 18:03:14,573 Epoch 348: Total Training Recognition Loss 0.46  Total Training Translation Loss 2.17 
2024-02-08 18:03:14,574 EPOCH 349
2024-02-08 18:03:19,167 Epoch 349: Total Training Recognition Loss 0.23  Total Training Translation Loss 1.69 
2024-02-08 18:03:19,167 EPOCH 350
2024-02-08 18:03:23,721 [Epoch: 350 Step: 00011900] Batch Recognition Loss:   0.000687 => Gls Tokens per Sec:     2333 || Batch Translation Loss:   0.074886 => Txt Tokens per Sec:     6453 || Lr: 0.000100
2024-02-08 18:03:23,722 Epoch 350: Total Training Recognition Loss 0.15  Total Training Translation Loss 2.17 
2024-02-08 18:03:23,722 EPOCH 351
2024-02-08 18:03:28,505 Epoch 351: Total Training Recognition Loss 0.12  Total Training Translation Loss 2.29 
2024-02-08 18:03:28,505 EPOCH 352
2024-02-08 18:03:32,961 Epoch 352: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.07 
2024-02-08 18:03:32,961 EPOCH 353
2024-02-08 18:03:37,454 [Epoch: 353 Step: 00012000] Batch Recognition Loss:   0.000418 => Gls Tokens per Sec:     2222 || Batch Translation Loss:   0.042480 => Txt Tokens per Sec:     6083 || Lr: 0.000100
2024-02-08 18:03:46,281 Validation result at epoch 353, step    12000: duration: 8.8276s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.74011	Translation Loss: 92937.74219	PPL: 10749.71680
	Eval Metric: BLEU
	WER 4.03	(DEL: 0.00,	INS: 0.00,	SUB: 4.03)
	BLEU-4 0.63	(BLEU-1: 11.20,	BLEU-2: 3.54,	BLEU-3: 1.40,	BLEU-4: 0.63)
	CHRF 17.14	ROUGE 9.57
2024-02-08 18:03:46,283 Logging Recognition and Translation Outputs
2024-02-08 18:03:46,283 ========================================================================================================================
2024-02-08 18:03:46,283 Logging Sequence: 168_56.00
2024-02-08 18:03:46,283 	Gloss Reference :	A B+C+D+E
2024-02-08 18:03:46,283 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:03:46,283 	Gloss Alignment :	         
2024-02-08 18:03:46,283 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:03:46,284 	Text Reference  :	** *** ****** fans    have  been waiting to see vamika for  a     long  time   
2024-02-08 18:03:46,285 	Text Hypothesis :	in the second innings india has  bowled  in the first  over while these attacks
2024-02-08 18:03:46,285 	Text Alignment  :	I  I   I      S       S     S    S       S  S   S      S    S     S     S      
2024-02-08 18:03:46,285 ========================================================================================================================
2024-02-08 18:03:46,285 Logging Sequence: 161_74.00
2024-02-08 18:03:46,285 	Gloss Reference :	A B+C+D+E
2024-02-08 18:03:46,285 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:03:46,286 	Gloss Alignment :	         
2024-02-08 18:03:46,286 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:03:46,287 	Text Reference  :	*** **** **** i     am     proud    of the ****** **** indian team's achievements
2024-02-08 18:03:46,287 	Text Hypothesis :	the bcci then rohit sharma escorted by the rising pune for    the    team        
2024-02-08 18:03:46,287 	Text Alignment  :	I   I    I    S     S      S        S      I      I    S      S      S           
2024-02-08 18:03:46,287 ========================================================================================================================
2024-02-08 18:03:46,287 Logging Sequence: 111_83.00
2024-02-08 18:03:46,287 	Gloss Reference :	A B+C+D+E
2024-02-08 18:03:46,287 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:03:46,287 	Gloss Alignment :	         
2024-02-08 18:03:46,288 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:03:46,289 	Text Reference  :	** and  the other 10 team members are     fined 25 of     the      match fee or    rs 6  lakh
2024-02-08 18:03:46,289 	Text Hypothesis :	in june the ***** ** **** bcci    doctors set   to resume training but   was fined rs 30 lakh
2024-02-08 18:03:46,289 	Text Alignment  :	I  S        D     D  D    S       S       S     S  S      S        S     S   S        S      
2024-02-08 18:03:46,290 ========================================================================================================================
2024-02-08 18:03:46,290 Logging Sequence: 61_218.00
2024-02-08 18:03:46,290 	Gloss Reference :	A B+C+D+E
2024-02-08 18:03:46,290 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:03:46,290 	Gloss Alignment :	         
2024-02-08 18:03:46,290 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:03:46,291 	Text Reference  :	in 2020 a woman had said    at the  press   conference
2024-02-08 18:03:46,291 	Text Hypothesis :	in **** a ***** *** caption on 14th october 2023      
2024-02-08 18:03:46,291 	Text Alignment  :	   D      D     D   S       S  S    S       S         
2024-02-08 18:03:46,291 ========================================================================================================================
2024-02-08 18:03:46,291 Logging Sequence: 94_123.00
2024-02-08 18:03:46,292 	Gloss Reference :	A B+C+D+E
2024-02-08 18:03:46,292 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:03:46,292 	Gloss Alignment :	         
2024-02-08 18:03:46,292 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:03:46,294 	Text Reference  :	**** * the venue   narendra modi stadium for the     india-pakistan match has     been kept the same    people  can book  flights etc
2024-02-08 18:03:46,294 	Text Hypothesis :	only 1 day earlier ie       14th october was decided by             14th  october and  does not majorly disrupt the plans of      ipl
2024-02-08 18:03:46,295 	Text Alignment  :	I    I S   S       S        S    S       S   S       S              S     S       S    S    S   S       S       S   S     S       S  
2024-02-08 18:03:46,295 ========================================================================================================================
2024-02-08 18:03:46,678 Epoch 353: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.90 
2024-02-08 18:03:46,678 EPOCH 354
2024-02-08 18:03:51,458 Epoch 354: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.50 
2024-02-08 18:03:51,458 EPOCH 355
2024-02-08 18:03:56,208 Epoch 355: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.39 
2024-02-08 18:03:56,208 EPOCH 356
2024-02-08 18:04:00,371 [Epoch: 356 Step: 00012100] Batch Recognition Loss:   0.000704 => Gls Tokens per Sec:     2245 || Batch Translation Loss:   0.027361 => Txt Tokens per Sec:     6197 || Lr: 0.000100
2024-02-08 18:04:00,792 Epoch 356: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.07 
2024-02-08 18:04:00,792 EPOCH 357
2024-02-08 18:04:05,270 Epoch 357: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.17 
2024-02-08 18:04:05,270 EPOCH 358
2024-02-08 18:04:10,157 Epoch 358: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.07 
2024-02-08 18:04:10,157 EPOCH 359
2024-02-08 18:04:13,575 [Epoch: 359 Step: 00012200] Batch Recognition Loss:   0.000381 => Gls Tokens per Sec:     2547 || Batch Translation Loss:   0.014709 => Txt Tokens per Sec:     7036 || Lr: 0.000100
2024-02-08 18:04:14,276 Epoch 359: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.39 
2024-02-08 18:04:14,277 EPOCH 360
2024-02-08 18:04:19,340 Epoch 360: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.75 
2024-02-08 18:04:19,340 EPOCH 361
2024-02-08 18:04:24,408 Epoch 361: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.60 
2024-02-08 18:04:24,409 EPOCH 362
2024-02-08 18:04:28,126 [Epoch: 362 Step: 00012300] Batch Recognition Loss:   0.001698 => Gls Tokens per Sec:     2169 || Batch Translation Loss:   0.071428 => Txt Tokens per Sec:     6102 || Lr: 0.000100
2024-02-08 18:04:29,164 Epoch 362: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.65 
2024-02-08 18:04:29,164 EPOCH 363
2024-02-08 18:04:33,320 Epoch 363: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.46 
2024-02-08 18:04:33,321 EPOCH 364
2024-02-08 18:04:38,248 Epoch 364: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.52 
2024-02-08 18:04:38,248 EPOCH 365
2024-02-08 18:04:41,449 [Epoch: 365 Step: 00012400] Batch Recognition Loss:   0.001079 => Gls Tokens per Sec:     2319 || Batch Translation Loss:   0.036203 => Txt Tokens per Sec:     6496 || Lr: 0.000100
2024-02-08 18:04:42,598 Epoch 365: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.38 
2024-02-08 18:04:42,598 EPOCH 366
2024-02-08 18:04:47,307 Epoch 366: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.11 
2024-02-08 18:04:47,307 EPOCH 367
2024-02-08 18:04:51,905 Epoch 367: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.39 
2024-02-08 18:04:51,905 EPOCH 368
2024-02-08 18:04:54,859 [Epoch: 368 Step: 00012500] Batch Recognition Loss:   0.000272 => Gls Tokens per Sec:     2296 || Batch Translation Loss:   0.112093 => Txt Tokens per Sec:     6562 || Lr: 0.000100
2024-02-08 18:04:56,474 Epoch 368: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.48 
2024-02-08 18:04:56,475 EPOCH 369
2024-02-08 18:05:01,316 Epoch 369: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.90 
2024-02-08 18:05:01,317 EPOCH 370
2024-02-08 18:05:05,402 Epoch 370: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.96 
2024-02-08 18:05:05,402 EPOCH 371
2024-02-08 18:05:08,483 [Epoch: 371 Step: 00012600] Batch Recognition Loss:   0.001607 => Gls Tokens per Sec:     1994 || Batch Translation Loss:   0.166497 => Txt Tokens per Sec:     5444 || Lr: 0.000100
2024-02-08 18:05:10,460 Epoch 371: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.53 
2024-02-08 18:05:10,461 EPOCH 372
2024-02-08 18:05:14,708 Epoch 372: Total Training Recognition Loss 0.05  Total Training Translation Loss 6.45 
2024-02-08 18:05:14,708 EPOCH 373
2024-02-08 18:05:19,623 Epoch 373: Total Training Recognition Loss 0.09  Total Training Translation Loss 6.90 
2024-02-08 18:05:19,624 EPOCH 374
2024-02-08 18:05:22,207 [Epoch: 374 Step: 00012700] Batch Recognition Loss:   0.000569 => Gls Tokens per Sec:     2130 || Batch Translation Loss:   0.213272 => Txt Tokens per Sec:     5953 || Lr: 0.000100
2024-02-08 18:05:24,120 Epoch 374: Total Training Recognition Loss 0.07  Total Training Translation Loss 5.92 
2024-02-08 18:05:24,121 EPOCH 375
2024-02-08 18:05:28,813 Epoch 375: Total Training Recognition Loss 0.07  Total Training Translation Loss 5.35 
2024-02-08 18:05:28,814 EPOCH 376
2024-02-08 18:05:33,497 Epoch 376: Total Training Recognition Loss 0.07  Total Training Translation Loss 7.08 
2024-02-08 18:05:33,497 EPOCH 377
2024-02-08 18:05:35,419 [Epoch: 377 Step: 00012800] Batch Recognition Loss:   0.001222 => Gls Tokens per Sec:     2667 || Batch Translation Loss:   0.120733 => Txt Tokens per Sec:     7526 || Lr: 0.000100
2024-02-08 18:05:37,922 Epoch 377: Total Training Recognition Loss 0.08  Total Training Translation Loss 6.61 
2024-02-08 18:05:37,923 EPOCH 378
2024-02-08 18:05:42,885 Epoch 378: Total Training Recognition Loss 0.08  Total Training Translation Loss 7.32 
2024-02-08 18:05:42,886 EPOCH 379
2024-02-08 18:05:47,022 Epoch 379: Total Training Recognition Loss 0.09  Total Training Translation Loss 5.70 
2024-02-08 18:05:47,022 EPOCH 380
2024-02-08 18:05:48,661 [Epoch: 380 Step: 00012900] Batch Recognition Loss:   0.000707 => Gls Tokens per Sec:     2576 || Batch Translation Loss:   0.150153 => Txt Tokens per Sec:     6700 || Lr: 0.000100
2024-02-08 18:05:52,013 Epoch 380: Total Training Recognition Loss 0.09  Total Training Translation Loss 6.66 
2024-02-08 18:05:52,013 EPOCH 381
2024-02-08 18:05:56,279 Epoch 381: Total Training Recognition Loss 0.12  Total Training Translation Loss 5.28 
2024-02-08 18:05:56,279 EPOCH 382
2024-02-08 18:06:01,147 Epoch 382: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.43 
2024-02-08 18:06:01,147 EPOCH 383
2024-02-08 18:06:02,695 [Epoch: 383 Step: 00013000] Batch Recognition Loss:   0.003242 => Gls Tokens per Sec:     2482 || Batch Translation Loss:   0.098975 => Txt Tokens per Sec:     6702 || Lr: 0.000100
2024-02-08 18:06:05,665 Epoch 383: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.42 
2024-02-08 18:06:05,665 EPOCH 384
2024-02-08 18:06:10,374 Epoch 384: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.95 
2024-02-08 18:06:10,375 EPOCH 385
2024-02-08 18:06:15,028 Epoch 385: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.09 
2024-02-08 18:06:15,028 EPOCH 386
2024-02-08 18:06:15,962 [Epoch: 386 Step: 00013100] Batch Recognition Loss:   0.000346 => Gls Tokens per Sec:     3433 || Batch Translation Loss:   0.030550 => Txt Tokens per Sec:     8758 || Lr: 0.000100
2024-02-08 18:06:19,497 Epoch 386: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.89 
2024-02-08 18:06:19,498 EPOCH 387
2024-02-08 18:06:24,541 Epoch 387: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.21 
2024-02-08 18:06:24,541 EPOCH 388
2024-02-08 18:06:29,286 Epoch 388: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.22 
2024-02-08 18:06:29,286 EPOCH 389
2024-02-08 18:06:30,225 [Epoch: 389 Step: 00013200] Batch Recognition Loss:   0.001168 => Gls Tokens per Sec:     2452 || Batch Translation Loss:   0.052820 => Txt Tokens per Sec:     5999 || Lr: 0.000100
2024-02-08 18:06:34,292 Epoch 389: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.40 
2024-02-08 18:06:34,293 EPOCH 390
2024-02-08 18:06:38,597 Epoch 390: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.03 
2024-02-08 18:06:38,597 EPOCH 391
2024-02-08 18:06:43,106 Epoch 391: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.31 
2024-02-08 18:06:43,107 EPOCH 392
2024-02-08 18:06:43,884 [Epoch: 392 Step: 00013300] Batch Recognition Loss:   0.000994 => Gls Tokens per Sec:     2474 || Batch Translation Loss:   0.041712 => Txt Tokens per Sec:     7040 || Lr: 0.000100
2024-02-08 18:06:47,884 Epoch 392: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.42 
2024-02-08 18:06:47,885 EPOCH 393
2024-02-08 18:06:52,474 Epoch 393: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.75 
2024-02-08 18:06:52,475 EPOCH 394
2024-02-08 18:06:57,544 Epoch 394: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.06 
2024-02-08 18:06:57,545 EPOCH 395
2024-02-08 18:06:58,051 [Epoch: 395 Step: 00013400] Batch Recognition Loss:   0.001750 => Gls Tokens per Sec:     2535 || Batch Translation Loss:   0.046369 => Txt Tokens per Sec:     7012 || Lr: 0.000100
2024-02-08 18:07:01,963 Epoch 395: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.61 
2024-02-08 18:07:01,963 EPOCH 396
2024-02-08 18:07:06,368 Epoch 396: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.66 
2024-02-08 18:07:06,369 EPOCH 397
2024-02-08 18:07:11,230 Epoch 397: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.58 
2024-02-08 18:07:11,230 EPOCH 398
2024-02-08 18:07:11,636 [Epoch: 398 Step: 00013500] Batch Recognition Loss:   0.000635 => Gls Tokens per Sec:     1580 || Batch Translation Loss:   0.041649 => Txt Tokens per Sec:     4180 || Lr: 0.000100
2024-02-08 18:07:16,242 Epoch 398: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.63 
2024-02-08 18:07:16,242 EPOCH 399
2024-02-08 18:07:20,997 Epoch 399: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.35 
2024-02-08 18:07:20,998 EPOCH 400
2024-02-08 18:07:25,854 [Epoch: 400 Step: 00013600] Batch Recognition Loss:   0.000780 => Gls Tokens per Sec:     2188 || Batch Translation Loss:   0.057883 => Txt Tokens per Sec:     6053 || Lr: 0.000100
2024-02-08 18:07:25,855 Epoch 400: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.49 
2024-02-08 18:07:25,855 EPOCH 401
2024-02-08 18:07:30,758 Epoch 401: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.31 
2024-02-08 18:07:30,759 EPOCH 402
2024-02-08 18:07:35,541 Epoch 402: Total Training Recognition Loss 0.06  Total Training Translation Loss 5.71 
2024-02-08 18:07:35,541 EPOCH 403
2024-02-08 18:07:40,382 [Epoch: 403 Step: 00013700] Batch Recognition Loss:   0.003117 => Gls Tokens per Sec:     2062 || Batch Translation Loss:   0.186280 => Txt Tokens per Sec:     5693 || Lr: 0.000100
2024-02-08 18:07:40,631 Epoch 403: Total Training Recognition Loss 0.08  Total Training Translation Loss 9.48 
2024-02-08 18:07:40,632 EPOCH 404
2024-02-08 18:07:45,450 Epoch 404: Total Training Recognition Loss 0.09  Total Training Translation Loss 8.07 
2024-02-08 18:07:45,450 EPOCH 405
2024-02-08 18:07:49,560 Epoch 405: Total Training Recognition Loss 0.08  Total Training Translation Loss 8.88 
2024-02-08 18:07:49,560 EPOCH 406
2024-02-08 18:07:53,825 [Epoch: 406 Step: 00013800] Batch Recognition Loss:   0.001165 => Gls Tokens per Sec:     2191 || Batch Translation Loss:   0.140223 => Txt Tokens per Sec:     6016 || Lr: 0.000100
2024-02-08 18:07:54,462 Epoch 406: Total Training Recognition Loss 0.09  Total Training Translation Loss 6.67 
2024-02-08 18:07:54,462 EPOCH 407
2024-02-08 18:07:58,946 Epoch 407: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.02 
2024-02-08 18:07:58,946 EPOCH 408
2024-02-08 18:08:03,664 Epoch 408: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.69 
2024-02-08 18:08:03,665 EPOCH 409
2024-02-08 18:08:07,676 [Epoch: 409 Step: 00013900] Batch Recognition Loss:   0.000308 => Gls Tokens per Sec:     2170 || Batch Translation Loss:   0.063813 => Txt Tokens per Sec:     6148 || Lr: 0.000100
2024-02-08 18:08:08,448 Epoch 409: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.44 
2024-02-08 18:08:08,448 EPOCH 410
2024-02-08 18:08:13,379 Epoch 410: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.28 
2024-02-08 18:08:13,380 EPOCH 411
2024-02-08 18:08:17,945 Epoch 411: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.95 
2024-02-08 18:08:17,946 EPOCH 412
2024-02-08 18:08:21,164 [Epoch: 412 Step: 00014000] Batch Recognition Loss:   0.000428 => Gls Tokens per Sec:     2586 || Batch Translation Loss:   0.057215 => Txt Tokens per Sec:     7103 || Lr: 0.000100
2024-02-08 18:08:30,056 Validation result at epoch 412, step    14000: duration: 8.8927s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.64353	Translation Loss: 94406.76562	PPL: 12448.57324
	Eval Metric: BLEU
	WER 4.03	(DEL: 0.00,	INS: 0.00,	SUB: 4.03)
	BLEU-4 0.51	(BLEU-1: 11.65,	BLEU-2: 3.74,	BLEU-3: 1.33,	BLEU-4: 0.51)
	CHRF 16.96	ROUGE 9.90
2024-02-08 18:08:30,057 Logging Recognition and Translation Outputs
2024-02-08 18:08:30,058 ========================================================================================================================
2024-02-08 18:08:30,058 Logging Sequence: 177_50.00
2024-02-08 18:08:30,058 	Gloss Reference :	A B+C+D+E
2024-02-08 18:08:30,058 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:08:30,058 	Gloss Alignment :	         
2024-02-08 18:08:30,058 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:08:30,060 	Text Reference  :	***** a     similar reward of    rs    50000  was announced    for    information against his associate ajay   kumar
2024-02-08 18:08:30,060 	Text Hypothesis :	after rana' death   a      delhi court issued a   non-bailable arrest warrant     against *** ********* sushil kumar
2024-02-08 18:08:30,060 	Text Alignment  :	I     S     S       S      S     S     S      S   S            S      S                   D   D         S           
2024-02-08 18:08:30,060 ========================================================================================================================
2024-02-08 18:08:30,060 Logging Sequence: 136_175.00
2024-02-08 18:08:30,060 	Gloss Reference :	A B+C+D+E      
2024-02-08 18:08:30,060 	Gloss Hypothesis:	A B+C+D+E+D+E+D
2024-02-08 18:08:30,061 	Gloss Alignment :	  S            
2024-02-08 18:08:30,061 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:08:30,062 	Text Reference  :	*** after     49          years     india' hockey team beat britain and qualified for the semi-finals
2024-02-08 18:08:30,062 	Text Hypothesis :	the broadcast advertisers ticketing etc    would  be   held in      the next      to  the world      
2024-02-08 18:08:30,062 	Text Alignment  :	I   S         S           S         S      S      S    S    S       S   S         S       S          
2024-02-08 18:08:30,062 ========================================================================================================================
2024-02-08 18:08:30,062 Logging Sequence: 126_159.00
2024-02-08 18:08:30,063 	Gloss Reference :	A B+C+D+E
2024-02-08 18:08:30,063 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:08:30,063 	Gloss Alignment :	         
2024-02-08 18:08:30,063 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:08:30,064 	Text Reference  :	despite multiple challenges and  injuries you did   not give    up   
2024-02-08 18:08:30,064 	Text Hypothesis :	******* ******** he         then took     a   medal in  javelin throw
2024-02-08 18:08:30,064 	Text Alignment  :	D       D        S          S    S        S   S     S   S       S    
2024-02-08 18:08:30,064 ========================================================================================================================
2024-02-08 18:08:30,064 Logging Sequence: 70_88.00
2024-02-08 18:08:30,064 	Gloss Reference :	A B+C+D+E
2024-02-08 18:08:30,064 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:08:30,064 	Gloss Alignment :	         
2024-02-08 18:08:30,065 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:08:30,066 	Text Reference  :	***** two  coca-cola bottles were placed   on   the table next  to the ******* mic     
2024-02-08 18:08:30,066 	Text Hypothesis :	after this there     was     a    backdrop with the ***** logos of the various sponsors
2024-02-08 18:08:30,066 	Text Alignment  :	I     S    S         S       S    S        S        D     S     S      I       S       
2024-02-08 18:08:30,066 ========================================================================================================================
2024-02-08 18:08:30,066 Logging Sequence: 54_201.00
2024-02-08 18:08:30,066 	Gloss Reference :	A B+C+D+E
2024-02-08 18:08:30,066 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:08:30,066 	Gloss Alignment :	         
2024-02-08 18:08:30,067 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:08:30,068 	Text Reference  :	there is a huge demand mostly from non-resident indians nris who are excited to see the match and they have  booked the hotel rooms
2024-02-08 18:08:30,068 	Text Hypothesis :	***** ** * **** ****** ****** **** ************ ******* **** *** *** ******* ** *** the bcci  can not  image of     the 2     years
2024-02-08 18:08:30,068 	Text Alignment  :	D     D  D D    D      D      D    D            D       D    D   D   D       D  D       S     S   S    S     S          S     S    
2024-02-08 18:08:30,068 ========================================================================================================================
2024-02-08 18:08:31,134 Epoch 412: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.15 
2024-02-08 18:08:31,134 EPOCH 413
2024-02-08 18:08:36,195 Epoch 413: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.13 
2024-02-08 18:08:36,196 EPOCH 414
2024-02-08 18:08:40,940 Epoch 414: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.07 
2024-02-08 18:08:40,941 EPOCH 415
2024-02-08 18:08:44,561 [Epoch: 415 Step: 00014100] Batch Recognition Loss:   0.003198 => Gls Tokens per Sec:     2051 || Batch Translation Loss:   0.044357 => Txt Tokens per Sec:     5784 || Lr: 0.000100
2024-02-08 18:08:45,849 Epoch 415: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.65 
2024-02-08 18:08:45,849 EPOCH 416
2024-02-08 18:08:50,539 Epoch 416: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.71 
2024-02-08 18:08:50,540 EPOCH 417
2024-02-08 18:08:55,370 Epoch 417: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.73 
2024-02-08 18:08:55,370 EPOCH 418
2024-02-08 18:08:58,609 [Epoch: 418 Step: 00014200] Batch Recognition Loss:   0.000242 => Gls Tokens per Sec:     2094 || Batch Translation Loss:   0.028949 => Txt Tokens per Sec:     5974 || Lr: 0.000100
2024-02-08 18:09:00,106 Epoch 418: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.45 
2024-02-08 18:09:00,106 EPOCH 419
2024-02-08 18:09:05,049 Epoch 419: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.53 
2024-02-08 18:09:05,050 EPOCH 420
2024-02-08 18:09:09,730 Epoch 420: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.57 
2024-02-08 18:09:09,731 EPOCH 421
2024-02-08 18:09:11,818 [Epoch: 421 Step: 00014300] Batch Recognition Loss:   0.000321 => Gls Tokens per Sec:     3067 || Batch Translation Loss:   0.093729 => Txt Tokens per Sec:     8164 || Lr: 0.000100
2024-02-08 18:09:13,886 Epoch 421: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.41 
2024-02-08 18:09:13,887 EPOCH 422
2024-02-08 18:09:18,809 Epoch 422: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.52 
2024-02-08 18:09:18,810 EPOCH 423
2024-02-08 18:09:23,481 Epoch 423: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.90 
2024-02-08 18:09:23,481 EPOCH 424
2024-02-08 18:09:25,826 [Epoch: 424 Step: 00014400] Batch Recognition Loss:   0.000348 => Gls Tokens per Sec:     2457 || Batch Translation Loss:   0.058072 => Txt Tokens per Sec:     6445 || Lr: 0.000100
2024-02-08 18:09:28,352 Epoch 424: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.67 
2024-02-08 18:09:28,352 EPOCH 425
2024-02-08 18:09:32,517 Epoch 425: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.81 
2024-02-08 18:09:32,517 EPOCH 426
2024-02-08 18:09:37,589 Epoch 426: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.92 
2024-02-08 18:09:37,589 EPOCH 427
2024-02-08 18:09:39,841 [Epoch: 427 Step: 00014500] Batch Recognition Loss:   0.000483 => Gls Tokens per Sec:     2159 || Batch Translation Loss:   0.073919 => Txt Tokens per Sec:     6161 || Lr: 0.000100
2024-02-08 18:09:41,878 Epoch 427: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.21 
2024-02-08 18:09:41,879 EPOCH 428
2024-02-08 18:09:46,715 Epoch 428: Total Training Recognition Loss 0.07  Total Training Translation Loss 6.10 
2024-02-08 18:09:46,716 EPOCH 429
2024-02-08 18:09:51,219 Epoch 429: Total Training Recognition Loss 0.08  Total Training Translation Loss 5.54 
2024-02-08 18:09:51,220 EPOCH 430
2024-02-08 18:09:52,898 [Epoch: 430 Step: 00014600] Batch Recognition Loss:   0.006094 => Gls Tokens per Sec:     2516 || Batch Translation Loss:   0.156926 => Txt Tokens per Sec:     7059 || Lr: 0.000100
2024-02-08 18:09:55,924 Epoch 430: Total Training Recognition Loss 0.07  Total Training Translation Loss 6.20 
2024-02-08 18:09:55,924 EPOCH 431
2024-02-08 18:10:00,620 Epoch 431: Total Training Recognition Loss 0.07  Total Training Translation Loss 5.19 
2024-02-08 18:10:00,620 EPOCH 432
2024-02-08 18:10:05,001 Epoch 432: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.63 
2024-02-08 18:10:05,001 EPOCH 433
2024-02-08 18:10:06,854 [Epoch: 433 Step: 00014700] Batch Recognition Loss:   0.000798 => Gls Tokens per Sec:     1933 || Batch Translation Loss:   0.065939 => Txt Tokens per Sec:     5363 || Lr: 0.000100
2024-02-08 18:10:09,912 Epoch 433: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.98 
2024-02-08 18:10:09,913 EPOCH 434
2024-02-08 18:10:14,033 Epoch 434: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.96 
2024-02-08 18:10:14,033 EPOCH 435
2024-02-08 18:10:19,079 Epoch 435: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.03 
2024-02-08 18:10:19,080 EPOCH 436
2024-02-08 18:10:20,309 [Epoch: 436 Step: 00014800] Batch Recognition Loss:   0.006145 => Gls Tokens per Sec:     2606 || Batch Translation Loss:   0.052646 => Txt Tokens per Sec:     6936 || Lr: 0.000100
2024-02-08 18:10:23,361 Epoch 436: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.94 
2024-02-08 18:10:23,361 EPOCH 437
2024-02-08 18:10:28,166 Epoch 437: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.46 
2024-02-08 18:10:28,167 EPOCH 438
2024-02-08 18:10:33,266 Epoch 438: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.03 
2024-02-08 18:10:33,266 EPOCH 439
2024-02-08 18:10:34,482 [Epoch: 439 Step: 00014900] Batch Recognition Loss:   0.000497 => Gls Tokens per Sec:     2108 || Batch Translation Loss:   0.036264 => Txt Tokens per Sec:     6333 || Lr: 0.000100
2024-02-08 18:10:37,821 Epoch 439: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.66 
2024-02-08 18:10:37,821 EPOCH 440
2024-02-08 18:10:42,795 Epoch 440: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.73 
2024-02-08 18:10:42,796 EPOCH 441
2024-02-08 18:10:47,156 Epoch 441: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.29 
2024-02-08 18:10:47,157 EPOCH 442
2024-02-08 18:10:47,884 [Epoch: 442 Step: 00015000] Batch Recognition Loss:   0.000322 => Gls Tokens per Sec:     2647 || Batch Translation Loss:   0.035107 => Txt Tokens per Sec:     6445 || Lr: 0.000100
2024-02-08 18:10:52,214 Epoch 442: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.39 
2024-02-08 18:10:52,215 EPOCH 443
2024-02-08 18:10:56,665 Epoch 443: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.05 
2024-02-08 18:10:56,666 EPOCH 444
2024-02-08 18:11:01,829 Epoch 444: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.84 
2024-02-08 18:11:01,830 EPOCH 445
2024-02-08 18:11:02,426 [Epoch: 445 Step: 00015100] Batch Recognition Loss:   0.001473 => Gls Tokens per Sec:     2151 || Batch Translation Loss:   0.178788 => Txt Tokens per Sec:     5761 || Lr: 0.000100
2024-02-08 18:11:06,881 Epoch 445: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.56 
2024-02-08 18:11:06,881 EPOCH 446
2024-02-08 18:11:11,094 Epoch 446: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.58 
2024-02-08 18:11:11,095 EPOCH 447
2024-02-08 18:11:16,079 Epoch 447: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.65 
2024-02-08 18:11:16,080 EPOCH 448
2024-02-08 18:11:16,434 [Epoch: 448 Step: 00015200] Batch Recognition Loss:   0.020992 => Gls Tokens per Sec:     1813 || Batch Translation Loss:   0.144248 => Txt Tokens per Sec:     5691 || Lr: 0.000100
2024-02-08 18:11:20,431 Epoch 448: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.39 
2024-02-08 18:11:20,431 EPOCH 449
2024-02-08 18:11:25,276 Epoch 449: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.51 
2024-02-08 18:11:25,277 EPOCH 450
2024-02-08 18:11:29,840 [Epoch: 450 Step: 00015300] Batch Recognition Loss:   0.001311 => Gls Tokens per Sec:     2328 || Batch Translation Loss:   0.018297 => Txt Tokens per Sec:     6440 || Lr: 0.000100
2024-02-08 18:11:29,840 Epoch 450: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.07 
2024-02-08 18:11:29,841 EPOCH 451
2024-02-08 18:11:34,118 Epoch 451: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.27 
2024-02-08 18:11:34,118 EPOCH 452
2024-02-08 18:11:39,092 Epoch 452: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.80 
2024-02-08 18:11:39,092 EPOCH 453
2024-02-08 18:11:43,020 [Epoch: 453 Step: 00015400] Batch Recognition Loss:   0.001017 => Gls Tokens per Sec:     2608 || Batch Translation Loss:   0.049843 => Txt Tokens per Sec:     7180 || Lr: 0.000100
2024-02-08 18:11:43,274 Epoch 453: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.16 
2024-02-08 18:11:43,275 EPOCH 454
2024-02-08 18:11:48,288 Epoch 454: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.86 
2024-02-08 18:11:48,289 EPOCH 455
2024-02-08 18:11:52,711 Epoch 455: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.97 
2024-02-08 18:11:52,711 EPOCH 456
2024-02-08 18:11:56,921 [Epoch: 456 Step: 00015500] Batch Recognition Loss:   0.000597 => Gls Tokens per Sec:     2219 || Batch Translation Loss:   0.007284 => Txt Tokens per Sec:     6176 || Lr: 0.000100
2024-02-08 18:11:57,439 Epoch 456: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.91 
2024-02-08 18:11:57,440 EPOCH 457
2024-02-08 18:12:02,039 Epoch 457: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.08 
2024-02-08 18:12:02,039 EPOCH 458
2024-02-08 18:12:06,571 Epoch 458: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.25 
2024-02-08 18:12:06,571 EPOCH 459
2024-02-08 18:12:10,610 [Epoch: 459 Step: 00015600] Batch Recognition Loss:   0.004504 => Gls Tokens per Sec:     2155 || Batch Translation Loss:   0.098041 => Txt Tokens per Sec:     6006 || Lr: 0.000100
2024-02-08 18:12:11,397 Epoch 459: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.66 
2024-02-08 18:12:11,398 EPOCH 460
2024-02-08 18:12:15,704 Epoch 460: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.35 
2024-02-08 18:12:15,705 EPOCH 461
2024-02-08 18:12:20,638 Epoch 461: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.65 
2024-02-08 18:12:20,639 EPOCH 462
2024-02-08 18:12:23,949 [Epoch: 462 Step: 00015700] Batch Recognition Loss:   0.000853 => Gls Tokens per Sec:     2435 || Batch Translation Loss:   0.076708 => Txt Tokens per Sec:     6888 || Lr: 0.000100
2024-02-08 18:12:24,793 Epoch 462: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.99 
2024-02-08 18:12:24,793 EPOCH 463
2024-02-08 18:12:29,729 Epoch 463: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.14 
2024-02-08 18:12:29,730 EPOCH 464
2024-02-08 18:12:34,165 Epoch 464: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.59 
2024-02-08 18:12:34,165 EPOCH 465
2024-02-08 18:12:37,713 [Epoch: 465 Step: 00015800] Batch Recognition Loss:   0.001263 => Gls Tokens per Sec:     2092 || Batch Translation Loss:   0.088764 => Txt Tokens per Sec:     5956 || Lr: 0.000100
2024-02-08 18:12:38,886 Epoch 465: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.20 
2024-02-08 18:12:38,886 EPOCH 466
2024-02-08 18:12:43,488 Epoch 466: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.82 
2024-02-08 18:12:43,488 EPOCH 467
2024-02-08 18:12:47,960 Epoch 467: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.16 
2024-02-08 18:12:47,961 EPOCH 468
2024-02-08 18:12:51,353 [Epoch: 468 Step: 00015900] Batch Recognition Loss:   0.000358 => Gls Tokens per Sec:     1999 || Batch Translation Loss:   0.076271 => Txt Tokens per Sec:     5746 || Lr: 0.000100
2024-02-08 18:12:52,807 Epoch 468: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.26 
2024-02-08 18:12:52,807 EPOCH 469
2024-02-08 18:12:57,004 Epoch 469: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.43 
2024-02-08 18:12:57,004 EPOCH 470
2024-02-08 18:13:02,095 Epoch 470: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.86 
2024-02-08 18:13:02,095 EPOCH 471
2024-02-08 18:13:05,000 [Epoch: 471 Step: 00016000] Batch Recognition Loss:   0.000396 => Gls Tokens per Sec:     2114 || Batch Translation Loss:   0.009169 => Txt Tokens per Sec:     6240 || Lr: 0.000100
2024-02-08 18:13:13,798 Validation result at epoch 471, step    16000: duration: 8.7970s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.63675	Translation Loss: 94130.39062	PPL: 12109.62988
	Eval Metric: BLEU
	WER 4.38	(DEL: 0.00,	INS: 0.00,	SUB: 4.38)
	BLEU-4 0.62	(BLEU-1: 11.08,	BLEU-2: 3.22,	BLEU-3: 1.20,	BLEU-4: 0.62)
	CHRF 16.52	ROUGE 9.37
2024-02-08 18:13:13,800 Logging Recognition and Translation Outputs
2024-02-08 18:13:13,800 ========================================================================================================================
2024-02-08 18:13:13,800 Logging Sequence: 163_116.00
2024-02-08 18:13:13,800 	Gloss Reference :	A B+C+D+E
2024-02-08 18:13:13,800 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:13:13,800 	Gloss Alignment :	         
2024-02-08 18:13:13,801 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:13:13,801 	Text Reference  :	people said that ***** she   looked similar to ** ******** ** virat   
2024-02-08 18:13:13,801 	Text Hypothesis :	they   say  that virat kohli and    respect to be pictures of pictures
2024-02-08 18:13:13,801 	Text Alignment  :	S      S         I     S     S      S          I  I        I  S       
2024-02-08 18:13:13,802 ========================================================================================================================
2024-02-08 18:13:13,802 Logging Sequence: 53_161.00
2024-02-08 18:13:13,802 	Gloss Reference :	A B+C+D+E
2024-02-08 18:13:13,802 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:13:13,802 	Gloss Alignment :	         
2024-02-08 18:13:13,802 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:13:13,804 	Text Reference  :	rashid has also been urging people to  donate  to      his      rashid  khan foundation  and   afghanistan cricket association
2024-02-08 18:13:13,804 	Text Hypothesis :	****** *** **** **** ****** ****** the taliban swiftly regained control of   afghanistan after us          troops  withdrawal 
2024-02-08 18:13:13,804 	Text Alignment  :	D      D   D    D    D      D      S   S       S       S        S       S    S           S     S           S       S          
2024-02-08 18:13:13,804 ========================================================================================================================
2024-02-08 18:13:13,804 Logging Sequence: 67_73.00
2024-02-08 18:13:13,804 	Gloss Reference :	A B+C+D+E
2024-02-08 18:13:13,804 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:13:13,805 	Gloss Alignment :	         
2024-02-08 18:13:13,805 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:13:13,805 	Text Reference  :	*** ***** **** **** ******** ** in        his tweet he  also said  
2024-02-08 18:13:13,806 	Text Hypothesis :	the first time that everyone is currently all out   for the  series
2024-02-08 18:13:13,806 	Text Alignment  :	I   I     I    I    I        I  S         S   S     S   S    S     
2024-02-08 18:13:13,806 ========================================================================================================================
2024-02-08 18:13:13,806 Logging Sequence: 137_44.00
2024-02-08 18:13:13,806 	Gloss Reference :	A B+C+D+E
2024-02-08 18:13:13,806 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:13:13,806 	Gloss Alignment :	         
2024-02-08 18:13:13,807 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:13:13,808 	Text Reference  :	let me tell you the rules that qatar  has announced for the fans travelling for the world cup  
2024-02-08 18:13:13,808 	Text Hypothesis :	*** ** **** *** the ***** **** series was played    at  the **** ********** age of  18    years
2024-02-08 18:13:13,808 	Text Alignment  :	D   D  D    D       D     D    S      S   S         S       D    D          S   S   S     S    
2024-02-08 18:13:13,808 ========================================================================================================================
2024-02-08 18:13:13,808 Logging Sequence: 99_158.00
2024-02-08 18:13:13,808 	Gloss Reference :	A B+C+D+E
2024-02-08 18:13:13,809 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:13:13,809 	Gloss Alignment :	         
2024-02-08 18:13:13,809 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:13:13,810 	Text Reference  :	*** the  incident occured  in  dubai and *********** ** it  was extremely shameful
2024-02-08 18:13:13,810 	Text Hypothesis :	his fans love     watching all posts and participate in the t20 world     cup     
2024-02-08 18:13:13,810 	Text Alignment  :	I   S    S        S        S   S         I           I  S   S   S         S       
2024-02-08 18:13:13,810 ========================================================================================================================
2024-02-08 18:13:15,347 Epoch 471: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.52 
2024-02-08 18:13:15,347 EPOCH 472
2024-02-08 18:13:20,303 Epoch 472: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.16 
2024-02-08 18:13:20,304 EPOCH 473
2024-02-08 18:13:24,937 Epoch 473: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.52 
2024-02-08 18:13:24,937 EPOCH 474
2024-02-08 18:13:27,255 [Epoch: 474 Step: 00016100] Batch Recognition Loss:   0.000328 => Gls Tokens per Sec:     2374 || Batch Translation Loss:   0.089649 => Txt Tokens per Sec:     6645 || Lr: 0.000100
2024-02-08 18:13:29,087 Epoch 474: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.20 
2024-02-08 18:13:29,087 EPOCH 475
2024-02-08 18:13:34,081 Epoch 475: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.42 
2024-02-08 18:13:34,082 EPOCH 476
2024-02-08 18:13:38,340 Epoch 476: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.19 
2024-02-08 18:13:38,340 EPOCH 477
2024-02-08 18:13:40,405 [Epoch: 477 Step: 00016200] Batch Recognition Loss:   0.002160 => Gls Tokens per Sec:     2356 || Batch Translation Loss:   0.046466 => Txt Tokens per Sec:     6719 || Lr: 0.000100
2024-02-08 18:13:43,189 Epoch 477: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.51 
2024-02-08 18:13:43,189 EPOCH 478
2024-02-08 18:13:47,675 Epoch 478: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.06 
2024-02-08 18:13:47,675 EPOCH 479
2024-02-08 18:13:51,806 Epoch 479: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.56 
2024-02-08 18:13:51,806 EPOCH 480
2024-02-08 18:13:53,440 [Epoch: 480 Step: 00016300] Batch Recognition Loss:   0.001508 => Gls Tokens per Sec:     2583 || Batch Translation Loss:   0.053013 => Txt Tokens per Sec:     6972 || Lr: 0.000100
2024-02-08 18:13:56,640 Epoch 480: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.20 
2024-02-08 18:13:56,641 EPOCH 481
2024-02-08 18:14:01,461 Epoch 481: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.60 
2024-02-08 18:14:01,461 EPOCH 482
2024-02-08 18:14:06,300 Epoch 482: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.32 
2024-02-08 18:14:06,300 EPOCH 483
2024-02-08 18:14:07,950 [Epoch: 483 Step: 00016400] Batch Recognition Loss:   0.003100 => Gls Tokens per Sec:     2172 || Batch Translation Loss:   0.021949 => Txt Tokens per Sec:     5749 || Lr: 0.000100
2024-02-08 18:14:10,770 Epoch 483: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.99 
2024-02-08 18:14:10,771 EPOCH 484
2024-02-08 18:14:15,074 Epoch 484: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.54 
2024-02-08 18:14:15,075 EPOCH 485
2024-02-08 18:14:20,062 Epoch 485: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.85 
2024-02-08 18:14:20,062 EPOCH 486
2024-02-08 18:14:21,416 [Epoch: 486 Step: 00016500] Batch Recognition Loss:   0.001860 => Gls Tokens per Sec:     2365 || Batch Translation Loss:   0.114046 => Txt Tokens per Sec:     6725 || Lr: 0.000100
2024-02-08 18:14:24,691 Epoch 486: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.23 
2024-02-08 18:14:24,692 EPOCH 487
2024-02-08 18:14:29,788 Epoch 487: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.40 
2024-02-08 18:14:29,789 EPOCH 488
2024-02-08 18:14:34,135 Epoch 488: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.70 
2024-02-08 18:14:34,135 EPOCH 489
2024-02-08 18:14:34,988 [Epoch: 489 Step: 00016600] Batch Recognition Loss:   0.001851 => Gls Tokens per Sec:     3005 || Batch Translation Loss:   0.313223 => Txt Tokens per Sec:     7920 || Lr: 0.000100
2024-02-08 18:14:38,858 Epoch 489: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.89 
2024-02-08 18:14:38,859 EPOCH 490
2024-02-08 18:14:43,427 Epoch 490: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.82 
2024-02-08 18:14:43,427 EPOCH 491
2024-02-08 18:14:47,876 Epoch 491: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.67 
2024-02-08 18:14:47,877 EPOCH 492
2024-02-08 18:14:48,489 [Epoch: 492 Step: 00016700] Batch Recognition Loss:   0.001775 => Gls Tokens per Sec:     3142 || Batch Translation Loss:   0.031862 => Txt Tokens per Sec:     7306 || Lr: 0.000100
2024-02-08 18:14:52,741 Epoch 492: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.03 
2024-02-08 18:14:52,741 EPOCH 493
2024-02-08 18:14:57,347 Epoch 493: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.87 
2024-02-08 18:14:57,348 EPOCH 494
2024-02-08 18:15:02,209 Epoch 494: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.50 
2024-02-08 18:15:02,209 EPOCH 495
2024-02-08 18:15:02,656 [Epoch: 495 Step: 00016800] Batch Recognition Loss:   0.000737 => Gls Tokens per Sec:     2870 || Batch Translation Loss:   0.017718 => Txt Tokens per Sec:     7767 || Lr: 0.000100
2024-02-08 18:15:06,285 Epoch 495: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.46 
2024-02-08 18:15:06,285 EPOCH 496
2024-02-08 18:15:11,325 Epoch 496: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.43 
2024-02-08 18:15:11,326 EPOCH 497
2024-02-08 18:15:15,616 Epoch 497: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.33 
2024-02-08 18:15:15,616 EPOCH 498
2024-02-08 18:15:15,863 [Epoch: 498 Step: 00016900] Batch Recognition Loss:   0.000875 => Gls Tokens per Sec:     2602 || Batch Translation Loss:   0.031514 => Txt Tokens per Sec:     7553 || Lr: 0.000100
2024-02-08 18:15:20,525 Epoch 498: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.99 
2024-02-08 18:15:20,526 EPOCH 499
2024-02-08 18:15:24,991 Epoch 499: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.10 
2024-02-08 18:15:24,991 EPOCH 500
2024-02-08 18:15:29,629 [Epoch: 500 Step: 00017000] Batch Recognition Loss:   0.000399 => Gls Tokens per Sec:     2291 || Batch Translation Loss:   0.020558 => Txt Tokens per Sec:     6338 || Lr: 0.000100
2024-02-08 18:15:29,629 Epoch 500: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.22 
2024-02-08 18:15:29,629 EPOCH 501
2024-02-08 18:15:34,290 Epoch 501: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.15 
2024-02-08 18:15:34,290 EPOCH 502
2024-02-08 18:15:38,678 Epoch 502: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.56 
2024-02-08 18:15:38,679 EPOCH 503
2024-02-08 18:15:43,384 [Epoch: 503 Step: 00017100] Batch Recognition Loss:   0.000320 => Gls Tokens per Sec:     2122 || Batch Translation Loss:   0.066166 => Txt Tokens per Sec:     5852 || Lr: 0.000100
2024-02-08 18:15:43,612 Epoch 503: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.99 
2024-02-08 18:15:43,612 EPOCH 504
2024-02-08 18:15:47,717 Epoch 504: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.67 
2024-02-08 18:15:47,717 EPOCH 505
2024-02-08 18:15:53,014 Epoch 505: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.66 
2024-02-08 18:15:53,014 EPOCH 506
2024-02-08 18:15:57,130 [Epoch: 506 Step: 00017200] Batch Recognition Loss:   0.000908 => Gls Tokens per Sec:     2270 || Batch Translation Loss:   0.093972 => Txt Tokens per Sec:     6198 || Lr: 0.000100
2024-02-08 18:15:57,737 Epoch 506: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.07 
2024-02-08 18:15:57,738 EPOCH 507
2024-02-08 18:16:02,030 Epoch 507: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.39 
2024-02-08 18:16:02,030 EPOCH 508
2024-02-08 18:16:07,114 Epoch 508: Total Training Recognition Loss 0.10  Total Training Translation Loss 4.79 
2024-02-08 18:16:07,115 EPOCH 509
2024-02-08 18:16:10,535 [Epoch: 509 Step: 00017300] Batch Recognition Loss:   0.001682 => Gls Tokens per Sec:     2544 || Batch Translation Loss:   0.066665 => Txt Tokens per Sec:     6949 || Lr: 0.000100
2024-02-08 18:16:11,299 Epoch 509: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.73 
2024-02-08 18:16:11,299 EPOCH 510
2024-02-08 18:16:16,289 Epoch 510: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.92 
2024-02-08 18:16:16,290 EPOCH 511
2024-02-08 18:16:20,712 Epoch 511: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.77 
2024-02-08 18:16:20,713 EPOCH 512
2024-02-08 18:16:24,072 [Epoch: 512 Step: 00017400] Batch Recognition Loss:   0.000366 => Gls Tokens per Sec:     2477 || Batch Translation Loss:   0.023923 => Txt Tokens per Sec:     6794 || Lr: 0.000100
2024-02-08 18:16:25,185 Epoch 512: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.83 
2024-02-08 18:16:25,186 EPOCH 513
2024-02-08 18:16:30,602 Epoch 513: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.49 
2024-02-08 18:16:30,603 EPOCH 514
2024-02-08 18:16:35,199 Epoch 514: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.89 
2024-02-08 18:16:35,199 EPOCH 515
2024-02-08 18:16:38,053 [Epoch: 515 Step: 00017500] Batch Recognition Loss:   0.000453 => Gls Tokens per Sec:     2601 || Batch Translation Loss:   0.055493 => Txt Tokens per Sec:     7138 || Lr: 0.000100
2024-02-08 18:16:39,519 Epoch 515: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.18 
2024-02-08 18:16:39,519 EPOCH 516
2024-02-08 18:16:44,611 Epoch 516: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.29 
2024-02-08 18:16:44,611 EPOCH 517
2024-02-08 18:16:48,892 Epoch 517: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.38 
2024-02-08 18:16:48,892 EPOCH 518
2024-02-08 18:16:52,072 [Epoch: 518 Step: 00017600] Batch Recognition Loss:   0.000368 => Gls Tokens per Sec:     2214 || Batch Translation Loss:   0.191296 => Txt Tokens per Sec:     5956 || Lr: 0.000100
2024-02-08 18:16:54,085 Epoch 518: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.69 
2024-02-08 18:16:54,086 EPOCH 519
2024-02-08 18:16:58,633 Epoch 519: Total Training Recognition Loss 0.08  Total Training Translation Loss 7.51 
2024-02-08 18:16:58,633 EPOCH 520
2024-02-08 18:17:03,557 Epoch 520: Total Training Recognition Loss 0.11  Total Training Translation Loss 7.38 
2024-02-08 18:17:03,558 EPOCH 521
2024-02-08 18:17:06,156 [Epoch: 521 Step: 00017700] Batch Recognition Loss:   0.002178 => Gls Tokens per Sec:     2364 || Batch Translation Loss:   0.016399 => Txt Tokens per Sec:     6445 || Lr: 0.000100
2024-02-08 18:17:08,132 Epoch 521: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.72 
2024-02-08 18:17:08,133 EPOCH 522
2024-02-08 18:17:12,872 Epoch 522: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.22 
2024-02-08 18:17:12,873 EPOCH 523
2024-02-08 18:17:17,608 Epoch 523: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.28 
2024-02-08 18:17:17,609 EPOCH 524
2024-02-08 18:17:20,049 [Epoch: 524 Step: 00017800] Batch Recognition Loss:   0.002260 => Gls Tokens per Sec:     2254 || Batch Translation Loss:   0.158253 => Txt Tokens per Sec:     6496 || Lr: 0.000100
2024-02-08 18:17:22,021 Epoch 524: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.85 
2024-02-08 18:17:22,021 EPOCH 525
2024-02-08 18:17:27,048 Epoch 525: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.17 
2024-02-08 18:17:27,049 EPOCH 526
2024-02-08 18:17:31,369 Epoch 526: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.75 
2024-02-08 18:17:31,370 EPOCH 527
2024-02-08 18:17:34,041 [Epoch: 527 Step: 00017900] Batch Recognition Loss:   0.000886 => Gls Tokens per Sec:     1820 || Batch Translation Loss:   0.031038 => Txt Tokens per Sec:     5112 || Lr: 0.000100
2024-02-08 18:17:36,685 Epoch 527: Total Training Recognition Loss 0.12  Total Training Translation Loss 1.68 
2024-02-08 18:17:36,685 EPOCH 528
2024-02-08 18:17:41,040 Epoch 528: Total Training Recognition Loss 1.53  Total Training Translation Loss 2.18 
2024-02-08 18:17:41,040 EPOCH 529
2024-02-08 18:17:46,147 Epoch 529: Total Training Recognition Loss 5.08  Total Training Translation Loss 3.06 
2024-02-08 18:17:46,148 EPOCH 530
2024-02-08 18:17:47,973 [Epoch: 530 Step: 00018000] Batch Recognition Loss:   0.032118 => Gls Tokens per Sec:     2313 || Batch Translation Loss:   0.030549 => Txt Tokens per Sec:     6404 || Lr: 0.000100
2024-02-08 18:17:57,031 Hooray! New best validation result [eval_metric]!
2024-02-08 18:17:57,032 Saving new checkpoint.
2024-02-08 18:17:57,305 Validation result at epoch 530, step    18000: duration: 9.3316s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.61907	Translation Loss: 94000.37500	PPL: 11953.40039
	Eval Metric: BLEU
	WER 6.57	(DEL: 0.00,	INS: 0.00,	SUB: 6.57)
	BLEU-4 0.92	(BLEU-1: 11.47,	BLEU-2: 3.78,	BLEU-3: 1.66,	BLEU-4: 0.92)
	CHRF 17.31	ROUGE 9.69
2024-02-08 18:17:57,306 Logging Recognition and Translation Outputs
2024-02-08 18:17:57,306 ========================================================================================================================
2024-02-08 18:17:57,307 Logging Sequence: 179_309.00
2024-02-08 18:17:57,307 	Gloss Reference :	A B+C+D+E
2024-02-08 18:17:57,307 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:17:57,307 	Gloss Alignment :	         
2024-02-08 18:17:57,307 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:17:57,308 	Text Reference  :	before the ioa could send the notice wfi    has asked phogat to  explain her  indiscipline
2024-02-08 18:17:57,308 	Text Hypothesis :	****** *** we  could **** *** not    travel to  delhi as     our home    town haryana     
2024-02-08 18:17:57,308 	Text Alignment  :	D      D   S         D    D   S      S      S   S     S      S   S       S    S           
2024-02-08 18:17:57,309 ========================================================================================================================
2024-02-08 18:17:57,309 Logging Sequence: 156_35.00
2024-02-08 18:17:57,309 	Gloss Reference :	A B+C+D+E
2024-02-08 18:17:57,309 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:17:57,309 	Gloss Alignment :	         
2024-02-08 18:17:57,309 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:17:57,311 	Text Reference  :	the first season of  mlc  began on 13th july 2023 and ended    on   30th july 2023 with six teams
2024-02-08 18:17:57,311 	Text Hypothesis :	*** ***** ****** ipl will carry on **** **** **** 15  december 2021 in   loss csk  has  10  teams
2024-02-08 18:17:57,312 	Text Alignment  :	D   D     D      S   S    S        D    D    D    S   S        S    S    S    S    S    S        
2024-02-08 18:17:57,312 ========================================================================================================================
2024-02-08 18:17:57,312 Logging Sequence: 129_45.00
2024-02-08 18:17:57,312 	Gloss Reference :	A B+C+D+E
2024-02-08 18:17:57,312 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:17:57,312 	Gloss Alignment :	         
2024-02-08 18:17:57,312 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:17:57,314 	Text Reference  :	suga then *** announced that     from 5    july onwards japan will be        in  a  state of    emergency
2024-02-08 18:17:57,314 	Text Hypothesis :	**** then the 2020      olympics were held in   2020    but   were postponed due to the   covid pandemic 
2024-02-08 18:17:57,314 	Text Alignment  :	D         I   S         S        S    S    S    S       S     S    S         S   S  S     S     S        
2024-02-08 18:17:57,314 ========================================================================================================================
2024-02-08 18:17:57,314 Logging Sequence: 56_17.00
2024-02-08 18:17:57,315 	Gloss Reference :	A B+C+D+E
2024-02-08 18:17:57,315 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:17:57,315 	Gloss Alignment :	         
2024-02-08 18:17:57,315 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:17:57,315 	Text Reference  :	it  was   held at   mumbai's wankhede stadium
2024-02-08 18:17:57,316 	Text Hypothesis :	the world cup  will last     for      india  
2024-02-08 18:17:57,316 	Text Alignment  :	S   S     S    S    S        S        S      
2024-02-08 18:17:57,316 ========================================================================================================================
2024-02-08 18:17:57,316 Logging Sequence: 152_73.00
2024-02-08 18:17:57,316 	Gloss Reference :	A B+C+D+E
2024-02-08 18:17:57,316 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:17:57,316 	Gloss Alignment :	         
2024-02-08 18:17:57,316 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:17:57,317 	Text Reference  :	***** *** ** ************ ******* * ******** eventually he     too      got out       by  shaheen afridi
2024-02-08 18:17:57,317 	Text Hypothesis :	after her no weightlifter secured a suburban of         sydney olympics in  ahmedabad are sold    out   
2024-02-08 18:17:57,318 	Text Alignment  :	I     I   I  I            I       I I        S          S      S        S   S         S   S       S     
2024-02-08 18:17:57,318 ========================================================================================================================
2024-02-08 18:18:00,366 Epoch 530: Total Training Recognition Loss 1.41  Total Training Translation Loss 2.81 
2024-02-08 18:18:00,366 EPOCH 531
2024-02-08 18:18:05,313 Epoch 531: Total Training Recognition Loss 0.64  Total Training Translation Loss 2.03 
2024-02-08 18:18:05,313 EPOCH 532
2024-02-08 18:18:10,225 Epoch 532: Total Training Recognition Loss 0.18  Total Training Translation Loss 1.94 
2024-02-08 18:18:10,225 EPOCH 533
2024-02-08 18:18:11,632 [Epoch: 533 Step: 00018100] Batch Recognition Loss:   0.003825 => Gls Tokens per Sec:     2732 || Batch Translation Loss:   0.407078 => Txt Tokens per Sec:     7402 || Lr: 0.000100
2024-02-08 18:18:14,898 Epoch 533: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.39 
2024-02-08 18:18:14,898 EPOCH 534
2024-02-08 18:18:19,723 Epoch 534: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.53 
2024-02-08 18:18:19,723 EPOCH 535
2024-02-08 18:18:24,091 Epoch 535: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.08 
2024-02-08 18:18:24,091 EPOCH 536
2024-02-08 18:18:25,705 [Epoch: 536 Step: 00018200] Batch Recognition Loss:   0.003109 => Gls Tokens per Sec:     1824 || Batch Translation Loss:   0.058081 => Txt Tokens per Sec:     5309 || Lr: 0.000100
2024-02-08 18:18:29,227 Epoch 536: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.51 
2024-02-08 18:18:29,227 EPOCH 537
2024-02-08 18:18:33,725 Epoch 537: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.62 
2024-02-08 18:18:33,725 EPOCH 538
2024-02-08 18:18:38,947 Epoch 538: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.46 
2024-02-08 18:18:38,948 EPOCH 539
2024-02-08 18:18:40,128 [Epoch: 539 Step: 00018300] Batch Recognition Loss:   0.000379 => Gls Tokens per Sec:     2173 || Batch Translation Loss:   0.050742 => Txt Tokens per Sec:     6437 || Lr: 0.000100
2024-02-08 18:18:43,332 Epoch 539: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.63 
2024-02-08 18:18:43,333 EPOCH 540
2024-02-08 18:18:48,199 Epoch 540: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.14 
2024-02-08 18:18:48,199 EPOCH 541
2024-02-08 18:18:53,141 Epoch 541: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.61 
2024-02-08 18:18:53,141 EPOCH 542
2024-02-08 18:18:53,755 [Epoch: 542 Step: 00018400] Batch Recognition Loss:   0.000753 => Gls Tokens per Sec:     3135 || Batch Translation Loss:   0.013447 => Txt Tokens per Sec:     8129 || Lr: 0.000100
2024-02-08 18:18:57,807 Epoch 542: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.51 
2024-02-08 18:18:57,808 EPOCH 543
2024-02-08 18:19:02,650 Epoch 543: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.60 
2024-02-08 18:19:02,650 EPOCH 544
2024-02-08 18:19:07,304 Epoch 544: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.13 
2024-02-08 18:19:07,305 EPOCH 545
2024-02-08 18:19:07,779 [Epoch: 545 Step: 00018500] Batch Recognition Loss:   0.000425 => Gls Tokens per Sec:     2713 || Batch Translation Loss:   0.039308 => Txt Tokens per Sec:     6664 || Lr: 0.000100
2024-02-08 18:19:12,470 Epoch 545: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.15 
2024-02-08 18:19:12,470 EPOCH 546
2024-02-08 18:19:16,859 Epoch 546: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.05 
2024-02-08 18:19:16,860 EPOCH 547
2024-02-08 18:19:22,000 Epoch 547: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.17 
2024-02-08 18:19:22,000 EPOCH 548
2024-02-08 18:19:22,217 [Epoch: 548 Step: 00018600] Batch Recognition Loss:   0.000248 => Gls Tokens per Sec:     2951 || Batch Translation Loss:   0.019213 => Txt Tokens per Sec:     7054 || Lr: 0.000100
2024-02-08 18:19:26,540 Epoch 548: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.28 
2024-02-08 18:19:26,540 EPOCH 549
2024-02-08 18:19:31,489 Epoch 549: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.24 
2024-02-08 18:19:31,490 EPOCH 550
2024-02-08 18:19:36,079 [Epoch: 550 Step: 00018700] Batch Recognition Loss:   0.002671 => Gls Tokens per Sec:     2315 || Batch Translation Loss:   0.023541 => Txt Tokens per Sec:     6404 || Lr: 0.000100
2024-02-08 18:19:36,079 Epoch 550: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.91 
2024-02-08 18:19:36,079 EPOCH 551
2024-02-08 18:19:40,844 Epoch 551: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.87 
2024-02-08 18:19:40,845 EPOCH 552
2024-02-08 18:19:45,756 Epoch 552: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.38 
2024-02-08 18:19:45,757 EPOCH 553
2024-02-08 18:19:50,145 [Epoch: 553 Step: 00018800] Batch Recognition Loss:   0.005481 => Gls Tokens per Sec:     2275 || Batch Translation Loss:   0.036802 => Txt Tokens per Sec:     6261 || Lr: 0.000100
2024-02-08 18:19:50,508 Epoch 553: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.26 
2024-02-08 18:19:50,510 EPOCH 554
2024-02-08 18:19:55,563 Epoch 554: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.79 
2024-02-08 18:19:55,563 EPOCH 555
2024-02-08 18:19:59,929 Epoch 555: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.86 
2024-02-08 18:19:59,930 EPOCH 556
2024-02-08 18:20:04,604 [Epoch: 556 Step: 00018900] Batch Recognition Loss:   0.000463 => Gls Tokens per Sec:     1998 || Batch Translation Loss:   0.058478 => Txt Tokens per Sec:     5529 || Lr: 0.000100
2024-02-08 18:20:05,209 Epoch 556: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.95 
2024-02-08 18:20:05,210 EPOCH 557
2024-02-08 18:20:09,545 Epoch 557: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.85 
2024-02-08 18:20:09,545 EPOCH 558
2024-02-08 18:20:14,688 Epoch 558: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.94 
2024-02-08 18:20:14,688 EPOCH 559
2024-02-08 18:20:18,356 [Epoch: 559 Step: 00019000] Batch Recognition Loss:   0.001908 => Gls Tokens per Sec:     2444 || Batch Translation Loss:   0.091363 => Txt Tokens per Sec:     6753 || Lr: 0.000100
2024-02-08 18:20:19,099 Epoch 559: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.95 
2024-02-08 18:20:19,100 EPOCH 560
2024-02-08 18:20:24,055 Epoch 560: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.25 
2024-02-08 18:20:24,055 EPOCH 561
2024-02-08 18:20:28,579 Epoch 561: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.98 
2024-02-08 18:20:28,579 EPOCH 562
2024-02-08 18:20:32,398 [Epoch: 562 Step: 00019100] Batch Recognition Loss:   0.000763 => Gls Tokens per Sec:     2111 || Batch Translation Loss:   0.180959 => Txt Tokens per Sec:     5982 || Lr: 0.000100
2024-02-08 18:20:33,403 Epoch 562: Total Training Recognition Loss 0.10  Total Training Translation Loss 6.00 
2024-02-08 18:20:33,404 EPOCH 563
2024-02-08 18:20:38,082 Epoch 563: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.52 
2024-02-08 18:20:38,082 EPOCH 564
2024-02-08 18:20:42,569 Epoch 564: Total Training Recognition Loss 0.06  Total Training Translation Loss 7.13 
2024-02-08 18:20:42,570 EPOCH 565
2024-02-08 18:20:46,159 [Epoch: 565 Step: 00019200] Batch Recognition Loss:   0.002893 => Gls Tokens per Sec:     2141 || Batch Translation Loss:   0.054160 => Txt Tokens per Sec:     6005 || Lr: 0.000100
2024-02-08 18:20:47,424 Epoch 565: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.84 
2024-02-08 18:20:47,425 EPOCH 566
2024-02-08 18:20:51,523 Epoch 566: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.31 
2024-02-08 18:20:51,523 EPOCH 567
2024-02-08 18:20:56,568 Epoch 567: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.39 
2024-02-08 18:20:56,569 EPOCH 568
2024-02-08 18:20:59,579 [Epoch: 568 Step: 00019300] Batch Recognition Loss:   0.000733 => Gls Tokens per Sec:     2253 || Batch Translation Loss:   0.080131 => Txt Tokens per Sec:     6398 || Lr: 0.000100
2024-02-08 18:21:00,874 Epoch 568: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.19 
2024-02-08 18:21:00,874 EPOCH 569
2024-02-08 18:21:05,769 Epoch 569: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.74 
2024-02-08 18:21:05,770 EPOCH 570
2024-02-08 18:21:10,268 Epoch 570: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.25 
2024-02-08 18:21:10,269 EPOCH 571
2024-02-08 18:21:12,777 [Epoch: 571 Step: 00019400] Batch Recognition Loss:   0.001065 => Gls Tokens per Sec:     2448 || Batch Translation Loss:   0.229031 => Txt Tokens per Sec:     6829 || Lr: 0.000100
2024-02-08 18:21:14,716 Epoch 571: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.52 
2024-02-08 18:21:14,717 EPOCH 572
2024-02-08 18:21:19,614 Epoch 572: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.39 
2024-02-08 18:21:19,614 EPOCH 573
2024-02-08 18:21:23,756 Epoch 573: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.21 
2024-02-08 18:21:23,757 EPOCH 574
2024-02-08 18:21:26,584 [Epoch: 574 Step: 00019500] Batch Recognition Loss:   0.000330 => Gls Tokens per Sec:     1947 || Batch Translation Loss:   0.030647 => Txt Tokens per Sec:     5439 || Lr: 0.000100
2024-02-08 18:21:28,820 Epoch 574: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.21 
2024-02-08 18:21:28,820 EPOCH 575
2024-02-08 18:21:33,102 Epoch 575: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.18 
2024-02-08 18:21:33,102 EPOCH 576
2024-02-08 18:21:37,983 Epoch 576: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.25 
2024-02-08 18:21:37,984 EPOCH 577
2024-02-08 18:21:40,373 [Epoch: 577 Step: 00019600] Batch Recognition Loss:   0.000467 => Gls Tokens per Sec:     2035 || Batch Translation Loss:   0.037125 => Txt Tokens per Sec:     5852 || Lr: 0.000100
2024-02-08 18:21:42,410 Epoch 577: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.98 
2024-02-08 18:21:42,410 EPOCH 578
2024-02-08 18:21:47,091 Epoch 578: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.68 
2024-02-08 18:21:47,092 EPOCH 579
2024-02-08 18:21:51,793 Epoch 579: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.49 
2024-02-08 18:21:51,793 EPOCH 580
2024-02-08 18:21:53,469 [Epoch: 580 Step: 00019700] Batch Recognition Loss:   0.000847 => Gls Tokens per Sec:     2521 || Batch Translation Loss:   0.046883 => Txt Tokens per Sec:     7078 || Lr: 0.000100
2024-02-08 18:21:56,109 Epoch 580: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.67 
2024-02-08 18:21:56,110 EPOCH 581
2024-02-08 18:22:01,064 Epoch 581: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.17 
2024-02-08 18:22:01,065 EPOCH 582
2024-02-08 18:22:05,197 Epoch 582: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.18 
2024-02-08 18:22:05,197 EPOCH 583
2024-02-08 18:22:07,071 [Epoch: 583 Step: 00019800] Batch Recognition Loss:   0.001640 => Gls Tokens per Sec:     2050 || Batch Translation Loss:   0.049215 => Txt Tokens per Sec:     5697 || Lr: 0.000100
2024-02-08 18:22:10,181 Epoch 583: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.02 
2024-02-08 18:22:10,181 EPOCH 584
2024-02-08 18:22:14,490 Epoch 584: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.74 
2024-02-08 18:22:14,490 EPOCH 585
2024-02-08 18:22:19,461 Epoch 585: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.29 
2024-02-08 18:22:19,462 EPOCH 586
2024-02-08 18:22:20,978 [Epoch: 586 Step: 00019900] Batch Recognition Loss:   0.000329 => Gls Tokens per Sec:     1941 || Batch Translation Loss:   0.098743 => Txt Tokens per Sec:     5764 || Lr: 0.000100
2024-02-08 18:22:23,857 Epoch 586: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.59 
2024-02-08 18:22:23,857 EPOCH 587
2024-02-08 18:22:28,629 Epoch 587: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.70 
2024-02-08 18:22:28,630 EPOCH 588
2024-02-08 18:22:33,183 Epoch 588: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.30 
2024-02-08 18:22:33,184 EPOCH 589
2024-02-08 18:22:33,933 [Epoch: 589 Step: 00020000] Batch Recognition Loss:   0.007380 => Gls Tokens per Sec:     3418 || Batch Translation Loss:   0.038654 => Txt Tokens per Sec:     8538 || Lr: 0.000100
2024-02-08 18:22:42,587 Validation result at epoch 589, step    20000: duration: 8.6543s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.42912	Translation Loss: 92852.85938	PPL: 10658.96484
	Eval Metric: BLEU
	WER 4.03	(DEL: 0.00,	INS: 0.00,	SUB: 4.03)
	BLEU-4 0.46	(BLEU-1: 9.81,	BLEU-2: 2.73,	BLEU-3: 0.91,	BLEU-4: 0.46)
	CHRF 16.62	ROUGE 8.44
2024-02-08 18:22:42,588 Logging Recognition and Translation Outputs
2024-02-08 18:22:42,589 ========================================================================================================================
2024-02-08 18:22:42,589 Logging Sequence: 120_7.00
2024-02-08 18:22:42,589 	Gloss Reference :	A B+C+D+E
2024-02-08 18:22:42,589 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:22:42,589 	Gloss Alignment :	         
2024-02-08 18:22:42,590 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:22:42,590 	Text Reference  :	he had tested positive for covid-19 on    may 19 
2024-02-08 18:22:42,590 	Text Hypothesis :	** on  5th    may      the police   filed an  fir
2024-02-08 18:22:42,590 	Text Alignment  :	D  S   S      S        S   S        S     S   S  
2024-02-08 18:22:42,590 ========================================================================================================================
2024-02-08 18:22:42,591 Logging Sequence: 148_186.00
2024-02-08 18:22:42,591 	Gloss Reference :	A B+C+D+E
2024-02-08 18:22:42,591 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:22:42,591 	Gloss Alignment :	         
2024-02-08 18:22:42,591 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:22:42,593 	Text Reference  :	siraj also took four    wickets in     1  over  thus becoming  the   record-holder for  most wickets in   an     over in  odis 
2024-02-08 18:22:42,594 	Text Hypothesis :	***** **** **** kolkata knight  riders is owned by   bollywood actor shah          rukh khan actress juhi chawla and  sri lanka
2024-02-08 18:22:42,594 	Text Alignment  :	D     D    D    S       S       S      S  S     S    S         S     S             S    S    S       S    S      S    S   S    
2024-02-08 18:22:42,594 ========================================================================================================================
2024-02-08 18:22:42,594 Logging Sequence: 67_73.00
2024-02-08 18:22:42,594 	Gloss Reference :	A B+C+D+E
2024-02-08 18:22:42,594 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:22:42,594 	Gloss Alignment :	         
2024-02-08 18:22:42,594 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:22:42,595 	Text Reference  :	** in   his tweet he also said 
2024-02-08 18:22:42,595 	Text Hypothesis :	pm modi and india is very sorry
2024-02-08 18:22:42,595 	Text Alignment  :	I  S    S   S     S  S    S    
2024-02-08 18:22:42,595 ========================================================================================================================
2024-02-08 18:22:42,595 Logging Sequence: 164_526.00
2024-02-08 18:22:42,596 	Gloss Reference :	A B+C+D+E
2024-02-08 18:22:42,596 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:22:42,596 	Gloss Alignment :	         
2024-02-08 18:22:42,596 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:22:42,597 	Text Reference  :	*** *** ***** ********* **** you   are aware that  viacom18 bought  the   broadcast rights of  ipl  
2024-02-08 18:22:42,597 	Text Hypothesis :	the two prime ministers were taken and each  other indian   skipper rohit sharma    lost   the match
2024-02-08 18:22:42,597 	Text Alignment  :	I   I   I     I         I    S     S   S     S     S        S       S     S         S      S   S    
2024-02-08 18:22:42,597 ========================================================================================================================
2024-02-08 18:22:42,597 Logging Sequence: 108_28.00
2024-02-08 18:22:42,598 	Gloss Reference :	A B+C+D+E
2024-02-08 18:22:42,598 	Gloss Hypothesis:	A B+C+D  
2024-02-08 18:22:42,598 	Gloss Alignment :	  S      
2024-02-08 18:22:42,598 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:22:42,600 	Text Reference  :	the 10 teams bought 204    players including 67      foreign players after spending a   total    of  rs   55170 crore
2024-02-08 18:22:42,600 	Text Hypothesis :	*** ** ***** ****** mumbai indians were      batting at      the     ipl   seasons  and pakistan was left pad   first
2024-02-08 18:22:42,600 	Text Alignment  :	D   D  D     D      S      S       S         S       S       S       S     S        S   S        S   S    S     S    
2024-02-08 18:22:42,600 ========================================================================================================================
2024-02-08 18:22:46,694 Epoch 589: Total Training Recognition Loss 0.06  Total Training Translation Loss 5.08 
2024-02-08 18:22:46,694 EPOCH 590
2024-02-08 18:22:51,672 Epoch 590: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.86 
2024-02-08 18:22:51,673 EPOCH 591
2024-02-08 18:22:55,905 Epoch 591: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.14 
2024-02-08 18:22:55,906 EPOCH 592
2024-02-08 18:22:56,788 [Epoch: 592 Step: 00020100] Batch Recognition Loss:   0.000487 => Gls Tokens per Sec:     2182 || Batch Translation Loss:   0.221555 => Txt Tokens per Sec:     6661 || Lr: 0.000100
2024-02-08 18:23:00,841 Epoch 592: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.79 
2024-02-08 18:23:00,842 EPOCH 593
2024-02-08 18:23:05,271 Epoch 593: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.98 
2024-02-08 18:23:05,271 EPOCH 594
2024-02-08 18:23:09,977 Epoch 594: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.83 
2024-02-08 18:23:09,978 EPOCH 595
2024-02-08 18:23:10,411 [Epoch: 595 Step: 00020200] Batch Recognition Loss:   0.000386 => Gls Tokens per Sec:     2970 || Batch Translation Loss:   0.033927 => Txt Tokens per Sec:     7543 || Lr: 0.000100
2024-02-08 18:23:14,583 Epoch 595: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.48 
2024-02-08 18:23:14,583 EPOCH 596
2024-02-08 18:23:19,024 Epoch 596: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.07 
2024-02-08 18:23:19,025 EPOCH 597
2024-02-08 18:23:23,943 Epoch 597: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.17 
2024-02-08 18:23:23,943 EPOCH 598
2024-02-08 18:23:24,191 [Epoch: 598 Step: 00020300] Batch Recognition Loss:   0.000364 => Gls Tokens per Sec:     2602 || Batch Translation Loss:   0.028251 => Txt Tokens per Sec:     7947 || Lr: 0.000100
2024-02-08 18:23:28,103 Epoch 598: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.94 
2024-02-08 18:23:28,103 EPOCH 599
2024-02-08 18:23:33,432 Epoch 599: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.89 
2024-02-08 18:23:33,433 EPOCH 600
2024-02-08 18:23:38,068 [Epoch: 600 Step: 00020400] Batch Recognition Loss:   0.000377 => Gls Tokens per Sec:     2292 || Batch Translation Loss:   0.003760 => Txt Tokens per Sec:     6340 || Lr: 0.000100
2024-02-08 18:23:38,068 Epoch 600: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.19 
2024-02-08 18:23:38,068 EPOCH 601
2024-02-08 18:23:42,441 Epoch 601: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.32 
2024-02-08 18:23:42,441 EPOCH 602
2024-02-08 18:23:47,366 Epoch 602: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.96 
2024-02-08 18:23:47,367 EPOCH 603
2024-02-08 18:23:51,331 [Epoch: 603 Step: 00020500] Batch Recognition Loss:   0.000372 => Gls Tokens per Sec:     2518 || Batch Translation Loss:   0.027771 => Txt Tokens per Sec:     6952 || Lr: 0.000100
2024-02-08 18:23:51,530 Epoch 603: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.95 
2024-02-08 18:23:51,530 EPOCH 604
2024-02-08 18:23:56,483 Epoch 604: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.68 
2024-02-08 18:23:56,483 EPOCH 605
2024-02-08 18:24:00,811 Epoch 605: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.59 
2024-02-08 18:24:00,811 EPOCH 606
2024-02-08 18:24:04,731 [Epoch: 606 Step: 00020600] Batch Recognition Loss:   0.000239 => Gls Tokens per Sec:     2383 || Batch Translation Loss:   0.027261 => Txt Tokens per Sec:     6553 || Lr: 0.000100
2024-02-08 18:24:05,390 Epoch 606: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.25 
2024-02-08 18:24:05,391 EPOCH 607
2024-02-08 18:24:10,197 Epoch 607: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.50 
2024-02-08 18:24:10,197 EPOCH 608
2024-02-08 18:24:14,317 Epoch 608: Total Training Recognition Loss 0.09  Total Training Translation Loss 4.09 
2024-02-08 18:24:14,318 EPOCH 609
2024-02-08 18:24:18,479 [Epoch: 609 Step: 00020700] Batch Recognition Loss:   0.000695 => Gls Tokens per Sec:     2091 || Batch Translation Loss:   0.119186 => Txt Tokens per Sec:     6062 || Lr: 0.000100
2024-02-08 18:24:19,045 Epoch 609: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.10 
2024-02-08 18:24:19,045 EPOCH 610
2024-02-08 18:24:23,626 Epoch 610: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.82 
2024-02-08 18:24:23,626 EPOCH 611
2024-02-08 18:24:28,193 Epoch 611: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.89 
2024-02-08 18:24:28,193 EPOCH 612
2024-02-08 18:24:31,899 [Epoch: 612 Step: 00020800] Batch Recognition Loss:   0.000837 => Gls Tokens per Sec:     2246 || Batch Translation Loss:   0.045228 => Txt Tokens per Sec:     6196 || Lr: 0.000100
2024-02-08 18:24:33,027 Epoch 612: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.59 
2024-02-08 18:24:33,027 EPOCH 613
2024-02-08 18:24:37,165 Epoch 613: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.45 
2024-02-08 18:24:37,165 EPOCH 614
2024-02-08 18:24:42,206 Epoch 614: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.69 
2024-02-08 18:24:42,206 EPOCH 615
2024-02-08 18:24:45,448 [Epoch: 615 Step: 00020900] Batch Recognition Loss:   0.000314 => Gls Tokens per Sec:     2289 || Batch Translation Loss:   0.042067 => Txt Tokens per Sec:     6437 || Lr: 0.000100
2024-02-08 18:24:46,501 Epoch 615: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.84 
2024-02-08 18:24:46,502 EPOCH 616
2024-02-08 18:24:51,404 Epoch 616: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.14 
2024-02-08 18:24:51,405 EPOCH 617
2024-02-08 18:24:55,876 Epoch 617: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.81 
2024-02-08 18:24:55,876 EPOCH 618
2024-02-08 18:24:58,771 [Epoch: 618 Step: 00021000] Batch Recognition Loss:   0.000327 => Gls Tokens per Sec:     2344 || Batch Translation Loss:   0.031977 => Txt Tokens per Sec:     6400 || Lr: 0.000100
2024-02-08 18:25:00,643 Epoch 618: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.79 
2024-02-08 18:25:00,644 EPOCH 619
2024-02-08 18:25:05,721 Epoch 619: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.19 
2024-02-08 18:25:05,722 EPOCH 620
2024-02-08 18:25:10,048 Epoch 620: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.25 
2024-02-08 18:25:10,048 EPOCH 621
2024-02-08 18:25:12,833 [Epoch: 621 Step: 00021100] Batch Recognition Loss:   0.000479 => Gls Tokens per Sec:     2205 || Batch Translation Loss:   0.024038 => Txt Tokens per Sec:     6045 || Lr: 0.000100
2024-02-08 18:25:14,808 Epoch 621: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.15 
2024-02-08 18:25:14,808 EPOCH 622
2024-02-08 18:25:19,371 Epoch 622: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.93 
2024-02-08 18:25:19,372 EPOCH 623
2024-02-08 18:25:24,008 Epoch 623: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.49 
2024-02-08 18:25:24,008 EPOCH 624
2024-02-08 18:25:26,963 [Epoch: 624 Step: 00021200] Batch Recognition Loss:   0.000320 => Gls Tokens per Sec:     1862 || Batch Translation Loss:   0.069247 => Txt Tokens per Sec:     5623 || Lr: 0.000100
2024-02-08 18:25:28,735 Epoch 624: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.56 
2024-02-08 18:25:28,735 EPOCH 625
2024-02-08 18:25:33,043 Epoch 625: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.32 
2024-02-08 18:25:33,044 EPOCH 626
2024-02-08 18:25:38,009 Epoch 626: Total Training Recognition Loss 0.05  Total Training Translation Loss 6.23 
2024-02-08 18:25:38,009 EPOCH 627
2024-02-08 18:25:40,212 [Epoch: 627 Step: 00021300] Batch Recognition Loss:   0.001382 => Gls Tokens per Sec:     2207 || Batch Translation Loss:   0.273903 => Txt Tokens per Sec:     6307 || Lr: 0.000100
2024-02-08 18:25:42,223 Epoch 627: Total Training Recognition Loss 0.06  Total Training Translation Loss 5.63 
2024-02-08 18:25:42,223 EPOCH 628
2024-02-08 18:25:47,268 Epoch 628: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.57 
2024-02-08 18:25:47,268 EPOCH 629
2024-02-08 18:25:51,723 Epoch 629: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.96 
2024-02-08 18:25:51,723 EPOCH 630
2024-02-08 18:25:53,922 [Epoch: 630 Step: 00021400] Batch Recognition Loss:   0.000374 => Gls Tokens per Sec:     2038 || Batch Translation Loss:   0.127753 => Txt Tokens per Sec:     5963 || Lr: 0.000100
2024-02-08 18:25:56,600 Epoch 630: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.45 
2024-02-08 18:25:56,600 EPOCH 631
2024-02-08 18:26:01,233 Epoch 631: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.25 
2024-02-08 18:26:01,233 EPOCH 632
2024-02-08 18:26:05,893 Epoch 632: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.49 
2024-02-08 18:26:05,894 EPOCH 633
2024-02-08 18:26:07,541 [Epoch: 633 Step: 00021500] Batch Recognition Loss:   0.000327 => Gls Tokens per Sec:     2334 || Batch Translation Loss:   0.026748 => Txt Tokens per Sec:     6548 || Lr: 0.000100
2024-02-08 18:26:10,600 Epoch 633: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.82 
2024-02-08 18:26:10,600 EPOCH 634
2024-02-08 18:26:15,016 Epoch 634: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.41 
2024-02-08 18:26:15,016 EPOCH 635
2024-02-08 18:26:20,012 Epoch 635: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.10 
2024-02-08 18:26:20,012 EPOCH 636
2024-02-08 18:26:21,036 [Epoch: 636 Step: 00021600] Batch Recognition Loss:   0.000973 => Gls Tokens per Sec:     3131 || Batch Translation Loss:   0.269765 => Txt Tokens per Sec:     8032 || Lr: 0.000100
2024-02-08 18:26:24,165 Epoch 636: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.52 
2024-02-08 18:26:24,165 EPOCH 637
2024-02-08 18:26:29,133 Epoch 637: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.70 
2024-02-08 18:26:29,134 EPOCH 638
2024-02-08 18:26:33,411 Epoch 638: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.46 
2024-02-08 18:26:33,412 EPOCH 639
2024-02-08 18:26:34,118 [Epoch: 639 Step: 00021700] Batch Recognition Loss:   0.001048 => Gls Tokens per Sec:     3626 || Batch Translation Loss:   0.033179 => Txt Tokens per Sec:     9127 || Lr: 0.000100
2024-02-08 18:26:38,236 Epoch 639: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.78 
2024-02-08 18:26:38,237 EPOCH 640
2024-02-08 18:26:42,793 Epoch 640: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.06 
2024-02-08 18:26:42,793 EPOCH 641
2024-02-08 18:26:47,333 Epoch 641: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.97 
2024-02-08 18:26:47,334 EPOCH 642
2024-02-08 18:26:48,309 [Epoch: 642 Step: 00021800] Batch Recognition Loss:   0.001050 => Gls Tokens per Sec:     1706 || Batch Translation Loss:   0.068977 => Txt Tokens per Sec:     5229 || Lr: 0.000100
2024-02-08 18:26:52,098 Epoch 642: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.59 
2024-02-08 18:26:52,098 EPOCH 643
2024-02-08 18:26:56,368 Epoch 643: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.06 
2024-02-08 18:26:56,369 EPOCH 644
2024-02-08 18:27:01,654 Epoch 644: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.73 
2024-02-08 18:27:01,654 EPOCH 645
2024-02-08 18:27:02,043 [Epoch: 645 Step: 00021900] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:     2629 || Batch Translation Loss:   0.014075 => Txt Tokens per Sec:     6093 || Lr: 0.000100
2024-02-08 18:27:06,308 Epoch 645: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.66 
2024-02-08 18:27:06,308 EPOCH 646
2024-02-08 18:27:10,574 Epoch 646: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.62 
2024-02-08 18:27:10,574 EPOCH 647
2024-02-08 18:27:15,558 Epoch 647: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.01 
2024-02-08 18:27:15,558 EPOCH 648
2024-02-08 18:27:15,788 [Epoch: 648 Step: 00022000] Batch Recognition Loss:   0.000777 => Gls Tokens per Sec:     2795 || Batch Translation Loss:   0.058863 => Txt Tokens per Sec:     8222 || Lr: 0.000100
2024-02-08 18:27:24,704 Validation result at epoch 648, step    22000: duration: 8.9145s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.57841	Translation Loss: 93927.10156	PPL: 11866.23926
	Eval Metric: BLEU
	WER 3.60	(DEL: 0.00,	INS: 0.00,	SUB: 3.60)
	BLEU-4 0.53	(BLEU-1: 10.96,	BLEU-2: 3.49,	BLEU-3: 1.23,	BLEU-4: 0.53)
	CHRF 17.16	ROUGE 9.35
2024-02-08 18:27:24,705 Logging Recognition and Translation Outputs
2024-02-08 18:27:24,705 ========================================================================================================================
2024-02-08 18:27:24,706 Logging Sequence: 179_2.00
2024-02-08 18:27:24,706 	Gloss Reference :	A B+C+D+E
2024-02-08 18:27:24,706 	Gloss Hypothesis:	A B+C+D  
2024-02-08 18:27:24,706 	Gloss Alignment :	  S      
2024-02-08 18:27:24,706 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:27:24,707 	Text Reference  :	* ** vinesh phogat is a         well    known wrestler
2024-02-08 18:27:24,707 	Text Hypothesis :	i am very   happy  to encourage harmony among nations 
2024-02-08 18:27:24,707 	Text Alignment  :	I I  S      S      S  S         S       S     S       
2024-02-08 18:27:24,707 ========================================================================================================================
2024-02-08 18:27:24,707 Logging Sequence: 55_124.00
2024-02-08 18:27:24,708 	Gloss Reference :	A B+C+D+E
2024-02-08 18:27:24,708 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:27:24,708 	Gloss Alignment :	         
2024-02-08 18:27:24,708 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:27:24,709 	Text Reference  :	*** ***** *** next  to him   with the    patel jersey was   ajaz patel  
2024-02-08 18:27:24,709 	Text Hypothesis :	the event was about to start any  minute when  i      asked the  parents
2024-02-08 18:27:24,709 	Text Alignment  :	I   I     I   S        S     S    S      S     S      S     S    S      
2024-02-08 18:27:24,709 ========================================================================================================================
2024-02-08 18:27:24,709 Logging Sequence: 148_105.00
2024-02-08 18:27:24,709 	Gloss Reference :	A B+C+D+E
2024-02-08 18:27:24,710 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:27:24,710 	Gloss Alignment :	         
2024-02-08 18:27:24,710 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:27:24,712 	Text Reference  :	later with amazing bowling by  hardik pandya and kuldeep yadav   sri lanka were all out in    just 50 runs ** ***
2024-02-08 18:27:24,712 	Text Hypothesis :	***** **** ******* ******* the second match  is  in      colombo sri lanka **** for a   total of   50 runs to bat
2024-02-08 18:27:24,712 	Text Alignment  :	D     D    D       D       S   S      S      S   S       S                 D    S   S   S     S            I  I  
2024-02-08 18:27:24,712 ========================================================================================================================
2024-02-08 18:27:24,712 Logging Sequence: 125_165.00
2024-02-08 18:27:24,712 	Gloss Reference :	A B+C+D+E
2024-02-08 18:27:24,712 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:27:24,712 	Gloss Alignment :	         
2024-02-08 18:27:24,713 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:27:24,714 	Text Reference  :	please do not target nadeem we speak to   each other and share a      good    bond  
2024-02-08 18:27:24,714 	Text Hypothesis :	****** ** *** ****** ****** ** so    here are  a     lot of    indian premier league
2024-02-08 18:27:24,714 	Text Alignment  :	D      D  D   D      D      D  S     S    S    S     S   S     S      S       S     
2024-02-08 18:27:24,714 ========================================================================================================================
2024-02-08 18:27:24,714 Logging Sequence: 77_52.00
2024-02-08 18:27:24,714 	Gloss Reference :	A B+C+D+E
2024-02-08 18:27:24,714 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:27:24,715 	Gloss Alignment :	         
2024-02-08 18:27:24,715 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:27:24,716 	Text Reference  :	kane williamson held down the fort for hyderabad by    scoring 66 runs and ended    the       match in            a        tie     
2024-02-08 18:27:24,716 	Text Hypothesis :	**** ********** **** **** *** **** *** ********* there is      a  many of  athletes passports of    uttarakhand's haridwar district
2024-02-08 18:27:24,716 	Text Alignment  :	D    D          D    D    D   D    D   D         S     S       S  S    S   S        S         S     S             S        S       
2024-02-08 18:27:24,716 ========================================================================================================================
2024-02-08 18:27:29,327 Epoch 648: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.52 
2024-02-08 18:27:29,327 EPOCH 649
2024-02-08 18:27:33,822 Epoch 649: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.68 
2024-02-08 18:27:33,822 EPOCH 650
2024-02-08 18:27:38,977 [Epoch: 650 Step: 00022100] Batch Recognition Loss:   0.000484 => Gls Tokens per Sec:     2061 || Batch Translation Loss:   0.044940 => Txt Tokens per Sec:     5702 || Lr: 0.000100
2024-02-08 18:27:38,977 Epoch 650: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.33 
2024-02-08 18:27:38,977 EPOCH 651
2024-02-08 18:27:43,437 Epoch 651: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.82 
2024-02-08 18:27:43,437 EPOCH 652
2024-02-08 18:27:48,081 Epoch 652: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.39 
2024-02-08 18:27:48,081 EPOCH 653
2024-02-08 18:27:52,521 [Epoch: 653 Step: 00022200] Batch Recognition Loss:   0.000560 => Gls Tokens per Sec:     2248 || Batch Translation Loss:   0.049528 => Txt Tokens per Sec:     6179 || Lr: 0.000100
2024-02-08 18:27:52,798 Epoch 653: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.70 
2024-02-08 18:27:52,798 EPOCH 654
2024-02-08 18:27:57,177 Epoch 654: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.01 
2024-02-08 18:27:57,177 EPOCH 655
2024-02-08 18:28:02,393 Epoch 655: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.09 
2024-02-08 18:28:02,394 EPOCH 656
2024-02-08 18:28:06,519 [Epoch: 656 Step: 00022300] Batch Recognition Loss:   0.000284 => Gls Tokens per Sec:     2264 || Batch Translation Loss:   0.050372 => Txt Tokens per Sec:     6256 || Lr: 0.000100
2024-02-08 18:28:06,986 Epoch 656: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.29 
2024-02-08 18:28:06,986 EPOCH 657
2024-02-08 18:28:11,750 Epoch 657: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.04 
2024-02-08 18:28:11,751 EPOCH 658
2024-02-08 18:28:16,341 Epoch 658: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.47 
2024-02-08 18:28:16,341 EPOCH 659
2024-02-08 18:28:20,190 [Epoch: 659 Step: 00022400] Batch Recognition Loss:   0.000412 => Gls Tokens per Sec:     2261 || Batch Translation Loss:   0.048682 => Txt Tokens per Sec:     6290 || Lr: 0.000100
2024-02-08 18:28:20,926 Epoch 659: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.98 
2024-02-08 18:28:20,926 EPOCH 660
2024-02-08 18:28:26,095 Epoch 660: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.36 
2024-02-08 18:28:26,096 EPOCH 661
2024-02-08 18:28:30,430 Epoch 661: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.02 
2024-02-08 18:28:30,430 EPOCH 662
2024-02-08 18:28:34,383 [Epoch: 662 Step: 00022500] Batch Recognition Loss:   0.000363 => Gls Tokens per Sec:     2105 || Batch Translation Loss:   0.051844 => Txt Tokens per Sec:     5942 || Lr: 0.000100
2024-02-08 18:28:35,276 Epoch 662: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.55 
2024-02-08 18:28:35,276 EPOCH 663
2024-02-08 18:28:39,710 Epoch 663: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.39 
2024-02-08 18:28:39,711 EPOCH 664
2024-02-08 18:28:44,414 Epoch 664: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.52 
2024-02-08 18:28:44,415 EPOCH 665
2024-02-08 18:28:48,234 [Epoch: 665 Step: 00022600] Batch Recognition Loss:   0.005426 => Gls Tokens per Sec:     1943 || Batch Translation Loss:   0.348423 => Txt Tokens per Sec:     5487 || Lr: 0.000100
2024-02-08 18:28:49,501 Epoch 665: Total Training Recognition Loss 0.20  Total Training Translation Loss 7.07 
2024-02-08 18:28:49,502 EPOCH 666
2024-02-08 18:28:53,801 Epoch 666: Total Training Recognition Loss 7.01  Total Training Translation Loss 7.75 
2024-02-08 18:28:53,801 EPOCH 667
2024-02-08 18:28:58,718 Epoch 667: Total Training Recognition Loss 5.13  Total Training Translation Loss 8.81 
2024-02-08 18:28:58,718 EPOCH 668
2024-02-08 18:29:01,362 [Epoch: 668 Step: 00022700] Batch Recognition Loss:   0.009791 => Gls Tokens per Sec:     2565 || Batch Translation Loss:   0.104619 => Txt Tokens per Sec:     6931 || Lr: 0.000100
2024-02-08 18:29:03,083 Epoch 668: Total Training Recognition Loss 1.92  Total Training Translation Loss 7.96 
2024-02-08 18:29:03,083 EPOCH 669
2024-02-08 18:29:07,796 Epoch 669: Total Training Recognition Loss 0.77  Total Training Translation Loss 6.89 
2024-02-08 18:29:07,797 EPOCH 670
2024-02-08 18:29:12,423 Epoch 670: Total Training Recognition Loss 0.28  Total Training Translation Loss 4.80 
2024-02-08 18:29:12,423 EPOCH 671
2024-02-08 18:29:14,908 [Epoch: 671 Step: 00022800] Batch Recognition Loss:   0.001127 => Gls Tokens per Sec:     2576 || Batch Translation Loss:   0.042560 => Txt Tokens per Sec:     6996 || Lr: 0.000100
2024-02-08 18:29:16,962 Epoch 671: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.29 
2024-02-08 18:29:16,963 EPOCH 672
2024-02-08 18:29:21,941 Epoch 672: Total Training Recognition Loss 0.12  Total Training Translation Loss 2.31 
2024-02-08 18:29:21,941 EPOCH 673
2024-02-08 18:29:26,737 Epoch 673: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.76 
2024-02-08 18:29:26,737 EPOCH 674
2024-02-08 18:29:29,213 [Epoch: 674 Step: 00022900] Batch Recognition Loss:   0.003365 => Gls Tokens per Sec:     2327 || Batch Translation Loss:   0.050538 => Txt Tokens per Sec:     6777 || Lr: 0.000100
2024-02-08 18:29:30,851 Epoch 674: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.42 
2024-02-08 18:29:30,851 EPOCH 675
2024-02-08 18:29:35,765 Epoch 675: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.42 
2024-02-08 18:29:35,766 EPOCH 676
2024-02-08 18:29:40,186 Epoch 676: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.20 
2024-02-08 18:29:40,186 EPOCH 677
2024-02-08 18:29:42,299 [Epoch: 677 Step: 00023000] Batch Recognition Loss:   0.000725 => Gls Tokens per Sec:     2300 || Batch Translation Loss:   0.184814 => Txt Tokens per Sec:     6620 || Lr: 0.000100
2024-02-08 18:29:44,928 Epoch 677: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.29 
2024-02-08 18:29:44,928 EPOCH 678
2024-02-08 18:29:49,537 Epoch 678: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.02 
2024-02-08 18:29:49,538 EPOCH 679
2024-02-08 18:29:54,072 Epoch 679: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.64 
2024-02-08 18:29:54,073 EPOCH 680
2024-02-08 18:29:56,047 [Epoch: 680 Step: 00023100] Batch Recognition Loss:   0.001253 => Gls Tokens per Sec:     2271 || Batch Translation Loss:   0.012129 => Txt Tokens per Sec:     6286 || Lr: 0.000100
2024-02-08 18:29:58,926 Epoch 680: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.48 
2024-02-08 18:29:58,926 EPOCH 681
2024-02-08 18:30:03,059 Epoch 681: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.74 
2024-02-08 18:30:03,059 EPOCH 682
2024-02-08 18:30:07,911 Epoch 682: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.52 
2024-02-08 18:30:07,912 EPOCH 683
2024-02-08 18:30:09,478 [Epoch: 683 Step: 00023200] Batch Recognition Loss:   0.000974 => Gls Tokens per Sec:     2288 || Batch Translation Loss:   0.226603 => Txt Tokens per Sec:     6214 || Lr: 0.000100
2024-02-08 18:30:12,395 Epoch 683: Total Training Recognition Loss 0.07  Total Training Translation Loss 5.66 
2024-02-08 18:30:12,395 EPOCH 684
2024-02-08 18:30:17,066 Epoch 684: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.64 
2024-02-08 18:30:17,067 EPOCH 685
2024-02-08 18:30:21,690 Epoch 685: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.51 
2024-02-08 18:30:21,691 EPOCH 686
2024-02-08 18:30:22,915 [Epoch: 686 Step: 00023300] Batch Recognition Loss:   0.000867 => Gls Tokens per Sec:     2614 || Batch Translation Loss:   0.018925 => Txt Tokens per Sec:     7381 || Lr: 0.000100
2024-02-08 18:30:26,002 Epoch 686: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.92 
2024-02-08 18:30:26,003 EPOCH 687
2024-02-08 18:30:30,957 Epoch 687: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.82 
2024-02-08 18:30:30,958 EPOCH 688
2024-02-08 18:30:35,106 Epoch 688: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.86 
2024-02-08 18:30:35,106 EPOCH 689
2024-02-08 18:30:35,989 [Epoch: 689 Step: 00023400] Batch Recognition Loss:   0.002911 => Gls Tokens per Sec:     2902 || Batch Translation Loss:   0.008357 => Txt Tokens per Sec:     7357 || Lr: 0.000100
2024-02-08 18:30:40,105 Epoch 689: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.83 
2024-02-08 18:30:40,106 EPOCH 690
2024-02-08 18:30:44,899 Epoch 690: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.81 
2024-02-08 18:30:44,900 EPOCH 691
2024-02-08 18:30:49,749 Epoch 691: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.78 
2024-02-08 18:30:49,749 EPOCH 692
2024-02-08 18:30:50,390 [Epoch: 692 Step: 00023500] Batch Recognition Loss:   0.000312 => Gls Tokens per Sec:     3000 || Batch Translation Loss:   0.024509 => Txt Tokens per Sec:     8192 || Lr: 0.000100
2024-02-08 18:30:54,064 Epoch 692: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.63 
2024-02-08 18:30:54,064 EPOCH 693
2024-02-08 18:30:58,657 Epoch 693: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.67 
2024-02-08 18:30:58,658 EPOCH 694
2024-02-08 18:31:03,336 Epoch 694: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.68 
2024-02-08 18:31:03,336 EPOCH 695
2024-02-08 18:31:03,689 [Epoch: 695 Step: 00023600] Batch Recognition Loss:   0.000587 => Gls Tokens per Sec:     3657 || Batch Translation Loss:   0.017065 => Txt Tokens per Sec:     8737 || Lr: 0.000100
2024-02-08 18:31:07,456 Epoch 695: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.85 
2024-02-08 18:31:07,457 EPOCH 696
2024-02-08 18:31:11,561 Epoch 696: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.75 
2024-02-08 18:31:11,561 EPOCH 697
2024-02-08 18:31:15,682 Epoch 697: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.56 
2024-02-08 18:31:15,682 EPOCH 698
2024-02-08 18:31:16,177 [Epoch: 698 Step: 00023700] Batch Recognition Loss:   0.000843 => Gls Tokens per Sec:     1298 || Batch Translation Loss:   0.021512 => Txt Tokens per Sec:     4189 || Lr: 0.000100
2024-02-08 18:31:19,832 Epoch 698: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.72 
2024-02-08 18:31:19,833 EPOCH 699
2024-02-08 18:31:24,756 Epoch 699: Total Training Recognition Loss 0.06  Total Training Translation Loss 5.34 
2024-02-08 18:31:24,757 EPOCH 700
2024-02-08 18:31:29,155 [Epoch: 700 Step: 00023800] Batch Recognition Loss:   0.000669 => Gls Tokens per Sec:     2415 || Batch Translation Loss:   0.107435 => Txt Tokens per Sec:     6682 || Lr: 0.000100
2024-02-08 18:31:29,156 Epoch 700: Total Training Recognition Loss 0.07  Total Training Translation Loss 6.94 
2024-02-08 18:31:29,156 EPOCH 701
2024-02-08 18:31:33,833 Epoch 701: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.70 
2024-02-08 18:31:33,834 EPOCH 702
2024-02-08 18:31:38,501 Epoch 702: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.41 
2024-02-08 18:31:38,502 EPOCH 703
2024-02-08 18:31:42,618 [Epoch: 703 Step: 00023900] Batch Recognition Loss:   0.000818 => Gls Tokens per Sec:     2425 || Batch Translation Loss:   0.064679 => Txt Tokens per Sec:     6683 || Lr: 0.000100
2024-02-08 18:31:42,983 Epoch 703: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.25 
2024-02-08 18:31:42,984 EPOCH 704
2024-02-08 18:31:47,832 Epoch 704: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.70 
2024-02-08 18:31:47,832 EPOCH 705
2024-02-08 18:31:51,930 Epoch 705: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.85 
2024-02-08 18:31:51,931 EPOCH 706
2024-02-08 18:31:56,430 [Epoch: 706 Step: 00024000] Batch Recognition Loss:   0.004193 => Gls Tokens per Sec:     2077 || Batch Translation Loss:   0.027780 => Txt Tokens per Sec:     5703 || Lr: 0.000100
2024-02-08 18:32:05,234 Validation result at epoch 706, step    24000: duration: 8.8030s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.30562	Translation Loss: 93767.31250	PPL: 11678.36035
	Eval Metric: BLEU
	WER 3.60	(DEL: 0.00,	INS: 0.00,	SUB: 3.60)
	BLEU-4 0.67	(BLEU-1: 11.25,	BLEU-2: 3.76,	BLEU-3: 1.43,	BLEU-4: 0.67)
	CHRF 17.33	ROUGE 9.50
2024-02-08 18:32:05,235 Logging Recognition and Translation Outputs
2024-02-08 18:32:05,236 ========================================================================================================================
2024-02-08 18:32:05,236 Logging Sequence: 171_2.00
2024-02-08 18:32:05,236 	Gloss Reference :	A B+C+D+E      
2024-02-08 18:32:05,236 	Gloss Hypothesis:	A B+C+D+E+D+E+D
2024-02-08 18:32:05,236 	Gloss Alignment :	  S            
2024-02-08 18:32:05,236 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:32:05,238 	Text Reference  :	as you might all   know that the     ipl   is  about    to end  the finals are on    28th may   
2024-02-08 18:32:05,238 	Text Hypothesis :	** *** in    t20is and  ipl  bowling sides are expected to bowl the ****** 20  overs in   mumbai
2024-02-08 18:32:05,238 	Text Alignment  :	D  D   S     S     S    S    S       S     S   S           S        D      S   S     S    S     
2024-02-08 18:32:05,239 ========================================================================================================================
2024-02-08 18:32:05,239 Logging Sequence: 119_33.00
2024-02-08 18:32:05,239 	Gloss Reference :	A B+C+D+E
2024-02-08 18:32:05,239 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:32:05,239 	Gloss Alignment :	         
2024-02-08 18:32:05,239 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:32:05,240 	Text Reference  :	he wanted   to *** *** gift 35    people wow wonderful
2024-02-08 18:32:05,240 	Text Hypothesis :	** shocking to see the gift staff to     the argentina
2024-02-08 18:32:05,240 	Text Alignment  :	D  S           I   I        S     S      S   S        
2024-02-08 18:32:05,240 ========================================================================================================================
2024-02-08 18:32:05,240 Logging Sequence: 158_131.00
2024-02-08 18:32:05,241 	Gloss Reference :	A B+C+D+E
2024-02-08 18:32:05,241 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:32:05,241 	Gloss Alignment :	         
2024-02-08 18:32:05,241 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:32:05,242 	Text Reference  :	on 10th april 2023 there was a match between rcb and    lsg  in     bengaluru
2024-02-08 18:32:05,242 	Text Hypothesis :	** **** ***** **** ***** *** * ***** former  rcb player anil kumble said     
2024-02-08 18:32:05,242 	Text Alignment  :	D  D    D     D    D     D   D D     S           S      S    S      S        
2024-02-08 18:32:05,242 ========================================================================================================================
2024-02-08 18:32:05,242 Logging Sequence: 164_412.00
2024-02-08 18:32:05,242 	Gloss Reference :	A B+C+D+E
2024-02-08 18:32:05,242 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:32:05,243 	Gloss Alignment :	         
2024-02-08 18:32:05,243 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:32:05,244 	Text Reference  :	if you divide these two figures you  will **** be   shocked to    know that each ball's worth is rs   50     lakhs 
2024-02-08 18:32:05,245 	Text Hypothesis :	** *** ****** ***** *** the     bcci will also earn rs      10-15 lakh from each ****** ***** ** over having tested
2024-02-08 18:32:05,245 	Text Alignment  :	D  D   D      D     D   S       S         I    S    S       S     S    S         D      D     D  S    S      S     
2024-02-08 18:32:05,245 ========================================================================================================================
2024-02-08 18:32:05,245 Logging Sequence: 159_112.00
2024-02-08 18:32:05,245 	Gloss Reference :	A B+C+D+E
2024-02-08 18:32:05,245 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:32:05,245 	Gloss Alignment :	         
2024-02-08 18:32:05,245 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:32:05,247 	Text Reference  :	kohli had revealed that before the tournament he    did not touch his bat    for     a       month yes   1    month       
2024-02-08 18:32:05,247 	Text Hypothesis :	***** *** ******** **** ****** *** whoa       there is  a   break for making cricket council acc   whose name participated
2024-02-08 18:32:05,247 	Text Alignment  :	D     D   D        D    D      D   S          S     S   S   S     S   S      S       S       S     S     S    S           
2024-02-08 18:32:05,247 ========================================================================================================================
2024-02-08 18:32:05,897 Epoch 706: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.49 
2024-02-08 18:32:05,898 EPOCH 707
2024-02-08 18:32:10,716 Epoch 707: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.40 
2024-02-08 18:32:10,716 EPOCH 708
2024-02-08 18:32:15,448 Epoch 708: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.14 
2024-02-08 18:32:15,449 EPOCH 709
2024-02-08 18:32:19,452 [Epoch: 709 Step: 00024100] Batch Recognition Loss:   0.000300 => Gls Tokens per Sec:     2174 || Batch Translation Loss:   0.035125 => Txt Tokens per Sec:     6033 || Lr: 0.000100
2024-02-08 18:32:20,135 Epoch 709: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.97 
2024-02-08 18:32:20,135 EPOCH 710
2024-02-08 18:32:24,528 Epoch 710: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.22 
2024-02-08 18:32:24,528 EPOCH 711
2024-02-08 18:32:29,422 Epoch 711: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.25 
2024-02-08 18:32:29,422 EPOCH 712
2024-02-08 18:32:32,509 [Epoch: 712 Step: 00024200] Batch Recognition Loss:   0.000612 => Gls Tokens per Sec:     2612 || Batch Translation Loss:   0.039600 => Txt Tokens per Sec:     7239 || Lr: 0.000100
2024-02-08 18:32:33,565 Epoch 712: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.28 
2024-02-08 18:32:33,565 EPOCH 713
2024-02-08 18:32:38,559 Epoch 713: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.34 
2024-02-08 18:32:38,560 EPOCH 714
2024-02-08 18:32:42,781 Epoch 714: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.98 
2024-02-08 18:32:42,781 EPOCH 715
2024-02-08 18:32:45,922 [Epoch: 715 Step: 00024300] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:     2364 || Batch Translation Loss:   0.021115 => Txt Tokens per Sec:     6543 || Lr: 0.000100
2024-02-08 18:32:47,589 Epoch 715: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.97 
2024-02-08 18:32:47,589 EPOCH 716
2024-02-08 18:32:52,111 Epoch 716: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.93 
2024-02-08 18:32:52,111 EPOCH 717
2024-02-08 18:32:56,513 Epoch 717: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.79 
2024-02-08 18:32:56,514 EPOCH 718
2024-02-08 18:32:59,689 [Epoch: 718 Step: 00024400] Batch Recognition Loss:   0.000317 => Gls Tokens per Sec:     2219 || Batch Translation Loss:   0.027754 => Txt Tokens per Sec:     6035 || Lr: 0.000100
2024-02-08 18:33:01,478 Epoch 718: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.09 
2024-02-08 18:33:01,478 EPOCH 719
2024-02-08 18:33:05,754 Epoch 719: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.09 
2024-02-08 18:33:05,755 EPOCH 720
2024-02-08 18:33:10,740 Epoch 720: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.36 
2024-02-08 18:33:10,740 EPOCH 721
2024-02-08 18:33:13,437 [Epoch: 721 Step: 00024500] Batch Recognition Loss:   0.003512 => Gls Tokens per Sec:     2374 || Batch Translation Loss:   0.044456 => Txt Tokens per Sec:     6658 || Lr: 0.000100
2024-02-08 18:33:14,909 Epoch 721: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.24 
2024-02-08 18:33:14,909 EPOCH 722
2024-02-08 18:33:19,852 Epoch 722: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.36 
2024-02-08 18:33:19,852 EPOCH 723
2024-02-08 18:33:24,646 Epoch 723: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.10 
2024-02-08 18:33:24,647 EPOCH 724
2024-02-08 18:33:27,303 [Epoch: 724 Step: 00024600] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     2071 || Batch Translation Loss:   0.029246 => Txt Tokens per Sec:     5742 || Lr: 0.000100
2024-02-08 18:33:29,524 Epoch 724: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.83 
2024-02-08 18:33:29,524 EPOCH 725
2024-02-08 18:33:33,839 Epoch 725: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.07 
2024-02-08 18:33:33,839 EPOCH 726
2024-02-08 18:33:37,986 Epoch 726: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.14 
2024-02-08 18:33:37,986 EPOCH 727
2024-02-08 18:33:40,046 [Epoch: 727 Step: 00024700] Batch Recognition Loss:   0.000368 => Gls Tokens per Sec:     2361 || Batch Translation Loss:   0.029190 => Txt Tokens per Sec:     6239 || Lr: 0.000100
2024-02-08 18:33:42,854 Epoch 727: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.06 
2024-02-08 18:33:42,855 EPOCH 728
2024-02-08 18:33:47,911 Epoch 728: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.05 
2024-02-08 18:33:47,912 EPOCH 729
2024-02-08 18:33:52,847 Epoch 729: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.87 
2024-02-08 18:33:52,848 EPOCH 730
2024-02-08 18:33:54,884 [Epoch: 730 Step: 00024800] Batch Recognition Loss:   0.000490 => Gls Tokens per Sec:     2074 || Batch Translation Loss:   0.046217 => Txt Tokens per Sec:     5766 || Lr: 0.000100
2024-02-08 18:33:57,501 Epoch 730: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.57 
2024-02-08 18:33:57,501 EPOCH 731
2024-02-08 18:34:02,529 Epoch 731: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.06 
2024-02-08 18:34:02,530 EPOCH 732
2024-02-08 18:34:07,315 Epoch 732: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.75 
2024-02-08 18:34:07,315 EPOCH 733
2024-02-08 18:34:08,833 [Epoch: 733 Step: 00024900] Batch Recognition Loss:   0.001150 => Gls Tokens per Sec:     2533 || Batch Translation Loss:   0.074064 => Txt Tokens per Sec:     6935 || Lr: 0.000100
2024-02-08 18:34:12,076 Epoch 733: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.32 
2024-02-08 18:34:12,077 EPOCH 734
2024-02-08 18:34:17,089 Epoch 734: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.62 
2024-02-08 18:34:17,089 EPOCH 735
2024-02-08 18:34:22,121 Epoch 735: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.76 
2024-02-08 18:34:22,121 EPOCH 736
2024-02-08 18:34:23,335 [Epoch: 736 Step: 00025000] Batch Recognition Loss:   0.001027 => Gls Tokens per Sec:     2638 || Batch Translation Loss:   0.025762 => Txt Tokens per Sec:     7322 || Lr: 0.000100
2024-02-08 18:34:26,819 Epoch 736: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.38 
2024-02-08 18:34:26,819 EPOCH 737
2024-02-08 18:34:31,169 Epoch 737: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.50 
2024-02-08 18:34:31,170 EPOCH 738
2024-02-08 18:34:36,211 Epoch 738: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.15 
2024-02-08 18:34:36,211 EPOCH 739
2024-02-08 18:34:37,039 [Epoch: 739 Step: 00025100] Batch Recognition Loss:   0.000270 => Gls Tokens per Sec:     3094 || Batch Translation Loss:   0.034916 => Txt Tokens per Sec:     7758 || Lr: 0.000100
2024-02-08 18:34:40,989 Epoch 739: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.82 
2024-02-08 18:34:40,989 EPOCH 740
2024-02-08 18:34:45,988 Epoch 740: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.65 
2024-02-08 18:34:45,988 EPOCH 741
2024-02-08 18:34:51,266 Epoch 741: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.92 
2024-02-08 18:34:51,266 EPOCH 742
2024-02-08 18:34:52,069 [Epoch: 742 Step: 00025200] Batch Recognition Loss:   0.003606 => Gls Tokens per Sec:     2397 || Batch Translation Loss:   0.042451 => Txt Tokens per Sec:     6209 || Lr: 0.000100
2024-02-08 18:34:55,997 Epoch 742: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.40 
2024-02-08 18:34:55,997 EPOCH 743
2024-02-08 18:35:00,224 Epoch 743: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.99 
2024-02-08 18:35:00,224 EPOCH 744
2024-02-08 18:35:05,193 Epoch 744: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.27 
2024-02-08 18:35:05,194 EPOCH 745
2024-02-08 18:35:05,643 [Epoch: 745 Step: 00025300] Batch Recognition Loss:   0.001581 => Gls Tokens per Sec:     2857 || Batch Translation Loss:   0.051513 => Txt Tokens per Sec:     7978 || Lr: 0.000100
2024-02-08 18:35:09,735 Epoch 745: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.60 
2024-02-08 18:35:09,735 EPOCH 746
2024-02-08 18:35:14,542 Epoch 746: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.64 
2024-02-08 18:35:14,542 EPOCH 747
2024-02-08 18:35:18,642 Epoch 747: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.11 
2024-02-08 18:35:18,643 EPOCH 748
2024-02-08 18:35:18,816 [Epoch: 748 Step: 00025400] Batch Recognition Loss:   0.011696 => Gls Tokens per Sec:     3721 || Batch Translation Loss:   0.054885 => Txt Tokens per Sec:     7233 || Lr: 0.000100
2024-02-08 18:35:23,695 Epoch 748: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.65 
2024-02-08 18:35:23,696 EPOCH 749
2024-02-08 18:35:27,946 Epoch 749: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.14 
2024-02-08 18:35:27,946 EPOCH 750
2024-02-08 18:35:32,090 [Epoch: 750 Step: 00025500] Batch Recognition Loss:   0.000566 => Gls Tokens per Sec:     2563 || Batch Translation Loss:   0.035814 => Txt Tokens per Sec:     7092 || Lr: 0.000100
2024-02-08 18:35:32,090 Epoch 750: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.94 
2024-02-08 18:35:32,090 EPOCH 751
2024-02-08 18:35:36,258 Epoch 751: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.45 
2024-02-08 18:35:36,259 EPOCH 752
2024-02-08 18:35:41,272 Epoch 752: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.83 
2024-02-08 18:35:41,272 EPOCH 753
2024-02-08 18:35:45,448 [Epoch: 753 Step: 00025600] Batch Recognition Loss:   0.000666 => Gls Tokens per Sec:     2390 || Batch Translation Loss:   0.025777 => Txt Tokens per Sec:     6604 || Lr: 0.000100
2024-02-08 18:35:45,735 Epoch 753: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.11 
2024-02-08 18:35:45,735 EPOCH 754
2024-02-08 18:35:50,701 Epoch 754: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.78 
2024-02-08 18:35:50,702 EPOCH 755
2024-02-08 18:35:54,889 Epoch 755: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.82 
2024-02-08 18:35:54,889 EPOCH 756
2024-02-08 18:35:59,224 [Epoch: 756 Step: 00025700] Batch Recognition Loss:   0.000415 => Gls Tokens per Sec:     2155 || Batch Translation Loss:   0.006418 => Txt Tokens per Sec:     5943 || Lr: 0.000100
2024-02-08 18:35:59,825 Epoch 756: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.15 
2024-02-08 18:35:59,825 EPOCH 757
2024-02-08 18:36:04,196 Epoch 757: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.36 
2024-02-08 18:36:04,197 EPOCH 758
2024-02-08 18:36:08,915 Epoch 758: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.43 
2024-02-08 18:36:08,916 EPOCH 759
2024-02-08 18:36:13,276 [Epoch: 759 Step: 00025800] Batch Recognition Loss:   0.000413 => Gls Tokens per Sec:     2056 || Batch Translation Loss:   0.033286 => Txt Tokens per Sec:     5663 || Lr: 0.000100
2024-02-08 18:36:14,014 Epoch 759: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.96 
2024-02-08 18:36:14,014 EPOCH 760
2024-02-08 18:36:18,289 Epoch 760: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.58 
2024-02-08 18:36:18,289 EPOCH 761
2024-02-08 18:36:22,973 Epoch 761: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.23 
2024-02-08 18:36:22,974 EPOCH 762
2024-02-08 18:36:26,474 [Epoch: 762 Step: 00025900] Batch Recognition Loss:   0.000384 => Gls Tokens per Sec:     2304 || Batch Translation Loss:   0.058532 => Txt Tokens per Sec:     6473 || Lr: 0.000100
2024-02-08 18:36:27,658 Epoch 762: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.86 
2024-02-08 18:36:27,659 EPOCH 763
2024-02-08 18:36:32,039 Epoch 763: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.35 
2024-02-08 18:36:32,039 EPOCH 764
2024-02-08 18:36:37,152 Epoch 764: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.53 
2024-02-08 18:36:37,152 EPOCH 765
2024-02-08 18:36:40,319 [Epoch: 765 Step: 00026000] Batch Recognition Loss:   0.000635 => Gls Tokens per Sec:     2344 || Batch Translation Loss:   0.061987 => Txt Tokens per Sec:     6281 || Lr: 0.000100
2024-02-08 18:36:48,910 Validation result at epoch 765, step    26000: duration: 8.5910s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.47309	Translation Loss: 93612.33594	PPL: 11498.98145
	Eval Metric: BLEU
	WER 3.67	(DEL: 0.00,	INS: 0.00,	SUB: 3.67)
	BLEU-4 0.80	(BLEU-1: 11.59,	BLEU-2: 3.82,	BLEU-3: 1.60,	BLEU-4: 0.80)
	CHRF 17.32	ROUGE 9.82
2024-02-08 18:36:48,912 Logging Recognition and Translation Outputs
2024-02-08 18:36:48,912 ========================================================================================================================
2024-02-08 18:36:48,912 Logging Sequence: 166_243.00
2024-02-08 18:36:48,912 	Gloss Reference :	A B+C+D+E
2024-02-08 18:36:48,912 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:36:48,912 	Gloss Alignment :	         
2024-02-08 18:36:48,912 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:36:48,914 	Text Reference  :	*** ********* *********** ********* *** ***** ** icc     worked with members boards like bcci pcb   cricket australia etc 
2024-02-08 18:36:48,914 	Text Hypothesis :	the broadcast advertisers ticketing etc would be decided by     the  board   of     the  2    teams playing the       test
2024-02-08 18:36:48,914 	Text Alignment  :	I   I         I           I         I   I     I  S       S      S    S       S      S    S    S     S       S         S   
2024-02-08 18:36:48,914 ========================================================================================================================
2024-02-08 18:36:48,914 Logging Sequence: 59_152.00
2024-02-08 18:36:48,915 	Gloss Reference :	A B+C+D+E
2024-02-08 18:36:48,915 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:36:48,915 	Gloss Alignment :	         
2024-02-08 18:36:48,915 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:36:48,916 	Text Reference  :	the organisers encouraged athletes to use    the         condoms in their home countries
2024-02-08 18:36:48,916 	Text Hypothesis :	the ********** ********** ******** ** second tie-breaker ended   in ***** a    draw     
2024-02-08 18:36:48,916 	Text Alignment  :	    D          D          D        D  S      S           S          D     S    S        
2024-02-08 18:36:48,916 ========================================================================================================================
2024-02-08 18:36:48,916 Logging Sequence: 145_52.00
2024-02-08 18:36:48,916 	Gloss Reference :	A B+C+D+E
2024-02-08 18:36:48,917 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:36:48,917 	Gloss Alignment :	         
2024-02-08 18:36:48,917 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:36:48,917 	Text Reference  :	her name was dropped despite having qualified as    she was the only      female  athlete
2024-02-08 18:36:48,918 	Text Hypothesis :	*** **** *** ******* ******* ****** after     which she *** *** announced women's team   
2024-02-08 18:36:48,918 	Text Alignment  :	D   D    D   D       D       D      S         S         D   D   S         S       S      
2024-02-08 18:36:48,918 ========================================================================================================================
2024-02-08 18:36:48,918 Logging Sequence: 172_163.00
2024-02-08 18:36:48,918 	Gloss Reference :	A B+C+D+E
2024-02-08 18:36:48,918 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:36:48,919 	Gloss Alignment :	         
2024-02-08 18:36:48,919 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:36:48,921 	Text Reference  :	*** ** if      the ***** match starts anywhere between 730  pm    to 935 pm a        full 20-over match can    be  played
2024-02-08 18:36:48,921 	Text Hypothesis :	the oc started the probe and   were   given    2       more weeks to *** ** complete the  probe   and   submit the report
2024-02-08 18:36:48,921 	Text Alignment  :	I   I  S           I     S     S      S        S       S    S        D   D  S        S    S       S     S      S   S     
2024-02-08 18:36:48,921 ========================================================================================================================
2024-02-08 18:36:48,921 Logging Sequence: 150_20.00
2024-02-08 18:36:48,921 	Gloss Reference :	A B+C+D+E
2024-02-08 18:36:48,921 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:36:48,922 	Gloss Alignment :	         
2024-02-08 18:36:48,922 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:36:48,923 	Text Reference  :	*** **** ** after a        tough   match india  won the saff championship 2023 title
2024-02-08 18:36:48,923 	Text Hypothesis :	now said 'i will  continue playing in    mumbai if  we  will play         sri  lanka
2024-02-08 18:36:48,923 	Text Alignment  :	I   I    I  S     S        S       S     S      S   S   S    S            S    S    
2024-02-08 18:36:48,923 ========================================================================================================================
2024-02-08 18:36:50,724 Epoch 765: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.24 
2024-02-08 18:36:50,724 EPOCH 766
2024-02-08 18:36:55,807 Epoch 766: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.22 
2024-02-08 18:36:55,808 EPOCH 767
2024-02-08 18:37:00,097 Epoch 767: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.60 
2024-02-08 18:37:00,097 EPOCH 768
2024-02-08 18:37:02,984 [Epoch: 768 Step: 00026100] Batch Recognition Loss:   0.001088 => Gls Tokens per Sec:     2350 || Batch Translation Loss:   0.020174 => Txt Tokens per Sec:     6436 || Lr: 0.000100
2024-02-08 18:37:04,914 Epoch 768: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.34 
2024-02-08 18:37:04,914 EPOCH 769
2024-02-08 18:37:09,420 Epoch 769: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.11 
2024-02-08 18:37:09,420 EPOCH 770
2024-02-08 18:37:14,103 Epoch 770: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.99 
2024-02-08 18:37:14,104 EPOCH 771
2024-02-08 18:37:17,029 [Epoch: 771 Step: 00026200] Batch Recognition Loss:   0.002854 => Gls Tokens per Sec:     2099 || Batch Translation Loss:   0.030831 => Txt Tokens per Sec:     5972 || Lr: 0.000100
2024-02-08 18:37:18,737 Epoch 771: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.91 
2024-02-08 18:37:18,738 EPOCH 772
2024-02-08 18:37:23,083 Epoch 772: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.98 
2024-02-08 18:37:23,083 EPOCH 773
2024-02-08 18:37:28,005 Epoch 773: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.08 
2024-02-08 18:37:28,005 EPOCH 774
2024-02-08 18:37:30,578 [Epoch: 774 Step: 00026300] Batch Recognition Loss:   0.002268 => Gls Tokens per Sec:     2240 || Batch Translation Loss:   0.050717 => Txt Tokens per Sec:     6582 || Lr: 0.000100
2024-02-08 18:37:32,198 Epoch 774: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.49 
2024-02-08 18:37:32,198 EPOCH 775
2024-02-08 18:37:37,465 Epoch 775: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.34 
2024-02-08 18:37:37,466 EPOCH 776
2024-02-08 18:37:42,172 Epoch 776: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.15 
2024-02-08 18:37:42,172 EPOCH 777
2024-02-08 18:37:43,754 [Epoch: 777 Step: 00026400] Batch Recognition Loss:   0.000240 => Gls Tokens per Sec:     3074 || Batch Translation Loss:   0.019404 => Txt Tokens per Sec:     7914 || Lr: 0.000100
2024-02-08 18:37:46,286 Epoch 777: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.07 
2024-02-08 18:37:46,286 EPOCH 778
2024-02-08 18:37:51,315 Epoch 778: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.53 
2024-02-08 18:37:51,316 EPOCH 779
2024-02-08 18:37:55,540 Epoch 779: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.80 
2024-02-08 18:37:55,540 EPOCH 780
2024-02-08 18:37:57,803 [Epoch: 780 Step: 00026500] Batch Recognition Loss:   0.000220 => Gls Tokens per Sec:     1866 || Batch Translation Loss:   0.004392 => Txt Tokens per Sec:     5359 || Lr: 0.000100
2024-02-08 18:38:00,572 Epoch 780: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.50 
2024-02-08 18:38:00,573 EPOCH 781
2024-02-08 18:38:05,489 Epoch 781: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.62 
2024-02-08 18:38:05,489 EPOCH 782
2024-02-08 18:38:09,567 Epoch 782: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.03 
2024-02-08 18:38:09,567 EPOCH 783
2024-02-08 18:38:11,222 [Epoch: 783 Step: 00026600] Batch Recognition Loss:   0.005606 => Gls Tokens per Sec:     2322 || Batch Translation Loss:   0.233779 => Txt Tokens per Sec:     5909 || Lr: 0.000100
2024-02-08 18:38:14,614 Epoch 783: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.14 
2024-02-08 18:38:14,615 EPOCH 784
2024-02-08 18:38:18,933 Epoch 784: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.76 
2024-02-08 18:38:18,933 EPOCH 785
2024-02-08 18:38:23,709 Epoch 785: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.31 
2024-02-08 18:38:23,710 EPOCH 786
2024-02-08 18:38:25,429 [Epoch: 786 Step: 00026700] Batch Recognition Loss:   0.000323 => Gls Tokens per Sec:     1863 || Batch Translation Loss:   0.117107 => Txt Tokens per Sec:     5361 || Lr: 0.000100
2024-02-08 18:38:28,218 Epoch 786: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.50 
2024-02-08 18:38:28,219 EPOCH 787
2024-02-08 18:38:32,330 Epoch 787: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.53 
2024-02-08 18:38:32,331 EPOCH 788
2024-02-08 18:38:36,483 Epoch 788: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.40 
2024-02-08 18:38:36,484 EPOCH 789
2024-02-08 18:38:37,986 [Epoch: 789 Step: 00026800] Batch Recognition Loss:   0.001338 => Gls Tokens per Sec:     1707 || Batch Translation Loss:   0.019811 => Txt Tokens per Sec:     4918 || Lr: 0.000100
2024-02-08 18:38:41,579 Epoch 789: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.66 
2024-02-08 18:38:41,579 EPOCH 790
2024-02-08 18:38:45,827 Epoch 790: Total Training Recognition Loss 0.11  Total Training Translation Loss 5.62 
2024-02-08 18:38:45,827 EPOCH 791
2024-02-08 18:38:49,944 Epoch 791: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.64 
2024-02-08 18:38:49,944 EPOCH 792
2024-02-08 18:38:50,691 [Epoch: 792 Step: 00026900] Batch Recognition Loss:   0.000352 => Gls Tokens per Sec:     2570 || Batch Translation Loss:   0.054160 => Txt Tokens per Sec:     7482 || Lr: 0.000100
2024-02-08 18:38:54,209 Epoch 792: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.00 
2024-02-08 18:38:54,210 EPOCH 793
2024-02-08 18:38:59,222 Epoch 793: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.14 
2024-02-08 18:38:59,223 EPOCH 794
2024-02-08 18:39:03,432 Epoch 794: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.78 
2024-02-08 18:39:03,432 EPOCH 795
2024-02-08 18:39:03,929 [Epoch: 795 Step: 00027000] Batch Recognition Loss:   0.000412 => Gls Tokens per Sec:     2056 || Batch Translation Loss:   0.040987 => Txt Tokens per Sec:     6302 || Lr: 0.000100
2024-02-08 18:39:08,452 Epoch 795: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.94 
2024-02-08 18:39:08,452 EPOCH 796
2024-02-08 18:39:12,778 Epoch 796: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.80 
2024-02-08 18:39:12,778 EPOCH 797
2024-02-08 18:39:17,472 Epoch 797: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.68 
2024-02-08 18:39:17,473 EPOCH 798
2024-02-08 18:39:17,647 [Epoch: 798 Step: 00027100] Batch Recognition Loss:   0.000399 => Gls Tokens per Sec:     3699 || Batch Translation Loss:   0.016560 => Txt Tokens per Sec:     9896 || Lr: 0.000100
2024-02-08 18:39:22,053 Epoch 798: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.87 
2024-02-08 18:39:22,053 EPOCH 799
2024-02-08 18:39:26,182 Epoch 799: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.32 
2024-02-08 18:39:26,182 EPOCH 800
2024-02-08 18:39:30,560 [Epoch: 800 Step: 00027200] Batch Recognition Loss:   0.000300 => Gls Tokens per Sec:     2426 || Batch Translation Loss:   0.029758 => Txt Tokens per Sec:     6713 || Lr: 0.000100
2024-02-08 18:39:30,560 Epoch 800: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.15 
2024-02-08 18:39:30,561 EPOCH 801
2024-02-08 18:39:35,535 Epoch 801: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.73 
2024-02-08 18:39:35,536 EPOCH 802
2024-02-08 18:39:40,188 Epoch 802: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.68 
2024-02-08 18:39:40,188 EPOCH 803
2024-02-08 18:39:44,014 [Epoch: 803 Step: 00027300] Batch Recognition Loss:   0.001646 => Gls Tokens per Sec:     2609 || Batch Translation Loss:   0.009873 => Txt Tokens per Sec:     7174 || Lr: 0.000100
2024-02-08 18:39:44,274 Epoch 803: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.54 
2024-02-08 18:39:44,274 EPOCH 804
2024-02-08 18:39:48,390 Epoch 804: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-08 18:39:48,390 EPOCH 805
2024-02-08 18:39:52,489 Epoch 805: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.06 
2024-02-08 18:39:52,489 EPOCH 806
2024-02-08 18:39:56,199 [Epoch: 806 Step: 00027400] Batch Recognition Loss:   0.000386 => Gls Tokens per Sec:     2518 || Batch Translation Loss:   0.041930 => Txt Tokens per Sec:     7066 || Lr: 0.000100
2024-02-08 18:39:56,564 Epoch 806: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.23 
2024-02-08 18:39:56,564 EPOCH 807
2024-02-08 18:40:00,649 Epoch 807: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.11 
2024-02-08 18:40:00,649 EPOCH 808
2024-02-08 18:40:05,054 Epoch 808: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.63 
2024-02-08 18:40:05,055 EPOCH 809
2024-02-08 18:40:09,281 [Epoch: 809 Step: 00027500] Batch Recognition Loss:   0.000755 => Gls Tokens per Sec:     2121 || Batch Translation Loss:   0.062923 => Txt Tokens per Sec:     5915 || Lr: 0.000100
2024-02-08 18:40:10,031 Epoch 809: Total Training Recognition Loss 0.06  Total Training Translation Loss 5.30 
2024-02-08 18:40:10,031 EPOCH 810
2024-02-08 18:40:15,022 Epoch 810: Total Training Recognition Loss 0.07  Total Training Translation Loss 7.48 
2024-02-08 18:40:15,022 EPOCH 811
2024-02-08 18:40:19,953 Epoch 811: Total Training Recognition Loss 0.15  Total Training Translation Loss 7.45 
2024-02-08 18:40:19,953 EPOCH 812
2024-02-08 18:40:23,358 [Epoch: 812 Step: 00027600] Batch Recognition Loss:   0.000729 => Gls Tokens per Sec:     2368 || Batch Translation Loss:   0.128679 => Txt Tokens per Sec:     6732 || Lr: 0.000100
2024-02-08 18:40:24,125 Epoch 812: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.96 
2024-02-08 18:40:24,125 EPOCH 813
2024-02-08 18:40:29,075 Epoch 813: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.30 
2024-02-08 18:40:29,075 EPOCH 814
2024-02-08 18:40:33,428 Epoch 814: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.89 
2024-02-08 18:40:33,429 EPOCH 815
2024-02-08 18:40:36,319 [Epoch: 815 Step: 00027700] Batch Recognition Loss:   0.000349 => Gls Tokens per Sec:     2568 || Batch Translation Loss:   0.033887 => Txt Tokens per Sec:     6867 || Lr: 0.000100
2024-02-08 18:40:38,128 Epoch 815: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.74 
2024-02-08 18:40:38,128 EPOCH 816
2024-02-08 18:40:42,678 Epoch 816: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.64 
2024-02-08 18:40:42,678 EPOCH 817
2024-02-08 18:40:47,188 Epoch 817: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.47 
2024-02-08 18:40:47,188 EPOCH 818
2024-02-08 18:40:50,405 [Epoch: 818 Step: 00027800] Batch Recognition Loss:   0.000328 => Gls Tokens per Sec:     2109 || Batch Translation Loss:   0.014072 => Txt Tokens per Sec:     5904 || Lr: 0.000100
2024-02-08 18:40:52,067 Epoch 818: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.33 
2024-02-08 18:40:52,068 EPOCH 819
2024-02-08 18:40:56,978 Epoch 819: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.44 
2024-02-08 18:40:56,979 EPOCH 820
2024-02-08 18:41:01,101 Epoch 820: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.66 
2024-02-08 18:41:01,101 EPOCH 821
2024-02-08 18:41:03,183 [Epoch: 821 Step: 00027900] Batch Recognition Loss:   0.000316 => Gls Tokens per Sec:     2950 || Batch Translation Loss:   0.034075 => Txt Tokens per Sec:     7782 || Lr: 0.000100
2024-02-08 18:41:05,160 Epoch 821: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.54 
2024-02-08 18:41:05,160 EPOCH 822
2024-02-08 18:41:09,987 Epoch 822: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.85 
2024-02-08 18:41:09,987 EPOCH 823
2024-02-08 18:41:14,914 Epoch 823: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.82 
2024-02-08 18:41:14,915 EPOCH 824
2024-02-08 18:41:16,817 [Epoch: 824 Step: 00028000] Batch Recognition Loss:   0.000298 => Gls Tokens per Sec:     3030 || Batch Translation Loss:   0.108413 => Txt Tokens per Sec:     7991 || Lr: 0.000100
2024-02-08 18:41:25,605 Validation result at epoch 824, step    28000: duration: 8.7880s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.66081	Translation Loss: 94164.29688	PPL: 12150.70801
	Eval Metric: BLEU
	WER 3.88	(DEL: 0.00,	INS: 0.00,	SUB: 3.88)
	BLEU-4 0.70	(BLEU-1: 11.56,	BLEU-2: 3.78,	BLEU-3: 1.51,	BLEU-4: 0.70)
	CHRF 17.56	ROUGE 9.91
2024-02-08 18:41:25,606 Logging Recognition and Translation Outputs
2024-02-08 18:41:25,606 ========================================================================================================================
2024-02-08 18:41:25,606 Logging Sequence: 156_288.00
2024-02-08 18:41:25,606 	Gloss Reference :	A B+C+D+E      
2024-02-08 18:41:25,607 	Gloss Hypothesis:	A B+C+D+E+D+E+D
2024-02-08 18:41:25,607 	Gloss Alignment :	  S            
2024-02-08 18:41:25,607 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:41:25,608 	Text Reference  :	****** pooran led  the         team to victory miny became winners of  the 1st season
2024-02-08 18:41:25,608 	Text Hypothesis :	people were   seen celebrating this to ******* **** ****** ******* see him as  well  
2024-02-08 18:41:25,608 	Text Alignment  :	I      S      S    S           S       D       D    D      D       S   S   S   S     
2024-02-08 18:41:25,608 ========================================================================================================================
2024-02-08 18:41:25,609 Logging Sequence: 98_135.00
2024-02-08 18:41:25,609 	Gloss Reference :	A B+C+D+E
2024-02-08 18:41:25,609 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:41:25,609 	Gloss Alignment :	         
2024-02-08 18:41:25,609 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:41:25,610 	Text Reference  :	however due to the rise   in   coronavirus cases the        tournament was shifted
2024-02-08 18:41:25,610 	Text Hypothesis :	******* *** ** the indian team was         very  particular about      his parents
2024-02-08 18:41:25,610 	Text Alignment  :	D       D   D      S      S    S           S     S          S          S   S      
2024-02-08 18:41:25,611 ========================================================================================================================
2024-02-08 18:41:25,611 Logging Sequence: 161_47.00
2024-02-08 18:41:25,611 	Gloss Reference :	A B+C+D+E
2024-02-08 18:41:25,611 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:41:25,611 	Gloss Alignment :	         
2024-02-08 18:41:25,611 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:41:25,612 	Text Reference  :	** **** he ********* requested confidentiality as he was planning to make an official announcement
2024-02-08 18:41:25,612 	Text Hypothesis :	we hope he continues to        rain            as he was ******** ** **** ** not      comfortable 
2024-02-08 18:41:25,613 	Text Alignment  :	I  I       I         S         S                         D        D  D    D  S        S           
2024-02-08 18:41:25,613 ========================================================================================================================
2024-02-08 18:41:25,613 Logging Sequence: 131_159.00
2024-02-08 18:41:25,613 	Gloss Reference :	A B+C+D+E
2024-02-08 18:41:25,613 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:41:25,613 	Gloss Alignment :	         
2024-02-08 18:41:25,614 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:41:25,615 	Text Reference  :	chanu also met  biren singh following the meeting singh described chanu as            our nation' pride  
2024-02-08 18:41:25,615 	Text Hypothesis :	***** the  bcci will  also  earn      rs  10-15   lakh  from      each  advertisement of  30      seconds
2024-02-08 18:41:25,615 	Text Alignment  :	D     S    S    S     S     S         S   S       S     S         S     S             S   S       S      
2024-02-08 18:41:25,615 ========================================================================================================================
2024-02-08 18:41:25,615 Logging Sequence: 137_167.00
2024-02-08 18:41:25,616 	Gloss Reference :	A B+C+D+E
2024-02-08 18:41:25,616 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:41:25,616 	Gloss Alignment :	         
2024-02-08 18:41:25,616 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:41:25,618 	Text Reference  :	however after 630 pm  there  will     be       certain fan zones    where beer    will be available and nowhere else 
2024-02-08 18:41:25,618 	Text Hypothesis :	******* ***** *** the polish national football team    was escorted by    fighter jets on their     way to      qatar
2024-02-08 18:41:25,618 	Text Alignment  :	D       D     D   S   S      S        S        S       S   S        S     S       S    S  S         S   S       S    
2024-02-08 18:41:25,618 ========================================================================================================================
2024-02-08 18:41:28,457 Epoch 824: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.82 
2024-02-08 18:41:28,458 EPOCH 825
2024-02-08 18:41:33,438 Epoch 825: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.68 
2024-02-08 18:41:33,438 EPOCH 826
2024-02-08 18:41:38,576 Epoch 826: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.68 
2024-02-08 18:41:38,577 EPOCH 827
2024-02-08 18:41:40,932 [Epoch: 827 Step: 00028100] Batch Recognition Loss:   0.003838 => Gls Tokens per Sec:     2065 || Batch Translation Loss:   0.018217 => Txt Tokens per Sec:     5952 || Lr: 0.000100
2024-02-08 18:41:43,016 Epoch 827: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.81 
2024-02-08 18:41:43,017 EPOCH 828
2024-02-08 18:41:47,864 Epoch 828: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.12 
2024-02-08 18:41:47,864 EPOCH 829
2024-02-08 18:41:52,618 Epoch 829: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.65 
2024-02-08 18:41:52,619 EPOCH 830
2024-02-08 18:41:54,625 [Epoch: 830 Step: 00028200] Batch Recognition Loss:   0.000243 => Gls Tokens per Sec:     2234 || Batch Translation Loss:   0.040835 => Txt Tokens per Sec:     6329 || Lr: 0.000100
2024-02-08 18:41:57,203 Epoch 830: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.31 
2024-02-08 18:41:57,204 EPOCH 831
2024-02-08 18:42:01,320 Epoch 831: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.03 
2024-02-08 18:42:01,320 EPOCH 832
2024-02-08 18:42:05,452 Epoch 832: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.76 
2024-02-08 18:42:05,452 EPOCH 833
2024-02-08 18:42:07,082 [Epoch: 833 Step: 00028300] Batch Recognition Loss:   0.000328 => Gls Tokens per Sec:     2359 || Batch Translation Loss:   0.056987 => Txt Tokens per Sec:     6641 || Lr: 0.000100
2024-02-08 18:42:10,280 Epoch 833: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.08 
2024-02-08 18:42:10,281 EPOCH 834
2024-02-08 18:42:14,767 Epoch 834: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.21 
2024-02-08 18:42:14,767 EPOCH 835
2024-02-08 18:42:19,304 Epoch 835: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.22 
2024-02-08 18:42:19,305 EPOCH 836
2024-02-08 18:42:20,438 [Epoch: 836 Step: 00028400] Batch Recognition Loss:   0.002521 => Gls Tokens per Sec:     2827 || Batch Translation Loss:   0.113848 => Txt Tokens per Sec:     7096 || Lr: 0.000100
2024-02-08 18:42:24,361 Epoch 836: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.37 
2024-02-08 18:42:24,362 EPOCH 837
2024-02-08 18:42:28,683 Epoch 837: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.38 
2024-02-08 18:42:28,684 EPOCH 838
2024-02-08 18:42:33,096 Epoch 838: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.43 
2024-02-08 18:42:33,096 EPOCH 839
2024-02-08 18:42:34,561 [Epoch: 839 Step: 00028500] Batch Recognition Loss:   0.004728 => Gls Tokens per Sec:     1572 || Batch Translation Loss:   0.180697 => Txt Tokens per Sec:     4785 || Lr: 0.000100
2024-02-08 18:42:38,039 Epoch 839: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.47 
2024-02-08 18:42:38,040 EPOCH 840
2024-02-08 18:42:42,119 Epoch 840: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.16 
2024-02-08 18:42:42,119 EPOCH 841
2024-02-08 18:42:47,097 Epoch 841: Total Training Recognition Loss 0.07  Total Training Translation Loss 5.49 
2024-02-08 18:42:47,097 EPOCH 842
2024-02-08 18:42:48,024 [Epoch: 842 Step: 00028600] Batch Recognition Loss:   0.000744 => Gls Tokens per Sec:     2076 || Batch Translation Loss:   0.203822 => Txt Tokens per Sec:     5662 || Lr: 0.000100
2024-02-08 18:42:51,398 Epoch 842: Total Training Recognition Loss 0.14  Total Training Translation Loss 2.52 
2024-02-08 18:42:51,398 EPOCH 843
2024-02-08 18:42:55,930 Epoch 843: Total Training Recognition Loss 0.99  Total Training Translation Loss 1.61 
2024-02-08 18:42:55,931 EPOCH 844
2024-02-08 18:43:00,697 Epoch 844: Total Training Recognition Loss 1.61  Total Training Translation Loss 1.31 
2024-02-08 18:43:00,697 EPOCH 845
2024-02-08 18:43:01,090 [Epoch: 845 Step: 00028700] Batch Recognition Loss:   0.002134 => Gls Tokens per Sec:     3265 || Batch Translation Loss:   0.029057 => Txt Tokens per Sec:     8079 || Lr: 0.000100
2024-02-08 18:43:04,776 Epoch 845: Total Training Recognition Loss 1.93  Total Training Translation Loss 1.36 
2024-02-08 18:43:04,776 EPOCH 846
2024-02-08 18:43:08,930 Epoch 846: Total Training Recognition Loss 0.52  Total Training Translation Loss 1.26 
2024-02-08 18:43:08,930 EPOCH 847
2024-02-08 18:43:13,628 Epoch 847: Total Training Recognition Loss 0.56  Total Training Translation Loss 0.95 
2024-02-08 18:43:13,629 EPOCH 848
2024-02-08 18:43:13,883 [Epoch: 848 Step: 00028800] Batch Recognition Loss:   0.011148 => Gls Tokens per Sec:     2530 || Batch Translation Loss:   0.026701 => Txt Tokens per Sec:     6549 || Lr: 0.000100
2024-02-08 18:43:18,266 Epoch 848: Total Training Recognition Loss 0.38  Total Training Translation Loss 0.90 
2024-02-08 18:43:18,266 EPOCH 849
2024-02-08 18:43:22,318 Epoch 849: Total Training Recognition Loss 0.11  Total Training Translation Loss 0.90 
2024-02-08 18:43:22,319 EPOCH 850
2024-02-08 18:43:26,949 [Epoch: 850 Step: 00028900] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:     2294 || Batch Translation Loss:   0.021996 => Txt Tokens per Sec:     6346 || Lr: 0.000100
2024-02-08 18:43:26,950 Epoch 850: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.74 
2024-02-08 18:43:26,950 EPOCH 851
2024-02-08 18:43:31,743 Epoch 851: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.74 
2024-02-08 18:43:31,743 EPOCH 852
2024-02-08 18:43:36,082 Epoch 852: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.84 
2024-02-08 18:43:36,082 EPOCH 853
2024-02-08 18:43:40,772 [Epoch: 853 Step: 00029000] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     2129 || Batch Translation Loss:   0.022609 => Txt Tokens per Sec:     5903 || Lr: 0.000100
2024-02-08 18:43:41,071 Epoch 853: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.86 
2024-02-08 18:43:41,071 EPOCH 854
2024-02-08 18:43:45,190 Epoch 854: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.79 
2024-02-08 18:43:45,190 EPOCH 855
2024-02-08 18:43:50,157 Epoch 855: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-08 18:43:50,158 EPOCH 856
2024-02-08 18:43:54,020 [Epoch: 856 Step: 00029100] Batch Recognition Loss:   0.000166 => Gls Tokens per Sec:     2486 || Batch Translation Loss:   0.014804 => Txt Tokens per Sec:     6848 || Lr: 0.000100
2024-02-08 18:43:54,514 Epoch 856: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.77 
2024-02-08 18:43:54,514 EPOCH 857
2024-02-08 18:43:59,276 Epoch 857: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.86 
2024-02-08 18:43:59,277 EPOCH 858
2024-02-08 18:44:03,817 Epoch 858: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.02 
2024-02-08 18:44:03,818 EPOCH 859
2024-02-08 18:44:07,446 [Epoch: 859 Step: 00029200] Batch Recognition Loss:   0.000197 => Gls Tokens per Sec:     2470 || Batch Translation Loss:   0.021461 => Txt Tokens per Sec:     6812 || Lr: 0.000100
2024-02-08 18:44:08,330 Epoch 859: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.54 
2024-02-08 18:44:08,330 EPOCH 860
2024-02-08 18:44:13,433 Epoch 860: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.99 
2024-02-08 18:44:13,434 EPOCH 861
2024-02-08 18:44:17,818 Epoch 861: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.09 
2024-02-08 18:44:17,818 EPOCH 862
2024-02-08 18:44:21,084 [Epoch: 862 Step: 00029300] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:     2469 || Batch Translation Loss:   0.023518 => Txt Tokens per Sec:     6775 || Lr: 0.000100
2024-02-08 18:44:22,493 Epoch 862: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.92 
2024-02-08 18:44:22,493 EPOCH 863
2024-02-08 18:44:27,157 Epoch 863: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.04 
2024-02-08 18:44:27,158 EPOCH 864
2024-02-08 18:44:31,495 Epoch 864: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.85 
2024-02-08 18:44:31,495 EPOCH 865
2024-02-08 18:44:35,315 [Epoch: 865 Step: 00029400] Batch Recognition Loss:   0.000299 => Gls Tokens per Sec:     2011 || Batch Translation Loss:   0.160841 => Txt Tokens per Sec:     5620 || Lr: 0.000100
2024-02-08 18:44:36,509 Epoch 865: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.80 
2024-02-08 18:44:36,509 EPOCH 866
2024-02-08 18:44:40,707 Epoch 866: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.78 
2024-02-08 18:44:40,708 EPOCH 867
2024-02-08 18:44:45,712 Epoch 867: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.59 
2024-02-08 18:44:45,713 EPOCH 868
2024-02-08 18:44:48,477 [Epoch: 868 Step: 00029500] Batch Recognition Loss:   0.000583 => Gls Tokens per Sec:     2548 || Batch Translation Loss:   0.096668 => Txt Tokens per Sec:     7082 || Lr: 0.000100
2024-02-08 18:44:49,972 Epoch 868: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.39 
2024-02-08 18:44:49,972 EPOCH 869
2024-02-08 18:44:54,772 Epoch 869: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.43 
2024-02-08 18:44:54,773 EPOCH 870
2024-02-08 18:44:59,784 Epoch 870: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.12 
2024-02-08 18:44:59,784 EPOCH 871
2024-02-08 18:45:02,349 [Epoch: 871 Step: 00029600] Batch Recognition Loss:   0.000464 => Gls Tokens per Sec:     2395 || Batch Translation Loss:   0.081952 => Txt Tokens per Sec:     6788 || Lr: 0.000100
2024-02-08 18:45:04,003 Epoch 871: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.44 
2024-02-08 18:45:04,003 EPOCH 872
2024-02-08 18:45:08,726 Epoch 872: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.20 
2024-02-08 18:45:08,726 EPOCH 873
2024-02-08 18:45:13,365 Epoch 873: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.81 
2024-02-08 18:45:13,365 EPOCH 874
2024-02-08 18:45:15,308 [Epoch: 874 Step: 00029700] Batch Recognition Loss:   0.000498 => Gls Tokens per Sec:     2966 || Batch Translation Loss:   0.067340 => Txt Tokens per Sec:     7804 || Lr: 0.000100
2024-02-08 18:45:17,725 Epoch 874: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.72 
2024-02-08 18:45:17,725 EPOCH 875
2024-02-08 18:45:22,646 Epoch 875: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.56 
2024-02-08 18:45:22,646 EPOCH 876
2024-02-08 18:45:26,808 Epoch 876: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.85 
2024-02-08 18:45:26,809 EPOCH 877
2024-02-08 18:45:29,396 [Epoch: 877 Step: 00029800] Batch Recognition Loss:   0.000834 => Gls Tokens per Sec:     1880 || Batch Translation Loss:   0.010406 => Txt Tokens per Sec:     5153 || Lr: 0.000100
2024-02-08 18:45:31,871 Epoch 877: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.44 
2024-02-08 18:45:31,871 EPOCH 878
2024-02-08 18:45:36,106 Epoch 878: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.55 
2024-02-08 18:45:36,106 EPOCH 879
2024-02-08 18:45:40,937 Epoch 879: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.55 
2024-02-08 18:45:40,937 EPOCH 880
2024-02-08 18:45:43,082 [Epoch: 880 Step: 00029900] Batch Recognition Loss:   0.000285 => Gls Tokens per Sec:     2090 || Batch Translation Loss:   0.046949 => Txt Tokens per Sec:     6338 || Lr: 0.000100
2024-02-08 18:45:45,429 Epoch 880: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.17 
2024-02-08 18:45:45,429 EPOCH 881
2024-02-08 18:45:50,134 Epoch 881: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.68 
2024-02-08 18:45:50,134 EPOCH 882
2024-02-08 18:45:54,793 Epoch 882: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.78 
2024-02-08 18:45:54,794 EPOCH 883
2024-02-08 18:45:56,120 [Epoch: 883 Step: 00030000] Batch Recognition Loss:   0.000343 => Gls Tokens per Sec:     2896 || Batch Translation Loss:   0.055944 => Txt Tokens per Sec:     7637 || Lr: 0.000100
2024-02-08 18:46:04,910 Validation result at epoch 883, step    30000: duration: 8.7890s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.44943	Translation Loss: 93273.26562	PPL: 11116.06641
	Eval Metric: BLEU
	WER 3.46	(DEL: 0.00,	INS: 0.00,	SUB: 3.46)
	BLEU-4 0.56	(BLEU-1: 11.03,	BLEU-2: 3.42,	BLEU-3: 1.21,	BLEU-4: 0.56)
	CHRF 17.13	ROUGE 9.09
2024-02-08 18:46:04,911 Logging Recognition and Translation Outputs
2024-02-08 18:46:04,912 ========================================================================================================================
2024-02-08 18:46:04,912 Logging Sequence: 146_102.00
2024-02-08 18:46:04,912 	Gloss Reference :	A B+C+D+E
2024-02-08 18:46:04,912 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:46:04,912 	Gloss Alignment :	         
2024-02-08 18:46:04,912 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:46:04,914 	Text Reference  :	***** ****** **** famous indian      champion players like kidambi srikanth and  ashwini ponappa have  tested positive for       coronavirus
2024-02-08 18:46:04,914 	Text Hypothesis :	rohit sharma will assume captainship while    kohli   will be      free     year old     it      today on     his      religious beliefs    
2024-02-08 18:46:04,914 	Text Alignment  :	I     I      I    S      S           S        S       S    S       S        S    S       S       S     S      S        S         S          
2024-02-08 18:46:04,914 ========================================================================================================================
2024-02-08 18:46:04,915 Logging Sequence: 53_178.00
2024-02-08 18:46:04,915 	Gloss Reference :	A B+C+D+E
2024-02-08 18:46:04,915 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:46:04,915 	Gloss Alignment :	         
2024-02-08 18:46:04,915 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:46:04,917 	Text Reference  :	* ** **** the   money would help all those affected by  the humanitarian crisis in *** **** afghanistan
2024-02-08 18:46:04,917 	Text Hypothesis :	i am also asked to    tell  you  all ***** rights   for the various      groups in the next season     
2024-02-08 18:46:04,917 	Text Alignment  :	I I  I    S     S     S     S        D     S        S       S            S         I   I    S          
2024-02-08 18:46:04,917 ========================================================================================================================
2024-02-08 18:46:04,917 Logging Sequence: 129_200.00
2024-02-08 18:46:04,917 	Gloss Reference :	A B+C+D+E  
2024-02-08 18:46:04,917 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-08 18:46:04,917 	Gloss Alignment :	  S        
2024-02-08 18:46:04,918 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:46:04,919 	Text Reference  :	the ioc would lose about 4    billion if   the **** **** olympics were  to be      cancelled
2024-02-08 18:46:04,919 	Text Hypothesis :	*** he  never used a     look to      know the 2000 2008 olympic  games in javelin throw    
2024-02-08 18:46:04,919 	Text Alignment  :	D   S   S     S    S     S    S       S        I    I    S        S     S  S       S        
2024-02-08 18:46:04,919 ========================================================================================================================
2024-02-08 18:46:04,919 Logging Sequence: 77_2.00
2024-02-08 18:46:04,920 	Gloss Reference :	A B+C+D+E  
2024-02-08 18:46:04,920 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-08 18:46:04,920 	Gloss Alignment :	  S        
2024-02-08 18:46:04,920 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:46:04,921 	Text Reference  :	on 25th april the ipl match between sunrisers hyderabad and delhi       capitals ended    in a   tie
2024-02-08 18:46:04,921 	Text Hypothesis :	on **** ***** 4th may 2022  bowler  bowled    in        the semi-finals against  pakistan of his run
2024-02-08 18:46:04,922 	Text Alignment  :	   D    D     S   S   S     S       S         S         S   S           S        S        S  S   S  
2024-02-08 18:46:04,922 ========================================================================================================================
2024-02-08 18:46:04,922 Logging Sequence: 119_170.00
2024-02-08 18:46:04,922 	Gloss Reference :	A B+C+D+E
2024-02-08 18:46:04,922 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:46:04,922 	Gloss Alignment :	         
2024-02-08 18:46:04,922 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:46:04,923 	Text Reference  :	they said  it  was  a     proud moment messi is a big hearted man
2024-02-08 18:46:04,923 	Text Hypothesis :	the  girls had made india proud ****** ***** ** * *** ******* ***
2024-02-08 18:46:04,923 	Text Alignment  :	S    S     S   S    S           D      D     D  D D   D       D  
2024-02-08 18:46:04,923 ========================================================================================================================
2024-02-08 18:46:08,246 Epoch 883: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.88 
2024-02-08 18:46:08,246 EPOCH 884
2024-02-08 18:46:13,142 Epoch 884: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.24 
2024-02-08 18:46:13,143 EPOCH 885
2024-02-08 18:46:17,857 Epoch 885: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.10 
2024-02-08 18:46:17,857 EPOCH 886
2024-02-08 18:46:19,076 [Epoch: 886 Step: 00030100] Batch Recognition Loss:   0.000505 => Gls Tokens per Sec:     2416 || Batch Translation Loss:   0.038769 => Txt Tokens per Sec:     6878 || Lr: 0.000100
2024-02-08 18:46:22,187 Epoch 886: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.90 
2024-02-08 18:46:22,188 EPOCH 887
2024-02-08 18:46:27,088 Epoch 887: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.63 
2024-02-08 18:46:27,089 EPOCH 888
2024-02-08 18:46:31,169 Epoch 888: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.03 
2024-02-08 18:46:31,169 EPOCH 889
2024-02-08 18:46:31,975 [Epoch: 889 Step: 00030200] Batch Recognition Loss:   0.000279 => Gls Tokens per Sec:     3180 || Batch Translation Loss:   0.022322 => Txt Tokens per Sec:     7995 || Lr: 0.000100
2024-02-08 18:46:36,118 Epoch 889: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.78 
2024-02-08 18:46:36,118 EPOCH 890
2024-02-08 18:46:40,841 Epoch 890: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.98 
2024-02-08 18:46:40,842 EPOCH 891
2024-02-08 18:46:45,707 Epoch 891: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.38 
2024-02-08 18:46:45,708 EPOCH 892
2024-02-08 18:46:46,229 [Epoch: 892 Step: 00030300] Batch Recognition Loss:   0.000496 => Gls Tokens per Sec:     3692 || Batch Translation Loss:   0.030199 => Txt Tokens per Sec:     8829 || Lr: 0.000100
2024-02-08 18:46:50,019 Epoch 892: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.11 
2024-02-08 18:46:50,019 EPOCH 893
2024-02-08 18:46:54,567 Epoch 893: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.00 
2024-02-08 18:46:54,567 EPOCH 894
2024-02-08 18:46:59,705 Epoch 894: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.07 
2024-02-08 18:46:59,705 EPOCH 895
2024-02-08 18:47:00,200 [Epoch: 895 Step: 00030400] Batch Recognition Loss:   0.000418 => Gls Tokens per Sec:     2065 || Batch Translation Loss:   0.027245 => Txt Tokens per Sec:     5723 || Lr: 0.000100
2024-02-08 18:47:04,134 Epoch 895: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.24 
2024-02-08 18:47:04,135 EPOCH 896
2024-02-08 18:47:08,779 Epoch 896: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.07 
2024-02-08 18:47:08,780 EPOCH 897
2024-02-08 18:47:13,417 Epoch 897: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.72 
2024-02-08 18:47:13,417 EPOCH 898
2024-02-08 18:47:13,759 [Epoch: 898 Step: 00030500] Batch Recognition Loss:   0.000499 => Gls Tokens per Sec:     1877 || Batch Translation Loss:   0.037552 => Txt Tokens per Sec:     6267 || Lr: 0.000100
2024-02-08 18:47:17,796 Epoch 898: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.02 
2024-02-08 18:47:17,797 EPOCH 899
2024-02-08 18:47:22,769 Epoch 899: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.19 
2024-02-08 18:47:22,769 EPOCH 900
2024-02-08 18:47:26,938 [Epoch: 900 Step: 00030600] Batch Recognition Loss:   0.000707 => Gls Tokens per Sec:     2549 || Batch Translation Loss:   0.013590 => Txt Tokens per Sec:     7051 || Lr: 0.000100
2024-02-08 18:47:26,938 Epoch 900: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.18 
2024-02-08 18:47:26,938 EPOCH 901
2024-02-08 18:47:32,073 Epoch 901: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.84 
2024-02-08 18:47:32,074 EPOCH 902
2024-02-08 18:47:36,989 Epoch 902: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.67 
2024-02-08 18:47:36,989 EPOCH 903
2024-02-08 18:47:41,052 [Epoch: 903 Step: 00030700] Batch Recognition Loss:   0.000585 => Gls Tokens per Sec:     2457 || Batch Translation Loss:   0.049385 => Txt Tokens per Sec:     6882 || Lr: 0.000100
2024-02-08 18:47:41,283 Epoch 903: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.95 
2024-02-08 18:47:41,283 EPOCH 904
2024-02-08 18:47:46,232 Epoch 904: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.36 
2024-02-08 18:47:46,232 EPOCH 905
2024-02-08 18:47:50,403 Epoch 905: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.09 
2024-02-08 18:47:50,403 EPOCH 906
2024-02-08 18:47:54,723 [Epoch: 906 Step: 00030800] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     2162 || Batch Translation Loss:   0.023256 => Txt Tokens per Sec:     6023 || Lr: 0.000100
2024-02-08 18:47:55,276 Epoch 906: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.19 
2024-02-08 18:47:55,276 EPOCH 907
2024-02-08 18:47:59,633 Epoch 907: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.99 
2024-02-08 18:47:59,633 EPOCH 908
2024-02-08 18:48:04,356 Epoch 908: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.59 
2024-02-08 18:48:04,357 EPOCH 909
2024-02-08 18:48:08,355 [Epoch: 909 Step: 00030900] Batch Recognition Loss:   0.000419 => Gls Tokens per Sec:     2242 || Batch Translation Loss:   0.055316 => Txt Tokens per Sec:     6255 || Lr: 0.000100
2024-02-08 18:48:08,983 Epoch 909: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.17 
2024-02-08 18:48:08,983 EPOCH 910
2024-02-08 18:48:13,513 Epoch 910: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.94 
2024-02-08 18:48:13,514 EPOCH 911
2024-02-08 18:48:18,308 Epoch 911: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.42 
2024-02-08 18:48:18,308 EPOCH 912
2024-02-08 18:48:21,507 [Epoch: 912 Step: 00031000] Batch Recognition Loss:   0.000334 => Gls Tokens per Sec:     2602 || Batch Translation Loss:   0.013671 => Txt Tokens per Sec:     7319 || Lr: 0.000100
2024-02-08 18:48:22,447 Epoch 912: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.32 
2024-02-08 18:48:22,447 EPOCH 913
2024-02-08 18:48:27,528 Epoch 913: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.92 
2024-02-08 18:48:27,529 EPOCH 914
2024-02-08 18:48:31,806 Epoch 914: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.19 
2024-02-08 18:48:31,806 EPOCH 915
2024-02-08 18:48:35,282 [Epoch: 915 Step: 00031100] Batch Recognition Loss:   0.000367 => Gls Tokens per Sec:     2135 || Batch Translation Loss:   0.032703 => Txt Tokens per Sec:     5807 || Lr: 0.000100
2024-02-08 18:48:36,699 Epoch 915: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.18 
2024-02-08 18:48:36,699 EPOCH 916
2024-02-08 18:48:41,131 Epoch 916: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.43 
2024-02-08 18:48:41,131 EPOCH 917
2024-02-08 18:48:45,736 Epoch 917: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.09 
2024-02-08 18:48:45,737 EPOCH 918
2024-02-08 18:48:49,037 [Epoch: 918 Step: 00031200] Batch Recognition Loss:   0.000365 => Gls Tokens per Sec:     2134 || Batch Translation Loss:   0.031280 => Txt Tokens per Sec:     6109 || Lr: 0.000100
2024-02-08 18:48:50,488 Epoch 918: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.72 
2024-02-08 18:48:50,488 EPOCH 919
2024-02-08 18:48:54,806 Epoch 919: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.31 
2024-02-08 18:48:54,806 EPOCH 920
2024-02-08 18:48:59,725 Epoch 920: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.52 
2024-02-08 18:48:59,725 EPOCH 921
2024-02-08 18:49:02,097 [Epoch: 921 Step: 00031300] Batch Recognition Loss:   0.000427 => Gls Tokens per Sec:     2699 || Batch Translation Loss:   0.260952 => Txt Tokens per Sec:     7688 || Lr: 0.000100
2024-02-08 18:49:03,830 Epoch 921: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.91 
2024-02-08 18:49:03,830 EPOCH 922
2024-02-08 18:49:08,817 Epoch 922: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.08 
2024-02-08 18:49:08,817 EPOCH 923
2024-02-08 18:49:13,186 Epoch 923: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.90 
2024-02-08 18:49:13,186 EPOCH 924
2024-02-08 18:49:15,656 [Epoch: 924 Step: 00031400] Batch Recognition Loss:   0.001090 => Gls Tokens per Sec:     2333 || Batch Translation Loss:   0.078240 => Txt Tokens per Sec:     6542 || Lr: 0.000100
2024-02-08 18:49:17,967 Epoch 924: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.76 
2024-02-08 18:49:17,967 EPOCH 925
2024-02-08 18:49:22,490 Epoch 925: Total Training Recognition Loss 0.12  Total Training Translation Loss 17.86 
2024-02-08 18:49:22,490 EPOCH 926
2024-02-08 18:49:26,772 Epoch 926: Total Training Recognition Loss 0.10  Total Training Translation Loss 4.57 
2024-02-08 18:49:26,772 EPOCH 927
2024-02-08 18:49:29,225 [Epoch: 927 Step: 00031500] Batch Recognition Loss:   0.000433 => Gls Tokens per Sec:     1982 || Batch Translation Loss:   0.036864 => Txt Tokens per Sec:     5377 || Lr: 0.000100
2024-02-08 18:49:31,698 Epoch 927: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.70 
2024-02-08 18:49:31,699 EPOCH 928
2024-02-08 18:49:35,882 Epoch 928: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.34 
2024-02-08 18:49:35,883 EPOCH 929
2024-02-08 18:49:40,824 Epoch 929: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.95 
2024-02-08 18:49:40,824 EPOCH 930
2024-02-08 18:49:42,590 [Epoch: 930 Step: 00031600] Batch Recognition Loss:   0.002251 => Gls Tokens per Sec:     2391 || Batch Translation Loss:   0.016349 => Txt Tokens per Sec:     6179 || Lr: 0.000100
2024-02-08 18:49:45,231 Epoch 930: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.71 
2024-02-08 18:49:45,232 EPOCH 931
2024-02-08 18:49:49,334 Epoch 931: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-08 18:49:49,334 EPOCH 932
2024-02-08 18:49:53,435 Epoch 932: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.65 
2024-02-08 18:49:53,436 EPOCH 933
2024-02-08 18:49:54,815 [Epoch: 933 Step: 00031700] Batch Recognition Loss:   0.000283 => Gls Tokens per Sec:     2785 || Batch Translation Loss:   0.017583 => Txt Tokens per Sec:     7228 || Lr: 0.000100
2024-02-08 18:49:58,248 Epoch 933: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.79 
2024-02-08 18:49:58,248 EPOCH 934
2024-02-08 18:50:02,835 Epoch 934: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.97 
2024-02-08 18:50:02,836 EPOCH 935
2024-02-08 18:50:07,392 Epoch 935: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.93 
2024-02-08 18:50:07,393 EPOCH 936
2024-02-08 18:50:08,856 [Epoch: 936 Step: 00031800] Batch Recognition Loss:   0.000242 => Gls Tokens per Sec:     2189 || Batch Translation Loss:   0.200806 => Txt Tokens per Sec:     5719 || Lr: 0.000100
2024-02-08 18:50:12,115 Epoch 936: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-08 18:50:12,115 EPOCH 937
2024-02-08 18:50:16,381 Epoch 937: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-08 18:50:16,381 EPOCH 938
2024-02-08 18:50:21,349 Epoch 938: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.18 
2024-02-08 18:50:21,350 EPOCH 939
2024-02-08 18:50:22,209 [Epoch: 939 Step: 00031900] Batch Recognition Loss:   0.000221 => Gls Tokens per Sec:     2989 || Batch Translation Loss:   0.014165 => Txt Tokens per Sec:     7804 || Lr: 0.000100
2024-02-08 18:50:25,539 Epoch 939: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.71 
2024-02-08 18:50:25,539 EPOCH 940
2024-02-08 18:50:30,428 Epoch 940: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.64 
2024-02-08 18:50:30,428 EPOCH 941
2024-02-08 18:50:34,819 Epoch 941: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.57 
2024-02-08 18:50:34,819 EPOCH 942
2024-02-08 18:50:35,427 [Epoch: 942 Step: 00032000] Batch Recognition Loss:   0.000460 => Gls Tokens per Sec:     3163 || Batch Translation Loss:   0.022604 => Txt Tokens per Sec:     8269 || Lr: 0.000100
2024-02-08 18:50:44,434 Validation result at epoch 942, step    32000: duration: 9.0060s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.42855	Translation Loss: 92643.82812	PPL: 10438.73535
	Eval Metric: BLEU
	WER 3.67	(DEL: 0.00,	INS: 0.00,	SUB: 3.67)
	BLEU-4 0.62	(BLEU-1: 11.02,	BLEU-2: 3.36,	BLEU-3: 1.29,	BLEU-4: 0.62)
	CHRF 17.15	ROUGE 9.24
2024-02-08 18:50:44,435 Logging Recognition and Translation Outputs
2024-02-08 18:50:44,435 ========================================================================================================================
2024-02-08 18:50:44,436 Logging Sequence: 162_133.00
2024-02-08 18:50:44,436 	Gloss Reference :	A B+C+D+E
2024-02-08 18:50:44,436 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:50:44,436 	Gloss Alignment :	         
2024-02-08 18:50:44,436 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:50:44,437 	Text Reference  :	they also sent rape threats to his 9-month old daughter
2024-02-08 18:50:44,437 	Text Hypothesis :	**** now  we   will have    to *** wait    for updates 
2024-02-08 18:50:44,437 	Text Alignment  :	D    S    S    S    S          D   S       S   S       
2024-02-08 18:50:44,437 ========================================================================================================================
2024-02-08 18:50:44,437 Logging Sequence: 134_236.00
2024-02-08 18:50:44,438 	Gloss Reference :	A B+C+D+E
2024-02-08 18:50:44,438 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:50:44,438 	Gloss Alignment :	         
2024-02-08 18:50:44,438 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:50:44,439 	Text Reference  :	**** *** * ****** after the interaction **** ******* modi    tweeted the images and         captioned it saying
2024-02-08 18:50:44,439 	Text Hypothesis :	2022 was a report by    the interaction with india's captain and     the ****** deaflympics debut     in 1965  
2024-02-08 18:50:44,439 	Text Alignment  :	I    I   I I      S                     I    I       S       S           D      S           S         S  S     
2024-02-08 18:50:44,440 ========================================================================================================================
2024-02-08 18:50:44,440 Logging Sequence: 145_52.00
2024-02-08 18:50:44,440 	Gloss Reference :	A B+C+D+E
2024-02-08 18:50:44,440 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:50:44,440 	Gloss Alignment :	         
2024-02-08 18:50:44,440 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:50:44,441 	Text Reference  :	her name was dropped despite having qualified as      she  was the    only   female athlete
2024-02-08 18:50:44,441 	Text Hypothesis :	*** **** *** ******* ******* ****** the       matches will be  played across 4      victory
2024-02-08 18:50:44,441 	Text Alignment  :	D   D    D   D       D       D      S         S       S    S   S      S      S      S      
2024-02-08 18:50:44,441 ========================================================================================================================
2024-02-08 18:50:44,442 Logging Sequence: 175_40.00
2024-02-08 18:50:44,442 	Gloss Reference :	A B+C+D+E
2024-02-08 18:50:44,442 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:50:44,442 	Gloss Alignment :	         
2024-02-08 18:50:44,442 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:50:44,443 	Text Reference  :	soumyadeep and shreya bagged three     medals each  including a        silver medal     each   
2024-02-08 18:50:44,443 	Text Hypothesis :	********** *** on     10th   september 2023   match between   pakistan and    different stories
2024-02-08 18:50:44,443 	Text Alignment  :	D          D   S      S      S         S      S     S         S        S      S         S      
2024-02-08 18:50:44,443 ========================================================================================================================
2024-02-08 18:50:44,443 Logging Sequence: 156_51.00
2024-02-08 18:50:44,444 	Gloss Reference :	A B+C+D+E
2024-02-08 18:50:44,444 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:50:44,444 	Gloss Alignment :	         
2024-02-08 18:50:44,444 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:50:44,445 	Text Reference  :	the selection of    the players was similar  to   that of ipl
2024-02-08 18:50:44,445 	Text Hypothesis :	*** what      about the world   cup stadiums have fun  in ipl
2024-02-08 18:50:44,445 	Text Alignment  :	D   S         S         S       S   S        S    S    S     
2024-02-08 18:50:44,445 ========================================================================================================================
2024-02-08 18:50:48,823 Epoch 942: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.18 
2024-02-08 18:50:48,824 EPOCH 943
2024-02-08 18:50:53,628 Epoch 943: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.24 
2024-02-08 18:50:53,628 EPOCH 944
2024-02-08 18:50:58,057 Epoch 944: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.88 
2024-02-08 18:50:58,058 EPOCH 945
2024-02-08 18:50:58,700 [Epoch: 945 Step: 00032100] Batch Recognition Loss:   0.000494 => Gls Tokens per Sec:     1997 || Batch Translation Loss:   0.022781 => Txt Tokens per Sec:     5967 || Lr: 0.000100
2024-02-08 18:51:02,944 Epoch 945: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.77 
2024-02-08 18:51:02,944 EPOCH 946
2024-02-08 18:51:07,051 Epoch 946: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.84 
2024-02-08 18:51:07,051 EPOCH 947
2024-02-08 18:51:12,083 Epoch 947: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-08 18:51:12,084 EPOCH 948
2024-02-08 18:51:12,334 [Epoch: 948 Step: 00032200] Batch Recognition Loss:   0.000312 => Gls Tokens per Sec:     2570 || Batch Translation Loss:   0.012095 => Txt Tokens per Sec:     7538 || Lr: 0.000100
2024-02-08 18:51:16,365 Epoch 948: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.39 
2024-02-08 18:51:16,365 EPOCH 949
2024-02-08 18:51:21,209 Epoch 949: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.86 
2024-02-08 18:51:21,210 EPOCH 950
2024-02-08 18:51:25,647 [Epoch: 950 Step: 00032300] Batch Recognition Loss:   0.000335 => Gls Tokens per Sec:     2394 || Batch Translation Loss:   0.039418 => Txt Tokens per Sec:     6623 || Lr: 0.000100
2024-02-08 18:51:25,647 Epoch 950: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.15 
2024-02-08 18:51:25,648 EPOCH 951
2024-02-08 18:51:30,195 Epoch 951: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.25 
2024-02-08 18:51:30,195 EPOCH 952
2024-02-08 18:51:34,937 Epoch 952: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.03 
2024-02-08 18:51:34,937 EPOCH 953
2024-02-08 18:51:38,944 [Epoch: 953 Step: 00032400] Batch Recognition Loss:   0.009519 => Gls Tokens per Sec:     2491 || Batch Translation Loss:   0.027157 => Txt Tokens per Sec:     6898 || Lr: 0.000100
2024-02-08 18:51:39,198 Epoch 953: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.83 
2024-02-08 18:51:39,199 EPOCH 954
2024-02-08 18:51:44,196 Epoch 954: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.97 
2024-02-08 18:51:44,197 EPOCH 955
2024-02-08 18:51:48,388 Epoch 955: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.23 
2024-02-08 18:51:48,389 EPOCH 956
2024-02-08 18:51:52,496 [Epoch: 956 Step: 00032500] Batch Recognition Loss:   0.001294 => Gls Tokens per Sec:     2275 || Batch Translation Loss:   0.062034 => Txt Tokens per Sec:     6272 || Lr: 0.000100
2024-02-08 18:51:53,070 Epoch 956: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.02 
2024-02-08 18:51:53,070 EPOCH 957
2024-02-08 18:51:57,779 Epoch 957: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.37 
2024-02-08 18:51:57,779 EPOCH 958
2024-02-08 18:52:02,046 Epoch 958: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.54 
2024-02-08 18:52:02,047 EPOCH 959
2024-02-08 18:52:06,129 [Epoch: 959 Step: 00032600] Batch Recognition Loss:   0.000239 => Gls Tokens per Sec:     2133 || Batch Translation Loss:   0.179553 => Txt Tokens per Sec:     5799 || Lr: 0.000100
2024-02-08 18:52:07,049 Epoch 959: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.11 
2024-02-08 18:52:07,049 EPOCH 960
2024-02-08 18:52:11,369 Epoch 960: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.90 
2024-02-08 18:52:11,369 EPOCH 961
2024-02-08 18:52:16,385 Epoch 961: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.47 
2024-02-08 18:52:16,386 EPOCH 962
2024-02-08 18:52:19,555 [Epoch: 962 Step: 00032700] Batch Recognition Loss:   0.003520 => Gls Tokens per Sec:     2543 || Batch Translation Loss:   0.034268 => Txt Tokens per Sec:     6924 || Lr: 0.000100
2024-02-08 18:52:20,658 Epoch 962: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.79 
2024-02-08 18:52:20,658 EPOCH 963
2024-02-08 18:52:25,375 Epoch 963: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.09 
2024-02-08 18:52:25,375 EPOCH 964
2024-02-08 18:52:29,985 Epoch 964: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.12 
2024-02-08 18:52:29,985 EPOCH 965
2024-02-08 18:52:32,867 [Epoch: 965 Step: 00032800] Batch Recognition Loss:   0.000444 => Gls Tokens per Sec:     2667 || Batch Translation Loss:   0.054808 => Txt Tokens per Sec:     7307 || Lr: 0.000100
2024-02-08 18:52:34,410 Epoch 965: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.05 
2024-02-08 18:52:34,411 EPOCH 966
2024-02-08 18:52:39,291 Epoch 966: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.12 
2024-02-08 18:52:39,292 EPOCH 967
2024-02-08 18:52:43,423 Epoch 967: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.31 
2024-02-08 18:52:43,424 EPOCH 968
2024-02-08 18:52:46,902 [Epoch: 968 Step: 00032900] Batch Recognition Loss:   0.000375 => Gls Tokens per Sec:     1950 || Batch Translation Loss:   0.027422 => Txt Tokens per Sec:     5318 || Lr: 0.000100
2024-02-08 18:52:48,565 Epoch 968: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.44 
2024-02-08 18:52:48,565 EPOCH 969
2024-02-08 18:52:52,861 Epoch 969: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.93 
2024-02-08 18:52:52,861 EPOCH 970
2024-02-08 18:52:56,928 Epoch 970: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.95 
2024-02-08 18:52:56,928 EPOCH 971
2024-02-08 18:53:00,282 [Epoch: 971 Step: 00033000] Batch Recognition Loss:   0.000626 => Gls Tokens per Sec:     1909 || Batch Translation Loss:   0.019467 => Txt Tokens per Sec:     5509 || Lr: 0.000100
2024-02-08 18:53:02,085 Epoch 971: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.15 
2024-02-08 18:53:02,086 EPOCH 972
2024-02-08 18:53:07,042 Epoch 972: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.97 
2024-02-08 18:53:07,042 EPOCH 973
2024-02-08 18:53:11,178 Epoch 973: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.15 
2024-02-08 18:53:11,178 EPOCH 974
2024-02-08 18:53:13,459 [Epoch: 974 Step: 00033100] Batch Recognition Loss:   0.000248 => Gls Tokens per Sec:     2412 || Batch Translation Loss:   0.024691 => Txt Tokens per Sec:     6842 || Lr: 0.000100
2024-02-08 18:53:15,960 Epoch 974: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.35 
2024-02-08 18:53:15,960 EPOCH 975
2024-02-08 18:53:20,508 Epoch 975: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.92 
2024-02-08 18:53:20,508 EPOCH 976
2024-02-08 18:53:25,018 Epoch 976: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.40 
2024-02-08 18:53:25,019 EPOCH 977
2024-02-08 18:53:27,392 [Epoch: 977 Step: 00033200] Batch Recognition Loss:   0.000583 => Gls Tokens per Sec:     2159 || Batch Translation Loss:   0.038028 => Txt Tokens per Sec:     5774 || Lr: 0.000100
2024-02-08 18:53:30,103 Epoch 977: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.56 
2024-02-08 18:53:30,103 EPOCH 978
2024-02-08 18:53:34,493 Epoch 978: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.47 
2024-02-08 18:53:34,493 EPOCH 979
2024-02-08 18:53:39,318 Epoch 979: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.58 
2024-02-08 18:53:39,319 EPOCH 980
2024-02-08 18:53:40,968 [Epoch: 980 Step: 00033300] Batch Recognition Loss:   0.000152 => Gls Tokens per Sec:     2561 || Batch Translation Loss:   0.005454 => Txt Tokens per Sec:     6883 || Lr: 0.000100
2024-02-08 18:53:43,811 Epoch 980: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.92 
2024-02-08 18:53:43,811 EPOCH 981
2024-02-08 18:53:48,809 Epoch 981: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.61 
2024-02-08 18:53:48,810 EPOCH 982
2024-02-08 18:53:53,632 Epoch 982: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.50 
2024-02-08 18:53:53,632 EPOCH 983
2024-02-08 18:53:55,120 [Epoch: 983 Step: 00033400] Batch Recognition Loss:   0.000348 => Gls Tokens per Sec:     2584 || Batch Translation Loss:   0.030609 => Txt Tokens per Sec:     7061 || Lr: 0.000100
2024-02-08 18:53:58,363 Epoch 983: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.03 
2024-02-08 18:53:58,364 EPOCH 984
2024-02-08 18:54:03,284 Epoch 984: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.41 
2024-02-08 18:54:03,285 EPOCH 985
2024-02-08 18:54:08,312 Epoch 985: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.32 
2024-02-08 18:54:08,313 EPOCH 986
2024-02-08 18:54:09,408 [Epoch: 986 Step: 00033500] Batch Recognition Loss:   0.000336 => Gls Tokens per Sec:     2687 || Batch Translation Loss:   0.210835 => Txt Tokens per Sec:     7000 || Lr: 0.000100
2024-02-08 18:54:13,016 Epoch 986: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.23 
2024-02-08 18:54:13,016 EPOCH 987
2024-02-08 18:54:17,935 Epoch 987: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.29 
2024-02-08 18:54:17,935 EPOCH 988
2024-02-08 18:54:22,806 Epoch 988: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.85 
2024-02-08 18:54:22,807 EPOCH 989
2024-02-08 18:54:23,788 [Epoch: 989 Step: 00033600] Batch Recognition Loss:   0.000919 => Gls Tokens per Sec:     2610 || Batch Translation Loss:   0.054283 => Txt Tokens per Sec:     6660 || Lr: 0.000100
2024-02-08 18:54:27,498 Epoch 989: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.30 
2024-02-08 18:54:27,499 EPOCH 990
2024-02-08 18:54:32,319 Epoch 990: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.55 
2024-02-08 18:54:32,320 EPOCH 991
2024-02-08 18:54:36,998 Epoch 991: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.58 
2024-02-08 18:54:36,998 EPOCH 992
2024-02-08 18:54:37,997 [Epoch: 992 Step: 00033700] Batch Recognition Loss:   0.001395 => Gls Tokens per Sec:     1926 || Batch Translation Loss:   0.089735 => Txt Tokens per Sec:     5735 || Lr: 0.000100
2024-02-08 18:54:41,833 Epoch 992: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.64 
2024-02-08 18:54:41,834 EPOCH 993
2024-02-08 18:54:46,358 Epoch 993: Total Training Recognition Loss 0.18  Total Training Translation Loss 2.27 
2024-02-08 18:54:46,358 EPOCH 994
2024-02-08 18:54:51,203 Epoch 994: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.45 
2024-02-08 18:54:51,204 EPOCH 995
2024-02-08 18:54:51,651 [Epoch: 995 Step: 00033800] Batch Recognition Loss:   0.000342 => Gls Tokens per Sec:     2864 || Batch Translation Loss:   0.015166 => Txt Tokens per Sec:     7995 || Lr: 0.000100
2024-02-08 18:54:55,955 Epoch 995: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.17 
2024-02-08 18:54:55,955 EPOCH 996
2024-02-08 18:55:00,893 Epoch 996: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.93 
2024-02-08 18:55:00,894 EPOCH 997
2024-02-08 18:55:05,549 Epoch 997: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.89 
2024-02-08 18:55:05,550 EPOCH 998
2024-02-08 18:55:05,817 [Epoch: 998 Step: 00033900] Batch Recognition Loss:   0.000665 => Gls Tokens per Sec:     2406 || Batch Translation Loss:   0.053712 => Txt Tokens per Sec:     7455 || Lr: 0.000100
2024-02-08 18:55:10,553 Epoch 998: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.69 
2024-02-08 18:55:10,554 EPOCH 999
2024-02-08 18:55:15,357 Epoch 999: Total Training Recognition Loss 0.05  Total Training Translation Loss 8.89 
2024-02-08 18:55:15,358 EPOCH 1000
2024-02-08 18:55:20,164 [Epoch: 1000 Step: 00034000] Batch Recognition Loss:   0.000927 => Gls Tokens per Sec:     2210 || Batch Translation Loss:   0.073218 => Txt Tokens per Sec:     6115 || Lr: 0.000100
2024-02-08 18:55:29,212 Validation result at epoch 1000, step    34000: duration: 9.0482s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.33577	Translation Loss: 90786.55469	PPL: 8671.29785
	Eval Metric: BLEU
	WER 3.60	(DEL: 0.00,	INS: 0.00,	SUB: 3.60)
	BLEU-4 0.31	(BLEU-1: 10.00,	BLEU-2: 2.95,	BLEU-3: 0.87,	BLEU-4: 0.31)
	CHRF 17.00	ROUGE 8.23
2024-02-08 18:55:29,213 Logging Recognition and Translation Outputs
2024-02-08 18:55:29,213 ========================================================================================================================
2024-02-08 18:55:29,213 Logging Sequence: 171_158.00
2024-02-08 18:55:29,214 	Gloss Reference :	A B+C+D+E
2024-02-08 18:55:29,214 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:55:29,214 	Gloss Alignment :	         
2024-02-08 18:55:29,214 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:55:29,215 	Text Reference  :	with speculations of dhoni being banned        are     spreading many    say that it       is unlikely to happen   
2024-02-08 18:55:29,215 	Text Hypothesis :	**** this         is why   the   international cricket team      members of  the  comments be allowed  to different
2024-02-08 18:55:29,216 	Text Alignment  :	D    S            S  S     S     S             S       S         S       S   S    S        S  S           S        
2024-02-08 18:55:29,216 ========================================================================================================================
2024-02-08 18:55:29,216 Logging Sequence: 108_235.00
2024-02-08 18:55:29,216 	Gloss Reference :	A B+C+D+E
2024-02-08 18:55:29,216 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:55:29,216 	Gloss Alignment :	         
2024-02-08 18:55:29,216 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:55:29,217 	Text Reference  :	he was taken to the hospital and   it  was     reported that he is  not  in    any danger
2024-02-08 18:55:29,218 	Text Hypothesis :	** *** ***** ** *** ******** those who violate saying   that ** the rule would be  jailed
2024-02-08 18:55:29,218 	Text Alignment  :	D  D   D     D  D   D        S     S   S       S             D  S   S    S     S   S     
2024-02-08 18:55:29,218 ========================================================================================================================
2024-02-08 18:55:29,218 Logging Sequence: 153_206.00
2024-02-08 18:55:29,218 	Gloss Reference :	A B+C+D+E
2024-02-08 18:55:29,218 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:55:29,218 	Gloss Alignment :	         
2024-02-08 18:55:29,219 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:55:29,220 	Text Reference  :	*** ****** **** ******** ** ** **** ** *** now  on  13th november everyone is    hoping  pakistan rewrites history
2024-02-08 18:55:29,220 	Text Hypothesis :	the couple were supposed to be held in the 2020 but they lost     the      match between the      same     time   
2024-02-08 18:55:29,220 	Text Alignment  :	I   I      I    I        I  I  I    I  I   S    S   S    S        S        S     S       S        S        S      
2024-02-08 18:55:29,220 ========================================================================================================================
2024-02-08 18:55:29,220 Logging Sequence: 87_202.00
2024-02-08 18:55:29,220 	Gloss Reference :	A B+C+D+E
2024-02-08 18:55:29,220 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:55:29,221 	Gloss Alignment :	         
2024-02-08 18:55:29,221 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:55:29,221 	Text Reference  :	i ** love our players and     i  love  my country
2024-02-08 18:55:29,222 	Text Hypothesis :	i am sure you all     because of dhoni or kohli  
2024-02-08 18:55:29,222 	Text Alignment  :	  I  S    S   S       S       S  S     S  S      
2024-02-08 18:55:29,222 ========================================================================================================================
2024-02-08 18:55:29,222 Logging Sequence: 84_2.00
2024-02-08 18:55:29,222 	Gloss Reference :	A B+C+D+E
2024-02-08 18:55:29,222 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 18:55:29,222 	Gloss Alignment :	         
2024-02-08 18:55:29,223 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 18:55:29,224 	Text Reference  :	*** ** the  2022 fifa  football world cup ****** is   going on in qatar from 20th november 2022 to     18th  december 2022 
2024-02-08 18:55:29,224 	Text Hypothesis :	let me tell you  about the      world cup trophy then goes  on ** ***** **** **** ******** **** social media that     india
2024-02-08 18:55:29,225 	Text Alignment  :	I   I  S    S    S     S                  I      S    S        D  D     D    D    D        D    S      S     S        S    
2024-02-08 18:55:29,225 ========================================================================================================================
2024-02-08 18:55:29,229 Epoch 1000: Total Training Recognition Loss 0.07  Total Training Translation Loss 6.30 
2024-02-08 18:55:29,229 EPOCH 1001
2024-02-08 18:55:34,293 Epoch 1001: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.76 
2024-02-08 18:55:34,294 EPOCH 1002
2024-02-08 18:55:38,889 Epoch 1002: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.04 
2024-02-08 18:55:38,890 EPOCH 1003
2024-02-08 18:55:43,510 [Epoch: 1003 Step: 00034100] Batch Recognition Loss:   0.001306 => Gls Tokens per Sec:     2161 || Batch Translation Loss:   0.121296 => Txt Tokens per Sec:     5957 || Lr: 0.000100
2024-02-08 18:55:43,768 Epoch 1003: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.13 
2024-02-08 18:55:43,768 EPOCH 1004
2024-02-08 18:55:48,432 Epoch 1004: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.11 
2024-02-08 18:55:48,433 EPOCH 1005
2024-02-08 18:55:53,490 Epoch 1005: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.61 
2024-02-08 18:55:53,491 EPOCH 1006
2024-02-08 18:55:57,601 [Epoch: 1006 Step: 00034200] Batch Recognition Loss:   0.001303 => Gls Tokens per Sec:     2273 || Batch Translation Loss:   0.036747 => Txt Tokens per Sec:     6199 || Lr: 0.000100
2024-02-08 18:55:58,504 Epoch 1006: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.15 
2024-02-08 18:55:58,505 EPOCH 1007
2024-02-08 18:56:03,324 Epoch 1007: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.13 
2024-02-08 18:56:03,324 EPOCH 1008
2024-02-08 18:56:08,031 Epoch 1008: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.14 
2024-02-08 18:56:08,032 EPOCH 1009
2024-02-08 18:56:12,132 [Epoch: 1009 Step: 00034300] Batch Recognition Loss:   0.001475 => Gls Tokens per Sec:     2186 || Batch Translation Loss:   0.013075 => Txt Tokens per Sec:     6138 || Lr: 0.000100
2024-02-08 18:56:12,862 Epoch 1009: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-08 18:56:12,862 EPOCH 1010
2024-02-08 18:56:17,161 Epoch 1010: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.60 
2024-02-08 18:56:17,161 EPOCH 1011
2024-02-08 18:56:21,861 Epoch 1011: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-08 18:56:21,862 EPOCH 1012
2024-02-08 18:56:25,437 [Epoch: 1012 Step: 00034400] Batch Recognition Loss:   0.000388 => Gls Tokens per Sec:     2329 || Batch Translation Loss:   0.019528 => Txt Tokens per Sec:     6341 || Lr: 0.000100
2024-02-08 18:56:26,478 Epoch 1012: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.52 
2024-02-08 18:56:26,478 EPOCH 1013
2024-02-08 18:56:30,877 Epoch 1013: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.49 
2024-02-08 18:56:30,878 EPOCH 1014
2024-02-08 18:56:35,767 Epoch 1014: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.44 
2024-02-08 18:56:35,767 EPOCH 1015
2024-02-08 18:56:38,685 [Epoch: 1015 Step: 00034500] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     2633 || Batch Translation Loss:   0.019393 => Txt Tokens per Sec:     7221 || Lr: 0.000100
2024-02-08 18:56:39,903 Epoch 1015: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.49 
2024-02-08 18:56:39,903 EPOCH 1016
2024-02-08 18:56:44,949 Epoch 1016: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.52 
2024-02-08 18:56:44,950 EPOCH 1017
2024-02-08 18:56:49,236 Epoch 1017: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.55 
2024-02-08 18:56:49,236 EPOCH 1018
2024-02-08 18:56:52,267 [Epoch: 1018 Step: 00034600] Batch Recognition Loss:   0.000535 => Gls Tokens per Sec:     2238 || Batch Translation Loss:   0.015270 => Txt Tokens per Sec:     5950 || Lr: 0.000100
2024-02-08 18:56:54,082 Epoch 1018: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.47 
2024-02-08 18:56:54,082 EPOCH 1019
2024-02-08 18:56:58,586 Epoch 1019: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.48 
2024-02-08 18:56:58,587 EPOCH 1020
2024-02-08 18:57:03,319 Epoch 1020: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-08 18:57:03,319 EPOCH 1021
2024-02-08 18:57:06,030 [Epoch: 1021 Step: 00034700] Batch Recognition Loss:   0.012543 => Gls Tokens per Sec:     2362 || Batch Translation Loss:   0.017677 => Txt Tokens per Sec:     6371 || Lr: 0.000100
2024-02-08 18:57:07,968 Epoch 1021: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.45 
2024-02-08 18:57:07,968 EPOCH 1022
2024-02-08 18:57:12,078 Epoch 1022: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-08 18:57:12,079 EPOCH 1023
2024-02-08 18:57:17,067 Epoch 1023: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-08 18:57:17,068 EPOCH 1024
2024-02-08 18:57:19,863 [Epoch: 1024 Step: 00034800] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     2062 || Batch Translation Loss:   0.010136 => Txt Tokens per Sec:     6046 || Lr: 0.000100
2024-02-08 18:57:21,438 Epoch 1024: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-08 18:57:21,439 EPOCH 1025
2024-02-08 18:57:26,108 Epoch 1025: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.87 
2024-02-08 18:57:26,108 EPOCH 1026
2024-02-08 18:57:30,845 Epoch 1026: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.20 
2024-02-08 18:57:30,846 EPOCH 1027
2024-02-08 18:57:32,491 [Epoch: 1027 Step: 00034900] Batch Recognition Loss:   0.000759 => Gls Tokens per Sec:     3115 || Batch Translation Loss:   0.027349 => Txt Tokens per Sec:     8180 || Lr: 0.000100
2024-02-08 18:57:35,076 Epoch 1027: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.70 
2024-02-08 18:57:35,076 EPOCH 1028
2024-02-08 18:57:40,275 Epoch 1028: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.49 
2024-02-08 18:57:40,276 EPOCH 1029
2024-02-08 18:57:44,811 Epoch 1029: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.38 
2024-02-08 18:57:44,811 EPOCH 1030
2024-02-08 18:57:46,676 [Epoch: 1030 Step: 00035000] Batch Recognition Loss:   0.000963 => Gls Tokens per Sec:     2403 || Batch Translation Loss:   0.028873 => Txt Tokens per Sec:     6982 || Lr: 0.000100
2024-02-08 18:57:49,294 Epoch 1030: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.08 
2024-02-08 18:57:49,295 EPOCH 1031
2024-02-08 18:57:54,168 Epoch 1031: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.07 
2024-02-08 18:57:54,168 EPOCH 1032
2024-02-08 18:57:58,264 Epoch 1032: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.26 
2024-02-08 18:57:58,264 EPOCH 1033
2024-02-08 18:58:00,018 [Epoch: 1033 Step: 00035100] Batch Recognition Loss:   0.000281 => Gls Tokens per Sec:     2043 || Batch Translation Loss:   0.036483 => Txt Tokens per Sec:     5869 || Lr: 0.000100
2024-02-08 18:58:03,239 Epoch 1033: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.01 
2024-02-08 18:58:03,239 EPOCH 1034
2024-02-08 18:58:07,586 Epoch 1034: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.59 
2024-02-08 18:58:07,586 EPOCH 1035
2024-02-08 18:58:12,491 Epoch 1035: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.68 
2024-02-08 18:58:12,491 EPOCH 1036
2024-02-08 18:58:13,636 [Epoch: 1036 Step: 00035200] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:     2800 || Batch Translation Loss:   0.024638 => Txt Tokens per Sec:     7501 || Lr: 0.000100
2024-02-08 18:58:17,007 Epoch 1036: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.57 
2024-02-08 18:58:17,007 EPOCH 1037
2024-02-08 18:58:21,417 Epoch 1037: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.40 
2024-02-08 18:58:21,418 EPOCH 1038
2024-02-08 18:58:26,635 Epoch 1038: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.61 
2024-02-08 18:58:26,636 EPOCH 1039
2024-02-08 18:58:27,570 [Epoch: 1039 Step: 00035300] Batch Recognition Loss:   0.000870 => Gls Tokens per Sec:     2741 || Batch Translation Loss:   0.056469 => Txt Tokens per Sec:     7634 || Lr: 0.000100
2024-02-08 18:58:31,191 Epoch 1039: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.96 
2024-02-08 18:58:31,191 EPOCH 1040
2024-02-08 18:58:35,729 Epoch 1040: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.20 
2024-02-08 18:58:35,730 EPOCH 1041
2024-02-08 18:58:40,478 Epoch 1041: Total Training Recognition Loss 0.08  Total Training Translation Loss 5.39 
2024-02-08 18:58:40,479 EPOCH 1042
2024-02-08 18:58:41,184 [Epoch: 1042 Step: 00035400] Batch Recognition Loss:   0.005054 => Gls Tokens per Sec:     2723 || Batch Translation Loss:   0.145093 => Txt Tokens per Sec:     7772 || Lr: 0.000100
2024-02-08 18:58:44,717 Epoch 1042: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.65 
2024-02-08 18:58:44,717 EPOCH 1043
2024-02-08 18:58:49,777 Epoch 1043: Total Training Recognition Loss 0.10  Total Training Translation Loss 8.54 
2024-02-08 18:58:49,778 EPOCH 1044
2024-02-08 18:58:54,011 Epoch 1044: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.55 
2024-02-08 18:58:54,011 EPOCH 1045
2024-02-08 18:58:54,449 [Epoch: 1045 Step: 00035500] Batch Recognition Loss:   0.000788 => Gls Tokens per Sec:     2933 || Batch Translation Loss:   0.032298 => Txt Tokens per Sec:     8216 || Lr: 0.000100
2024-02-08 18:58:58,703 Epoch 1045: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.29 
2024-02-08 18:58:58,703 EPOCH 1046
2024-02-08 18:59:03,296 Epoch 1046: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.85 
2024-02-08 18:59:03,296 EPOCH 1047
2024-02-08 18:59:07,708 Epoch 1047: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.78 
2024-02-08 18:59:07,709 EPOCH 1048
2024-02-08 18:59:08,404 [Epoch: 1048 Step: 00035600] Batch Recognition Loss:   0.001868 => Gls Tokens per Sec:      922 || Batch Translation Loss:   0.021286 => Txt Tokens per Sec:     3202 || Lr: 0.000100
2024-02-08 18:59:12,591 Epoch 1048: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.73 
2024-02-08 18:59:12,591 EPOCH 1049
2024-02-08 18:59:16,694 Epoch 1049: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.79 
2024-02-08 18:59:16,694 EPOCH 1050
2024-02-08 18:59:21,865 [Epoch: 1050 Step: 00035700] Batch Recognition Loss:   0.000554 => Gls Tokens per Sec:     2054 || Batch Translation Loss:   0.018198 => Txt Tokens per Sec:     5683 || Lr: 0.000100
2024-02-08 18:59:21,866 Epoch 1050: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.80 
2024-02-08 18:59:21,866 EPOCH 1051
2024-02-08 18:59:26,546 Epoch 1051: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.94 
2024-02-08 18:59:26,546 EPOCH 1052
2024-02-08 18:59:30,658 Epoch 1052: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.89 
2024-02-08 18:59:30,658 EPOCH 1053
2024-02-08 18:59:34,485 [Epoch: 1053 Step: 00035800] Batch Recognition Loss:   0.003487 => Gls Tokens per Sec:     2608 || Batch Translation Loss:   0.018841 => Txt Tokens per Sec:     7158 || Lr: 0.000100
2024-02-08 18:59:34,801 Epoch 1053: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.68 
2024-02-08 18:59:34,801 EPOCH 1054
2024-02-08 18:59:38,989 Epoch 1054: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.71 
2024-02-08 18:59:38,990 EPOCH 1055
2024-02-08 18:59:43,999 Epoch 1055: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.87 
2024-02-08 18:59:44,000 EPOCH 1056
2024-02-08 18:59:47,351 [Epoch: 1056 Step: 00035900] Batch Recognition Loss:   0.000310 => Gls Tokens per Sec:     2788 || Batch Translation Loss:   0.031904 => Txt Tokens per Sec:     7521 || Lr: 0.000100
2024-02-08 18:59:48,271 Epoch 1056: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.32 
2024-02-08 18:59:48,271 EPOCH 1057
2024-02-08 18:59:53,355 Epoch 1057: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.71 
2024-02-08 18:59:53,356 EPOCH 1058
2024-02-08 18:59:57,624 Epoch 1058: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.08 
2024-02-08 18:59:57,624 EPOCH 1059
2024-02-08 19:00:01,546 [Epoch: 1059 Step: 00036000] Batch Recognition Loss:   0.000946 => Gls Tokens per Sec:     2286 || Batch Translation Loss:   0.057825 => Txt Tokens per Sec:     6184 || Lr: 0.000100
2024-02-08 19:00:10,260 Validation result at epoch 1059, step    36000: duration: 8.7140s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.40635	Translation Loss: 94208.26562	PPL: 12204.18750
	Eval Metric: BLEU
	WER 3.53	(DEL: 0.00,	INS: 0.00,	SUB: 3.53)
	BLEU-4 0.72	(BLEU-1: 11.03,	BLEU-2: 3.42,	BLEU-3: 1.39,	BLEU-4: 0.72)
	CHRF 17.19	ROUGE 9.31
2024-02-08 19:00:10,261 Logging Recognition and Translation Outputs
2024-02-08 19:00:10,261 ========================================================================================================================
2024-02-08 19:00:10,261 Logging Sequence: 153_36.00
2024-02-08 19:00:10,262 	Gloss Reference :	A B+C+D+E
2024-02-08 19:00:10,262 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:00:10,262 	Gloss Alignment :	         
2024-02-08 19:00:10,262 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:00:10,263 	Text Reference  :	india made a  good score   of  1686 in **** 20    overs
2024-02-08 19:00:10,263 	Text Hypothesis :	***** **** at the  stadium was held in 1992 world cup  
2024-02-08 19:00:10,263 	Text Alignment  :	D     D    S  S    S       S   S       I    S     S    
2024-02-08 19:00:10,263 ========================================================================================================================
2024-02-08 19:00:10,263 Logging Sequence: 163_30.00
2024-02-08 19:00:10,263 	Gloss Reference :	A B+C+D+E
2024-02-08 19:00:10,263 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:00:10,264 	Gloss Alignment :	         
2024-02-08 19:00:10,264 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:00:10,265 	Text Reference  :	they ***** *** ******* **** ****** never permitted anyone to  reveal her    face    
2024-02-08 19:00:10,265 	Text Hypothesis :	they could not believe that sushil kumar who       is     the most   famous wrestler
2024-02-08 19:00:10,265 	Text Alignment  :	     I     I   I       I    I      S     S         S      S   S      S      S       
2024-02-08 19:00:10,265 ========================================================================================================================
2024-02-08 19:00:10,265 Logging Sequence: 167_60.00
2024-02-08 19:00:10,265 	Gloss Reference :	A B+C+D+E
2024-02-08 19:00:10,266 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:00:10,266 	Gloss Alignment :	         
2024-02-08 19:00:10,266 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:00:10,267 	Text Reference  :	camel flu spreads rapidly when one  comes in      close contact with      the infected
2024-02-08 19:00:10,267 	Text Hypothesis :	***** *** the     stadium was  held on    twitter and   prevent infection hiv etc     
2024-02-08 19:00:10,267 	Text Alignment  :	D     D   S       S       S    S    S     S       S     S       S         S   S       
2024-02-08 19:00:10,267 ========================================================================================================================
2024-02-08 19:00:10,267 Logging Sequence: 84_35.00
2024-02-08 19:00:10,268 	Gloss Reference :	A B+C+D+E
2024-02-08 19:00:10,268 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:00:10,268 	Gloss Alignment :	         
2024-02-08 19:00:10,268 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:00:10,269 	Text Reference  :	*** **** *** here is     the reason why  they covered their mouth  
2024-02-08 19:00:10,269 	Text Hypothesis :	and they had to   second we  will   have to   wait    for   updates
2024-02-08 19:00:10,269 	Text Alignment  :	I   I    I   S    S      S   S      S    S    S       S     S      
2024-02-08 19:00:10,269 ========================================================================================================================
2024-02-08 19:00:10,269 Logging Sequence: 96_2.00
2024-02-08 19:00:10,270 	Gloss Reference :	A B+C+D+E
2024-02-08 19:00:10,270 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:00:10,270 	Gloss Alignment :	         
2024-02-08 19:00:10,270 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:00:10,271 	Text Reference  :	the world is preparing for the      t20     world cup scheduled to start from 16th  october this year
2024-02-08 19:00:10,271 	Text Hypothesis :	the ***** ** ********* icc under-19 cricket world cup ********* ** ***** was  first played  in   1988
2024-02-08 19:00:10,271 	Text Alignment  :	    D     D  D         S   S        S                 D         D  D     S    S     S       S    S   
2024-02-08 19:00:10,271 ========================================================================================================================
2024-02-08 19:00:11,292 Epoch 1059: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.55 
2024-02-08 19:00:11,293 EPOCH 1060
2024-02-08 19:00:15,968 Epoch 1060: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.38 
2024-02-08 19:00:15,968 EPOCH 1061
2024-02-08 19:00:20,430 Epoch 1061: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.73 
2024-02-08 19:00:20,430 EPOCH 1062
2024-02-08 19:00:24,114 [Epoch: 1062 Step: 00036100] Batch Recognition Loss:   0.000982 => Gls Tokens per Sec:     2188 || Batch Translation Loss:   0.006786 => Txt Tokens per Sec:     6057 || Lr: 0.000100
2024-02-08 19:00:25,291 Epoch 1062: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.24 
2024-02-08 19:00:25,291 EPOCH 1063
2024-02-08 19:00:29,476 Epoch 1063: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.48 
2024-02-08 19:00:29,477 EPOCH 1064
2024-02-08 19:00:34,514 Epoch 1064: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.77 
2024-02-08 19:00:34,514 EPOCH 1065
2024-02-08 19:00:37,720 [Epoch: 1065 Step: 00036200] Batch Recognition Loss:   0.000374 => Gls Tokens per Sec:     2315 || Batch Translation Loss:   0.161353 => Txt Tokens per Sec:     6521 || Lr: 0.000100
2024-02-08 19:00:38,755 Epoch 1065: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.80 
2024-02-08 19:00:38,755 EPOCH 1066
2024-02-08 19:00:43,392 Epoch 1066: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.18 
2024-02-08 19:00:43,392 EPOCH 1067
2024-02-08 19:00:48,132 Epoch 1067: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.15 
2024-02-08 19:00:48,132 EPOCH 1068
2024-02-08 19:00:50,797 [Epoch: 1068 Step: 00036300] Batch Recognition Loss:   0.000558 => Gls Tokens per Sec:     2642 || Batch Translation Loss:   0.105064 => Txt Tokens per Sec:     7389 || Lr: 0.000100
2024-02-08 19:00:52,603 Epoch 1068: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.72 
2024-02-08 19:00:52,604 EPOCH 1069
2024-02-08 19:00:57,484 Epoch 1069: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.26 
2024-02-08 19:00:57,485 EPOCH 1070
2024-02-08 19:01:01,611 Epoch 1070: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.40 
2024-02-08 19:01:01,611 EPOCH 1071
2024-02-08 19:01:04,227 [Epoch: 1071 Step: 00036400] Batch Recognition Loss:   0.000445 => Gls Tokens per Sec:     2348 || Batch Translation Loss:   0.036436 => Txt Tokens per Sec:     6625 || Lr: 0.000100
2024-02-08 19:01:05,732 Epoch 1071: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.58 
2024-02-08 19:01:05,733 EPOCH 1072
2024-02-08 19:01:10,758 Epoch 1072: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.15 
2024-02-08 19:01:10,758 EPOCH 1073
2024-02-08 19:01:15,152 Epoch 1073: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.78 
2024-02-08 19:01:15,153 EPOCH 1074
2024-02-08 19:01:17,147 [Epoch: 1074 Step: 00036500] Batch Recognition Loss:   0.001697 => Gls Tokens per Sec:     2891 || Batch Translation Loss:   0.021468 => Txt Tokens per Sec:     7739 || Lr: 0.000100
2024-02-08 19:01:19,710 Epoch 1074: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.84 
2024-02-08 19:01:19,711 EPOCH 1075
2024-02-08 19:01:24,445 Epoch 1075: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.80 
2024-02-08 19:01:24,445 EPOCH 1076
2024-02-08 19:01:28,708 Epoch 1076: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-08 19:01:28,708 EPOCH 1077
2024-02-08 19:01:30,999 [Epoch: 1077 Step: 00036600] Batch Recognition Loss:   0.000175 => Gls Tokens per Sec:     2237 || Batch Translation Loss:   0.029475 => Txt Tokens per Sec:     6236 || Lr: 0.000100
2024-02-08 19:01:33,690 Epoch 1077: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-08 19:01:33,690 EPOCH 1078
2024-02-08 19:01:37,910 Epoch 1078: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-08 19:01:37,910 EPOCH 1079
2024-02-08 19:01:42,865 Epoch 1079: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-08 19:01:42,865 EPOCH 1080
2024-02-08 19:01:44,326 [Epoch: 1080 Step: 00036700] Batch Recognition Loss:   0.001716 => Gls Tokens per Sec:     3068 || Batch Translation Loss:   0.005624 => Txt Tokens per Sec:     8111 || Lr: 0.000100
2024-02-08 19:01:47,229 Epoch 1080: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-08 19:01:47,229 EPOCH 1081
2024-02-08 19:01:51,919 Epoch 1081: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.13 
2024-02-08 19:01:51,920 EPOCH 1082
2024-02-08 19:01:56,491 Epoch 1082: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.41 
2024-02-08 19:01:56,492 EPOCH 1083
2024-02-08 19:01:57,992 [Epoch: 1083 Step: 00036800] Batch Recognition Loss:   0.000649 => Gls Tokens per Sec:     2387 || Batch Translation Loss:   0.027352 => Txt Tokens per Sec:     6887 || Lr: 0.000100
2024-02-08 19:02:00,927 Epoch 1083: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.96 
2024-02-08 19:02:00,927 EPOCH 1084
2024-02-08 19:02:05,822 Epoch 1084: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.07 
2024-02-08 19:02:05,822 EPOCH 1085
2024-02-08 19:02:09,928 Epoch 1085: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.92 
2024-02-08 19:02:09,928 EPOCH 1086
2024-02-08 19:02:11,227 [Epoch: 1086 Step: 00036900] Batch Recognition Loss:   0.000364 => Gls Tokens per Sec:     2465 || Batch Translation Loss:   0.024508 => Txt Tokens per Sec:     6346 || Lr: 0.000100
2024-02-08 19:02:14,972 Epoch 1086: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.36 
2024-02-08 19:02:14,973 EPOCH 1087
2024-02-08 19:02:19,288 Epoch 1087: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.21 
2024-02-08 19:02:19,289 EPOCH 1088
2024-02-08 19:02:24,124 Epoch 1088: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.06 
2024-02-08 19:02:24,125 EPOCH 1089
2024-02-08 19:02:25,117 [Epoch: 1089 Step: 00037000] Batch Recognition Loss:   0.000319 => Gls Tokens per Sec:     2321 || Batch Translation Loss:   0.005906 => Txt Tokens per Sec:     6606 || Lr: 0.000100
2024-02-08 19:02:28,559 Epoch 1089: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.71 
2024-02-08 19:02:28,559 EPOCH 1090
2024-02-08 19:02:33,168 Epoch 1090: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.93 
2024-02-08 19:02:33,169 EPOCH 1091
2024-02-08 19:02:37,927 Epoch 1091: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.11 
2024-02-08 19:02:37,928 EPOCH 1092
2024-02-08 19:02:38,703 [Epoch: 1092 Step: 00037100] Batch Recognition Loss:   0.000344 => Gls Tokens per Sec:     2477 || Batch Translation Loss:   0.076983 => Txt Tokens per Sec:     7152 || Lr: 0.000100
2024-02-08 19:02:42,192 Epoch 1092: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.55 
2024-02-08 19:02:42,193 EPOCH 1093
2024-02-08 19:02:47,171 Epoch 1093: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.41 
2024-02-08 19:02:47,171 EPOCH 1094
2024-02-08 19:02:51,371 Epoch 1094: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.24 
2024-02-08 19:02:51,371 EPOCH 1095
2024-02-08 19:02:51,933 [Epoch: 1095 Step: 00037200] Batch Recognition Loss:   0.002380 => Gls Tokens per Sec:     2282 || Batch Translation Loss:   0.058528 => Txt Tokens per Sec:     6783 || Lr: 0.000100
2024-02-08 19:02:56,298 Epoch 1095: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.50 
2024-02-08 19:02:56,299 EPOCH 1096
2024-02-08 19:03:00,702 Epoch 1096: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.72 
2024-02-08 19:03:00,703 EPOCH 1097
2024-02-08 19:03:05,506 Epoch 1097: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.68 
2024-02-08 19:03:05,507 EPOCH 1098
2024-02-08 19:03:05,794 [Epoch: 1098 Step: 00037300] Batch Recognition Loss:   0.003260 => Gls Tokens per Sec:     2238 || Batch Translation Loss:   0.021843 => Txt Tokens per Sec:     6462 || Lr: 0.000100
2024-02-08 19:03:10,029 Epoch 1098: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.37 
2024-02-08 19:03:10,029 EPOCH 1099
2024-02-08 19:03:14,559 Epoch 1099: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.17 
2024-02-08 19:03:14,559 EPOCH 1100
2024-02-08 19:03:19,379 [Epoch: 1100 Step: 00037400] Batch Recognition Loss:   0.004731 => Gls Tokens per Sec:     2204 || Batch Translation Loss:   0.012975 => Txt Tokens per Sec:     6097 || Lr: 0.000100
2024-02-08 19:03:19,380 Epoch 1100: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.71 
2024-02-08 19:03:19,380 EPOCH 1101
2024-02-08 19:03:23,573 Epoch 1101: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.69 
2024-02-08 19:03:23,573 EPOCH 1102
2024-02-08 19:03:28,540 Epoch 1102: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.69 
2024-02-08 19:03:28,541 EPOCH 1103
2024-02-08 19:03:32,479 [Epoch: 1103 Step: 00037500] Batch Recognition Loss:   0.001375 => Gls Tokens per Sec:     2535 || Batch Translation Loss:   0.021618 => Txt Tokens per Sec:     6990 || Lr: 0.000100
2024-02-08 19:03:32,715 Epoch 1103: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-08 19:03:32,716 EPOCH 1104
2024-02-08 19:03:37,545 Epoch 1104: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.82 
2024-02-08 19:03:37,546 EPOCH 1105
2024-02-08 19:03:42,084 Epoch 1105: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.82 
2024-02-08 19:03:42,084 EPOCH 1106
2024-02-08 19:03:46,124 [Epoch: 1106 Step: 00037600] Batch Recognition Loss:   0.000446 => Gls Tokens per Sec:     2313 || Batch Translation Loss:   0.019604 => Txt Tokens per Sec:     6372 || Lr: 0.000100
2024-02-08 19:03:46,735 Epoch 1106: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.65 
2024-02-08 19:03:46,735 EPOCH 1107
2024-02-08 19:03:51,421 Epoch 1107: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.98 
2024-02-08 19:03:51,422 EPOCH 1108
2024-02-08 19:03:55,817 Epoch 1108: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.79 
2024-02-08 19:03:55,818 EPOCH 1109
2024-02-08 19:04:00,040 [Epoch: 1109 Step: 00037700] Batch Recognition Loss:   0.001050 => Gls Tokens per Sec:     2123 || Batch Translation Loss:   0.032610 => Txt Tokens per Sec:     5897 || Lr: 0.000100
2024-02-08 19:04:00,768 Epoch 1109: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-08 19:04:00,769 EPOCH 1110
2024-02-08 19:04:04,913 Epoch 1110: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.66 
2024-02-08 19:04:04,914 EPOCH 1111
2024-02-08 19:04:09,927 Epoch 1111: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.74 
2024-02-08 19:04:09,927 EPOCH 1112
2024-02-08 19:04:13,463 [Epoch: 1112 Step: 00037800] Batch Recognition Loss:   0.000256 => Gls Tokens per Sec:     2280 || Batch Translation Loss:   0.011829 => Txt Tokens per Sec:     6506 || Lr: 0.000100
2024-02-08 19:04:14,215 Epoch 1112: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.03 
2024-02-08 19:04:14,215 EPOCH 1113
2024-02-08 19:04:19,000 Epoch 1113: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.94 
2024-02-08 19:04:19,000 EPOCH 1114
2024-02-08 19:04:23,514 Epoch 1114: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.90 
2024-02-08 19:04:23,514 EPOCH 1115
2024-02-08 19:04:26,262 [Epoch: 1115 Step: 00037900] Batch Recognition Loss:   0.000240 => Gls Tokens per Sec:     2796 || Batch Translation Loss:   0.015194 => Txt Tokens per Sec:     7730 || Lr: 0.000100
2024-02-08 19:04:28,113 Epoch 1115: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.73 
2024-02-08 19:04:28,114 EPOCH 1116
2024-02-08 19:04:32,929 Epoch 1116: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.04 
2024-02-08 19:04:32,929 EPOCH 1117
2024-02-08 19:04:37,022 Epoch 1117: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.05 
2024-02-08 19:04:37,022 EPOCH 1118
2024-02-08 19:04:40,225 [Epoch: 1118 Step: 00038000] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:     2199 || Batch Translation Loss:   0.028460 => Txt Tokens per Sec:     6041 || Lr: 0.000100
2024-02-08 19:04:48,795 Validation result at epoch 1118, step    38000: duration: 8.5680s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.56175	Translation Loss: 94598.20312	PPL: 12688.88281
	Eval Metric: BLEU
	WER 3.04	(DEL: 0.00,	INS: 0.00,	SUB: 3.04)
	BLEU-4 0.32	(BLEU-1: 9.21,	BLEU-2: 2.56,	BLEU-3: 0.91,	BLEU-4: 0.32)
	CHRF 16.31	ROUGE 8.09
2024-02-08 19:04:48,796 Logging Recognition and Translation Outputs
2024-02-08 19:04:48,796 ========================================================================================================================
2024-02-08 19:04:48,796 Logging Sequence: 59_152.00
2024-02-08 19:04:48,796 	Gloss Reference :	A B+C+D+E
2024-02-08 19:04:48,796 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:04:48,797 	Gloss Alignment :	         
2024-02-08 19:04:48,797 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:04:48,798 	Text Reference  :	the organisers encouraged athletes to use the condoms in *** their home countries
2024-02-08 19:04:48,798 	Text Hypothesis :	*** they       are        supposed to *** be  held    in uae as    the  players  
2024-02-08 19:04:48,798 	Text Alignment  :	D   S          S          S           D   S   S          I   S     S    S        
2024-02-08 19:04:48,798 ========================================================================================================================
2024-02-08 19:04:48,798 Logging Sequence: 155_78.00
2024-02-08 19:04:48,799 	Gloss Reference :	A B+C+D+E
2024-02-08 19:04:48,799 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:04:48,799 	Gloss Alignment :	         
2024-02-08 19:04:48,799 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:04:48,800 	Text Reference  :	it was difficult for icc to disqualify the afghan team at the last minute so they    included them as        per      the   schedule
2024-02-08 19:04:48,801 	Text Hypothesis :	** *** ********* *** *** ** ********** *** ****** **** ** the ban  will   be applied when     the  company's finances staff salaries
2024-02-08 19:04:48,801 	Text Alignment  :	D  D   D         D   D   D  D          D   D      D    D      S    S      S  S       S        S    S         S        S     S       
2024-02-08 19:04:48,801 ========================================================================================================================
2024-02-08 19:04:48,801 Logging Sequence: 102_147.00
2024-02-08 19:04:48,801 	Gloss Reference :	A B+C+D+E
2024-02-08 19:04:48,801 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:04:48,802 	Gloss Alignment :	         
2024-02-08 19:04:48,802 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:04:48,803 	Text Reference  :	despite the muscle cramps this young boy lifted such a huge weight and      made the country proud by  securing a gold   medal
2024-02-08 19:04:48,803 	Text Hypothesis :	******* *** ****** ****** **** ***** *** ****** **** * **** she    competed in   the ******* games and secured  a silver medal
2024-02-08 19:04:48,803 	Text Alignment  :	D       D   D      D      D    D     D   D      D    D D    S      S        S        D       S     S   S          S           
2024-02-08 19:04:48,803 ========================================================================================================================
2024-02-08 19:04:48,803 Logging Sequence: 105_2.00
2024-02-08 19:04:48,804 	Gloss Reference :	A B+C+D+E
2024-02-08 19:04:48,804 	Gloss Hypothesis:	A B+C+D  
2024-02-08 19:04:48,804 	Gloss Alignment :	  S      
2024-02-08 19:04:48,804 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:04:48,805 	Text Reference  :	** *** **** the  airthings masters tournament is    an     online chess tournament
2024-02-08 19:04:48,805 	Text Hypothesis :	do you know that india     had     won        their rights for    its   series    
2024-02-08 19:04:48,805 	Text Alignment  :	I  I   I    S    S         S       S          S     S      S      S     S         
2024-02-08 19:04:48,805 ========================================================================================================================
2024-02-08 19:04:48,805 Logging Sequence: 96_31.00
2024-02-08 19:04:48,806 	Gloss Reference :	A B+C+D+E
2024-02-08 19:04:48,806 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:04:48,806 	Gloss Alignment :	         
2024-02-08 19:04:48,806 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:04:48,807 	Text Reference  :	and then 2 teams will go   on   to play the   final
2024-02-08 19:04:48,807 	Text Hypothesis :	*** **** * ***** **** that time a  very happy that 
2024-02-08 19:04:48,807 	Text Alignment  :	D   D    D D     D    S    S    S  S    S     S    
2024-02-08 19:04:48,807 ========================================================================================================================
2024-02-08 19:04:50,636 Epoch 1118: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.62 
2024-02-08 19:04:50,637 EPOCH 1119
2024-02-08 19:04:55,256 Epoch 1119: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.51 
2024-02-08 19:04:55,257 EPOCH 1120
2024-02-08 19:04:59,618 Epoch 1120: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.88 
2024-02-08 19:04:59,619 EPOCH 1121
2024-02-08 19:05:02,731 [Epoch: 1121 Step: 00038100] Batch Recognition Loss:   0.000355 => Gls Tokens per Sec:     1974 || Batch Translation Loss:   0.184235 => Txt Tokens per Sec:     5391 || Lr: 0.000100
2024-02-08 19:05:04,885 Epoch 1121: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.09 
2024-02-08 19:05:04,885 EPOCH 1122
2024-02-08 19:05:09,359 Epoch 1122: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.63 
2024-02-08 19:05:09,359 EPOCH 1123
2024-02-08 19:05:13,866 Epoch 1123: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.02 
2024-02-08 19:05:13,866 EPOCH 1124
2024-02-08 19:05:16,200 [Epoch: 1124 Step: 00038200] Batch Recognition Loss:   0.000866 => Gls Tokens per Sec:     2357 || Batch Translation Loss:   0.038986 => Txt Tokens per Sec:     6441 || Lr: 0.000100
2024-02-08 19:05:18,634 Epoch 1124: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.67 
2024-02-08 19:05:18,634 EPOCH 1125
2024-02-08 19:05:23,189 Epoch 1125: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.97 
2024-02-08 19:05:23,190 EPOCH 1126
2024-02-08 19:05:28,024 Epoch 1126: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.55 
2024-02-08 19:05:28,024 EPOCH 1127
2024-02-08 19:05:29,846 [Epoch: 1127 Step: 00038300] Batch Recognition Loss:   0.000851 => Gls Tokens per Sec:     2669 || Batch Translation Loss:   0.029453 => Txt Tokens per Sec:     7197 || Lr: 0.000100
2024-02-08 19:05:32,098 Epoch 1127: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.08 
2024-02-08 19:05:32,098 EPOCH 1128
2024-02-08 19:05:36,935 Epoch 1128: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.92 
2024-02-08 19:05:36,935 EPOCH 1129
2024-02-08 19:05:41,391 Epoch 1129: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.93 
2024-02-08 19:05:41,391 EPOCH 1130
2024-02-08 19:05:43,022 [Epoch: 1130 Step: 00038400] Batch Recognition Loss:   0.000239 => Gls Tokens per Sec:     2589 || Batch Translation Loss:   0.013383 => Txt Tokens per Sec:     7157 || Lr: 0.000100
2024-02-08 19:05:46,033 Epoch 1130: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.81 
2024-02-08 19:05:46,034 EPOCH 1131
2024-02-08 19:05:50,716 Epoch 1131: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.81 
2024-02-08 19:05:50,717 EPOCH 1132
2024-02-08 19:05:55,031 Epoch 1132: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.76 
2024-02-08 19:05:55,032 EPOCH 1133
2024-02-08 19:05:57,278 [Epoch: 1133 Step: 00038500] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     1710 || Batch Translation Loss:   0.019327 => Txt Tokens per Sec:     4903 || Lr: 0.000100
2024-02-08 19:06:00,097 Epoch 1133: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.66 
2024-02-08 19:06:00,097 EPOCH 1134
2024-02-08 19:06:04,947 Epoch 1134: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-08 19:06:04,947 EPOCH 1135
2024-02-08 19:06:09,875 Epoch 1135: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.40 
2024-02-08 19:06:09,876 EPOCH 1136
2024-02-08 19:06:11,452 [Epoch: 1136 Step: 00038600] Batch Recognition Loss:   0.000148 => Gls Tokens per Sec:     2030 || Batch Translation Loss:   0.014563 => Txt Tokens per Sec:     5897 || Lr: 0.000100
2024-02-08 19:06:14,539 Epoch 1136: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.19 
2024-02-08 19:06:14,539 EPOCH 1137
2024-02-08 19:06:19,433 Epoch 1137: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.77 
2024-02-08 19:06:19,434 EPOCH 1138
2024-02-08 19:06:24,055 Epoch 1138: Total Training Recognition Loss 0.07  Total Training Translation Loss 13.84 
2024-02-08 19:06:24,056 EPOCH 1139
2024-02-08 19:06:24,835 [Epoch: 1139 Step: 00038700] Batch Recognition Loss:   0.003797 => Gls Tokens per Sec:     3295 || Batch Translation Loss:   0.833550 => Txt Tokens per Sec:     7605 || Lr: 0.000100
2024-02-08 19:06:29,032 Epoch 1139: Total Training Recognition Loss 0.18  Total Training Translation Loss 13.64 
2024-02-08 19:06:29,033 EPOCH 1140
2024-02-08 19:06:33,783 Epoch 1140: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.12 
2024-02-08 19:06:33,783 EPOCH 1141
2024-02-08 19:06:38,800 Epoch 1141: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.85 
2024-02-08 19:06:38,801 EPOCH 1142
2024-02-08 19:06:39,475 [Epoch: 1142 Step: 00038800] Batch Recognition Loss:   0.001153 => Gls Tokens per Sec:     2853 || Batch Translation Loss:   0.025614 => Txt Tokens per Sec:     7661 || Lr: 0.000100
2024-02-08 19:06:43,572 Epoch 1142: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.90 
2024-02-08 19:06:43,572 EPOCH 1143
2024-02-08 19:06:48,769 Epoch 1143: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.49 
2024-02-08 19:06:48,769 EPOCH 1144
2024-02-08 19:06:53,449 Epoch 1144: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.50 
2024-02-08 19:06:53,449 EPOCH 1145
2024-02-08 19:06:53,847 [Epoch: 1145 Step: 00038900] Batch Recognition Loss:   0.000540 => Gls Tokens per Sec:     3224 || Batch Translation Loss:   0.013668 => Txt Tokens per Sec:     8579 || Lr: 0.000100
2024-02-08 19:06:57,567 Epoch 1145: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.93 
2024-02-08 19:06:57,567 EPOCH 1146
2024-02-08 19:07:02,581 Epoch 1146: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.91 
2024-02-08 19:07:02,582 EPOCH 1147
2024-02-08 19:07:06,860 Epoch 1147: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.78 
2024-02-08 19:07:06,860 EPOCH 1148
2024-02-08 19:07:07,169 [Epoch: 1148 Step: 00039000] Batch Recognition Loss:   0.000284 => Gls Tokens per Sec:     2078 || Batch Translation Loss:   0.018799 => Txt Tokens per Sec:     6364 || Lr: 0.000100
2024-02-08 19:07:11,598 Epoch 1148: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.74 
2024-02-08 19:07:11,599 EPOCH 1149
2024-02-08 19:07:16,204 Epoch 1149: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-08 19:07:16,204 EPOCH 1150
2024-02-08 19:07:20,744 [Epoch: 1150 Step: 00039100] Batch Recognition Loss:   0.000354 => Gls Tokens per Sec:     2340 || Batch Translation Loss:   0.017052 => Txt Tokens per Sec:     6473 || Lr: 0.000100
2024-02-08 19:07:20,745 Epoch 1150: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-08 19:07:20,745 EPOCH 1151
2024-02-08 19:07:25,519 Epoch 1151: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.73 
2024-02-08 19:07:25,519 EPOCH 1152
2024-02-08 19:07:30,465 Epoch 1152: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-08 19:07:30,466 EPOCH 1153
2024-02-08 19:07:34,607 [Epoch: 1153 Step: 00039200] Batch Recognition Loss:   0.000514 => Gls Tokens per Sec:     2473 || Batch Translation Loss:   0.009717 => Txt Tokens per Sec:     6888 || Lr: 0.000100
2024-02-08 19:07:34,787 Epoch 1153: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.77 
2024-02-08 19:07:34,788 EPOCH 1154
2024-02-08 19:07:39,738 Epoch 1154: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.66 
2024-02-08 19:07:39,739 EPOCH 1155
2024-02-08 19:07:44,122 Epoch 1155: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-08 19:07:44,122 EPOCH 1156
2024-02-08 19:07:48,187 [Epoch: 1156 Step: 00039300] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:     2298 || Batch Translation Loss:   0.077348 => Txt Tokens per Sec:     6320 || Lr: 0.000100
2024-02-08 19:07:48,752 Epoch 1156: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.80 
2024-02-08 19:07:48,752 EPOCH 1157
2024-02-08 19:07:53,812 Epoch 1157: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.10 
2024-02-08 19:07:53,812 EPOCH 1158
2024-02-08 19:07:58,079 Epoch 1158: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-08 19:07:58,079 EPOCH 1159
2024-02-08 19:08:01,925 [Epoch: 1159 Step: 00039400] Batch Recognition Loss:   0.000239 => Gls Tokens per Sec:     2263 || Batch Translation Loss:   0.032335 => Txt Tokens per Sec:     6254 || Lr: 0.000100
2024-02-08 19:08:02,697 Epoch 1159: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.32 
2024-02-08 19:08:02,698 EPOCH 1160
2024-02-08 19:08:07,427 Epoch 1160: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.01 
2024-02-08 19:08:07,427 EPOCH 1161
2024-02-08 19:08:11,658 Epoch 1161: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-08 19:08:11,659 EPOCH 1162
2024-02-08 19:08:15,334 [Epoch: 1162 Step: 00039500] Batch Recognition Loss:   0.000330 => Gls Tokens per Sec:     2265 || Batch Translation Loss:   0.034239 => Txt Tokens per Sec:     6272 || Lr: 0.000100
2024-02-08 19:08:16,664 Epoch 1162: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.36 
2024-02-08 19:08:16,664 EPOCH 1163
2024-02-08 19:08:20,900 Epoch 1163: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.01 
2024-02-08 19:08:20,900 EPOCH 1164
2024-02-08 19:08:25,877 Epoch 1164: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.11 
2024-02-08 19:08:25,878 EPOCH 1165
2024-02-08 19:08:28,943 [Epoch: 1165 Step: 00039600] Batch Recognition Loss:   0.000807 => Gls Tokens per Sec:     2422 || Batch Translation Loss:   0.021873 => Txt Tokens per Sec:     6747 || Lr: 0.000100
2024-02-08 19:08:30,220 Epoch 1165: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-08 19:08:30,220 EPOCH 1166
2024-02-08 19:08:35,034 Epoch 1166: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-08 19:08:35,035 EPOCH 1167
2024-02-08 19:08:40,039 Epoch 1167: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.93 
2024-02-08 19:08:40,040 EPOCH 1168
2024-02-08 19:08:43,008 [Epoch: 1168 Step: 00039700] Batch Recognition Loss:   0.000633 => Gls Tokens per Sec:     2373 || Batch Translation Loss:   0.026881 => Txt Tokens per Sec:     6860 || Lr: 0.000100
2024-02-08 19:08:44,244 Epoch 1168: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-08 19:08:44,244 EPOCH 1169
2024-02-08 19:08:49,128 Epoch 1169: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-08 19:08:49,129 EPOCH 1170
2024-02-08 19:08:53,550 Epoch 1170: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-08 19:08:53,550 EPOCH 1171
2024-02-08 19:08:56,251 [Epoch: 1171 Step: 00039800] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:     2370 || Batch Translation Loss:   0.014372 => Txt Tokens per Sec:     6659 || Lr: 0.000100
2024-02-08 19:08:58,196 Epoch 1171: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.12 
2024-02-08 19:08:58,196 EPOCH 1172
2024-02-08 19:09:02,858 Epoch 1172: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-08 19:09:02,858 EPOCH 1173
2024-02-08 19:09:07,238 Epoch 1173: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-08 19:09:07,239 EPOCH 1174
2024-02-08 19:09:10,196 [Epoch: 1174 Step: 00039900] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:     1949 || Batch Translation Loss:   0.025420 => Txt Tokens per Sec:     5608 || Lr: 0.000100
2024-02-08 19:09:12,181 Epoch 1174: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-08 19:09:12,181 EPOCH 1175
2024-02-08 19:09:16,260 Epoch 1175: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.71 
2024-02-08 19:09:16,260 EPOCH 1176
2024-02-08 19:09:21,202 Epoch 1176: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.92 
2024-02-08 19:09:21,203 EPOCH 1177
2024-02-08 19:09:23,189 [Epoch: 1177 Step: 00040000] Batch Recognition Loss:   0.000312 => Gls Tokens per Sec:     2448 || Batch Translation Loss:   0.046411 => Txt Tokens per Sec:     6524 || Lr: 0.000100
2024-02-08 19:09:32,173 Validation result at epoch 1177, step    40000: duration: 8.9840s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.36420	Translation Loss: 92801.75000	PPL: 10604.68945
	Eval Metric: BLEU
	WER 3.46	(DEL: 0.00,	INS: 0.00,	SUB: 3.46)
	BLEU-4 0.33	(BLEU-1: 9.90,	BLEU-2: 2.84,	BLEU-3: 0.94,	BLEU-4: 0.33)
	CHRF 16.51	ROUGE 8.10
2024-02-08 19:09:32,175 Logging Recognition and Translation Outputs
2024-02-08 19:09:32,175 ========================================================================================================================
2024-02-08 19:09:32,175 Logging Sequence: 86_84.00
2024-02-08 19:09:32,175 	Gloss Reference :	A B+C+D+E
2024-02-08 19:09:32,175 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:09:32,175 	Gloss Alignment :	         
2024-02-08 19:09:32,176 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:09:32,177 	Text Reference  :	amassing 8933 runs which included 21 centuries with    a       highest score of  201    not out 
2024-02-08 19:09:32,177 	Text Hypothesis :	******** **** **** ***** ******** vs jammu     kashmir schools match   he    had scored 260 runs
2024-02-08 19:09:32,177 	Text Alignment  :	D        D    D    D     D        S  S         S       S       S       S     S   S      S   S   
2024-02-08 19:09:32,177 ========================================================================================================================
2024-02-08 19:09:32,177 Logging Sequence: 179_110.00
2024-02-08 19:09:32,177 	Gloss Reference :	A B+C+D+E
2024-02-08 19:09:32,177 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:09:32,178 	Gloss Alignment :	         
2024-02-08 19:09:32,178 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:09:32,179 	Text Reference  :	**** ****** ****** *** * ******* *** **** ***** ****** phogat refused to       stay in the same    room with other indian female wrestlers
2024-02-08 19:09:32,180 	Text Hypothesis :	even though sunday was a holiday wfi kept their office and    sonam   received it   at the airport on   the  day   of     her    departure
2024-02-08 19:09:32,180 	Text Alignment  :	I    I      I      I   I I       I   I    I     I      S      S       S        S    S      S       S    S    S     S      S      S        
2024-02-08 19:09:32,180 ========================================================================================================================
2024-02-08 19:09:32,180 Logging Sequence: 102_2.00
2024-02-08 19:09:32,180 	Gloss Reference :	A B+C+D+E    
2024-02-08 19:09:32,180 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-08 19:09:32,180 	Gloss Alignment :	  S          
2024-02-08 19:09:32,181 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:09:32,182 	Text Reference  :	** commonwealth games are  among the world's most recognised gaming championships after   the olympics
2024-02-08 19:09:32,182 	Text Hypothesis :	on 23rd         march 2021 at    the ******* **** 1st        match  against       england in  japan   
2024-02-08 19:09:32,182 	Text Alignment  :	I  S            S     S    S         D       D    S          S      S             S       S   S       
2024-02-08 19:09:32,182 ========================================================================================================================
2024-02-08 19:09:32,182 Logging Sequence: 60_195.00
2024-02-08 19:09:32,183 	Gloss Reference :	A B+C+D+E
2024-02-08 19:09:32,183 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:09:32,183 	Gloss Alignment :	         
2024-02-08 19:09:32,183 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:09:32,184 	Text Reference  :	******** ****** people loved to    watch his aggressive expressions and  his bowling
2024-02-08 19:09:32,184 	Text Hypothesis :	recently during india  -     nepal asia  cup match      when        play was caught 
2024-02-08 19:09:32,184 	Text Alignment  :	I        I      S      S     S     S     S   S          S           S    S   S      
2024-02-08 19:09:32,184 ========================================================================================================================
2024-02-08 19:09:32,184 Logging Sequence: 70_200.00
2024-02-08 19:09:32,185 	Gloss Reference :	A B+C+D+E
2024-02-08 19:09:32,185 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:09:32,185 	Gloss Alignment :	         
2024-02-08 19:09:32,185 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:09:32,185 	Text Reference  :	showing ronaldo whole-heartedly endorsing the    brand     
2024-02-08 19:09:32,186 	Text Hypothesis :	in      june    is              an        ardent bike-lover
2024-02-08 19:09:32,186 	Text Alignment  :	S       S       S               S         S      S         
2024-02-08 19:09:32,186 ========================================================================================================================
2024-02-08 19:09:34,815 Epoch 1177: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.25 
2024-02-08 19:09:34,815 EPOCH 1178
2024-02-08 19:09:39,634 Epoch 1178: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.22 
2024-02-08 19:09:39,634 EPOCH 1179
2024-02-08 19:09:44,453 Epoch 1179: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.87 
2024-02-08 19:09:44,454 EPOCH 1180
2024-02-08 19:09:46,147 [Epoch: 1180 Step: 00040100] Batch Recognition Loss:   0.000934 => Gls Tokens per Sec:     2645 || Batch Translation Loss:   0.078731 => Txt Tokens per Sec:     7131 || Lr: 0.000100
2024-02-08 19:09:48,603 Epoch 1180: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.98 
2024-02-08 19:09:48,604 EPOCH 1181
2024-02-08 19:09:53,704 Epoch 1181: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.94 
2024-02-08 19:09:53,705 EPOCH 1182
2024-02-08 19:09:58,367 Epoch 1182: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.96 
2024-02-08 19:09:58,367 EPOCH 1183
2024-02-08 19:10:00,215 [Epoch: 1183 Step: 00040200] Batch Recognition Loss:   0.000483 => Gls Tokens per Sec:     2080 || Batch Translation Loss:   0.030232 => Txt Tokens per Sec:     6293 || Lr: 0.000100
2024-02-08 19:10:02,740 Epoch 1183: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.32 
2024-02-08 19:10:02,740 EPOCH 1184
2024-02-08 19:10:07,745 Epoch 1184: Total Training Recognition Loss 0.13  Total Training Translation Loss 2.85 
2024-02-08 19:10:07,745 EPOCH 1185
2024-02-08 19:10:11,895 Epoch 1185: Total Training Recognition Loss 2.58  Total Training Translation Loss 4.43 
2024-02-08 19:10:11,895 EPOCH 1186
2024-02-08 19:10:13,381 [Epoch: 1186 Step: 00040300] Batch Recognition Loss:   0.225675 => Gls Tokens per Sec:     2155 || Batch Translation Loss:   0.096992 => Txt Tokens per Sec:     6176 || Lr: 0.000100
2024-02-08 19:10:16,901 Epoch 1186: Total Training Recognition Loss 14.90  Total Training Translation Loss 5.28 
2024-02-08 19:10:16,901 EPOCH 1187
2024-02-08 19:10:21,235 Epoch 1187: Total Training Recognition Loss 2.12  Total Training Translation Loss 4.72 
2024-02-08 19:10:21,235 EPOCH 1188
2024-02-08 19:10:25,850 Epoch 1188: Total Training Recognition Loss 0.82  Total Training Translation Loss 1.98 
2024-02-08 19:10:25,851 EPOCH 1189
2024-02-08 19:10:27,015 [Epoch: 1189 Step: 00040400] Batch Recognition Loss:   0.000387 => Gls Tokens per Sec:     2201 || Batch Translation Loss:   0.030837 => Txt Tokens per Sec:     6062 || Lr: 0.000100
2024-02-08 19:10:30,585 Epoch 1189: Total Training Recognition Loss 0.39  Total Training Translation Loss 1.21 
2024-02-08 19:10:30,585 EPOCH 1190
2024-02-08 19:10:34,814 Epoch 1190: Total Training Recognition Loss 0.10  Total Training Translation Loss 0.77 
2024-02-08 19:10:34,814 EPOCH 1191
2024-02-08 19:10:39,771 Epoch 1191: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.75 
2024-02-08 19:10:39,772 EPOCH 1192
2024-02-08 19:10:40,377 [Epoch: 1192 Step: 00040500] Batch Recognition Loss:   0.000251 => Gls Tokens per Sec:     3179 || Batch Translation Loss:   0.012635 => Txt Tokens per Sec:     8200 || Lr: 0.000100
2024-02-08 19:10:44,020 Epoch 1192: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.65 
2024-02-08 19:10:44,020 EPOCH 1193
2024-02-08 19:10:49,038 Epoch 1193: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.72 
2024-02-08 19:10:49,039 EPOCH 1194
2024-02-08 19:10:53,430 Epoch 1194: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.68 
2024-02-08 19:10:53,430 EPOCH 1195
2024-02-08 19:10:53,889 [Epoch: 1195 Step: 00040600] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:     2795 || Batch Translation Loss:   0.055151 => Txt Tokens per Sec:     7939 || Lr: 0.000100
2024-02-08 19:10:58,133 Epoch 1195: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.73 
2024-02-08 19:10:58,133 EPOCH 1196
2024-02-08 19:11:03,171 Epoch 1196: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.64 
2024-02-08 19:11:03,172 EPOCH 1197
2024-02-08 19:11:07,455 Epoch 1197: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.58 
2024-02-08 19:11:07,456 EPOCH 1198
2024-02-08 19:11:07,698 [Epoch: 1198 Step: 00040700] Batch Recognition Loss:   0.012589 => Gls Tokens per Sec:     2656 || Batch Translation Loss:   0.018083 => Txt Tokens per Sec:     7718 || Lr: 0.000100
2024-02-08 19:11:12,259 Epoch 1198: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.68 
2024-02-08 19:11:12,260 EPOCH 1199
2024-02-08 19:11:16,795 Epoch 1199: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.87 
2024-02-08 19:11:16,795 EPOCH 1200
2024-02-08 19:11:21,338 [Epoch: 1200 Step: 00040800] Batch Recognition Loss:   0.000115 => Gls Tokens per Sec:     2339 || Batch Translation Loss:   0.021570 => Txt Tokens per Sec:     6470 || Lr: 0.000100
2024-02-08 19:11:21,339 Epoch 1200: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.67 
2024-02-08 19:11:21,339 EPOCH 1201
2024-02-08 19:11:26,130 Epoch 1201: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-08 19:11:26,130 EPOCH 1202
2024-02-08 19:11:30,412 Epoch 1202: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.56 
2024-02-08 19:11:30,412 EPOCH 1203
2024-02-08 19:11:35,105 [Epoch: 1203 Step: 00040900] Batch Recognition Loss:   0.006556 => Gls Tokens per Sec:     2127 || Batch Translation Loss:   0.025336 => Txt Tokens per Sec:     5861 || Lr: 0.000100
2024-02-08 19:11:35,415 Epoch 1203: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.66 
2024-02-08 19:11:35,415 EPOCH 1204
2024-02-08 19:11:39,601 Epoch 1204: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.97 
2024-02-08 19:11:39,601 EPOCH 1205
2024-02-08 19:11:44,498 Epoch 1205: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.95 
2024-02-08 19:11:44,499 EPOCH 1206
2024-02-08 19:11:48,027 [Epoch: 1206 Step: 00041000] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:     2649 || Batch Translation Loss:   0.051988 => Txt Tokens per Sec:     7160 || Lr: 0.000100
2024-02-08 19:11:48,879 Epoch 1206: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.60 
2024-02-08 19:11:48,879 EPOCH 1207
2024-02-08 19:11:53,636 Epoch 1207: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.07 
2024-02-08 19:11:53,637 EPOCH 1208
2024-02-08 19:11:58,494 Epoch 1208: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.46 
2024-02-08 19:11:58,494 EPOCH 1209
2024-02-08 19:12:02,627 [Epoch: 1209 Step: 00041100] Batch Recognition Loss:   0.000246 => Gls Tokens per Sec:     2168 || Batch Translation Loss:   0.025091 => Txt Tokens per Sec:     6108 || Lr: 0.000100
2024-02-08 19:12:03,355 Epoch 1209: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.26 
2024-02-08 19:12:03,355 EPOCH 1210
2024-02-08 19:12:07,898 Epoch 1210: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.05 
2024-02-08 19:12:07,898 EPOCH 1211
2024-02-08 19:12:12,084 Epoch 1211: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.11 
2024-02-08 19:12:12,084 EPOCH 1212
2024-02-08 19:12:16,027 [Epoch: 1212 Step: 00041200] Batch Recognition Loss:   0.000213 => Gls Tokens per Sec:     2111 || Batch Translation Loss:   0.024963 => Txt Tokens per Sec:     5732 || Lr: 0.000100
2024-02-08 19:12:17,141 Epoch 1212: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.98 
2024-02-08 19:12:17,141 EPOCH 1213
2024-02-08 19:12:21,371 Epoch 1213: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.44 
2024-02-08 19:12:21,371 EPOCH 1214
2024-02-08 19:12:26,407 Epoch 1214: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.46 
2024-02-08 19:12:26,407 EPOCH 1215
2024-02-08 19:12:29,710 [Epoch: 1215 Step: 00041300] Batch Recognition Loss:   0.002937 => Gls Tokens per Sec:     2326 || Batch Translation Loss:   0.038979 => Txt Tokens per Sec:     6273 || Lr: 0.000100
2024-02-08 19:12:31,262 Epoch 1215: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.34 
2024-02-08 19:12:31,262 EPOCH 1216
2024-02-08 19:12:35,610 Epoch 1216: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.49 
2024-02-08 19:12:35,611 EPOCH 1217
2024-02-08 19:12:40,558 Epoch 1217: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.59 
2024-02-08 19:12:40,558 EPOCH 1218
2024-02-08 19:12:43,418 [Epoch: 1218 Step: 00041400] Batch Recognition Loss:   0.000395 => Gls Tokens per Sec:     2371 || Batch Translation Loss:   0.275632 => Txt Tokens per Sec:     6608 || Lr: 0.000100
2024-02-08 19:12:44,744 Epoch 1218: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.32 
2024-02-08 19:12:44,744 EPOCH 1219
2024-02-08 19:12:49,726 Epoch 1219: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.81 
2024-02-08 19:12:49,727 EPOCH 1220
2024-02-08 19:12:54,032 Epoch 1220: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.80 
2024-02-08 19:12:54,032 EPOCH 1221
2024-02-08 19:12:56,583 [Epoch: 1221 Step: 00041500] Batch Recognition Loss:   0.000396 => Gls Tokens per Sec:     2409 || Batch Translation Loss:   0.018518 => Txt Tokens per Sec:     6781 || Lr: 0.000100
2024-02-08 19:12:58,812 Epoch 1221: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.92 
2024-02-08 19:12:58,813 EPOCH 1222
2024-02-08 19:13:03,435 Epoch 1222: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.56 
2024-02-08 19:13:03,436 EPOCH 1223
2024-02-08 19:13:08,159 Epoch 1223: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.88 
2024-02-08 19:13:08,159 EPOCH 1224
2024-02-08 19:13:10,444 [Epoch: 1224 Step: 00041600] Batch Recognition Loss:   0.000595 => Gls Tokens per Sec:     2522 || Batch Translation Loss:   0.027694 => Txt Tokens per Sec:     6792 || Lr: 0.000100
2024-02-08 19:13:12,931 Epoch 1224: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.71 
2024-02-08 19:13:12,931 EPOCH 1225
2024-02-08 19:13:17,017 Epoch 1225: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.40 
2024-02-08 19:13:17,018 EPOCH 1226
2024-02-08 19:13:21,855 Epoch 1226: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.71 
2024-02-08 19:13:21,855 EPOCH 1227
2024-02-08 19:13:24,020 [Epoch: 1227 Step: 00041700] Batch Recognition Loss:   0.000526 => Gls Tokens per Sec:     2367 || Batch Translation Loss:   0.023565 => Txt Tokens per Sec:     6386 || Lr: 0.000100
2024-02-08 19:13:26,648 Epoch 1227: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.15 
2024-02-08 19:13:26,649 EPOCH 1228
2024-02-08 19:13:31,498 Epoch 1228: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.92 
2024-02-08 19:13:31,499 EPOCH 1229
2024-02-08 19:13:35,896 Epoch 1229: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.70 
2024-02-08 19:13:35,896 EPOCH 1230
2024-02-08 19:13:37,352 [Epoch: 1230 Step: 00041800] Batch Recognition Loss:   0.000731 => Gls Tokens per Sec:     3079 || Batch Translation Loss:   0.023849 => Txt Tokens per Sec:     8089 || Lr: 0.000100
2024-02-08 19:13:40,272 Epoch 1230: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.61 
2024-02-08 19:13:40,273 EPOCH 1231
2024-02-08 19:13:45,234 Epoch 1231: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.62 
2024-02-08 19:13:45,234 EPOCH 1232
2024-02-08 19:13:50,294 Epoch 1232: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.60 
2024-02-08 19:13:50,294 EPOCH 1233
2024-02-08 19:13:51,989 [Epoch: 1233 Step: 00041900] Batch Recognition Loss:   0.000366 => Gls Tokens per Sec:     2268 || Batch Translation Loss:   0.012820 => Txt Tokens per Sec:     6482 || Lr: 0.000100
2024-02-08 19:13:55,025 Epoch 1233: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.73 
2024-02-08 19:13:55,025 EPOCH 1234
2024-02-08 19:13:59,848 Epoch 1234: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.64 
2024-02-08 19:13:59,848 EPOCH 1235
2024-02-08 19:14:04,803 Epoch 1235: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.80 
2024-02-08 19:14:04,804 EPOCH 1236
2024-02-08 19:14:06,054 [Epoch: 1236 Step: 00042000] Batch Recognition Loss:   0.000257 => Gls Tokens per Sec:     2562 || Batch Translation Loss:   0.013900 => Txt Tokens per Sec:     7552 || Lr: 0.000100
2024-02-08 19:14:15,111 Validation result at epoch 1236, step    42000: duration: 9.0562s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.19254	Translation Loss: 93493.27344	PPL: 11363.04102
	Eval Metric: BLEU
	WER 2.68	(DEL: 0.00,	INS: 0.00,	SUB: 2.68)
	BLEU-4 0.65	(BLEU-1: 11.27,	BLEU-2: 3.53,	BLEU-3: 1.36,	BLEU-4: 0.65)
	CHRF 17.57	ROUGE 9.34
2024-02-08 19:14:15,113 Logging Recognition and Translation Outputs
2024-02-08 19:14:15,113 ========================================================================================================================
2024-02-08 19:14:15,113 Logging Sequence: 154_94.00
2024-02-08 19:14:15,113 	Gloss Reference :	A B+C+D+E
2024-02-08 19:14:15,114 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:14:15,114 	Gloss Alignment :	         
2024-02-08 19:14:15,114 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:14:15,115 	Text Reference  :	***** the ipl will also be held  in uae  from september 19        to  october 15       
2024-02-08 19:14:15,115 	Text Hypothesis :	after the *** **** **** ** start of july new  zealand   singapore and then    restarted
2024-02-08 19:14:15,115 	Text Alignment  :	I         D   D    D    D  S     S  S    S    S         S         S   S       S        
2024-02-08 19:14:15,115 ========================================================================================================================
2024-02-08 19:14:15,116 Logging Sequence: 118_2.00
2024-02-08 19:14:15,116 	Gloss Reference :	A B+C+D+E
2024-02-08 19:14:15,116 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:14:15,116 	Gloss Alignment :	         
2024-02-08 19:14:15,116 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:14:15,117 	Text Reference  :	yesterday was a   very   exciting day      people across the   world were  watching
2024-02-08 19:14:15,117 	Text Hypothesis :	********* *** the people started  trolling was    being  being being being made    
2024-02-08 19:14:15,117 	Text Alignment  :	D         D   S   S      S        S        S      S      S     S     S     S       
2024-02-08 19:14:15,117 ========================================================================================================================
2024-02-08 19:14:15,118 Logging Sequence: 165_453.00
2024-02-08 19:14:15,118 	Gloss Reference :	A B+C+D+E
2024-02-08 19:14:15,118 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:14:15,118 	Gloss Alignment :	         
2024-02-08 19:14:15,118 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:14:15,119 	Text Reference  :	icc     did not agree to   sehwag' decision of   wearing a   numberless jersey 
2024-02-08 19:14:15,119 	Text Hypothesis :	however he  has only  wore this    wicket   when he      got india      england
2024-02-08 19:14:15,119 	Text Alignment  :	S       S   S   S     S    S       S        S    S       S   S          S      
2024-02-08 19:14:15,120 ========================================================================================================================
2024-02-08 19:14:15,120 Logging Sequence: 126_163.00
2024-02-08 19:14:15,120 	Gloss Reference :	A B+C+D+E
2024-02-08 19:14:15,120 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:14:15,120 	Gloss Alignment :	         
2024-02-08 19:14:15,120 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:14:15,121 	Text Reference  :	*** your hard     work has helped secure a     medal at the tokyo olympics  
2024-02-08 19:14:15,122 	Text Hypothesis :	for the  olympics one  day and    i      never spoke to the ***** government
2024-02-08 19:14:15,122 	Text Alignment  :	I   S    S        S    S   S      S      S     S     S      D     S         
2024-02-08 19:14:15,122 ========================================================================================================================
2024-02-08 19:14:15,122 Logging Sequence: 84_2.00
2024-02-08 19:14:15,122 	Gloss Reference :	A B+C+D+E
2024-02-08 19:14:15,122 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:14:15,122 	Gloss Alignment :	         
2024-02-08 19:14:15,123 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:14:15,124 	Text Reference  :	the ********* 2022 fifa  football world cup ****** is     going on      in qatar from 20th november 2022 to 18th december 2022 
2024-02-08 19:14:15,124 	Text Hypothesis :	the cricketer was  about the      world cup trophy former rcb   captain in ***** **** **** ******** **** ** **** ******** dubai
2024-02-08 19:14:15,124 	Text Alignment  :	    I         S    S     S                  I      S      S     S          D     D    D    D        D    D  D    D        S    
2024-02-08 19:14:15,125 ========================================================================================================================
2024-02-08 19:14:18,602 Epoch 1236: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-08 19:14:18,603 EPOCH 1237
2024-02-08 19:14:23,494 Epoch 1237: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.79 
2024-02-08 19:14:23,494 EPOCH 1238
2024-02-08 19:14:27,636 Epoch 1238: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-08 19:14:27,636 EPOCH 1239
2024-02-08 19:14:28,505 [Epoch: 1239 Step: 00042100] Batch Recognition Loss:   0.000483 => Gls Tokens per Sec:     2650 || Batch Translation Loss:   0.018937 => Txt Tokens per Sec:     6631 || Lr: 0.000100
2024-02-08 19:14:31,700 Epoch 1239: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.71 
2024-02-08 19:14:31,700 EPOCH 1240
2024-02-08 19:14:36,700 Epoch 1240: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.86 
2024-02-08 19:14:36,700 EPOCH 1241
2024-02-08 19:14:41,114 Epoch 1241: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.93 
2024-02-08 19:14:41,115 EPOCH 1242
2024-02-08 19:14:42,286 [Epoch: 1242 Step: 00042200] Batch Recognition Loss:   0.000284 => Gls Tokens per Sec:     1642 || Batch Translation Loss:   0.010832 => Txt Tokens per Sec:     4713 || Lr: 0.000100
2024-02-08 19:14:46,159 Epoch 1242: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-08 19:14:46,159 EPOCH 1243
2024-02-08 19:14:50,880 Epoch 1243: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.00 
2024-02-08 19:14:50,880 EPOCH 1244
2024-02-08 19:14:54,982 Epoch 1244: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-08 19:14:54,982 EPOCH 1245
2024-02-08 19:14:55,518 [Epoch: 1245 Step: 00042300] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     2393 || Batch Translation Loss:   0.090505 => Txt Tokens per Sec:     7161 || Lr: 0.000100
2024-02-08 19:14:59,896 Epoch 1245: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.10 
2024-02-08 19:14:59,897 EPOCH 1246
2024-02-08 19:15:04,681 Epoch 1246: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.42 
2024-02-08 19:15:04,681 EPOCH 1247
2024-02-08 19:15:09,608 Epoch 1247: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.41 
2024-02-08 19:15:09,609 EPOCH 1248
2024-02-08 19:15:09,797 [Epoch: 1248 Step: 00042400] Batch Recognition Loss:   0.000618 => Gls Tokens per Sec:     3422 || Batch Translation Loss:   0.025888 => Txt Tokens per Sec:     9326 || Lr: 0.000100
2024-02-08 19:15:14,268 Epoch 1248: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.97 
2024-02-08 19:15:14,268 EPOCH 1249
2024-02-08 19:15:19,152 Epoch 1249: Total Training Recognition Loss 0.08  Total Training Translation Loss 12.52 
2024-02-08 19:15:19,152 EPOCH 1250
2024-02-08 19:15:24,175 [Epoch: 1250 Step: 00042500] Batch Recognition Loss:   0.000842 => Gls Tokens per Sec:     2115 || Batch Translation Loss:   0.137567 => Txt Tokens per Sec:     5852 || Lr: 0.000100
2024-02-08 19:15:24,175 Epoch 1250: Total Training Recognition Loss 0.06  Total Training Translation Loss 6.55 
2024-02-08 19:15:24,175 EPOCH 1251
2024-02-08 19:15:28,556 Epoch 1251: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.05 
2024-02-08 19:15:28,556 EPOCH 1252
2024-02-08 19:15:33,079 Epoch 1252: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.15 
2024-02-08 19:15:33,079 EPOCH 1253
2024-02-08 19:15:37,280 [Epoch: 1253 Step: 00042600] Batch Recognition Loss:   0.000608 => Gls Tokens per Sec:     2377 || Batch Translation Loss:   0.019228 => Txt Tokens per Sec:     6487 || Lr: 0.000100
2024-02-08 19:15:37,875 Epoch 1253: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.78 
2024-02-08 19:15:37,875 EPOCH 1254
2024-02-08 19:15:42,155 Epoch 1254: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.73 
2024-02-08 19:15:42,155 EPOCH 1255
2024-02-08 19:15:47,136 Epoch 1255: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.10 
2024-02-08 19:15:47,136 EPOCH 1256
2024-02-08 19:15:50,805 [Epoch: 1256 Step: 00042700] Batch Recognition Loss:   0.000994 => Gls Tokens per Sec:     2546 || Batch Translation Loss:   0.022586 => Txt Tokens per Sec:     6983 || Lr: 0.000100
2024-02-08 19:15:51,401 Epoch 1256: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-08 19:15:51,401 EPOCH 1257
2024-02-08 19:15:56,291 Epoch 1257: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-08 19:15:56,292 EPOCH 1258
2024-02-08 19:16:00,702 Epoch 1258: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-08 19:16:00,702 EPOCH 1259
2024-02-08 19:16:04,742 [Epoch: 1259 Step: 00042800] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     2154 || Batch Translation Loss:   0.014212 => Txt Tokens per Sec:     6005 || Lr: 0.000100
2024-02-08 19:16:05,446 Epoch 1259: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-08 19:16:05,446 EPOCH 1260
2024-02-08 19:16:10,141 Epoch 1260: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-08 19:16:10,141 EPOCH 1261
2024-02-08 19:16:14,572 Epoch 1261: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.50 
2024-02-08 19:16:14,573 EPOCH 1262
2024-02-08 19:16:18,252 [Epoch: 1262 Step: 00042900] Batch Recognition Loss:   0.000277 => Gls Tokens per Sec:     2262 || Batch Translation Loss:   0.015682 => Txt Tokens per Sec:     6405 || Lr: 0.000100
2024-02-08 19:16:19,422 Epoch 1262: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.44 
2024-02-08 19:16:19,422 EPOCH 1263
2024-02-08 19:16:23,493 Epoch 1263: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.94 
2024-02-08 19:16:23,493 EPOCH 1264
2024-02-08 19:16:28,582 Epoch 1264: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.95 
2024-02-08 19:16:28,582 EPOCH 1265
2024-02-08 19:16:31,635 [Epoch: 1265 Step: 00043000] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     2432 || Batch Translation Loss:   0.014671 => Txt Tokens per Sec:     6755 || Lr: 0.000100
2024-02-08 19:16:32,949 Epoch 1265: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.82 
2024-02-08 19:16:32,949 EPOCH 1266
2024-02-08 19:16:37,819 Epoch 1266: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.87 
2024-02-08 19:16:37,819 EPOCH 1267
2024-02-08 19:16:42,253 Epoch 1267: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.42 
2024-02-08 19:16:42,253 EPOCH 1268
2024-02-08 19:16:44,653 [Epoch: 1268 Step: 00043100] Batch Recognition Loss:   0.000772 => Gls Tokens per Sec:     2935 || Batch Translation Loss:   0.050344 => Txt Tokens per Sec:     7820 || Lr: 0.000100
2024-02-08 19:16:46,892 Epoch 1268: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.30 
2024-02-08 19:16:46,893 EPOCH 1269
2024-02-08 19:16:51,650 Epoch 1269: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.18 
2024-02-08 19:16:51,651 EPOCH 1270
2024-02-08 19:16:56,003 Epoch 1270: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.82 
2024-02-08 19:16:56,004 EPOCH 1271
2024-02-08 19:16:59,042 [Epoch: 1271 Step: 00043200] Batch Recognition Loss:   0.001902 => Gls Tokens per Sec:     2022 || Batch Translation Loss:   0.129843 => Txt Tokens per Sec:     5839 || Lr: 0.000100
2024-02-08 19:17:00,924 Epoch 1271: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.38 
2024-02-08 19:17:00,924 EPOCH 1272
2024-02-08 19:17:05,058 Epoch 1272: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.48 
2024-02-08 19:17:05,058 EPOCH 1273
2024-02-08 19:17:10,007 Epoch 1273: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.18 
2024-02-08 19:17:10,008 EPOCH 1274
2024-02-08 19:17:12,168 [Epoch: 1274 Step: 00043300] Batch Recognition Loss:   0.000482 => Gls Tokens per Sec:     2668 || Batch Translation Loss:   0.035256 => Txt Tokens per Sec:     7317 || Lr: 0.000100
2024-02-08 19:17:14,396 Epoch 1274: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.34 
2024-02-08 19:17:14,397 EPOCH 1275
2024-02-08 19:17:19,139 Epoch 1275: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.72 
2024-02-08 19:17:19,140 EPOCH 1276
2024-02-08 19:17:23,680 Epoch 1276: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.02 
2024-02-08 19:17:23,681 EPOCH 1277
2024-02-08 19:17:25,597 [Epoch: 1277 Step: 00043400] Batch Recognition Loss:   0.000904 => Gls Tokens per Sec:     2674 || Batch Translation Loss:   0.022092 => Txt Tokens per Sec:     7586 || Lr: 0.000100
2024-02-08 19:17:28,185 Epoch 1277: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.93 
2024-02-08 19:17:28,185 EPOCH 1278
2024-02-08 19:17:33,048 Epoch 1278: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.85 
2024-02-08 19:17:33,048 EPOCH 1279
2024-02-08 19:17:37,212 Epoch 1279: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.37 
2024-02-08 19:17:37,212 EPOCH 1280
2024-02-08 19:17:39,584 [Epoch: 1280 Step: 00043500] Batch Recognition Loss:   0.000726 => Gls Tokens per Sec:     1781 || Batch Translation Loss:   0.077132 => Txt Tokens per Sec:     5026 || Lr: 0.000100
2024-02-08 19:17:42,270 Epoch 1280: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.59 
2024-02-08 19:17:42,270 EPOCH 1281
2024-02-08 19:17:46,492 Epoch 1281: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.71 
2024-02-08 19:17:46,492 EPOCH 1282
2024-02-08 19:17:51,382 Epoch 1282: Total Training Recognition Loss 0.15  Total Training Translation Loss 6.62 
2024-02-08 19:17:51,382 EPOCH 1283
2024-02-08 19:17:52,825 [Epoch: 1283 Step: 00043600] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:     2663 || Batch Translation Loss:   0.042650 => Txt Tokens per Sec:     7144 || Lr: 0.000100
2024-02-08 19:17:55,827 Epoch 1283: Total Training Recognition Loss 0.83  Total Training Translation Loss 2.25 
2024-02-08 19:17:55,827 EPOCH 1284
2024-02-08 19:18:00,180 Epoch 1284: Total Training Recognition Loss 1.87  Total Training Translation Loss 1.78 
2024-02-08 19:18:00,181 EPOCH 1285
2024-02-08 19:18:05,133 Epoch 1285: Total Training Recognition Loss 2.45  Total Training Translation Loss 1.64 
2024-02-08 19:18:05,133 EPOCH 1286
2024-02-08 19:18:06,275 [Epoch: 1286 Step: 00043700] Batch Recognition Loss:   0.000274 => Gls Tokens per Sec:     2577 || Batch Translation Loss:   0.023341 => Txt Tokens per Sec:     7263 || Lr: 0.000100
2024-02-08 19:18:09,251 Epoch 1286: Total Training Recognition Loss 2.32  Total Training Translation Loss 1.07 
2024-02-08 19:18:09,251 EPOCH 1287
2024-02-08 19:18:14,228 Epoch 1287: Total Training Recognition Loss 0.20  Total Training Translation Loss 0.83 
2024-02-08 19:18:14,229 EPOCH 1288
2024-02-08 19:18:18,638 Epoch 1288: Total Training Recognition Loss 0.12  Total Training Translation Loss 0.81 
2024-02-08 19:18:18,638 EPOCH 1289
2024-02-08 19:18:19,394 [Epoch: 1289 Step: 00043800] Batch Recognition Loss:   0.000678 => Gls Tokens per Sec:     3395 || Batch Translation Loss:   0.007020 => Txt Tokens per Sec:     8137 || Lr: 0.000100
2024-02-08 19:18:23,409 Epoch 1289: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.97 
2024-02-08 19:18:23,409 EPOCH 1290
2024-02-08 19:18:27,976 Epoch 1290: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.63 
2024-02-08 19:18:27,976 EPOCH 1291
2024-02-08 19:18:32,573 Epoch 1291: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.58 
2024-02-08 19:18:32,574 EPOCH 1292
2024-02-08 19:18:33,530 [Epoch: 1292 Step: 00043900] Batch Recognition Loss:   0.000831 => Gls Tokens per Sec:     2013 || Batch Translation Loss:   0.014892 => Txt Tokens per Sec:     6103 || Lr: 0.000100
2024-02-08 19:18:37,358 Epoch 1292: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.56 
2024-02-08 19:18:37,358 EPOCH 1293
2024-02-08 19:18:41,893 Epoch 1293: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.54 
2024-02-08 19:18:41,894 EPOCH 1294
2024-02-08 19:18:46,658 Epoch 1294: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.53 
2024-02-08 19:18:46,658 EPOCH 1295
2024-02-08 19:18:47,056 [Epoch: 1295 Step: 00044000] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     3224 || Batch Translation Loss:   0.010381 => Txt Tokens per Sec:     7708 || Lr: 0.000100
2024-02-08 19:18:55,700 Validation result at epoch 1295, step    44000: duration: 8.6440s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.05552	Translation Loss: 94673.21875	PPL: 12784.31836
	Eval Metric: BLEU
	WER 2.40	(DEL: 0.00,	INS: 0.00,	SUB: 2.40)
	BLEU-4 0.47	(BLEU-1: 10.13,	BLEU-2: 2.81,	BLEU-3: 1.04,	BLEU-4: 0.47)
	CHRF 16.62	ROUGE 8.81
2024-02-08 19:18:55,701 Logging Recognition and Translation Outputs
2024-02-08 19:18:55,702 ========================================================================================================================
2024-02-08 19:18:55,702 Logging Sequence: 57_104.00
2024-02-08 19:18:55,702 	Gloss Reference :	A B+C+D+E
2024-02-08 19:18:55,702 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:18:55,702 	Gloss Alignment :	         
2024-02-08 19:18:55,702 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:18:55,705 	Text Reference  :	*** the next day kohli and kl rahul continued from  where they  had left and   displayed amazing batting performance without losing  their wickets
2024-02-08 19:18:55,705 	Text Hypothesis :	for the **** *** ***** *** ** ***** ********* first time  india won the  world cup       while   each    other       in      colombo sri   lanka  
2024-02-08 19:18:55,705 	Text Alignment  :	I       D    D   D     D   D  D     D         S     S     S     S   S    S     S         S       S       S           S       S       S     S      
2024-02-08 19:18:55,705 ========================================================================================================================
2024-02-08 19:18:55,705 Logging Sequence: 136_64.00
2024-02-08 19:18:55,705 	Gloss Reference :	A B+C+D+E
2024-02-08 19:18:55,706 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:18:55,706 	Gloss Alignment :	         
2024-02-08 19:18:55,706 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:18:55,707 	Text Reference  :	* ** **** ***** in all she   has won       2  medals
2024-02-08 19:18:55,707 	Text Hypothesis :	i am very sorry it was never my  intention to bowl  
2024-02-08 19:18:55,707 	Text Alignment  :	I I  I    I     S  S   S     S   S         S  S     
2024-02-08 19:18:55,707 ========================================================================================================================
2024-02-08 19:18:55,707 Logging Sequence: 54_123.00
2024-02-08 19:18:55,707 	Gloss Reference :	A B+C+D+E
2024-02-08 19:18:55,708 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:18:55,708 	Gloss Alignment :	         
2024-02-08 19:18:55,708 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:18:55,709 	Text Reference  :	vips sponsors international cricket groups have    already booked their hotel rooms
2024-02-08 19:18:55,709 	Text Hypothesis :	**** ******** there         are     no     clarity over    when   the   hotel *****
2024-02-08 19:18:55,709 	Text Alignment  :	D    D        S             S       S      S       S       S      S           D    
2024-02-08 19:18:55,709 ========================================================================================================================
2024-02-08 19:18:55,709 Logging Sequence: 168_115.00
2024-02-08 19:18:55,709 	Gloss Reference :	A B+C+D+E
2024-02-08 19:18:55,710 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:18:55,710 	Gloss Alignment :	         
2024-02-08 19:18:55,710 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:18:55,711 	Text Reference  :	****** **** this    has sparked a   major discussion on    social     media
2024-02-08 19:18:55,711 	Text Hypothesis :	people were shocked to  see     the game  that       their respective teams
2024-02-08 19:18:55,711 	Text Alignment  :	I      I    S       S   S       S   S     S          S     S          S    
2024-02-08 19:18:55,711 ========================================================================================================================
2024-02-08 19:18:55,711 Logging Sequence: 121_132.00
2024-02-08 19:18:55,712 	Gloss Reference :	A B+C+D+E
2024-02-08 19:18:55,712 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:18:55,712 	Gloss Alignment :	         
2024-02-08 19:18:55,712 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:18:55,713 	Text Reference  :	which is why they will be      retesting her to   check if  she consumed any stamina enhancing drugs
2024-02-08 19:18:55,714 	Text Hypothesis :	***** ** *** **** now  finally he        is  part of    the cwg lawn     be  held    in        july 
2024-02-08 19:18:55,714 	Text Alignment  :	D     D  D   D    S    S       S         S   S    S     S   S   S        S   S       S         S    
2024-02-08 19:18:55,714 ========================================================================================================================
2024-02-08 19:19:00,278 Epoch 1295: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-08 19:19:00,279 EPOCH 1296
2024-02-08 19:19:04,637 Epoch 1296: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.46 
2024-02-08 19:19:04,637 EPOCH 1297
2024-02-08 19:19:09,093 Epoch 1297: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-08 19:19:09,094 EPOCH 1298
2024-02-08 19:19:09,267 [Epoch: 1298 Step: 00044100] Batch Recognition Loss:   0.002972 => Gls Tokens per Sec:     3721 || Batch Translation Loss:   0.007812 => Txt Tokens per Sec:     7977 || Lr: 0.000050
2024-02-08 19:19:13,999 Epoch 1298: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.44 
2024-02-08 19:19:13,999 EPOCH 1299
2024-02-08 19:19:18,619 Epoch 1299: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.43 
2024-02-08 19:19:18,620 EPOCH 1300
2024-02-08 19:19:23,550 [Epoch: 1300 Step: 00044200] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:     2155 || Batch Translation Loss:   0.008997 => Txt Tokens per Sec:     5961 || Lr: 0.000050
2024-02-08 19:19:23,551 Epoch 1300: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-08 19:19:23,551 EPOCH 1301
2024-02-08 19:19:28,217 Epoch 1301: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-08 19:19:28,218 EPOCH 1302
2024-02-08 19:19:33,093 Epoch 1302: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.41 
2024-02-08 19:19:33,093 EPOCH 1303
2024-02-08 19:19:37,639 [Epoch: 1303 Step: 00044300] Batch Recognition Loss:   0.000537 => Gls Tokens per Sec:     2196 || Batch Translation Loss:   0.014841 => Txt Tokens per Sec:     6242 || Lr: 0.000050
2024-02-08 19:19:37,759 Epoch 1303: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.40 
2024-02-08 19:19:37,759 EPOCH 1304
2024-02-08 19:19:42,660 Epoch 1304: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-08 19:19:42,661 EPOCH 1305
2024-02-08 19:19:47,096 Epoch 1305: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-08 19:19:47,097 EPOCH 1306
2024-02-08 19:19:51,289 [Epoch: 1306 Step: 00044400] Batch Recognition Loss:   0.000531 => Gls Tokens per Sec:     2229 || Batch Translation Loss:   0.011077 => Txt Tokens per Sec:     6286 || Lr: 0.000050
2024-02-08 19:19:51,654 Epoch 1306: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.40 
2024-02-08 19:19:51,655 EPOCH 1307
2024-02-08 19:19:56,423 Epoch 1307: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-08 19:19:56,424 EPOCH 1308
2024-02-08 19:20:00,964 Epoch 1308: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.38 
2024-02-08 19:20:00,964 EPOCH 1309
2024-02-08 19:20:04,366 [Epoch: 1309 Step: 00044500] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     2558 || Batch Translation Loss:   0.012202 => Txt Tokens per Sec:     7017 || Lr: 0.000050
2024-02-08 19:20:05,231 Epoch 1309: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.37 
2024-02-08 19:20:05,231 EPOCH 1310
2024-02-08 19:20:10,196 Epoch 1310: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.39 
2024-02-08 19:20:10,196 EPOCH 1311
2024-02-08 19:20:14,348 Epoch 1311: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.38 
2024-02-08 19:20:14,348 EPOCH 1312
2024-02-08 19:20:18,170 [Epoch: 1312 Step: 00044600] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     2110 || Batch Translation Loss:   0.010179 => Txt Tokens per Sec:     5966 || Lr: 0.000050
2024-02-08 19:20:19,196 Epoch 1312: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-08 19:20:19,196 EPOCH 1313
2024-02-08 19:20:23,631 Epoch 1313: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-08 19:20:23,631 EPOCH 1314
2024-02-08 19:20:28,334 Epoch 1314: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-08 19:20:28,335 EPOCH 1315
2024-02-08 19:20:31,517 [Epoch: 1315 Step: 00044700] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     2415 || Batch Translation Loss:   0.015278 => Txt Tokens per Sec:     6502 || Lr: 0.000050
2024-02-08 19:20:32,993 Epoch 1315: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-08 19:20:32,993 EPOCH 1316
2024-02-08 19:20:37,413 Epoch 1316: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-08 19:20:37,414 EPOCH 1317
2024-02-08 19:20:42,339 Epoch 1317: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-08 19:20:42,339 EPOCH 1318
2024-02-08 19:20:44,988 [Epoch: 1318 Step: 00044800] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     2560 || Batch Translation Loss:   0.010141 => Txt Tokens per Sec:     7208 || Lr: 0.000050
2024-02-08 19:20:46,463 Epoch 1318: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-08 19:20:46,463 EPOCH 1319
2024-02-08 19:20:51,454 Epoch 1319: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-08 19:20:51,454 EPOCH 1320
2024-02-08 19:20:55,693 Epoch 1320: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-08 19:20:55,693 EPOCH 1321
2024-02-08 19:20:58,580 [Epoch: 1321 Step: 00044900] Batch Recognition Loss:   0.000098 => Gls Tokens per Sec:     2218 || Batch Translation Loss:   0.010743 => Txt Tokens per Sec:     6062 || Lr: 0.000050
2024-02-08 19:21:00,505 Epoch 1321: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.43 
2024-02-08 19:21:00,505 EPOCH 1322
2024-02-08 19:21:05,051 Epoch 1322: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-08 19:21:05,051 EPOCH 1323
2024-02-08 19:21:09,642 Epoch 1323: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-08 19:21:09,643 EPOCH 1324
2024-02-08 19:21:12,170 [Epoch: 1324 Step: 00045000] Batch Recognition Loss:   0.000125 => Gls Tokens per Sec:     2177 || Batch Translation Loss:   0.009635 => Txt Tokens per Sec:     6196 || Lr: 0.000050
2024-02-08 19:21:14,447 Epoch 1324: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-08 19:21:14,448 EPOCH 1325
2024-02-08 19:21:18,680 Epoch 1325: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.45 
2024-02-08 19:21:18,681 EPOCH 1326
2024-02-08 19:21:23,608 Epoch 1326: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-08 19:21:23,609 EPOCH 1327
2024-02-08 19:21:25,660 [Epoch: 1327 Step: 00045100] Batch Recognition Loss:   0.000083 => Gls Tokens per Sec:     2371 || Batch Translation Loss:   0.009958 => Txt Tokens per Sec:     6376 || Lr: 0.000050
2024-02-08 19:21:28,483 Epoch 1327: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-08 19:21:28,484 EPOCH 1328
2024-02-08 19:21:33,432 Epoch 1328: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-08 19:21:33,433 EPOCH 1329
2024-02-08 19:21:37,575 Epoch 1329: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-08 19:21:37,576 EPOCH 1330
2024-02-08 19:21:39,146 [Epoch: 1330 Step: 00045200] Batch Recognition Loss:   0.000166 => Gls Tokens per Sec:     2854 || Batch Translation Loss:   0.031304 => Txt Tokens per Sec:     7545 || Lr: 0.000050
2024-02-08 19:21:42,373 Epoch 1330: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.60 
2024-02-08 19:21:42,373 EPOCH 1331
2024-02-08 19:21:46,873 Epoch 1331: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-08 19:21:46,873 EPOCH 1332
2024-02-08 19:21:51,535 Epoch 1332: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-08 19:21:51,535 EPOCH 1333
2024-02-08 19:21:52,924 [Epoch: 1333 Step: 00045300] Batch Recognition Loss:   0.000096 => Gls Tokens per Sec:     2579 || Batch Translation Loss:   0.013356 => Txt Tokens per Sec:     6568 || Lr: 0.000050
2024-02-08 19:21:56,268 Epoch 1333: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-08 19:21:56,269 EPOCH 1334
2024-02-08 19:22:00,520 Epoch 1334: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-08 19:22:00,521 EPOCH 1335
2024-02-08 19:22:05,474 Epoch 1335: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.16 
2024-02-08 19:22:05,475 EPOCH 1336
2024-02-08 19:22:06,638 [Epoch: 1336 Step: 00045400] Batch Recognition Loss:   0.000189 => Gls Tokens per Sec:     2530 || Batch Translation Loss:   0.032204 => Txt Tokens per Sec:     6941 || Lr: 0.000050
2024-02-08 19:22:09,680 Epoch 1336: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.55 
2024-02-08 19:22:09,680 EPOCH 1337
2024-02-08 19:22:14,654 Epoch 1337: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-08 19:22:14,654 EPOCH 1338
2024-02-08 19:22:19,080 Epoch 1338: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.76 
2024-02-08 19:22:19,080 EPOCH 1339
2024-02-08 19:22:19,805 [Epoch: 1339 Step: 00045500] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     3536 || Batch Translation Loss:   0.018681 => Txt Tokens per Sec:     8359 || Lr: 0.000050
2024-02-08 19:22:23,784 Epoch 1339: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-08 19:22:23,784 EPOCH 1340
2024-02-08 19:22:28,825 Epoch 1340: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-08 19:22:28,825 EPOCH 1341
2024-02-08 19:22:33,089 Epoch 1341: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-08 19:22:33,089 EPOCH 1342
2024-02-08 19:22:33,840 [Epoch: 1342 Step: 00045600] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     2560 || Batch Translation Loss:   0.013003 => Txt Tokens per Sec:     7140 || Lr: 0.000050
2024-02-08 19:22:37,877 Epoch 1342: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-08 19:22:37,878 EPOCH 1343
2024-02-08 19:22:42,418 Epoch 1343: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.70 
2024-02-08 19:22:42,419 EPOCH 1344
2024-02-08 19:22:46,948 Epoch 1344: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-08 19:22:46,948 EPOCH 1345
2024-02-08 19:22:47,416 [Epoch: 1345 Step: 00045700] Batch Recognition Loss:   0.000152 => Gls Tokens per Sec:     2741 || Batch Translation Loss:   0.011402 => Txt Tokens per Sec:     6889 || Lr: 0.000050
2024-02-08 19:22:51,689 Epoch 1345: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-08 19:22:51,689 EPOCH 1346
2024-02-08 19:22:55,922 Epoch 1346: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-08 19:22:55,923 EPOCH 1347
2024-02-08 19:23:01,003 Epoch 1347: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.11 
2024-02-08 19:23:01,003 EPOCH 1348
2024-02-08 19:23:01,307 [Epoch: 1348 Step: 00045800] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:     2112 || Batch Translation Loss:   0.013708 => Txt Tokens per Sec:     6224 || Lr: 0.000050
2024-02-08 19:23:05,187 Epoch 1348: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.75 
2024-02-08 19:23:05,188 EPOCH 1349
2024-02-08 19:23:10,098 Epoch 1349: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-08 19:23:10,098 EPOCH 1350
2024-02-08 19:23:14,493 [Epoch: 1350 Step: 00045900] Batch Recognition Loss:   0.000245 => Gls Tokens per Sec:     2417 || Batch Translation Loss:   0.018587 => Txt Tokens per Sec:     6688 || Lr: 0.000050
2024-02-08 19:23:14,493 Epoch 1350: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-08 19:23:14,493 EPOCH 1351
2024-02-08 19:23:19,164 Epoch 1351: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-08 19:23:19,165 EPOCH 1352
2024-02-08 19:23:23,865 Epoch 1352: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.48 
2024-02-08 19:23:23,865 EPOCH 1353
2024-02-08 19:23:27,990 [Epoch: 1353 Step: 00046000] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     2421 || Batch Translation Loss:   0.017991 => Txt Tokens per Sec:     6763 || Lr: 0.000050
2024-02-08 19:23:36,621 Validation result at epoch 1353, step    46000: duration: 8.6310s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.09474	Translation Loss: 93311.10938	PPL: 11158.15918
	Eval Metric: BLEU
	WER 2.68	(DEL: 0.00,	INS: 0.00,	SUB: 2.68)
	BLEU-4 0.61	(BLEU-1: 10.50,	BLEU-2: 3.05,	BLEU-3: 1.26,	BLEU-4: 0.61)
	CHRF 17.00	ROUGE 8.90
2024-02-08 19:23:36,622 Logging Recognition and Translation Outputs
2024-02-08 19:23:36,622 ========================================================================================================================
2024-02-08 19:23:36,622 Logging Sequence: 87_207.00
2024-02-08 19:23:36,622 	Gloss Reference :	A B+C+D+E
2024-02-08 19:23:36,623 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:23:36,623 	Gloss Alignment :	         
2024-02-08 19:23:36,623 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:23:36,624 	Text Reference  :	**** *********** ** there were    2-3  pakistanis who  were speaking anti-india things and things on *** ****** kashmir
2024-02-08 19:23:36,624 	Text Hypothesis :	this information on been  sharing your story      like pubg call     of         duty   and others on his mobile ipad   
2024-02-08 19:23:36,624 	Text Alignment  :	I    I           I  S     S       S    S          S    S    S        S          S          S         I   I      S      
2024-02-08 19:23:36,625 ========================================================================================================================
2024-02-08 19:23:36,625 Logging Sequence: 67_73.00
2024-02-08 19:23:36,625 	Gloss Reference :	A B+C+D+E
2024-02-08 19:23:36,625 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:23:36,625 	Gloss Alignment :	         
2024-02-08 19:23:36,625 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:23:36,626 	Text Reference  :	*** in    his     tweet he      also said   
2024-02-08 19:23:36,626 	Text Hypothesis :	the match started its   batting and  england
2024-02-08 19:23:36,626 	Text Alignment  :	I   S     S       S     S       S    S      
2024-02-08 19:23:36,626 ========================================================================================================================
2024-02-08 19:23:36,626 Logging Sequence: 172_267.00
2024-02-08 19:23:36,626 	Gloss Reference :	A B+C+D+E
2024-02-08 19:23:36,627 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:23:36,627 	Gloss Alignment :	         
2024-02-08 19:23:36,627 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:23:36,627 	Text Reference  :	*** ***** ********* *** *** such provisions have been made    
2024-02-08 19:23:36,627 	Text Hypothesis :	and media personnel are all wear clothes    will be   lifelong
2024-02-08 19:23:36,627 	Text Alignment  :	I   I     I         I   I   S    S          S    S    S       
2024-02-08 19:23:36,628 ========================================================================================================================
2024-02-08 19:23:36,628 Logging Sequence: 144_23.00
2024-02-08 19:23:36,628 	Gloss Reference :	A B+C+D+E
2024-02-08 19:23:36,628 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:23:36,628 	Gloss Alignment :	         
2024-02-08 19:23:36,628 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:23:36,630 	Text Reference  :	the girl is    14-year-old mumal mehar and she is     from kanasar village of  barmer in  rajasthan
2024-02-08 19:23:36,630 	Text Hypothesis :	*** **** singh also        said  it    was a   screen test before  the     big final  and japan    
2024-02-08 19:23:36,630 	Text Alignment  :	D   D    S     S           S     S     S   S   S      S    S       S       S   S      S   S        
2024-02-08 19:23:36,630 ========================================================================================================================
2024-02-08 19:23:36,630 Logging Sequence: 133_202.00
2024-02-08 19:23:36,630 	Gloss Reference :	A B+C+D+E
2024-02-08 19:23:36,630 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:23:36,631 	Gloss Alignment :	         
2024-02-08 19:23:36,631 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:23:36,632 	Text Reference  :	australia has already qualified for the       final   if   india   wins it  will face australia
2024-02-08 19:23:36,632 	Text Hypothesis :	********* *** ******* ********* *** pakistani batsmen kept scoring run  but lost 5    times    
2024-02-08 19:23:36,632 	Text Alignment  :	D         D   D       D         D   S         S       S    S       S    S   S    S    S        
2024-02-08 19:23:36,632 ========================================================================================================================
2024-02-08 19:23:36,819 Epoch 1353: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-08 19:23:36,820 EPOCH 1354
2024-02-08 19:23:41,954 Epoch 1354: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-08 19:23:41,955 EPOCH 1355
2024-02-08 19:23:46,315 Epoch 1355: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-08 19:23:46,315 EPOCH 1356
2024-02-08 19:23:50,546 [Epoch: 1356 Step: 00046100] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:     2208 || Batch Translation Loss:   0.012875 => Txt Tokens per Sec:     6102 || Lr: 0.000050
2024-02-08 19:23:51,171 Epoch 1356: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.42 
2024-02-08 19:23:51,172 EPOCH 1357
2024-02-08 19:23:56,189 Epoch 1357: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-08 19:23:56,190 EPOCH 1358
2024-02-08 19:24:00,424 Epoch 1358: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-08 19:24:00,424 EPOCH 1359
2024-02-08 19:24:04,249 [Epoch: 1359 Step: 00046200] Batch Recognition Loss:   0.000111 => Gls Tokens per Sec:     2275 || Batch Translation Loss:   0.014535 => Txt Tokens per Sec:     6114 || Lr: 0.000050
2024-02-08 19:24:05,439 Epoch 1359: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-08 19:24:05,439 EPOCH 1360
2024-02-08 19:24:09,754 Epoch 1360: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-08 19:24:09,755 EPOCH 1361
2024-02-08 19:24:14,593 Epoch 1361: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-08 19:24:14,594 EPOCH 1362
2024-02-08 19:24:18,155 [Epoch: 1362 Step: 00046300] Batch Recognition Loss:   0.000906 => Gls Tokens per Sec:     2337 || Batch Translation Loss:   0.015507 => Txt Tokens per Sec:     6494 || Lr: 0.000050
2024-02-08 19:24:19,079 Epoch 1362: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-08 19:24:19,079 EPOCH 1363
2024-02-08 19:24:23,685 Epoch 1363: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-08 19:24:23,686 EPOCH 1364
2024-02-08 19:24:28,378 Epoch 1364: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.60 
2024-02-08 19:24:28,378 EPOCH 1365
2024-02-08 19:24:31,178 [Epoch: 1365 Step: 00046400] Batch Recognition Loss:   0.000102 => Gls Tokens per Sec:     2651 || Batch Translation Loss:   0.015120 => Txt Tokens per Sec:     7268 || Lr: 0.000050
2024-02-08 19:24:32,639 Epoch 1365: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.11 
2024-02-08 19:24:32,639 EPOCH 1366
2024-02-08 19:24:37,668 Epoch 1366: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.42 
2024-02-08 19:24:37,668 EPOCH 1367
2024-02-08 19:24:41,834 Epoch 1367: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-08 19:24:41,834 EPOCH 1368
2024-02-08 19:24:45,231 [Epoch: 1368 Step: 00046500] Batch Recognition Loss:   0.000396 => Gls Tokens per Sec:     1997 || Batch Translation Loss:   0.034719 => Txt Tokens per Sec:     5637 || Lr: 0.000050
2024-02-08 19:24:46,776 Epoch 1368: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-08 19:24:46,776 EPOCH 1369
2024-02-08 19:24:51,512 Epoch 1369: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-08 19:24:51,513 EPOCH 1370
2024-02-08 19:24:56,417 Epoch 1370: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-08 19:24:56,417 EPOCH 1371
2024-02-08 19:24:58,959 [Epoch: 1371 Step: 00046600] Batch Recognition Loss:   0.000175 => Gls Tokens per Sec:     2520 || Batch Translation Loss:   0.013491 => Txt Tokens per Sec:     6885 || Lr: 0.000050
2024-02-08 19:25:01,063 Epoch 1371: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-08 19:25:01,063 EPOCH 1372
2024-02-08 19:25:05,920 Epoch 1372: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-08 19:25:05,921 EPOCH 1373
2024-02-08 19:25:10,601 Epoch 1373: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.60 
2024-02-08 19:25:10,602 EPOCH 1374
2024-02-08 19:25:13,448 [Epoch: 1374 Step: 00046700] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     2024 || Batch Translation Loss:   0.034850 => Txt Tokens per Sec:     5779 || Lr: 0.000050
2024-02-08 19:25:15,614 Epoch 1374: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-08 19:25:15,615 EPOCH 1375
2024-02-08 19:25:20,216 Epoch 1375: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-08 19:25:20,216 EPOCH 1376
2024-02-08 19:25:25,246 Epoch 1376: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-08 19:25:25,247 EPOCH 1377
2024-02-08 19:25:27,246 [Epoch: 1377 Step: 00046800] Batch Recognition Loss:   0.000209 => Gls Tokens per Sec:     2563 || Batch Translation Loss:   0.012766 => Txt Tokens per Sec:     6712 || Lr: 0.000050
2024-02-08 19:25:30,025 Epoch 1377: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-08 19:25:30,025 EPOCH 1378
2024-02-08 19:25:34,838 Epoch 1378: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.60 
2024-02-08 19:25:34,839 EPOCH 1379
2024-02-08 19:25:39,780 Epoch 1379: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.87 
2024-02-08 19:25:39,780 EPOCH 1380
2024-02-08 19:25:41,406 [Epoch: 1380 Step: 00046900] Batch Recognition Loss:   0.000163 => Gls Tokens per Sec:     2759 || Batch Translation Loss:   0.015791 => Txt Tokens per Sec:     7426 || Lr: 0.000050
2024-02-08 19:25:44,467 Epoch 1380: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.09 
2024-02-08 19:25:44,467 EPOCH 1381
2024-02-08 19:25:49,535 Epoch 1381: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.34 
2024-02-08 19:25:49,535 EPOCH 1382
2024-02-08 19:25:54,307 Epoch 1382: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.15 
2024-02-08 19:25:54,307 EPOCH 1383
2024-02-08 19:25:55,939 [Epoch: 1383 Step: 00047000] Batch Recognition Loss:   0.000208 => Gls Tokens per Sec:     2355 || Batch Translation Loss:   0.036428 => Txt Tokens per Sec:     6667 || Lr: 0.000050
2024-02-08 19:25:59,267 Epoch 1383: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.18 
2024-02-08 19:25:59,268 EPOCH 1384
2024-02-08 19:26:04,267 Epoch 1384: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.15 
2024-02-08 19:26:04,268 EPOCH 1385
2024-02-08 19:26:09,099 Epoch 1385: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-08 19:26:09,100 EPOCH 1386
2024-02-08 19:26:10,493 [Epoch: 1386 Step: 00047100] Batch Recognition Loss:   0.000326 => Gls Tokens per Sec:     2299 || Batch Translation Loss:   0.025783 => Txt Tokens per Sec:     6394 || Lr: 0.000050
2024-02-08 19:26:13,961 Epoch 1386: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.01 
2024-02-08 19:26:13,961 EPOCH 1387
2024-02-08 19:26:18,565 Epoch 1387: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.73 
2024-02-08 19:26:18,565 EPOCH 1388
2024-02-08 19:26:23,426 Epoch 1388: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-08 19:26:23,427 EPOCH 1389
2024-02-08 19:26:24,618 [Epoch: 1389 Step: 00047200] Batch Recognition Loss:   0.000540 => Gls Tokens per Sec:     2153 || Batch Translation Loss:   0.029464 => Txt Tokens per Sec:     5575 || Lr: 0.000050
2024-02-08 19:26:27,790 Epoch 1389: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-08 19:26:27,790 EPOCH 1390
2024-02-08 19:26:32,414 Epoch 1390: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.03 
2024-02-08 19:26:32,414 EPOCH 1391
2024-02-08 19:26:37,142 Epoch 1391: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-08 19:26:37,142 EPOCH 1392
2024-02-08 19:26:38,051 [Epoch: 1392 Step: 00047300] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     2115 || Batch Translation Loss:   0.022652 => Txt Tokens per Sec:     6304 || Lr: 0.000050
2024-02-08 19:26:41,247 Epoch 1392: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-08 19:26:41,247 EPOCH 1393
2024-02-08 19:26:46,109 Epoch 1393: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-08 19:26:46,109 EPOCH 1394
2024-02-08 19:26:50,497 Epoch 1394: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-08 19:26:50,497 EPOCH 1395
2024-02-08 19:26:50,814 [Epoch: 1395 Step: 00047400] Batch Recognition Loss:   0.000311 => Gls Tokens per Sec:     3228 || Batch Translation Loss:   0.002954 => Txt Tokens per Sec:     6627 || Lr: 0.000050
2024-02-08 19:26:55,204 Epoch 1395: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-08 19:26:55,205 EPOCH 1396
2024-02-08 19:26:59,845 Epoch 1396: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-08 19:26:59,845 EPOCH 1397
2024-02-08 19:27:04,170 Epoch 1397: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-08 19:27:04,171 EPOCH 1398
2024-02-08 19:27:04,484 [Epoch: 1398 Step: 00047500] Batch Recognition Loss:   0.000373 => Gls Tokens per Sec:     2058 || Batch Translation Loss:   0.014356 => Txt Tokens per Sec:     5826 || Lr: 0.000050
2024-02-08 19:27:09,096 Epoch 1398: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-08 19:27:09,096 EPOCH 1399
2024-02-08 19:27:13,260 Epoch 1399: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-08 19:27:13,260 EPOCH 1400
2024-02-08 19:27:18,305 [Epoch: 1400 Step: 00047600] Batch Recognition Loss:   0.000279 => Gls Tokens per Sec:     2106 || Batch Translation Loss:   0.005418 => Txt Tokens per Sec:     5826 || Lr: 0.000050
2024-02-08 19:27:18,305 Epoch 1400: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-08 19:27:18,305 EPOCH 1401
2024-02-08 19:27:22,636 Epoch 1401: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-08 19:27:22,636 EPOCH 1402
2024-02-08 19:27:27,380 Epoch 1402: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-08 19:27:27,380 EPOCH 1403
2024-02-08 19:27:31,616 [Epoch: 1403 Step: 00047700] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:     2358 || Batch Translation Loss:   0.016478 => Txt Tokens per Sec:     6474 || Lr: 0.000050
2024-02-08 19:27:31,916 Epoch 1403: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-08 19:27:31,916 EPOCH 1404
2024-02-08 19:27:36,507 Epoch 1404: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-08 19:27:36,507 EPOCH 1405
2024-02-08 19:27:41,265 Epoch 1405: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-08 19:27:41,265 EPOCH 1406
2024-02-08 19:27:44,978 [Epoch: 1406 Step: 00047800] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     2516 || Batch Translation Loss:   0.010733 => Txt Tokens per Sec:     6986 || Lr: 0.000050
2024-02-08 19:27:45,459 Epoch 1406: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-08 19:27:45,459 EPOCH 1407
2024-02-08 19:27:50,416 Epoch 1407: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.45 
2024-02-08 19:27:50,417 EPOCH 1408
2024-02-08 19:27:54,635 Epoch 1408: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-08 19:27:54,635 EPOCH 1409
2024-02-08 19:27:58,590 [Epoch: 1409 Step: 00047900] Batch Recognition Loss:   0.000423 => Gls Tokens per Sec:     2266 || Batch Translation Loss:   0.004289 => Txt Tokens per Sec:     6161 || Lr: 0.000050
2024-02-08 19:27:59,564 Epoch 1409: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-08 19:27:59,564 EPOCH 1410
2024-02-08 19:28:04,009 Epoch 1410: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-08 19:28:04,009 EPOCH 1411
2024-02-08 19:28:08,397 Epoch 1411: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-08 19:28:08,397 EPOCH 1412
2024-02-08 19:28:12,133 [Epoch: 1412 Step: 00048000] Batch Recognition Loss:   0.000105 => Gls Tokens per Sec:     2158 || Batch Translation Loss:   0.024362 => Txt Tokens per Sec:     6008 || Lr: 0.000050
2024-02-08 19:28:21,049 Validation result at epoch 1412, step    48000: duration: 8.9155s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.16978	Translation Loss: 94163.46094	PPL: 12149.70020
	Eval Metric: BLEU
	WER 2.68	(DEL: 0.00,	INS: 0.00,	SUB: 2.68)
	BLEU-4 0.52	(BLEU-1: 9.79,	BLEU-2: 2.75,	BLEU-3: 1.08,	BLEU-4: 0.52)
	CHRF 16.62	ROUGE 8.28
2024-02-08 19:28:21,050 Logging Recognition and Translation Outputs
2024-02-08 19:28:21,050 ========================================================================================================================
2024-02-08 19:28:21,050 Logging Sequence: 96_93.00
2024-02-08 19:28:21,051 	Gloss Reference :	A B+C+D+E
2024-02-08 19:28:21,051 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:28:21,051 	Gloss Alignment :	         
2024-02-08 19:28:21,051 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:28:21,052 	Text Reference  :	bhuvneshwar kumar took 4   wickets and    hardik pandya took 3    wickets  wonderful
2024-02-08 19:28:21,052 	Text Hypothesis :	*********** this  is   why the     finals were   played well with pakistan cricket  
2024-02-08 19:28:21,053 	Text Alignment  :	D           S     S    S   S       S      S      S      S    S    S        S        
2024-02-08 19:28:21,053 ========================================================================================================================
2024-02-08 19:28:21,053 Logging Sequence: 144_2.00
2024-02-08 19:28:21,053 	Gloss Reference :	A B+C+D+E  
2024-02-08 19:28:21,053 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-08 19:28:21,053 	Gloss Alignment :	  S        
2024-02-08 19:28:21,053 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:28:21,055 	Text Reference  :	a girl posted a     video of herself  playing cricket on  a        village      farm    on social media the video has gone viral
2024-02-08 19:28:21,055 	Text Hypothesis :	* **** ****** after being in lockdown for     months  the football championship started on ****** ***** *** ***** *** 11th june 
2024-02-08 19:28:21,055 	Text Alignment  :	D D    D      S     S     S  S        S       S       S   S        S            S          D      D     D   D     D   S    S    
2024-02-08 19:28:21,056 ========================================================================================================================
2024-02-08 19:28:21,056 Logging Sequence: 178_83.00
2024-02-08 19:28:21,056 	Gloss Reference :	A B+C+D+E
2024-02-08 19:28:21,056 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:28:21,056 	Gloss Alignment :	         
2024-02-08 19:28:21,056 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:28:21,057 	Text Reference  :	and the   police    still haven't     apprehended the wrestler  
2024-02-08 19:28:21,057 	Text Hypothesis :	and media personnel are   immediately after       the tournament
2024-02-08 19:28:21,057 	Text Alignment  :	    S     S         S     S           S               S         
2024-02-08 19:28:21,057 ========================================================================================================================
2024-02-08 19:28:21,057 Logging Sequence: 169_214.00
2024-02-08 19:28:21,058 	Gloss Reference :	A B+C+D+E
2024-02-08 19:28:21,058 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:28:21,058 	Gloss Alignment :	         
2024-02-08 19:28:21,058 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:28:21,060 	Text Reference  :	virat kohli said that though arshdeep dropped the       catch    he          is still  a    strong part   of  the    indian     team
2024-02-08 19:28:21,060 	Text Hypothesis :	***** you   know that ****** ******** ******* wikipedia provides information on celebs like their  height age family background etc 
2024-02-08 19:28:21,060 	Text Alignment  :	D     S     S         D      D        D       S         S        S           S  S      S    S      S      S   S      S          S   
2024-02-08 19:28:21,060 ========================================================================================================================
2024-02-08 19:28:21,060 Logging Sequence: 147_202.00
2024-02-08 19:28:21,060 	Gloss Reference :	A B+C+D+E
2024-02-08 19:28:21,061 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:28:21,061 	Gloss Alignment :	         
2024-02-08 19:28:21,061 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:28:21,063 	Text Reference  :	*** ******* ******* were **** impressed   that she  took    the difficult decision to withdraw from the olympics and focus on   her  mental health
2024-02-08 19:28:21,063 	Text Hypothesis :	new zealand players were seen celebrating and  then lifting the ********* ******** ** trophy   for  the ******** *** first time have a      look  
2024-02-08 19:28:21,063 	Text Alignment  :	I   I       I            I    S           S    S    S           D         D        D  S        S        D        D   S     S    S    S      S     
2024-02-08 19:28:21,063 ========================================================================================================================
2024-02-08 19:28:22,221 Epoch 1412: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.34 
2024-02-08 19:28:22,221 EPOCH 1413
2024-02-08 19:28:26,895 Epoch 1413: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.53 
2024-02-08 19:28:26,895 EPOCH 1414
2024-02-08 19:28:31,812 Epoch 1414: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.51 
2024-02-08 19:28:31,812 EPOCH 1415
2024-02-08 19:28:35,038 [Epoch: 1415 Step: 00048100] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     2301 || Batch Translation Loss:   0.020099 => Txt Tokens per Sec:     6323 || Lr: 0.000050
2024-02-08 19:28:36,564 Epoch 1415: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.70 
2024-02-08 19:28:36,564 EPOCH 1416
2024-02-08 19:28:41,356 Epoch 1416: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-08 19:28:41,357 EPOCH 1417
2024-02-08 19:28:45,804 Epoch 1417: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.87 
2024-02-08 19:28:45,804 EPOCH 1418
2024-02-08 19:28:48,648 [Epoch: 1418 Step: 00048200] Batch Recognition Loss:   0.000588 => Gls Tokens per Sec:     2386 || Batch Translation Loss:   0.023748 => Txt Tokens per Sec:     6642 || Lr: 0.000050
2024-02-08 19:28:50,257 Epoch 1418: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-08 19:28:50,258 EPOCH 1419
2024-02-08 19:28:55,169 Epoch 1419: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.77 
2024-02-08 19:28:55,170 EPOCH 1420
2024-02-08 19:28:59,285 Epoch 1420: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-08 19:28:59,286 EPOCH 1421
2024-02-08 19:29:02,373 [Epoch: 1421 Step: 00048300] Batch Recognition Loss:   0.000312 => Gls Tokens per Sec:     1990 || Batch Translation Loss:   0.012165 => Txt Tokens per Sec:     5640 || Lr: 0.000050
2024-02-08 19:29:04,312 Epoch 1421: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-08 19:29:04,312 EPOCH 1422
2024-02-08 19:29:08,582 Epoch 1422: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-08 19:29:08,582 EPOCH 1423
2024-02-08 19:29:13,443 Epoch 1423: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-08 19:29:13,444 EPOCH 1424
2024-02-08 19:29:15,365 [Epoch: 1424 Step: 00048400] Batch Recognition Loss:   0.000107 => Gls Tokens per Sec:     3000 || Batch Translation Loss:   0.009272 => Txt Tokens per Sec:     7729 || Lr: 0.000050
2024-02-08 19:29:17,928 Epoch 1424: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-08 19:29:17,928 EPOCH 1425
2024-02-08 19:29:22,501 Epoch 1425: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-08 19:29:22,501 EPOCH 1426
2024-02-08 19:29:27,211 Epoch 1426: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-08 19:29:27,211 EPOCH 1427
2024-02-08 19:29:29,294 [Epoch: 1427 Step: 00048500] Batch Recognition Loss:   0.000509 => Gls Tokens per Sec:     2459 || Batch Translation Loss:   0.021151 => Txt Tokens per Sec:     6839 || Lr: 0.000050
2024-02-08 19:29:31,460 Epoch 1427: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-08 19:29:31,460 EPOCH 1428
2024-02-08 19:29:36,472 Epoch 1428: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-08 19:29:36,472 EPOCH 1429
2024-02-08 19:29:40,691 Epoch 1429: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-08 19:29:40,691 EPOCH 1430
2024-02-08 19:29:42,639 [Epoch: 1430 Step: 00048600] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     2302 || Batch Translation Loss:   0.010753 => Txt Tokens per Sec:     6524 || Lr: 0.000050
2024-02-08 19:29:45,579 Epoch 1430: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-08 19:29:45,579 EPOCH 1431
2024-02-08 19:29:49,958 Epoch 1431: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-08 19:29:49,958 EPOCH 1432
2024-02-08 19:29:54,679 Epoch 1432: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.22 
2024-02-08 19:29:54,679 EPOCH 1433
2024-02-08 19:29:56,312 [Epoch: 1433 Step: 00048700] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     2196 || Batch Translation Loss:   0.019562 => Txt Tokens per Sec:     6170 || Lr: 0.000050
2024-02-08 19:29:59,375 Epoch 1433: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.96 
2024-02-08 19:29:59,376 EPOCH 1434
2024-02-08 19:30:03,774 Epoch 1434: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.60 
2024-02-08 19:30:03,775 EPOCH 1435
2024-02-08 19:30:08,677 Epoch 1435: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-08 19:30:08,677 EPOCH 1436
2024-02-08 19:30:09,765 [Epoch: 1436 Step: 00048800] Batch Recognition Loss:   0.000163 => Gls Tokens per Sec:     2944 || Batch Translation Loss:   0.013489 => Txt Tokens per Sec:     7946 || Lr: 0.000050
2024-02-08 19:30:12,756 Epoch 1436: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-08 19:30:12,756 EPOCH 1437
2024-02-08 19:30:17,869 Epoch 1437: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-08 19:30:17,870 EPOCH 1438
2024-02-08 19:30:22,204 Epoch 1438: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-08 19:30:22,204 EPOCH 1439
2024-02-08 19:30:23,214 [Epoch: 1439 Step: 00048900] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     2540 || Batch Translation Loss:   0.020496 => Txt Tokens per Sec:     6428 || Lr: 0.000050
2024-02-08 19:30:26,988 Epoch 1439: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.77 
2024-02-08 19:30:26,988 EPOCH 1440
2024-02-08 19:30:31,475 Epoch 1440: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-08 19:30:31,476 EPOCH 1441
2024-02-08 19:30:35,806 Epoch 1441: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-08 19:30:35,807 EPOCH 1442
2024-02-08 19:30:36,700 [Epoch: 1442 Step: 00049000] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     2152 || Batch Translation Loss:   0.026622 => Txt Tokens per Sec:     6026 || Lr: 0.000050
2024-02-08 19:30:40,789 Epoch 1442: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-08 19:30:40,790 EPOCH 1443
2024-02-08 19:30:45,011 Epoch 1443: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-08 19:30:45,011 EPOCH 1444
2024-02-08 19:30:49,991 Epoch 1444: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-08 19:30:49,991 EPOCH 1445
2024-02-08 19:30:50,668 [Epoch: 1445 Step: 00049100] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     1893 || Batch Translation Loss:   0.012184 => Txt Tokens per Sec:     5907 || Lr: 0.000050
2024-02-08 19:30:54,314 Epoch 1445: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-08 19:30:54,314 EPOCH 1446
2024-02-08 19:30:59,106 Epoch 1446: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-08 19:30:59,107 EPOCH 1447
2024-02-08 19:31:03,707 Epoch 1447: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-08 19:31:03,707 EPOCH 1448
2024-02-08 19:31:03,882 [Epoch: 1448 Step: 00049200] Batch Recognition Loss:   0.004963 => Gls Tokens per Sec:     3678 || Batch Translation Loss:   0.030850 => Txt Tokens per Sec:     9069 || Lr: 0.000050
2024-02-08 19:31:08,172 Epoch 1448: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.05 
2024-02-08 19:31:08,173 EPOCH 1449
2024-02-08 19:31:12,998 Epoch 1449: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.33 
2024-02-08 19:31:12,998 EPOCH 1450
2024-02-08 19:31:17,075 [Epoch: 1450 Step: 00049300] Batch Recognition Loss:   0.000678 => Gls Tokens per Sec:     2605 || Batch Translation Loss:   0.086281 => Txt Tokens per Sec:     7208 || Lr: 0.000050
2024-02-08 19:31:17,076 Epoch 1450: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.87 
2024-02-08 19:31:17,076 EPOCH 1451
2024-02-08 19:31:22,186 Epoch 1451: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.53 
2024-02-08 19:31:22,186 EPOCH 1452
2024-02-08 19:31:26,481 Epoch 1452: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.94 
2024-02-08 19:31:26,481 EPOCH 1453
2024-02-08 19:31:30,292 [Epoch: 1453 Step: 00049400] Batch Recognition Loss:   0.000157 => Gls Tokens per Sec:     2619 || Batch Translation Loss:   0.017132 => Txt Tokens per Sec:     7202 || Lr: 0.000050
2024-02-08 19:31:30,590 Epoch 1453: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-08 19:31:30,590 EPOCH 1454
2024-02-08 19:31:34,680 Epoch 1454: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-08 19:31:34,680 EPOCH 1455
2024-02-08 19:31:38,781 Epoch 1455: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-08 19:31:38,781 EPOCH 1456
2024-02-08 19:31:43,149 [Epoch: 1456 Step: 00049500] Batch Recognition Loss:   0.000224 => Gls Tokens per Sec:     2139 || Batch Translation Loss:   0.021859 => Txt Tokens per Sec:     5838 || Lr: 0.000050
2024-02-08 19:31:43,766 Epoch 1456: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.76 
2024-02-08 19:31:43,766 EPOCH 1457
2024-02-08 19:31:48,240 Epoch 1457: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.04 
2024-02-08 19:31:48,240 EPOCH 1458
2024-02-08 19:31:53,306 Epoch 1458: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.87 
2024-02-08 19:31:53,306 EPOCH 1459
2024-02-08 19:31:57,439 [Epoch: 1459 Step: 00049600] Batch Recognition Loss:   0.000213 => Gls Tokens per Sec:     2106 || Batch Translation Loss:   0.024004 => Txt Tokens per Sec:     6030 || Lr: 0.000050
2024-02-08 19:31:57,961 Epoch 1459: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-08 19:31:57,961 EPOCH 1460
2024-02-08 19:32:02,915 Epoch 1460: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-08 19:32:02,916 EPOCH 1461
2024-02-08 19:32:07,745 Epoch 1461: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.99 
2024-02-08 19:32:07,745 EPOCH 1462
2024-02-08 19:32:11,159 [Epoch: 1462 Step: 00049700] Batch Recognition Loss:   0.000892 => Gls Tokens per Sec:     2362 || Batch Translation Loss:   0.039248 => Txt Tokens per Sec:     6336 || Lr: 0.000050
2024-02-08 19:32:12,542 Epoch 1462: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.13 
2024-02-08 19:32:12,542 EPOCH 1463
2024-02-08 19:32:17,493 Epoch 1463: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-08 19:32:17,494 EPOCH 1464
2024-02-08 19:32:21,650 Epoch 1464: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.64 
2024-02-08 19:32:21,650 EPOCH 1465
2024-02-08 19:32:24,496 [Epoch: 1465 Step: 00049800] Batch Recognition Loss:   0.000420 => Gls Tokens per Sec:     2608 || Batch Translation Loss:   0.052872 => Txt Tokens per Sec:     7131 || Lr: 0.000050
2024-02-08 19:32:26,099 Epoch 1465: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.02 
2024-02-08 19:32:26,100 EPOCH 1466
2024-02-08 19:32:30,950 Epoch 1466: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.90 
2024-02-08 19:32:30,950 EPOCH 1467
2024-02-08 19:32:35,073 Epoch 1467: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.99 
2024-02-08 19:32:35,074 EPOCH 1468
2024-02-08 19:32:37,940 [Epoch: 1468 Step: 00049900] Batch Recognition Loss:   0.000533 => Gls Tokens per Sec:     2457 || Batch Translation Loss:   0.053699 => Txt Tokens per Sec:     6839 || Lr: 0.000050
2024-02-08 19:32:39,651 Epoch 1468: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.58 
2024-02-08 19:32:39,651 EPOCH 1469
2024-02-08 19:32:44,355 Epoch 1469: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.99 
2024-02-08 19:32:44,355 EPOCH 1470
2024-02-08 19:32:48,651 Epoch 1470: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-08 19:32:48,652 EPOCH 1471
2024-02-08 19:32:51,532 [Epoch: 1471 Step: 00050000] Batch Recognition Loss:   0.000387 => Gls Tokens per Sec:     2133 || Batch Translation Loss:   0.028416 => Txt Tokens per Sec:     5799 || Lr: 0.000050
2024-02-08 19:33:00,182 Validation result at epoch 1471, step    50000: duration: 8.6500s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.19205	Translation Loss: 94714.31250	PPL: 12836.90039
	Eval Metric: BLEU
	WER 3.04	(DEL: 0.00,	INS: 0.00,	SUB: 3.04)
	BLEU-4 0.31	(BLEU-1: 9.88,	BLEU-2: 2.67,	BLEU-3: 0.86,	BLEU-4: 0.31)
	CHRF 16.71	ROUGE 8.01
2024-02-08 19:33:00,183 Logging Recognition and Translation Outputs
2024-02-08 19:33:00,183 ========================================================================================================================
2024-02-08 19:33:00,183 Logging Sequence: 178_157.00
2024-02-08 19:33:00,184 	Gloss Reference :	A B+C+D+E
2024-02-08 19:33:00,184 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:33:00,184 	Gloss Alignment :	         
2024-02-08 19:33:00,184 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:33:00,185 	Text Reference  :	this is   why sushil kumar will have to       be      arrested 
2024-02-08 19:33:00,185 	Text Hypothesis :	**** that her win    had   a    huge argument between argentina
2024-02-08 19:33:00,185 	Text Alignment  :	D    S    S   S      S     S    S    S        S       S        
2024-02-08 19:33:00,185 ========================================================================================================================
2024-02-08 19:33:00,186 Logging Sequence: 118_111.00
2024-02-08 19:33:00,186 	Gloss Reference :	A B+C+D+E
2024-02-08 19:33:00,186 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:33:00,186 	Gloss Alignment :	         
2024-02-08 19:33:00,186 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:33:00,187 	Text Reference  :	and people encourage him have hope for the  next      world cup 
2024-02-08 19:33:00,187 	Text Hypothesis :	*** people ********* *** **** **** *** were jubiliant over  this
2024-02-08 19:33:00,187 	Text Alignment  :	D          D         D   D    D    D   S    S         S     S   
2024-02-08 19:33:00,187 ========================================================================================================================
2024-02-08 19:33:00,188 Logging Sequence: 148_2.00
2024-02-08 19:33:00,188 	Gloss Reference :	A B+C+D+E
2024-02-08 19:33:00,188 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:33:00,188 	Gloss Alignment :	         
2024-02-08 19:33:00,188 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:33:00,190 	Text Reference  :	the final of the asia  cup 2023 cricket tournament was played between india       and ******** sri   lanka on 17th  september   2023     
2024-02-08 19:33:00,190 	Text Hypothesis :	*** ***** ** the world cup will held    in         uae deaf   cricket association and pakistan could score of royal challengers bangalore
2024-02-08 19:33:00,191 	Text Alignment  :	D   D     D      S         S    S       S          S   S      S       S               I        S     S     S  S     S           S        
2024-02-08 19:33:00,191 ========================================================================================================================
2024-02-08 19:33:00,191 Logging Sequence: 83_129.00
2024-02-08 19:33:00,191 	Gloss Reference :	A B+C+D+E
2024-02-08 19:33:00,191 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:33:00,191 	Gloss Alignment :	         
2024-02-08 19:33:00,191 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:33:00,192 	Text Reference  :	later the denmark football association tweeted
2024-02-08 19:33:00,192 	Text Hypothesis :	***** a   new     zealand  went        viral  
2024-02-08 19:33:00,192 	Text Alignment  :	D     S   S       S        S           S      
2024-02-08 19:33:00,192 ========================================================================================================================
2024-02-08 19:33:00,192 Logging Sequence: 99_158.00
2024-02-08 19:33:00,192 	Gloss Reference :	A B+C+D+E
2024-02-08 19:33:00,193 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:33:00,193 	Gloss Alignment :	         
2024-02-08 19:33:00,193 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:33:00,194 	Text Reference  :	*** the  incident occured in  dubai and  it **** was extremely shameful  
2024-02-08 19:33:00,194 	Text Hypothesis :	his fans want     to      see him   play it will be  very      protective
2024-02-08 19:33:00,194 	Text Alignment  :	I   S    S        S       S   S     S       I    S   S         S         
2024-02-08 19:33:00,194 ========================================================================================================================
2024-02-08 19:33:02,365 Epoch 1471: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.76 
2024-02-08 19:33:02,365 EPOCH 1472
2024-02-08 19:33:07,205 Epoch 1472: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-08 19:33:07,206 EPOCH 1473
2024-02-08 19:33:12,140 Epoch 1473: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-08 19:33:12,140 EPOCH 1474
2024-02-08 19:33:15,023 [Epoch: 1474 Step: 00050100] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     1908 || Batch Translation Loss:   0.012436 => Txt Tokens per Sec:     5605 || Lr: 0.000050
2024-02-08 19:33:16,984 Epoch 1474: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.60 
2024-02-08 19:33:16,984 EPOCH 1475
2024-02-08 19:33:21,290 Epoch 1475: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-08 19:33:21,291 EPOCH 1476
2024-02-08 19:33:26,311 Epoch 1476: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.82 
2024-02-08 19:33:26,311 EPOCH 1477
2024-02-08 19:33:28,405 [Epoch: 1477 Step: 00050200] Batch Recognition Loss:   0.000348 => Gls Tokens per Sec:     2446 || Batch Translation Loss:   0.011757 => Txt Tokens per Sec:     6722 || Lr: 0.000050
2024-02-08 19:33:30,516 Epoch 1477: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-08 19:33:30,517 EPOCH 1478
2024-02-08 19:33:35,755 Epoch 1478: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-08 19:33:35,756 EPOCH 1479
2024-02-08 19:33:40,488 Epoch 1479: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-08 19:33:40,488 EPOCH 1480
2024-02-08 19:33:42,564 [Epoch: 1480 Step: 00050300] Batch Recognition Loss:   0.000180 => Gls Tokens per Sec:     2160 || Batch Translation Loss:   0.015660 => Txt Tokens per Sec:     6490 || Lr: 0.000050
2024-02-08 19:33:44,741 Epoch 1480: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-08 19:33:44,742 EPOCH 1481
2024-02-08 19:33:49,736 Epoch 1481: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-08 19:33:49,736 EPOCH 1482
2024-02-08 19:33:53,955 Epoch 1482: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-08 19:33:53,955 EPOCH 1483
2024-02-08 19:33:56,103 [Epoch: 1483 Step: 00050400] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     1789 || Batch Translation Loss:   0.010691 => Txt Tokens per Sec:     5456 || Lr: 0.000050
2024-02-08 19:33:58,869 Epoch 1483: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.65 
2024-02-08 19:33:58,869 EPOCH 1484
2024-02-08 19:34:03,274 Epoch 1484: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-08 19:34:03,274 EPOCH 1485
2024-02-08 19:34:07,930 Epoch 1485: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-08 19:34:07,931 EPOCH 1486
2024-02-08 19:34:09,354 [Epoch: 1486 Step: 00050500] Batch Recognition Loss:   0.000208 => Gls Tokens per Sec:     2250 || Batch Translation Loss:   0.017307 => Txt Tokens per Sec:     6196 || Lr: 0.000050
2024-02-08 19:34:12,554 Epoch 1486: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-08 19:34:12,555 EPOCH 1487
2024-02-08 19:34:16,667 Epoch 1487: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-08 19:34:16,668 EPOCH 1488
2024-02-08 19:34:21,718 Epoch 1488: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-08 19:34:21,718 EPOCH 1489
2024-02-08 19:34:22,647 [Epoch: 1489 Step: 00050600] Batch Recognition Loss:   0.000319 => Gls Tokens per Sec:     2478 || Batch Translation Loss:   0.016481 => Txt Tokens per Sec:     6913 || Lr: 0.000050
2024-02-08 19:34:26,026 Epoch 1489: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-08 19:34:26,026 EPOCH 1490
2024-02-08 19:34:30,843 Epoch 1490: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-08 19:34:30,844 EPOCH 1491
2024-02-08 19:34:35,398 Epoch 1491: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-08 19:34:35,398 EPOCH 1492
2024-02-08 19:34:36,105 [Epoch: 1492 Step: 00050700] Batch Recognition Loss:   0.000350 => Gls Tokens per Sec:     2717 || Batch Translation Loss:   0.012045 => Txt Tokens per Sec:     7586 || Lr: 0.000050
2024-02-08 19:34:39,478 Epoch 1492: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-08 19:34:39,478 EPOCH 1493
2024-02-08 19:34:44,422 Epoch 1493: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-08 19:34:44,423 EPOCH 1494
2024-02-08 19:34:48,837 Epoch 1494: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-08 19:34:48,837 EPOCH 1495
2024-02-08 19:34:49,286 [Epoch: 1495 Step: 00050800] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:     2857 || Batch Translation Loss:   0.012383 => Txt Tokens per Sec:     7826 || Lr: 0.000050
2024-02-08 19:34:53,598 Epoch 1495: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.77 
2024-02-08 19:34:53,598 EPOCH 1496
2024-02-08 19:34:58,206 Epoch 1496: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-08 19:34:58,206 EPOCH 1497
2024-02-08 19:35:02,730 Epoch 1497: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-08 19:35:02,730 EPOCH 1498
2024-02-08 19:35:03,099 [Epoch: 1498 Step: 00050900] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:     1739 || Batch Translation Loss:   0.012467 => Txt Tokens per Sec:     5171 || Lr: 0.000050
2024-02-08 19:35:07,551 Epoch 1498: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-08 19:35:07,551 EPOCH 1499
2024-02-08 19:35:11,690 Epoch 1499: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-08 19:35:11,690 EPOCH 1500
2024-02-08 19:35:16,590 [Epoch: 1500 Step: 00051000] Batch Recognition Loss:   0.000467 => Gls Tokens per Sec:     2168 || Batch Translation Loss:   0.011121 => Txt Tokens per Sec:     5999 || Lr: 0.000050
2024-02-08 19:35:16,590 Epoch 1500: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-08 19:35:16,590 EPOCH 1501
2024-02-08 19:35:21,065 Epoch 1501: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-08 19:35:21,065 EPOCH 1502
2024-02-08 19:35:25,711 Epoch 1502: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.56 
2024-02-08 19:35:25,711 EPOCH 1503
2024-02-08 19:35:30,102 [Epoch: 1503 Step: 00051100] Batch Recognition Loss:   0.000515 => Gls Tokens per Sec:     2274 || Batch Translation Loss:   0.021364 => Txt Tokens per Sec:     6246 || Lr: 0.000050
2024-02-08 19:35:30,382 Epoch 1503: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-08 19:35:30,382 EPOCH 1504
2024-02-08 19:35:34,715 Epoch 1504: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.67 
2024-02-08 19:35:34,716 EPOCH 1505
2024-02-08 19:35:39,703 Epoch 1505: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.35 
2024-02-08 19:35:39,703 EPOCH 1506
2024-02-08 19:35:43,528 [Epoch: 1506 Step: 00051200] Batch Recognition Loss:   0.000317 => Gls Tokens per Sec:     2442 || Batch Translation Loss:   0.091014 => Txt Tokens per Sec:     6888 || Lr: 0.000050
2024-02-08 19:35:43,855 Epoch 1506: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.39 
2024-02-08 19:35:43,855 EPOCH 1507
2024-02-08 19:35:48,812 Epoch 1507: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.44 
2024-02-08 19:35:48,813 EPOCH 1508
2024-02-08 19:35:53,139 Epoch 1508: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.07 
2024-02-08 19:35:53,139 EPOCH 1509
2024-02-08 19:35:56,521 [Epoch: 1509 Step: 00051300] Batch Recognition Loss:   0.000294 => Gls Tokens per Sec:     2573 || Batch Translation Loss:   0.005084 => Txt Tokens per Sec:     7110 || Lr: 0.000050
2024-02-08 19:35:57,235 Epoch 1509: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-08 19:35:57,235 EPOCH 1510
2024-02-08 19:36:01,350 Epoch 1510: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-08 19:36:01,350 EPOCH 1511
2024-02-08 19:36:06,457 Epoch 1511: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-08 19:36:06,458 EPOCH 1512
2024-02-08 19:36:09,712 [Epoch: 1512 Step: 00051400] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     2557 || Batch Translation Loss:   0.012331 => Txt Tokens per Sec:     7034 || Lr: 0.000050
2024-02-08 19:36:10,684 Epoch 1512: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-08 19:36:10,684 EPOCH 1513
2024-02-08 19:36:15,484 Epoch 1513: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-08 19:36:15,485 EPOCH 1514
2024-02-08 19:36:20,011 Epoch 1514: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-08 19:36:20,011 EPOCH 1515
2024-02-08 19:36:22,995 [Epoch: 1515 Step: 00051500] Batch Recognition Loss:   0.000250 => Gls Tokens per Sec:     2487 || Batch Translation Loss:   0.021064 => Txt Tokens per Sec:     7087 || Lr: 0.000050
2024-02-08 19:36:24,605 Epoch 1515: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.93 
2024-02-08 19:36:24,605 EPOCH 1516
2024-02-08 19:36:29,290 Epoch 1516: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.87 
2024-02-08 19:36:29,290 EPOCH 1517
2024-02-08 19:36:33,614 Epoch 1517: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.87 
2024-02-08 19:36:33,614 EPOCH 1518
2024-02-08 19:36:36,705 [Epoch: 1518 Step: 00051600] Batch Recognition Loss:   0.000335 => Gls Tokens per Sec:     2194 || Batch Translation Loss:   0.104909 => Txt Tokens per Sec:     6080 || Lr: 0.000050
2024-02-08 19:36:38,558 Epoch 1518: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.00 
2024-02-08 19:36:38,558 EPOCH 1519
2024-02-08 19:36:42,751 Epoch 1519: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-08 19:36:42,751 EPOCH 1520
2024-02-08 19:36:47,706 Epoch 1520: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-08 19:36:47,707 EPOCH 1521
2024-02-08 19:36:50,206 [Epoch: 1521 Step: 00051700] Batch Recognition Loss:   0.000271 => Gls Tokens per Sec:     2458 || Batch Translation Loss:   0.019197 => Txt Tokens per Sec:     6908 || Lr: 0.000050
2024-02-08 19:36:52,158 Epoch 1521: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-08 19:36:52,158 EPOCH 1522
2024-02-08 19:36:56,764 Epoch 1522: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-08 19:36:56,764 EPOCH 1523
2024-02-08 19:37:01,761 Epoch 1523: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-08 19:37:01,761 EPOCH 1524
2024-02-08 19:37:04,318 [Epoch: 1524 Step: 00051800] Batch Recognition Loss:   0.000282 => Gls Tokens per Sec:     2152 || Batch Translation Loss:   0.019704 => Txt Tokens per Sec:     6260 || Lr: 0.000050
2024-02-08 19:37:06,151 Epoch 1524: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-08 19:37:06,152 EPOCH 1525
2024-02-08 19:37:11,171 Epoch 1525: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.70 
2024-02-08 19:37:11,172 EPOCH 1526
2024-02-08 19:37:15,356 Epoch 1526: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-08 19:37:15,356 EPOCH 1527
2024-02-08 19:37:17,792 [Epoch: 1527 Step: 00051900] Batch Recognition Loss:   0.002213 => Gls Tokens per Sec:     1996 || Batch Translation Loss:   0.013898 => Txt Tokens per Sec:     5637 || Lr: 0.000050
2024-02-08 19:37:20,445 Epoch 1527: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-08 19:37:20,445 EPOCH 1528
2024-02-08 19:37:25,446 Epoch 1528: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-08 19:37:25,447 EPOCH 1529
2024-02-08 19:37:30,395 Epoch 1529: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-08 19:37:30,396 EPOCH 1530
2024-02-08 19:37:32,436 [Epoch: 1530 Step: 00052000] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     2199 || Batch Translation Loss:   0.022440 => Txt Tokens per Sec:     6300 || Lr: 0.000050
2024-02-08 19:37:41,247 Validation result at epoch 1530, step    52000: duration: 8.8110s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.25656	Translation Loss: 95879.35156	PPL: 14421.02832
	Eval Metric: BLEU
	WER 2.82	(DEL: 0.00,	INS: 0.00,	SUB: 2.82)
	BLEU-4 0.56	(BLEU-1: 9.77,	BLEU-2: 3.00,	BLEU-3: 1.20,	BLEU-4: 0.56)
	CHRF 16.55	ROUGE 8.68
2024-02-08 19:37:41,249 Logging Recognition and Translation Outputs
2024-02-08 19:37:41,249 ========================================================================================================================
2024-02-08 19:37:41,249 Logging Sequence: 59_101.00
2024-02-08 19:37:41,250 	Gloss Reference :	A B+C+D+E
2024-02-08 19:37:41,250 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:37:41,250 	Gloss Alignment :	         
2024-02-08 19:37:41,250 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:37:41,252 	Text Reference  :	did you see the video fox said she won  her      medals because of a       condom and  is    very     happy 
2024-02-08 19:37:41,252 	Text Hypothesis :	*** *** in  the ***** *** **** *** 2022 athletes good   luck    in trouble at     each other domestic events
2024-02-08 19:37:41,252 	Text Alignment  :	D   D   S       D     D   D    D   S    S        S      S       S  S       S      S    S     S        S     
2024-02-08 19:37:41,252 ========================================================================================================================
2024-02-08 19:37:41,252 Logging Sequence: 103_112.00
2024-02-08 19:37:41,253 	Gloss Reference :	A B+C+D+E
2024-02-08 19:37:41,253 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:37:41,253 	Gloss Alignment :	         
2024-02-08 19:37:41,253 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:37:41,254 	Text Reference  :	you are aware that  earlier the britishers had colonized a lot of countries in the world
2024-02-08 19:37:41,254 	Text Hypothesis :	*** *** ***** there is      the ********** *** ********* * *** ** richest   in the world
2024-02-08 19:37:41,254 	Text Alignment  :	D   D   D     S     S           D          D   D         D D   D  S                     
2024-02-08 19:37:41,254 ========================================================================================================================
2024-02-08 19:37:41,255 Logging Sequence: 143_11.00
2024-02-08 19:37:41,255 	Gloss Reference :	A B+C+D+E
2024-02-08 19:37:41,255 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:37:41,255 	Gloss Alignment :	         
2024-02-08 19:37:41,255 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:37:41,257 	Text Reference  :	ronaldo has also become the first person to have 500 million followers on      instagram he is the   most    loved    footballer
2024-02-08 19:37:41,257 	Text Hypothesis :	******* *** **** ****** the ***** ****** ** **** ban will    be        applied when      he ** joins another football club      
2024-02-08 19:37:41,257 	Text Alignment  :	D       D   D    D          D     D      D  D    S   S       S         S       S            D  S     S       S        S         
2024-02-08 19:37:41,257 ========================================================================================================================
2024-02-08 19:37:41,257 Logging Sequence: 183_23.00
2024-02-08 19:37:41,257 	Gloss Reference :	A B+C+D+E
2024-02-08 19:37:41,258 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:37:41,258 	Gloss Alignment :	         
2024-02-08 19:37:41,258 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:37:41,259 	Text Reference  :	however everybody has been waiting for  them to     announce the name   of      the   child
2024-02-08 19:37:41,259 	Text Hypothesis :	like    india     has **** ******* bcci a    source from     the afghan cricket board said 
2024-02-08 19:37:41,259 	Text Alignment  :	S       S             D    D       S    S    S      S            S      S       S     S    
2024-02-08 19:37:41,259 ========================================================================================================================
2024-02-08 19:37:41,260 Logging Sequence: 169_165.00
2024-02-08 19:37:41,260 	Gloss Reference :	A B+C+D+E
2024-02-08 19:37:41,260 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:37:41,260 	Gloss Alignment :	         
2024-02-08 19:37:41,260 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:37:41,262 	Text Reference  :	***** ** the indian government was  outraged by the  incident and  these    changes were   undone  by wikipedia
2024-02-08 19:37:41,262 	Text Hypothesis :	kohli is the ****** first      time india    to play an       easy watching the     women' cricket or ipad     
2024-02-08 19:37:41,262 	Text Alignment  :	I     I      D      S          S    S        S  S    S        S    S        S       S      S       S  S        
2024-02-08 19:37:41,262 ========================================================================================================================
2024-02-08 19:37:44,165 Epoch 1530: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-08 19:37:44,165 EPOCH 1531
2024-02-08 19:37:48,738 Epoch 1531: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-08 19:37:48,739 EPOCH 1532
2024-02-08 19:37:53,752 Epoch 1532: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-08 19:37:53,753 EPOCH 1533
2024-02-08 19:37:55,387 [Epoch: 1533 Step: 00052100] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     2350 || Batch Translation Loss:   0.011642 => Txt Tokens per Sec:     6531 || Lr: 0.000050
2024-02-08 19:37:58,503 Epoch 1533: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-08 19:37:58,503 EPOCH 1534
2024-02-08 19:38:03,371 Epoch 1534: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.64 
2024-02-08 19:38:03,372 EPOCH 1535
2024-02-08 19:38:08,315 Epoch 1535: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.21 
2024-02-08 19:38:08,316 EPOCH 1536
2024-02-08 19:38:09,263 [Epoch: 1536 Step: 00052200] Batch Recognition Loss:   0.000228 => Gls Tokens per Sec:     3108 || Batch Translation Loss:   0.015779 => Txt Tokens per Sec:     7831 || Lr: 0.000050
2024-02-08 19:38:13,084 Epoch 1536: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.00 
2024-02-08 19:38:13,084 EPOCH 1537
2024-02-08 19:38:17,987 Epoch 1537: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-08 19:38:17,988 EPOCH 1538
2024-02-08 19:38:22,266 Epoch 1538: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-08 19:38:22,266 EPOCH 1539
2024-02-08 19:38:23,055 [Epoch: 1539 Step: 00052300] Batch Recognition Loss:   0.000486 => Gls Tokens per Sec:     2919 || Batch Translation Loss:   0.022813 => Txt Tokens per Sec:     7204 || Lr: 0.000050
2024-02-08 19:38:26,913 Epoch 1539: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-08 19:38:26,914 EPOCH 1540
2024-02-08 19:38:31,622 Epoch 1540: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-08 19:38:31,622 EPOCH 1541
2024-02-08 19:38:35,980 Epoch 1541: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.96 
2024-02-08 19:38:35,981 EPOCH 1542
2024-02-08 19:38:36,787 [Epoch: 1542 Step: 00052400] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:     2385 || Batch Translation Loss:   0.029458 => Txt Tokens per Sec:     6417 || Lr: 0.000050
2024-02-08 19:38:40,893 Epoch 1542: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.90 
2024-02-08 19:38:40,893 EPOCH 1543
2024-02-08 19:38:45,020 Epoch 1543: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.79 
2024-02-08 19:38:45,020 EPOCH 1544
2024-02-08 19:38:49,841 Epoch 1544: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-08 19:38:49,841 EPOCH 1545
2024-02-08 19:38:50,427 [Epoch: 1545 Step: 00052500] Batch Recognition Loss:   0.000276 => Gls Tokens per Sec:     2188 || Batch Translation Loss:   0.022796 => Txt Tokens per Sec:     6140 || Lr: 0.000050
2024-02-08 19:38:54,380 Epoch 1545: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.76 
2024-02-08 19:38:54,380 EPOCH 1546
2024-02-08 19:38:58,602 Epoch 1546: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.87 
2024-02-08 19:38:58,602 EPOCH 1547
2024-02-08 19:39:03,604 Epoch 1547: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-08 19:39:03,605 EPOCH 1548
2024-02-08 19:39:03,747 [Epoch: 1548 Step: 00052600] Batch Recognition Loss:   0.000152 => Gls Tokens per Sec:     4507 || Batch Translation Loss:   0.008336 => Txt Tokens per Sec:     9169 || Lr: 0.000050
2024-02-08 19:39:07,828 Epoch 1548: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-08 19:39:07,829 EPOCH 1549
2024-02-08 19:39:11,962 Epoch 1549: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-08 19:39:11,962 EPOCH 1550
2024-02-08 19:39:16,093 [Epoch: 1550 Step: 00052700] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     2571 || Batch Translation Loss:   0.014792 => Txt Tokens per Sec:     7114 || Lr: 0.000050
2024-02-08 19:39:16,093 Epoch 1550: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-08 19:39:16,094 EPOCH 1551
2024-02-08 19:39:20,734 Epoch 1551: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-08 19:39:20,735 EPOCH 1552
2024-02-08 19:39:25,381 Epoch 1552: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-08 19:39:25,381 EPOCH 1553
2024-02-08 19:39:29,479 [Epoch: 1553 Step: 00052800] Batch Recognition Loss:   0.000285 => Gls Tokens per Sec:     2435 || Batch Translation Loss:   0.011288 => Txt Tokens per Sec:     6737 || Lr: 0.000050
2024-02-08 19:39:29,759 Epoch 1553: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-08 19:39:29,760 EPOCH 1554
2024-02-08 19:39:34,695 Epoch 1554: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-08 19:39:34,696 EPOCH 1555
2024-02-08 19:39:38,837 Epoch 1555: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-08 19:39:38,837 EPOCH 1556
2024-02-08 19:39:43,200 [Epoch: 1556 Step: 00052900] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     2142 || Batch Translation Loss:   0.013515 => Txt Tokens per Sec:     5841 || Lr: 0.000050
2024-02-08 19:39:43,815 Epoch 1556: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-08 19:39:43,816 EPOCH 1557
2024-02-08 19:39:48,116 Epoch 1557: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-08 19:39:48,116 EPOCH 1558
2024-02-08 19:39:52,733 Epoch 1558: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-08 19:39:52,734 EPOCH 1559
2024-02-08 19:39:56,928 [Epoch: 1559 Step: 00053000] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     2075 || Batch Translation Loss:   0.012455 => Txt Tokens per Sec:     5864 || Lr: 0.000050
2024-02-08 19:39:57,501 Epoch 1559: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-08 19:39:57,502 EPOCH 1560
2024-02-08 19:40:01,683 Epoch 1560: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.74 
2024-02-08 19:40:01,684 EPOCH 1561
2024-02-08 19:40:06,886 Epoch 1561: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.96 
2024-02-08 19:40:06,886 EPOCH 1562
2024-02-08 19:40:10,490 [Epoch: 1562 Step: 00053100] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     2238 || Batch Translation Loss:   0.025375 => Txt Tokens per Sec:     6250 || Lr: 0.000050
2024-02-08 19:40:11,510 Epoch 1562: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-08 19:40:11,511 EPOCH 1563
2024-02-08 19:40:15,647 Epoch 1563: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-08 19:40:15,648 EPOCH 1564
2024-02-08 19:40:20,466 Epoch 1564: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-08 19:40:20,466 EPOCH 1565
2024-02-08 19:40:23,561 [Epoch: 1565 Step: 00053200] Batch Recognition Loss:   0.000743 => Gls Tokens per Sec:     2399 || Batch Translation Loss:   0.018197 => Txt Tokens per Sec:     6616 || Lr: 0.000050
2024-02-08 19:40:24,930 Epoch 1565: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.60 
2024-02-08 19:40:24,930 EPOCH 1566
2024-02-08 19:40:29,437 Epoch 1566: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-08 19:40:29,437 EPOCH 1567
2024-02-08 19:40:34,247 Epoch 1567: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.40 
2024-02-08 19:40:34,247 EPOCH 1568
2024-02-08 19:40:36,971 [Epoch: 1568 Step: 00053300] Batch Recognition Loss:   0.001837 => Gls Tokens per Sec:     2490 || Batch Translation Loss:   0.029215 => Txt Tokens per Sec:     6800 || Lr: 0.000050
2024-02-08 19:40:38,347 Epoch 1568: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-08 19:40:38,348 EPOCH 1569
2024-02-08 19:40:43,281 Epoch 1569: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-08 19:40:43,282 EPOCH 1570
2024-02-08 19:40:47,708 Epoch 1570: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-08 19:40:47,708 EPOCH 1571
2024-02-08 19:40:50,455 [Epoch: 1571 Step: 00053400] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     2236 || Batch Translation Loss:   0.012386 => Txt Tokens per Sec:     6285 || Lr: 0.000050
2024-02-08 19:40:52,418 Epoch 1571: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.91 
2024-02-08 19:40:52,418 EPOCH 1572
2024-02-08 19:40:57,103 Epoch 1572: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.39 
2024-02-08 19:40:57,103 EPOCH 1573
2024-02-08 19:41:01,588 Epoch 1573: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.27 
2024-02-08 19:41:01,589 EPOCH 1574
2024-02-08 19:41:04,004 [Epoch: 1574 Step: 00053500] Batch Recognition Loss:   0.000613 => Gls Tokens per Sec:     2278 || Batch Translation Loss:   0.114855 => Txt Tokens per Sec:     6083 || Lr: 0.000050
2024-02-08 19:41:06,450 Epoch 1574: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.91 
2024-02-08 19:41:06,450 EPOCH 1575
2024-02-08 19:41:10,560 Epoch 1575: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.65 
2024-02-08 19:41:10,561 EPOCH 1576
2024-02-08 19:41:15,613 Epoch 1576: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.84 
2024-02-08 19:41:15,614 EPOCH 1577
2024-02-08 19:41:17,784 [Epoch: 1577 Step: 00053600] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     2361 || Batch Translation Loss:   0.025620 => Txt Tokens per Sec:     6606 || Lr: 0.000050
2024-02-08 19:41:19,933 Epoch 1577: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.26 
2024-02-08 19:41:19,933 EPOCH 1578
2024-02-08 19:41:24,830 Epoch 1578: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.99 
2024-02-08 19:41:24,830 EPOCH 1579
2024-02-08 19:41:29,293 Epoch 1579: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.81 
2024-02-08 19:41:29,293 EPOCH 1580
2024-02-08 19:41:30,603 [Epoch: 1580 Step: 00053700] Batch Recognition Loss:   0.000316 => Gls Tokens per Sec:     3224 || Batch Translation Loss:   0.017652 => Txt Tokens per Sec:     8007 || Lr: 0.000050
2024-02-08 19:41:33,576 Epoch 1580: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-08 19:41:33,576 EPOCH 1581
2024-02-08 19:41:38,575 Epoch 1581: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-08 19:41:38,575 EPOCH 1582
2024-02-08 19:41:42,797 Epoch 1582: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-08 19:41:42,797 EPOCH 1583
2024-02-08 19:41:44,186 [Epoch: 1583 Step: 00053800] Batch Recognition Loss:   0.000348 => Gls Tokens per Sec:     2767 || Batch Translation Loss:   0.015766 => Txt Tokens per Sec:     6990 || Lr: 0.000050
2024-02-08 19:41:47,779 Epoch 1583: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-08 19:41:47,779 EPOCH 1584
2024-02-08 19:41:52,119 Epoch 1584: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-08 19:41:52,119 EPOCH 1585
2024-02-08 19:41:56,852 Epoch 1585: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-08 19:41:56,852 EPOCH 1586
2024-02-08 19:41:57,949 [Epoch: 1586 Step: 00053900] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     2920 || Batch Translation Loss:   0.016959 => Txt Tokens per Sec:     7714 || Lr: 0.000050
2024-02-08 19:42:01,431 Epoch 1586: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-08 19:42:01,432 EPOCH 1587
2024-02-08 19:42:05,937 Epoch 1587: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-08 19:42:05,938 EPOCH 1588
2024-02-08 19:42:10,732 Epoch 1588: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-08 19:42:10,733 EPOCH 1589
2024-02-08 19:42:11,761 [Epoch: 1589 Step: 00054000] Batch Recognition Loss:   0.000180 => Gls Tokens per Sec:     2240 || Batch Translation Loss:   0.016733 => Txt Tokens per Sec:     6442 || Lr: 0.000050
2024-02-08 19:42:20,818 Validation result at epoch 1589, step    54000: duration: 9.0553s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.08401	Translation Loss: 95509.49219	PPL: 13898.01367
	Eval Metric: BLEU
	WER 2.68	(DEL: 0.00,	INS: 0.00,	SUB: 2.68)
	BLEU-4 0.45	(BLEU-1: 10.42,	BLEU-2: 2.85,	BLEU-3: 0.98,	BLEU-4: 0.45)
	CHRF 16.88	ROUGE 8.71
2024-02-08 19:42:20,819 Logging Recognition and Translation Outputs
2024-02-08 19:42:20,819 ========================================================================================================================
2024-02-08 19:42:20,819 Logging Sequence: 166_243.00
2024-02-08 19:42:20,820 	Gloss Reference :	A B+C+D+E
2024-02-08 19:42:20,820 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:42:20,820 	Gloss Alignment :	         
2024-02-08 19:42:20,820 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:42:20,821 	Text Reference  :	*** **** ********* ****** ******* *** ***** icc       worked with members  boards like bcci pcb   cricket australia etc 
2024-02-08 19:42:20,821 	Text Hypothesis :	the bcci president sourav ganguly and board secretary jay    shah welcomed the    two  new  teams to      the       team
2024-02-08 19:42:20,821 	Text Alignment  :	I   I    I         I      I       I   I     S         S      S    S        S      S    S    S     S       S         S   
2024-02-08 19:42:20,822 ========================================================================================================================
2024-02-08 19:42:20,822 Logging Sequence: 179_409.00
2024-02-08 19:42:20,822 	Gloss Reference :	A B+C+D+E
2024-02-08 19:42:20,822 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:42:20,822 	Gloss Alignment :	         
2024-02-08 19:42:20,822 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:42:20,824 	Text Reference  :	** *** ********* **** the ******** ******* *** *** *** * ***** passport was at the  wfi    office in      delhi
2024-02-08 19:42:20,824 	Text Hypothesis :	if she continued with the athletes however now and has a other team     and 3  days before the    bottles now  
2024-02-08 19:42:20,824 	Text Alignment  :	I  I   I         I        I        I       I   I   I   I I     S        S   S  S    S      S      S       S    
2024-02-08 19:42:20,824 ========================================================================================================================
2024-02-08 19:42:20,824 Logging Sequence: 81_407.00
2024-02-08 19:42:20,824 	Gloss Reference :	A B+C+D+E
2024-02-08 19:42:20,824 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:42:20,825 	Gloss Alignment :	         
2024-02-08 19:42:20,825 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:42:20,826 	Text Reference  :	******* the government company - national buildings construction corporation and they  will complete them    in   a  time-bound manner
2024-02-08 19:42:20,826 	Text Hypothesis :	however the ********** ******* * ******** ********* bcci         told        you don't that it's     players have to rhiti      sports
2024-02-08 19:42:20,827 	Text Alignment  :	I           D          D       D D        D         S            S           S   S     S    S        S       S    S  S          S     
2024-02-08 19:42:20,827 ========================================================================================================================
2024-02-08 19:42:20,827 Logging Sequence: 96_31.00
2024-02-08 19:42:20,827 	Gloss Reference :	A B+C+D+E
2024-02-08 19:42:20,827 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:42:20,827 	Gloss Alignment :	         
2024-02-08 19:42:20,827 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:42:20,828 	Text Reference  :	and then 2 teams will go on      to  play the final
2024-02-08 19:42:20,828 	Text Hypothesis :	*** **** * ***** **** ** however the next day that 
2024-02-08 19:42:20,828 	Text Alignment  :	D   D    D D     D    D  S       S   S    S   S    
2024-02-08 19:42:20,828 ========================================================================================================================
2024-02-08 19:42:20,828 Logging Sequence: 160_87.00
2024-02-08 19:42:20,829 	Gloss Reference :	A B+C+D+E
2024-02-08 19:42:20,829 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:42:20,829 	Gloss Alignment :	         
2024-02-08 19:42:20,829 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:42:20,830 	Text Reference  :	***** **** *** *** kohli held a  press conference and said
2024-02-08 19:42:20,830 	Text Hypothesis :	after this was the first time he was   glued      to  see 
2024-02-08 19:42:20,830 	Text Alignment  :	I     I    I   I   S     S    S  S     S          S   S   
2024-02-08 19:42:20,830 ========================================================================================================================
2024-02-08 19:42:24,482 Epoch 1589: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-08 19:42:24,482 EPOCH 1590
2024-02-08 19:42:29,417 Epoch 1590: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-08 19:42:29,417 EPOCH 1591
2024-02-08 19:42:33,867 Epoch 1591: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-08 19:42:33,867 EPOCH 1592
2024-02-08 19:42:34,419 [Epoch: 1592 Step: 00054100] Batch Recognition Loss:   0.000616 => Gls Tokens per Sec:     3491 || Batch Translation Loss:   0.005086 => Txt Tokens per Sec:     8347 || Lr: 0.000050
2024-02-08 19:42:38,866 Epoch 1592: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-08 19:42:38,867 EPOCH 1593
2024-02-08 19:42:43,294 Epoch 1593: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-08 19:42:43,295 EPOCH 1594
2024-02-08 19:42:47,950 Epoch 1594: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-08 19:42:47,950 EPOCH 1595
2024-02-08 19:42:48,296 [Epoch: 1595 Step: 00054200] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     3732 || Batch Translation Loss:   0.011491 => Txt Tokens per Sec:     9291 || Lr: 0.000050
2024-02-08 19:42:52,590 Epoch 1595: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.46 
2024-02-08 19:42:52,591 EPOCH 1596
2024-02-08 19:42:56,993 Epoch 1596: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-08 19:42:56,994 EPOCH 1597
2024-02-08 19:43:01,959 Epoch 1597: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-08 19:43:01,959 EPOCH 1598
2024-02-08 19:43:02,218 [Epoch: 1598 Step: 00054300] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:     2481 || Batch Translation Loss:   0.010407 => Txt Tokens per Sec:     7229 || Lr: 0.000050
2024-02-08 19:43:06,094 Epoch 1598: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-08 19:43:06,094 EPOCH 1599
2024-02-08 19:43:11,105 Epoch 1599: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-08 19:43:11,106 EPOCH 1600
2024-02-08 19:43:15,448 [Epoch: 1600 Step: 00054400] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     2446 || Batch Translation Loss:   0.016552 => Txt Tokens per Sec:     6768 || Lr: 0.000050
2024-02-08 19:43:15,448 Epoch 1600: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-08 19:43:15,449 EPOCH 1601
2024-02-08 19:43:19,586 Epoch 1601: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-08 19:43:19,586 EPOCH 1602
2024-02-08 19:43:24,111 Epoch 1602: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-08 19:43:24,112 EPOCH 1603
2024-02-08 19:43:28,739 [Epoch: 1603 Step: 00054500] Batch Recognition Loss:   0.000507 => Gls Tokens per Sec:     2157 || Batch Translation Loss:   0.030675 => Txt Tokens per Sec:     6008 || Lr: 0.000050
2024-02-08 19:43:28,940 Epoch 1603: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.39 
2024-02-08 19:43:28,940 EPOCH 1604
2024-02-08 19:43:33,096 Epoch 1604: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.57 
2024-02-08 19:43:33,097 EPOCH 1605
2024-02-08 19:43:38,127 Epoch 1605: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.05 
2024-02-08 19:43:38,128 EPOCH 1606
2024-02-08 19:43:41,987 [Epoch: 1606 Step: 00054600] Batch Recognition Loss:   0.000771 => Gls Tokens per Sec:     2489 || Batch Translation Loss:   0.048655 => Txt Tokens per Sec:     7006 || Lr: 0.000050
2024-02-08 19:43:42,421 Epoch 1606: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.22 
2024-02-08 19:43:42,421 EPOCH 1607
2024-02-08 19:43:47,372 Epoch 1607: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.72 
2024-02-08 19:43:47,372 EPOCH 1608
2024-02-08 19:43:51,812 Epoch 1608: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.67 
2024-02-08 19:43:51,812 EPOCH 1609
2024-02-08 19:43:55,795 [Epoch: 1609 Step: 00054700] Batch Recognition Loss:   0.000294 => Gls Tokens per Sec:     2251 || Batch Translation Loss:   0.022575 => Txt Tokens per Sec:     6300 || Lr: 0.000050
2024-02-08 19:43:56,494 Epoch 1609: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.13 
2024-02-08 19:43:56,494 EPOCH 1610
2024-02-08 19:44:01,212 Epoch 1610: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-08 19:44:01,212 EPOCH 1611
2024-02-08 19:44:05,589 Epoch 1611: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-08 19:44:05,590 EPOCH 1612
2024-02-08 19:44:09,376 [Epoch: 1612 Step: 00054800] Batch Recognition Loss:   0.000338 => Gls Tokens per Sec:     2199 || Batch Translation Loss:   0.014795 => Txt Tokens per Sec:     6123 || Lr: 0.000050
2024-02-08 19:44:10,584 Epoch 1612: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.67 
2024-02-08 19:44:10,584 EPOCH 1613
2024-02-08 19:44:14,704 Epoch 1613: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-08 19:44:14,704 EPOCH 1614
2024-02-08 19:44:19,932 Epoch 1614: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.85 
2024-02-08 19:44:19,933 EPOCH 1615
2024-02-08 19:44:23,154 [Epoch: 1615 Step: 00054900] Batch Recognition Loss:   0.000324 => Gls Tokens per Sec:     2304 || Batch Translation Loss:   0.023644 => Txt Tokens per Sec:     6146 || Lr: 0.000050
2024-02-08 19:44:24,703 Epoch 1615: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-08 19:44:24,703 EPOCH 1616
2024-02-08 19:44:28,954 Epoch 1616: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-08 19:44:28,954 EPOCH 1617
2024-02-08 19:44:33,968 Epoch 1617: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-08 19:44:33,968 EPOCH 1618
2024-02-08 19:44:36,816 [Epoch: 1618 Step: 00055000] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:     2474 || Batch Translation Loss:   0.017235 => Txt Tokens per Sec:     6942 || Lr: 0.000050
2024-02-08 19:44:38,195 Epoch 1618: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-08 19:44:38,195 EPOCH 1619
2024-02-08 19:44:43,022 Epoch 1619: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-08 19:44:43,023 EPOCH 1620
2024-02-08 19:44:47,495 Epoch 1620: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-08 19:44:47,496 EPOCH 1621
2024-02-08 19:44:49,951 [Epoch: 1621 Step: 00055100] Batch Recognition Loss:   0.000302 => Gls Tokens per Sec:     2608 || Batch Translation Loss:   0.014382 => Txt Tokens per Sec:     7100 || Lr: 0.000050
2024-02-08 19:44:52,174 Epoch 1621: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-08 19:44:52,175 EPOCH 1622
2024-02-08 19:44:56,791 Epoch 1622: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-08 19:44:56,791 EPOCH 1623
2024-02-08 19:45:00,973 Epoch 1623: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-08 19:45:00,973 EPOCH 1624
2024-02-08 19:45:03,764 [Epoch: 1624 Step: 00055200] Batch Recognition Loss:   0.000513 => Gls Tokens per Sec:     1972 || Batch Translation Loss:   0.026152 => Txt Tokens per Sec:     5325 || Lr: 0.000050
2024-02-08 19:45:06,028 Epoch 1624: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-08 19:45:06,028 EPOCH 1625
2024-02-08 19:45:10,363 Epoch 1625: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-08 19:45:10,363 EPOCH 1626
2024-02-08 19:45:15,254 Epoch 1626: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-08 19:45:15,254 EPOCH 1627
2024-02-08 19:45:16,980 [Epoch: 1627 Step: 00055300] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:     2968 || Batch Translation Loss:   0.020411 => Txt Tokens per Sec:     7475 || Lr: 0.000050
2024-02-08 19:45:19,786 Epoch 1627: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-08 19:45:19,786 EPOCH 1628
2024-02-08 19:45:24,333 Epoch 1628: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.70 
2024-02-08 19:45:24,333 EPOCH 1629
2024-02-08 19:45:29,042 Epoch 1629: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-08 19:45:29,042 EPOCH 1630
2024-02-08 19:45:30,935 [Epoch: 1630 Step: 00055400] Batch Recognition Loss:   0.000405 => Gls Tokens per Sec:     2232 || Batch Translation Loss:   0.020743 => Txt Tokens per Sec:     6411 || Lr: 0.000050
2024-02-08 19:45:33,271 Epoch 1630: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-08 19:45:33,271 EPOCH 1631
2024-02-08 19:45:38,231 Epoch 1631: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-08 19:45:38,231 EPOCH 1632
2024-02-08 19:45:42,436 Epoch 1632: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-08 19:45:42,436 EPOCH 1633
2024-02-08 19:45:43,931 [Epoch: 1633 Step: 00055500] Batch Recognition Loss:   0.000129 => Gls Tokens per Sec:     2570 || Batch Translation Loss:   0.025737 => Txt Tokens per Sec:     7117 || Lr: 0.000050
2024-02-08 19:45:47,307 Epoch 1633: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.44 
2024-02-08 19:45:47,308 EPOCH 1634
2024-02-08 19:45:51,769 Epoch 1634: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.97 
2024-02-08 19:45:51,769 EPOCH 1635
2024-02-08 19:45:56,486 Epoch 1635: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-08 19:45:56,487 EPOCH 1636
2024-02-08 19:45:58,034 [Epoch: 1636 Step: 00055600] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     1903 || Batch Translation Loss:   0.010714 => Txt Tokens per Sec:     5146 || Lr: 0.000050
2024-02-08 19:46:01,138 Epoch 1636: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.70 
2024-02-08 19:46:01,138 EPOCH 1637
2024-02-08 19:46:05,574 Epoch 1637: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-08 19:46:05,574 EPOCH 1638
2024-02-08 19:46:10,485 Epoch 1638: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.43 
2024-02-08 19:46:10,485 EPOCH 1639
2024-02-08 19:46:11,437 [Epoch: 1639 Step: 00055700] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     2419 || Batch Translation Loss:   0.019638 => Txt Tokens per Sec:     6899 || Lr: 0.000050
2024-02-08 19:46:14,579 Epoch 1639: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.24 
2024-02-08 19:46:14,579 EPOCH 1640
2024-02-08 19:46:19,636 Epoch 1640: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.91 
2024-02-08 19:46:19,637 EPOCH 1641
2024-02-08 19:46:23,959 Epoch 1641: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.10 
2024-02-08 19:46:23,959 EPOCH 1642
2024-02-08 19:46:24,675 [Epoch: 1642 Step: 00055800] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:     2322 || Batch Translation Loss:   0.118136 => Txt Tokens per Sec:     6771 || Lr: 0.000050
2024-02-08 19:46:28,542 Epoch 1642: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.45 
2024-02-08 19:46:28,542 EPOCH 1643
2024-02-08 19:46:33,279 Epoch 1643: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.38 
2024-02-08 19:46:33,280 EPOCH 1644
2024-02-08 19:46:37,616 Epoch 1644: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.33 
2024-02-08 19:46:37,616 EPOCH 1645
2024-02-08 19:46:38,009 [Epoch: 1645 Step: 00055900] Batch Recognition Loss:   0.000267 => Gls Tokens per Sec:     2602 || Batch Translation Loss:   0.025378 => Txt Tokens per Sec:     5890 || Lr: 0.000050
2024-02-08 19:46:42,759 Epoch 1645: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.26 
2024-02-08 19:46:42,760 EPOCH 1646
2024-02-08 19:46:47,684 Epoch 1646: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.95 
2024-02-08 19:46:47,685 EPOCH 1647
2024-02-08 19:46:51,808 Epoch 1647: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-08 19:46:51,808 EPOCH 1648
2024-02-08 19:46:52,136 [Epoch: 1648 Step: 00056000] Batch Recognition Loss:   0.001067 => Gls Tokens per Sec:     1957 || Batch Translation Loss:   0.022066 => Txt Tokens per Sec:     6587 || Lr: 0.000050
2024-02-08 19:47:01,085 Validation result at epoch 1648, step    56000: duration: 8.9492s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.06745	Translation Loss: 94880.88281	PPL: 13052.25391
	Eval Metric: BLEU
	WER 2.54	(DEL: 0.00,	INS: 0.00,	SUB: 2.54)
	BLEU-4 0.73	(BLEU-1: 10.25,	BLEU-2: 3.19,	BLEU-3: 1.30,	BLEU-4: 0.73)
	CHRF 16.95	ROUGE 8.76
2024-02-08 19:47:01,086 Logging Recognition and Translation Outputs
2024-02-08 19:47:01,086 ========================================================================================================================
2024-02-08 19:47:01,086 Logging Sequence: 177_167.00
2024-02-08 19:47:01,086 	Gloss Reference :	A B+C+D+E
2024-02-08 19:47:01,087 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:47:01,087 	Gloss Alignment :	         
2024-02-08 19:47:01,087 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:47:01,088 	Text Reference  :	this is       because  sushil wanted      to   establish his fear to ensure no one   would oppose him   
2024-02-08 19:47:01,089 	Text Hypothesis :	and  arrested danushka for    encouraging even at        the age  of '    2  crore was   taken  tennis
2024-02-08 19:47:01,089 	Text Alignment  :	S    S        S        S      S           S    S         S   S    S  S      S  S     S     S      S     
2024-02-08 19:47:01,089 ========================================================================================================================
2024-02-08 19:47:01,089 Logging Sequence: 127_140.00
2024-02-08 19:47:01,089 	Gloss Reference :	A B+C+D+E
2024-02-08 19:47:01,089 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:47:01,089 	Gloss Alignment :	         
2024-02-08 19:47:01,090 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:47:01,091 	Text Reference  :	this is india' 3rd medal in the ****** world athletics championships he is  very   talented and his performance is  highly impressive
2024-02-08 19:47:01,091 	Text Hypothesis :	**** ** ****** *** ***** ** the indian team  was       india         by sri lankan batsmen  and *** during      the world  cup       
2024-02-08 19:47:01,092 	Text Alignment  :	D    D  D      D   D     D      I      S     S         S             S  S   S      S            D   S           S   S      S         
2024-02-08 19:47:01,092 ========================================================================================================================
2024-02-08 19:47:01,092 Logging Sequence: 126_200.00
2024-02-08 19:47:01,092 	Gloss Reference :	A B+C+D+E
2024-02-08 19:47:01,092 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:47:01,092 	Gloss Alignment :	         
2024-02-08 19:47:01,093 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:47:01,093 	Text Reference  :	let   me   tell you about them 
2024-02-08 19:47:01,093 	Text Hypothesis :	these were some of  the   match
2024-02-08 19:47:01,093 	Text Alignment  :	S     S    S    S   S     S    
2024-02-08 19:47:01,093 ========================================================================================================================
2024-02-08 19:47:01,094 Logging Sequence: 104_119.00
2024-02-08 19:47:01,094 	Gloss Reference :	A B+C+D+E
2024-02-08 19:47:01,094 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:47:01,094 	Gloss Alignment :	         
2024-02-08 19:47:01,094 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:47:01,096 	Text Reference  :	*** ** ****** *** famous       chess players like viswanathan anand and praggnanandhaa's coach r     b    ramesh congratulated him   for his impressive performance
2024-02-08 19:47:01,097 	Text Hypothesis :	the tv rights for broadcasting ipl   matches in   india       for   the next             5     years went to     star          india for rs  23575      crore      
2024-02-08 19:47:01,097 	Text Alignment  :	I   I  I      I   S            S     S       S    S           S     S   S                S     S     S    S      S             S         S   S          S          
2024-02-08 19:47:01,097 ========================================================================================================================
2024-02-08 19:47:01,097 Logging Sequence: 172_267.00
2024-02-08 19:47:01,097 	Gloss Reference :	A B+C+D+E
2024-02-08 19:47:01,097 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:47:01,097 	Gloss Alignment :	         
2024-02-08 19:47:01,097 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:47:01,098 	Text Reference  :	**** ** *** such provisions have  been made     
2024-02-08 19:47:01,098 	Text Hypothesis :	this is why the  police     filed a    statement
2024-02-08 19:47:01,098 	Text Alignment  :	I    I  I   S    S          S     S    S        
2024-02-08 19:47:01,098 ========================================================================================================================
2024-02-08 19:47:05,815 Epoch 1648: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.66 
2024-02-08 19:47:05,815 EPOCH 1649
2024-02-08 19:47:10,667 Epoch 1649: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-08 19:47:10,667 EPOCH 1650
2024-02-08 19:47:14,780 [Epoch: 1650 Step: 00056100] Batch Recognition Loss:   0.000256 => Gls Tokens per Sec:     2583 || Batch Translation Loss:   0.012842 => Txt Tokens per Sec:     7145 || Lr: 0.000050
2024-02-08 19:47:14,780 Epoch 1650: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-08 19:47:14,780 EPOCH 1651
2024-02-08 19:47:19,663 Epoch 1651: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-08 19:47:19,664 EPOCH 1652
2024-02-08 19:47:24,059 Epoch 1652: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.75 
2024-02-08 19:47:24,059 EPOCH 1653
2024-02-08 19:47:28,334 [Epoch: 1653 Step: 00056200] Batch Recognition Loss:   0.000208 => Gls Tokens per Sec:     2335 || Batch Translation Loss:   0.016580 => Txt Tokens per Sec:     6382 || Lr: 0.000050
2024-02-08 19:47:28,736 Epoch 1653: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-08 19:47:28,737 EPOCH 1654
2024-02-08 19:47:33,381 Epoch 1654: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.62 
2024-02-08 19:47:33,381 EPOCH 1655
2024-02-08 19:47:37,761 Epoch 1655: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-08 19:47:37,762 EPOCH 1656
2024-02-08 19:47:42,129 [Epoch: 1656 Step: 00056300] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     2199 || Batch Translation Loss:   0.013570 => Txt Tokens per Sec:     6036 || Lr: 0.000050
2024-02-08 19:47:42,720 Epoch 1656: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.78 
2024-02-08 19:47:42,721 EPOCH 1657
2024-02-08 19:47:46,882 Epoch 1657: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.56 
2024-02-08 19:47:46,882 EPOCH 1658
2024-02-08 19:47:51,872 Epoch 1658: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.71 
2024-02-08 19:47:51,873 EPOCH 1659
2024-02-08 19:47:55,218 [Epoch: 1659 Step: 00056400] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:     2602 || Batch Translation Loss:   0.012336 => Txt Tokens per Sec:     7112 || Lr: 0.000050
2024-02-08 19:47:56,180 Epoch 1659: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-08 19:47:56,180 EPOCH 1660
2024-02-08 19:48:00,829 Epoch 1660: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-08 19:48:00,830 EPOCH 1661
2024-02-08 19:48:05,562 Epoch 1661: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-08 19:48:05,562 EPOCH 1662
2024-02-08 19:48:08,859 [Epoch: 1662 Step: 00056500] Batch Recognition Loss:   0.000318 => Gls Tokens per Sec:     2446 || Batch Translation Loss:   0.021659 => Txt Tokens per Sec:     6886 || Lr: 0.000050
2024-02-08 19:48:09,762 Epoch 1662: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-08 19:48:09,762 EPOCH 1663
2024-02-08 19:48:14,755 Epoch 1663: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-08 19:48:14,755 EPOCH 1664
2024-02-08 19:48:18,948 Epoch 1664: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-08 19:48:18,948 EPOCH 1665
2024-02-08 19:48:22,353 [Epoch: 1665 Step: 00056600] Batch Recognition Loss:   0.000297 => Gls Tokens per Sec:     2180 || Batch Translation Loss:   0.012435 => Txt Tokens per Sec:     6133 || Lr: 0.000050
2024-02-08 19:48:23,626 Epoch 1665: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-08 19:48:23,627 EPOCH 1666
2024-02-08 19:48:28,268 Epoch 1666: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.84 
2024-02-08 19:48:28,268 EPOCH 1667
2024-02-08 19:48:32,595 Epoch 1667: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-08 19:48:32,596 EPOCH 1668
2024-02-08 19:48:35,954 [Epoch: 1668 Step: 00056700] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:     2020 || Batch Translation Loss:   0.024954 => Txt Tokens per Sec:     5654 || Lr: 0.000050
2024-02-08 19:48:37,594 Epoch 1668: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-08 19:48:37,594 EPOCH 1669
2024-02-08 19:48:41,771 Epoch 1669: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.80 
2024-02-08 19:48:41,772 EPOCH 1670
2024-02-08 19:48:46,787 Epoch 1670: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-08 19:48:46,788 EPOCH 1671
2024-02-08 19:48:49,191 [Epoch: 1671 Step: 00056800] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:     2664 || Batch Translation Loss:   0.022081 => Txt Tokens per Sec:     7295 || Lr: 0.000050
2024-02-08 19:48:51,082 Epoch 1671: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.86 
2024-02-08 19:48:51,082 EPOCH 1672
2024-02-08 19:48:55,983 Epoch 1672: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-08 19:48:55,984 EPOCH 1673
2024-02-08 19:49:00,988 Epoch 1673: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-08 19:49:00,989 EPOCH 1674
2024-02-08 19:49:03,352 [Epoch: 1674 Step: 00056900] Batch Recognition Loss:   0.000731 => Gls Tokens per Sec:     2329 || Batch Translation Loss:   0.019772 => Txt Tokens per Sec:     6477 || Lr: 0.000050
2024-02-08 19:49:05,240 Epoch 1674: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-08 19:49:05,240 EPOCH 1675
2024-02-08 19:49:09,938 Epoch 1675: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-08 19:49:09,939 EPOCH 1676
2024-02-08 19:49:14,538 Epoch 1676: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.71 
2024-02-08 19:49:14,538 EPOCH 1677
2024-02-08 19:49:16,484 [Epoch: 1677 Step: 00057000] Batch Recognition Loss:   0.000189 => Gls Tokens per Sec:     2499 || Batch Translation Loss:   0.015396 => Txt Tokens per Sec:     6793 || Lr: 0.000050
2024-02-08 19:49:19,003 Epoch 1677: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-08 19:49:19,003 EPOCH 1678
2024-02-08 19:49:23,906 Epoch 1678: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-08 19:49:23,906 EPOCH 1679
2024-02-08 19:49:28,043 Epoch 1679: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-08 19:49:28,044 EPOCH 1680
2024-02-08 19:49:29,776 [Epoch: 1680 Step: 00057100] Batch Recognition Loss:   0.000282 => Gls Tokens per Sec:     2436 || Batch Translation Loss:   0.024685 => Txt Tokens per Sec:     6758 || Lr: 0.000050
2024-02-08 19:49:32,685 Epoch 1680: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-08 19:49:32,686 EPOCH 1681
2024-02-08 19:49:37,352 Epoch 1681: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.94 
2024-02-08 19:49:37,353 EPOCH 1682
2024-02-08 19:49:42,026 Epoch 1682: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.34 
2024-02-08 19:49:42,026 EPOCH 1683
2024-02-08 19:49:43,782 [Epoch: 1683 Step: 00057200] Batch Recognition Loss:   0.000277 => Gls Tokens per Sec:     2041 || Batch Translation Loss:   0.030342 => Txt Tokens per Sec:     5888 || Lr: 0.000050
2024-02-08 19:49:46,732 Epoch 1683: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.46 
2024-02-08 19:49:46,732 EPOCH 1684
2024-02-08 19:49:51,030 Epoch 1684: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.99 
2024-02-08 19:49:51,031 EPOCH 1685
2024-02-08 19:49:55,987 Epoch 1685: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-08 19:49:55,987 EPOCH 1686
2024-02-08 19:49:57,225 [Epoch: 1686 Step: 00057300] Batch Recognition Loss:   0.000438 => Gls Tokens per Sec:     2377 || Batch Translation Loss:   0.006065 => Txt Tokens per Sec:     6556 || Lr: 0.000050
2024-02-08 19:50:00,155 Epoch 1686: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.44 
2024-02-08 19:50:00,155 EPOCH 1687
2024-02-08 19:50:05,129 Epoch 1687: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.64 
2024-02-08 19:50:05,130 EPOCH 1688
2024-02-08 19:50:09,563 Epoch 1688: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.49 
2024-02-08 19:50:09,563 EPOCH 1689
2024-02-08 19:50:10,729 [Epoch: 1689 Step: 00057400] Batch Recognition Loss:   0.000364 => Gls Tokens per Sec:     1973 || Batch Translation Loss:   0.045133 => Txt Tokens per Sec:     5877 || Lr: 0.000050
2024-02-08 19:50:14,399 Epoch 1689: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.97 
2024-02-08 19:50:14,399 EPOCH 1690
2024-02-08 19:50:18,882 Epoch 1690: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.79 
2024-02-08 19:50:18,882 EPOCH 1691
2024-02-08 19:50:23,337 Epoch 1691: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-08 19:50:23,338 EPOCH 1692
2024-02-08 19:50:23,991 [Epoch: 1692 Step: 00057500] Batch Recognition Loss:   0.000315 => Gls Tokens per Sec:     2945 || Batch Translation Loss:   0.014495 => Txt Tokens per Sec:     6913 || Lr: 0.000050
2024-02-08 19:50:28,191 Epoch 1692: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.66 
2024-02-08 19:50:28,192 EPOCH 1693
2024-02-08 19:50:32,422 Epoch 1693: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-08 19:50:32,422 EPOCH 1694
2024-02-08 19:50:37,485 Epoch 1694: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.97 
2024-02-08 19:50:37,485 EPOCH 1695
2024-02-08 19:50:37,965 [Epoch: 1695 Step: 00057600] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:     2129 || Batch Translation Loss:   0.020536 => Txt Tokens per Sec:     6154 || Lr: 0.000050
2024-02-08 19:50:41,704 Epoch 1695: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-08 19:50:41,705 EPOCH 1696
2024-02-08 19:50:46,584 Epoch 1696: Total Training Recognition Loss 0.04  Total Training Translation Loss 13.04 
2024-02-08 19:50:46,585 EPOCH 1697
2024-02-08 19:50:51,032 Epoch 1697: Total Training Recognition Loss 0.14  Total Training Translation Loss 7.14 
2024-02-08 19:50:51,032 EPOCH 1698
2024-02-08 19:50:51,259 [Epoch: 1698 Step: 00057700] Batch Recognition Loss:   0.000639 => Gls Tokens per Sec:     2832 || Batch Translation Loss:   0.055749 => Txt Tokens per Sec:     7398 || Lr: 0.000050
2024-02-08 19:50:55,703 Epoch 1698: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.29 
2024-02-08 19:50:55,703 EPOCH 1699
2024-02-08 19:51:00,328 Epoch 1699: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.81 
2024-02-08 19:51:00,328 EPOCH 1700
2024-02-08 19:51:04,645 [Epoch: 1700 Step: 00057800] Batch Recognition Loss:   0.000272 => Gls Tokens per Sec:     2461 || Batch Translation Loss:   0.016967 => Txt Tokens per Sec:     6807 || Lr: 0.000050
2024-02-08 19:51:04,646 Epoch 1700: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.70 
2024-02-08 19:51:04,646 EPOCH 1701
2024-02-08 19:51:09,570 Epoch 1701: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.71 
2024-02-08 19:51:09,571 EPOCH 1702
2024-02-08 19:51:13,929 Epoch 1702: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-08 19:51:13,930 EPOCH 1703
2024-02-08 19:51:18,703 [Epoch: 1703 Step: 00057900] Batch Recognition Loss:   0.000734 => Gls Tokens per Sec:     2091 || Batch Translation Loss:   0.014374 => Txt Tokens per Sec:     5793 || Lr: 0.000050
2024-02-08 19:51:18,916 Epoch 1703: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-08 19:51:18,916 EPOCH 1704
2024-02-08 19:51:23,099 Epoch 1704: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-08 19:51:23,099 EPOCH 1705
2024-02-08 19:51:28,042 Epoch 1705: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-08 19:51:28,042 EPOCH 1706
2024-02-08 19:51:32,029 [Epoch: 1706 Step: 00058000] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     2343 || Batch Translation Loss:   0.019509 => Txt Tokens per Sec:     6530 || Lr: 0.000050
2024-02-08 19:51:40,873 Validation result at epoch 1706, step    58000: duration: 8.8420s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.11761	Translation Loss: 94992.92188	PPL: 13199.13281
	Eval Metric: BLEU
	WER 2.90	(DEL: 0.00,	INS: 0.00,	SUB: 2.90)
	BLEU-4 0.48	(BLEU-1: 10.60,	BLEU-2: 3.11,	BLEU-3: 1.07,	BLEU-4: 0.48)
	CHRF 16.83	ROUGE 8.99
2024-02-08 19:51:40,874 Logging Recognition and Translation Outputs
2024-02-08 19:51:40,874 ========================================================================================================================
2024-02-08 19:51:40,874 Logging Sequence: 60_264.00
2024-02-08 19:51:40,874 	Gloss Reference :	A B+C+D+E
2024-02-08 19:51:40,874 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:51:40,874 	Gloss Alignment :	         
2024-02-08 19:51:40,874 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:51:40,876 	Text Reference  :	** plus do   you       know that     a             sex  tape    of  his      with two    women had  gone  viral 
2024-02-08 19:51:40,876 	Text Hypothesis :	he has  been embroiled in   multiple controversies like affairs sex scandals with models and   many other issues
2024-02-08 19:51:40,876 	Text Alignment  :	I  S    S    S         S    S        S             S    S       S   S             S      S     S    S     S     
2024-02-08 19:51:40,876 ========================================================================================================================
2024-02-08 19:51:40,877 Logging Sequence: 100_50.00
2024-02-08 19:51:40,877 	Gloss Reference :	A B+C+D+E
2024-02-08 19:51:40,877 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:51:40,877 	Gloss Alignment :	         
2024-02-08 19:51:40,877 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:51:40,878 	Text Reference  :	***** with virat    kohli  as     the captain
2024-02-08 19:51:40,878 	Text Hypothesis :	after this incident yuvraj showed his yet    
2024-02-08 19:51:40,878 	Text Alignment  :	I     S    S        S      S      S   S      
2024-02-08 19:51:40,878 ========================================================================================================================
2024-02-08 19:51:40,878 Logging Sequence: 137_44.00
2024-02-08 19:51:40,878 	Gloss Reference :	A B+C+D+E
2024-02-08 19:51:40,878 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:51:40,878 	Gloss Alignment :	         
2024-02-08 19:51:40,879 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:51:40,880 	Text Reference  :	let me   tell you    the rules that qatar has announced for the fans    travelling for the world cup 
2024-02-08 19:51:40,880 	Text Hypothesis :	*** they also called the ***** **** ***** *** news      of  all crashed out        of  the long  time
2024-02-08 19:51:40,880 	Text Alignment  :	D   S    S    S          D     D    D     D   S         S   S   S       S          S       S     S   
2024-02-08 19:51:40,880 ========================================================================================================================
2024-02-08 19:51:40,880 Logging Sequence: 58_27.00
2024-02-08 19:51:40,881 	Gloss Reference :	A B+C+D+E
2024-02-08 19:51:40,881 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:51:40,881 	Gloss Alignment :	         
2024-02-08 19:51:40,881 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:51:40,882 	Text Reference  :	the 19th  asian games 2022 were to be  held in hangzhou china 
2024-02-08 19:51:40,882 	Text Hypothesis :	the first asian games **** **** ** are held in 39       sports
2024-02-08 19:51:40,882 	Text Alignment  :	    S                 D    D    D  S           S        S     
2024-02-08 19:51:40,882 ========================================================================================================================
2024-02-08 19:51:40,882 Logging Sequence: 75_255.00
2024-02-08 19:51:40,883 	Gloss Reference :	A B+C+D+E  
2024-02-08 19:51:40,883 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-08 19:51:40,883 	Gloss Alignment :	  S        
2024-02-08 19:51:40,883 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:51:40,884 	Text Reference  :	we miss our baby boy with this ronaldo' total baby   count has     reached 5 with  2  boys 3   girls
2024-02-08 19:51:40,885 	Text Hypothesis :	** **** *** **** *** **** **** ******** later rooney then  invited the     3 girls to his  vip booth
2024-02-08 19:51:40,885 	Text Alignment  :	D  D    D   D    D   D    D    D        S     S      S     S       S       S S     S  S    S   S    
2024-02-08 19:51:40,885 ========================================================================================================================
2024-02-08 19:51:41,336 Epoch 1706: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-08 19:51:41,336 EPOCH 1707
2024-02-08 19:51:46,313 Epoch 1707: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-08 19:51:46,313 EPOCH 1708
2024-02-08 19:51:51,238 Epoch 1708: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-08 19:51:51,238 EPOCH 1709
2024-02-08 19:51:54,441 [Epoch: 1709 Step: 00058100] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     2717 || Batch Translation Loss:   0.015241 => Txt Tokens per Sec:     7410 || Lr: 0.000050
2024-02-08 19:51:55,315 Epoch 1709: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-08 19:51:55,315 EPOCH 1710
2024-02-08 19:52:00,398 Epoch 1710: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-08 19:52:00,398 EPOCH 1711
2024-02-08 19:52:04,695 Epoch 1711: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-08 19:52:04,695 EPOCH 1712
2024-02-08 19:52:08,335 [Epoch: 1712 Step: 00058200] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     2216 || Batch Translation Loss:   0.014035 => Txt Tokens per Sec:     6072 || Lr: 0.000050
2024-02-08 19:52:09,505 Epoch 1712: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-08 19:52:09,505 EPOCH 1713
2024-02-08 19:52:14,053 Epoch 1713: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-08 19:52:14,053 EPOCH 1714
2024-02-08 19:52:18,584 Epoch 1714: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-08 19:52:18,585 EPOCH 1715
2024-02-08 19:52:22,048 [Epoch: 1715 Step: 00058300] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:     2143 || Batch Translation Loss:   0.017634 => Txt Tokens per Sec:     5993 || Lr: 0.000050
2024-02-08 19:52:23,347 Epoch 1715: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-08 19:52:23,348 EPOCH 1716
2024-02-08 19:52:27,616 Epoch 1716: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-08 19:52:27,616 EPOCH 1717
2024-02-08 19:52:32,577 Epoch 1717: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-08 19:52:32,578 EPOCH 1718
2024-02-08 19:52:35,035 [Epoch: 1718 Step: 00058400] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     2761 || Batch Translation Loss:   0.022285 => Txt Tokens per Sec:     7354 || Lr: 0.000050
2024-02-08 19:52:36,736 Epoch 1718: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-08 19:52:36,736 EPOCH 1719
2024-02-08 19:52:41,660 Epoch 1719: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-08 19:52:41,661 EPOCH 1720
2024-02-08 19:52:46,028 Epoch 1720: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.71 
2024-02-08 19:52:46,029 EPOCH 1721
2024-02-08 19:52:48,467 [Epoch: 1721 Step: 00058500] Batch Recognition Loss:   0.000454 => Gls Tokens per Sec:     2519 || Batch Translation Loss:   0.010811 => Txt Tokens per Sec:     6733 || Lr: 0.000050
2024-02-08 19:52:50,733 Epoch 1721: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-08 19:52:50,734 EPOCH 1722
2024-02-08 19:52:55,380 Epoch 1722: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-08 19:52:55,380 EPOCH 1723
2024-02-08 19:52:59,718 Epoch 1723: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-08 19:52:59,719 EPOCH 1724
2024-02-08 19:53:02,791 [Epoch: 1724 Step: 00058600] Batch Recognition Loss:   0.001317 => Gls Tokens per Sec:     1791 || Batch Translation Loss:   0.040340 => Txt Tokens per Sec:     5213 || Lr: 0.000050
2024-02-08 19:53:04,675 Epoch 1724: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-08 19:53:04,675 EPOCH 1725
2024-02-08 19:53:08,806 Epoch 1725: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-08 19:53:08,806 EPOCH 1726
2024-02-08 19:53:13,820 Epoch 1726: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-08 19:53:13,821 EPOCH 1727
2024-02-08 19:53:15,830 [Epoch: 1727 Step: 00058700] Batch Recognition Loss:   0.000997 => Gls Tokens per Sec:     2549 || Batch Translation Loss:   0.016224 => Txt Tokens per Sec:     7117 || Lr: 0.000050
2024-02-08 19:53:18,193 Epoch 1727: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-08 19:53:18,193 EPOCH 1728
2024-02-08 19:53:22,983 Epoch 1728: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-08 19:53:22,984 EPOCH 1729
2024-02-08 19:53:27,531 Epoch 1729: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-08 19:53:27,531 EPOCH 1730
2024-02-08 19:53:29,061 [Epoch: 1730 Step: 00058800] Batch Recognition Loss:   0.000306 => Gls Tokens per Sec:     2932 || Batch Translation Loss:   0.007689 => Txt Tokens per Sec:     7468 || Lr: 0.000050
2024-02-08 19:53:32,048 Epoch 1730: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-08 19:53:32,049 EPOCH 1731
2024-02-08 19:53:36,808 Epoch 1731: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-08 19:53:36,808 EPOCH 1732
2024-02-08 19:53:41,066 Epoch 1732: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-08 19:53:41,066 EPOCH 1733
2024-02-08 19:53:42,795 [Epoch: 1733 Step: 00058900] Batch Recognition Loss:   0.003095 => Gls Tokens per Sec:     2073 || Batch Translation Loss:   0.143185 => Txt Tokens per Sec:     5737 || Lr: 0.000050
2024-02-08 19:53:46,093 Epoch 1733: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.23 
2024-02-08 19:53:46,093 EPOCH 1734
2024-02-08 19:53:50,320 Epoch 1734: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.09 
2024-02-08 19:53:50,321 EPOCH 1735
2024-02-08 19:53:55,382 Epoch 1735: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.34 
2024-02-08 19:53:55,382 EPOCH 1736
2024-02-08 19:53:56,861 [Epoch: 1736 Step: 00059000] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:     2165 || Batch Translation Loss:   0.020129 => Txt Tokens per Sec:     6127 || Lr: 0.000050
2024-02-08 19:53:59,634 Epoch 1736: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-08 19:53:59,635 EPOCH 1737
2024-02-08 19:54:04,500 Epoch 1737: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-08 19:54:04,501 EPOCH 1738
2024-02-08 19:54:08,991 Epoch 1738: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.87 
2024-02-08 19:54:08,991 EPOCH 1739
2024-02-08 19:54:09,865 [Epoch: 1739 Step: 00059100] Batch Recognition Loss:   0.000295 => Gls Tokens per Sec:     2929 || Batch Translation Loss:   0.018452 => Txt Tokens per Sec:     7955 || Lr: 0.000050
2024-02-08 19:54:13,579 Epoch 1739: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-08 19:54:13,580 EPOCH 1740
2024-02-08 19:54:18,441 Epoch 1740: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-08 19:54:18,442 EPOCH 1741
2024-02-08 19:54:23,422 Epoch 1741: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-08 19:54:23,422 EPOCH 1742
2024-02-08 19:54:24,443 [Epoch: 1742 Step: 00059200] Batch Recognition Loss:   0.000792 => Gls Tokens per Sec:     1882 || Batch Translation Loss:   0.008025 => Txt Tokens per Sec:     5138 || Lr: 0.000050
2024-02-08 19:54:28,097 Epoch 1742: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-08 19:54:28,098 EPOCH 1743
2024-02-08 19:54:33,099 Epoch 1743: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-08 19:54:33,100 EPOCH 1744
2024-02-08 19:54:37,888 Epoch 1744: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.00 
2024-02-08 19:54:37,888 EPOCH 1745
2024-02-08 19:54:38,666 [Epoch: 1745 Step: 00059300] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:     1652 || Batch Translation Loss:   0.032291 => Txt Tokens per Sec:     4865 || Lr: 0.000050
2024-02-08 19:54:42,943 Epoch 1745: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-08 19:54:42,944 EPOCH 1746
2024-02-08 19:54:47,658 Epoch 1746: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-08 19:54:47,658 EPOCH 1747
2024-02-08 19:54:52,589 Epoch 1747: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-08 19:54:52,589 EPOCH 1748
2024-02-08 19:54:52,888 [Epoch: 1748 Step: 00059400] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:     2162 || Batch Translation Loss:   0.015300 => Txt Tokens per Sec:     6422 || Lr: 0.000050
2024-02-08 19:54:57,447 Epoch 1748: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-08 19:54:57,447 EPOCH 1749
2024-02-08 19:55:02,220 Epoch 1749: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-08 19:55:02,221 EPOCH 1750
2024-02-08 19:55:07,110 [Epoch: 1750 Step: 00059500] Batch Recognition Loss:   0.000282 => Gls Tokens per Sec:     2173 || Batch Translation Loss:   0.024646 => Txt Tokens per Sec:     6012 || Lr: 0.000050
2024-02-08 19:55:07,110 Epoch 1750: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.16 
2024-02-08 19:55:07,111 EPOCH 1751
2024-02-08 19:55:11,790 Epoch 1751: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.30 
2024-02-08 19:55:11,791 EPOCH 1752
2024-02-08 19:55:16,673 Epoch 1752: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.54 
2024-02-08 19:55:16,673 EPOCH 1753
2024-02-08 19:55:21,075 [Epoch: 1753 Step: 00059600] Batch Recognition Loss:   0.000219 => Gls Tokens per Sec:     2268 || Batch Translation Loss:   0.026080 => Txt Tokens per Sec:     6355 || Lr: 0.000050
2024-02-08 19:55:21,270 Epoch 1753: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.80 
2024-02-08 19:55:21,270 EPOCH 1754
2024-02-08 19:55:26,165 Epoch 1754: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.98 
2024-02-08 19:55:26,166 EPOCH 1755
2024-02-08 19:55:30,708 Epoch 1755: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-08 19:55:30,708 EPOCH 1756
2024-02-08 19:55:34,401 [Epoch: 1756 Step: 00059700] Batch Recognition Loss:   0.000265 => Gls Tokens per Sec:     2531 || Batch Translation Loss:   0.015248 => Txt Tokens per Sec:     6953 || Lr: 0.000050
2024-02-08 19:55:34,958 Epoch 1756: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.79 
2024-02-08 19:55:34,959 EPOCH 1757
2024-02-08 19:55:39,933 Epoch 1757: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-08 19:55:39,934 EPOCH 1758
2024-02-08 19:55:44,425 Epoch 1758: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-08 19:55:44,425 EPOCH 1759
2024-02-08 19:55:48,413 [Epoch: 1759 Step: 00059800] Batch Recognition Loss:   0.000490 => Gls Tokens per Sec:     2247 || Batch Translation Loss:   0.020299 => Txt Tokens per Sec:     6162 || Lr: 0.000050
2024-02-08 19:55:49,303 Epoch 1759: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.77 
2024-02-08 19:55:49,303 EPOCH 1760
2024-02-08 19:55:53,885 Epoch 1760: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-08 19:55:53,886 EPOCH 1761
2024-02-08 19:55:58,132 Epoch 1761: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-08 19:55:58,132 EPOCH 1762
2024-02-08 19:56:01,846 [Epoch: 1762 Step: 00059900] Batch Recognition Loss:   0.000411 => Gls Tokens per Sec:     2241 || Batch Translation Loss:   0.015205 => Txt Tokens per Sec:     6176 || Lr: 0.000050
2024-02-08 19:56:03,153 Epoch 1762: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-08 19:56:03,154 EPOCH 1763
2024-02-08 19:56:07,367 Epoch 1763: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-08 19:56:07,367 EPOCH 1764
2024-02-08 19:56:12,310 Epoch 1764: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-08 19:56:12,310 EPOCH 1765
2024-02-08 19:56:15,421 [Epoch: 1765 Step: 00060000] Batch Recognition Loss:   0.000733 => Gls Tokens per Sec:     2386 || Batch Translation Loss:   0.010985 => Txt Tokens per Sec:     6536 || Lr: 0.000050
2024-02-08 19:56:24,652 Validation result at epoch 1765, step    60000: duration: 9.2311s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.12387	Translation Loss: 94963.17969	PPL: 13159.98145
	Eval Metric: BLEU
	WER 3.11	(DEL: 0.00,	INS: 0.00,	SUB: 3.11)
	BLEU-4 0.64	(BLEU-1: 10.57,	BLEU-2: 3.12,	BLEU-3: 1.25,	BLEU-4: 0.64)
	CHRF 17.20	ROUGE 8.70
2024-02-08 19:56:24,653 Logging Recognition and Translation Outputs
2024-02-08 19:56:24,653 ========================================================================================================================
2024-02-08 19:56:24,654 Logging Sequence: 75_58.00
2024-02-08 19:56:24,654 	Gloss Reference :	A B+C+D+E
2024-02-08 19:56:24,654 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:56:24,654 	Gloss Alignment :	         
2024-02-08 19:56:24,654 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:56:24,656 	Text Reference  :	it seems like he like to date women and does not want to      get married people seem    to respect his       choices
2024-02-08 19:56:24,656 	Text Hypothesis :	** ***** **** ** **** ** **** ***** *** **** *** we   request the media   for    privacy at this    difficult time   
2024-02-08 19:56:24,656 	Text Alignment  :	D  D     D    D  D    D  D    D     D   D    D   S    S       S   S       S      S       S  S       S         S      
2024-02-08 19:56:24,656 ========================================================================================================================
2024-02-08 19:56:24,656 Logging Sequence: 152_113.00
2024-02-08 19:56:24,657 	Gloss Reference :	A B+C+D+E
2024-02-08 19:56:24,657 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:56:24,657 	Gloss Alignment :	         
2024-02-08 19:56:24,657 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:56:24,658 	Text Reference  :	indians hoping for a         victory were   distraught at     the defeat
2024-02-08 19:56:24,658 	Text Hypothesis :	when    it     was pakistan' turn    lionel messi      scored his bat   
2024-02-08 19:56:24,658 	Text Alignment  :	S       S      S   S         S       S      S          S      S   S     
2024-02-08 19:56:24,658 ========================================================================================================================
2024-02-08 19:56:24,658 Logging Sequence: 176_41.00
2024-02-08 19:56:24,659 	Gloss Reference :	A B+C+D+E
2024-02-08 19:56:24,659 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:56:24,659 	Gloss Alignment :	         
2024-02-08 19:56:24,659 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:56:24,660 	Text Reference  :	dahiya did not   loose  hope and   put up    a    strong fight   
2024-02-08 19:56:24,660 	Text Hypothesis :	after  the match people go   viral 3   balls will be     refunded
2024-02-08 19:56:24,660 	Text Alignment  :	S      S   S     S      S    S     S   S     S    S      S       
2024-02-08 19:56:24,660 ========================================================================================================================
2024-02-08 19:56:24,660 Logging Sequence: 77_190.00
2024-02-08 19:56:24,661 	Gloss Reference :	A B+C+D+E
2024-02-08 19:56:24,661 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:56:24,661 	Gloss Alignment :	         
2024-02-08 19:56:24,661 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:56:24,662 	Text Reference  :	* ** **** ***** there are many batsmen who      have scrored 36  runs in        6   balls   
2024-02-08 19:56:24,662 	Text Hypothesis :	i am very grate to    my  fans and     everyone to   see     him to   celebrate the position
2024-02-08 19:56:24,662 	Text Alignment  :	I I  I    I     S     S   S    S       S        S    S       S   S    S         S   S       
2024-02-08 19:56:24,663 ========================================================================================================================
2024-02-08 19:56:24,663 Logging Sequence: 155_170.00
2024-02-08 19:56:24,663 	Gloss Reference :	A B+C+D+E
2024-02-08 19:56:24,663 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 19:56:24,663 	Gloss Alignment :	         
2024-02-08 19:56:24,663 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 19:56:24,664 	Text Reference  :	india lost the matches and  could    not secure a      place   in the semi final 
2024-02-08 19:56:24,664 	Text Hypothesis :	***** **** i   am      very grateful to  my     family friends of the **** sports
2024-02-08 19:56:24,664 	Text Alignment  :	D     D    S   S       S    S        S   S      S      S       S      D    S     
2024-02-08 19:56:24,665 ========================================================================================================================
2024-02-08 19:56:26,113 Epoch 1765: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.90 
2024-02-08 19:56:26,113 EPOCH 1766
2024-02-08 19:56:30,788 Epoch 1766: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.94 
2024-02-08 19:56:30,788 EPOCH 1767
2024-02-08 19:56:35,760 Epoch 1767: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.65 
2024-02-08 19:56:35,761 EPOCH 1768
2024-02-08 19:56:38,382 [Epoch: 1768 Step: 00060100] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     2588 || Batch Translation Loss:   0.019077 => Txt Tokens per Sec:     7066 || Lr: 0.000050
2024-02-08 19:56:40,225 Epoch 1768: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-08 19:56:40,225 EPOCH 1769
2024-02-08 19:56:45,039 Epoch 1769: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-08 19:56:45,040 EPOCH 1770
2024-02-08 19:56:49,683 Epoch 1770: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-08 19:56:49,684 EPOCH 1771
2024-02-08 19:56:52,053 [Epoch: 1771 Step: 00060200] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:     2593 || Batch Translation Loss:   0.013388 => Txt Tokens per Sec:     7243 || Lr: 0.000050
2024-02-08 19:56:54,294 Epoch 1771: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-08 19:56:54,294 EPOCH 1772
2024-02-08 19:56:59,064 Epoch 1772: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-08 19:56:59,064 EPOCH 1773
2024-02-08 19:57:03,247 Epoch 1773: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-08 19:57:03,248 EPOCH 1774
2024-02-08 19:57:06,119 [Epoch: 1774 Step: 00060300] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     2007 || Batch Translation Loss:   0.015070 => Txt Tokens per Sec:     5681 || Lr: 0.000050
2024-02-08 19:57:08,285 Epoch 1774: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-08 19:57:08,286 EPOCH 1775
2024-02-08 19:57:12,570 Epoch 1775: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-08 19:57:12,571 EPOCH 1776
2024-02-08 19:57:17,603 Epoch 1776: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-08 19:57:17,603 EPOCH 1777
2024-02-08 19:57:19,149 [Epoch: 1777 Step: 00060400] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:     3316 || Batch Translation Loss:   0.019007 => Txt Tokens per Sec:     8413 || Lr: 0.000050
2024-02-08 19:57:21,815 Epoch 1777: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-08 19:57:21,815 EPOCH 1778
2024-02-08 19:57:26,713 Epoch 1778: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.19 
2024-02-08 19:57:26,713 EPOCH 1779
2024-02-08 19:57:31,249 Epoch 1779: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.14 
2024-02-08 19:57:31,249 EPOCH 1780
2024-02-08 19:57:33,075 [Epoch: 1780 Step: 00060500] Batch Recognition Loss:   0.000288 => Gls Tokens per Sec:     2314 || Batch Translation Loss:   0.039940 => Txt Tokens per Sec:     6457 || Lr: 0.000050
2024-02-08 19:57:35,730 Epoch 1780: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-08 19:57:35,731 EPOCH 1781
2024-02-08 19:57:40,583 Epoch 1781: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.81 
2024-02-08 19:57:40,584 EPOCH 1782
2024-02-08 19:57:44,706 Epoch 1782: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.19 
2024-02-08 19:57:44,706 EPOCH 1783
2024-02-08 19:57:46,375 [Epoch: 1783 Step: 00060600] Batch Recognition Loss:   0.000448 => Gls Tokens per Sec:     2148 || Batch Translation Loss:   0.053189 => Txt Tokens per Sec:     5643 || Lr: 0.000050
2024-02-08 19:57:49,757 Epoch 1783: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.31 
2024-02-08 19:57:49,757 EPOCH 1784
2024-02-08 19:57:54,046 Epoch 1784: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.46 
2024-02-08 19:57:54,047 EPOCH 1785
2024-02-08 19:57:58,988 Epoch 1785: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.85 
2024-02-08 19:57:58,989 EPOCH 1786
2024-02-08 19:58:00,224 [Epoch: 1786 Step: 00060700] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     2595 || Batch Translation Loss:   0.015055 => Txt Tokens per Sec:     7329 || Lr: 0.000050
2024-02-08 19:58:03,532 Epoch 1786: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-08 19:58:03,532 EPOCH 1787
2024-02-08 19:58:07,639 Epoch 1787: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-08 19:58:07,640 EPOCH 1788
2024-02-08 19:58:11,900 Epoch 1788: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.77 
2024-02-08 19:58:11,900 EPOCH 1789
2024-02-08 19:58:13,186 [Epoch: 1789 Step: 00060800] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:     1793 || Batch Translation Loss:   0.024883 => Txt Tokens per Sec:     5097 || Lr: 0.000050
2024-02-08 19:58:16,926 Epoch 1789: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-08 19:58:16,926 EPOCH 1790
2024-02-08 19:58:21,177 Epoch 1790: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.91 
2024-02-08 19:58:21,177 EPOCH 1791
2024-02-08 19:58:26,062 Epoch 1791: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.68 
2024-02-08 19:58:26,062 EPOCH 1792
2024-02-08 19:58:26,900 [Epoch: 1792 Step: 00060900] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:     2294 || Batch Translation Loss:   0.013870 => Txt Tokens per Sec:     6566 || Lr: 0.000050
2024-02-08 19:58:30,459 Epoch 1792: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.81 
2024-02-08 19:58:30,459 EPOCH 1793
2024-02-08 19:58:35,111 Epoch 1793: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-08 19:58:35,112 EPOCH 1794
2024-02-08 19:58:39,810 Epoch 1794: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.16 
2024-02-08 19:58:39,810 EPOCH 1795
2024-02-08 19:58:40,372 [Epoch: 1795 Step: 00061000] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:     2282 || Batch Translation Loss:   0.036871 => Txt Tokens per Sec:     6840 || Lr: 0.000050
2024-02-08 19:58:43,917 Epoch 1795: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-08 19:58:43,917 EPOCH 1796
2024-02-08 19:58:48,954 Epoch 1796: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.85 
2024-02-08 19:58:48,955 EPOCH 1797
2024-02-08 19:58:53,204 Epoch 1797: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.12 
2024-02-08 19:58:53,204 EPOCH 1798
2024-02-08 19:58:53,382 [Epoch: 1798 Step: 00061100] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     3616 || Batch Translation Loss:   0.024264 => Txt Tokens per Sec:     9249 || Lr: 0.000050
2024-02-08 19:58:57,995 Epoch 1798: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.35 
2024-02-08 19:58:57,996 EPOCH 1799
2024-02-08 19:59:02,777 Epoch 1799: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.09 
2024-02-08 19:59:02,778 EPOCH 1800
2024-02-08 19:59:07,613 [Epoch: 1800 Step: 00061200] Batch Recognition Loss:   0.000437 => Gls Tokens per Sec:     2197 || Batch Translation Loss:   0.059450 => Txt Tokens per Sec:     6079 || Lr: 0.000050
2024-02-08 19:59:07,613 Epoch 1800: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-08 19:59:07,614 EPOCH 1801
2024-02-08 19:59:12,892 Epoch 1801: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.96 
2024-02-08 19:59:12,893 EPOCH 1802
2024-02-08 19:59:17,151 Epoch 1802: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.28 
2024-02-08 19:59:17,152 EPOCH 1803
2024-02-08 19:59:21,988 [Epoch: 1803 Step: 00061300] Batch Recognition Loss:   0.000478 => Gls Tokens per Sec:     2064 || Batch Translation Loss:   0.032191 => Txt Tokens per Sec:     5718 || Lr: 0.000050
2024-02-08 19:59:22,197 Epoch 1803: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.50 
2024-02-08 19:59:22,197 EPOCH 1804
2024-02-08 19:59:26,473 Epoch 1804: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.86 
2024-02-08 19:59:26,474 EPOCH 1805
2024-02-08 19:59:31,509 Epoch 1805: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-08 19:59:31,510 EPOCH 1806
2024-02-08 19:59:35,215 [Epoch: 1806 Step: 00061400] Batch Recognition Loss:   0.000550 => Gls Tokens per Sec:     2522 || Batch Translation Loss:   0.013998 => Txt Tokens per Sec:     6926 || Lr: 0.000050
2024-02-08 19:59:35,799 Epoch 1806: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.74 
2024-02-08 19:59:35,799 EPOCH 1807
2024-02-08 19:59:40,475 Epoch 1807: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.76 
2024-02-08 19:59:40,476 EPOCH 1808
2024-02-08 19:59:45,304 Epoch 1808: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.65 
2024-02-08 19:59:45,304 EPOCH 1809
2024-02-08 19:59:49,028 [Epoch: 1809 Step: 00061500] Batch Recognition Loss:   0.000304 => Gls Tokens per Sec:     2406 || Batch Translation Loss:   0.022929 => Txt Tokens per Sec:     6629 || Lr: 0.000050
2024-02-08 19:59:49,963 Epoch 1809: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-08 19:59:49,964 EPOCH 1810
2024-02-08 19:59:54,670 Epoch 1810: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-08 19:59:54,670 EPOCH 1811
2024-02-08 19:59:58,922 Epoch 1811: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-08 19:59:58,922 EPOCH 1812
2024-02-08 20:00:02,764 [Epoch: 1812 Step: 00061600] Batch Recognition Loss:   0.000333 => Gls Tokens per Sec:     2098 || Batch Translation Loss:   0.007192 => Txt Tokens per Sec:     5819 || Lr: 0.000050
2024-02-08 20:00:03,860 Epoch 1812: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-08 20:00:03,860 EPOCH 1813
2024-02-08 20:00:08,056 Epoch 1813: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-08 20:00:08,056 EPOCH 1814
2024-02-08 20:00:13,066 Epoch 1814: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-08 20:00:13,066 EPOCH 1815
2024-02-08 20:00:16,438 [Epoch: 1815 Step: 00061700] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:     2201 || Batch Translation Loss:   0.015778 => Txt Tokens per Sec:     6349 || Lr: 0.000050
2024-02-08 20:00:17,406 Epoch 1815: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-08 20:00:17,406 EPOCH 1816
2024-02-08 20:00:22,153 Epoch 1816: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.90 
2024-02-08 20:00:22,154 EPOCH 1817
2024-02-08 20:00:26,730 Epoch 1817: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-08 20:00:26,731 EPOCH 1818
2024-02-08 20:00:29,653 [Epoch: 1818 Step: 00061800] Batch Recognition Loss:   0.001097 => Gls Tokens per Sec:     2321 || Batch Translation Loss:   0.025979 => Txt Tokens per Sec:     6729 || Lr: 0.000050
2024-02-08 20:00:31,215 Epoch 1818: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-08 20:00:31,216 EPOCH 1819
2024-02-08 20:00:36,090 Epoch 1819: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-08 20:00:36,091 EPOCH 1820
2024-02-08 20:00:40,155 Epoch 1820: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-08 20:00:40,156 EPOCH 1821
2024-02-08 20:00:43,319 [Epoch: 1821 Step: 00061900] Batch Recognition Loss:   0.000605 => Gls Tokens per Sec:     2025 || Batch Translation Loss:   0.021486 => Txt Tokens per Sec:     5507 || Lr: 0.000050
2024-02-08 20:00:45,214 Epoch 1821: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-08 20:00:45,214 EPOCH 1822
2024-02-08 20:00:49,493 Epoch 1822: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-08 20:00:49,493 EPOCH 1823
2024-02-08 20:00:54,304 Epoch 1823: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-08 20:00:54,305 EPOCH 1824
2024-02-08 20:00:56,568 [Epoch: 1824 Step: 00062000] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     2546 || Batch Translation Loss:   0.015785 => Txt Tokens per Sec:     6893 || Lr: 0.000050
2024-02-08 20:01:06,348 Validation result at epoch 1824, step    62000: duration: 9.7800s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.08389	Translation Loss: 96123.72656	PPL: 14777.35254
	Eval Metric: BLEU
	WER 2.75	(DEL: 0.00,	INS: 0.00,	SUB: 2.75)
	BLEU-4 0.43	(BLEU-1: 9.97,	BLEU-2: 2.79,	BLEU-3: 1.06,	BLEU-4: 0.43)
	CHRF 16.93	ROUGE 8.40
2024-02-08 20:01:06,350 Logging Recognition and Translation Outputs
2024-02-08 20:01:06,350 ========================================================================================================================
2024-02-08 20:01:06,350 Logging Sequence: 165_523.00
2024-02-08 20:01:06,350 	Gloss Reference :	A B+C+D+E
2024-02-08 20:01:06,350 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 20:01:06,350 	Gloss Alignment :	         
2024-02-08 20:01:06,351 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 20:01:06,352 	Text Reference  :	as he believed that his team might lose if   he takes off his batting pads    
2024-02-08 20:01:06,352 	Text Hypothesis :	** ** ******** **** *** **** ***** **** when he came  out to  penalty shootout
2024-02-08 20:01:06,352 	Text Alignment  :	D  D  D        D    D   D    D     D    S       S     S   S   S       S       
2024-02-08 20:01:06,352 ========================================================================================================================
2024-02-08 20:01:06,352 Logging Sequence: 165_233.00
2024-02-08 20:01:06,352 	Gloss Reference :	A B+C+D+E
2024-02-08 20:01:06,353 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 20:01:06,353 	Gloss Alignment :	         
2024-02-08 20:01:06,353 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 20:01:06,354 	Text Reference  :	irrespective of whether he was playing the match or not he always sat with his bag he was happy when    the          team     won    
2024-02-08 20:01:06,354 	Text Hypothesis :	************ ** ******* ** *** ******* *** ***** ** *** ** ****** *** **** *** *** ** to  play  against kazakhstan's nurislam sanayev
2024-02-08 20:01:06,354 	Text Alignment  :	D            D  D       D  D   D       D   D     D  D   D  D      D   D    D   D   D  S   S     S       S            S        S      
2024-02-08 20:01:06,354 ========================================================================================================================
2024-02-08 20:01:06,354 Logging Sequence: 169_214.00
2024-02-08 20:01:06,355 	Gloss Reference :	A B+C+D+E
2024-02-08 20:01:06,355 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 20:01:06,355 	Gloss Alignment :	         
2024-02-08 20:01:06,355 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 20:01:06,357 	Text Reference  :	virat kohli said that though arshdeep dropped the       catch    he          is still  a    strong part   of  the    indian     team
2024-02-08 20:01:06,357 	Text Hypothesis :	do    you   know that ****** ******** ******* wikipedia provides information on celebs like their  height age family background etc 
2024-02-08 20:01:06,357 	Text Alignment  :	S     S     S         D      D        D       S         S        S           S  S      S    S      S      S   S      S          S   
2024-02-08 20:01:06,357 ========================================================================================================================
2024-02-08 20:01:06,357 Logging Sequence: 88_67.00
2024-02-08 20:01:06,358 	Gloss Reference :	A B+C+D+E    
2024-02-08 20:01:06,358 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-08 20:01:06,358 	Gloss Alignment :	  S          
2024-02-08 20:01:06,358 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 20:01:06,360 	Text Reference  :	pablo javkin the         mayor of   rosario is  also      a        drug   trafficker so  he  won't    take  care of       you        
2024-02-08 20:01:06,360 	Text Hypothesis :	***** police authorities said  that they    are reviewing security camera footage    and his culprits would be   punished accordingly
2024-02-08 20:01:06,361 	Text Alignment  :	D     S      S           S     S    S       S   S         S        S      S          S   S   S        S     S    S        S          
2024-02-08 20:01:06,361 ========================================================================================================================
2024-02-08 20:01:06,361 Logging Sequence: 69_95.00
2024-02-08 20:01:06,361 	Gloss Reference :	A B+C+D+E
2024-02-08 20:01:06,361 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 20:01:06,361 	Gloss Alignment :	         
2024-02-08 20:01:06,362 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 20:01:06,363 	Text Reference  :	***** ** a six and a   four sealed csk's    victory     and ********** ****** ***** the team won the match
2024-02-08 20:01:06,363 	Text Hypothesis :	there is a end of  the you  all    football association and merseyside police filed the end  of  the field
2024-02-08 20:01:06,363 	Text Alignment  :	I     I    S   S   S   S    S      S        S               I          I      I         S    S       S    
2024-02-08 20:01:06,364 ========================================================================================================================
2024-02-08 20:01:08,802 Epoch 1824: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-08 20:01:08,802 EPOCH 1825
2024-02-08 20:01:13,299 Epoch 1825: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.95 
2024-02-08 20:01:13,300 EPOCH 1826
2024-02-08 20:01:18,293 Epoch 1826: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.02 
2024-02-08 20:01:18,293 EPOCH 1827
2024-02-08 20:01:20,623 [Epoch: 1827 Step: 00062100] Batch Recognition Loss:   0.000653 => Gls Tokens per Sec:     2087 || Batch Translation Loss:   0.016331 => Txt Tokens per Sec:     6039 || Lr: 0.000050
2024-02-08 20:01:22,569 Epoch 1827: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.00 
2024-02-08 20:01:22,569 EPOCH 1828
2024-02-08 20:01:27,658 Epoch 1828: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.00 
2024-02-08 20:01:27,659 EPOCH 1829
2024-02-08 20:01:31,946 Epoch 1829: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.92 
2024-02-08 20:01:31,947 EPOCH 1830
2024-02-08 20:01:33,593 [Epoch: 1830 Step: 00062200] Batch Recognition Loss:   0.000175 => Gls Tokens per Sec:     2723 || Batch Translation Loss:   0.016781 => Txt Tokens per Sec:     6872 || Lr: 0.000050
2024-02-08 20:01:36,804 Epoch 1830: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-08 20:01:36,805 EPOCH 1831
2024-02-08 20:01:41,263 Epoch 1831: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.97 
2024-02-08 20:01:41,264 EPOCH 1832
2024-02-08 20:01:45,957 Epoch 1832: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-08 20:01:45,957 EPOCH 1833
2024-02-08 20:01:47,877 [Epoch: 1833 Step: 00062300] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     1866 || Batch Translation Loss:   0.019335 => Txt Tokens per Sec:     5327 || Lr: 0.000050
2024-02-08 20:01:50,638 Epoch 1833: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-08 20:01:50,638 EPOCH 1834
2024-02-08 20:01:54,930 Epoch 1834: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.81 
2024-02-08 20:01:54,930 EPOCH 1835
2024-02-08 20:01:59,872 Epoch 1835: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.89 
2024-02-08 20:01:59,872 EPOCH 1836
2024-02-08 20:02:01,057 [Epoch: 1836 Step: 00062400] Batch Recognition Loss:   0.000654 => Gls Tokens per Sec:     2703 || Batch Translation Loss:   0.026725 => Txt Tokens per Sec:     7429 || Lr: 0.000050
2024-02-08 20:02:04,000 Epoch 1836: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-08 20:02:04,000 EPOCH 1837
2024-02-08 20:02:08,982 Epoch 1837: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-08 20:02:08,983 EPOCH 1838
2024-02-08 20:02:13,406 Epoch 1838: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.61 
2024-02-08 20:02:13,406 EPOCH 1839
2024-02-08 20:02:14,336 [Epoch: 1839 Step: 00062500] Batch Recognition Loss:   0.000202 => Gls Tokens per Sec:     2759 || Batch Translation Loss:   0.015854 => Txt Tokens per Sec:     7744 || Lr: 0.000050
2024-02-08 20:02:17,517 Epoch 1839: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-08 20:02:17,517 EPOCH 1840
2024-02-08 20:02:21,906 Epoch 1840: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-08 20:02:21,907 EPOCH 1841
2024-02-08 20:02:27,030 Epoch 1841: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-08 20:02:27,031 EPOCH 1842
2024-02-08 20:02:27,867 [Epoch: 1842 Step: 00062600] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     1988 || Batch Translation Loss:   0.006666 => Txt Tokens per Sec:     5241 || Lr: 0.000050
2024-02-08 20:02:31,835 Epoch 1842: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-08 20:02:31,835 EPOCH 1843
2024-02-08 20:02:36,396 Epoch 1843: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-08 20:02:36,396 EPOCH 1844
2024-02-08 20:02:41,308 Epoch 1844: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-08 20:02:41,309 EPOCH 1845
2024-02-08 20:02:41,636 [Epoch: 1845 Step: 00062700] Batch Recognition Loss:   0.000117 => Gls Tokens per Sec:     3926 || Batch Translation Loss:   0.007003 => Txt Tokens per Sec:     8205 || Lr: 0.000050
2024-02-08 20:02:45,931 Epoch 1845: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-08 20:02:45,932 EPOCH 1846
2024-02-08 20:02:50,856 Epoch 1846: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-08 20:02:50,856 EPOCH 1847
2024-02-08 20:02:55,500 Epoch 1847: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-08 20:02:55,501 EPOCH 1848
2024-02-08 20:02:55,992 [Epoch: 1848 Step: 00062800] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:     1306 || Batch Translation Loss:   0.015882 => Txt Tokens per Sec:     4416 || Lr: 0.000050
2024-02-08 20:03:00,494 Epoch 1848: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-08 20:03:00,494 EPOCH 1849
2024-02-08 20:03:05,141 Epoch 1849: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-08 20:03:05,141 EPOCH 1850
2024-02-08 20:03:10,084 [Epoch: 1850 Step: 00062900] Batch Recognition Loss:   0.000270 => Gls Tokens per Sec:     2149 || Batch Translation Loss:   0.247056 => Txt Tokens per Sec:     5945 || Lr: 0.000050
2024-02-08 20:03:10,085 Epoch 1850: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.18 
2024-02-08 20:03:10,085 EPOCH 1851
2024-02-08 20:03:15,149 Epoch 1851: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.20 
2024-02-08 20:03:15,150 EPOCH 1852
2024-02-08 20:03:19,682 Epoch 1852: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.58 
2024-02-08 20:03:19,682 EPOCH 1853
2024-02-08 20:03:24,222 [Epoch: 1853 Step: 00063000] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     2199 || Batch Translation Loss:   0.009729 => Txt Tokens per Sec:     6111 || Lr: 0.000050
2024-02-08 20:03:24,482 Epoch 1853: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-08 20:03:24,482 EPOCH 1854
2024-02-08 20:03:29,210 Epoch 1854: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.87 
2024-02-08 20:03:29,210 EPOCH 1855
2024-02-08 20:03:34,108 Epoch 1855: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-08 20:03:34,109 EPOCH 1856
2024-02-08 20:03:37,872 [Epoch: 1856 Step: 00063100] Batch Recognition Loss:   0.000266 => Gls Tokens per Sec:     2482 || Batch Translation Loss:   0.043924 => Txt Tokens per Sec:     6753 || Lr: 0.000050
2024-02-08 20:03:38,800 Epoch 1856: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.66 
2024-02-08 20:03:38,801 EPOCH 1857
2024-02-08 20:03:43,803 Epoch 1857: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.23 
2024-02-08 20:03:43,804 EPOCH 1858
2024-02-08 20:03:48,490 Epoch 1858: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-08 20:03:48,491 EPOCH 1859
2024-02-08 20:03:52,641 [Epoch: 1859 Step: 00063200] Batch Recognition Loss:   0.000346 => Gls Tokens per Sec:     2160 || Batch Translation Loss:   0.026214 => Txt Tokens per Sec:     6004 || Lr: 0.000050
2024-02-08 20:03:53,480 Epoch 1859: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.71 
2024-02-08 20:03:53,480 EPOCH 1860
2024-02-08 20:03:58,314 Epoch 1860: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.71 
2024-02-08 20:03:58,314 EPOCH 1861
2024-02-08 20:04:03,021 Epoch 1861: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.29 
2024-02-08 20:04:03,021 EPOCH 1862
2024-02-08 20:04:06,559 [Epoch: 1862 Step: 00063300] Batch Recognition Loss:   0.000295 => Gls Tokens per Sec:     2352 || Batch Translation Loss:   0.015289 => Txt Tokens per Sec:     6480 || Lr: 0.000050
2024-02-08 20:04:07,676 Epoch 1862: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.81 
2024-02-08 20:04:07,677 EPOCH 1863
2024-02-08 20:04:12,042 Epoch 1863: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.66 
2024-02-08 20:04:12,043 EPOCH 1864
2024-02-08 20:04:16,970 Epoch 1864: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-08 20:04:16,970 EPOCH 1865
2024-02-08 20:04:20,008 [Epoch: 1865 Step: 00063400] Batch Recognition Loss:   0.003390 => Gls Tokens per Sec:     2443 || Batch Translation Loss:   0.020776 => Txt Tokens per Sec:     6908 || Lr: 0.000050
2024-02-08 20:04:21,115 Epoch 1865: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-08 20:04:21,115 EPOCH 1866
2024-02-08 20:04:26,180 Epoch 1866: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-08 20:04:26,181 EPOCH 1867
2024-02-08 20:04:30,498 Epoch 1867: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.78 
2024-02-08 20:04:30,498 EPOCH 1868
2024-02-08 20:04:33,320 [Epoch: 1868 Step: 00063500] Batch Recognition Loss:   0.000502 => Gls Tokens per Sec:     2403 || Batch Translation Loss:   0.019423 => Txt Tokens per Sec:     6479 || Lr: 0.000050
2024-02-08 20:04:35,320 Epoch 1868: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-08 20:04:35,320 EPOCH 1869
2024-02-08 20:04:39,813 Epoch 1869: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-08 20:04:39,813 EPOCH 1870
2024-02-08 20:04:44,373 Epoch 1870: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.39 
2024-02-08 20:04:44,374 EPOCH 1871
2024-02-08 20:04:47,200 [Epoch: 1871 Step: 00063600] Batch Recognition Loss:   0.000275 => Gls Tokens per Sec:     2173 || Batch Translation Loss:   0.234231 => Txt Tokens per Sec:     5947 || Lr: 0.000050
2024-02-08 20:04:49,136 Epoch 1871: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.63 
2024-02-08 20:04:49,136 EPOCH 1872
2024-02-08 20:04:53,387 Epoch 1872: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.23 
2024-02-08 20:04:53,387 EPOCH 1873
2024-02-08 20:04:58,374 Epoch 1873: Total Training Recognition Loss 0.13  Total Training Translation Loss 0.79 
2024-02-08 20:04:58,374 EPOCH 1874
2024-02-08 20:05:00,799 [Epoch: 1874 Step: 00063700] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:     2269 || Batch Translation Loss:   0.010603 => Txt Tokens per Sec:     6521 || Lr: 0.000050
2024-02-08 20:05:02,535 Epoch 1874: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.64 
2024-02-08 20:05:02,535 EPOCH 1875
2024-02-08 20:05:07,468 Epoch 1875: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.64 
2024-02-08 20:05:07,469 EPOCH 1876
2024-02-08 20:05:11,887 Epoch 1876: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.70 
2024-02-08 20:05:11,888 EPOCH 1877
2024-02-08 20:05:13,945 [Epoch: 1877 Step: 00063800] Batch Recognition Loss:   0.029884 => Gls Tokens per Sec:     2364 || Batch Translation Loss:   0.023049 => Txt Tokens per Sec:     6607 || Lr: 0.000050
2024-02-08 20:05:16,635 Epoch 1877: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.76 
2024-02-08 20:05:16,636 EPOCH 1878
2024-02-08 20:05:21,237 Epoch 1878: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.59 
2024-02-08 20:05:21,238 EPOCH 1879
2024-02-08 20:05:25,650 Epoch 1879: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-08 20:05:25,651 EPOCH 1880
2024-02-08 20:05:27,643 [Epoch: 1880 Step: 00063900] Batch Recognition Loss:   0.000336 => Gls Tokens per Sec:     2121 || Batch Translation Loss:   0.009507 => Txt Tokens per Sec:     5807 || Lr: 0.000050
2024-02-08 20:05:30,590 Epoch 1880: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-08 20:05:30,590 EPOCH 1881
2024-02-08 20:05:34,718 Epoch 1881: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-08 20:05:34,718 EPOCH 1882
2024-02-08 20:05:39,717 Epoch 1882: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.81 
2024-02-08 20:05:39,717 EPOCH 1883
2024-02-08 20:05:41,309 [Epoch: 1883 Step: 00064000] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     2414 || Batch Translation Loss:   0.021230 => Txt Tokens per Sec:     6859 || Lr: 0.000050
2024-02-08 20:05:50,216 Validation result at epoch 1883, step    64000: duration: 8.9060s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.94836	Translation Loss: 96630.06250	PPL: 15543.91309
	Eval Metric: BLEU
	WER 2.90	(DEL: 0.00,	INS: 0.00,	SUB: 2.90)
	BLEU-4 0.46	(BLEU-1: 10.38,	BLEU-2: 3.09,	BLEU-3: 1.03,	BLEU-4: 0.46)
	CHRF 17.15	ROUGE 8.70
2024-02-08 20:05:50,218 Logging Recognition and Translation Outputs
2024-02-08 20:05:50,218 ========================================================================================================================
2024-02-08 20:05:50,218 Logging Sequence: 122_86.00
2024-02-08 20:05:50,218 	Gloss Reference :	A B+C+D+E
2024-02-08 20:05:50,218 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 20:05:50,218 	Gloss Alignment :	         
2024-02-08 20:05:50,219 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 20:05:50,219 	Text Reference  :	** after winning chanu spoke   to  the    media and       said  
2024-02-08 20:05:50,219 	Text Hypothesis :	he said  that    the   ronaldo has issued a     statement saying
2024-02-08 20:05:50,220 	Text Alignment  :	I  S     S       S     S       S   S      S     S         S     
2024-02-08 20:05:50,220 ========================================================================================================================
2024-02-08 20:05:50,220 Logging Sequence: 82_81.00
2024-02-08 20:05:50,220 	Gloss Reference :	A B+C+D+E
2024-02-08 20:05:50,220 	Gloss Hypothesis:	A B+C+D  
2024-02-08 20:05:50,220 	Gloss Alignment :	  S      
2024-02-08 20:05:50,221 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 20:05:50,222 	Text Reference  :	since the couple were residents of mumbai the       mumbai police  cyber cell began investigating the matter
2024-02-08 20:05:50,222 	Text Hypothesis :	***** the ****** **** ********* ** bcci   president sourav ganguly along with board secretary     jay shah  
2024-02-08 20:05:50,222 	Text Alignment  :	D         D      D    D         D  S      S         S      S       S     S    S     S             S   S     
2024-02-08 20:05:50,222 ========================================================================================================================
2024-02-08 20:05:50,222 Logging Sequence: 61_65.00
2024-02-08 20:05:50,222 	Gloss Reference :	A B+C+D+E
2024-02-08 20:05:50,223 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 20:05:50,223 	Gloss Alignment :	         
2024-02-08 20:05:50,223 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 20:05:50,224 	Text Reference  :	the name seems indian   but  whether it  has been made by  an  indian
2024-02-08 20:05:50,224 	Text Hypothesis :	*** **** ***** everyone were all     out and not  know who had tested
2024-02-08 20:05:50,224 	Text Alignment  :	D   D    D     S        S    S       S   S   S    S    S   S   S     
2024-02-08 20:05:50,224 ========================================================================================================================
2024-02-08 20:05:50,224 Logging Sequence: 179_126.00
2024-02-08 20:05:50,224 	Gloss Reference :	A B+C+D+E
2024-02-08 20:05:50,225 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 20:05:50,225 	Gloss Alignment :	         
2024-02-08 20:05:50,225 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 20:05:50,227 	Text Reference  :	vinesh argued that she might      contract coronavirus since  these wrestlers travelled from india where there       are many infections
2024-02-08 20:05:50,227 	Text Hypothesis :	****** ****** and  the federation or       sai         people are   well      to        see  him   a     deaflympics at  his  residence 
2024-02-08 20:05:50,227 	Text Alignment  :	D      D      S    S   S          S        S           S      S     S         S         S    S     S     S           S   S    S         
2024-02-08 20:05:50,227 ========================================================================================================================
2024-02-08 20:05:50,227 Logging Sequence: 62_24.00
2024-02-08 20:05:50,227 	Gloss Reference :	A B+C+D+E
2024-02-08 20:05:50,227 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 20:05:50,228 	Gloss Alignment :	         
2024-02-08 20:05:50,228 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 20:05:50,230 	Text Reference  :	*** *** **** now  the ***** ********** *** *** women's cricket team too    is   giving    splendid performances which are at      par with the men's         team   
2024-02-08 20:05:50,230 	Text Hypothesis :	you all know that the women cricketers can see india   england in   ensure that cricketer was      very         b     by  parents and know in  international matches
2024-02-08 20:05:50,231 	Text Alignment  :	I   I   I    S        I     I          I   I   S       S       S    S      S    S         S        S            S     S   S       S   S    S   S             S      
2024-02-08 20:05:50,231 ========================================================================================================================
2024-02-08 20:05:53,382 Epoch 1883: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.98 
2024-02-08 20:05:53,383 EPOCH 1884
2024-02-08 20:05:57,940 Epoch 1884: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-08 20:05:57,941 EPOCH 1885
2024-02-08 20:06:02,967 Epoch 1885: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-08 20:06:02,968 EPOCH 1886
2024-02-08 20:06:04,206 [Epoch: 1886 Step: 00064100] Batch Recognition Loss:   0.000228 => Gls Tokens per Sec:     2589 || Batch Translation Loss:   0.026529 => Txt Tokens per Sec:     7202 || Lr: 0.000050
2024-02-08 20:06:07,605 Epoch 1886: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-08 20:06:07,606 EPOCH 1887
2024-02-08 20:06:12,456 Epoch 1887: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-08 20:06:12,457 EPOCH 1888
2024-02-08 20:06:17,036 Epoch 1888: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-08 20:06:17,037 EPOCH 1889
2024-02-08 20:06:17,982 [Epoch: 1889 Step: 00064200] Batch Recognition Loss:   0.000224 => Gls Tokens per Sec:     2436 || Batch Translation Loss:   0.020544 => Txt Tokens per Sec:     6159 || Lr: 0.000050
2024-02-08 20:06:21,973 Epoch 1889: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-08 20:06:21,974 EPOCH 1890
2024-02-08 20:06:26,647 Epoch 1890: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-08 20:06:26,647 EPOCH 1891
2024-02-08 20:06:30,776 Epoch 1891: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.39 
2024-02-08 20:06:30,776 EPOCH 1892
2024-02-08 20:06:31,637 [Epoch: 1892 Step: 00064300] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:     2233 || Batch Translation Loss:   0.024736 => Txt Tokens per Sec:     6242 || Lr: 0.000050
2024-02-08 20:06:35,820 Epoch 1892: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.83 
2024-02-08 20:06:35,820 EPOCH 1893
2024-02-08 20:06:40,145 Epoch 1893: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.66 
2024-02-08 20:06:40,146 EPOCH 1894
2024-02-08 20:06:45,216 Epoch 1894: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.81 
2024-02-08 20:06:45,216 EPOCH 1895
2024-02-08 20:06:45,647 [Epoch: 1895 Step: 00064400] Batch Recognition Loss:   0.000311 => Gls Tokens per Sec:     2977 || Batch Translation Loss:   0.043285 => Txt Tokens per Sec:     8395 || Lr: 0.000050
2024-02-08 20:06:49,998 Epoch 1895: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.20 
2024-02-08 20:06:49,998 EPOCH 1896
2024-02-08 20:06:54,851 Epoch 1896: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.10 
2024-02-08 20:06:54,851 EPOCH 1897
2024-02-08 20:06:59,136 Epoch 1897: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.74 
2024-02-08 20:06:59,136 EPOCH 1898
2024-02-08 20:06:59,362 [Epoch: 1898 Step: 00064500] Batch Recognition Loss:   0.000268 => Gls Tokens per Sec:     2844 || Batch Translation Loss:   0.018394 => Txt Tokens per Sec:     6484 || Lr: 0.000050
2024-02-08 20:07:03,318 Epoch 1898: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-08 20:07:03,319 EPOCH 1899
2024-02-08 20:07:08,289 Epoch 1899: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-08 20:07:08,290 EPOCH 1900
2024-02-08 20:07:13,335 [Epoch: 1900 Step: 00064600] Batch Recognition Loss:   0.000225 => Gls Tokens per Sec:     2106 || Batch Translation Loss:   0.013576 => Txt Tokens per Sec:     5826 || Lr: 0.000050
2024-02-08 20:07:13,335 Epoch 1900: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-08 20:07:13,335 EPOCH 1901
2024-02-08 20:07:18,125 Epoch 1901: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-08 20:07:18,126 EPOCH 1902
2024-02-08 20:07:23,024 Epoch 1902: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-08 20:07:23,024 EPOCH 1903
2024-02-08 20:07:27,462 [Epoch: 1903 Step: 00064700] Batch Recognition Loss:   0.000178 => Gls Tokens per Sec:     2249 || Batch Translation Loss:   0.040154 => Txt Tokens per Sec:     6279 || Lr: 0.000050
2024-02-08 20:07:27,649 Epoch 1903: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-08 20:07:27,649 EPOCH 1904
2024-02-08 20:07:32,540 Epoch 1904: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-08 20:07:32,541 EPOCH 1905
2024-02-08 20:07:37,162 Epoch 1905: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-08 20:07:37,163 EPOCH 1906
2024-02-08 20:07:41,698 [Epoch: 1906 Step: 00064800] Batch Recognition Loss:   0.000202 => Gls Tokens per Sec:     2117 || Batch Translation Loss:   0.045871 => Txt Tokens per Sec:     5875 || Lr: 0.000050
2024-02-08 20:07:42,142 Epoch 1906: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-08 20:07:42,142 EPOCH 1907
2024-02-08 20:07:46,667 Epoch 1907: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-08 20:07:46,667 EPOCH 1908
2024-02-08 20:07:50,772 Epoch 1908: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-08 20:07:50,772 EPOCH 1909
2024-02-08 20:07:54,690 [Epoch: 1909 Step: 00064900] Batch Recognition Loss:   0.000345 => Gls Tokens per Sec:     2221 || Batch Translation Loss:   0.015946 => Txt Tokens per Sec:     6167 || Lr: 0.000050
2024-02-08 20:07:55,710 Epoch 1909: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-08 20:07:55,710 EPOCH 1910
2024-02-08 20:08:00,186 Epoch 1910: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-08 20:08:00,186 EPOCH 1911
2024-02-08 20:08:04,866 Epoch 1911: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.28 
2024-02-08 20:08:04,867 EPOCH 1912
2024-02-08 20:08:08,575 [Epoch: 1912 Step: 00065000] Batch Recognition Loss:   0.000752 => Gls Tokens per Sec:     2174 || Batch Translation Loss:   0.036296 => Txt Tokens per Sec:     6119 || Lr: 0.000050
2024-02-08 20:08:09,511 Epoch 1912: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.20 
2024-02-08 20:08:09,511 EPOCH 1913
2024-02-08 20:08:13,879 Epoch 1913: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.28 
2024-02-08 20:08:13,880 EPOCH 1914
2024-02-08 20:08:18,868 Epoch 1914: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.88 
2024-02-08 20:08:18,868 EPOCH 1915
2024-02-08 20:08:21,970 [Epoch: 1915 Step: 00065100] Batch Recognition Loss:   0.000469 => Gls Tokens per Sec:     2477 || Batch Translation Loss:   0.016061 => Txt Tokens per Sec:     7059 || Lr: 0.000050
2024-02-08 20:08:23,018 Epoch 1915: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-08 20:08:23,018 EPOCH 1916
2024-02-08 20:08:27,984 Epoch 1916: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-08 20:08:27,984 EPOCH 1917
2024-02-08 20:08:32,314 Epoch 1917: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-08 20:08:32,314 EPOCH 1918
2024-02-08 20:08:35,490 [Epoch: 1918 Step: 00065200] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:     2135 || Batch Translation Loss:   0.011195 => Txt Tokens per Sec:     5983 || Lr: 0.000050
2024-02-08 20:08:37,118 Epoch 1918: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-08 20:08:37,118 EPOCH 1919
2024-02-08 20:08:41,658 Epoch 1919: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-08 20:08:41,659 EPOCH 1920
2024-02-08 20:08:46,295 Epoch 1920: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-08 20:08:46,296 EPOCH 1921
2024-02-08 20:08:49,106 [Epoch: 1921 Step: 00065300] Batch Recognition Loss:   0.001705 => Gls Tokens per Sec:     2186 || Batch Translation Loss:   0.010455 => Txt Tokens per Sec:     6070 || Lr: 0.000050
2024-02-08 20:08:51,052 Epoch 1921: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-08 20:08:51,053 EPOCH 1922
2024-02-08 20:08:55,211 Epoch 1922: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.51 
2024-02-08 20:08:55,211 EPOCH 1923
2024-02-08 20:09:00,232 Epoch 1923: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-08 20:09:00,233 EPOCH 1924
2024-02-08 20:09:02,376 [Epoch: 1924 Step: 00065400] Batch Recognition Loss:   0.000226 => Gls Tokens per Sec:     2689 || Batch Translation Loss:   0.022617 => Txt Tokens per Sec:     7369 || Lr: 0.000050
2024-02-08 20:09:04,468 Epoch 1924: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-08 20:09:04,469 EPOCH 1925
2024-02-08 20:09:09,360 Epoch 1925: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-08 20:09:09,360 EPOCH 1926
2024-02-08 20:09:13,776 Epoch 1926: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-08 20:09:13,777 EPOCH 1927
2024-02-08 20:09:15,796 [Epoch: 1927 Step: 00065500] Batch Recognition Loss:   0.002726 => Gls Tokens per Sec:     2536 || Batch Translation Loss:   0.024298 => Txt Tokens per Sec:     6984 || Lr: 0.000050
2024-02-08 20:09:18,484 Epoch 1927: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-08 20:09:18,485 EPOCH 1928
2024-02-08 20:09:23,171 Epoch 1928: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-08 20:09:23,171 EPOCH 1929
2024-02-08 20:09:27,317 Epoch 1929: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-08 20:09:27,317 EPOCH 1930
2024-02-08 20:09:28,966 [Epoch: 1930 Step: 00065600] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:     2718 || Batch Translation Loss:   0.014756 => Txt Tokens per Sec:     7584 || Lr: 0.000050
2024-02-08 20:09:31,999 Epoch 1930: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.41 
2024-02-08 20:09:32,000 EPOCH 1931
2024-02-08 20:09:36,834 Epoch 1931: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.37 
2024-02-08 20:09:36,835 EPOCH 1932
2024-02-08 20:09:41,459 Epoch 1932: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.51 
2024-02-08 20:09:41,460 EPOCH 1933
2024-02-08 20:09:42,847 [Epoch: 1933 Step: 00065700] Batch Recognition Loss:   0.000404 => Gls Tokens per Sec:     2585 || Batch Translation Loss:   0.041298 => Txt Tokens per Sec:     6752 || Lr: 0.000050
2024-02-08 20:09:46,314 Epoch 1933: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.84 
2024-02-08 20:09:46,315 EPOCH 1934
2024-02-08 20:09:50,921 Epoch 1934: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.53 
2024-02-08 20:09:50,922 EPOCH 1935
2024-02-08 20:09:55,828 Epoch 1935: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.95 
2024-02-08 20:09:55,828 EPOCH 1936
2024-02-08 20:09:57,211 [Epoch: 1936 Step: 00065800] Batch Recognition Loss:   0.000489 => Gls Tokens per Sec:     2315 || Batch Translation Loss:   0.029333 => Txt Tokens per Sec:     6589 || Lr: 0.000050
2024-02-08 20:10:00,458 Epoch 1936: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-08 20:10:00,458 EPOCH 1937
2024-02-08 20:10:05,530 Epoch 1937: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-08 20:10:05,531 EPOCH 1938
2024-02-08 20:10:10,287 Epoch 1938: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.68 
2024-02-08 20:10:10,287 EPOCH 1939
2024-02-08 20:10:11,155 [Epoch: 1939 Step: 00065900] Batch Recognition Loss:   0.000309 => Gls Tokens per Sec:     2653 || Batch Translation Loss:   0.010857 => Txt Tokens per Sec:     6800 || Lr: 0.000050
2024-02-08 20:10:15,076 Epoch 1939: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-08 20:10:15,077 EPOCH 1940
2024-02-08 20:10:19,948 Epoch 1940: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-08 20:10:19,948 EPOCH 1941
2024-02-08 20:10:24,615 Epoch 1941: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.61 
2024-02-08 20:10:24,616 EPOCH 1942
2024-02-08 20:10:25,613 [Epoch: 1942 Step: 00066000] Batch Recognition Loss:   0.001446 => Gls Tokens per Sec:     1930 || Batch Translation Loss:   0.029204 => Txt Tokens per Sec:     5548 || Lr: 0.000050
2024-02-08 20:10:34,556 Validation result at epoch 1942, step    66000: duration: 8.9430s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.99667	Translation Loss: 97896.29688	PPL: 17639.50000
	Eval Metric: BLEU
	WER 2.33	(DEL: 0.00,	INS: 0.00,	SUB: 2.33)
	BLEU-4 0.38	(BLEU-1: 10.11,	BLEU-2: 2.75,	BLEU-3: 0.92,	BLEU-4: 0.38)
	CHRF 16.70	ROUGE 8.68
2024-02-08 20:10:34,557 Logging Recognition and Translation Outputs
2024-02-08 20:10:34,557 ========================================================================================================================
2024-02-08 20:10:34,557 Logging Sequence: 85_97.00
2024-02-08 20:10:34,557 	Gloss Reference :	A B+C+D+E
2024-02-08 20:10:34,558 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 20:10:34,558 	Gloss Alignment :	         
2024-02-08 20:10:34,558 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 20:10:34,559 	Text Reference  :	******* *** ****** * ********* ** ** like  india's bcci australia has cricket australia
2024-02-08 20:10:34,559 	Text Hypothesis :	symonds has scored 2 centuries in 26 tests that    he   played    for his     country  
2024-02-08 20:10:34,559 	Text Alignment  :	I       I   I      I I         I  I  S     S       S    S         S   S       S        
2024-02-08 20:10:34,559 ========================================================================================================================
2024-02-08 20:10:34,559 Logging Sequence: 53_161.00
2024-02-08 20:10:34,559 	Gloss Reference :	A B+C+D+E
2024-02-08 20:10:34,560 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 20:10:34,560 	Gloss Alignment :	         
2024-02-08 20:10:34,560 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 20:10:34,562 	Text Reference  :	rashid has     also     been urging people  to donate   to    his      rashid khan foundation and     afghanistan cricket association
2024-02-08 20:10:34,562 	Text Hypothesis :	****** another incident was  a      picture of mohammed shami fielding with   a    towel      wrapped over        his     jersey     
2024-02-08 20:10:34,562 	Text Alignment  :	D      S       S        S    S      S       S  S        S     S        S      S    S          S       S           S       S          
2024-02-08 20:10:34,562 ========================================================================================================================
2024-02-08 20:10:34,562 Logging Sequence: 101_97.00
2024-02-08 20:10:34,563 	Gloss Reference :	A B+C+D+E
2024-02-08 20:10:34,563 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 20:10:34,563 	Gloss Alignment :	         
2024-02-08 20:10:34,563 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 20:10:34,565 	Text Reference  :	2 of the best indian bowlers who bowled very well were raj     bawa    who     took 5   wickets and   ravi kumar who    took 4  wickets
2024-02-08 20:10:34,565 	Text Hypothesis :	* ** *** **** ****** ******* *** ****** **** **** when england started batting the  won the     first time a     scored only 50 runs   
2024-02-08 20:10:34,565 	Text Alignment  :	D D  D   D    D      D       D   D      D    D    S    S       S       S       S    S   S       S     S    S     S      S    S  S      
2024-02-08 20:10:34,565 ========================================================================================================================
2024-02-08 20:10:34,566 Logging Sequence: 118_130.00
2024-02-08 20:10:34,566 	Gloss Reference :	A B+C+D+E
2024-02-08 20:10:34,566 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 20:10:34,566 	Gloss Alignment :	         
2024-02-08 20:10:34,566 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 20:10:34,567 	Text Reference  :	messi's fans the entire team were in tears everyone was overwhelmed by       the    victory
2024-02-08 20:10:34,567 	Text Hypothesis :	******* **** *** ****** **** **** ** ***** this     was krunal      pandya's maiden odi    
2024-02-08 20:10:34,567 	Text Alignment  :	D       D    D   D      D    D    D  D     S            S           S        S      S      
2024-02-08 20:10:34,567 ========================================================================================================================
2024-02-08 20:10:34,568 Logging Sequence: 170_195.00
2024-02-08 20:10:34,568 	Gloss Reference :	A B+C+D+E
2024-02-08 20:10:34,568 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 20:10:34,568 	Gloss Alignment :	         
2024-02-08 20:10:34,568 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 20:10:34,569 	Text Reference  :	i moved to   rajasthan royals team  as  they      paid me     8     crores
2024-02-08 20:10:34,569 	Text Hypothesis :	* they  will be        told   their fan following the  social media 2023  
2024-02-08 20:10:34,569 	Text Alignment  :	D S     S    S         S      S     S   S         S    S      S     S     
2024-02-08 20:10:34,570 ========================================================================================================================
2024-02-08 20:10:38,432 Epoch 1942: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.81 
2024-02-08 20:10:38,432 EPOCH 1943
2024-02-08 20:10:42,695 Epoch 1943: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.03 
2024-02-08 20:10:42,696 EPOCH 1944
2024-02-08 20:10:47,312 Epoch 1944: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.48 
2024-02-08 20:10:47,313 EPOCH 1945
2024-02-08 20:10:47,900 [Epoch: 1945 Step: 00066100] Batch Recognition Loss:   0.000646 => Gls Tokens per Sec:     2184 || Batch Translation Loss:   0.029350 => Txt Tokens per Sec:     6230 || Lr: 0.000050
2024-02-08 20:10:52,078 Epoch 1945: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.06 
2024-02-08 20:10:52,079 EPOCH 1946
2024-02-08 20:10:56,330 Epoch 1946: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-08 20:10:56,330 EPOCH 1947
2024-02-08 20:11:01,300 Epoch 1947: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.75 
2024-02-08 20:11:01,300 EPOCH 1948
2024-02-08 20:11:01,472 [Epoch: 1948 Step: 00066200] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:     3743 || Batch Translation Loss:   0.018097 => Txt Tokens per Sec:     9292 || Lr: 0.000050
2024-02-08 20:11:05,505 Epoch 1948: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-08 20:11:05,505 EPOCH 1949
2024-02-08 20:11:10,448 Epoch 1949: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.45 
2024-02-08 20:11:10,449 EPOCH 1950
2024-02-08 20:11:14,855 [Epoch: 1950 Step: 00066300] Batch Recognition Loss:   0.000301 => Gls Tokens per Sec:     2411 || Batch Translation Loss:   0.020203 => Txt Tokens per Sec:     6670 || Lr: 0.000050
2024-02-08 20:11:14,856 Epoch 1950: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.89 
2024-02-08 20:11:14,856 EPOCH 1951
2024-02-08 20:11:19,637 Epoch 1951: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-08 20:11:19,637 EPOCH 1952
2024-02-08 20:11:24,696 Epoch 1952: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-08 20:11:24,697 EPOCH 1953
2024-02-08 20:11:28,787 [Epoch: 1953 Step: 00066400] Batch Recognition Loss:   0.000219 => Gls Tokens per Sec:     2441 || Batch Translation Loss:   0.035344 => Txt Tokens per Sec:     6781 || Lr: 0.000050
2024-02-08 20:11:28,958 Epoch 1953: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.56 
2024-02-08 20:11:28,959 EPOCH 1954
2024-02-08 20:11:33,757 Epoch 1954: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-08 20:11:33,758 EPOCH 1955
2024-02-08 20:11:38,253 Epoch 1955: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-08 20:11:38,253 EPOCH 1956
2024-02-08 20:11:42,242 [Epoch: 1956 Step: 00066500] Batch Recognition Loss:   0.001175 => Gls Tokens per Sec:     2342 || Batch Translation Loss:   0.025109 => Txt Tokens per Sec:     6468 || Lr: 0.000050
2024-02-08 20:11:42,791 Epoch 1956: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-08 20:11:42,791 EPOCH 1957
2024-02-08 20:11:47,579 Epoch 1957: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-08 20:11:47,579 EPOCH 1958
2024-02-08 20:11:51,802 Epoch 1958: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-08 20:11:51,802 EPOCH 1959
2024-02-08 20:11:56,062 [Epoch: 1959 Step: 00066600] Batch Recognition Loss:   0.000619 => Gls Tokens per Sec:     2104 || Batch Translation Loss:   0.034685 => Txt Tokens per Sec:     5942 || Lr: 0.000050
2024-02-08 20:11:56,796 Epoch 1959: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-08 20:11:56,796 EPOCH 1960
2024-02-08 20:12:00,990 Epoch 1960: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-08 20:12:00,990 EPOCH 1961
2024-02-08 20:12:05,860 Epoch 1961: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-08 20:12:05,861 EPOCH 1962
2024-02-08 20:12:09,099 [Epoch: 1962 Step: 00066700] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     2490 || Batch Translation Loss:   0.015399 => Txt Tokens per Sec:     6770 || Lr: 0.000050
2024-02-08 20:12:10,344 Epoch 1962: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-08 20:12:10,344 EPOCH 1963
2024-02-08 20:12:14,985 Epoch 1963: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-08 20:12:14,986 EPOCH 1964
2024-02-08 20:12:19,633 Epoch 1964: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-08 20:12:19,633 EPOCH 1965
2024-02-08 20:12:22,608 [Epoch: 1965 Step: 00066800] Batch Recognition Loss:   0.000125 => Gls Tokens per Sec:     2582 || Batch Translation Loss:   0.016414 => Txt Tokens per Sec:     7138 || Lr: 0.000050
2024-02-08 20:12:23,929 Epoch 1965: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-08 20:12:23,929 EPOCH 1966
2024-02-08 20:12:28,849 Epoch 1966: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-08 20:12:28,850 EPOCH 1967
2024-02-08 20:12:33,008 Epoch 1967: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-08 20:12:33,008 EPOCH 1968
2024-02-08 20:12:35,931 [Epoch: 1968 Step: 00066900] Batch Recognition Loss:   0.000232 => Gls Tokens per Sec:     2321 || Batch Translation Loss:   0.045994 => Txt Tokens per Sec:     6656 || Lr: 0.000050
2024-02-08 20:12:37,140 Epoch 1968: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-08 20:12:37,141 EPOCH 1969
2024-02-08 20:12:41,205 Epoch 1969: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.42 
2024-02-08 20:12:41,205 EPOCH 1970
2024-02-08 20:12:46,413 Epoch 1970: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.97 
2024-02-08 20:12:46,413 EPOCH 1971
2024-02-08 20:12:49,612 [Epoch: 1971 Step: 00067000] Batch Recognition Loss:   0.000328 => Gls Tokens per Sec:     2002 || Batch Translation Loss:   0.103140 => Txt Tokens per Sec:     5558 || Lr: 0.000050
2024-02-08 20:12:51,354 Epoch 1971: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.32 
2024-02-08 20:12:51,355 EPOCH 1972
2024-02-08 20:12:55,662 Epoch 1972: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.56 
2024-02-08 20:12:55,663 EPOCH 1973
2024-02-08 20:13:00,618 Epoch 1973: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.10 
2024-02-08 20:13:00,618 EPOCH 1974
2024-02-08 20:13:02,757 [Epoch: 1974 Step: 00067100] Batch Recognition Loss:   0.000305 => Gls Tokens per Sec:     2694 || Batch Translation Loss:   0.021560 => Txt Tokens per Sec:     7495 || Lr: 0.000050
2024-02-08 20:13:04,746 Epoch 1974: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.85 
2024-02-08 20:13:04,746 EPOCH 1975
2024-02-08 20:13:09,654 Epoch 1975: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.71 
2024-02-08 20:13:09,655 EPOCH 1976
2024-02-08 20:13:14,092 Epoch 1976: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-08 20:13:14,093 EPOCH 1977
2024-02-08 20:13:16,043 [Epoch: 1977 Step: 00067200] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:     2625 || Batch Translation Loss:   0.010970 => Txt Tokens per Sec:     7101 || Lr: 0.000050
2024-02-08 20:13:18,824 Epoch 1977: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-08 20:13:18,824 EPOCH 1978
2024-02-08 20:13:23,478 Epoch 1978: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-08 20:13:23,478 EPOCH 1979
2024-02-08 20:13:27,986 Epoch 1979: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-08 20:13:27,987 EPOCH 1980
2024-02-08 20:13:30,224 [Epoch: 1980 Step: 00067300] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:     2004 || Batch Translation Loss:   0.018790 => Txt Tokens per Sec:     5708 || Lr: 0.000050
2024-02-08 20:13:32,796 Epoch 1980: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-08 20:13:32,796 EPOCH 1981
2024-02-08 20:13:36,924 Epoch 1981: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-08 20:13:36,924 EPOCH 1982
2024-02-08 20:13:42,189 Epoch 1982: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-08 20:13:42,189 EPOCH 1983
2024-02-08 20:13:43,938 [Epoch: 1983 Step: 00067400] Batch Recognition Loss:   0.002600 => Gls Tokens per Sec:     2048 || Batch Translation Loss:   0.015766 => Txt Tokens per Sec:     6018 || Lr: 0.000050
2024-02-08 20:13:46,822 Epoch 1983: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-08 20:13:46,822 EPOCH 1984
2024-02-08 20:13:51,366 Epoch 1984: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-08 20:13:51,367 EPOCH 1985
2024-02-08 20:13:56,220 Epoch 1985: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-08 20:13:56,221 EPOCH 1986
2024-02-08 20:13:57,350 [Epoch: 1986 Step: 00067500] Batch Recognition Loss:   0.000261 => Gls Tokens per Sec:     2834 || Batch Translation Loss:   0.010996 => Txt Tokens per Sec:     7808 || Lr: 0.000050
2024-02-08 20:14:00,420 Epoch 1986: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-08 20:14:00,421 EPOCH 1987
2024-02-08 20:14:05,408 Epoch 1987: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-08 20:14:05,409 EPOCH 1988
2024-02-08 20:14:09,626 Epoch 1988: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-08 20:14:09,626 EPOCH 1989
2024-02-08 20:14:10,756 [Epoch: 1989 Step: 00067600] Batch Recognition Loss:   0.000148 => Gls Tokens per Sec:     2037 || Batch Translation Loss:   0.014677 => Txt Tokens per Sec:     6151 || Lr: 0.000050
2024-02-08 20:14:14,568 Epoch 1989: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.43 
2024-02-08 20:14:14,568 EPOCH 1990
2024-02-08 20:14:19,019 Epoch 1990: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-08 20:14:19,019 EPOCH 1991
2024-02-08 20:14:23,656 Epoch 1991: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.44 
2024-02-08 20:14:23,656 EPOCH 1992
2024-02-08 20:14:24,462 [Epoch: 1992 Step: 00067700] Batch Recognition Loss:   0.000100 => Gls Tokens per Sec:     2385 || Batch Translation Loss:   0.011527 => Txt Tokens per Sec:     6211 || Lr: 0.000050
2024-02-08 20:14:28,326 Epoch 1992: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-08 20:14:28,327 EPOCH 1993
2024-02-08 20:14:32,635 Epoch 1993: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-08 20:14:32,636 EPOCH 1994
2024-02-08 20:14:37,808 Epoch 1994: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.41 
2024-02-08 20:14:37,808 EPOCH 1995
2024-02-08 20:14:38,361 [Epoch: 1995 Step: 00067800] Batch Recognition Loss:   0.000865 => Gls Tokens per Sec:     2319 || Batch Translation Loss:   0.016575 => Txt Tokens per Sec:     6737 || Lr: 0.000050
2024-02-08 20:14:42,531 Epoch 1995: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.47 
2024-02-08 20:14:42,531 EPOCH 1996
2024-02-08 20:14:47,313 Epoch 1996: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.63 
2024-02-08 20:14:47,313 EPOCH 1997
2024-02-08 20:14:51,844 Epoch 1997: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-08 20:14:51,844 EPOCH 1998
2024-02-08 20:14:52,033 [Epoch: 1998 Step: 00067900] Batch Recognition Loss:   0.000683 => Gls Tokens per Sec:     3404 || Batch Translation Loss:   0.013327 => Txt Tokens per Sec:     8048 || Lr: 0.000050
2024-02-08 20:14:56,285 Epoch 1998: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-08 20:14:56,285 EPOCH 1999
2024-02-08 20:15:01,081 Epoch 1999: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-08 20:15:01,081 EPOCH 2000
2024-02-08 20:15:05,172 [Epoch: 2000 Step: 00068000] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:     2597 || Batch Translation Loss:   0.012663 => Txt Tokens per Sec:     7185 || Lr: 0.000050
2024-02-08 20:15:14,042 Validation result at epoch 2000, step    68000: duration: 8.8700s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.13869	Translation Loss: 99311.90625	PPL: 20318.49414
	Eval Metric: BLEU
	WER 2.26	(DEL: 0.00,	INS: 0.00,	SUB: 2.26)
	BLEU-4 0.55	(BLEU-1: 9.76,	BLEU-2: 2.70,	BLEU-3: 1.10,	BLEU-4: 0.55)
	CHRF 15.98	ROUGE 8.66
2024-02-08 20:15:14,043 Logging Recognition and Translation Outputs
2024-02-08 20:15:14,043 ========================================================================================================================
2024-02-08 20:15:14,044 Logging Sequence: 148_186.00
2024-02-08 20:15:14,044 	Gloss Reference :	A B+C+D+E
2024-02-08 20:15:14,044 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 20:15:14,044 	Gloss Alignment :	         
2024-02-08 20:15:14,044 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 20:15:14,046 	Text Reference  :	siraj also took four   wickets  in     1      over thus    becoming the record-holder for          most wickets in an     over in    odis  
2024-02-08 20:15:14,046 	Text Hypothesis :	***** **** the  sports minister anurag thakur also present at       the ************* participated in   several 5  months of   south africa
2024-02-08 20:15:14,046 	Text Alignment  :	D     D    S    S      S        S      S      S    S       S            D             S            S    S       S  S      S    S     S     
2024-02-08 20:15:14,047 ========================================================================================================================
2024-02-08 20:15:14,047 Logging Sequence: 61_181.00
2024-02-08 20:15:14,047 	Gloss Reference :	A B+C+D+E
2024-02-08 20:15:14,047 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 20:15:14,047 	Gloss Alignment :	         
2024-02-08 20:15:14,048 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 20:15:14,049 	Text Reference  :	one other fan said     it is  babar's personal chat    we   should focuc on   this       cricketing career  and not his personal life
2024-02-08 20:15:14,050 	Text Hypothesis :	*** ***** *** shocking he has now     being    brought down as     one   more viewership by         parents and *** *** ******** 2019
2024-02-08 20:15:14,050 	Text Alignment  :	D   D     D   S        S  S   S       S        S       S    S      S     S    S          S          S           D   D   D        S   
2024-02-08 20:15:14,050 ========================================================================================================================
2024-02-08 20:15:14,050 Logging Sequence: 123_24.00
2024-02-08 20:15:14,050 	Gloss Reference :	A B+C+D+E
2024-02-08 20:15:14,050 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 20:15:14,051 	Gloss Alignment :	         
2024-02-08 20:15:14,051 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 20:15:14,052 	Text Reference  :	did you know that other than cricket dhoni  has  another passion
2024-02-08 20:15:14,052 	Text Hypothesis :	*** *** **** **** he    was  then    calmed down by      head   
2024-02-08 20:15:14,052 	Text Alignment  :	D   D   D    D    S     S    S       S      S    S       S      
2024-02-08 20:15:14,052 ========================================================================================================================
2024-02-08 20:15:14,052 Logging Sequence: 84_76.00
2024-02-08 20:15:14,052 	Gloss Reference :	A B+C+D+E
2024-02-08 20:15:14,053 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 20:15:14,053 	Gloss Alignment :	         
2024-02-08 20:15:14,053 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 20:15:14,054 	Text Reference  :	** *** the  teams wanted to    support but     were refused 
2024-02-08 20:15:14,054 	Text Hypothesis :	do you know that  the    match and     respect was  shocking
2024-02-08 20:15:14,054 	Text Alignment  :	I  I   S    S     S      S     S       S       S    S       
2024-02-08 20:15:14,054 ========================================================================================================================
2024-02-08 20:15:14,054 Logging Sequence: 126_188.00
2024-02-08 20:15:14,054 	Gloss Reference :	A B+C+D+E
2024-02-08 20:15:14,055 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 20:15:14,055 	Gloss Alignment :	         
2024-02-08 20:15:14,055 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 20:15:14,056 	Text Reference  :	now he  has become a    gold  medalist at   the   2020 tokyo olympics
2024-02-08 20:15:14,056 	Text Hypothesis :	*** and the second time india were     very happy in   gold  medal   
2024-02-08 20:15:14,056 	Text Alignment  :	D   S   S   S      S    S     S        S    S     S    S     S       
2024-02-08 20:15:14,056 ========================================================================================================================
2024-02-08 20:15:14,060 Epoch 2000: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-08 20:15:14,060 EPOCH 2001
2024-02-08 20:15:19,338 Epoch 2001: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-08 20:15:19,338 EPOCH 2002
2024-02-08 20:15:23,854 Epoch 2002: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.82 
2024-02-08 20:15:23,855 EPOCH 2003
2024-02-08 20:15:28,046 [Epoch: 2003 Step: 00068100] Batch Recognition Loss:   0.000276 => Gls Tokens per Sec:     2382 || Batch Translation Loss:   0.075695 => Txt Tokens per Sec:     6529 || Lr: 0.000050
2024-02-08 20:15:28,395 Epoch 2003: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.90 
2024-02-08 20:15:28,395 EPOCH 2004
2024-02-08 20:15:33,145 Epoch 2004: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.21 
2024-02-08 20:15:33,145 EPOCH 2005
2024-02-08 20:15:37,339 Epoch 2005: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.41 
2024-02-08 20:15:37,340 EPOCH 2006
2024-02-08 20:15:41,974 [Epoch: 2006 Step: 00068200] Batch Recognition Loss:   0.002041 => Gls Tokens per Sec:     2016 || Batch Translation Loss:   0.031511 => Txt Tokens per Sec:     5672 || Lr: 0.000050
2024-02-08 20:15:42,365 Epoch 2006: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.01 
2024-02-08 20:15:42,366 EPOCH 2007
2024-02-08 20:15:46,573 Epoch 2007: Total Training Recognition Loss 0.14  Total Training Translation Loss 0.80 
2024-02-08 20:15:46,573 EPOCH 2008
2024-02-08 20:15:51,417 Epoch 2008: Total Training Recognition Loss 0.46  Total Training Translation Loss 0.95 
2024-02-08 20:15:51,418 EPOCH 2009
2024-02-08 20:15:55,083 [Epoch: 2009 Step: 00068300] Batch Recognition Loss:   0.000453 => Gls Tokens per Sec:     2445 || Batch Translation Loss:   0.033822 => Txt Tokens per Sec:     6740 || Lr: 0.000050
2024-02-08 20:15:55,877 Epoch 2009: Total Training Recognition Loss 0.13  Total Training Translation Loss 1.12 
2024-02-08 20:15:55,877 EPOCH 2010
2024-02-08 20:16:00,415 Epoch 2010: Total Training Recognition Loss 0.23  Total Training Translation Loss 0.87 
2024-02-08 20:16:00,415 EPOCH 2011
2024-02-08 20:16:05,270 Epoch 2011: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.96 
2024-02-08 20:16:05,270 EPOCH 2012
2024-02-08 20:16:08,922 [Epoch: 2012 Step: 00068400] Batch Recognition Loss:   0.000433 => Gls Tokens per Sec:     2208 || Batch Translation Loss:   0.019397 => Txt Tokens per Sec:     6075 || Lr: 0.000050
2024-02-08 20:16:10,333 Epoch 2012: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.73 
2024-02-08 20:16:10,333 EPOCH 2013
2024-02-08 20:16:15,141 Epoch 2013: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.71 
2024-02-08 20:16:15,141 EPOCH 2014
2024-02-08 20:16:19,251 Epoch 2014: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-08 20:16:19,251 EPOCH 2015
2024-02-08 20:16:23,072 [Epoch: 2015 Step: 00068500] Batch Recognition Loss:   0.001202 => Gls Tokens per Sec:     1943 || Batch Translation Loss:   0.004523 => Txt Tokens per Sec:     5375 || Lr: 0.000050
2024-02-08 20:16:24,476 Epoch 2015: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-08 20:16:24,477 EPOCH 2016
2024-02-08 20:16:29,317 Epoch 2016: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-08 20:16:29,318 EPOCH 2017
2024-02-08 20:16:33,967 Epoch 2017: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-08 20:16:33,968 EPOCH 2018
2024-02-08 20:16:37,133 [Epoch: 2018 Step: 00068600] Batch Recognition Loss:   0.000825 => Gls Tokens per Sec:     2143 || Batch Translation Loss:   0.026829 => Txt Tokens per Sec:     5992 || Lr: 0.000050
2024-02-08 20:16:38,766 Epoch 2018: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-08 20:16:38,766 EPOCH 2019
2024-02-08 20:16:43,345 Epoch 2019: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-08 20:16:43,345 EPOCH 2020
2024-02-08 20:16:48,232 Epoch 2020: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-08 20:16:48,233 EPOCH 2021
2024-02-08 20:16:50,725 [Epoch: 2021 Step: 00068700] Batch Recognition Loss:   0.000208 => Gls Tokens per Sec:     2465 || Batch Translation Loss:   0.016114 => Txt Tokens per Sec:     6777 || Lr: 0.000050
2024-02-08 20:16:52,838 Epoch 2021: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-08 20:16:52,838 EPOCH 2022
2024-02-08 20:16:57,719 Epoch 2022: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-08 20:16:57,720 EPOCH 2023
2024-02-08 20:17:02,379 Epoch 2023: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-08 20:17:02,380 EPOCH 2024
2024-02-08 20:17:04,420 [Epoch: 2024 Step: 00068800] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     2824 || Batch Translation Loss:   0.018727 => Txt Tokens per Sec:     7556 || Lr: 0.000050
2024-02-08 20:17:06,469 Epoch 2024: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.28 
2024-02-08 20:17:06,469 EPOCH 2025
2024-02-08 20:17:11,304 Epoch 2025: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.20 
2024-02-08 20:17:11,305 EPOCH 2026
2024-02-08 20:17:16,121 Epoch 2026: Total Training Recognition Loss 0.05  Total Training Translation Loss 6.68 
2024-02-08 20:17:16,122 EPOCH 2027
2024-02-08 20:17:18,464 [Epoch: 2027 Step: 00068900] Batch Recognition Loss:   0.000425 => Gls Tokens per Sec:     2076 || Batch Translation Loss:   0.017829 => Txt Tokens per Sec:     5850 || Lr: 0.000050
2024-02-08 20:17:20,838 Epoch 2027: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.72 
2024-02-08 20:17:20,838 EPOCH 2028
2024-02-08 20:17:24,988 Epoch 2028: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.83 
2024-02-08 20:17:24,988 EPOCH 2029
2024-02-08 20:17:29,647 Epoch 2029: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.65 
2024-02-08 20:17:29,648 EPOCH 2030
2024-02-08 20:17:31,735 [Epoch: 2030 Step: 00069000] Batch Recognition Loss:   0.000444 => Gls Tokens per Sec:     2023 || Batch Translation Loss:   0.011898 => Txt Tokens per Sec:     5710 || Lr: 0.000050
2024-02-08 20:17:34,298 Epoch 2030: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.62 
2024-02-08 20:17:34,298 EPOCH 2031
2024-02-08 20:17:38,672 Epoch 2031: Total Training Recognition Loss 0.28  Total Training Translation Loss 0.60 
2024-02-08 20:17:38,672 EPOCH 2032
2024-02-08 20:17:43,521 Epoch 2032: Total Training Recognition Loss 0.68  Total Training Translation Loss 0.87 
2024-02-08 20:17:43,521 EPOCH 2033
2024-02-08 20:17:44,995 [Epoch: 2033 Step: 00069100] Batch Recognition Loss:   0.000573 => Gls Tokens per Sec:     2430 || Batch Translation Loss:   0.013649 => Txt Tokens per Sec:     6574 || Lr: 0.000050
2024-02-08 20:17:47,667 Epoch 2033: Total Training Recognition Loss 0.16  Total Training Translation Loss 0.61 
2024-02-08 20:17:47,667 EPOCH 2034
2024-02-08 20:17:52,649 Epoch 2034: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-08 20:17:52,650 EPOCH 2035
2024-02-08 20:17:57,010 Epoch 2035: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-08 20:17:57,010 EPOCH 2036
2024-02-08 20:17:58,150 [Epoch: 2036 Step: 00069200] Batch Recognition Loss:   0.000241 => Gls Tokens per Sec:     2581 || Batch Translation Loss:   0.011504 => Txt Tokens per Sec:     7088 || Lr: 0.000050
2024-02-08 20:18:01,071 Epoch 2036: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-08 20:18:01,071 EPOCH 2037
2024-02-08 20:18:05,192 Epoch 2037: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-08 20:18:05,192 EPOCH 2038
2024-02-08 20:18:10,227 Epoch 2038: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-08 20:18:10,227 EPOCH 2039
2024-02-08 20:18:11,462 [Epoch: 2039 Step: 00069300] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     2075 || Batch Translation Loss:   0.014829 => Txt Tokens per Sec:     6019 || Lr: 0.000050
2024-02-08 20:18:14,480 Epoch 2039: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-08 20:18:14,480 EPOCH 2040
2024-02-08 20:18:19,295 Epoch 2040: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-08 20:18:19,295 EPOCH 2041
2024-02-08 20:18:23,797 Epoch 2041: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-08 20:18:23,797 EPOCH 2042
2024-02-08 20:18:24,299 [Epoch: 2042 Step: 00069400] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:     3832 || Batch Translation Loss:   0.015055 => Txt Tokens per Sec:     9399 || Lr: 0.000050
2024-02-08 20:18:27,978 Epoch 2042: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-08 20:18:27,979 EPOCH 2043
2024-02-08 20:18:32,961 Epoch 2043: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-08 20:18:32,961 EPOCH 2044
2024-02-08 20:18:37,198 Epoch 2044: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-08 20:18:37,198 EPOCH 2045
2024-02-08 20:18:37,678 [Epoch: 2045 Step: 00069500] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     2678 || Batch Translation Loss:   0.012640 => Txt Tokens per Sec:     7933 || Lr: 0.000050
2024-02-08 20:18:42,038 Epoch 2045: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-08 20:18:42,039 EPOCH 2046
2024-02-08 20:18:46,527 Epoch 2046: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-08 20:18:46,527 EPOCH 2047
2024-02-08 20:18:51,171 Epoch 2047: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-08 20:18:51,172 EPOCH 2048
2024-02-08 20:18:51,389 [Epoch: 2048 Step: 00069600] Batch Recognition Loss:   0.000129 => Gls Tokens per Sec:     2977 || Batch Translation Loss:   0.010875 => Txt Tokens per Sec:     7814 || Lr: 0.000050
2024-02-08 20:18:55,837 Epoch 2048: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-08 20:18:55,837 EPOCH 2049
2024-02-08 20:19:00,183 Epoch 2049: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.53 
2024-02-08 20:19:00,184 EPOCH 2050
2024-02-08 20:19:05,144 [Epoch: 2050 Step: 00069700] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     2141 || Batch Translation Loss:   0.015269 => Txt Tokens per Sec:     5924 || Lr: 0.000050
2024-02-08 20:19:05,145 Epoch 2050: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-08 20:19:05,145 EPOCH 2051
2024-02-08 20:19:09,264 Epoch 2051: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.68 
2024-02-08 20:19:09,264 EPOCH 2052
2024-02-08 20:19:14,267 Epoch 2052: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-08 20:19:14,268 EPOCH 2053
2024-02-08 20:19:18,380 [Epoch: 2053 Step: 00069800] Batch Recognition Loss:   0.000553 => Gls Tokens per Sec:     2427 || Batch Translation Loss:   0.027292 => Txt Tokens per Sec:     6737 || Lr: 0.000050
2024-02-08 20:19:18,606 Epoch 2053: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-08 20:19:18,606 EPOCH 2054
2024-02-08 20:19:23,397 Epoch 2054: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.00 
2024-02-08 20:19:23,398 EPOCH 2055
2024-02-08 20:19:28,458 Epoch 2055: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.22 
2024-02-08 20:19:28,459 EPOCH 2056
2024-02-08 20:19:32,251 [Epoch: 2056 Step: 00069900] Batch Recognition Loss:   0.000437 => Gls Tokens per Sec:     2464 || Batch Translation Loss:   0.117740 => Txt Tokens per Sec:     6834 || Lr: 0.000050
2024-02-08 20:19:32,663 Epoch 2056: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.04 
2024-02-08 20:19:32,663 EPOCH 2057
2024-02-08 20:19:37,440 Epoch 2057: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.68 
2024-02-08 20:19:37,440 EPOCH 2058
2024-02-08 20:19:41,966 Epoch 2058: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.13 
2024-02-08 20:19:41,966 EPOCH 2059
2024-02-08 20:19:45,702 [Epoch: 2059 Step: 00070000] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     2330 || Batch Translation Loss:   0.025627 => Txt Tokens per Sec:     6388 || Lr: 0.000050
2024-02-08 20:19:54,396 Validation result at epoch 2059, step    70000: duration: 8.6943s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.09741	Translation Loss: 97299.81250	PPL: 16619.28320
	Eval Metric: BLEU
	WER 2.40	(DEL: 0.00,	INS: 0.00,	SUB: 2.40)
	BLEU-4 0.58	(BLEU-1: 10.35,	BLEU-2: 3.11,	BLEU-3: 1.17,	BLEU-4: 0.58)
	CHRF 16.79	ROUGE 8.74
2024-02-08 20:19:54,397 Logging Recognition and Translation Outputs
2024-02-08 20:19:54,397 ========================================================================================================================
2024-02-08 20:19:54,397 Logging Sequence: 129_90.00
2024-02-08 20:19:54,398 	Gloss Reference :	A B+C+D+E
2024-02-08 20:19:54,398 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 20:19:54,398 	Gloss Alignment :	         
2024-02-08 20:19:54,398 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 20:19:54,400 	Text Reference  :	******** ******* **** **** **** **** however because of    the     emergency games will now be held    without any  spectators
2024-02-08 20:19:54,400 	Text Hypothesis :	everyone thought that they have been glued   to      their screens as        they  will *** be similar to      euro 2020      
2024-02-08 20:19:54,400 	Text Alignment  :	I        I       I    I    I    I    S       S       S     S       S         S          D      S       S       S    S         
2024-02-08 20:19:54,400 ========================================================================================================================
2024-02-08 20:19:54,400 Logging Sequence: 179_378.00
2024-02-08 20:19:54,400 	Gloss Reference :	A B+C+D+E
2024-02-08 20:19:54,401 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 20:19:54,401 	Gloss Alignment :	         
2024-02-08 20:19:54,401 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 20:19:54,402 	Text Reference  :	these kids think that they are      going to  the ***** olympics so     they've become some kind of   stars 
2024-02-08 20:19:54,402 	Text Hypothesis :	***** **** ***** **** the  response is    not the covid pandemic people are     now    get  a    huge amount
2024-02-08 20:19:54,402 	Text Alignment  :	D     D    D     D    S    S        S     S       I     S        S      S       S      S    S    S    S     
2024-02-08 20:19:54,403 ========================================================================================================================
2024-02-08 20:19:54,403 Logging Sequence: 162_20.00
2024-02-08 20:19:54,403 	Gloss Reference :	A B+C+D+E
2024-02-08 20:19:54,403 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 20:19:54,403 	Gloss Alignment :	         
2024-02-08 20:19:54,403 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 20:19:54,405 	Text Reference  :	not only this but they     blamed mohammed shami's religion as  the reason for  india's   loss 
2024-02-08 20:19:54,405 	Text Hypothesis :	*** **** he   is  survived by     his      wife    their    son and a      very captaincy crore
2024-02-08 20:19:54,405 	Text Alignment  :	D   D    S    S   S        S      S        S       S        S   S   S      S    S         S    
2024-02-08 20:19:54,405 ========================================================================================================================
2024-02-08 20:19:54,405 Logging Sequence: 106_169.00
2024-02-08 20:19:54,405 	Gloss Reference :	A B+C+D+E
2024-02-08 20:19:54,405 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 20:19:54,405 	Gloss Alignment :	         
2024-02-08 20:19:54,406 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 20:19:54,406 	Text Reference  :	prime minister narendra modi also expressed his happiness while congratulating the   team on   twitter
2024-02-08 20:19:54,407 	Text Hypothesis :	***** ******** ******** **** **** we        won the       toss  and            chose to   bowl indian 
2024-02-08 20:19:54,407 	Text Alignment  :	D     D        D        D    D    S         S   S         S     S              S     S    S    S      
2024-02-08 20:19:54,407 ========================================================================================================================
2024-02-08 20:19:54,407 Logging Sequence: 65_77.00
2024-02-08 20:19:54,407 	Gloss Reference :	A B+C+D+E
2024-02-08 20:19:54,407 	Gloss Hypothesis:	A B+C+D+E
2024-02-08 20:19:54,407 	Gloss Alignment :	         
2024-02-08 20:19:54,407 	--------------------------------------------------------------------------------------------------------------------
2024-02-08 20:19:54,408 	Text Reference  :	** *** ***** indian team travelling included 16    players   
2024-02-08 20:19:54,408 	Text Hypothesis :	in the match he     was  held       in       quick succession
2024-02-08 20:19:54,408 	Text Alignment  :	I  I   I     S      S    S          S        S     S         
2024-02-08 20:19:54,408 ========================================================================================================================
2024-02-08 20:19:54,412 Training ended since there were no improvements inthe last learning rate step: 0.000050
2024-02-08 20:19:54,412 Best validation result at step    18000:   0.92 eval_metric.
2024-02-08 20:20:18,987 ------------------------------------------------------------
2024-02-08 20:20:18,987 [DEV] partition [RECOGNITION] experiment [BW]: 1
2024-02-08 20:20:27,712 finished in 8.7245s 
2024-02-08 20:20:27,712 ************************************************************
2024-02-08 20:20:27,712 [DEV] partition [RECOGNITION] results:
	New Best CTC Decode Beam Size: 1
	WER 6.57	(DEL: 0.00,	INS: 0.00,	SUB: 6.57)
2024-02-08 20:20:27,713 ************************************************************
2024-02-08 20:20:27,713 ------------------------------------------------------------
2024-02-08 20:20:27,713 [DEV] partition [RECOGNITION] experiment [BW]: 2
2024-02-08 20:20:36,463 finished in 8.7504s 
2024-02-08 20:20:36,463 ************************************************************
2024-02-08 20:20:36,464 [DEV] partition [RECOGNITION] results:
	New Best CTC Decode Beam Size: 2
	WER 6.43	(DEL: 0.00,	INS: 0.00,	SUB: 6.43)
2024-02-08 20:20:36,464 ************************************************************
2024-02-08 20:20:36,464 ------------------------------------------------------------
2024-02-08 20:20:36,464 [DEV] partition [RECOGNITION] experiment [BW]: 3
2024-02-08 20:20:45,087 finished in 8.6220s 
2024-02-08 20:20:45,087 ------------------------------------------------------------
2024-02-08 20:20:45,087 [DEV] partition [RECOGNITION] experiment [BW]: 4
2024-02-08 20:20:53,688 finished in 8.6010s 
2024-02-08 20:20:53,689 ------------------------------------------------------------
2024-02-08 20:20:53,689 [DEV] partition [RECOGNITION] experiment [BW]: 5
2024-02-08 20:21:02,419 finished in 8.7300s 
2024-02-08 20:21:02,420 ------------------------------------------------------------
2024-02-08 20:21:02,420 [DEV] partition [RECOGNITION] experiment [BW]: 6
2024-02-08 20:21:10,837 finished in 8.4163s 
2024-02-08 20:21:10,838 ------------------------------------------------------------
2024-02-08 20:21:10,838 [DEV] partition [RECOGNITION] experiment [BW]: 7
2024-02-08 20:21:19,555 finished in 8.7161s 
2024-02-08 20:21:19,555 ------------------------------------------------------------
2024-02-08 20:21:19,555 [DEV] partition [RECOGNITION] experiment [BW]: 8
2024-02-08 20:21:28,282 finished in 8.7270s 
2024-02-08 20:21:28,283 ------------------------------------------------------------
2024-02-08 20:21:28,283 [DEV] partition [RECOGNITION] experiment [BW]: 9
2024-02-08 20:21:37,099 finished in 8.8160s 
2024-02-08 20:21:37,099 ------------------------------------------------------------
2024-02-08 20:21:37,099 [DEV] partition [RECOGNITION] experiment [BW]: 10
2024-02-08 20:21:45,781 finished in 8.6825s 
2024-02-08 20:21:45,782 ============================================================
2024-02-08 20:21:54,139 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 1 and Alpha: -1
	BLEU-4 0.92	(BLEU-1: 11.47,	BLEU-2: 3.78,	BLEU-3: 1.66,	BLEU-4: 0.92)
	CHRF 17.31	ROUGE 9.69
2024-02-08 20:21:54,139 ------------------------------------------------------------
2024-02-08 20:26:16,301 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 4 and Alpha: 3
	BLEU-4 0.95	(BLEU-1: 10.81,	BLEU-2: 3.75,	BLEU-3: 1.65,	BLEU-4: 0.95)
	CHRF 16.65	ROUGE 9.28
2024-02-08 20:26:16,301 ------------------------------------------------------------
2024-02-08 20:38:43,765 ************************************************************
2024-02-08 20:38:43,766 [DEV] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 2
	Best Translation Beam Size: 4 and Alpha: 3
	WER 6.43	(DEL: 0.00,	INS: 0.00,	SUB: 6.43)
	BLEU-4 0.95	(BLEU-1: 10.81,	BLEU-2: 3.75,	BLEU-3: 1.65,	BLEU-4: 0.95)
	CHRF 16.65	ROUGE 9.28
2024-02-08 20:38:43,766 ************************************************************
2024-02-08 20:38:56,055 [TEST] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 2
	Best Translation Beam Size: 4 and Alpha: 3
	WER 5.44	(DEL: 0.00,	INS: 0.00,	SUB: 5.44)
	BLEU-4 0.67	(BLEU-1: 10.09,	BLEU-2: 3.24,	BLEU-3: 1.30,	BLEU-4: 0.67)
	CHRF 16.65	ROUGE 8.74
2024-02-08 20:38:56,056 ************************************************************
