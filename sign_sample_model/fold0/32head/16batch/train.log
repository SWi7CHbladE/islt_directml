2024-02-02 22:23:02,055 Hello! This is Joey-NMT.
2024-02-02 22:23:02,063 Total params: 25639944
2024-02-02 22:23:02,064 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'gloss_output_layer.bias', 'gloss_output_layer.weight', 'sgn_embed.ln.bias', 'sgn_embed.ln.weight', 'sgn_embed.norm.norm.bias', 'sgn_embed.norm.norm.weight', 'txt_embed.norm.norm.bias', 'txt_embed.norm.norm.weight']
2024-02-02 22:23:03,025 cfg.name                           : sign_experiment
2024-02-02 22:23:03,025 cfg.data.data_path                 : ./data/Sports_dataset/0/
2024-02-02 22:23:03,025 cfg.data.version                   : phoenix_2014_trans
2024-02-02 22:23:03,025 cfg.data.sgn                       : sign
2024-02-02 22:23:03,025 cfg.data.txt                       : text
2024-02-02 22:23:03,026 cfg.data.gls                       : gloss
2024-02-02 22:23:03,026 cfg.data.train                     : excel_data.train
2024-02-02 22:23:03,026 cfg.data.dev                       : excel_data.dev
2024-02-02 22:23:03,026 cfg.data.test                      : excel_data.test
2024-02-02 22:23:03,026 cfg.data.feature_size              : 2560
2024-02-02 22:23:03,026 cfg.data.level                     : word
2024-02-02 22:23:03,026 cfg.data.txt_lowercase             : True
2024-02-02 22:23:03,026 cfg.data.max_sent_length           : 500
2024-02-02 22:23:03,026 cfg.data.random_train_subset       : -1
2024-02-02 22:23:03,027 cfg.data.random_dev_subset         : -1
2024-02-02 22:23:03,027 cfg.testing.recognition_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-02 22:23:03,027 cfg.testing.translation_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-02 22:23:03,027 cfg.testing.translation_beam_alphas : [-1, 0, 1, 2, 3, 4, 5]
2024-02-02 22:23:03,027 cfg.training.reset_best_ckpt       : False
2024-02-02 22:23:03,027 cfg.training.reset_scheduler       : False
2024-02-02 22:23:03,027 cfg.training.reset_optimizer       : False
2024-02-02 22:23:03,027 cfg.training.random_seed           : 42
2024-02-02 22:23:03,028 cfg.training.model_dir             : ./sign_sample_model/fold0/32head/16batch
2024-02-02 22:23:03,028 cfg.training.recognition_loss_weight : 1.0
2024-02-02 22:23:03,028 cfg.training.translation_loss_weight : 1.0
2024-02-02 22:23:03,028 cfg.training.eval_metric           : bleu
2024-02-02 22:23:03,028 cfg.training.optimizer             : adam
2024-02-02 22:23:03,028 cfg.training.learning_rate         : 0.0001
2024-02-02 22:23:03,028 cfg.training.batch_size            : 16
2024-02-02 22:23:03,028 cfg.training.num_valid_log         : 5
2024-02-02 22:23:03,028 cfg.training.epochs                : 50000
2024-02-02 22:23:03,029 cfg.training.early_stopping_metric : eval_metric
2024-02-02 22:23:03,029 cfg.training.batch_type            : sentence
2024-02-02 22:23:03,029 cfg.training.translation_normalization : batch
2024-02-02 22:23:03,029 cfg.training.eval_recognition_beam_size : 1
2024-02-02 22:23:03,029 cfg.training.eval_translation_beam_size : 1
2024-02-02 22:23:03,029 cfg.training.eval_translation_beam_alpha : -1
2024-02-02 22:23:03,029 cfg.training.overwrite             : True
2024-02-02 22:23:03,030 cfg.training.shuffle               : True
2024-02-02 22:23:03,030 cfg.training.use_cuda              : True
2024-02-02 22:23:03,030 cfg.training.translation_max_output_length : 40
2024-02-02 22:23:03,030 cfg.training.keep_last_ckpts       : 1
2024-02-02 22:23:03,030 cfg.training.batch_multiplier      : 1
2024-02-02 22:23:03,030 cfg.training.logging_freq          : 100
2024-02-02 22:23:03,030 cfg.training.validation_freq       : 2000
2024-02-02 22:23:03,030 cfg.training.betas                 : [0.9, 0.998]
2024-02-02 22:23:03,031 cfg.training.scheduling            : plateau
2024-02-02 22:23:03,031 cfg.training.learning_rate_min     : 1e-08
2024-02-02 22:23:03,031 cfg.training.weight_decay          : 0.0001
2024-02-02 22:23:03,031 cfg.training.patience              : 12
2024-02-02 22:23:03,031 cfg.training.decrease_factor       : 0.5
2024-02-02 22:23:03,031 cfg.training.label_smoothing       : 0.0
2024-02-02 22:23:03,031 cfg.model.initializer              : xavier
2024-02-02 22:23:03,031 cfg.model.bias_initializer         : zeros
2024-02-02 22:23:03,031 cfg.model.init_gain                : 1.0
2024-02-02 22:23:03,032 cfg.model.embed_initializer        : xavier
2024-02-02 22:23:03,032 cfg.model.embed_init_gain          : 1.0
2024-02-02 22:23:03,032 cfg.model.tied_softmax             : True
2024-02-02 22:23:03,032 cfg.model.encoder.type             : transformer
2024-02-02 22:23:03,032 cfg.model.encoder.num_layers       : 3
2024-02-02 22:23:03,032 cfg.model.encoder.num_heads        : 32
2024-02-02 22:23:03,032 cfg.model.encoder.embeddings.embedding_dim : 512
2024-02-02 22:23:03,032 cfg.model.encoder.embeddings.scale : False
2024-02-02 22:23:03,032 cfg.model.encoder.embeddings.dropout : 0.1
2024-02-02 22:23:03,033 cfg.model.encoder.embeddings.norm_type : batch
2024-02-02 22:23:03,033 cfg.model.encoder.embeddings.activation_type : softsign
2024-02-02 22:23:03,033 cfg.model.encoder.hidden_size      : 512
2024-02-02 22:23:03,033 cfg.model.encoder.ff_size          : 2048
2024-02-02 22:23:03,033 cfg.model.encoder.dropout          : 0.1
2024-02-02 22:23:03,033 cfg.model.decoder.type             : transformer
2024-02-02 22:23:03,033 cfg.model.decoder.num_layers       : 3
2024-02-02 22:23:03,033 cfg.model.decoder.num_heads        : 32
2024-02-02 22:23:03,033 cfg.model.decoder.embeddings.embedding_dim : 512
2024-02-02 22:23:03,034 cfg.model.decoder.embeddings.scale : False
2024-02-02 22:23:03,034 cfg.model.decoder.embeddings.dropout : 0.1
2024-02-02 22:23:03,034 cfg.model.decoder.embeddings.norm_type : batch
2024-02-02 22:23:03,034 cfg.model.decoder.embeddings.activation_type : softsign
2024-02-02 22:23:03,034 cfg.model.decoder.hidden_size      : 512
2024-02-02 22:23:03,034 cfg.model.decoder.ff_size          : 2048
2024-02-02 22:23:03,034 cfg.model.decoder.dropout          : 0.1
2024-02-02 22:23:03,034 Data set sizes: 
	train 2126,
	valid 708,
	test 706
2024-02-02 22:23:03,035 First training example:
	[GLS] A B C D E
	[TXT] although new zealand was disappointed to faltered at the finals against australia they did well throughout the tournament
2024-02-02 22:23:03,035 First 10 words (gls): (0) <si> (1) <unk> (2) <pad> (3) A (4) B (5) C (6) D (7) E
2024-02-02 22:23:03,035 First 10 words (txt): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) and (6) to (7) a (8) in (9) of
2024-02-02 22:23:03,035 Number of unique glosses (types): 8
2024-02-02 22:23:03,035 Number of unique words (types): 4397
2024-02-02 22:23:03,035 SignModel(
	encoder=TransformerEncoder(num_layers=3, num_heads=32),
	decoder=TransformerDecoder(num_layers=3, num_heads=32),
	sgn_embed=SpatialEmbeddings(embedding_dim=512, input_size=2560),
	txt_embed=Embeddings(embedding_dim=512, vocab_size=4397))
2024-02-02 22:23:03,039 EPOCH 1
2024-02-02 22:23:08,794 [Epoch: 001 Step: 00000100] Batch Recognition Loss:   0.192999 => Gls Tokens per Sec:     1390 || Batch Translation Loss:  82.114212 => Txt Tokens per Sec:     3859 || Lr: 0.000100
2024-02-02 22:23:10,733 Epoch   1: Total Training Recognition Loss 235.40  Total Training Translation Loss 12520.80 
2024-02-02 22:23:10,734 EPOCH 2
2024-02-02 22:23:14,283 [Epoch: 002 Step: 00000200] Batch Recognition Loss:   0.019866 => Gls Tokens per Sec:     1511 || Batch Translation Loss:  87.004219 => Txt Tokens per Sec:     4192 || Lr: 0.000100
2024-02-02 22:23:18,154 Epoch   2: Total Training Recognition Loss 3.24  Total Training Translation Loss 11647.28 
2024-02-02 22:23:18,155 EPOCH 3
2024-02-02 22:23:20,185 [Epoch: 003 Step: 00000300] Batch Recognition Loss:   0.008750 => Gls Tokens per Sec:     1340 || Batch Translation Loss:  64.699432 => Txt Tokens per Sec:     3672 || Lr: 0.000100
2024-02-02 22:23:26,020 Epoch   3: Total Training Recognition Loss 1.27  Total Training Translation Loss 11144.21 
2024-02-02 22:23:26,021 EPOCH 4
2024-02-02 22:23:26,083 [Epoch: 004 Step: 00000400] Batch Recognition Loss:   0.007556 => Gls Tokens per Sec:     1311 || Batch Translation Loss: 101.413605 => Txt Tokens per Sec:     4475 || Lr: 0.000100
2024-02-02 22:23:31,621 [Epoch: 004 Step: 00000500] Batch Recognition Loss:   0.004226 => Gls Tokens per Sec:     1445 || Batch Translation Loss:  60.471687 => Txt Tokens per Sec:     4016 || Lr: 0.000100
2024-02-02 22:23:33,372 Epoch   4: Total Training Recognition Loss 1.00  Total Training Translation Loss 10578.59 
2024-02-02 22:23:33,372 EPOCH 5
2024-02-02 22:23:37,077 [Epoch: 005 Step: 00000600] Batch Recognition Loss:   0.002064 => Gls Tokens per Sec:     1469 || Batch Translation Loss:  74.643112 => Txt Tokens per Sec:     4134 || Lr: 0.000100
2024-02-02 22:23:40,775 Epoch   5: Total Training Recognition Loss 0.86  Total Training Translation Loss 10048.67 
2024-02-02 22:23:40,776 EPOCH 6
2024-02-02 22:23:42,843 [Epoch: 006 Step: 00000700] Batch Recognition Loss:   0.004732 => Gls Tokens per Sec:     1355 || Batch Translation Loss:  60.886265 => Txt Tokens per Sec:     3708 || Lr: 0.000100
2024-02-02 22:23:48,438 Epoch   6: Total Training Recognition Loss 0.91  Total Training Translation Loss 9565.82 
2024-02-02 22:23:48,438 EPOCH 7
2024-02-02 22:23:48,567 [Epoch: 007 Step: 00000800] Batch Recognition Loss:   0.006415 => Gls Tokens per Sec:     1250 || Batch Translation Loss:  42.378590 => Txt Tokens per Sec:     3375 || Lr: 0.000100
2024-02-02 22:23:54,182 [Epoch: 007 Step: 00000900] Batch Recognition Loss:   0.005632 => Gls Tokens per Sec:     1425 || Batch Translation Loss:  69.968918 => Txt Tokens per Sec:     3970 || Lr: 0.000100
2024-02-02 22:23:55,883 Epoch   7: Total Training Recognition Loss 0.76  Total Training Translation Loss 9090.69 
2024-02-02 22:23:55,883 EPOCH 8
2024-02-02 22:23:59,196 [Epoch: 008 Step: 00001000] Batch Recognition Loss:   0.008466 => Gls Tokens per Sec:     1667 || Batch Translation Loss:  51.759972 => Txt Tokens per Sec:     4454 || Lr: 0.000100
2024-02-02 22:24:02,832 Epoch   8: Total Training Recognition Loss 0.83  Total Training Translation Loss 8644.00 
2024-02-02 22:24:02,832 EPOCH 9
2024-02-02 22:24:04,684 [Epoch: 009 Step: 00001100] Batch Recognition Loss:   0.007297 => Gls Tokens per Sec:     1557 || Batch Translation Loss:  59.367970 => Txt Tokens per Sec:     4458 || Lr: 0.000100
2024-02-02 22:24:10,336 Epoch   9: Total Training Recognition Loss 1.00  Total Training Translation Loss 8211.51 
2024-02-02 22:24:10,337 EPOCH 10
2024-02-02 22:24:10,511 [Epoch: 010 Step: 00001200] Batch Recognition Loss:   0.007425 => Gls Tokens per Sec:     1387 || Batch Translation Loss:  35.645149 => Txt Tokens per Sec:     3908 || Lr: 0.000100
2024-02-02 22:24:15,917 [Epoch: 010 Step: 00001300] Batch Recognition Loss:   0.004361 => Gls Tokens per Sec:     1480 || Batch Translation Loss:  61.554497 => Txt Tokens per Sec:     4094 || Lr: 0.000100
2024-02-02 22:24:17,648 Epoch  10: Total Training Recognition Loss 1.08  Total Training Translation Loss 7777.60 
2024-02-02 22:24:17,648 EPOCH 11
2024-02-02 22:24:21,538 [Epoch: 011 Step: 00001400] Batch Recognition Loss:   0.013077 => Gls Tokens per Sec:     1440 || Batch Translation Loss:  54.998573 => Txt Tokens per Sec:     3928 || Lr: 0.000100
2024-02-02 22:24:25,319 Epoch  11: Total Training Recognition Loss 1.15  Total Training Translation Loss 7357.88 
2024-02-02 22:24:25,319 EPOCH 12
2024-02-02 22:24:27,318 [Epoch: 012 Step: 00001500] Batch Recognition Loss:   0.011193 => Gls Tokens per Sec:     1482 || Batch Translation Loss:  47.149269 => Txt Tokens per Sec:     4123 || Lr: 0.000100
2024-02-02 22:24:32,888 Epoch  12: Total Training Recognition Loss 1.26  Total Training Translation Loss 6945.04 
2024-02-02 22:24:32,888 EPOCH 13
2024-02-02 22:24:33,093 [Epoch: 013 Step: 00001600] Batch Recognition Loss:   0.009076 => Gls Tokens per Sec:     1569 || Batch Translation Loss:  53.201324 => Txt Tokens per Sec:     4441 || Lr: 0.000100
2024-02-02 22:24:38,559 [Epoch: 013 Step: 00001700] Batch Recognition Loss:   0.040085 => Gls Tokens per Sec:     1464 || Batch Translation Loss:  48.112804 => Txt Tokens per Sec:     4098 || Lr: 0.000100
2024-02-02 22:24:40,174 Epoch  13: Total Training Recognition Loss 1.44  Total Training Translation Loss 6546.47 
2024-02-02 22:24:40,175 EPOCH 14
2024-02-02 22:24:43,927 [Epoch: 014 Step: 00001800] Batch Recognition Loss:   0.008311 => Gls Tokens per Sec:     1515 || Batch Translation Loss:  33.844940 => Txt Tokens per Sec:     4202 || Lr: 0.000100
2024-02-02 22:24:47,549 Epoch  14: Total Training Recognition Loss 1.53  Total Training Translation Loss 6153.62 
2024-02-02 22:24:47,549 EPOCH 15
2024-02-02 22:24:49,502 [Epoch: 015 Step: 00001900] Batch Recognition Loss:   0.006562 => Gls Tokens per Sec:     1558 || Batch Translation Loss:  38.527763 => Txt Tokens per Sec:     4395 || Lr: 0.000100
2024-02-02 22:24:55,017 Epoch  15: Total Training Recognition Loss 1.67  Total Training Translation Loss 5765.17 
2024-02-02 22:24:55,017 EPOCH 16
2024-02-02 22:24:55,274 [Epoch: 016 Step: 00002000] Batch Recognition Loss:   0.026922 => Gls Tokens per Sec:     1563 || Batch Translation Loss:  27.959824 => Txt Tokens per Sec:     4363 || Lr: 0.000100
2024-02-02 22:25:06,899 Hooray! New best validation result [eval_metric]!
2024-02-02 22:25:06,900 Saving new checkpoint.
2024-02-02 22:25:07,150 Validation result at epoch  16, step     2000: duration: 11.8765s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.22679	Translation Loss: 58417.03906	PPL: 345.77774
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.88	(BLEU-1: 13.69,	BLEU-2: 4.63,	BLEU-3: 1.92,	BLEU-4: 0.88)
	CHRF 16.20	ROUGE 11.61
2024-02-02 22:25:07,151 Logging Recognition and Translation Outputs
2024-02-02 22:25:07,151 ========================================================================================================================
2024-02-02 22:25:07,151 Logging Sequence: 182_115.00
2024-02-02 22:25:07,152 	Gloss Reference :	A B+C+D+E
2024-02-02 22:25:07,152 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:25:07,152 	Gloss Alignment :	         
2024-02-02 22:25:07,152 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:25:07,153 	Text Reference  :	fans are unclear whether yuvraj will be returning to play  test match odi or     in     t20     leagues from  february 2022 
2024-02-02 22:25:07,153 	Text Hypothesis :	**** *** ******* ******* ****** **** ** ********* ** after the  match he  played played against the     match between  teams
2024-02-02 22:25:07,154 	Text Alignment  :	D    D   D       D       D      D    D  D         D  S     S          S   S      S      S       S       S     S        S    
2024-02-02 22:25:07,154 ========================================================================================================================
2024-02-02 22:25:07,154 Logging Sequence: 140_120.00
2024-02-02 22:25:07,154 	Gloss Reference :	A B+C+D+E
2024-02-02 22:25:07,154 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:25:07,154 	Gloss Alignment :	         
2024-02-02 22:25:07,154 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:25:07,155 	Text Reference  :	but why so it is because pant is a talented player and it will help encouraging the youth of uttarakhand toward sports
2024-02-02 22:25:07,156 	Text Hypothesis :	*** *** ** ** ** ******* he   is a ******** ****** *** ** **** **** *********** *** lot   of *********** camel  flu   
2024-02-02 22:25:07,156 	Text Alignment  :	D   D   D  D  D  D       S         D        D      D   D  D    D    D           D   S        D           S      S     
2024-02-02 22:25:07,156 ========================================================================================================================
2024-02-02 22:25:07,156 Logging Sequence: 85_36.00
2024-02-02 22:25:07,156 	Gloss Reference :	A B+C+D+E
2024-02-02 22:25:07,156 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:25:07,156 	Gloss Alignment :	         
2024-02-02 22:25:07,156 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:25:07,158 	Text Reference  :	**** symonds has scored 2    centuries in 26  tests that    he    played for his country
2024-02-02 22:25:07,158 	Text Hypothesis :	when he      was a      gold medal     in the match between india had    won the match  
2024-02-02 22:25:07,158 	Text Alignment  :	I    S       S   S      S    S            S   S     S       S     S      S   S   S      
2024-02-02 22:25:07,158 ========================================================================================================================
2024-02-02 22:25:07,158 Logging Sequence: 164_100.00
2024-02-02 22:25:07,158 	Gloss Reference :	A B+C+D+E
2024-02-02 22:25:07,159 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:25:07,159 	Gloss Alignment :	         
2024-02-02 22:25:07,159 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:25:07,161 	Text Reference  :	the tv rights for broadcasting ipl  matches in india  for the next 5 years went   to star india  for rs ****** *** 23575 crore 
2024-02-02 22:25:07,161 	Text Hypothesis :	*** ** you    are aware        that rights  of rights for the **** * ***** rights of the  rights for rs rights for the   rights
2024-02-02 22:25:07,161 	Text Alignment  :	D   D  S      S   S            S    S       S  S              D    D D     S      S  S    S             I      I   S     S     
2024-02-02 22:25:07,162 ========================================================================================================================
2024-02-02 22:25:07,162 Logging Sequence: 76_79.00
2024-02-02 22:25:07,162 	Gloss Reference :	A B+C+D+E
2024-02-02 22:25:07,162 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:25:07,162 	Gloss Alignment :	         
2024-02-02 22:25:07,162 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:25:07,163 	Text Reference  :	speaking to  ani csk  ceo kasi viswanathan said  
2024-02-02 22:25:07,163 	Text Hypothesis :	******** the ipl will be  held in          mumbai
2024-02-02 22:25:07,163 	Text Alignment  :	D        S   S   S    S   S    S           S     
2024-02-02 22:25:07,163 ========================================================================================================================
2024-02-02 22:25:13,035 [Epoch: 016 Step: 00002100] Batch Recognition Loss:   0.018101 => Gls Tokens per Sec:     1359 || Batch Translation Loss:  39.840290 => Txt Tokens per Sec:     3788 || Lr: 0.000100
2024-02-02 22:25:14,539 Epoch  16: Total Training Recognition Loss 1.74  Total Training Translation Loss 5377.30 
2024-02-02 22:25:14,539 EPOCH 17
2024-02-02 22:25:18,202 [Epoch: 017 Step: 00002200] Batch Recognition Loss:   0.020662 => Gls Tokens per Sec:     1573 || Batch Translation Loss:  29.285595 => Txt Tokens per Sec:     4355 || Lr: 0.000100
2024-02-02 22:25:21,873 Epoch  17: Total Training Recognition Loss 1.75  Total Training Translation Loss 5005.56 
2024-02-02 22:25:21,873 EPOCH 18
2024-02-02 22:25:24,129 [Epoch: 018 Step: 00002300] Batch Recognition Loss:   0.016340 => Gls Tokens per Sec:     1384 || Batch Translation Loss:  25.410231 => Txt Tokens per Sec:     3865 || Lr: 0.000100
2024-02-02 22:25:29,577 Epoch  18: Total Training Recognition Loss 1.63  Total Training Translation Loss 4686.61 
2024-02-02 22:25:29,577 EPOCH 19
2024-02-02 22:25:29,872 [Epoch: 019 Step: 00002400] Batch Recognition Loss:   0.019268 => Gls Tokens per Sec:     1633 || Batch Translation Loss:  41.089462 => Txt Tokens per Sec:     4469 || Lr: 0.000100
2024-02-02 22:25:35,046 [Epoch: 019 Step: 00002500] Batch Recognition Loss:   0.010679 => Gls Tokens per Sec:     1546 || Batch Translation Loss:  37.300442 => Txt Tokens per Sec:     4315 || Lr: 0.000100
2024-02-02 22:25:36,512 Epoch  19: Total Training Recognition Loss 1.81  Total Training Translation Loss 4287.25 
2024-02-02 22:25:36,513 EPOCH 20
2024-02-02 22:25:40,626 [Epoch: 020 Step: 00002600] Batch Recognition Loss:   0.019993 => Gls Tokens per Sec:     1420 || Batch Translation Loss:  38.952587 => Txt Tokens per Sec:     3895 || Lr: 0.000100
2024-02-02 22:25:43,901 Epoch  20: Total Training Recognition Loss 1.65  Total Training Translation Loss 3911.10 
2024-02-02 22:25:43,901 EPOCH 21
2024-02-02 22:25:46,438 [Epoch: 021 Step: 00002700] Batch Recognition Loss:   0.013433 => Gls Tokens per Sec:     1261 || Batch Translation Loss:  23.667072 => Txt Tokens per Sec:     3608 || Lr: 0.000100
2024-02-02 22:25:51,828 Epoch  21: Total Training Recognition Loss 1.54  Total Training Translation Loss 3611.11 
2024-02-02 22:25:51,828 EPOCH 22
2024-02-02 22:25:52,206 [Epoch: 022 Step: 00002800] Batch Recognition Loss:   0.009266 => Gls Tokens per Sec:     1485 || Batch Translation Loss:  18.254725 => Txt Tokens per Sec:     4188 || Lr: 0.000100
2024-02-02 22:25:57,383 [Epoch: 022 Step: 00002900] Batch Recognition Loss:   0.011683 => Gls Tokens per Sec:     1544 || Batch Translation Loss:  16.110605 => Txt Tokens per Sec:     4301 || Lr: 0.000100
2024-02-02 22:25:58,703 Epoch  22: Total Training Recognition Loss 1.73  Total Training Translation Loss 3283.64 
2024-02-02 22:25:58,704 EPOCH 23
2024-02-02 22:26:03,052 [Epoch: 023 Step: 00003000] Batch Recognition Loss:   0.010863 => Gls Tokens per Sec:     1362 || Batch Translation Loss:   7.411166 => Txt Tokens per Sec:     3789 || Lr: 0.000100
2024-02-02 22:26:06,648 Epoch  23: Total Training Recognition Loss 1.48  Total Training Translation Loss 2992.20 
2024-02-02 22:26:06,649 EPOCH 24
2024-02-02 22:26:08,976 [Epoch: 024 Step: 00003100] Batch Recognition Loss:   0.010661 => Gls Tokens per Sec:     1410 || Batch Translation Loss:  14.486421 => Txt Tokens per Sec:     3939 || Lr: 0.000100
2024-02-02 22:26:14,346 Epoch  24: Total Training Recognition Loss 1.46  Total Training Translation Loss 2676.85 
2024-02-02 22:26:14,347 EPOCH 25
2024-02-02 22:26:14,791 [Epoch: 025 Step: 00003200] Batch Recognition Loss:   0.007599 => Gls Tokens per Sec:     1445 || Batch Translation Loss:  15.542123 => Txt Tokens per Sec:     4226 || Lr: 0.000100
2024-02-02 22:26:20,435 [Epoch: 025 Step: 00003300] Batch Recognition Loss:   0.004999 => Gls Tokens per Sec:     1416 || Batch Translation Loss:  14.972177 => Txt Tokens per Sec:     3916 || Lr: 0.000100
2024-02-02 22:26:21,664 Epoch  25: Total Training Recognition Loss 1.33  Total Training Translation Loss 2437.08 
2024-02-02 22:26:21,664 EPOCH 26
2024-02-02 22:26:25,241 [Epoch: 026 Step: 00003400] Batch Recognition Loss:   0.010525 => Gls Tokens per Sec:     1678 || Batch Translation Loss:   5.880079 => Txt Tokens per Sec:     4604 || Lr: 0.000100
2024-02-02 22:26:28,093 Epoch  26: Total Training Recognition Loss 1.26  Total Training Translation Loss 2153.92 
2024-02-02 22:26:28,093 EPOCH 27
2024-02-02 22:26:30,276 [Epoch: 027 Step: 00003500] Batch Recognition Loss:   0.012698 => Gls Tokens per Sec:     1540 || Batch Translation Loss:  11.774353 => Txt Tokens per Sec:     4030 || Lr: 0.000100
2024-02-02 22:26:35,422 Epoch  27: Total Training Recognition Loss 1.16  Total Training Translation Loss 1917.75 
2024-02-02 22:26:35,422 EPOCH 28
2024-02-02 22:26:35,966 [Epoch: 028 Step: 00003600] Batch Recognition Loss:   0.011523 => Gls Tokens per Sec:     1328 || Batch Translation Loss:  14.076869 => Txt Tokens per Sec:     3819 || Lr: 0.000100
2024-02-02 22:26:41,601 [Epoch: 028 Step: 00003700] Batch Recognition Loss:   0.004974 => Gls Tokens per Sec:     1420 || Batch Translation Loss:  10.619044 => Txt Tokens per Sec:     3954 || Lr: 0.000100
2024-02-02 22:26:43,024 Epoch  28: Total Training Recognition Loss 1.05  Total Training Translation Loss 1685.23 
2024-02-02 22:26:43,024 EPOCH 29
2024-02-02 22:26:47,661 [Epoch: 029 Step: 00003800] Batch Recognition Loss:   0.007744 => Gls Tokens per Sec:     1311 || Batch Translation Loss:  15.045277 => Txt Tokens per Sec:     3669 || Lr: 0.000100
2024-02-02 22:26:50,750 Epoch  29: Total Training Recognition Loss 1.04  Total Training Translation Loss 1484.36 
2024-02-02 22:26:50,751 EPOCH 30
2024-02-02 22:26:53,369 [Epoch: 030 Step: 00003900] Batch Recognition Loss:   0.003448 => Gls Tokens per Sec:     1314 || Batch Translation Loss:  11.687709 => Txt Tokens per Sec:     3843 || Lr: 0.000100
2024-02-02 22:26:58,530 Epoch  30: Total Training Recognition Loss 0.92  Total Training Translation Loss 1296.51 
2024-02-02 22:26:58,530 EPOCH 31
2024-02-02 22:26:58,975 [Epoch: 031 Step: 00004000] Batch Recognition Loss:   0.007346 => Gls Tokens per Sec:     1802 || Batch Translation Loss:   3.546916 => Txt Tokens per Sec:     4642 || Lr: 0.000100
2024-02-02 22:27:09,664 Validation result at epoch  31, step     4000: duration: 10.6877s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.07010	Translation Loss: 67255.35938	PPL: 837.35406
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.59	(BLEU-1: 11.13,	BLEU-2: 3.51,	BLEU-3: 1.28,	BLEU-4: 0.59)
	CHRF 15.92	ROUGE 9.97
2024-02-02 22:27:09,665 Logging Recognition and Translation Outputs
2024-02-02 22:27:09,665 ========================================================================================================================
2024-02-02 22:27:09,665 Logging Sequence: 133_173.00
2024-02-02 22:27:09,666 	Gloss Reference :	A B+C+D+E
2024-02-02 22:27:09,666 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:27:09,666 	Gloss Alignment :	         
2024-02-02 22:27:09,666 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:27:09,668 	Text Reference  :	according to sources the leaders of the two countries are   set to   join  the  commentary panel as  well     
2024-02-02 22:27:09,668 	Text Hypothesis :	********* pm modi    is  one     of the *** most      loved and they would have a          huge  fan following
2024-02-02 22:27:09,668 	Text Alignment  :	D         S  S       S   S              D   S         S     S   S    S     S    S          S     S   S        
2024-02-02 22:27:09,668 ========================================================================================================================
2024-02-02 22:27:09,668 Logging Sequence: 83_33.00
2024-02-02 22:27:09,669 	Gloss Reference :	A B+C+D+E
2024-02-02 22:27:09,669 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:27:09,669 	Gloss Alignment :	         
2024-02-02 22:27:09,669 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:27:09,670 	Text Reference  :	*** a       football match lasts       for two equal   halves of  45  minutes
2024-02-02 22:27:09,670 	Text Hypothesis :	the denmark team     was   responsible for the denmark team   for any matches
2024-02-02 22:27:09,670 	Text Alignment  :	I   S       S        S     S               S   S       S      S   S   S      
2024-02-02 22:27:09,670 ========================================================================================================================
2024-02-02 22:27:09,670 Logging Sequence: 68_147.00
2024-02-02 22:27:09,670 	Gloss Reference :	A B+C+D+E
2024-02-02 22:27:09,671 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:27:09,671 	Gloss Alignment :	         
2024-02-02 22:27:09,671 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:27:09,672 	Text Reference  :	***** ****** *** **** remember the 2007  t20 world cup amid  a lot of sledging by english players
2024-02-02 22:27:09,672 	Text Hypothesis :	while bumrah was seen smashing the match as  he    was given a lot of ******** ** 7       runs   
2024-02-02 22:27:09,673 	Text Alignment  :	I     I      I   I    S            S     S   S     S   S              D        D  S       S      
2024-02-02 22:27:09,673 ========================================================================================================================
2024-02-02 22:27:09,673 Logging Sequence: 165_8.00
2024-02-02 22:27:09,673 	Gloss Reference :	A B+C+D+E
2024-02-02 22:27:09,673 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:27:09,673 	Gloss Alignment :	         
2024-02-02 22:27:09,673 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:27:09,674 	Text Reference  :	however many don't believe in   it      it      varies among people 
2024-02-02 22:27:09,674 	Text Hypothesis :	******* **** but   they    have already already booked their victory
2024-02-02 22:27:09,674 	Text Alignment  :	D       D    S     S       S    S       S       S      S     S      
2024-02-02 22:27:09,674 ========================================================================================================================
2024-02-02 22:27:09,674 Logging Sequence: 119_71.00
2024-02-02 22:27:09,675 	Gloss Reference :	A B+C+D+E
2024-02-02 22:27:09,675 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:27:09,675 	Gloss Alignment :	         
2024-02-02 22:27:09,675 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:27:09,676 	Text Reference  :	the special gold devices have each player' names and jersey numbers next to the     camera
2024-02-02 22:27:09,676 	Text Hypothesis :	*** and     told the     gift that he      is    one of     the     gift to idesign gold  
2024-02-02 22:27:09,676 	Text Alignment  :	D   S       S    S       S    S    S       S     S   S      S       S       S       S     
2024-02-02 22:27:09,677 ========================================================================================================================
2024-02-02 22:27:15,551 [Epoch: 031 Step: 00004100] Batch Recognition Loss:   0.004809 => Gls Tokens per Sec:     1359 || Batch Translation Loss:  11.137196 => Txt Tokens per Sec:     3783 || Lr: 0.000100
2024-02-02 22:27:16,863 Epoch  31: Total Training Recognition Loss 0.88  Total Training Translation Loss 1127.94 
2024-02-02 22:27:16,863 EPOCH 32
2024-02-02 22:27:20,981 [Epoch: 032 Step: 00004200] Batch Recognition Loss:   0.005993 => Gls Tokens per Sec:     1496 || Batch Translation Loss:   9.119851 => Txt Tokens per Sec:     4097 || Lr: 0.000100
2024-02-02 22:27:24,202 Epoch  32: Total Training Recognition Loss 0.80  Total Training Translation Loss 994.39 
2024-02-02 22:27:24,202 EPOCH 33
2024-02-02 22:27:26,669 [Epoch: 033 Step: 00004300] Batch Recognition Loss:   0.004231 => Gls Tokens per Sec:     1427 || Batch Translation Loss:   9.392918 => Txt Tokens per Sec:     4147 || Lr: 0.000100
2024-02-02 22:27:31,706 Epoch  33: Total Training Recognition Loss 0.81  Total Training Translation Loss 884.07 
2024-02-02 22:27:31,707 EPOCH 34
2024-02-02 22:27:32,320 [Epoch: 034 Step: 00004400] Batch Recognition Loss:   0.002764 => Gls Tokens per Sec:     1438 || Batch Translation Loss:   7.719358 => Txt Tokens per Sec:     3891 || Lr: 0.000100
2024-02-02 22:27:38,002 [Epoch: 034 Step: 00004500] Batch Recognition Loss:   0.004836 => Gls Tokens per Sec:     1408 || Batch Translation Loss:   6.004529 => Txt Tokens per Sec:     3870 || Lr: 0.000100
2024-02-02 22:27:39,280 Epoch  34: Total Training Recognition Loss 0.70  Total Training Translation Loss 781.66 
2024-02-02 22:27:39,280 EPOCH 35
2024-02-02 22:27:43,397 [Epoch: 035 Step: 00004600] Batch Recognition Loss:   0.003746 => Gls Tokens per Sec:     1517 || Batch Translation Loss:   6.245111 => Txt Tokens per Sec:     4234 || Lr: 0.000100
2024-02-02 22:27:46,784 Epoch  35: Total Training Recognition Loss 0.63  Total Training Translation Loss 691.55 
2024-02-02 22:27:46,785 EPOCH 36
2024-02-02 22:27:49,339 [Epoch: 036 Step: 00004700] Batch Recognition Loss:   0.003931 => Gls Tokens per Sec:     1410 || Batch Translation Loss:   4.086421 => Txt Tokens per Sec:     3816 || Lr: 0.000100
2024-02-02 22:27:54,396 Epoch  36: Total Training Recognition Loss 0.56  Total Training Translation Loss 579.98 
2024-02-02 22:27:54,396 EPOCH 37
2024-02-02 22:27:55,035 [Epoch: 037 Step: 00004800] Batch Recognition Loss:   0.002551 => Gls Tokens per Sec:     1505 || Batch Translation Loss:   3.668629 => Txt Tokens per Sec:     4361 || Lr: 0.000100
2024-02-02 22:28:00,993 [Epoch: 037 Step: 00004900] Batch Recognition Loss:   0.003842 => Gls Tokens per Sec:     1341 || Batch Translation Loss:   4.420977 => Txt Tokens per Sec:     3722 || Lr: 0.000100
2024-02-02 22:28:02,112 Epoch  37: Total Training Recognition Loss 0.54  Total Training Translation Loss 519.40 
2024-02-02 22:28:02,112 EPOCH 38
2024-02-02 22:28:06,274 [Epoch: 038 Step: 00005000] Batch Recognition Loss:   0.002538 => Gls Tokens per Sec:     1519 || Batch Translation Loss:   4.607191 => Txt Tokens per Sec:     4257 || Lr: 0.000100
2024-02-02 22:28:09,443 Epoch  38: Total Training Recognition Loss 0.46  Total Training Translation Loss 476.87 
2024-02-02 22:28:09,443 EPOCH 39
2024-02-02 22:28:12,036 [Epoch: 039 Step: 00005100] Batch Recognition Loss:   0.001703 => Gls Tokens per Sec:     1420 || Batch Translation Loss:   2.546542 => Txt Tokens per Sec:     3903 || Lr: 0.000100
2024-02-02 22:28:17,246 Epoch  39: Total Training Recognition Loss 0.47  Total Training Translation Loss 435.44 
2024-02-02 22:28:17,247 EPOCH 40
2024-02-02 22:28:18,098 [Epoch: 040 Step: 00005200] Batch Recognition Loss:   0.001699 => Gls Tokens per Sec:     1222 || Batch Translation Loss:   2.539528 => Txt Tokens per Sec:     3548 || Lr: 0.000100
2024-02-02 22:28:23,393 [Epoch: 040 Step: 00005300] Batch Recognition Loss:   0.004389 => Gls Tokens per Sec:     1511 || Batch Translation Loss:   3.434193 => Txt Tokens per Sec:     4119 || Lr: 0.000100
2024-02-02 22:28:24,666 Epoch  40: Total Training Recognition Loss 0.38  Total Training Translation Loss 395.81 
2024-02-02 22:28:24,666 EPOCH 41
2024-02-02 22:28:29,088 [Epoch: 041 Step: 00005400] Batch Recognition Loss:   0.002400 => Gls Tokens per Sec:     1448 || Batch Translation Loss:   3.748390 => Txt Tokens per Sec:     3962 || Lr: 0.000100
2024-02-02 22:28:32,234 Epoch  41: Total Training Recognition Loss 0.41  Total Training Translation Loss 351.52 
2024-02-02 22:28:32,234 EPOCH 42
2024-02-02 22:28:34,734 [Epoch: 042 Step: 00005500] Batch Recognition Loss:   0.001715 => Gls Tokens per Sec:     1505 || Batch Translation Loss:   2.145131 => Txt Tokens per Sec:     4072 || Lr: 0.000100
2024-02-02 22:28:39,856 Epoch  42: Total Training Recognition Loss 0.37  Total Training Translation Loss 317.71 
2024-02-02 22:28:39,857 EPOCH 43
2024-02-02 22:28:40,673 [Epoch: 043 Step: 00005600] Batch Recognition Loss:   0.002834 => Gls Tokens per Sec:     1374 || Batch Translation Loss:   2.537222 => Txt Tokens per Sec:     3957 || Lr: 0.000100
2024-02-02 22:28:46,442 [Epoch: 043 Step: 00005700] Batch Recognition Loss:   0.003000 => Gls Tokens per Sec:     1386 || Batch Translation Loss:   2.059395 => Txt Tokens per Sec:     3810 || Lr: 0.000100
2024-02-02 22:28:47,446 Epoch  43: Total Training Recognition Loss 0.33  Total Training Translation Loss 293.32 
2024-02-02 22:28:47,446 EPOCH 44
2024-02-02 22:28:51,548 [Epoch: 044 Step: 00005800] Batch Recognition Loss:   0.002261 => Gls Tokens per Sec:     1580 || Batch Translation Loss:   1.731687 => Txt Tokens per Sec:     4297 || Lr: 0.000100
2024-02-02 22:28:54,705 Epoch  44: Total Training Recognition Loss 0.33  Total Training Translation Loss 298.07 
2024-02-02 22:28:54,706 EPOCH 45
2024-02-02 22:28:57,393 [Epoch: 045 Step: 00005900] Batch Recognition Loss:   0.003215 => Gls Tokens per Sec:     1429 || Batch Translation Loss:   1.265860 => Txt Tokens per Sec:     3958 || Lr: 0.000100
2024-02-02 22:29:02,288 Epoch  45: Total Training Recognition Loss 0.34  Total Training Translation Loss 269.31 
2024-02-02 22:29:02,288 EPOCH 46
2024-02-02 22:29:03,172 [Epoch: 046 Step: 00006000] Batch Recognition Loss:   0.002531 => Gls Tokens per Sec:     1359 || Batch Translation Loss:   1.896258 => Txt Tokens per Sec:     3847 || Lr: 0.000100
2024-02-02 22:29:13,508 Validation result at epoch  46, step     6000: duration: 10.3356s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.01719	Translation Loss: 75082.30469	PPL: 1832.59851
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.59	(BLEU-1: 11.05,	BLEU-2: 3.84,	BLEU-3: 1.42,	BLEU-4: 0.59)
	CHRF 16.31	ROUGE 9.69
2024-02-02 22:29:13,509 Logging Recognition and Translation Outputs
2024-02-02 22:29:13,509 ========================================================================================================================
2024-02-02 22:29:13,510 Logging Sequence: 89_111.00
2024-02-02 22:29:13,510 	Gloss Reference :	A B+C+D+E
2024-02-02 22:29:13,510 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:29:13,510 	Gloss Alignment :	         
2024-02-02 22:29:13,510 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:29:13,511 	Text Reference  :	** ** *** ***** ** ** however selectors never selected me for  the team  
2024-02-02 22:29:13,511 	Text Hypothesis :	it is not known if he had     any       match he       is also a   bowler
2024-02-02 22:29:13,511 	Text Alignment  :	I  I  I   I     I  I  S       S         S     S        S  S    S   S     
2024-02-02 22:29:13,511 ========================================================================================================================
2024-02-02 22:29:13,511 Logging Sequence: 137_23.00
2024-02-02 22:29:13,512 	Gloss Reference :	A B+C+D+E
2024-02-02 22:29:13,512 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:29:13,512 	Gloss Alignment :	         
2024-02-02 22:29:13,512 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:29:13,513 	Text Reference  :	fan from around the world are in qatar      for the fifa world cup 
2024-02-02 22:29:13,513 	Text Hypothesis :	*** **** ****** *** ***** i   am devastated by  the **** ***** fans
2024-02-02 22:29:13,513 	Text Alignment  :	D   D    D      D   D     S   S  S          S       D    D     S   
2024-02-02 22:29:13,513 ========================================================================================================================
2024-02-02 22:29:13,513 Logging Sequence: 128_145.00
2024-02-02 22:29:13,513 	Gloss Reference :	A B+C+D+E
2024-02-02 22:29:13,513 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:29:13,514 	Gloss Alignment :	         
2024-02-02 22:29:13,514 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:29:13,514 	Text Reference  :	**** ** icc also uploaded a    video    of the same
2024-02-02 22:29:13,514 	Text Hypothesis :	this is why the  bcci     will continue to be  nice
2024-02-02 22:29:13,515 	Text Alignment  :	I    I  S   S    S        S    S        S  S   S   
2024-02-02 22:29:13,515 ========================================================================================================================
2024-02-02 22:29:13,515 Logging Sequence: 165_192.00
2024-02-02 22:29:13,515 	Gloss Reference :	A B+C+D+E
2024-02-02 22:29:13,515 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:29:13,516 	Gloss Alignment :	         
2024-02-02 22:29:13,516 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:29:13,517 	Text Reference  :	***** ** **** 3       ravichandran ashwin  believes that   his       bag is ** ** ****** ******* ***** lucky
2024-02-02 22:29:13,517 	Text Hypothesis :	there is many batsmen and          bowlers for      sachin tendulkar who is so he always nothing about it   
2024-02-02 22:29:13,517 	Text Alignment  :	I     I  I    S       S            S       S        S      S         S      I  I  I      I       I     S    
2024-02-02 22:29:13,517 ========================================================================================================================
2024-02-02 22:29:13,517 Logging Sequence: 180_494.00
2024-02-02 22:29:13,517 	Gloss Reference :	A B+C+D+E
2024-02-02 22:29:13,518 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:29:13,518 	Gloss Alignment :	         
2024-02-02 22:29:13,518 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:29:13,519 	Text Reference  :	the women wrestlers spoke angrily against the  police and the controversy in   front of      the media
2024-02-02 22:29:13,519 	Text Hypothesis :	*** ***** ********* ***** ******* however they have   won the wrestlers   from seven matches as  well 
2024-02-02 22:29:13,519 	Text Alignment  :	D   D     D         D     D       S       S    S      S       S           S    S     S       S   S    
2024-02-02 22:29:13,519 ========================================================================================================================
2024-02-02 22:29:19,141 [Epoch: 046 Step: 00006100] Batch Recognition Loss:   0.002942 => Gls Tokens per Sec:     1419 || Batch Translation Loss:   1.765082 => Txt Tokens per Sec:     3974 || Lr: 0.000100
2024-02-02 22:29:20,058 Epoch  46: Total Training Recognition Loss 0.29  Total Training Translation Loss 241.73 
2024-02-02 22:29:20,059 EPOCH 47
2024-02-02 22:29:24,612 [Epoch: 047 Step: 00006200] Batch Recognition Loss:   0.002003 => Gls Tokens per Sec:     1441 || Batch Translation Loss:   1.800962 => Txt Tokens per Sec:     3972 || Lr: 0.000100
2024-02-02 22:29:27,427 Epoch  47: Total Training Recognition Loss 0.30  Total Training Translation Loss 225.30 
2024-02-02 22:29:27,427 EPOCH 48
2024-02-02 22:29:30,120 [Epoch: 048 Step: 00006300] Batch Recognition Loss:   0.001080 => Gls Tokens per Sec:     1456 || Batch Translation Loss:   2.109030 => Txt Tokens per Sec:     4089 || Lr: 0.000100
2024-02-02 22:29:34,830 Epoch  48: Total Training Recognition Loss 0.27  Total Training Translation Loss 210.32 
2024-02-02 22:29:34,830 EPOCH 49
2024-02-02 22:29:35,661 [Epoch: 049 Step: 00006400] Batch Recognition Loss:   0.004225 => Gls Tokens per Sec:     1543 || Batch Translation Loss:   1.087851 => Txt Tokens per Sec:     4380 || Lr: 0.000100
2024-02-02 22:29:41,405 [Epoch: 049 Step: 00006500] Batch Recognition Loss:   0.001603 => Gls Tokens per Sec:     1393 || Batch Translation Loss:   1.498742 => Txt Tokens per Sec:     3845 || Lr: 0.000100
2024-02-02 22:29:42,476 Epoch  49: Total Training Recognition Loss 0.24  Total Training Translation Loss 190.74 
2024-02-02 22:29:42,477 EPOCH 50
2024-02-02 22:29:46,774 [Epoch: 050 Step: 00006600] Batch Recognition Loss:   0.001917 => Gls Tokens per Sec:     1546 || Batch Translation Loss:   0.641006 => Txt Tokens per Sec:     4336 || Lr: 0.000100
2024-02-02 22:29:49,424 Epoch  50: Total Training Recognition Loss 0.23  Total Training Translation Loss 187.61 
2024-02-02 22:29:49,425 EPOCH 51
2024-02-02 22:29:52,391 [Epoch: 051 Step: 00006700] Batch Recognition Loss:   0.000815 => Gls Tokens per Sec:     1349 || Batch Translation Loss:   0.841932 => Txt Tokens per Sec:     3692 || Lr: 0.000100
2024-02-02 22:29:57,292 Epoch  51: Total Training Recognition Loss 0.20  Total Training Translation Loss 167.40 
2024-02-02 22:29:57,292 EPOCH 52
2024-02-02 22:29:58,380 [Epoch: 052 Step: 00006800] Batch Recognition Loss:   0.003507 => Gls Tokens per Sec:     1252 || Batch Translation Loss:   1.827690 => Txt Tokens per Sec:     3500 || Lr: 0.000100
2024-02-02 22:30:03,826 [Epoch: 052 Step: 00006900] Batch Recognition Loss:   0.001145 => Gls Tokens per Sec:     1469 || Batch Translation Loss:   1.762726 => Txt Tokens per Sec:     4048 || Lr: 0.000100
2024-02-02 22:30:04,865 Epoch  52: Total Training Recognition Loss 0.18  Total Training Translation Loss 172.13 
2024-02-02 22:30:04,865 EPOCH 53
2024-02-02 22:30:09,625 [Epoch: 053 Step: 00007000] Batch Recognition Loss:   0.006569 => Gls Tokens per Sec:     1412 || Batch Translation Loss:   1.303522 => Txt Tokens per Sec:     3997 || Lr: 0.000100
2024-02-02 22:30:12,199 Epoch  53: Total Training Recognition Loss 0.19  Total Training Translation Loss 160.79 
2024-02-02 22:30:12,199 EPOCH 54
2024-02-02 22:30:14,601 [Epoch: 054 Step: 00007100] Batch Recognition Loss:   0.001082 => Gls Tokens per Sec:     1700 || Batch Translation Loss:   2.399275 => Txt Tokens per Sec:     4669 || Lr: 0.000100
2024-02-02 22:30:19,233 Epoch  54: Total Training Recognition Loss 0.21  Total Training Translation Loss 156.16 
2024-02-02 22:30:19,233 EPOCH 55
2024-02-02 22:30:20,173 [Epoch: 055 Step: 00007200] Batch Recognition Loss:   0.000790 => Gls Tokens per Sec:     1535 || Batch Translation Loss:   1.639315 => Txt Tokens per Sec:     4017 || Lr: 0.000100
2024-02-02 22:30:25,958 [Epoch: 055 Step: 00007300] Batch Recognition Loss:   0.001797 => Gls Tokens per Sec:     1381 || Batch Translation Loss:   0.961747 => Txt Tokens per Sec:     3863 || Lr: 0.000100
2024-02-02 22:30:26,882 Epoch  55: Total Training Recognition Loss 0.21  Total Training Translation Loss 144.94 
2024-02-02 22:30:26,882 EPOCH 56
2024-02-02 22:30:31,375 [Epoch: 056 Step: 00007400] Batch Recognition Loss:   0.000720 => Gls Tokens per Sec:     1514 || Batch Translation Loss:   0.957018 => Txt Tokens per Sec:     4234 || Lr: 0.000100
2024-02-02 22:30:34,212 Epoch  56: Total Training Recognition Loss 0.18  Total Training Translation Loss 145.50 
2024-02-02 22:30:34,213 EPOCH 57
2024-02-02 22:30:36,970 [Epoch: 057 Step: 00007500] Batch Recognition Loss:   0.003359 => Gls Tokens per Sec:     1509 || Batch Translation Loss:   1.457090 => Txt Tokens per Sec:     4142 || Lr: 0.000100
2024-02-02 22:30:41,925 Epoch  57: Total Training Recognition Loss 0.22  Total Training Translation Loss 155.78 
2024-02-02 22:30:41,926 EPOCH 58
2024-02-02 22:30:43,143 [Epoch: 058 Step: 00007600] Batch Recognition Loss:   0.003622 => Gls Tokens per Sec:     1249 || Batch Translation Loss:   1.398207 => Txt Tokens per Sec:     3722 || Lr: 0.000100
2024-02-02 22:30:48,883 [Epoch: 058 Step: 00007700] Batch Recognition Loss:   0.000545 => Gls Tokens per Sec:     1394 || Batch Translation Loss:   0.799510 => Txt Tokens per Sec:     3840 || Lr: 0.000100
2024-02-02 22:30:49,825 Epoch  58: Total Training Recognition Loss 0.17  Total Training Translation Loss 130.31 
2024-02-02 22:30:49,825 EPOCH 59
2024-02-02 22:30:54,608 [Epoch: 059 Step: 00007800] Batch Recognition Loss:   0.001002 => Gls Tokens per Sec:     1439 || Batch Translation Loss:   0.842997 => Txt Tokens per Sec:     3987 || Lr: 0.000100
2024-02-02 22:30:57,218 Epoch  59: Total Training Recognition Loss 0.18  Total Training Translation Loss 121.15 
2024-02-02 22:30:57,219 EPOCH 60
2024-02-02 22:31:00,288 [Epoch: 060 Step: 00007900] Batch Recognition Loss:   0.019702 => Gls Tokens per Sec:     1382 || Batch Translation Loss:   0.512078 => Txt Tokens per Sec:     4057 || Lr: 0.000100
2024-02-02 22:31:04,890 Epoch  60: Total Training Recognition Loss 0.21  Total Training Translation Loss 119.65 
2024-02-02 22:31:04,891 EPOCH 61
2024-02-02 22:31:05,910 [Epoch: 061 Step: 00008000] Batch Recognition Loss:   0.001611 => Gls Tokens per Sec:     1573 || Batch Translation Loss:   0.701481 => Txt Tokens per Sec:     4208 || Lr: 0.000100
2024-02-02 22:31:16,710 Hooray! New best validation result [eval_metric]!
2024-02-02 22:31:16,711 Saving new checkpoint.
2024-02-02 22:31:16,982 Validation result at epoch  61, step     8000: duration: 11.0727s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00771	Translation Loss: 80972.08594	PPL: 3303.97144
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.98	(BLEU-1: 12.52,	BLEU-2: 4.00,	BLEU-3: 1.79,	BLEU-4: 0.98)
	CHRF 17.82	ROUGE 10.84
2024-02-02 22:31:16,983 Logging Recognition and Translation Outputs
2024-02-02 22:31:16,983 ========================================================================================================================
2024-02-02 22:31:16,983 Logging Sequence: 88_57.00
2024-02-02 22:31:16,983 	Gloss Reference :	A B+C+D+E
2024-02-02 22:31:16,984 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:31:16,984 	Gloss Alignment :	         
2024-02-02 22:31:16,984 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:31:16,986 	Text Reference  :	******* which   stated messi  we're waiting for     you  to come      here you will be  finished when you come 
2024-02-02 22:31:16,986 	Text Hypothesis :	notably rosario has    become the   most    violent city in argentina with 250 to   300 murders  in   the world
2024-02-02 22:31:16,986 	Text Alignment  :	I       S       S      S      S     S       S       S    S  S         S    S   S    S   S        S    S   S    
2024-02-02 22:31:16,986 ========================================================================================================================
2024-02-02 22:31:16,986 Logging Sequence: 171_142.00
2024-02-02 22:31:16,986 	Gloss Reference :	A B+C+D+E
2024-02-02 22:31:16,987 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:31:16,987 	Gloss Alignment :	         
2024-02-02 22:31:16,987 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:31:16,988 	Text Reference  :	****** this decision on   dhoni     made     a     significant impact as   pathirana claimed two tough wickets
2024-02-02 22:31:16,988 	Text Hypothesis :	dahiya will now      have lucknow's staudium named after       late   with a         total   of  6     medals 
2024-02-02 22:31:16,988 	Text Alignment  :	I      S    S        S    S         S        S     S           S      S    S         S       S   S     S      
2024-02-02 22:31:16,989 ========================================================================================================================
2024-02-02 22:31:16,989 Logging Sequence: 125_207.00
2024-02-02 22:31:16,989 	Gloss Reference :	A B+C+D+E
2024-02-02 22:31:16,989 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:31:16,989 	Gloss Alignment :	         
2024-02-02 22:31:16,989 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:31:16,990 	Text Reference  :	he had not practised since he   returned and he  had    also fallen sick  
2024-02-02 22:31:16,990 	Text Hypothesis :	** *** *** neeraj    was   very happy    by  the finals of   the    finals
2024-02-02 22:31:16,990 	Text Alignment  :	D  D   D   S         S     S    S        S   S   S      S    S      S     
2024-02-02 22:31:16,990 ========================================================================================================================
2024-02-02 22:31:16,990 Logging Sequence: 68_230.00
2024-02-02 22:31:16,991 	Gloss Reference :	A B+C+D+E
2024-02-02 22:31:16,991 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:31:16,991 	Gloss Alignment :	         
2024-02-02 22:31:16,991 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:31:16,992 	Text Reference  :	** ****** ** **** ** let us     know  what    you think in    the **** *** ****** comments below
2024-02-02 22:31:16,993 	Text Hypothesis :	he scored 29 runs in a   wicket while spinner did not   touch the over who bowled very     well 
2024-02-02 22:31:16,993 	Text Alignment  :	I  I      I  I    I  S   S      S     S       S   S     S         I    I   I      S        S    
2024-02-02 22:31:16,993 ========================================================================================================================
2024-02-02 22:31:16,993 Logging Sequence: 126_82.00
2024-02-02 22:31:16,993 	Gloss Reference :	A B+C+D+E
2024-02-02 22:31:16,993 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:31:16,993 	Gloss Alignment :	         
2024-02-02 22:31:16,993 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:31:16,995 	Text Reference  :	neeraj also dedicated his gold medal  to ********* **** ** former indian olympians who came close to winning *** medals
2024-02-02 22:31:16,995 	Text Hypothesis :	he     also dedicated *** the  medals to olympians like pt usha   and    others    who came close to winning the medals
2024-02-02 22:31:16,995 	Text Alignment  :	S                     D   S    S         I         I    I  S      S      S                                   I         
2024-02-02 22:31:16,996 ========================================================================================================================
2024-02-02 22:31:23,135 [Epoch: 061 Step: 00008100] Batch Recognition Loss:   0.002679 => Gls Tokens per Sec:     1300 || Batch Translation Loss:   0.643906 => Txt Tokens per Sec:     3617 || Lr: 0.000100
2024-02-02 22:31:23,925 Epoch  61: Total Training Recognition Loss 0.17  Total Training Translation Loss 127.17 
2024-02-02 22:31:23,925 EPOCH 62
2024-02-02 22:31:28,456 [Epoch: 062 Step: 00008200] Batch Recognition Loss:   0.001005 => Gls Tokens per Sec:     1537 || Batch Translation Loss:   0.786585 => Txt Tokens per Sec:     4272 || Lr: 0.000100
2024-02-02 22:31:30,904 Epoch  62: Total Training Recognition Loss 0.14  Total Training Translation Loss 112.40 
2024-02-02 22:31:30,905 EPOCH 63
2024-02-02 22:31:33,945 [Epoch: 063 Step: 00008300] Batch Recognition Loss:   0.001432 => Gls Tokens per Sec:     1421 || Batch Translation Loss:   1.168138 => Txt Tokens per Sec:     3881 || Lr: 0.000100
2024-02-02 22:31:38,673 Epoch  63: Total Training Recognition Loss 0.16  Total Training Translation Loss 99.38 
2024-02-02 22:31:38,673 EPOCH 64
2024-02-02 22:31:39,749 [Epoch: 064 Step: 00008400] Batch Recognition Loss:   0.000555 => Gls Tokens per Sec:     1563 || Batch Translation Loss:   0.674747 => Txt Tokens per Sec:     4208 || Lr: 0.000100
2024-02-02 22:31:45,652 [Epoch: 064 Step: 00008500] Batch Recognition Loss:   0.000708 => Gls Tokens per Sec:     1354 || Batch Translation Loss:   0.919876 => Txt Tokens per Sec:     3776 || Lr: 0.000100
2024-02-02 22:31:46,311 Epoch  64: Total Training Recognition Loss 0.14  Total Training Translation Loss 91.93 
2024-02-02 22:31:46,311 EPOCH 65
2024-02-02 22:31:51,136 [Epoch: 065 Step: 00008600] Batch Recognition Loss:   0.000718 => Gls Tokens per Sec:     1460 || Batch Translation Loss:   0.586015 => Txt Tokens per Sec:     3994 || Lr: 0.000100
2024-02-02 22:31:53,616 Epoch  65: Total Training Recognition Loss 0.15  Total Training Translation Loss 98.58 
2024-02-02 22:31:53,616 EPOCH 66
2024-02-02 22:31:56,451 [Epoch: 066 Step: 00008700] Batch Recognition Loss:   0.000745 => Gls Tokens per Sec:     1552 || Batch Translation Loss:   0.817943 => Txt Tokens per Sec:     4293 || Lr: 0.000100
2024-02-02 22:32:01,095 Epoch  66: Total Training Recognition Loss 0.14  Total Training Translation Loss 109.40 
2024-02-02 22:32:01,096 EPOCH 67
2024-02-02 22:32:02,351 [Epoch: 067 Step: 00008800] Batch Recognition Loss:   0.000319 => Gls Tokens per Sec:     1404 || Batch Translation Loss:   0.792616 => Txt Tokens per Sec:     4012 || Lr: 0.000100
2024-02-02 22:32:08,125 [Epoch: 067 Step: 00008900] Batch Recognition Loss:   0.002320 => Gls Tokens per Sec:     1384 || Batch Translation Loss:   0.531734 => Txt Tokens per Sec:     3793 || Lr: 0.000100
2024-02-02 22:32:08,813 Epoch  67: Total Training Recognition Loss 0.15  Total Training Translation Loss 93.39 
2024-02-02 22:32:08,814 EPOCH 68
2024-02-02 22:32:13,564 [Epoch: 068 Step: 00009000] Batch Recognition Loss:   0.000391 => Gls Tokens per Sec:     1499 || Batch Translation Loss:   0.395112 => Txt Tokens per Sec:     4106 || Lr: 0.000100
2024-02-02 22:32:16,254 Epoch  68: Total Training Recognition Loss 0.14  Total Training Translation Loss 93.86 
2024-02-02 22:32:16,255 EPOCH 69
2024-02-02 22:32:19,315 [Epoch: 069 Step: 00009100] Batch Recognition Loss:   0.000398 => Gls Tokens per Sec:     1465 || Batch Translation Loss:   0.788211 => Txt Tokens per Sec:     4009 || Lr: 0.000100
2024-02-02 22:32:23,805 Epoch  69: Total Training Recognition Loss 0.16  Total Training Translation Loss 85.98 
2024-02-02 22:32:23,805 EPOCH 70
2024-02-02 22:32:25,370 [Epoch: 070 Step: 00009200] Batch Recognition Loss:   0.000763 => Gls Tokens per Sec:     1177 || Batch Translation Loss:   0.324390 => Txt Tokens per Sec:     3326 || Lr: 0.000100
2024-02-02 22:32:31,389 [Epoch: 070 Step: 00009300] Batch Recognition Loss:   0.001597 => Gls Tokens per Sec:     1328 || Batch Translation Loss:   0.794722 => Txt Tokens per Sec:     3656 || Lr: 0.000100
2024-02-02 22:32:32,004 Epoch  70: Total Training Recognition Loss 0.16  Total Training Translation Loss 92.47 
2024-02-02 22:32:32,004 EPOCH 71
2024-02-02 22:32:36,910 [Epoch: 071 Step: 00009400] Batch Recognition Loss:   0.001172 => Gls Tokens per Sec:     1468 || Batch Translation Loss:   0.858537 => Txt Tokens per Sec:     4039 || Lr: 0.000100
2024-02-02 22:32:39,388 Epoch  71: Total Training Recognition Loss 0.13  Total Training Translation Loss 93.32 
2024-02-02 22:32:39,388 EPOCH 72
2024-02-02 22:32:42,642 [Epoch: 072 Step: 00009500] Batch Recognition Loss:   0.000737 => Gls Tokens per Sec:     1402 || Batch Translation Loss:   0.351842 => Txt Tokens per Sec:     3834 || Lr: 0.000100
2024-02-02 22:32:47,030 Epoch  72: Total Training Recognition Loss 0.17  Total Training Translation Loss 87.21 
2024-02-02 22:32:47,030 EPOCH 73
2024-02-02 22:32:48,380 [Epoch: 073 Step: 00009600] Batch Recognition Loss:   0.000641 => Gls Tokens per Sec:     1423 || Batch Translation Loss:   0.308688 => Txt Tokens per Sec:     3986 || Lr: 0.000100
2024-02-02 22:32:54,160 [Epoch: 073 Step: 00009700] Batch Recognition Loss:   0.000415 => Gls Tokens per Sec:     1383 || Batch Translation Loss:   0.789193 => Txt Tokens per Sec:     3843 || Lr: 0.000100
2024-02-02 22:32:54,617 Epoch  73: Total Training Recognition Loss 0.12  Total Training Translation Loss 85.70 
2024-02-02 22:32:54,617 EPOCH 74
2024-02-02 22:32:59,708 [Epoch: 074 Step: 00009800] Batch Recognition Loss:   0.001054 => Gls Tokens per Sec:     1430 || Batch Translation Loss:   0.765983 => Txt Tokens per Sec:     3924 || Lr: 0.000100
2024-02-02 22:33:02,237 Epoch  74: Total Training Recognition Loss 0.15  Total Training Translation Loss 78.54 
2024-02-02 22:33:02,238 EPOCH 75
2024-02-02 22:33:05,391 [Epoch: 075 Step: 00009900] Batch Recognition Loss:   0.001477 => Gls Tokens per Sec:     1472 || Batch Translation Loss:   0.350024 => Txt Tokens per Sec:     4149 || Lr: 0.000100
2024-02-02 22:33:09,618 Epoch  75: Total Training Recognition Loss 0.15  Total Training Translation Loss 85.77 
2024-02-02 22:33:09,619 EPOCH 76
2024-02-02 22:33:11,266 [Epoch: 076 Step: 00010000] Batch Recognition Loss:   0.000365 => Gls Tokens per Sec:     1215 || Batch Translation Loss:   0.357883 => Txt Tokens per Sec:     3708 || Lr: 0.000100
2024-02-02 22:33:22,171 Validation result at epoch  76, step    10000: duration: 10.9048s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00473	Translation Loss: 86098.09375	PPL: 5518.38770
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.90	(BLEU-1: 11.49,	BLEU-2: 3.68,	BLEU-3: 1.63,	BLEU-4: 0.90)
	CHRF 17.21	ROUGE 9.55
2024-02-02 22:33:22,172 Logging Recognition and Translation Outputs
2024-02-02 22:33:22,172 ========================================================================================================================
2024-02-02 22:33:22,172 Logging Sequence: 159_139.00
2024-02-02 22:33:22,173 	Gloss Reference :	A B+C+D+E
2024-02-02 22:33:22,173 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:33:22,173 	Gloss Alignment :	         
2024-02-02 22:33:22,173 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:33:22,174 	Text Reference  :	he took time and finally was ready for the    asia cup     where he   scored the century  
2024-02-02 22:33:22,174 	Text Hypothesis :	** **** **** *** ******* *** ***** now people were shocked when  this is     not permitted
2024-02-02 22:33:22,174 	Text Alignment  :	D  D    D    D   D       D   D     S   S      S    S       S     S    S      S   S        
2024-02-02 22:33:22,174 ========================================================================================================================
2024-02-02 22:33:22,174 Logging Sequence: 159_159.00
2024-02-02 22:33:22,175 	Gloss Reference :	A B+C+D+E
2024-02-02 22:33:22,175 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:33:22,175 	Gloss Alignment :	         
2024-02-02 22:33:22,175 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:33:22,177 	Text Reference  :	he said it wasn't easy the mind has to be focussed and he    is **** glad  that he is back in form with    the       asia cup     century 
2024-02-02 22:33:22,177 	Text Hypothesis :	** **** ** ****** **** *** **** *** ** ** ******** *** kohli is very happy that ** ** **** ** **** earlier australia new  zealand pakistan
2024-02-02 22:33:22,177 	Text Alignment  :	D  D    D  D      D    D   D    D   D  D  D        D   S        I    S          D  D  D    D  D    S       S         S    S       S       
2024-02-02 22:33:22,177 ========================================================================================================================
2024-02-02 22:33:22,177 Logging Sequence: 103_8.00
2024-02-02 22:33:22,177 	Gloss Reference :	A B+C+D+E
2024-02-02 22:33:22,177 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:33:22,177 	Gloss Alignment :	         
2024-02-02 22:33:22,178 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:33:22,178 	Text Reference  :	were going on           in    birmingham england      from 28th july to  8th     august 2022 
2024-02-02 22:33:22,179 	Text Hypothesis :	**** the   commonwealth games encourage  independence from **** **** the british empire games
2024-02-02 22:33:22,179 	Text Alignment  :	D    S     S            S     S          S                 D    D    S   S       S      S    
2024-02-02 22:33:22,179 ========================================================================================================================
2024-02-02 22:33:22,179 Logging Sequence: 164_546.00
2024-02-02 22:33:22,179 	Gloss Reference :	A B+C+D+E
2024-02-02 22:33:22,179 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:33:22,179 	Gloss Alignment :	         
2024-02-02 22:33:22,179 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:33:22,180 	Text Reference  :	reliance has     turned out to   be          the ****** strongest company
2024-02-02 22:33:22,180 	Text Hypothesis :	for      another man    is  seen celebrating the common wealth    games  
2024-02-02 22:33:22,180 	Text Alignment  :	S        S       S      S   S    S               I      S         S      
2024-02-02 22:33:22,181 ========================================================================================================================
2024-02-02 22:33:22,181 Logging Sequence: 132_173.00
2024-02-02 22:33:22,181 	Gloss Reference :	A B+C+D+E
2024-02-02 22:33:22,181 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:33:22,181 	Gloss Alignment :	         
2024-02-02 22:33:22,181 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:33:22,182 	Text Reference  :	usman is **** ******* ** *** **** ****** ** **** australia' first muslim player
2024-02-02 22:33:22,182 	Text Hypothesis :	virat is very popular on the most shared by this has        not   stop   him   
2024-02-02 22:33:22,182 	Text Alignment  :	S        I    I       I  I   I    I      I  I    S          S     S      S     
2024-02-02 22:33:22,182 ========================================================================================================================
2024-02-02 22:33:27,886 [Epoch: 076 Step: 00010100] Batch Recognition Loss:   0.000886 => Gls Tokens per Sec:     1398 || Batch Translation Loss:   0.464790 => Txt Tokens per Sec:     3809 || Lr: 0.000100
2024-02-02 22:33:28,350 Epoch  76: Total Training Recognition Loss 0.12  Total Training Translation Loss 70.47 
2024-02-02 22:33:28,350 EPOCH 77
2024-02-02 22:33:33,526 [Epoch: 077 Step: 00010200] Batch Recognition Loss:   0.000286 => Gls Tokens per Sec:     1423 || Batch Translation Loss:   0.312861 => Txt Tokens per Sec:     3983 || Lr: 0.000100
2024-02-02 22:33:35,803 Epoch  77: Total Training Recognition Loss 0.13  Total Training Translation Loss 71.69 
2024-02-02 22:33:35,804 EPOCH 78
2024-02-02 22:33:39,036 [Epoch: 078 Step: 00010300] Batch Recognition Loss:   0.001354 => Gls Tokens per Sec:     1460 || Batch Translation Loss:   1.016946 => Txt Tokens per Sec:     4111 || Lr: 0.000100
2024-02-02 22:33:43,398 Epoch  78: Total Training Recognition Loss 0.10  Total Training Translation Loss 69.05 
2024-02-02 22:33:43,399 EPOCH 79
2024-02-02 22:33:44,926 [Epoch: 079 Step: 00010400] Batch Recognition Loss:   0.000349 => Gls Tokens per Sec:     1363 || Batch Translation Loss:   0.416576 => Txt Tokens per Sec:     3903 || Lr: 0.000100
2024-02-02 22:33:50,732 [Epoch: 079 Step: 00010500] Batch Recognition Loss:   0.000726 => Gls Tokens per Sec:     1376 || Batch Translation Loss:   0.338214 => Txt Tokens per Sec:     3830 || Lr: 0.000100
2024-02-02 22:33:51,065 Epoch  79: Total Training Recognition Loss 0.13  Total Training Translation Loss 67.98 
2024-02-02 22:33:51,065 EPOCH 80
2024-02-02 22:33:56,407 [Epoch: 080 Step: 00010600] Batch Recognition Loss:   0.000728 => Gls Tokens per Sec:     1393 || Batch Translation Loss:   0.602303 => Txt Tokens per Sec:     3852 || Lr: 0.000100
2024-02-02 22:33:58,681 Epoch  80: Total Training Recognition Loss 0.10  Total Training Translation Loss 62.20 
2024-02-02 22:33:58,682 EPOCH 81
2024-02-02 22:34:01,703 [Epoch: 081 Step: 00010700] Batch Recognition Loss:   0.000652 => Gls Tokens per Sec:     1589 || Batch Translation Loss:   0.360950 => Txt Tokens per Sec:     4396 || Lr: 0.000100
2024-02-02 22:34:05,862 Epoch  81: Total Training Recognition Loss 0.11  Total Training Translation Loss 71.69 
2024-02-02 22:34:05,862 EPOCH 82
2024-02-02 22:34:07,269 [Epoch: 082 Step: 00010800] Batch Recognition Loss:   0.000792 => Gls Tokens per Sec:     1538 || Batch Translation Loss:   1.238693 => Txt Tokens per Sec:     3916 || Lr: 0.000100
2024-02-02 22:34:13,397 [Epoch: 082 Step: 00010900] Batch Recognition Loss:   0.003169 => Gls Tokens per Sec:     1304 || Batch Translation Loss:   0.516825 => Txt Tokens per Sec:     3675 || Lr: 0.000100
2024-02-02 22:34:13,831 Epoch  82: Total Training Recognition Loss 0.14  Total Training Translation Loss 76.68 
2024-02-02 22:34:13,831 EPOCH 83
2024-02-02 22:34:19,113 [Epoch: 083 Step: 00011000] Batch Recognition Loss:   0.000443 => Gls Tokens per Sec:     1424 || Batch Translation Loss:   0.192723 => Txt Tokens per Sec:     3929 || Lr: 0.000100
2024-02-02 22:34:21,251 Epoch  83: Total Training Recognition Loss 0.13  Total Training Translation Loss 77.49 
2024-02-02 22:34:21,251 EPOCH 84
2024-02-02 22:34:24,715 [Epoch: 084 Step: 00011100] Batch Recognition Loss:   0.001236 => Gls Tokens per Sec:     1409 || Batch Translation Loss:   0.292391 => Txt Tokens per Sec:     3949 || Lr: 0.000100
2024-02-02 22:34:28,782 Epoch  84: Total Training Recognition Loss 0.10  Total Training Translation Loss 72.47 
2024-02-02 22:34:28,783 EPOCH 85
2024-02-02 22:34:30,503 [Epoch: 085 Step: 00011200] Batch Recognition Loss:   0.001403 => Gls Tokens per Sec:     1303 || Batch Translation Loss:   0.558476 => Txt Tokens per Sec:     3853 || Lr: 0.000100
2024-02-02 22:34:36,312 [Epoch: 085 Step: 00011300] Batch Recognition Loss:   0.001905 => Gls Tokens per Sec:     1375 || Batch Translation Loss:   0.700404 => Txt Tokens per Sec:     3765 || Lr: 0.000100
2024-02-02 22:34:36,535 Epoch  85: Total Training Recognition Loss 0.10  Total Training Translation Loss 65.80 
2024-02-02 22:34:36,535 EPOCH 86
2024-02-02 22:34:41,416 [Epoch: 086 Step: 00011400] Batch Recognition Loss:   0.000696 => Gls Tokens per Sec:     1557 || Batch Translation Loss:   0.158669 => Txt Tokens per Sec:     4336 || Lr: 0.000100
2024-02-02 22:34:43,662 Epoch  86: Total Training Recognition Loss 0.10  Total Training Translation Loss 64.56 
2024-02-02 22:34:43,663 EPOCH 87
2024-02-02 22:34:46,972 [Epoch: 087 Step: 00011500] Batch Recognition Loss:   0.003089 => Gls Tokens per Sec:     1499 || Batch Translation Loss:   0.379168 => Txt Tokens per Sec:     4200 || Lr: 0.000100
2024-02-02 22:34:50,908 Epoch  87: Total Training Recognition Loss 0.09  Total Training Translation Loss 58.70 
2024-02-02 22:34:50,909 EPOCH 88
2024-02-02 22:34:52,588 [Epoch: 088 Step: 00011600] Batch Recognition Loss:   0.005297 => Gls Tokens per Sec:     1382 || Batch Translation Loss:   0.353397 => Txt Tokens per Sec:     3757 || Lr: 0.000100
2024-02-02 22:34:58,254 [Epoch: 088 Step: 00011700] Batch Recognition Loss:   0.000304 => Gls Tokens per Sec:     1411 || Batch Translation Loss:   0.359260 => Txt Tokens per Sec:     3945 || Lr: 0.000100
2024-02-02 22:34:58,484 Epoch  88: Total Training Recognition Loss 0.12  Total Training Translation Loss 59.14 
2024-02-02 22:34:58,484 EPOCH 89
2024-02-02 22:35:03,880 [Epoch: 089 Step: 00011800] Batch Recognition Loss:   0.000971 => Gls Tokens per Sec:     1424 || Batch Translation Loss:   0.949806 => Txt Tokens per Sec:     3937 || Lr: 0.000100
2024-02-02 22:35:05,930 Epoch  89: Total Training Recognition Loss 0.10  Total Training Translation Loss 55.67 
2024-02-02 22:35:05,930 EPOCH 90
2024-02-02 22:35:09,266 [Epoch: 090 Step: 00011900] Batch Recognition Loss:   0.000837 => Gls Tokens per Sec:     1511 || Batch Translation Loss:   0.144400 => Txt Tokens per Sec:     4207 || Lr: 0.000100
2024-02-02 22:35:13,135 Epoch  90: Total Training Recognition Loss 0.11  Total Training Translation Loss 52.82 
2024-02-02 22:35:13,136 EPOCH 91
2024-02-02 22:35:14,732 [Epoch: 091 Step: 00012000] Batch Recognition Loss:   0.000523 => Gls Tokens per Sec:     1505 || Batch Translation Loss:   0.278034 => Txt Tokens per Sec:     4136 || Lr: 0.000100
2024-02-02 22:35:25,469 Validation result at epoch  91, step    12000: duration: 10.7360s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00508	Translation Loss: 88432.92969	PPL: 6970.81396
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.72	(BLEU-1: 11.39,	BLEU-2: 3.59,	BLEU-3: 1.38,	BLEU-4: 0.72)
	CHRF 16.96	ROUGE 9.93
2024-02-02 22:35:25,470 Logging Recognition and Translation Outputs
2024-02-02 22:35:25,470 ========================================================================================================================
2024-02-02 22:35:25,470 Logging Sequence: 177_50.00
2024-02-02 22:35:25,470 	Gloss Reference :	A B+C+D+E
2024-02-02 22:35:25,470 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:35:25,471 	Gloss Alignment :	         
2024-02-02 22:35:25,471 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:35:25,472 	Text Reference  :	***** a   similar reward of       rs   50000 was announced for information against his   associate ajay kumar
2024-02-02 22:35:25,472 	Text Hypothesis :	after her son     was    arrested when she   was postponed to  the         delhi   court would     be   held 
2024-02-02 22:35:25,472 	Text Alignment  :	I     S   S       S      S        S    S         S         S   S           S       S     S         S    S    
2024-02-02 22:35:25,473 ========================================================================================================================
2024-02-02 22:35:25,473 Logging Sequence: 122_86.00
2024-02-02 22:35:25,473 	Gloss Reference :	A B+C+D+E
2024-02-02 22:35:25,473 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:35:25,473 	Gloss Alignment :	         
2024-02-02 22:35:25,473 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:35:25,474 	Text Reference  :	*** after  winning chanu spoke       to  the media and said 
2024-02-02 22:35:25,474 	Text Hypothesis :	the indian team    was   responsible for the ***** 1st match
2024-02-02 22:35:25,474 	Text Alignment  :	I   S      S       S     S           S       D     S   S    
2024-02-02 22:35:25,474 ========================================================================================================================
2024-02-02 22:35:25,474 Logging Sequence: 165_27.00
2024-02-02 22:35:25,475 	Gloss Reference :	A B+C+D+E
2024-02-02 22:35:25,475 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:35:25,475 	Gloss Alignment :	         
2024-02-02 22:35:25,475 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:35:25,476 	Text Reference  :	******** so then they change their routes some      people believe in   this while some   don't   
2024-02-02 22:35:25,476 	Text Hypothesis :	whenever he has  now  handed over  the    captaincy to     his     wife for  a     sudden decision
2024-02-02 22:35:25,477 	Text Alignment  :	I        S  S    S    S      S     S      S         S      S       S    S    S     S      S       
2024-02-02 22:35:25,477 ========================================================================================================================
2024-02-02 22:35:25,477 Logging Sequence: 70_65.00
2024-02-02 22:35:25,477 	Gloss Reference :	A B+C+D+E
2024-02-02 22:35:25,477 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:35:25,477 	Gloss Alignment :	         
2024-02-02 22:35:25,477 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:35:25,478 	Text Reference  :	during the press conference a   table was placed in  front   of        the   media   
2024-02-02 22:35:25,478 	Text Hypothesis :	****** *** ***** italy      had won   the toss   and prevent infection boost immunity
2024-02-02 22:35:25,478 	Text Alignment  :	D      D   D     S          S   S     S   S      S   S       S         S     S       
2024-02-02 22:35:25,479 ========================================================================================================================
2024-02-02 22:35:25,479 Logging Sequence: 149_65.00
2024-02-02 22:35:25,479 	Gloss Reference :	A B+C+D+E
2024-02-02 22:35:25,479 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:35:25,479 	Gloss Alignment :	         
2024-02-02 22:35:25,479 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:35:25,481 	Text Reference  :	at 6am on 6th november 2022 the   police reached sri  lankan team's    hotel in sydney australia's central business district cbd       
2024-02-02 22:35:25,481 	Text Hypothesis :	** *** in an  image    of   other indian will    play more   intensive audit in ****** *********** ******* ******** the      tournament
2024-02-02 22:35:25,482 	Text Alignment  :	D  D   S  S   S        S    S     S      S       S    S      S         S        D      D           D       D        S        S         
2024-02-02 22:35:25,482 ========================================================================================================================
2024-02-02 22:35:31,515 [Epoch: 091 Step: 00012100] Batch Recognition Loss:   0.000265 => Gls Tokens per Sec:     1322 || Batch Translation Loss:   0.694741 => Txt Tokens per Sec:     3688 || Lr: 0.000100
2024-02-02 22:35:31,664 Epoch  91: Total Training Recognition Loss 0.12  Total Training Translation Loss 58.51 
2024-02-02 22:35:31,665 EPOCH 92
2024-02-02 22:35:37,134 [Epoch: 092 Step: 00012200] Batch Recognition Loss:   0.001253 => Gls Tokens per Sec:     1419 || Batch Translation Loss:   0.189429 => Txt Tokens per Sec:     3923 || Lr: 0.000100
2024-02-02 22:35:39,124 Epoch  92: Total Training Recognition Loss 0.12  Total Training Translation Loss 61.71 
2024-02-02 22:35:39,124 EPOCH 93
2024-02-02 22:35:42,575 [Epoch: 093 Step: 00012300] Batch Recognition Loss:   0.000381 => Gls Tokens per Sec:     1484 || Batch Translation Loss:   0.518939 => Txt Tokens per Sec:     4074 || Lr: 0.000100
2024-02-02 22:35:46,413 Epoch  93: Total Training Recognition Loss 0.12  Total Training Translation Loss 57.66 
2024-02-02 22:35:46,413 EPOCH 94
2024-02-02 22:35:48,244 [Epoch: 094 Step: 00012400] Batch Recognition Loss:   0.002510 => Gls Tokens per Sec:     1355 || Batch Translation Loss:   0.291858 => Txt Tokens per Sec:     3815 || Lr: 0.000100
2024-02-02 22:35:53,076 [Epoch: 094 Step: 00012500] Batch Recognition Loss:   0.001723 => Gls Tokens per Sec:     1653 || Batch Translation Loss:   0.266014 => Txt Tokens per Sec:     4553 || Lr: 0.000100
2024-02-02 22:35:53,203 Epoch  94: Total Training Recognition Loss 0.10  Total Training Translation Loss 46.85 
2024-02-02 22:35:53,204 EPOCH 95
2024-02-02 22:35:58,588 [Epoch: 095 Step: 00012600] Batch Recognition Loss:   0.000706 => Gls Tokens per Sec:     1456 || Batch Translation Loss:   0.491457 => Txt Tokens per Sec:     4054 || Lr: 0.000100
2024-02-02 22:36:00,446 Epoch  95: Total Training Recognition Loss 0.09  Total Training Translation Loss 47.19 
2024-02-02 22:36:00,446 EPOCH 96
2024-02-02 22:36:03,908 [Epoch: 096 Step: 00012700] Batch Recognition Loss:   0.000358 => Gls Tokens per Sec:     1502 || Batch Translation Loss:   0.166190 => Txt Tokens per Sec:     4214 || Lr: 0.000100
2024-02-02 22:36:07,893 Epoch  96: Total Training Recognition Loss 0.11  Total Training Translation Loss 49.51 
2024-02-02 22:36:07,894 EPOCH 97
2024-02-02 22:36:09,843 [Epoch: 097 Step: 00012800] Batch Recognition Loss:   0.000596 => Gls Tokens per Sec:     1313 || Batch Translation Loss:   0.571062 => Txt Tokens per Sec:     3717 || Lr: 0.000100
2024-02-02 22:36:15,392 [Epoch: 097 Step: 00012900] Batch Recognition Loss:   0.000270 => Gls Tokens per Sec:     1440 || Batch Translation Loss:   0.186529 => Txt Tokens per Sec:     3967 || Lr: 0.000100
2024-02-02 22:36:15,443 Epoch  97: Total Training Recognition Loss 0.08  Total Training Translation Loss 51.15 
2024-02-02 22:36:15,443 EPOCH 98
2024-02-02 22:36:21,130 [Epoch: 098 Step: 00013000] Batch Recognition Loss:   0.000491 => Gls Tokens per Sec:     1393 || Batch Translation Loss:   0.090312 => Txt Tokens per Sec:     3854 || Lr: 0.000100
2024-02-02 22:36:22,969 Epoch  98: Total Training Recognition Loss 0.12  Total Training Translation Loss 58.27 
2024-02-02 22:36:22,970 EPOCH 99
2024-02-02 22:36:26,414 [Epoch: 099 Step: 00013100] Batch Recognition Loss:   0.000478 => Gls Tokens per Sec:     1534 || Batch Translation Loss:   0.186629 => Txt Tokens per Sec:     4097 || Lr: 0.000100
2024-02-02 22:36:30,383 Epoch  99: Total Training Recognition Loss 0.09  Total Training Translation Loss 54.59 
2024-02-02 22:36:30,383 EPOCH 100
2024-02-02 22:36:32,169 [Epoch: 100 Step: 00013200] Batch Recognition Loss:   0.000419 => Gls Tokens per Sec:     1479 || Batch Translation Loss:   0.119571 => Txt Tokens per Sec:     4316 || Lr: 0.000100
2024-02-02 22:36:37,950 [Epoch: 100 Step: 00013300] Batch Recognition Loss:   0.001088 => Gls Tokens per Sec:     1382 || Batch Translation Loss:   0.870389 => Txt Tokens per Sec:     3771 || Lr: 0.000100
2024-02-02 22:36:37,951 Epoch 100: Total Training Recognition Loss 0.13  Total Training Translation Loss 50.68 
2024-02-02 22:36:37,951 EPOCH 101
2024-02-02 22:36:43,563 [Epoch: 101 Step: 00013400] Batch Recognition Loss:   0.000294 => Gls Tokens per Sec:     1426 || Batch Translation Loss:   0.276845 => Txt Tokens per Sec:     3975 || Lr: 0.000100
2024-02-02 22:36:45,426 Epoch 101: Total Training Recognition Loss 0.11  Total Training Translation Loss 53.56 
2024-02-02 22:36:45,427 EPOCH 102
2024-02-02 22:36:49,117 [Epoch: 102 Step: 00013500] Batch Recognition Loss:   0.002495 => Gls Tokens per Sec:     1453 || Batch Translation Loss:   0.186509 => Txt Tokens per Sec:     4099 || Lr: 0.000100
2024-02-02 22:36:52,752 Epoch 102: Total Training Recognition Loss 0.14  Total Training Translation Loss 51.40 
2024-02-02 22:36:52,752 EPOCH 103
2024-02-02 22:36:54,727 [Epoch: 103 Step: 00013600] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     1378 || Batch Translation Loss:   0.240328 => Txt Tokens per Sec:     3819 || Lr: 0.000100
2024-02-02 22:37:00,455 Epoch 103: Total Training Recognition Loss 0.08  Total Training Translation Loss 52.29 
2024-02-02 22:37:00,456 EPOCH 104
2024-02-02 22:37:00,536 [Epoch: 104 Step: 00013700] Batch Recognition Loss:   0.000324 => Gls Tokens per Sec:     1013 || Batch Translation Loss:   0.194843 => Txt Tokens per Sec:     3050 || Lr: 0.000100
2024-02-02 22:37:06,132 [Epoch: 104 Step: 00013800] Batch Recognition Loss:   0.000580 => Gls Tokens per Sec:     1430 || Batch Translation Loss:   0.409668 => Txt Tokens per Sec:     3963 || Lr: 0.000100
2024-02-02 22:37:08,046 Epoch 104: Total Training Recognition Loss 0.12  Total Training Translation Loss 51.93 
2024-02-02 22:37:08,046 EPOCH 105
2024-02-02 22:37:11,597 [Epoch: 105 Step: 00013900] Batch Recognition Loss:   0.000511 => Gls Tokens per Sec:     1533 || Batch Translation Loss:   0.600016 => Txt Tokens per Sec:     4159 || Lr: 0.000100
2024-02-02 22:37:15,369 Epoch 105: Total Training Recognition Loss 0.14  Total Training Translation Loss 58.87 
2024-02-02 22:37:15,369 EPOCH 106
2024-02-02 22:37:17,254 [Epoch: 106 Step: 00014000] Batch Recognition Loss:   0.000334 => Gls Tokens per Sec:     1486 || Batch Translation Loss:   0.305944 => Txt Tokens per Sec:     4005 || Lr: 0.000100
2024-02-02 22:37:28,480 Hooray! New best validation result [eval_metric]!
2024-02-02 22:37:28,481 Saving new checkpoint.
2024-02-02 22:37:28,745 Validation result at epoch 106, step    14000: duration: 11.4913s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00385	Translation Loss: 89762.45312	PPL: 7962.78027
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.99	(BLEU-1: 11.00,	BLEU-2: 3.71,	BLEU-3: 1.74,	BLEU-4: 0.99)
	CHRF 17.18	ROUGE 9.42
2024-02-02 22:37:28,746 Logging Recognition and Translation Outputs
2024-02-02 22:37:28,746 ========================================================================================================================
2024-02-02 22:37:28,746 Logging Sequence: 141_40.00
2024-02-02 22:37:28,746 	Gloss Reference :	A B+C+D+E
2024-02-02 22:37:28,746 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:37:28,747 	Gloss Alignment :	         
2024-02-02 22:37:28,747 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:37:28,748 	Text Reference  :	got infected with  covid-19      he was quarantined and  could not    take    part     in  the  warmup match
2024-02-02 22:37:28,748 	Text Hypothesis :	*** ******** since participating in the 2020        2020 he    played against welcomed him back to     bat  
2024-02-02 22:37:28,748 	Text Alignment  :	D   D        S     S             S  S   S           S    S     S      S       S        S   S    S      S    
2024-02-02 22:37:28,748 ========================================================================================================================
2024-02-02 22:37:28,749 Logging Sequence: 117_37.00
2024-02-02 22:37:28,749 	Gloss Reference :	A B+C+D+E
2024-02-02 22:37:28,749 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:37:28,749 	Gloss Alignment :	         
2024-02-02 22:37:28,749 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:37:28,750 	Text Reference  :	shikhar dhawan put   up   a  wonderful performance scoring 98      runs    
2024-02-02 22:37:28,750 	Text Hypothesis :	on      23rd   march 2023 at the       1st         match   between pakistan
2024-02-02 22:37:28,750 	Text Alignment  :	S       S      S     S    S  S         S           S       S       S       
2024-02-02 22:37:28,750 ========================================================================================================================
2024-02-02 22:37:28,750 Logging Sequence: 64_13.00
2024-02-02 22:37:28,751 	Gloss Reference :	A B+C+D+E
2024-02-02 22:37:28,751 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:37:28,751 	Gloss Alignment :	         
2024-02-02 22:37:28,751 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:37:28,752 	Text Reference  :	arrangements were made to  move all     the  ipl  matches     to the wankhede stadium in        mumbai
2024-02-02 22:37:28,752 	Text Hypothesis :	************ **** **** the two  players have been quarantined at the ******** ******* ahmedabad hotel 
2024-02-02 22:37:28,752 	Text Alignment  :	D            D    D    S   S    S       S    S    S           S      D        D       S         S     
2024-02-02 22:37:28,752 ========================================================================================================================
2024-02-02 22:37:28,753 Logging Sequence: 98_121.00
2024-02-02 22:37:28,753 	Gloss Reference :	A B+C+D+E
2024-02-02 22:37:28,753 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:37:28,753 	Gloss Alignment :	         
2024-02-02 22:37:28,753 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:37:28,754 	Text Reference  :	so then england legends and   bangladesh legends were added to the  tournament
2024-02-02 22:37:28,754 	Text Hypothesis :	** **** ******* ******* mumal shared     that    they lost  to take hotel     
2024-02-02 22:37:28,754 	Text Alignment  :	D  D    D       D       S     S          S       S    S        S    S         
2024-02-02 22:37:28,754 ========================================================================================================================
2024-02-02 22:37:28,754 Logging Sequence: 179_414.00
2024-02-02 22:37:28,754 	Gloss Reference :	A B+C+D+E
2024-02-02 22:37:28,755 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:37:28,755 	Gloss Alignment :	         
2024-02-02 22:37:28,755 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:37:28,756 	Text Reference  :	we could  not travel to    delhi as  there  was a   lockdown in     our home    town haryana 
2024-02-02 22:37:28,756 	Text Hypothesis :	** before the ioa    could send  the notice wfi has asked    phogat to  collect her  passport
2024-02-02 22:37:28,756 	Text Alignment  :	D  S      S   S      S     S     S   S      S   S   S        S      S   S       S    S       
2024-02-02 22:37:28,757 ========================================================================================================================
2024-02-02 22:37:34,814 Epoch 106: Total Training Recognition Loss 0.10  Total Training Translation Loss 48.52 
2024-02-02 22:37:34,815 EPOCH 107
2024-02-02 22:37:34,928 [Epoch: 107 Step: 00014100] Batch Recognition Loss:   0.000588 => Gls Tokens per Sec:     1429 || Batch Translation Loss:   0.133532 => Txt Tokens per Sec:     4411 || Lr: 0.000100
2024-02-02 22:37:40,564 [Epoch: 107 Step: 00014200] Batch Recognition Loss:   0.000353 => Gls Tokens per Sec:     1420 || Batch Translation Loss:   0.257604 => Txt Tokens per Sec:     3942 || Lr: 0.000100
2024-02-02 22:37:42,234 Epoch 107: Total Training Recognition Loss 0.09  Total Training Translation Loss 45.35 
2024-02-02 22:37:42,234 EPOCH 108
2024-02-02 22:37:46,108 [Epoch: 108 Step: 00014300] Batch Recognition Loss:   0.000599 => Gls Tokens per Sec:     1425 || Batch Translation Loss:   0.143628 => Txt Tokens per Sec:     3943 || Lr: 0.000100
2024-02-02 22:37:49,790 Epoch 108: Total Training Recognition Loss 0.10  Total Training Translation Loss 44.96 
2024-02-02 22:37:49,791 EPOCH 109
2024-02-02 22:37:51,637 [Epoch: 109 Step: 00014400] Batch Recognition Loss:   0.000666 => Gls Tokens per Sec:     1561 || Batch Translation Loss:   0.637014 => Txt Tokens per Sec:     4148 || Lr: 0.000100
2024-02-02 22:37:57,375 Epoch 109: Total Training Recognition Loss 0.10  Total Training Translation Loss 45.63 
2024-02-02 22:37:57,376 EPOCH 110
2024-02-02 22:37:57,548 [Epoch: 110 Step: 00014500] Batch Recognition Loss:   0.000311 => Gls Tokens per Sec:     1403 || Batch Translation Loss:   0.559375 => Txt Tokens per Sec:     3807 || Lr: 0.000100
2024-02-02 22:38:03,206 [Epoch: 110 Step: 00014600] Batch Recognition Loss:   0.000889 => Gls Tokens per Sec:     1414 || Batch Translation Loss:   0.398972 => Txt Tokens per Sec:     3949 || Lr: 0.000100
2024-02-02 22:38:04,870 Epoch 110: Total Training Recognition Loss 0.09  Total Training Translation Loss 46.61 
2024-02-02 22:38:04,870 EPOCH 111
2024-02-02 22:38:08,325 [Epoch: 111 Step: 00014700] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:     1621 || Batch Translation Loss:   0.201230 => Txt Tokens per Sec:     4411 || Lr: 0.000100
2024-02-02 22:38:11,799 Epoch 111: Total Training Recognition Loss 0.10  Total Training Translation Loss 42.28 
2024-02-02 22:38:11,799 EPOCH 112
2024-02-02 22:38:13,864 [Epoch: 112 Step: 00014800] Batch Recognition Loss:   0.000537 => Gls Tokens per Sec:     1435 || Batch Translation Loss:   0.215121 => Txt Tokens per Sec:     3862 || Lr: 0.000100
2024-02-02 22:38:19,445 Epoch 112: Total Training Recognition Loss 0.11  Total Training Translation Loss 38.14 
2024-02-02 22:38:19,445 EPOCH 113
2024-02-02 22:38:19,648 [Epoch: 113 Step: 00014900] Batch Recognition Loss:   0.000368 => Gls Tokens per Sec:     1589 || Batch Translation Loss:   0.543614 => Txt Tokens per Sec:     4638 || Lr: 0.000100
2024-02-02 22:38:25,476 [Epoch: 113 Step: 00015000] Batch Recognition Loss:   0.000690 => Gls Tokens per Sec:     1373 || Batch Translation Loss:   0.261040 => Txt Tokens per Sec:     3834 || Lr: 0.000100
2024-02-02 22:38:27,056 Epoch 113: Total Training Recognition Loss 0.10  Total Training Translation Loss 43.74 
2024-02-02 22:38:27,056 EPOCH 114
2024-02-02 22:38:30,854 [Epoch: 114 Step: 00015100] Batch Recognition Loss:   0.000566 => Gls Tokens per Sec:     1496 || Batch Translation Loss:   0.649573 => Txt Tokens per Sec:     4179 || Lr: 0.000100
2024-02-02 22:38:34,209 Epoch 114: Total Training Recognition Loss 0.09  Total Training Translation Loss 47.74 
2024-02-02 22:38:34,209 EPOCH 115
2024-02-02 22:38:36,212 [Epoch: 115 Step: 00015200] Batch Recognition Loss:   0.000410 => Gls Tokens per Sec:     1519 || Batch Translation Loss:   0.127724 => Txt Tokens per Sec:     4236 || Lr: 0.000100
2024-02-02 22:38:41,618 Epoch 115: Total Training Recognition Loss 0.09  Total Training Translation Loss 44.29 
2024-02-02 22:38:41,618 EPOCH 116
2024-02-02 22:38:41,947 [Epoch: 116 Step: 00015300] Batch Recognition Loss:   0.001264 => Gls Tokens per Sec:     1220 || Batch Translation Loss:   0.142671 => Txt Tokens per Sec:     3579 || Lr: 0.000100
2024-02-02 22:38:47,578 [Epoch: 116 Step: 00015400] Batch Recognition Loss:   0.000737 => Gls Tokens per Sec:     1421 || Batch Translation Loss:   0.756527 => Txt Tokens per Sec:     3935 || Lr: 0.000100
2024-02-02 22:38:49,171 Epoch 116: Total Training Recognition Loss 0.11  Total Training Translation Loss 43.86 
2024-02-02 22:38:49,171 EPOCH 117
2024-02-02 22:38:53,239 [Epoch: 117 Step: 00015500] Batch Recognition Loss:   0.000737 => Gls Tokens per Sec:     1416 || Batch Translation Loss:   0.350166 => Txt Tokens per Sec:     4020 || Lr: 0.000100
2024-02-02 22:38:56,766 Epoch 117: Total Training Recognition Loss 0.09  Total Training Translation Loss 43.36 
2024-02-02 22:38:56,767 EPOCH 118
2024-02-02 22:38:58,866 [Epoch: 118 Step: 00015600] Batch Recognition Loss:   0.000270 => Gls Tokens per Sec:     1487 || Batch Translation Loss:   0.274649 => Txt Tokens per Sec:     4137 || Lr: 0.000100
2024-02-02 22:39:04,058 Epoch 118: Total Training Recognition Loss 0.09  Total Training Translation Loss 50.51 
2024-02-02 22:39:04,059 EPOCH 119
2024-02-02 22:39:04,417 [Epoch: 119 Step: 00015700] Batch Recognition Loss:   0.000709 => Gls Tokens per Sec:     1341 || Batch Translation Loss:   0.349386 => Txt Tokens per Sec:     4025 || Lr: 0.000100
2024-02-02 22:39:10,075 [Epoch: 119 Step: 00015800] Batch Recognition Loss:   0.000337 => Gls Tokens per Sec:     1412 || Batch Translation Loss:   0.647973 => Txt Tokens per Sec:     3932 || Lr: 0.000100
2024-02-02 22:39:11,494 Epoch 119: Total Training Recognition Loss 0.10  Total Training Translation Loss 51.32 
2024-02-02 22:39:11,494 EPOCH 120
2024-02-02 22:39:15,255 [Epoch: 120 Step: 00015900] Batch Recognition Loss:   0.000420 => Gls Tokens per Sec:     1553 || Batch Translation Loss:   0.346462 => Txt Tokens per Sec:     4441 || Lr: 0.000100
2024-02-02 22:39:18,495 Epoch 120: Total Training Recognition Loss 0.08  Total Training Translation Loss 42.17 
2024-02-02 22:39:18,496 EPOCH 121
2024-02-02 22:39:20,670 [Epoch: 121 Step: 00016000] Batch Recognition Loss:   0.000491 => Gls Tokens per Sec:     1473 || Batch Translation Loss:   0.485075 => Txt Tokens per Sec:     4038 || Lr: 0.000100
2024-02-02 22:39:31,572 Validation result at epoch 121, step    16000: duration: 10.9028s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00524	Translation Loss: 91426.57031	PPL: 9405.60645
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.82	(BLEU-1: 11.25,	BLEU-2: 3.68,	BLEU-3: 1.51,	BLEU-4: 0.82)
	CHRF 17.17	ROUGE 9.59
2024-02-02 22:39:31,573 Logging Recognition and Translation Outputs
2024-02-02 22:39:31,573 ========================================================================================================================
2024-02-02 22:39:31,574 Logging Sequence: 147_132.00
2024-02-02 22:39:31,574 	Gloss Reference :	A B+C+D+E
2024-02-02 22:39:31,574 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:39:31,574 	Gloss Alignment :	         
2024-02-02 22:39:31,574 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:39:31,575 	Text Reference  :	i    can    not earlier i  used to     have   fun in   gymnastics
2024-02-02 22:39:31,575 	Text Hypothesis :	they wanted to  focus   on her  mental health and well being     
2024-02-02 22:39:31,575 	Text Alignment  :	S    S      S   S       S  S    S      S      S   S    S         
2024-02-02 22:39:31,575 ========================================================================================================================
2024-02-02 22:39:31,575 Logging Sequence: 116_162.00
2024-02-02 22:39:31,576 	Gloss Reference :	A B+C+D+E
2024-02-02 22:39:31,576 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:39:31,576 	Gloss Alignment :	         
2024-02-02 22:39:31,576 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:39:31,577 	Text Reference  :	turned out     the video was shared on  social media by     a       staff at the ***** ************* hotel     
2024-02-02 22:39:31,577 	Text Hypothesis :	****** towards the ***** 6th world  cup 2022   is    played against arch  of the dubai international tournament
2024-02-02 22:39:31,578 	Text Alignment  :	D      S           D     S   S      S   S      S     S      S       S     S      I     I             S         
2024-02-02 22:39:31,578 ========================================================================================================================
2024-02-02 22:39:31,578 Logging Sequence: 73_79.00
2024-02-02 22:39:31,578 	Gloss Reference :	A B+C+D+E
2024-02-02 22:39:31,578 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:39:31,578 	Gloss Alignment :	         
2024-02-02 22:39:31,578 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:39:31,580 	Text Reference  :	raina resturant has      food from the rich spices of   north india to      the aromatic curries of        south india
2024-02-02 22:39:31,580 	Text Hypothesis :	***** the       cheapest dish on   the **** ****** menu is    raita whereas the ******** ******* costliest dish  is   
2024-02-02 22:39:31,580 	Text Alignment  :	D     S         S        S    S        D    D      S    S     S     S           D        D       S         S     S    
2024-02-02 22:39:31,580 ========================================================================================================================
2024-02-02 22:39:31,580 Logging Sequence: 165_523.00
2024-02-02 22:39:31,580 	Gloss Reference :	A B+C+D+E
2024-02-02 22:39:31,581 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:39:31,581 	Gloss Alignment :	         
2024-02-02 22:39:31,581 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:39:31,582 	Text Reference  :	as he believed that his team might lose if  he  takes off   his batting pads      
2024-02-02 22:39:31,582 	Text Hypothesis :	** ** ******** **** *** **** ***** **** icc did not   agree to  the     tournament
2024-02-02 22:39:31,582 	Text Alignment  :	D  D  D        D    D   D    D     D    S   S   S     S     S   S       S         
2024-02-02 22:39:31,582 ========================================================================================================================
2024-02-02 22:39:31,582 Logging Sequence: 125_72.00
2024-02-02 22:39:31,582 	Gloss Reference :	A B+C+D+E
2024-02-02 22:39:31,582 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:39:31,583 	Gloss Alignment :	         
2024-02-02 22:39:31,583 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:39:31,584 	Text Reference  :	some said the pakistani javelineer had    milicious intentions of tampering with       the javelin out of jealousy
2024-02-02 22:39:31,584 	Text Hypothesis :	**** **** *** ********* ********** neeraj was       flooded    by these     cricketers are javelin *** ** throw   
2024-02-02 22:39:31,584 	Text Alignment  :	D    D    D   D         D          S      S         S          S  S         S          S           D   D  S       
2024-02-02 22:39:31,584 ========================================================================================================================
2024-02-02 22:39:37,208 Epoch 121: Total Training Recognition Loss 0.09  Total Training Translation Loss 31.30 
2024-02-02 22:39:37,208 EPOCH 122
2024-02-02 22:39:37,610 [Epoch: 122 Step: 00016100] Batch Recognition Loss:   0.000527 => Gls Tokens per Sec:     1395 || Batch Translation Loss:   0.126942 => Txt Tokens per Sec:     3615 || Lr: 0.000100
2024-02-02 22:39:43,397 [Epoch: 122 Step: 00016200] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:     1383 || Batch Translation Loss:   0.318634 => Txt Tokens per Sec:     3893 || Lr: 0.000100
2024-02-02 22:39:44,723 Epoch 122: Total Training Recognition Loss 0.10  Total Training Translation Loss 33.32 
2024-02-02 22:39:44,723 EPOCH 123
2024-02-02 22:39:48,681 [Epoch: 123 Step: 00016300] Batch Recognition Loss:   0.000706 => Gls Tokens per Sec:     1496 || Batch Translation Loss:   0.311153 => Txt Tokens per Sec:     4106 || Lr: 0.000100
2024-02-02 22:39:52,007 Epoch 123: Total Training Recognition Loss 0.09  Total Training Translation Loss 33.52 
2024-02-02 22:39:52,007 EPOCH 124
2024-02-02 22:39:54,253 [Epoch: 124 Step: 00016400] Batch Recognition Loss:   0.001377 => Gls Tokens per Sec:     1462 || Batch Translation Loss:   0.317982 => Txt Tokens per Sec:     4018 || Lr: 0.000100
2024-02-02 22:39:59,551 Epoch 124: Total Training Recognition Loss 0.09  Total Training Translation Loss 35.49 
2024-02-02 22:39:59,551 EPOCH 125
2024-02-02 22:40:00,118 [Epoch: 125 Step: 00016500] Batch Recognition Loss:   0.000357 => Gls Tokens per Sec:     1133 || Batch Translation Loss:   0.233674 => Txt Tokens per Sec:     3747 || Lr: 0.000100
2024-02-02 22:40:05,794 [Epoch: 125 Step: 00016600] Batch Recognition Loss:   0.000702 => Gls Tokens per Sec:     1408 || Batch Translation Loss:   0.293800 => Txt Tokens per Sec:     3848 || Lr: 0.000100
2024-02-02 22:40:07,308 Epoch 125: Total Training Recognition Loss 0.10  Total Training Translation Loss 43.90 
2024-02-02 22:40:07,309 EPOCH 126
2024-02-02 22:40:11,627 [Epoch: 126 Step: 00016700] Batch Recognition Loss:   0.000302 => Gls Tokens per Sec:     1390 || Batch Translation Loss:   0.132054 => Txt Tokens per Sec:     3902 || Lr: 0.000100
2024-02-02 22:40:14,722 Epoch 126: Total Training Recognition Loss 0.09  Total Training Translation Loss 39.33 
2024-02-02 22:40:14,722 EPOCH 127
2024-02-02 22:40:17,052 [Epoch: 127 Step: 00016800] Batch Recognition Loss:   0.000502 => Gls Tokens per Sec:     1443 || Batch Translation Loss:   0.272034 => Txt Tokens per Sec:     3988 || Lr: 0.000100
2024-02-02 22:40:22,320 Epoch 127: Total Training Recognition Loss 0.08  Total Training Translation Loss 40.94 
2024-02-02 22:40:22,320 EPOCH 128
2024-02-02 22:40:22,782 [Epoch: 128 Step: 00016900] Batch Recognition Loss:   0.000333 => Gls Tokens per Sec:     1563 || Batch Translation Loss:   0.131413 => Txt Tokens per Sec:     4219 || Lr: 0.000100
2024-02-02 22:40:28,423 [Epoch: 128 Step: 00017000] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     1418 || Batch Translation Loss:   0.212911 => Txt Tokens per Sec:     3965 || Lr: 0.000100
2024-02-02 22:40:29,754 Epoch 128: Total Training Recognition Loss 0.08  Total Training Translation Loss 38.73 
2024-02-02 22:40:29,754 EPOCH 129
2024-02-02 22:40:33,909 [Epoch: 129 Step: 00017100] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     1464 || Batch Translation Loss:   0.241323 => Txt Tokens per Sec:     3974 || Lr: 0.000100
2024-02-02 22:40:37,356 Epoch 129: Total Training Recognition Loss 0.08  Total Training Translation Loss 37.53 
2024-02-02 22:40:37,357 EPOCH 130
2024-02-02 22:40:39,687 [Epoch: 130 Step: 00017200] Batch Recognition Loss:   0.001961 => Gls Tokens per Sec:     1477 || Batch Translation Loss:   0.211155 => Txt Tokens per Sec:     4207 || Lr: 0.000100
2024-02-02 22:40:44,596 Epoch 130: Total Training Recognition Loss 0.08  Total Training Translation Loss 39.47 
2024-02-02 22:40:44,596 EPOCH 131
2024-02-02 22:40:45,356 [Epoch: 131 Step: 00017300] Batch Recognition Loss:   0.003998 => Gls Tokens per Sec:     1054 || Batch Translation Loss:   0.144415 => Txt Tokens per Sec:     3216 || Lr: 0.000100
2024-02-02 22:40:51,058 [Epoch: 131 Step: 00017400] Batch Recognition Loss:   0.000891 => Gls Tokens per Sec:     1403 || Batch Translation Loss:   0.351468 => Txt Tokens per Sec:     3876 || Lr: 0.000100
2024-02-02 22:40:52,555 Epoch 131: Total Training Recognition Loss 0.10  Total Training Translation Loss 39.56 
2024-02-02 22:40:52,555 EPOCH 132
2024-02-02 22:40:56,718 [Epoch: 132 Step: 00017500] Batch Recognition Loss:   0.000766 => Gls Tokens per Sec:     1480 || Batch Translation Loss:   0.673992 => Txt Tokens per Sec:     4137 || Lr: 0.000100
2024-02-02 22:40:59,907 Epoch 132: Total Training Recognition Loss 0.09  Total Training Translation Loss 37.60 
2024-02-02 22:40:59,907 EPOCH 133
2024-02-02 22:41:02,351 [Epoch: 133 Step: 00017600] Batch Recognition Loss:   0.000245 => Gls Tokens per Sec:     1441 || Batch Translation Loss:   0.314444 => Txt Tokens per Sec:     4017 || Lr: 0.000100
2024-02-02 22:41:07,573 Epoch 133: Total Training Recognition Loss 0.09  Total Training Translation Loss 37.36 
2024-02-02 22:41:07,574 EPOCH 134
2024-02-02 22:41:08,273 [Epoch: 134 Step: 00017700] Batch Recognition Loss:   0.000359 => Gls Tokens per Sec:     1261 || Batch Translation Loss:   0.702372 => Txt Tokens per Sec:     3721 || Lr: 0.000100
2024-02-02 22:41:13,683 [Epoch: 134 Step: 00017800] Batch Recognition Loss:   0.000310 => Gls Tokens per Sec:     1479 || Batch Translation Loss:   0.192352 => Txt Tokens per Sec:     4038 || Lr: 0.000100
2024-02-02 22:41:14,985 Epoch 134: Total Training Recognition Loss 0.07  Total Training Translation Loss 36.45 
2024-02-02 22:41:14,986 EPOCH 135
2024-02-02 22:41:19,492 [Epoch: 135 Step: 00017900] Batch Recognition Loss:   0.000334 => Gls Tokens per Sec:     1386 || Batch Translation Loss:   0.578116 => Txt Tokens per Sec:     3892 || Lr: 0.000100
2024-02-02 22:41:22,376 Epoch 135: Total Training Recognition Loss 0.11  Total Training Translation Loss 40.82 
2024-02-02 22:41:22,376 EPOCH 136
2024-02-02 22:41:24,417 [Epoch: 136 Step: 00018000] Batch Recognition Loss:   0.000621 => Gls Tokens per Sec:     1765 || Batch Translation Loss:   0.223387 => Txt Tokens per Sec:     4732 || Lr: 0.000100
2024-02-02 22:41:35,847 Validation result at epoch 136, step    18000: duration: 11.4298s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00259	Translation Loss: 93005.19531	PPL: 11015.23438
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.68	(BLEU-1: 11.55,	BLEU-2: 3.51,	BLEU-3: 1.44,	BLEU-4: 0.68)
	CHRF 17.20	ROUGE 9.93
2024-02-02 22:41:35,848 Logging Recognition and Translation Outputs
2024-02-02 22:41:35,848 ========================================================================================================================
2024-02-02 22:41:35,848 Logging Sequence: 155_119.00
2024-02-02 22:41:35,848 	Gloss Reference :	A B+C+D+E
2024-02-02 22:41:35,848 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:41:35,848 	Gloss Alignment :	         
2024-02-02 22:41:35,849 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:41:35,850 	Text Reference  :	a report  said that the ***** taliban wanted icc   to replace the    afghan  flag with        its own  
2024-02-02 22:41:35,850 	Text Hypothesis :	* however they took the first time    in     qatar to ******* decide whether to   participate in  qatar
2024-02-02 22:41:35,850 	Text Alignment  :	D S       S    S        I     S       S      S        D       S      S       S    S           S   S    
2024-02-02 22:41:35,850 ========================================================================================================================
2024-02-02 22:41:35,851 Logging Sequence: 153_43.00
2024-02-02 22:41:35,851 	Gloss Reference :	A B+C+D+E
2024-02-02 22:41:35,851 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:41:35,851 	Gloss Alignment :	         
2024-02-02 22:41:35,851 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:41:35,852 	Text Reference  :	***** *** these runs were all         because of   hardik pandya and  virat  kohli
2024-02-02 22:41:35,852 	Text Hypothesis :	hence the bcci  is   not  comfortable and     went to     his    wife sakshi dhoni
2024-02-02 22:41:35,852 	Text Alignment  :	I     I   S     S    S    S           S       S    S      S      S    S      S    
2024-02-02 22:41:35,852 ========================================================================================================================
2024-02-02 22:41:35,853 Logging Sequence: 150_35.00
2024-02-02 22:41:35,853 	Gloss Reference :	A B+C+D+E
2024-02-02 22:41:35,853 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:41:35,853 	Gloss Alignment :	         
2024-02-02 22:41:35,853 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:41:35,853 	Text Reference  :	wow india football team is really strong
2024-02-02 22:41:35,854 	Text Hypothesis :	*** ***** ******** **** 1  high   fever 
2024-02-02 22:41:35,854 	Text Alignment  :	D   D     D        D    S  S      S     
2024-02-02 22:41:35,854 ========================================================================================================================
2024-02-02 22:41:35,854 Logging Sequence: 146_154.00
2024-02-02 22:41:35,854 	Gloss Reference :	A B+C+D+E
2024-02-02 22:41:35,854 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:41:35,854 	Gloss Alignment :	         
2024-02-02 22:41:35,854 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:41:35,856 	Text Reference  :	bwf said that   testing protocols have been implemented to *** ensure the     health and **** safety of     all    participants
2024-02-02 22:41:35,856 	Text Hypothesis :	*** the  couple were    infected  by   the  media       to his wife   anushka sharma and show a      number number '˜7'        
2024-02-02 22:41:35,856 	Text Alignment  :	D   S    S      S       S         S    S    S              I   S      S       S          I    S      S      S      S           
2024-02-02 22:41:35,857 ========================================================================================================================
2024-02-02 22:41:35,857 Logging Sequence: 76_79.00
2024-02-02 22:41:35,857 	Gloss Reference :	A B+C+D+E
2024-02-02 22:41:35,857 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:41:35,857 	Gloss Alignment :	         
2024-02-02 22:41:35,857 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:41:35,859 	Text Reference  :	** **** ******** **** *** *** ***** *** ** ********* ***** ** speaking to       ani  csk  ceo kasi viswanathan said   
2024-02-02 22:41:35,859 	Text Hypothesis :	on 13th february 2023 the t20 world cup is currently going on and      pakistan will have to  play against     england
2024-02-02 22:41:35,859 	Text Alignment  :	I  I    I        I    I   I   I     I   I  I         I     I  S        S        S    S    S   S    S           S      
2024-02-02 22:41:35,859 ========================================================================================================================
2024-02-02 22:41:40,995 Epoch 136: Total Training Recognition Loss 0.09  Total Training Translation Loss 33.36 
2024-02-02 22:41:40,996 EPOCH 137
2024-02-02 22:41:41,750 [Epoch: 137 Step: 00018100] Batch Recognition Loss:   0.000593 => Gls Tokens per Sec:     1274 || Batch Translation Loss:   0.265722 => Txt Tokens per Sec:     3371 || Lr: 0.000100
2024-02-02 22:41:47,338 [Epoch: 137 Step: 00018200] Batch Recognition Loss:   0.000234 => Gls Tokens per Sec:     1430 || Batch Translation Loss:   0.149622 => Txt Tokens per Sec:     3989 || Lr: 0.000100
2024-02-02 22:41:48,563 Epoch 137: Total Training Recognition Loss 0.09  Total Training Translation Loss 28.90 
2024-02-02 22:41:48,563 EPOCH 138
2024-02-02 22:41:52,648 [Epoch: 138 Step: 00018300] Batch Recognition Loss:   0.000447 => Gls Tokens per Sec:     1548 || Batch Translation Loss:   0.114194 => Txt Tokens per Sec:     4309 || Lr: 0.000100
2024-02-02 22:41:55,676 Epoch 138: Total Training Recognition Loss 0.07  Total Training Translation Loss 27.60 
2024-02-02 22:41:55,677 EPOCH 139
2024-02-02 22:41:58,307 [Epoch: 139 Step: 00018400] Batch Recognition Loss:   0.000387 => Gls Tokens per Sec:     1400 || Batch Translation Loss:   0.152617 => Txt Tokens per Sec:     3961 || Lr: 0.000100
2024-02-02 22:42:03,450 Epoch 139: Total Training Recognition Loss 0.06  Total Training Translation Loss 30.89 
2024-02-02 22:42:03,451 EPOCH 140
2024-02-02 22:42:04,200 [Epoch: 140 Step: 00018500] Batch Recognition Loss:   0.000403 => Gls Tokens per Sec:     1390 || Batch Translation Loss:   0.061294 => Txt Tokens per Sec:     3733 || Lr: 0.000100
2024-02-02 22:42:10,004 [Epoch: 140 Step: 00018600] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:     1377 || Batch Translation Loss:   0.268853 => Txt Tokens per Sec:     3856 || Lr: 0.000100
2024-02-02 22:42:11,139 Epoch 140: Total Training Recognition Loss 0.09  Total Training Translation Loss 39.07 
2024-02-02 22:42:11,140 EPOCH 141
2024-02-02 22:42:15,651 [Epoch: 141 Step: 00018700] Batch Recognition Loss:   0.000888 => Gls Tokens per Sec:     1419 || Batch Translation Loss:   0.281012 => Txt Tokens per Sec:     3931 || Lr: 0.000100
2024-02-02 22:42:18,794 Epoch 141: Total Training Recognition Loss 0.10  Total Training Translation Loss 37.88 
2024-02-02 22:42:18,794 EPOCH 142
2024-02-02 22:42:21,222 [Epoch: 142 Step: 00018800] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:     1550 || Batch Translation Loss:   0.212523 => Txt Tokens per Sec:     4489 || Lr: 0.000100
2024-02-02 22:42:25,969 Epoch 142: Total Training Recognition Loss 0.09  Total Training Translation Loss 47.13 
2024-02-02 22:42:25,969 EPOCH 143
2024-02-02 22:42:26,749 [Epoch: 143 Step: 00018900] Batch Recognition Loss:   0.000473 => Gls Tokens per Sec:     1437 || Batch Translation Loss:   0.197503 => Txt Tokens per Sec:     3927 || Lr: 0.000100
2024-02-02 22:42:32,546 [Epoch: 143 Step: 00019000] Batch Recognition Loss:   0.001477 => Gls Tokens per Sec:     1381 || Batch Translation Loss:   0.207819 => Txt Tokens per Sec:     3861 || Lr: 0.000100
2024-02-02 22:42:33,573 Epoch 143: Total Training Recognition Loss 0.08  Total Training Translation Loss 45.69 
2024-02-02 22:42:33,573 EPOCH 144
2024-02-02 22:42:37,883 [Epoch: 144 Step: 00019100] Batch Recognition Loss:   0.000336 => Gls Tokens per Sec:     1504 || Batch Translation Loss:   0.231726 => Txt Tokens per Sec:     4205 || Lr: 0.000100
2024-02-02 22:42:40,690 Epoch 144: Total Training Recognition Loss 0.06  Total Training Translation Loss 34.63 
2024-02-02 22:42:40,691 EPOCH 145
2024-02-02 22:42:43,326 [Epoch: 145 Step: 00019200] Batch Recognition Loss:   0.000108 => Gls Tokens per Sec:     1458 || Batch Translation Loss:   0.112388 => Txt Tokens per Sec:     3928 || Lr: 0.000100
2024-02-02 22:42:48,127 Epoch 145: Total Training Recognition Loss 0.06  Total Training Translation Loss 29.00 
2024-02-02 22:42:48,127 EPOCH 146
2024-02-02 22:42:48,904 [Epoch: 146 Step: 00019300] Batch Recognition Loss:   0.000588 => Gls Tokens per Sec:     1546 || Batch Translation Loss:   0.070250 => Txt Tokens per Sec:     4158 || Lr: 0.000100
2024-02-02 22:42:54,672 [Epoch: 146 Step: 00019400] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     1385 || Batch Translation Loss:   0.124739 => Txt Tokens per Sec:     3870 || Lr: 0.000100
2024-02-02 22:42:55,654 Epoch 146: Total Training Recognition Loss 0.07  Total Training Translation Loss 28.96 
2024-02-02 22:42:55,654 EPOCH 147
2024-02-02 22:43:00,200 [Epoch: 147 Step: 00019500] Batch Recognition Loss:   0.001824 => Gls Tokens per Sec:     1444 || Batch Translation Loss:   0.054235 => Txt Tokens per Sec:     4029 || Lr: 0.000100
2024-02-02 22:43:02,951 Epoch 147: Total Training Recognition Loss 0.06  Total Training Translation Loss 31.55 
2024-02-02 22:43:02,951 EPOCH 148
2024-02-02 22:43:05,347 [Epoch: 148 Step: 00019600] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:     1636 || Batch Translation Loss:   0.054957 => Txt Tokens per Sec:     4514 || Lr: 0.000100
2024-02-02 22:43:10,181 Epoch 148: Total Training Recognition Loss 0.06  Total Training Translation Loss 28.20 
2024-02-02 22:43:10,182 EPOCH 149
2024-02-02 22:43:11,023 [Epoch: 149 Step: 00019700] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:     1524 || Batch Translation Loss:   0.055555 => Txt Tokens per Sec:     4051 || Lr: 0.000100
2024-02-02 22:43:16,855 [Epoch: 149 Step: 00019800] Batch Recognition Loss:   0.000390 => Gls Tokens per Sec:     1372 || Batch Translation Loss:   0.428476 => Txt Tokens per Sec:     3840 || Lr: 0.000100
2024-02-02 22:43:17,864 Epoch 149: Total Training Recognition Loss 0.05  Total Training Translation Loss 27.06 
2024-02-02 22:43:17,864 EPOCH 150
2024-02-02 22:43:22,693 [Epoch: 150 Step: 00019900] Batch Recognition Loss:   0.001323 => Gls Tokens per Sec:     1376 || Batch Translation Loss:   0.297628 => Txt Tokens per Sec:     3838 || Lr: 0.000100
2024-02-02 22:43:25,431 Epoch 150: Total Training Recognition Loss 0.07  Total Training Translation Loss 36.80 
2024-02-02 22:43:25,431 EPOCH 151
2024-02-02 22:43:28,342 [Epoch: 151 Step: 00020000] Batch Recognition Loss:   0.000528 => Gls Tokens per Sec:     1375 || Batch Translation Loss:   0.077349 => Txt Tokens per Sec:     3821 || Lr: 0.000100
2024-02-02 22:43:39,139 Validation result at epoch 151, step    20000: duration: 10.7979s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00385	Translation Loss: 94429.38281	PPL: 12702.49219
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.71	(BLEU-1: 10.58,	BLEU-2: 3.07,	BLEU-3: 1.22,	BLEU-4: 0.71)
	CHRF 16.48	ROUGE 9.24
2024-02-02 22:43:39,140 Logging Recognition and Translation Outputs
2024-02-02 22:43:39,140 ========================================================================================================================
2024-02-02 22:43:39,140 Logging Sequence: 174_121.00
2024-02-02 22:43:39,141 	Gloss Reference :	A B+C+D+E
2024-02-02 22:43:39,141 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:43:39,141 	Gloss Alignment :	         
2024-02-02 22:43:39,141 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:43:39,142 	Text Reference  :	there was a strong competition and  a    difficult auction for  the  5  franchise owners
2024-02-02 22:43:39,142 	Text Hypothesis :	***** *** * ****** *********** they have sent      its     deaf girl on social    media 
2024-02-02 22:43:39,142 	Text Alignment  :	D     D   D D      D           S    S    S         S       S    S    S  S         S     
2024-02-02 22:43:39,142 ========================================================================================================================
2024-02-02 22:43:39,143 Logging Sequence: 170_24.00
2024-02-02 22:43:39,143 	Gloss Reference :	A B+C+D+E
2024-02-02 22:43:39,143 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:43:39,143 	Gloss Alignment :	         
2024-02-02 22:43:39,143 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:43:39,143 	Text Reference  :	let me tell you about it
2024-02-02 22:43:39,144 	Text Hypothesis :	let me tell you about it
2024-02-02 22:43:39,144 	Text Alignment  :	                        
2024-02-02 22:43:39,144 ========================================================================================================================
2024-02-02 22:43:39,144 Logging Sequence: 73_79.00
2024-02-02 22:43:39,144 	Gloss Reference :	A B+C+D+E
2024-02-02 22:43:39,144 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:43:39,144 	Gloss Alignment :	         
2024-02-02 22:43:39,145 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:43:39,146 	Text Reference  :	raina resturant has      food from the rich spices of   north india to      the aromatic curries of        south india
2024-02-02 22:43:39,146 	Text Hypothesis :	***** the       cheapest dish on   the **** ****** menu is    raita whereas the ******** ******* costliest dish  is   
2024-02-02 22:43:39,146 	Text Alignment  :	D     S         S        S    S        D    D      S    S     S     S           D        D       S         S     S    
2024-02-02 22:43:39,146 ========================================================================================================================
2024-02-02 22:43:39,147 Logging Sequence: 140_2.00
2024-02-02 22:43:39,147 	Gloss Reference :	A B+C+D+E
2024-02-02 22:43:39,147 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:43:39,147 	Gloss Alignment :	         
2024-02-02 22:43:39,147 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:43:39,148 	Text Reference  :	**** indian batsman-wicket keeper    rishabh pant has outstanding skills in cricket
2024-02-02 22:43:39,148 	Text Hypothesis :	when csk    has            announced the     bag  can playing     such   as well   
2024-02-02 22:43:39,148 	Text Alignment  :	I    S      S              S         S       S    S   S           S      S  S      
2024-02-02 22:43:39,148 ========================================================================================================================
2024-02-02 22:43:39,148 Logging Sequence: 81_470.00
2024-02-02 22:43:39,149 	Gloss Reference :	A B+C+D+E
2024-02-02 22:43:39,149 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:43:39,149 	Gloss Alignment :	         
2024-02-02 22:43:39,149 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:43:39,150 	Text Reference  :	*** or  you     don't know if    you do   let us   know in   the comments
2024-02-02 22:43:39,150 	Text Hypothesis :	and the supreme court then moved to  keep the same or   into a   lot     
2024-02-02 22:43:39,150 	Text Alignment  :	I   S   S       S     S    S     S   S    S   S    S    S    S   S       
2024-02-02 22:43:39,151 ========================================================================================================================
2024-02-02 22:43:44,041 Epoch 151: Total Training Recognition Loss 0.07  Total Training Translation Loss 35.64 
2024-02-02 22:43:44,041 EPOCH 152
2024-02-02 22:43:44,995 [Epoch: 152 Step: 00020100] Batch Recognition Loss:   0.000515 => Gls Tokens per Sec:     1427 || Batch Translation Loss:   0.601018 => Txt Tokens per Sec:     3823 || Lr: 0.000100
2024-02-02 22:43:50,840 [Epoch: 152 Step: 00020200] Batch Recognition Loss:   0.000623 => Gls Tokens per Sec:     1367 || Batch Translation Loss:   0.222377 => Txt Tokens per Sec:     3820 || Lr: 0.000100
2024-02-02 22:43:51,745 Epoch 152: Total Training Recognition Loss 0.11  Total Training Translation Loss 34.03 
2024-02-02 22:43:51,746 EPOCH 153
2024-02-02 22:43:56,750 [Epoch: 153 Step: 00020300] Batch Recognition Loss:   0.001659 => Gls Tokens per Sec:     1343 || Batch Translation Loss:   0.083040 => Txt Tokens per Sec:     3778 || Lr: 0.000100
2024-02-02 22:43:59,483 Epoch 153: Total Training Recognition Loss 0.11  Total Training Translation Loss 29.72 
2024-02-02 22:43:59,483 EPOCH 154
2024-02-02 22:44:02,094 [Epoch: 154 Step: 00020400] Batch Recognition Loss:   0.000459 => Gls Tokens per Sec:     1563 || Batch Translation Loss:   0.080435 => Txt Tokens per Sec:     4137 || Lr: 0.000100
2024-02-02 22:44:06,732 Epoch 154: Total Training Recognition Loss 0.07  Total Training Translation Loss 33.40 
2024-02-02 22:44:06,732 EPOCH 155
2024-02-02 22:44:07,762 [Epoch: 155 Step: 00020500] Batch Recognition Loss:   0.000280 => Gls Tokens per Sec:     1400 || Batch Translation Loss:   0.111717 => Txt Tokens per Sec:     3948 || Lr: 0.000100
2024-02-02 22:44:13,609 [Epoch: 155 Step: 00020600] Batch Recognition Loss:   0.000585 => Gls Tokens per Sec:     1368 || Batch Translation Loss:   0.272172 => Txt Tokens per Sec:     3790 || Lr: 0.000100
2024-02-02 22:44:14,489 Epoch 155: Total Training Recognition Loss 0.07  Total Training Translation Loss 30.78 
2024-02-02 22:44:14,489 EPOCH 156
2024-02-02 22:44:19,198 [Epoch: 156 Step: 00020700] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:     1445 || Batch Translation Loss:   0.175886 => Txt Tokens per Sec:     3998 || Lr: 0.000100
2024-02-02 22:44:21,902 Epoch 156: Total Training Recognition Loss 0.07  Total Training Translation Loss 28.32 
2024-02-02 22:44:21,903 EPOCH 157
2024-02-02 22:44:24,782 [Epoch: 157 Step: 00020800] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:     1445 || Batch Translation Loss:   0.495573 => Txt Tokens per Sec:     4069 || Lr: 0.000100
2024-02-02 22:44:29,461 Epoch 157: Total Training Recognition Loss 0.07  Total Training Translation Loss 28.33 
2024-02-02 22:44:29,462 EPOCH 158
2024-02-02 22:44:30,566 [Epoch: 158 Step: 00020900] Batch Recognition Loss:   0.000235 => Gls Tokens per Sec:     1378 || Batch Translation Loss:   0.120218 => Txt Tokens per Sec:     3736 || Lr: 0.000100
2024-02-02 22:44:36,202 [Epoch: 158 Step: 00021000] Batch Recognition Loss:   0.000812 => Gls Tokens per Sec:     1420 || Batch Translation Loss:   0.251625 => Txt Tokens per Sec:     3995 || Lr: 0.000100
2024-02-02 22:44:37,057 Epoch 158: Total Training Recognition Loss 0.06  Total Training Translation Loss 30.03 
2024-02-02 22:44:37,057 EPOCH 159
2024-02-02 22:44:42,059 [Epoch: 159 Step: 00021100] Batch Recognition Loss:   0.000513 => Gls Tokens per Sec:     1376 || Batch Translation Loss:   0.145740 => Txt Tokens per Sec:     3804 || Lr: 0.000100
2024-02-02 22:44:44,646 Epoch 159: Total Training Recognition Loss 0.08  Total Training Translation Loss 33.54 
2024-02-02 22:44:44,646 EPOCH 160
2024-02-02 22:44:47,560 [Epoch: 160 Step: 00021200] Batch Recognition Loss:   0.000284 => Gls Tokens per Sec:     1456 || Batch Translation Loss:   0.122433 => Txt Tokens per Sec:     3946 || Lr: 0.000100
2024-02-02 22:44:52,335 Epoch 160: Total Training Recognition Loss 0.07  Total Training Translation Loss 27.93 
2024-02-02 22:44:52,336 EPOCH 161
2024-02-02 22:44:53,434 [Epoch: 161 Step: 00021300] Batch Recognition Loss:   0.000253 => Gls Tokens per Sec:     1459 || Batch Translation Loss:   0.210732 => Txt Tokens per Sec:     3864 || Lr: 0.000100
2024-02-02 22:44:59,074 [Epoch: 161 Step: 00021400] Batch Recognition Loss:   0.001178 => Gls Tokens per Sec:     1417 || Batch Translation Loss:   0.274674 => Txt Tokens per Sec:     3944 || Lr: 0.000100
2024-02-02 22:44:59,922 Epoch 161: Total Training Recognition Loss 0.07  Total Training Translation Loss 25.36 
2024-02-02 22:44:59,922 EPOCH 162
2024-02-02 22:45:04,847 [Epoch: 162 Step: 00021500] Batch Recognition Loss:   0.000684 => Gls Tokens per Sec:     1414 || Batch Translation Loss:   0.121156 => Txt Tokens per Sec:     3938 || Lr: 0.000100
2024-02-02 22:45:07,643 Epoch 162: Total Training Recognition Loss 0.08  Total Training Translation Loss 23.15 
2024-02-02 22:45:07,643 EPOCH 163
2024-02-02 22:45:10,437 [Epoch: 163 Step: 00021600] Batch Recognition Loss:   0.000561 => Gls Tokens per Sec:     1547 || Batch Translation Loss:   0.082657 => Txt Tokens per Sec:     4427 || Lr: 0.000100
2024-02-02 22:45:14,793 Epoch 163: Total Training Recognition Loss 0.07  Total Training Translation Loss 23.32 
2024-02-02 22:45:14,794 EPOCH 164
2024-02-02 22:45:16,068 [Epoch: 164 Step: 00021700] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     1319 || Batch Translation Loss:   0.243149 => Txt Tokens per Sec:     3657 || Lr: 0.000100
2024-02-02 22:45:21,739 [Epoch: 164 Step: 00021800] Batch Recognition Loss:   0.001933 => Gls Tokens per Sec:     1411 || Batch Translation Loss:   0.134129 => Txt Tokens per Sec:     3870 || Lr: 0.000100
2024-02-02 22:45:22,523 Epoch 164: Total Training Recognition Loss 0.06  Total Training Translation Loss 30.64 
2024-02-02 22:45:22,523 EPOCH 165
2024-02-02 22:45:27,471 [Epoch: 165 Step: 00021900] Batch Recognition Loss:   0.001133 => Gls Tokens per Sec:     1423 || Batch Translation Loss:   0.416089 => Txt Tokens per Sec:     3861 || Lr: 0.000100
2024-02-02 22:45:30,073 Epoch 165: Total Training Recognition Loss 0.07  Total Training Translation Loss 30.60 
2024-02-02 22:45:30,073 EPOCH 166
2024-02-02 22:45:33,310 [Epoch: 166 Step: 00022000] Batch Recognition Loss:   0.001109 => Gls Tokens per Sec:     1360 || Batch Translation Loss:   0.102352 => Txt Tokens per Sec:     3775 || Lr: 0.000100
2024-02-02 22:45:44,137 Validation result at epoch 166, step    22000: duration: 10.8277s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00344	Translation Loss: 96135.69531	PPL: 15067.63086
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.77	(BLEU-1: 10.45,	BLEU-2: 3.42,	BLEU-3: 1.52,	BLEU-4: 0.77)
	CHRF 16.15	ROUGE 9.31
2024-02-02 22:45:44,138 Logging Recognition and Translation Outputs
2024-02-02 22:45:44,138 ========================================================================================================================
2024-02-02 22:45:44,139 Logging Sequence: 146_56.00
2024-02-02 22:45:44,139 	Gloss Reference :	A B+C+D+E
2024-02-02 22:45:44,139 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:45:44,139 	Gloss Alignment :	         
2024-02-02 22:45:44,139 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:45:44,141 	Text Reference  :	when the players go       back to      the    hotel as   per rules all of them have to undergo rtpcr test for covid-19 everyday
2024-02-02 22:45:44,141 	Text Hypothesis :	**** he  is      stepping with anushka sharma and   only he  got   out of **** **** ** ******* ***** **** *** his      bat     
2024-02-02 22:45:44,141 	Text Alignment  :	D    S   S       S        S    S       S      S     S    S   S     S      D    D    D  D       D     D    D   S        S       
2024-02-02 22:45:44,142 ========================================================================================================================
2024-02-02 22:45:44,142 Logging Sequence: 118_338.00
2024-02-02 22:45:44,142 	Gloss Reference :	A B+C+D+E
2024-02-02 22:45:44,142 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:45:44,142 	Gloss Alignment :	         
2024-02-02 22:45:44,142 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:45:44,143 	Text Reference  :	** **** *** ***** **** *** this   is   why even messi   wore it   
2024-02-02 22:45:44,143 	Text Hypothesis :	as they are aware that the golden ball for the  winning the  match
2024-02-02 22:45:44,143 	Text Alignment  :	I  I    I   I     I    I   S      S    S   S    S       S    S    
2024-02-02 22:45:44,143 ========================================================================================================================
2024-02-02 22:45:44,143 Logging Sequence: 66_61.00
2024-02-02 22:45:44,144 	Gloss Reference :	A B+C+D+E
2024-02-02 22:45:44,144 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:45:44,144 	Gloss Alignment :	         
2024-02-02 22:45:44,144 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:45:44,145 	Text Reference  :	instead of returning back to  his homeland because  of his injury 
2024-02-02 22:45:44,145 	Text Hypothesis :	******* ** ********* **** and she was      consoled by his brother
2024-02-02 22:45:44,145 	Text Alignment  :	D       D  D         D    S   S   S        S        S      S      
2024-02-02 22:45:44,145 ========================================================================================================================
2024-02-02 22:45:44,145 Logging Sequence: 81_278.00
2024-02-02 22:45:44,145 	Gloss Reference :	A B+C+D+E
2024-02-02 22:45:44,145 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:45:44,145 	Gloss Alignment :	         
2024-02-02 22:45:44,146 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:45:44,148 	Text Reference  :	of this amrapali group paid rs 3570 crore the remaining rs 652    crore was paid by   amrapali    sapphire developers a    subsidiary of    amrapali group     
2024-02-02 22:45:44,148 	Text Hypothesis :	** **** ******** ***** **** ** **** ***** *** ********* a  person told  you can  find information on       29th       july 2023       after the      semi-final
2024-02-02 22:45:44,148 	Text Alignment  :	D  D    D        D     D    D  D    D     D   D         S  S      S     S   S    S    S           S        S          S    S          S     S        S         
2024-02-02 22:45:44,148 ========================================================================================================================
2024-02-02 22:45:44,148 Logging Sequence: 162_125.00
2024-02-02 22:45:44,148 	Gloss Reference :	A B+C+D+E
2024-02-02 22:45:44,148 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:45:44,148 	Gloss Alignment :	         
2024-02-02 22:45:44,149 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:45:44,149 	Text Reference  :	in response to this kohli received many hate comments on  social    media
2024-02-02 22:45:44,149 	Text Hypothesis :	** ******** as per  a     hard     to   have beaten   the ahmedabad team 
2024-02-02 22:45:44,150 	Text Alignment  :	D  D        S  S    S     S        S    S    S        S   S         S    
2024-02-02 22:45:44,150 ========================================================================================================================
2024-02-02 22:45:48,498 Epoch 166: Total Training Recognition Loss 0.08  Total Training Translation Loss 28.34 
2024-02-02 22:45:48,498 EPOCH 167
2024-02-02 22:45:49,905 [Epoch: 167 Step: 00022100] Batch Recognition Loss:   0.000377 => Gls Tokens per Sec:     1252 || Batch Translation Loss:   0.319130 => Txt Tokens per Sec:     3435 || Lr: 0.000100
2024-02-02 22:45:55,644 [Epoch: 167 Step: 00022200] Batch Recognition Loss:   0.000307 => Gls Tokens per Sec:     1392 || Batch Translation Loss:   0.096035 => Txt Tokens per Sec:     3863 || Lr: 0.000100
2024-02-02 22:45:56,265 Epoch 167: Total Training Recognition Loss 0.06  Total Training Translation Loss 31.07 
2024-02-02 22:45:56,266 EPOCH 168
2024-02-02 22:46:01,201 [Epoch: 168 Step: 00022300] Batch Recognition Loss:   0.000463 => Gls Tokens per Sec:     1443 || Batch Translation Loss:   0.204972 => Txt Tokens per Sec:     3928 || Lr: 0.000100
2024-02-02 22:46:03,926 Epoch 168: Total Training Recognition Loss 0.08  Total Training Translation Loss 30.05 
2024-02-02 22:46:03,926 EPOCH 169
2024-02-02 22:46:07,096 [Epoch: 169 Step: 00022400] Batch Recognition Loss:   0.000335 => Gls Tokens per Sec:     1414 || Batch Translation Loss:   0.065460 => Txt Tokens per Sec:     4048 || Lr: 0.000100
2024-02-02 22:46:11,293 Epoch 169: Total Training Recognition Loss 0.08  Total Training Translation Loss 29.33 
2024-02-02 22:46:11,294 EPOCH 170
2024-02-02 22:46:12,697 [Epoch: 170 Step: 00022500] Batch Recognition Loss:   0.000354 => Gls Tokens per Sec:     1312 || Batch Translation Loss:   0.227936 => Txt Tokens per Sec:     3486 || Lr: 0.000100
2024-02-02 22:46:18,779 [Epoch: 170 Step: 00022600] Batch Recognition Loss:   0.000444 => Gls Tokens per Sec:     1314 || Batch Translation Loss:   0.315054 => Txt Tokens per Sec:     3684 || Lr: 0.000100
2024-02-02 22:46:19,319 Epoch 170: Total Training Recognition Loss 0.06  Total Training Translation Loss 28.07 
2024-02-02 22:46:19,319 EPOCH 171
2024-02-02 22:46:24,287 [Epoch: 171 Step: 00022700] Batch Recognition Loss:   0.000581 => Gls Tokens per Sec:     1449 || Batch Translation Loss:   0.324335 => Txt Tokens per Sec:     4063 || Lr: 0.000100
2024-02-02 22:46:26,571 Epoch 171: Total Training Recognition Loss 0.09  Total Training Translation Loss 40.28 
2024-02-02 22:46:26,571 EPOCH 172
2024-02-02 22:46:29,518 [Epoch: 172 Step: 00022800] Batch Recognition Loss:   0.000288 => Gls Tokens per Sec:     1548 || Batch Translation Loss:   0.053577 => Txt Tokens per Sec:     4204 || Lr: 0.000100
2024-02-02 22:46:34,064 Epoch 172: Total Training Recognition Loss 0.08  Total Training Translation Loss 36.44 
2024-02-02 22:46:34,065 EPOCH 173
2024-02-02 22:46:35,357 [Epoch: 173 Step: 00022900] Batch Recognition Loss:   0.000711 => Gls Tokens per Sec:     1488 || Batch Translation Loss:   0.076344 => Txt Tokens per Sec:     4301 || Lr: 0.000100
2024-02-02 22:46:41,080 [Epoch: 173 Step: 00023000] Batch Recognition Loss:   0.001152 => Gls Tokens per Sec:     1396 || Batch Translation Loss:   0.363669 => Txt Tokens per Sec:     3817 || Lr: 0.000100
2024-02-02 22:46:41,646 Epoch 173: Total Training Recognition Loss 0.07  Total Training Translation Loss 27.94 
2024-02-02 22:46:41,647 EPOCH 174
2024-02-02 22:46:46,534 [Epoch: 174 Step: 00023100] Batch Recognition Loss:   0.001192 => Gls Tokens per Sec:     1490 || Batch Translation Loss:   0.182957 => Txt Tokens per Sec:     4097 || Lr: 0.000100
2024-02-02 22:46:48,874 Epoch 174: Total Training Recognition Loss 0.07  Total Training Translation Loss 26.00 
2024-02-02 22:46:48,875 EPOCH 175
2024-02-02 22:46:52,125 [Epoch: 175 Step: 00023200] Batch Recognition Loss:   0.000425 => Gls Tokens per Sec:     1429 || Batch Translation Loss:   0.205998 => Txt Tokens per Sec:     4003 || Lr: 0.000100
2024-02-02 22:46:56,253 Epoch 175: Total Training Recognition Loss 0.07  Total Training Translation Loss 28.76 
2024-02-02 22:46:56,253 EPOCH 176
2024-02-02 22:46:57,677 [Epoch: 176 Step: 00023300] Batch Recognition Loss:   0.000219 => Gls Tokens per Sec:     1405 || Batch Translation Loss:   0.166609 => Txt Tokens per Sec:     3775 || Lr: 0.000100
2024-02-02 22:47:03,540 [Epoch: 176 Step: 00023400] Batch Recognition Loss:   0.000419 => Gls Tokens per Sec:     1363 || Batch Translation Loss:   0.298078 => Txt Tokens per Sec:     3854 || Lr: 0.000100
2024-02-02 22:47:03,971 Epoch 176: Total Training Recognition Loss 0.07  Total Training Translation Loss 24.08 
2024-02-02 22:47:03,971 EPOCH 177
2024-02-02 22:47:09,305 [Epoch: 177 Step: 00023500] Batch Recognition Loss:   0.000256 => Gls Tokens per Sec:     1380 || Batch Translation Loss:   0.052215 => Txt Tokens per Sec:     3801 || Lr: 0.000100
2024-02-02 22:47:11,629 Epoch 177: Total Training Recognition Loss 0.06  Total Training Translation Loss 24.66 
2024-02-02 22:47:11,629 EPOCH 178
2024-02-02 22:47:14,598 [Epoch: 178 Step: 00023600] Batch Recognition Loss:   0.000521 => Gls Tokens per Sec:     1590 || Batch Translation Loss:   0.078253 => Txt Tokens per Sec:     4457 || Lr: 0.000100
2024-02-02 22:47:18,456 Epoch 178: Total Training Recognition Loss 0.07  Total Training Translation Loss 25.20 
2024-02-02 22:47:18,456 EPOCH 179
2024-02-02 22:47:19,788 [Epoch: 179 Step: 00023700] Batch Recognition Loss:   0.000528 => Gls Tokens per Sec:     1565 || Batch Translation Loss:   0.057896 => Txt Tokens per Sec:     4195 || Lr: 0.000100
2024-02-02 22:47:25,805 [Epoch: 179 Step: 00023800] Batch Recognition Loss:   0.000329 => Gls Tokens per Sec:     1328 || Batch Translation Loss:   0.053842 => Txt Tokens per Sec:     3705 || Lr: 0.000100
2024-02-02 22:47:26,242 Epoch 179: Total Training Recognition Loss 0.09  Total Training Translation Loss 25.72 
2024-02-02 22:47:26,243 EPOCH 180
2024-02-02 22:47:31,585 [Epoch: 180 Step: 00023900] Batch Recognition Loss:   0.000192 => Gls Tokens per Sec:     1393 || Batch Translation Loss:   0.176387 => Txt Tokens per Sec:     3871 || Lr: 0.000100
2024-02-02 22:47:33,869 Epoch 180: Total Training Recognition Loss 0.08  Total Training Translation Loss 29.82 
2024-02-02 22:47:33,869 EPOCH 181
2024-02-02 22:47:37,420 [Epoch: 181 Step: 00024000] Batch Recognition Loss:   0.000361 => Gls Tokens per Sec:     1352 || Batch Translation Loss:   0.336315 => Txt Tokens per Sec:     3767 || Lr: 0.000100
2024-02-02 22:47:47,819 Validation result at epoch 181, step    24000: duration: 10.3970s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00327	Translation Loss: 95082.90625	PPL: 13560.96582
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.70	(BLEU-1: 10.89,	BLEU-2: 3.37,	BLEU-3: 1.33,	BLEU-4: 0.70)
	CHRF 16.74	ROUGE 9.60
2024-02-02 22:47:47,820 Logging Recognition and Translation Outputs
2024-02-02 22:47:47,820 ========================================================================================================================
2024-02-02 22:47:47,820 Logging Sequence: 169_165.00
2024-02-02 22:47:47,820 	Gloss Reference :	A B+C+D+E
2024-02-02 22:47:47,820 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:47:47,820 	Gloss Alignment :	         
2024-02-02 22:47:47,821 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:47:47,822 	Text Reference  :	**** the      indian government was  outraged by     the **** incident and these     changes were    undone   by   wikipedia
2024-02-02 22:47:47,822 	Text Hypothesis :	they diverted this   money      they have     booked the same time     and australia new     zealand directly full minutes  
2024-02-02 22:47:47,822 	Text Alignment  :	I    S        S      S          S    S        S          I    S            S         S       S       S        S    S        
2024-02-02 22:47:47,823 ========================================================================================================================
2024-02-02 22:47:47,823 Logging Sequence: 175_60.00
2024-02-02 22:47:47,823 	Gloss Reference :	A B+C+D+E
2024-02-02 22:47:47,823 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:47:47,823 	Gloss Alignment :	         
2024-02-02 22:47:47,823 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:47:47,824 	Text Reference  :	that is how india bagged 9    medals in      the  youth tournament
2024-02-02 22:47:47,824 	Text Hypothesis :	**** ** *** ***** ****** this upset  gambhir have been  made      
2024-02-02 22:47:47,824 	Text Alignment  :	D    D  D   D     D      S    S      S       S    S     S         
2024-02-02 22:47:47,824 ========================================================================================================================
2024-02-02 22:47:47,824 Logging Sequence: 61_255.00
2024-02-02 22:47:47,824 	Gloss Reference :	A B+C+D+E
2024-02-02 22:47:47,824 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:47:47,825 	Gloss Alignment :	         
2024-02-02 22:47:47,825 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:47:47,825 	Text Reference  :	in 2011 we  decided to     marry    and     informed our   families     
2024-02-02 22:47:47,826 	Text Hypothesis :	** it   was a       sudden decision without any      prior communication
2024-02-02 22:47:47,826 	Text Alignment  :	D  S    S   S       S      S        S       S        S     S            
2024-02-02 22:47:47,826 ========================================================================================================================
2024-02-02 22:47:47,826 Logging Sequence: 173_39.00
2024-02-02 22:47:47,826 	Gloss Reference :	A B+C+D+E
2024-02-02 22:47:47,826 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:47:47,826 	Gloss Alignment :	         
2024-02-02 22:47:47,826 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:47:47,827 	Text Reference  :	********* kohli  will step   down as     india' captain
2024-02-02 22:47:47,827 	Text Hypothesis :	rajasthan royals ben  stokes were broken his    finger 
2024-02-02 22:47:47,827 	Text Alignment  :	I         S      S    S      S    S      S      S      
2024-02-02 22:47:47,827 ========================================================================================================================
2024-02-02 22:47:47,827 Logging Sequence: 172_82.00
2024-02-02 22:47:47,828 	Gloss Reference :	A B+C+D+E
2024-02-02 22:47:47,828 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:47:47,828 	Gloss Alignment :	         
2024-02-02 22:47:47,828 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:47:47,829 	Text Reference  :	you all know that the toss was about to start at   700 pm   but it  started raining at       around 630 pm    
2024-02-02 22:47:47,830 	Text Hypothesis :	*** *** **** **** *** **** *** ***** ** she   said 'i  hope one day she     is      selected for    the finals
2024-02-02 22:47:47,830 	Text Alignment  :	D   D   D    D    D   D    D   D     D  S     S    S   S    S   S   S       S       S        S      S   S     
2024-02-02 22:47:47,830 ========================================================================================================================
2024-02-02 22:47:51,733 Epoch 181: Total Training Recognition Loss 0.07  Total Training Translation Loss 28.93 
2024-02-02 22:47:51,734 EPOCH 182
2024-02-02 22:47:53,176 [Epoch: 182 Step: 00024100] Batch Recognition Loss:   0.000731 => Gls Tokens per Sec:     1499 || Batch Translation Loss:   0.799537 => Txt Tokens per Sec:     4120 || Lr: 0.000100
2024-02-02 22:47:58,977 [Epoch: 182 Step: 00024200] Batch Recognition Loss:   0.000311 => Gls Tokens per Sec:     1377 || Batch Translation Loss:   0.063784 => Txt Tokens per Sec:     3845 || Lr: 0.000100
2024-02-02 22:47:59,314 Epoch 182: Total Training Recognition Loss 0.06  Total Training Translation Loss 26.27 
2024-02-02 22:47:59,314 EPOCH 183
2024-02-02 22:48:04,794 [Epoch: 183 Step: 00024300] Batch Recognition Loss:   0.000644 => Gls Tokens per Sec:     1373 || Batch Translation Loss:   0.131494 => Txt Tokens per Sec:     3785 || Lr: 0.000100
2024-02-02 22:48:06,939 Epoch 183: Total Training Recognition Loss 0.07  Total Training Translation Loss 23.24 
2024-02-02 22:48:06,939 EPOCH 184
2024-02-02 22:48:10,277 [Epoch: 184 Step: 00024400] Batch Recognition Loss:   0.000392 => Gls Tokens per Sec:     1462 || Batch Translation Loss:   0.402297 => Txt Tokens per Sec:     4077 || Lr: 0.000100
2024-02-02 22:48:14,334 Epoch 184: Total Training Recognition Loss 0.07  Total Training Translation Loss 27.85 
2024-02-02 22:48:14,335 EPOCH 185
2024-02-02 22:48:15,989 [Epoch: 185 Step: 00024500] Batch Recognition Loss:   0.000539 => Gls Tokens per Sec:     1355 || Batch Translation Loss:   0.113237 => Txt Tokens per Sec:     3988 || Lr: 0.000100
2024-02-02 22:48:21,475 [Epoch: 185 Step: 00024600] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:     1457 || Batch Translation Loss:   0.085233 => Txt Tokens per Sec:     3976 || Lr: 0.000100
2024-02-02 22:48:21,793 Epoch 185: Total Training Recognition Loss 0.07  Total Training Translation Loss 25.75 
2024-02-02 22:48:21,794 EPOCH 186
2024-02-02 22:48:27,386 [Epoch: 186 Step: 00024700] Batch Recognition Loss:   0.000610 => Gls Tokens per Sec:     1359 || Batch Translation Loss:   0.435596 => Txt Tokens per Sec:     3749 || Lr: 0.000100
2024-02-02 22:48:29,550 Epoch 186: Total Training Recognition Loss 0.07  Total Training Translation Loss 34.84 
2024-02-02 22:48:29,550 EPOCH 187
2024-02-02 22:48:33,040 [Epoch: 187 Step: 00024800] Batch Recognition Loss:   0.000374 => Gls Tokens per Sec:     1422 || Batch Translation Loss:   0.233700 => Txt Tokens per Sec:     3939 || Lr: 0.000100
2024-02-02 22:48:36,908 Epoch 187: Total Training Recognition Loss 0.09  Total Training Translation Loss 33.03 
2024-02-02 22:48:36,908 EPOCH 188
2024-02-02 22:48:38,571 [Epoch: 188 Step: 00024900] Batch Recognition Loss:   0.000529 => Gls Tokens per Sec:     1397 || Batch Translation Loss:   0.200533 => Txt Tokens per Sec:     3764 || Lr: 0.000100
2024-02-02 22:48:44,253 [Epoch: 188 Step: 00025000] Batch Recognition Loss:   0.000303 => Gls Tokens per Sec:     1408 || Batch Translation Loss:   0.169536 => Txt Tokens per Sec:     3956 || Lr: 0.000100
2024-02-02 22:48:44,526 Epoch 188: Total Training Recognition Loss 0.07  Total Training Translation Loss 23.44 
2024-02-02 22:48:44,527 EPOCH 189
2024-02-02 22:48:50,124 [Epoch: 189 Step: 00025100] Batch Recognition Loss:   0.000202 => Gls Tokens per Sec:     1373 || Batch Translation Loss:   0.085985 => Txt Tokens per Sec:     3806 || Lr: 0.000100
2024-02-02 22:48:52,232 Epoch 189: Total Training Recognition Loss 0.06  Total Training Translation Loss 21.94 
2024-02-02 22:48:52,233 EPOCH 190
2024-02-02 22:48:55,674 [Epoch: 190 Step: 00025200] Batch Recognition Loss:   0.000456 => Gls Tokens per Sec:     1465 || Batch Translation Loss:   0.050287 => Txt Tokens per Sec:     4135 || Lr: 0.000100
2024-02-02 22:48:59,641 Epoch 190: Total Training Recognition Loss 0.06  Total Training Translation Loss 26.34 
2024-02-02 22:48:59,641 EPOCH 191
2024-02-02 22:49:01,272 [Epoch: 191 Step: 00025300] Batch Recognition Loss:   0.000338 => Gls Tokens per Sec:     1473 || Batch Translation Loss:   0.058313 => Txt Tokens per Sec:     4069 || Lr: 0.000100
2024-02-02 22:49:07,214 [Epoch: 191 Step: 00025400] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     1345 || Batch Translation Loss:   0.262954 => Txt Tokens per Sec:     3729 || Lr: 0.000100
2024-02-02 22:49:07,455 Epoch 191: Total Training Recognition Loss 0.07  Total Training Translation Loss 32.06 
2024-02-02 22:49:07,456 EPOCH 192
2024-02-02 22:49:13,030 [Epoch: 192 Step: 00025500] Batch Recognition Loss:   0.000542 => Gls Tokens per Sec:     1392 || Batch Translation Loss:   0.111113 => Txt Tokens per Sec:     3838 || Lr: 0.000100
2024-02-02 22:49:15,234 Epoch 192: Total Training Recognition Loss 0.06  Total Training Translation Loss 31.05 
2024-02-02 22:49:15,235 EPOCH 193
2024-02-02 22:49:18,930 [Epoch: 193 Step: 00025600] Batch Recognition Loss:   0.000690 => Gls Tokens per Sec:     1386 || Batch Translation Loss:   0.270458 => Txt Tokens per Sec:     3851 || Lr: 0.000100
2024-02-02 22:49:22,557 Epoch 193: Total Training Recognition Loss 0.07  Total Training Translation Loss 26.03 
2024-02-02 22:49:22,558 EPOCH 194
2024-02-02 22:49:24,139 [Epoch: 194 Step: 00025700] Batch Recognition Loss:   0.000389 => Gls Tokens per Sec:     1568 || Batch Translation Loss:   0.164444 => Txt Tokens per Sec:     4511 || Lr: 0.000100
2024-02-02 22:49:29,936 [Epoch: 194 Step: 00025800] Batch Recognition Loss:   0.000592 => Gls Tokens per Sec:     1379 || Batch Translation Loss:   0.507054 => Txt Tokens per Sec:     3782 || Lr: 0.000100
2024-02-02 22:49:30,051 Epoch 194: Total Training Recognition Loss 0.07  Total Training Translation Loss 26.84 
2024-02-02 22:49:30,051 EPOCH 195
2024-02-02 22:49:35,947 [Epoch: 195 Step: 00025900] Batch Recognition Loss:   0.000163 => Gls Tokens per Sec:     1330 || Batch Translation Loss:   0.167008 => Txt Tokens per Sec:     3739 || Lr: 0.000100
2024-02-02 22:49:37,857 Epoch 195: Total Training Recognition Loss 0.06  Total Training Translation Loss 24.42 
2024-02-02 22:49:37,857 EPOCH 196
2024-02-02 22:49:41,478 [Epoch: 196 Step: 00026000] Batch Recognition Loss:   0.000591 => Gls Tokens per Sec:     1437 || Batch Translation Loss:   0.051818 => Txt Tokens per Sec:     4004 || Lr: 0.000100
2024-02-02 22:49:52,561 Validation result at epoch 196, step    26000: duration: 11.0831s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00222	Translation Loss: 95098.05469	PPL: 13581.54395
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.91	(BLEU-1: 10.68,	BLEU-2: 3.55,	BLEU-3: 1.63,	BLEU-4: 0.91)
	CHRF 17.22	ROUGE 9.02
2024-02-02 22:49:52,562 Logging Recognition and Translation Outputs
2024-02-02 22:49:52,562 ========================================================================================================================
2024-02-02 22:49:52,562 Logging Sequence: 130_139.00
2024-02-02 22:49:52,563 	Gloss Reference :	A B+C+D+E
2024-02-02 22:49:52,563 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:49:52,563 	Gloss Alignment :	         
2024-02-02 22:49:52,563 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:49:52,565 	Text Reference  :	he shared a picture of  a  little pouch he knit for his olympic gold medal with uk flag on one side and japanese flag on the other
2024-02-02 22:49:52,565 	Text Hypothesis :	he ****** * came    out as gay    when  he **** *** *** ******* **** ***** **** ** **** ** *** **** *** ******** **** ** was 19   
2024-02-02 22:49:52,565 	Text Alignment  :	   D      D S       S   S  S      S        D    D   D   D       D    D     D    D  D    D  D   D    D   D        D    D  S   S    
2024-02-02 22:49:52,565 ========================================================================================================================
2024-02-02 22:49:52,565 Logging Sequence: 72_194.00
2024-02-02 22:49:52,565 	Gloss Reference :	A B+C+D+E
2024-02-02 22:49:52,566 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:49:52,566 	Gloss Alignment :	         
2024-02-02 22:49:52,566 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:49:52,567 	Text Reference  :	shah told her to do what she wants and filed a    police complaint against her  
2024-02-02 22:49:52,567 	Text Hypothesis :	**** **** *** ** as per  the rules you can   have been   booked    their   hotel
2024-02-02 22:49:52,567 	Text Alignment  :	D    D    D   D  S  S    S   S     S   S     S    S      S         S       S    
2024-02-02 22:49:52,567 ========================================================================================================================
2024-02-02 22:49:52,567 Logging Sequence: 69_177.00
2024-02-02 22:49:52,567 	Gloss Reference :	A B+C+D+E
2024-02-02 22:49:52,568 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:49:52,568 	Gloss Alignment :	         
2024-02-02 22:49:52,568 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:49:52,569 	Text Reference  :	he said 'i will continue playing i know it's about time i   retire i      also have a       knee condition
2024-02-02 22:49:52,569 	Text Hypothesis :	** **** ** **** ******** ******* * **** **** ***** when csk were   elated with the  winning the  stadium  
2024-02-02 22:49:52,569 	Text Alignment  :	D  D    D  D    D        D       D D    D    D     S    S   S      S      S    S    S       S    S        
2024-02-02 22:49:52,569 ========================================================================================================================
2024-02-02 22:49:52,569 Logging Sequence: 95_118.00
2024-02-02 22:49:52,570 	Gloss Reference :	A B+C+D+E
2024-02-02 22:49:52,570 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:49:52,570 	Gloss Alignment :	         
2024-02-02 22:49:52,570 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:49:52,571 	Text Reference  :	******* the game was            stopped strangely due   to    excessive sunlight
2024-02-02 22:49:52,571 	Text Hypothesis :	however an  old  praggnanandhaa stood   comments  about their south     africa  
2024-02-02 22:49:52,571 	Text Alignment  :	I       S   S    S              S       S         S     S     S         S       
2024-02-02 22:49:52,571 ========================================================================================================================
2024-02-02 22:49:52,571 Logging Sequence: 112_8.00
2024-02-02 22:49:52,571 	Gloss Reference :	A B+C+D+E
2024-02-02 22:49:52,571 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:49:52,571 	Gloss Alignment :	         
2024-02-02 22:49:52,572 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:49:52,573 	Text Reference  :	before there were 8 teams such as mumbai indians delhi capitals punjab kings etc and now     there will   be  10      teams in  2022   
2024-02-02 22:49:52,573 	Text Hypothesis :	****** ***** **** * ***** **** ** ****** ******* ***** ******** ****** ***** *** *** however my    family was grabbed by    cvc capital
2024-02-02 22:49:52,573 	Text Alignment  :	D      D     D    D D     D    D  D      D       D     D        D      D     D   D   S       S     S      S   S       S     S   S      
2024-02-02 22:49:52,573 ========================================================================================================================
2024-02-02 22:49:56,290 Epoch 196: Total Training Recognition Loss 0.07  Total Training Translation Loss 19.61 
2024-02-02 22:49:56,291 EPOCH 197
2024-02-02 22:49:58,152 [Epoch: 197 Step: 00026100] Batch Recognition Loss:   0.000423 => Gls Tokens per Sec:     1376 || Batch Translation Loss:   0.084369 => Txt Tokens per Sec:     4096 || Lr: 0.000100
2024-02-02 22:50:03,904 [Epoch: 197 Step: 00026200] Batch Recognition Loss:   0.000527 => Gls Tokens per Sec:     1389 || Batch Translation Loss:   0.084241 => Txt Tokens per Sec:     3763 || Lr: 0.000100
2024-02-02 22:50:03,966 Epoch 197: Total Training Recognition Loss 0.05  Total Training Translation Loss 21.68 
2024-02-02 22:50:03,967 EPOCH 198
2024-02-02 22:50:09,534 [Epoch: 198 Step: 00026300] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     1423 || Batch Translation Loss:   0.680755 => Txt Tokens per Sec:     3950 || Lr: 0.000100
2024-02-02 22:50:11,516 Epoch 198: Total Training Recognition Loss 0.07  Total Training Translation Loss 24.15 
2024-02-02 22:50:11,516 EPOCH 199
2024-02-02 22:50:15,218 [Epoch: 199 Step: 00026400] Batch Recognition Loss:   0.000432 => Gls Tokens per Sec:     1427 || Batch Translation Loss:   0.054078 => Txt Tokens per Sec:     3986 || Lr: 0.000100
2024-02-02 22:50:19,080 Epoch 199: Total Training Recognition Loss 0.10  Total Training Translation Loss 24.98 
2024-02-02 22:50:19,081 EPOCH 200
2024-02-02 22:50:21,007 [Epoch: 200 Step: 00026500] Batch Recognition Loss:   0.000386 => Gls Tokens per Sec:     1370 || Batch Translation Loss:   0.222515 => Txt Tokens per Sec:     3726 || Lr: 0.000100
2024-02-02 22:50:26,816 [Epoch: 200 Step: 00026600] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:     1376 || Batch Translation Loss:   0.231935 => Txt Tokens per Sec:     3845 || Lr: 0.000100
2024-02-02 22:50:26,816 Epoch 200: Total Training Recognition Loss 0.06  Total Training Translation Loss 22.24 
2024-02-02 22:50:26,816 EPOCH 201
2024-02-02 22:50:32,481 [Epoch: 201 Step: 00026700] Batch Recognition Loss:   0.000626 => Gls Tokens per Sec:     1412 || Batch Translation Loss:   0.157651 => Txt Tokens per Sec:     3936 || Lr: 0.000100
2024-02-02 22:50:34,366 Epoch 201: Total Training Recognition Loss 0.07  Total Training Translation Loss 32.70 
2024-02-02 22:50:34,366 EPOCH 202
2024-02-02 22:50:38,153 [Epoch: 202 Step: 00026800] Batch Recognition Loss:   0.000466 => Gls Tokens per Sec:     1416 || Batch Translation Loss:   0.155509 => Txt Tokens per Sec:     4005 || Lr: 0.000100
2024-02-02 22:50:41,652 Epoch 202: Total Training Recognition Loss 0.09  Total Training Translation Loss 27.09 
2024-02-02 22:50:41,652 EPOCH 203
2024-02-02 22:50:43,431 [Epoch: 203 Step: 00026900] Batch Recognition Loss:   0.005145 => Gls Tokens per Sec:     1529 || Batch Translation Loss:   0.061212 => Txt Tokens per Sec:     4250 || Lr: 0.000100
2024-02-02 22:50:49,475 Epoch 203: Total Training Recognition Loss 0.08  Total Training Translation Loss 27.55 
2024-02-02 22:50:49,476 EPOCH 204
2024-02-02 22:50:49,544 [Epoch: 204 Step: 00027000] Batch Recognition Loss:   0.000434 => Gls Tokens per Sec:     1212 || Batch Translation Loss:   0.079615 => Txt Tokens per Sec:     4273 || Lr: 0.000100
2024-02-02 22:50:55,021 [Epoch: 204 Step: 00027100] Batch Recognition Loss:   0.000647 => Gls Tokens per Sec:     1459 || Batch Translation Loss:   0.079229 => Txt Tokens per Sec:     4069 || Lr: 0.000100
2024-02-02 22:50:56,908 Epoch 204: Total Training Recognition Loss 0.07  Total Training Translation Loss 21.64 
2024-02-02 22:50:56,909 EPOCH 205
2024-02-02 22:51:00,620 [Epoch: 205 Step: 00027200] Batch Recognition Loss:   0.000689 => Gls Tokens per Sec:     1466 || Batch Translation Loss:   0.251769 => Txt Tokens per Sec:     4077 || Lr: 0.000100
2024-02-02 22:51:04,249 Epoch 205: Total Training Recognition Loss 0.07  Total Training Translation Loss 24.65 
2024-02-02 22:51:04,249 EPOCH 206
2024-02-02 22:51:05,942 [Epoch: 206 Step: 00027300] Batch Recognition Loss:   0.004834 => Gls Tokens per Sec:     1655 || Batch Translation Loss:   0.152051 => Txt Tokens per Sec:     4592 || Lr: 0.000100
2024-02-02 22:51:11,545 Epoch 206: Total Training Recognition Loss 0.05  Total Training Translation Loss 23.21 
2024-02-02 22:51:11,545 EPOCH 207
2024-02-02 22:51:11,637 [Epoch: 207 Step: 00027400] Batch Recognition Loss:   0.000389 => Gls Tokens per Sec:     1758 || Batch Translation Loss:   0.034851 => Txt Tokens per Sec:     3747 || Lr: 0.000100
2024-02-02 22:51:17,368 [Epoch: 207 Step: 00027500] Batch Recognition Loss:   0.000421 => Gls Tokens per Sec:     1396 || Batch Translation Loss:   0.240600 => Txt Tokens per Sec:     3884 || Lr: 0.000100
2024-02-02 22:51:19,146 Epoch 207: Total Training Recognition Loss 0.07  Total Training Translation Loss 20.37 
2024-02-02 22:51:19,146 EPOCH 208
2024-02-02 22:51:22,879 [Epoch: 208 Step: 00027600] Batch Recognition Loss:   0.000289 => Gls Tokens per Sec:     1479 || Batch Translation Loss:   0.043623 => Txt Tokens per Sec:     4203 || Lr: 0.000100
2024-02-02 22:51:26,457 Epoch 208: Total Training Recognition Loss 0.05  Total Training Translation Loss 22.06 
2024-02-02 22:51:26,458 EPOCH 209
2024-02-02 22:51:28,377 [Epoch: 209 Step: 00027700] Batch Recognition Loss:   0.000332 => Gls Tokens per Sec:     1502 || Batch Translation Loss:   0.322763 => Txt Tokens per Sec:     4137 || Lr: 0.000100
2024-02-02 22:51:33,638 Epoch 209: Total Training Recognition Loss 0.06  Total Training Translation Loss 23.91 
2024-02-02 22:51:33,638 EPOCH 210
2024-02-02 22:51:33,802 [Epoch: 210 Step: 00027800] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:     1472 || Batch Translation Loss:   0.142724 => Txt Tokens per Sec:     3190 || Lr: 0.000100
2024-02-02 22:51:39,484 [Epoch: 210 Step: 00027900] Batch Recognition Loss:   0.000283 => Gls Tokens per Sec:     1408 || Batch Translation Loss:   0.211149 => Txt Tokens per Sec:     3927 || Lr: 0.000100
2024-02-02 22:51:41,225 Epoch 210: Total Training Recognition Loss 0.06  Total Training Translation Loss 30.59 
2024-02-02 22:51:41,225 EPOCH 211
2024-02-02 22:51:45,072 [Epoch: 211 Step: 00028000] Batch Recognition Loss:   0.000474 => Gls Tokens per Sec:     1456 || Batch Translation Loss:   0.345066 => Txt Tokens per Sec:     4130 || Lr: 0.000100
2024-02-02 22:51:55,475 Validation result at epoch 211, step    28000: duration: 10.4015s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00262	Translation Loss: 95493.88281	PPL: 14130.31836
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.56	(BLEU-1: 9.93,	BLEU-2: 2.83,	BLEU-3: 1.10,	BLEU-4: 0.56)
	CHRF 16.13	ROUGE 8.52
2024-02-02 22:51:55,476 Logging Recognition and Translation Outputs
2024-02-02 22:51:55,476 ========================================================================================================================
2024-02-02 22:51:55,476 Logging Sequence: 67_98.00
2024-02-02 22:51:55,477 	Gloss Reference :	A B+C+D+E
2024-02-02 22:51:55,477 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:51:55,477 	Gloss Alignment :	         
2024-02-02 22:51:55,477 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:51:55,478 	Text Reference  :	it saddens me to   see people suffering   and  dying  due   to     lack of    oxygen    
2024-02-02 22:51:55,478 	Text Hypothesis :	** ******* ** when he  was    argentina's turn lionel messi scored the  brand ambassador
2024-02-02 22:51:55,478 	Text Alignment  :	D  D       D  S    S   S      S           S    S      S     S      S    S     S         
2024-02-02 22:51:55,478 ========================================================================================================================
2024-02-02 22:51:55,479 Logging Sequence: 157_83.00
2024-02-02 22:51:55,479 	Gloss Reference :	A B+C+D+E
2024-02-02 22:51:55,479 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:51:55,479 	Gloss Alignment :	         
2024-02-02 22:51:55,479 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:51:55,481 	Text Reference  :	also when you eat sandwich at  a    streetside hawker    or  stall the sandwich maker    will first apply butter with a         knife
2024-02-02 22:51:55,481 	Text Hypothesis :	**** **** *** *** the      ipl that was        postponed due to    the covid    pandemic will ***** be    held   in   september 2021 
2024-02-02 22:51:55,481 	Text Alignment  :	D    D    D   D   S        S   S    S          S         S   S         S        S             D     S     S      S    S         S    
2024-02-02 22:51:55,481 ========================================================================================================================
2024-02-02 22:51:55,481 Logging Sequence: 76_35.00
2024-02-02 22:51:55,482 	Gloss Reference :	A B+C+D+E
2024-02-02 22:51:55,482 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:51:55,482 	Gloss Alignment :	         
2024-02-02 22:51:55,482 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:51:55,483 	Text Reference  :	bcci president sourav    ganguly   along  with board   secretary jay shah
2024-02-02 22:51:55,483 	Text Hypothesis :	**** indian    cricketer yuzvendra chahal is   married to        his wife
2024-02-02 22:51:55,483 	Text Alignment  :	D    S         S         S         S      S    S       S         S   S   
2024-02-02 22:51:55,483 ========================================================================================================================
2024-02-02 22:51:55,483 Logging Sequence: 139_180.00
2024-02-02 22:51:55,483 	Gloss Reference :	A B+C+D+E
2024-02-02 22:51:55,483 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:51:55,484 	Gloss Alignment :	         
2024-02-02 22:51:55,484 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:51:55,484 	Text Reference  :	*** *** *** **** ****** ****** *** ***** *** ** netherlands also  faced similar riots
2024-02-02 22:51:55,484 	Text Hypothesis :	the 1st day then deepak chahar and chest led to a           total of    7       goals
2024-02-02 22:51:55,485 	Text Alignment  :	I   I   I   I    I      I      I   I     I   I  S           S     S     S       S    
2024-02-02 22:51:55,485 ========================================================================================================================
2024-02-02 22:51:55,485 Logging Sequence: 98_87.00
2024-02-02 22:51:55,485 	Gloss Reference :	A B+C+D+E
2024-02-02 22:51:55,485 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:51:55,485 	Gloss Alignment :	         
2024-02-02 22:51:55,485 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:51:55,487 	Text Reference  :	instead of     starting afresh in     2021 the organizers opted   to ***** ** resume with the previous edition
2024-02-02 22:51:55,487 	Text Hypothesis :	mumal   shared that     he     looked like her and        decided to start so he     got  the ******** field  
2024-02-02 22:51:55,487 	Text Alignment  :	S       S      S        S      S      S    S   S          S          I     I  S      S        D        S      
2024-02-02 22:51:55,487 ========================================================================================================================
2024-02-02 22:51:59,026 Epoch 211: Total Training Recognition Loss 0.06  Total Training Translation Loss 25.94 
2024-02-02 22:51:59,026 EPOCH 212
2024-02-02 22:52:01,306 [Epoch: 212 Step: 00028100] Batch Recognition Loss:   0.000261 => Gls Tokens per Sec:     1299 || Batch Translation Loss:   0.046014 => Txt Tokens per Sec:     3718 || Lr: 0.000100
2024-02-02 22:52:06,705 Epoch 212: Total Training Recognition Loss 0.05  Total Training Translation Loss 20.46 
2024-02-02 22:52:06,706 EPOCH 213
2024-02-02 22:52:06,928 [Epoch: 213 Step: 00028200] Batch Recognition Loss:   0.000424 => Gls Tokens per Sec:     1448 || Batch Translation Loss:   0.076390 => Txt Tokens per Sec:     4199 || Lr: 0.000100
2024-02-02 22:52:12,818 [Epoch: 213 Step: 00028300] Batch Recognition Loss:   0.000348 => Gls Tokens per Sec:     1358 || Batch Translation Loss:   0.192273 => Txt Tokens per Sec:     3764 || Lr: 0.000100
2024-02-02 22:52:14,521 Epoch 213: Total Training Recognition Loss 0.06  Total Training Translation Loss 17.37 
2024-02-02 22:52:14,521 EPOCH 214
2024-02-02 22:52:18,680 [Epoch: 214 Step: 00028400] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:     1366 || Batch Translation Loss:   0.090693 => Txt Tokens per Sec:     3810 || Lr: 0.000100
2024-02-02 22:52:22,065 Epoch 214: Total Training Recognition Loss 0.05  Total Training Translation Loss 19.17 
2024-02-02 22:52:22,065 EPOCH 215
2024-02-02 22:52:24,205 [Epoch: 215 Step: 00028500] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     1421 || Batch Translation Loss:   0.159985 => Txt Tokens per Sec:     3963 || Lr: 0.000100
2024-02-02 22:52:29,655 Epoch 215: Total Training Recognition Loss 0.06  Total Training Translation Loss 19.92 
2024-02-02 22:52:29,655 EPOCH 216
2024-02-02 22:52:29,907 [Epoch: 216 Step: 00028600] Batch Recognition Loss:   0.000423 => Gls Tokens per Sec:     1594 || Batch Translation Loss:   0.152996 => Txt Tokens per Sec:     4323 || Lr: 0.000100
2024-02-02 22:52:35,687 [Epoch: 216 Step: 00028700] Batch Recognition Loss:   0.000299 => Gls Tokens per Sec:     1384 || Batch Translation Loss:   0.167172 => Txt Tokens per Sec:     3877 || Lr: 0.000100
2024-02-02 22:52:37,288 Epoch 216: Total Training Recognition Loss 0.08  Total Training Translation Loss 22.52 
2024-02-02 22:52:37,288 EPOCH 217
2024-02-02 22:52:41,380 [Epoch: 217 Step: 00028800] Batch Recognition Loss:   0.000333 => Gls Tokens per Sec:     1408 || Batch Translation Loss:   0.034216 => Txt Tokens per Sec:     3966 || Lr: 0.000100
2024-02-02 22:52:44,740 Epoch 217: Total Training Recognition Loss 0.06  Total Training Translation Loss 24.25 
2024-02-02 22:52:44,740 EPOCH 218
2024-02-02 22:52:46,945 [Epoch: 218 Step: 00028900] Batch Recognition Loss:   0.000282 => Gls Tokens per Sec:     1416 || Batch Translation Loss:   0.086662 => Txt Tokens per Sec:     3931 || Lr: 0.000100
2024-02-02 22:52:52,310 Epoch 218: Total Training Recognition Loss 0.07  Total Training Translation Loss 23.79 
2024-02-02 22:52:52,311 EPOCH 219
2024-02-02 22:52:52,657 [Epoch: 219 Step: 00029000] Batch Recognition Loss:   0.000202 => Gls Tokens per Sec:     1391 || Batch Translation Loss:   0.075552 => Txt Tokens per Sec:     4023 || Lr: 0.000100
2024-02-02 22:52:58,329 [Epoch: 219 Step: 00029100] Batch Recognition Loss:   0.000779 => Gls Tokens per Sec:     1411 || Batch Translation Loss:   0.145071 => Txt Tokens per Sec:     3896 || Lr: 0.000100
2024-02-02 22:52:59,980 Epoch 219: Total Training Recognition Loss 0.06  Total Training Translation Loss 26.26 
2024-02-02 22:52:59,980 EPOCH 220
2024-02-02 22:53:04,221 [Epoch: 220 Step: 00029200] Batch Recognition Loss:   0.000772 => Gls Tokens per Sec:     1377 || Batch Translation Loss:   0.126136 => Txt Tokens per Sec:     3826 || Lr: 0.000100
2024-02-02 22:53:07,697 Epoch 220: Total Training Recognition Loss 0.06  Total Training Translation Loss 22.61 
2024-02-02 22:53:07,697 EPOCH 221
2024-02-02 22:53:09,777 [Epoch: 221 Step: 00029300] Batch Recognition Loss:   0.000420 => Gls Tokens per Sec:     1539 || Batch Translation Loss:   0.166104 => Txt Tokens per Sec:     4157 || Lr: 0.000100
2024-02-02 22:53:14,877 Epoch 221: Total Training Recognition Loss 0.06  Total Training Translation Loss 16.47 
2024-02-02 22:53:14,878 EPOCH 222
2024-02-02 22:53:15,285 [Epoch: 222 Step: 00029400] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     1379 || Batch Translation Loss:   0.163550 => Txt Tokens per Sec:     4219 || Lr: 0.000100
2024-02-02 22:53:21,181 [Epoch: 222 Step: 00029500] Batch Recognition Loss:   0.000264 => Gls Tokens per Sec:     1357 || Batch Translation Loss:   0.054446 => Txt Tokens per Sec:     3754 || Lr: 0.000100
2024-02-02 22:53:22,719 Epoch 222: Total Training Recognition Loss 0.05  Total Training Translation Loss 21.72 
2024-02-02 22:53:22,720 EPOCH 223
2024-02-02 22:53:26,635 [Epoch: 223 Step: 00029600] Batch Recognition Loss:   0.000725 => Gls Tokens per Sec:     1513 || Batch Translation Loss:   0.109200 => Txt Tokens per Sec:     4132 || Lr: 0.000100
2024-02-02 22:53:29,781 Epoch 223: Total Training Recognition Loss 0.05  Total Training Translation Loss 26.36 
2024-02-02 22:53:29,781 EPOCH 224
2024-02-02 22:53:32,142 [Epoch: 224 Step: 00029700] Batch Recognition Loss:   0.000392 => Gls Tokens per Sec:     1391 || Batch Translation Loss:   0.018495 => Txt Tokens per Sec:     3815 || Lr: 0.000100
2024-02-02 22:53:37,237 Epoch 224: Total Training Recognition Loss 0.06  Total Training Translation Loss 27.76 
2024-02-02 22:53:37,237 EPOCH 225
2024-02-02 22:53:37,666 [Epoch: 225 Step: 00029800] Batch Recognition Loss:   0.000234 => Gls Tokens per Sec:     1495 || Batch Translation Loss:   0.136954 => Txt Tokens per Sec:     4171 || Lr: 0.000100
2024-02-02 22:53:43,422 [Epoch: 225 Step: 00029900] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     1388 || Batch Translation Loss:   0.208460 => Txt Tokens per Sec:     3866 || Lr: 0.000100
2024-02-02 22:53:44,732 Epoch 225: Total Training Recognition Loss 0.06  Total Training Translation Loss 29.90 
2024-02-02 22:53:44,732 EPOCH 226
2024-02-02 22:53:48,974 [Epoch: 226 Step: 00030000] Batch Recognition Loss:   0.000352 => Gls Tokens per Sec:     1415 || Batch Translation Loss:   0.048505 => Txt Tokens per Sec:     3938 || Lr: 0.000100
2024-02-02 22:54:00,020 Validation result at epoch 226, step    30000: duration: 11.0458s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00548	Translation Loss: 94195.98438	PPL: 12409.24414
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.72	(BLEU-1: 11.48,	BLEU-2: 3.57,	BLEU-3: 1.48,	BLEU-4: 0.72)
	CHRF 17.37	ROUGE 9.90
2024-02-02 22:54:00,021 Logging Recognition and Translation Outputs
2024-02-02 22:54:00,022 ========================================================================================================================
2024-02-02 22:54:00,022 Logging Sequence: 165_502.00
2024-02-02 22:54:00,022 	Gloss Reference :	A B+C+D+E
2024-02-02 22:54:00,022 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:54:00,022 	Gloss Alignment :	         
2024-02-02 22:54:00,022 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:54:00,024 	Text Reference  :	tendulkar would sit   in the **** ***** *** pavilion wearing both his  batting pads even after he got   out
2024-02-02 22:54:00,025 	Text Hypothesis :	when      the   final of the 2011 world cup 2023     will    be   held by      the  same rule  of camel flu
2024-02-02 22:54:00,025 	Text Alignment  :	S         S     S     S      I    I     I   S        S       S    S    S       S    S    S     S  S     S  
2024-02-02 22:54:00,025 ========================================================================================================================
2024-02-02 22:54:00,025 Logging Sequence: 127_57.00
2024-02-02 22:54:00,025 	Gloss Reference :	A B+C+D+E
2024-02-02 22:54:00,025 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:54:00,025 	Gloss Alignment :	         
2024-02-02 22:54:00,026 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:54:00,027 	Text Reference  :	till date india had won only 2 medals at the championships which like the    olympics is the    highest level championship
2024-02-02 22:54:00,027 	Text Hypothesis :	**** **** india had won only * ****** ** *** ************* ***** a    bronze and      a  silver medal   in    2016        
2024-02-02 22:54:00,027 	Text Alignment  :	D    D                       D D      D  D   D             D     S    S      S        S  S      S       S     S           
2024-02-02 22:54:00,027 ========================================================================================================================
2024-02-02 22:54:00,028 Logging Sequence: 169_10.00
2024-02-02 22:54:00,028 	Gloss Reference :	A B+C+D+E
2024-02-02 22:54:00,028 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:54:00,028 	Gloss Alignment :	         
2024-02-02 22:54:00,028 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:54:00,030 	Text Reference  :	the 18th over was ********** bowled by     ravi  bishnoi with khushdil shah and asif ali on the ******* crease   
2024-02-02 22:54:00,030 	Text Hypothesis :	*** when it   was australia' turn   lionel messi scored  the  first    time and gave him to the lgbtqia community
2024-02-02 22:54:00,030 	Text Alignment  :	D   S    S        I          S      S      S     S       S    S        S        S    S   S      I       S        
2024-02-02 22:54:00,030 ========================================================================================================================
2024-02-02 22:54:00,031 Logging Sequence: 64_89.00
2024-02-02 22:54:00,031 	Gloss Reference :	A B+C+D+E
2024-02-02 22:54:00,031 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:54:00,031 	Gloss Alignment :	         
2024-02-02 22:54:00,031 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:54:00,032 	Text Reference  :	but this can  not go on amidst the rising cases human lives    need to   be safeguarded
2024-02-02 22:54:00,032 	Text Hypothesis :	*** ipl  will not ** ** ****** *** ****** ***** be    possible in   june as well       
2024-02-02 22:54:00,032 	Text Alignment  :	D   S    S        D  D  D      D   D      D     S     S        S    S    S  S          
2024-02-02 22:54:00,032 ========================================================================================================================
2024-02-02 22:54:00,033 Logging Sequence: 166_261.00
2024-02-02 22:54:00,033 	Gloss Reference :	A B+C+D+E
2024-02-02 22:54:00,033 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:54:00,033 	Gloss Alignment :	         
2024-02-02 22:54:00,033 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:54:00,034 	Text Reference  :	*** ***** *** **** for all    organizational matters and the schedule
2024-02-02 22:54:00,034 	Text Hypothesis :	the world cup will be  played between        india   and 16  teams   
2024-02-02 22:54:00,034 	Text Alignment  :	I   I     I   I    S   S      S              S           S   S       
2024-02-02 22:54:00,034 ========================================================================================================================
2024-02-02 22:54:03,099 Epoch 226: Total Training Recognition Loss 0.06  Total Training Translation Loss 25.96 
2024-02-02 22:54:03,099 EPOCH 227
2024-02-02 22:54:05,479 [Epoch: 227 Step: 00030100] Batch Recognition Loss:   0.000442 => Gls Tokens per Sec:     1412 || Batch Translation Loss:   0.076451 => Txt Tokens per Sec:     3853 || Lr: 0.000100
2024-02-02 22:54:10,616 Epoch 227: Total Training Recognition Loss 0.07  Total Training Translation Loss 21.88 
2024-02-02 22:54:10,617 EPOCH 228
2024-02-02 22:54:11,090 [Epoch: 228 Step: 00030200] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     1525 || Batch Translation Loss:   0.170312 => Txt Tokens per Sec:     4189 || Lr: 0.000100
2024-02-02 22:54:16,894 [Epoch: 228 Step: 00030300] Batch Recognition Loss:   0.000345 => Gls Tokens per Sec:     1377 || Batch Translation Loss:   0.082396 => Txt Tokens per Sec:     3823 || Lr: 0.000100
2024-02-02 22:54:18,261 Epoch 228: Total Training Recognition Loss 0.05  Total Training Translation Loss 20.01 
2024-02-02 22:54:18,261 EPOCH 229
2024-02-02 22:54:22,616 [Epoch: 229 Step: 00030400] Batch Recognition Loss:   0.000598 => Gls Tokens per Sec:     1397 || Batch Translation Loss:   0.103276 => Txt Tokens per Sec:     3821 || Lr: 0.000100
2024-02-02 22:54:25,652 Epoch 229: Total Training Recognition Loss 0.06  Total Training Translation Loss 20.86 
2024-02-02 22:54:25,652 EPOCH 230
2024-02-02 22:54:28,022 [Epoch: 230 Step: 00030500] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     1452 || Batch Translation Loss:   0.156273 => Txt Tokens per Sec:     3948 || Lr: 0.000100
2024-02-02 22:54:33,066 Epoch 230: Total Training Recognition Loss 0.07  Total Training Translation Loss 18.65 
2024-02-02 22:54:33,066 EPOCH 231
2024-02-02 22:54:33,566 [Epoch: 231 Step: 00030600] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     1603 || Batch Translation Loss:   0.030646 => Txt Tokens per Sec:     4563 || Lr: 0.000100
2024-02-02 22:54:38,925 [Epoch: 231 Step: 00030700] Batch Recognition Loss:   0.000673 => Gls Tokens per Sec:     1493 || Batch Translation Loss:   0.084656 => Txt Tokens per Sec:     4105 || Lr: 0.000100
2024-02-02 22:54:40,315 Epoch 231: Total Training Recognition Loss 0.06  Total Training Translation Loss 21.71 
2024-02-02 22:54:40,316 EPOCH 232
2024-02-02 22:54:44,643 [Epoch: 232 Step: 00030800] Batch Recognition Loss:   0.000423 => Gls Tokens per Sec:     1424 || Batch Translation Loss:   0.047792 => Txt Tokens per Sec:     4004 || Lr: 0.000100
2024-02-02 22:54:47,963 Epoch 232: Total Training Recognition Loss 0.07  Total Training Translation Loss 29.73 
2024-02-02 22:54:47,964 EPOCH 233
2024-02-02 22:54:50,350 [Epoch: 233 Step: 00030900] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     1476 || Batch Translation Loss:   0.080946 => Txt Tokens per Sec:     4113 || Lr: 0.000100
2024-02-02 22:54:55,143 Epoch 233: Total Training Recognition Loss 0.05  Total Training Translation Loss 21.13 
2024-02-02 22:54:55,144 EPOCH 234
2024-02-02 22:54:55,810 [Epoch: 234 Step: 00031000] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     1323 || Batch Translation Loss:   0.148361 => Txt Tokens per Sec:     3492 || Lr: 0.000100
2024-02-02 22:55:01,637 [Epoch: 234 Step: 00031100] Batch Recognition Loss:   0.000471 => Gls Tokens per Sec:     1373 || Batch Translation Loss:   0.034865 => Txt Tokens per Sec:     3813 || Lr: 0.000100
2024-02-02 22:55:02,936 Epoch 234: Total Training Recognition Loss 0.06  Total Training Translation Loss 16.22 
2024-02-02 22:55:02,936 EPOCH 235
2024-02-02 22:55:07,583 [Epoch: 235 Step: 00031200] Batch Recognition Loss:   0.000375 => Gls Tokens per Sec:     1343 || Batch Translation Loss:   0.739877 => Txt Tokens per Sec:     3784 || Lr: 0.000100
2024-02-02 22:55:10,354 Epoch 235: Total Training Recognition Loss 0.06  Total Training Translation Loss 24.78 
2024-02-02 22:55:10,354 EPOCH 236
2024-02-02 22:55:12,777 [Epoch: 236 Step: 00031300] Batch Recognition Loss:   0.000336 => Gls Tokens per Sec:     1486 || Batch Translation Loss:   0.124032 => Txt Tokens per Sec:     4268 || Lr: 0.000100
2024-02-02 22:55:17,688 Epoch 236: Total Training Recognition Loss 0.05  Total Training Translation Loss 26.11 
2024-02-02 22:55:17,689 EPOCH 237
2024-02-02 22:55:18,293 [Epoch: 237 Step: 00031400] Batch Recognition Loss:   0.000553 => Gls Tokens per Sec:     1594 || Batch Translation Loss:   0.074639 => Txt Tokens per Sec:     4665 || Lr: 0.000100
2024-02-02 22:55:23,811 [Epoch: 237 Step: 00031500] Batch Recognition Loss:   0.000389 => Gls Tokens per Sec:     1448 || Batch Translation Loss:   0.680751 => Txt Tokens per Sec:     4024 || Lr: 0.000100
2024-02-02 22:55:24,891 Epoch 237: Total Training Recognition Loss 0.07  Total Training Translation Loss 27.54 
2024-02-02 22:55:24,892 EPOCH 238
2024-02-02 22:55:29,318 [Epoch: 238 Step: 00031600] Batch Recognition Loss:   0.000264 => Gls Tokens per Sec:     1428 || Batch Translation Loss:   0.254874 => Txt Tokens per Sec:     3972 || Lr: 0.000100
2024-02-02 22:55:32,367 Epoch 238: Total Training Recognition Loss 0.05  Total Training Translation Loss 17.56 
2024-02-02 22:55:32,367 EPOCH 239
2024-02-02 22:55:34,966 [Epoch: 239 Step: 00031700] Batch Recognition Loss:   0.000515 => Gls Tokens per Sec:     1416 || Batch Translation Loss:   0.052603 => Txt Tokens per Sec:     3921 || Lr: 0.000100
2024-02-02 22:55:39,576 Epoch 239: Total Training Recognition Loss 0.06  Total Training Translation Loss 19.05 
2024-02-02 22:55:39,576 EPOCH 240
2024-02-02 22:55:40,291 [Epoch: 240 Step: 00031800] Batch Recognition Loss:   0.000151 => Gls Tokens per Sec:     1456 || Batch Translation Loss:   0.123165 => Txt Tokens per Sec:     4460 || Lr: 0.000100
2024-02-02 22:55:45,848 [Epoch: 240 Step: 00031900] Batch Recognition Loss:   0.000266 => Gls Tokens per Sec:     1440 || Batch Translation Loss:   0.262513 => Txt Tokens per Sec:     3942 || Lr: 0.000100
2024-02-02 22:55:47,106 Epoch 240: Total Training Recognition Loss 0.06  Total Training Translation Loss 26.46 
2024-02-02 22:55:47,106 EPOCH 241
2024-02-02 22:55:51,701 [Epoch: 241 Step: 00032000] Batch Recognition Loss:   0.001144 => Gls Tokens per Sec:     1393 || Batch Translation Loss:   0.071701 => Txt Tokens per Sec:     3894 || Lr: 0.000100
2024-02-02 22:56:02,433 Validation result at epoch 241, step    32000: duration: 10.7308s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00308	Translation Loss: 94652.14062	PPL: 12988.82520
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.59	(BLEU-1: 11.36,	BLEU-2: 3.26,	BLEU-3: 1.21,	BLEU-4: 0.59)
	CHRF 17.44	ROUGE 9.67
2024-02-02 22:56:02,435 Logging Recognition and Translation Outputs
2024-02-02 22:56:02,435 ========================================================================================================================
2024-02-02 22:56:02,435 Logging Sequence: 86_11.00
2024-02-02 22:56:02,435 	Gloss Reference :	A B+C+D+E
2024-02-02 22:56:02,435 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:56:02,435 	Gloss Alignment :	         
2024-02-02 22:56:02,435 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:56:02,436 	Text Reference  :	he       was ******* ****** 66  years    old  
2024-02-02 22:56:02,436 	Text Hypothesis :	everyone was another strong and pakistan match
2024-02-02 22:56:02,436 	Text Alignment  :	S            I       I      S   S        S    
2024-02-02 22:56:02,436 ========================================================================================================================
2024-02-02 22:56:02,436 Logging Sequence: 67_16.00
2024-02-02 22:56:02,437 	Gloss Reference :	A B+C+D+E
2024-02-02 22:56:02,437 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:56:02,437 	Gloss Alignment :	         
2024-02-02 22:56:02,437 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:56:02,438 	Text Reference  :	* **** ********* *** ***** ** to help  india's fight   against the covid-19 pandemic  
2024-02-02 22:56:02,438 	Text Hypothesis :	i have dedicated the match as a  close to      compete in      the ******** tournament
2024-02-02 22:56:02,438 	Text Alignment  :	I I    I         I   I     I  S  S     S       S       S           D        S         
2024-02-02 22:56:02,438 ========================================================================================================================
2024-02-02 22:56:02,438 Logging Sequence: 69_177.00
2024-02-02 22:56:02,438 	Gloss Reference :	A B+C+D+E
2024-02-02 22:56:02,439 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:56:02,439 	Gloss Alignment :	         
2024-02-02 22:56:02,439 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:56:02,440 	Text Reference  :	he said 'i will continue playing i know it's  about time i   retire i   also    have   a   knee   condition
2024-02-02 22:56:02,440 	Text Hypothesis :	** **** ** **** ******** ******* * **** since the   4th  ipl final  and gujarat became the runner ups      
2024-02-02 22:56:02,440 	Text Alignment  :	D  D    D  D    D        D       D D    S     S     S    S   S      S   S       S      S   S      S        
2024-02-02 22:56:02,441 ========================================================================================================================
2024-02-02 22:56:02,441 Logging Sequence: 165_615.00
2024-02-02 22:56:02,441 	Gloss Reference :	A B+C+D+E
2024-02-02 22:56:02,441 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:56:02,441 	Gloss Alignment :	         
2024-02-02 22:56:02,441 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:56:02,442 	Text Reference  :	***** *** ***** **** **** **** ***** we defeated pakistan too 
2024-02-02 22:56:02,442 	Text Hypothesis :	after the match when they went viral on 5th      may      2023
2024-02-02 22:56:02,442 	Text Alignment  :	I     I   I     I    I    I    I     S  S        S        S   
2024-02-02 22:56:02,442 ========================================================================================================================
2024-02-02 22:56:02,442 Logging Sequence: 61_5.00
2024-02-02 22:56:02,443 	Gloss Reference :	A B+C+D+E
2024-02-02 22:56:02,443 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:56:02,443 	Gloss Alignment :	         
2024-02-02 22:56:02,443 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:56:02,444 	Text Reference  :	they rivalry is      seen the     most during  india pakistan cricket matches
2024-02-02 22:56:02,444 	Text Hypothesis :	**** ******* indians love cricket and  excited to    see      dhoni's message
2024-02-02 22:56:02,444 	Text Alignment  :	D    D       S       S    S       S    S       S     S        S       S      
2024-02-02 22:56:02,444 ========================================================================================================================
2024-02-02 22:56:05,599 Epoch 241: Total Training Recognition Loss 0.05  Total Training Translation Loss 23.67 
2024-02-02 22:56:05,600 EPOCH 242
2024-02-02 22:56:07,950 [Epoch: 242 Step: 00032100] Batch Recognition Loss:   0.000553 => Gls Tokens per Sec:     1600 || Batch Translation Loss:   0.406887 => Txt Tokens per Sec:     4502 || Lr: 0.000100
2024-02-02 22:56:12,727 Epoch 242: Total Training Recognition Loss 0.06  Total Training Translation Loss 18.76 
2024-02-02 22:56:12,727 EPOCH 243
2024-02-02 22:56:13,474 [Epoch: 243 Step: 00032200] Batch Recognition Loss:   0.000259 => Gls Tokens per Sec:     1500 || Batch Translation Loss:   0.126384 => Txt Tokens per Sec:     4061 || Lr: 0.000100
2024-02-02 22:56:19,138 [Epoch: 243 Step: 00032300] Batch Recognition Loss:   0.000189 => Gls Tokens per Sec:     1413 || Batch Translation Loss:   0.372439 => Txt Tokens per Sec:     3953 || Lr: 0.000100
2024-02-02 22:56:20,312 Epoch 243: Total Training Recognition Loss 0.06  Total Training Translation Loss 23.15 
2024-02-02 22:56:20,312 EPOCH 244
2024-02-02 22:56:24,857 [Epoch: 244 Step: 00032400] Batch Recognition Loss:   0.000552 => Gls Tokens per Sec:     1426 || Batch Translation Loss:   0.183032 => Txt Tokens per Sec:     3997 || Lr: 0.000100
2024-02-02 22:56:28,084 Epoch 244: Total Training Recognition Loss 0.06  Total Training Translation Loss 22.11 
2024-02-02 22:56:28,085 EPOCH 245
2024-02-02 22:56:30,934 [Epoch: 245 Step: 00032500] Batch Recognition Loss:   0.000345 => Gls Tokens per Sec:     1349 || Batch Translation Loss:   0.012374 => Txt Tokens per Sec:     3773 || Lr: 0.000100
2024-02-02 22:56:35,853 Epoch 245: Total Training Recognition Loss 0.07  Total Training Translation Loss 18.48 
2024-02-02 22:56:35,854 EPOCH 246
2024-02-02 22:56:36,641 [Epoch: 246 Step: 00032600] Batch Recognition Loss:   0.000677 => Gls Tokens per Sec:     1528 || Batch Translation Loss:   0.285721 => Txt Tokens per Sec:     3892 || Lr: 0.000100
2024-02-02 22:56:42,631 [Epoch: 246 Step: 00032700] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:     1335 || Batch Translation Loss:   0.049834 => Txt Tokens per Sec:     3763 || Lr: 0.000100
2024-02-02 22:56:43,656 Epoch 246: Total Training Recognition Loss 0.05  Total Training Translation Loss 24.97 
2024-02-02 22:56:43,656 EPOCH 247
2024-02-02 22:56:48,252 [Epoch: 247 Step: 00032800] Batch Recognition Loss:   0.000259 => Gls Tokens per Sec:     1428 || Batch Translation Loss:   0.156867 => Txt Tokens per Sec:     3961 || Lr: 0.000100
2024-02-02 22:56:51,089 Epoch 247: Total Training Recognition Loss 0.06  Total Training Translation Loss 20.81 
2024-02-02 22:56:51,089 EPOCH 248
2024-02-02 22:56:53,823 [Epoch: 248 Step: 00032900] Batch Recognition Loss:   0.000597 => Gls Tokens per Sec:     1434 || Batch Translation Loss:   0.861620 => Txt Tokens per Sec:     3923 || Lr: 0.000100
2024-02-02 22:56:59,382 Epoch 248: Total Training Recognition Loss 0.06  Total Training Translation Loss 15.36 
2024-02-02 22:56:59,382 EPOCH 249
2024-02-02 22:57:00,262 [Epoch: 249 Step: 00033000] Batch Recognition Loss:   0.000437 => Gls Tokens per Sec:     1457 || Batch Translation Loss:   0.040186 => Txt Tokens per Sec:     4025 || Lr: 0.000100
2024-02-02 22:57:05,909 [Epoch: 249 Step: 00033100] Batch Recognition Loss:   0.000320 => Gls Tokens per Sec:     1417 || Batch Translation Loss:   0.128006 => Txt Tokens per Sec:     3912 || Lr: 0.000100
2024-02-02 22:57:07,032 Epoch 249: Total Training Recognition Loss 0.06  Total Training Translation Loss 19.39 
2024-02-02 22:57:07,032 EPOCH 250
2024-02-02 22:57:11,886 [Epoch: 250 Step: 00033200] Batch Recognition Loss:   0.000453 => Gls Tokens per Sec:     1369 || Batch Translation Loss:   0.079909 => Txt Tokens per Sec:     3780 || Lr: 0.000100
2024-02-02 22:57:14,749 Epoch 250: Total Training Recognition Loss 0.07  Total Training Translation Loss 19.71 
2024-02-02 22:57:14,749 EPOCH 251
2024-02-02 22:57:17,572 [Epoch: 251 Step: 00033300] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     1418 || Batch Translation Loss:   0.043702 => Txt Tokens per Sec:     3964 || Lr: 0.000100
2024-02-02 22:57:22,270 Epoch 251: Total Training Recognition Loss 0.06  Total Training Translation Loss 18.40 
2024-02-02 22:57:22,270 EPOCH 252
2024-02-02 22:57:23,119 [Epoch: 252 Step: 00033400] Batch Recognition Loss:   0.000308 => Gls Tokens per Sec:     1605 || Batch Translation Loss:   0.037696 => Txt Tokens per Sec:     4250 || Lr: 0.000100
2024-02-02 22:57:29,145 [Epoch: 252 Step: 00033500] Batch Recognition Loss:   0.001160 => Gls Tokens per Sec:     1326 || Batch Translation Loss:   0.207018 => Txt Tokens per Sec:     3698 || Lr: 0.000100
2024-02-02 22:57:30,052 Epoch 252: Total Training Recognition Loss 0.06  Total Training Translation Loss 19.32 
2024-02-02 22:57:30,053 EPOCH 253
2024-02-02 22:57:34,878 [Epoch: 253 Step: 00033600] Batch Recognition Loss:   0.000462 => Gls Tokens per Sec:     1393 || Batch Translation Loss:   0.080726 => Txt Tokens per Sec:     3820 || Lr: 0.000100
2024-02-02 22:57:37,838 Epoch 253: Total Training Recognition Loss 0.05  Total Training Translation Loss 19.32 
2024-02-02 22:57:37,839 EPOCH 254
2024-02-02 22:57:40,609 [Epoch: 254 Step: 00033700] Batch Recognition Loss:   0.000580 => Gls Tokens per Sec:     1474 || Batch Translation Loss:   0.040320 => Txt Tokens per Sec:     4089 || Lr: 0.000100
2024-02-02 22:57:45,092 Epoch 254: Total Training Recognition Loss 0.06  Total Training Translation Loss 19.60 
2024-02-02 22:57:45,093 EPOCH 255
2024-02-02 22:57:46,084 [Epoch: 255 Step: 00033800] Batch Recognition Loss:   0.000262 => Gls Tokens per Sec:     1454 || Batch Translation Loss:   0.151026 => Txt Tokens per Sec:     3648 || Lr: 0.000100
2024-02-02 22:57:51,720 [Epoch: 255 Step: 00033900] Batch Recognition Loss:   0.000240 => Gls Tokens per Sec:     1418 || Batch Translation Loss:   0.126679 => Txt Tokens per Sec:     4018 || Lr: 0.000100
2024-02-02 22:57:52,647 Epoch 255: Total Training Recognition Loss 0.07  Total Training Translation Loss 21.99 
2024-02-02 22:57:52,647 EPOCH 256
2024-02-02 22:57:57,325 [Epoch: 256 Step: 00034000] Batch Recognition Loss:   0.000376 => Gls Tokens per Sec:     1454 || Batch Translation Loss:   0.483632 => Txt Tokens per Sec:     3983 || Lr: 0.000100
2024-02-02 22:58:08,109 Validation result at epoch 256, step    34000: duration: 10.7820s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00334	Translation Loss: 95813.03125	PPL: 14588.87598
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.53	(BLEU-1: 10.31,	BLEU-2: 2.71,	BLEU-3: 1.04,	BLEU-4: 0.53)
	CHRF 16.34	ROUGE 9.03
2024-02-02 22:58:08,110 Logging Recognition and Translation Outputs
2024-02-02 22:58:08,110 ========================================================================================================================
2024-02-02 22:58:08,110 Logging Sequence: 92_199.00
2024-02-02 22:58:08,111 	Gloss Reference :	A B+C+D+E
2024-02-02 22:58:08,111 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:58:08,111 	Gloss Alignment :	         
2024-02-02 22:58:08,111 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:58:08,112 	Text Reference  :	**** ** *** ** people on        social media said that 
2024-02-02 22:58:08,112 	Text Hypothesis :	this is why he was    extremely fit    and   his  image
2024-02-02 22:58:08,112 	Text Alignment  :	I    I  I   I  S      S         S      S     S    S    
2024-02-02 22:58:08,112 ========================================================================================================================
2024-02-02 22:58:08,112 Logging Sequence: 109_64.00
2024-02-02 22:58:08,112 	Gloss Reference :	A B+C+D+E
2024-02-02 22:58:08,113 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:58:08,113 	Gloss Alignment :	         
2024-02-02 22:58:08,113 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:58:08,114 	Text Reference  :	the ******* 2    players as        well as   the entire kkr     team  have been  quarantined
2024-02-02 22:58:08,114 	Text Hypothesis :	the medical team is      extremely fit  when the ****** players after the  covid team       
2024-02-02 22:58:08,114 	Text Alignment  :	    I       S    S       S         S    S        D      S       S     S    S     S          
2024-02-02 22:58:08,114 ========================================================================================================================
2024-02-02 22:58:08,114 Logging Sequence: 84_108.00
2024-02-02 22:58:08,114 	Gloss Reference :	A B+C+D+E
2024-02-02 22:58:08,114 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:58:08,114 	Gloss Alignment :	         
2024-02-02 22:58:08,116 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:58:08,116 	Text Reference  :	so in order to show their protest they covered their mouth in the photos which then went viral
2024-02-02 22:58:08,116 	Text Hypothesis :	** ** ***** ** **** ***** ******* **** ******* ***** ***** ** the ****** ***** news went viral
2024-02-02 22:58:08,116 	Text Alignment  :	D  D  D     D  D    D     D       D    D       D     D     D      D      D     S              
2024-02-02 22:58:08,117 ========================================================================================================================
2024-02-02 22:58:08,117 Logging Sequence: 115_24.00
2024-02-02 22:58:08,117 	Gloss Reference :	A B+C+D+E
2024-02-02 22:58:08,117 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:58:08,117 	Gloss Alignment :	         
2024-02-02 22:58:08,117 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:58:08,118 	Text Reference  :	bumrah also did not participate in   the  5  match t20    series
2024-02-02 22:58:08,118 	Text Hypothesis :	****** this was the first       time when he was   widely shared
2024-02-02 22:58:08,118 	Text Alignment  :	D      S    S   S   S           S    S    S  S     S      S     
2024-02-02 22:58:08,118 ========================================================================================================================
2024-02-02 22:58:08,119 Logging Sequence: 96_129.00
2024-02-02 22:58:08,119 	Gloss Reference :	A B+C+D+E
2024-02-02 22:58:08,119 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 22:58:08,119 	Gloss Alignment :	         
2024-02-02 22:58:08,119 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 22:58:08,120 	Text Reference  :	** ** ******* viewers were very stressed
2024-02-02 22:58:08,120 	Text Hypothesis :	it is because dhoni   was  very excited 
2024-02-02 22:58:08,120 	Text Alignment  :	I  I  I       S       S         S       
2024-02-02 22:58:08,120 ========================================================================================================================
2024-02-02 22:58:10,972 Epoch 256: Total Training Recognition Loss 0.06  Total Training Translation Loss 22.32 
2024-02-02 22:58:10,972 EPOCH 257
2024-02-02 22:58:13,650 [Epoch: 257 Step: 00034100] Batch Recognition Loss:   0.000737 => Gls Tokens per Sec:     1553 || Batch Translation Loss:   0.215935 => Txt Tokens per Sec:     4246 || Lr: 0.000100
2024-02-02 22:58:18,326 Epoch 257: Total Training Recognition Loss 0.09  Total Training Translation Loss 24.48 
2024-02-02 22:58:18,327 EPOCH 258
2024-02-02 22:58:19,399 [Epoch: 258 Step: 00034200] Batch Recognition Loss:   0.000252 => Gls Tokens per Sec:     1421 || Batch Translation Loss:   0.262374 => Txt Tokens per Sec:     3949 || Lr: 0.000100
2024-02-02 22:58:24,992 [Epoch: 258 Step: 00034300] Batch Recognition Loss:   0.000285 => Gls Tokens per Sec:     1430 || Batch Translation Loss:   0.088917 => Txt Tokens per Sec:     3971 || Lr: 0.000100
2024-02-02 22:58:25,859 Epoch 258: Total Training Recognition Loss 0.06  Total Training Translation Loss 25.62 
2024-02-02 22:58:25,859 EPOCH 259
2024-02-02 22:58:30,505 [Epoch: 259 Step: 00034400] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:     1482 || Batch Translation Loss:   0.040639 => Txt Tokens per Sec:     4149 || Lr: 0.000100
2024-02-02 22:58:33,033 Epoch 259: Total Training Recognition Loss 0.07  Total Training Translation Loss 17.62 
2024-02-02 22:58:33,034 EPOCH 260
2024-02-02 22:58:36,161 [Epoch: 260 Step: 00034500] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     1357 || Batch Translation Loss:   0.068429 => Txt Tokens per Sec:     3818 || Lr: 0.000100
2024-02-02 22:58:40,400 Epoch 260: Total Training Recognition Loss 0.04  Total Training Translation Loss 16.81 
2024-02-02 22:58:40,400 EPOCH 261
2024-02-02 22:58:41,464 [Epoch: 261 Step: 00034600] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     1506 || Batch Translation Loss:   0.046493 => Txt Tokens per Sec:     4139 || Lr: 0.000100
2024-02-02 22:58:47,101 [Epoch: 261 Step: 00034700] Batch Recognition Loss:   0.002819 => Gls Tokens per Sec:     1418 || Batch Translation Loss:   0.233159 => Txt Tokens per Sec:     3921 || Lr: 0.000100
2024-02-02 22:58:47,889 Epoch 261: Total Training Recognition Loss 0.05  Total Training Translation Loss 12.61 
2024-02-02 22:58:47,889 EPOCH 262
2024-02-02 22:58:52,507 [Epoch: 262 Step: 00034800] Batch Recognition Loss:   0.000677 => Gls Tokens per Sec:     1508 || Batch Translation Loss:   0.033776 => Txt Tokens per Sec:     4222 || Lr: 0.000100
2024-02-02 22:58:55,138 Epoch 262: Total Training Recognition Loss 0.05  Total Training Translation Loss 14.21 
2024-02-02 22:58:55,138 EPOCH 263
2024-02-02 22:58:58,240 [Epoch: 263 Step: 00034900] Batch Recognition Loss:   0.000866 => Gls Tokens per Sec:     1393 || Batch Translation Loss:   0.775460 => Txt Tokens per Sec:     4009 || Lr: 0.000100
2024-02-02 22:59:02,769 Epoch 263: Total Training Recognition Loss 0.05  Total Training Translation Loss 19.11 
2024-02-02 22:59:02,769 EPOCH 264
2024-02-02 22:59:03,848 [Epoch: 264 Step: 00035000] Batch Recognition Loss:   0.004261 => Gls Tokens per Sec:     1557 || Batch Translation Loss:   0.336905 => Txt Tokens per Sec:     4185 || Lr: 0.000100
2024-02-02 22:59:09,619 [Epoch: 264 Step: 00035100] Batch Recognition Loss:   0.000677 => Gls Tokens per Sec:     1386 || Batch Translation Loss:   0.062484 => Txt Tokens per Sec:     3842 || Lr: 0.000100
2024-02-02 22:59:10,409 Epoch 264: Total Training Recognition Loss 0.06  Total Training Translation Loss 19.22 
2024-02-02 22:59:10,409 EPOCH 265
2024-02-02 22:59:15,522 [Epoch: 265 Step: 00035200] Batch Recognition Loss:   0.000516 => Gls Tokens per Sec:     1378 || Batch Translation Loss:   0.069105 => Txt Tokens per Sec:     3776 || Lr: 0.000100
2024-02-02 22:59:18,114 Epoch 265: Total Training Recognition Loss 0.06  Total Training Translation Loss 21.30 
2024-02-02 22:59:18,116 EPOCH 266
2024-02-02 22:59:21,164 [Epoch: 266 Step: 00035300] Batch Recognition Loss:   0.000219 => Gls Tokens per Sec:     1444 || Batch Translation Loss:   0.298089 => Txt Tokens per Sec:     3931 || Lr: 0.000100
2024-02-02 22:59:25,557 Epoch 266: Total Training Recognition Loss 0.04  Total Training Translation Loss 20.86 
2024-02-02 22:59:25,558 EPOCH 267
2024-02-02 22:59:26,891 [Epoch: 267 Step: 00035400] Batch Recognition Loss:   0.000443 => Gls Tokens per Sec:     1321 || Batch Translation Loss:   0.129449 => Txt Tokens per Sec:     3590 || Lr: 0.000100
2024-02-02 22:59:32,694 [Epoch: 267 Step: 00035500] Batch Recognition Loss:   0.000490 => Gls Tokens per Sec:     1377 || Batch Translation Loss:   0.103145 => Txt Tokens per Sec:     3850 || Lr: 0.000100
2024-02-02 22:59:33,280 Epoch 267: Total Training Recognition Loss 0.05  Total Training Translation Loss 22.44 
2024-02-02 22:59:33,280 EPOCH 268
2024-02-02 22:59:37,972 [Epoch: 268 Step: 00035600] Batch Recognition Loss:   0.000268 => Gls Tokens per Sec:     1518 || Batch Translation Loss:   0.179284 => Txt Tokens per Sec:     4145 || Lr: 0.000100
2024-02-02 22:59:40,679 Epoch 268: Total Training Recognition Loss 0.07  Total Training Translation Loss 26.78 
2024-02-02 22:59:40,679 EPOCH 269
2024-02-02 22:59:43,730 [Epoch: 269 Step: 00035700] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:     1469 || Batch Translation Loss:   0.122865 => Txt Tokens per Sec:     4031 || Lr: 0.000100
2024-02-02 22:59:47,858 Epoch 269: Total Training Recognition Loss 0.05  Total Training Translation Loss 19.24 
2024-02-02 22:59:47,858 EPOCH 270
2024-02-02 22:59:49,250 [Epoch: 270 Step: 00035800] Batch Recognition Loss:   0.000264 => Gls Tokens per Sec:     1324 || Batch Translation Loss:   0.096855 => Txt Tokens per Sec:     3656 || Lr: 0.000100
2024-02-02 22:59:55,039 [Epoch: 270 Step: 00035900] Batch Recognition Loss:   0.000424 => Gls Tokens per Sec:     1380 || Batch Translation Loss:   0.023765 => Txt Tokens per Sec:     3870 || Lr: 0.000100
2024-02-02 22:59:55,611 Epoch 270: Total Training Recognition Loss 0.06  Total Training Translation Loss 19.88 
2024-02-02 22:59:55,611 EPOCH 271
2024-02-02 23:00:00,556 [Epoch: 271 Step: 00036000] Batch Recognition Loss:   0.000207 => Gls Tokens per Sec:     1457 || Batch Translation Loss:   0.301038 => Txt Tokens per Sec:     4013 || Lr: 0.000100
2024-02-02 23:00:11,298 Validation result at epoch 271, step    36000: duration: 10.7418s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00298	Translation Loss: 97409.25000	PPL: 17115.64844
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.93	(BLEU-1: 10.13,	BLEU-2: 3.42,	BLEU-3: 1.58,	BLEU-4: 0.93)
	CHRF 16.38	ROUGE 9.26
2024-02-02 23:00:11,299 Logging Recognition and Translation Outputs
2024-02-02 23:00:11,299 ========================================================================================================================
2024-02-02 23:00:11,299 Logging Sequence: 78_198.00
2024-02-02 23:00:11,300 	Gloss Reference :	A B+C+D+E
2024-02-02 23:00:11,300 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:00:11,300 	Gloss Alignment :	         
2024-02-02 23:00:11,300 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:00:11,300 	Text Reference  :	they have been  flooded    with     congratulations comments
2024-02-02 23:00:11,301 	Text Hypothesis :	**** **** queen elizabeth' grandson prince          william 
2024-02-02 23:00:11,301 	Text Alignment  :	D    D    S     S          S        S               S       
2024-02-02 23:00:11,301 ========================================================================================================================
2024-02-02 23:00:11,301 Logging Sequence: 145_216.00
2024-02-02 23:00:11,301 	Gloss Reference :	A B+C+D+E
2024-02-02 23:00:11,301 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:00:11,302 	Gloss Alignment :	         
2024-02-02 23:00:11,302 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:00:11,302 	Text Reference  :	asking him to include sameeha in the world championship as   she    was a  talented    athlete     
2024-02-02 23:00:11,303 	Text Hypothesis :	****** *** ** ******* ******* ** the ***** spinner      anil kumble has an interesting superstition
2024-02-02 23:00:11,303 	Text Alignment  :	D      D   D  D       D       D      D     S            S    S      S   S  S           S           
2024-02-02 23:00:11,303 ========================================================================================================================
2024-02-02 23:00:11,303 Logging Sequence: 70_137.00
2024-02-02 23:00:11,303 	Gloss Reference :	A B+C+D+E
2024-02-02 23:00:11,303 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:00:11,303 	Gloss Alignment :	         
2024-02-02 23:00:11,303 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:00:11,304 	Text Reference  :	the small gesture appeared to      encourage people to    drink    water instead of  aerated drinks    
2024-02-02 23:00:11,304 	Text Hypothesis :	*** ***** however a        violent brawl     in     tokyo olympics which is      the brand   ambassador
2024-02-02 23:00:11,305 	Text Alignment  :	D   D     S       S        S       S         S      S     S        S     S       S   S       S         
2024-02-02 23:00:11,305 ========================================================================================================================
2024-02-02 23:00:11,305 Logging Sequence: 119_20.00
2024-02-02 23:00:11,305 	Gloss Reference :	A B+C+D+E
2024-02-02 23:00:11,305 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:00:11,305 	Gloss Alignment :	         
2024-02-02 23:00:11,305 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:00:11,306 	Text Reference  :	messi intended to gift something to all the players and   the staff to special to celebrate the    moment      
2024-02-02 23:00:11,306 	Text Hypothesis :	***** ******** ** **** ********* as per the ******* rules the ***** ** ******* ** ********* team's achievements
2024-02-02 23:00:11,306 	Text Alignment  :	D     D        D  D    D         S  S       D       S         D     D  D       D  D         S      S           
2024-02-02 23:00:11,307 ========================================================================================================================
2024-02-02 23:00:11,307 Logging Sequence: 106_15.00
2024-02-02 23:00:11,307 	Gloss Reference :	A B+C+D+E
2024-02-02 23:00:11,307 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:00:11,307 	Gloss Alignment :	         
2024-02-02 23:00:11,307 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:00:11,308 	Text Reference  :	*** but what about     women's cricket earlier we never spoke about it   
2024-02-02 23:00:11,308 	Text Hypothesis :	and it  was  surprised that    the     times   of the   oc    be    given
2024-02-02 23:00:11,308 	Text Alignment  :	I   S   S    S         S       S       S       S  S     S     S     S    
2024-02-02 23:00:11,308 ========================================================================================================================
2024-02-02 23:00:13,943 Epoch 271: Total Training Recognition Loss 0.06  Total Training Translation Loss 20.02 
2024-02-02 23:00:13,943 EPOCH 272
2024-02-02 23:00:16,917 [Epoch: 272 Step: 00036100] Batch Recognition Loss:   0.000312 => Gls Tokens per Sec:     1534 || Batch Translation Loss:   0.062098 => Txt Tokens per Sec:     4132 || Lr: 0.000100
2024-02-02 23:00:21,269 Epoch 272: Total Training Recognition Loss 0.04  Total Training Translation Loss 14.91 
2024-02-02 23:00:21,270 EPOCH 273
2024-02-02 23:00:22,626 [Epoch: 273 Step: 00036200] Batch Recognition Loss:   0.000367 => Gls Tokens per Sec:     1416 || Batch Translation Loss:   0.091415 => Txt Tokens per Sec:     4061 || Lr: 0.000100
2024-02-02 23:00:28,548 [Epoch: 273 Step: 00036300] Batch Recognition Loss:   0.000362 => Gls Tokens per Sec:     1349 || Batch Translation Loss:   0.079400 => Txt Tokens per Sec:     3733 || Lr: 0.000100
2024-02-02 23:00:29,028 Epoch 273: Total Training Recognition Loss 0.04  Total Training Translation Loss 13.12 
2024-02-02 23:00:29,029 EPOCH 274
2024-02-02 23:00:33,943 [Epoch: 274 Step: 00036400] Batch Recognition Loss:   0.000351 => Gls Tokens per Sec:     1482 || Batch Translation Loss:   0.054254 => Txt Tokens per Sec:     4129 || Lr: 0.000100
2024-02-02 23:00:36,382 Epoch 274: Total Training Recognition Loss 0.05  Total Training Translation Loss 12.49 
2024-02-02 23:00:36,383 EPOCH 275
2024-02-02 23:00:39,660 [Epoch: 275 Step: 00036500] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:     1416 || Batch Translation Loss:   0.047372 => Txt Tokens per Sec:     3978 || Lr: 0.000100
2024-02-02 23:00:43,770 Epoch 275: Total Training Recognition Loss 0.05  Total Training Translation Loss 10.46 
2024-02-02 23:00:43,770 EPOCH 276
2024-02-02 23:00:45,321 [Epoch: 276 Step: 00036600] Batch Recognition Loss:   0.000306 => Gls Tokens per Sec:     1291 || Batch Translation Loss:   0.062278 => Txt Tokens per Sec:     3716 || Lr: 0.000100
2024-02-02 23:00:51,054 [Epoch: 276 Step: 00036700] Batch Recognition Loss:   0.001296 => Gls Tokens per Sec:     1394 || Batch Translation Loss:   0.140790 => Txt Tokens per Sec:     3854 || Lr: 0.000100
2024-02-02 23:00:51,546 Epoch 276: Total Training Recognition Loss 0.05  Total Training Translation Loss 16.29 
2024-02-02 23:00:51,547 EPOCH 277
2024-02-02 23:00:56,594 [Epoch: 277 Step: 00036800] Batch Recognition Loss:   0.000348 => Gls Tokens per Sec:     1459 || Batch Translation Loss:   0.121238 => Txt Tokens per Sec:     4028 || Lr: 0.000100
2024-02-02 23:00:58,974 Epoch 277: Total Training Recognition Loss 0.06  Total Training Translation Loss 26.74 
2024-02-02 23:00:58,975 EPOCH 278
2024-02-02 23:01:02,401 [Epoch: 278 Step: 00036900] Batch Recognition Loss:   0.000595 => Gls Tokens per Sec:     1378 || Batch Translation Loss:   0.113507 => Txt Tokens per Sec:     3951 || Lr: 0.000100
2024-02-02 23:01:06,446 Epoch 278: Total Training Recognition Loss 0.07  Total Training Translation Loss 30.04 
2024-02-02 23:01:06,446 EPOCH 279
2024-02-02 23:01:07,826 [Epoch: 279 Step: 00037000] Batch Recognition Loss:   0.000727 => Gls Tokens per Sec:     1508 || Batch Translation Loss:   0.194129 => Txt Tokens per Sec:     4007 || Lr: 0.000100
2024-02-02 23:01:13,713 [Epoch: 279 Step: 00037100] Batch Recognition Loss:   0.000534 => Gls Tokens per Sec:     1358 || Batch Translation Loss:   0.095739 => Txt Tokens per Sec:     3809 || Lr: 0.000100
2024-02-02 23:01:14,161 Epoch 279: Total Training Recognition Loss 0.08  Total Training Translation Loss 26.86 
2024-02-02 23:01:14,162 EPOCH 280
2024-02-02 23:01:19,487 [Epoch: 280 Step: 00037200] Batch Recognition Loss:   0.000457 => Gls Tokens per Sec:     1397 || Batch Translation Loss:   0.064372 => Txt Tokens per Sec:     3880 || Lr: 0.000100
2024-02-02 23:01:21,683 Epoch 280: Total Training Recognition Loss 0.07  Total Training Translation Loss 14.76 
2024-02-02 23:01:21,683 EPOCH 281
2024-02-02 23:01:25,039 [Epoch: 281 Step: 00037300] Batch Recognition Loss:   0.000404 => Gls Tokens per Sec:     1431 || Batch Translation Loss:   0.068110 => Txt Tokens per Sec:     3890 || Lr: 0.000100
2024-02-02 23:01:29,109 Epoch 281: Total Training Recognition Loss 0.07  Total Training Translation Loss 16.04 
2024-02-02 23:01:29,110 EPOCH 282
2024-02-02 23:01:30,639 [Epoch: 282 Step: 00037400] Batch Recognition Loss:   0.000274 => Gls Tokens per Sec:     1414 || Batch Translation Loss:   0.050363 => Txt Tokens per Sec:     3781 || Lr: 0.000100
2024-02-02 23:01:36,357 [Epoch: 282 Step: 00037500] Batch Recognition Loss:   0.000363 => Gls Tokens per Sec:     1399 || Batch Translation Loss:   0.094713 => Txt Tokens per Sec:     3921 || Lr: 0.000100
2024-02-02 23:01:36,779 Epoch 282: Total Training Recognition Loss 0.05  Total Training Translation Loss 16.42 
2024-02-02 23:01:36,779 EPOCH 283
2024-02-02 23:01:42,257 [Epoch: 283 Step: 00037600] Batch Recognition Loss:   0.001120 => Gls Tokens per Sec:     1373 || Batch Translation Loss:   0.548482 => Txt Tokens per Sec:     3779 || Lr: 0.000100
2024-02-02 23:01:44,480 Epoch 283: Total Training Recognition Loss 0.05  Total Training Translation Loss 11.67 
2024-02-02 23:01:44,480 EPOCH 284
2024-02-02 23:01:47,713 [Epoch: 284 Step: 00037700] Batch Recognition Loss:   0.000436 => Gls Tokens per Sec:     1510 || Batch Translation Loss:   0.189344 => Txt Tokens per Sec:     4212 || Lr: 0.000100
2024-02-02 23:01:51,896 Epoch 284: Total Training Recognition Loss 0.05  Total Training Translation Loss 16.18 
2024-02-02 23:01:51,897 EPOCH 285
2024-02-02 23:01:53,526 [Epoch: 285 Step: 00037800] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:     1375 || Batch Translation Loss:   0.034899 => Txt Tokens per Sec:     3793 || Lr: 0.000100
2024-02-02 23:01:59,321 [Epoch: 285 Step: 00037900] Batch Recognition Loss:   0.000293 => Gls Tokens per Sec:     1379 || Batch Translation Loss:   0.058561 => Txt Tokens per Sec:     3841 || Lr: 0.000100
2024-02-02 23:01:59,578 Epoch 285: Total Training Recognition Loss 0.05  Total Training Translation Loss 20.39 
2024-02-02 23:01:59,578 EPOCH 286
2024-02-02 23:02:05,010 [Epoch: 286 Step: 00038000] Batch Recognition Loss:   0.000326 => Gls Tokens per Sec:     1400 || Batch Translation Loss:   0.076228 => Txt Tokens per Sec:     3864 || Lr: 0.000100
2024-02-02 23:02:15,805 Validation result at epoch 286, step    38000: duration: 10.7952s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00333	Translation Loss: 93850.39062	PPL: 11987.41895
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.56	(BLEU-1: 10.10,	BLEU-2: 2.86,	BLEU-3: 1.11,	BLEU-4: 0.56)
	CHRF 16.40	ROUGE 8.67
2024-02-02 23:02:15,807 Logging Recognition and Translation Outputs
2024-02-02 23:02:15,807 ========================================================================================================================
2024-02-02 23:02:15,807 Logging Sequence: 72_194.00
2024-02-02 23:02:15,807 	Gloss Reference :	A B+C+D+E
2024-02-02 23:02:15,808 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:02:15,808 	Gloss Alignment :	         
2024-02-02 23:02:15,808 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:02:15,809 	Text Reference  :	shah told her   to   do what she    wants  and filed a police complaint against her
2024-02-02 23:02:15,809 	Text Hypothesis :	**** **** babar kept me in   rented houses and ***** * ****** ********* ******* ***
2024-02-02 23:02:15,809 	Text Alignment  :	D    D    S     S    S  S    S      S          D     D D      D         D       D  
2024-02-02 23:02:15,809 ========================================================================================================================
2024-02-02 23:02:15,809 Logging Sequence: 108_59.00
2024-02-02 23:02:15,809 	Gloss Reference :	A B+C+D+E
2024-02-02 23:02:15,810 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:02:15,810 	Gloss Alignment :	         
2024-02-02 23:02:15,810 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:02:15,812 	Text Reference  :	ishan kishan remained the biggest buy of ipl as    mumbai indians paid      a   whopping rs      1525  crore   to   keep    him    
2024-02-02 23:02:15,812 	Text Hypothesis :	***** ****** ******** *** ******* *** ** a   close friend andrew  neophitou and warne'   manager james erskine kept india's victory
2024-02-02 23:02:15,812 	Text Alignment  :	D     D      D        D   D       D   D  S   S     S      S       S         S   S        S       S     S       S    S       S      
2024-02-02 23:02:15,812 ========================================================================================================================
2024-02-02 23:02:15,812 Logging Sequence: 109_10.00
2024-02-02 23:02:15,813 	Gloss Reference :	A B+C+D+E
2024-02-02 23:02:15,813 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:02:15,813 	Gloss Alignment :	         
2024-02-02 23:02:15,813 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:02:15,815 	Text Reference  :	******* *** was   scheduled to be played at   the   narendra modi stadium in   ahmedabad
2024-02-02 23:02:15,815 	Text Hypothesis :	however the match ended     in a  year   1930 which will     be   banned  from well     
2024-02-02 23:02:15,815 	Text Alignment  :	I       I   S     S         S  S  S      S    S     S        S    S       S    S        
2024-02-02 23:02:15,815 ========================================================================================================================
2024-02-02 23:02:15,815 Logging Sequence: 103_202.00
2024-02-02 23:02:15,815 	Gloss Reference :	A B+C+D+E
2024-02-02 23:02:15,816 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:02:15,816 	Gloss Alignment :	         
2024-02-02 23:02:15,816 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:02:15,818 	Text Reference  :	india in total has   won       61 medals including 22 gold    medals 16  silver medals 23  bronze       medals
2024-02-02 23:02:15,818 	Text Hypothesis :	***** ** ***** queen elizabeth ii the    queen     of england is     the patron of     the commonwealth games 
2024-02-02 23:02:15,818 	Text Alignment  :	D     D  D     S     S         S  S      S         S  S       S      S   S      S      S   S            S     
2024-02-02 23:02:15,818 ========================================================================================================================
2024-02-02 23:02:15,819 Logging Sequence: 149_77.00
2024-02-02 23:02:15,819 	Gloss Reference :	A B+C+D+E
2024-02-02 23:02:15,819 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:02:15,819 	Gloss Alignment :	         
2024-02-02 23:02:15,819 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:02:15,821 	Text Reference  :	and arrested danushka for alleged sexual assault of a *** 29   year old  woman whose  name    has not been disclosed
2024-02-02 23:02:15,821 	Text Hypothesis :	*** ******** ******** *** ******* ****** but     if a few days went into the   afghan picture of  50  of   him      
2024-02-02 23:02:15,821 	Text Alignment  :	D   D        D        D   D       D      S       S    I   S    S    S    S     S      S       S   S   S    S        
2024-02-02 23:02:15,822 ========================================================================================================================
2024-02-02 23:02:18,006 Epoch 286: Total Training Recognition Loss 0.06  Total Training Translation Loss 21.26 
2024-02-02 23:02:18,006 EPOCH 287
2024-02-02 23:02:21,600 [Epoch: 287 Step: 00038100] Batch Recognition Loss:   0.000381 => Gls Tokens per Sec:     1381 || Batch Translation Loss:   0.139691 => Txt Tokens per Sec:     3853 || Lr: 0.000100
2024-02-02 23:02:25,452 Epoch 287: Total Training Recognition Loss 0.04  Total Training Translation Loss 18.41 
2024-02-02 23:02:25,453 EPOCH 288
2024-02-02 23:02:27,041 [Epoch: 288 Step: 00038200] Batch Recognition Loss:   0.000297 => Gls Tokens per Sec:     1462 || Batch Translation Loss:   0.052739 => Txt Tokens per Sec:     4178 || Lr: 0.000100
2024-02-02 23:02:32,719 [Epoch: 288 Step: 00038300] Batch Recognition Loss:   0.000117 => Gls Tokens per Sec:     1407 || Batch Translation Loss:   0.038368 => Txt Tokens per Sec:     3860 || Lr: 0.000100
2024-02-02 23:02:32,961 Epoch 288: Total Training Recognition Loss 0.05  Total Training Translation Loss 13.52 
2024-02-02 23:02:32,961 EPOCH 289
2024-02-02 23:02:38,368 [Epoch: 289 Step: 00038400] Batch Recognition Loss:   0.000428 => Gls Tokens per Sec:     1421 || Batch Translation Loss:   0.093294 => Txt Tokens per Sec:     3955 || Lr: 0.000100
2024-02-02 23:02:40,516 Epoch 289: Total Training Recognition Loss 0.05  Total Training Translation Loss 18.31 
2024-02-02 23:02:40,517 EPOCH 290
2024-02-02 23:02:43,898 [Epoch: 290 Step: 00038500] Batch Recognition Loss:   0.000573 => Gls Tokens per Sec:     1491 || Batch Translation Loss:   0.057333 => Txt Tokens per Sec:     4223 || Lr: 0.000100
2024-02-02 23:02:47,880 Epoch 290: Total Training Recognition Loss 0.05  Total Training Translation Loss 15.64 
2024-02-02 23:02:47,881 EPOCH 291
2024-02-02 23:02:49,628 [Epoch: 291 Step: 00038600] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     1375 || Batch Translation Loss:   0.217159 => Txt Tokens per Sec:     4026 || Lr: 0.000100
2024-02-02 23:02:55,257 [Epoch: 291 Step: 00038700] Batch Recognition Loss:   0.000454 => Gls Tokens per Sec:     1420 || Batch Translation Loss:   0.041972 => Txt Tokens per Sec:     3886 || Lr: 0.000100
2024-02-02 23:02:55,395 Epoch 291: Total Training Recognition Loss 0.05  Total Training Translation Loss 20.51 
2024-02-02 23:02:55,395 EPOCH 292
2024-02-02 23:03:00,931 [Epoch: 292 Step: 00038800] Batch Recognition Loss:   0.000252 => Gls Tokens per Sec:     1402 || Batch Translation Loss:   0.074976 => Txt Tokens per Sec:     3889 || Lr: 0.000100
2024-02-02 23:03:02,968 Epoch 292: Total Training Recognition Loss 0.06  Total Training Translation Loss 27.95 
2024-02-02 23:03:02,969 EPOCH 293
2024-02-02 23:03:06,443 [Epoch: 293 Step: 00038900] Batch Recognition Loss:   0.000289 => Gls Tokens per Sec:     1475 || Batch Translation Loss:   0.195062 => Txt Tokens per Sec:     4172 || Lr: 0.000100
2024-02-02 23:03:10,263 Epoch 293: Total Training Recognition Loss 0.07  Total Training Translation Loss 24.86 
2024-02-02 23:03:10,264 EPOCH 294
2024-02-02 23:03:11,795 [Epoch: 294 Step: 00039000] Batch Recognition Loss:   0.000339 => Gls Tokens per Sec:     1621 || Batch Translation Loss:   0.052594 => Txt Tokens per Sec:     4226 || Lr: 0.000100
2024-02-02 23:03:17,760 [Epoch: 294 Step: 00039100] Batch Recognition Loss:   0.000193 => Gls Tokens per Sec:     1339 || Batch Translation Loss:   0.059979 => Txt Tokens per Sec:     3778 || Lr: 0.000100
2024-02-02 23:03:17,909 Epoch 294: Total Training Recognition Loss 0.08  Total Training Translation Loss 18.14 
2024-02-02 23:03:17,909 EPOCH 295
2024-02-02 23:03:23,326 [Epoch: 295 Step: 00039200] Batch Recognition Loss:   0.000388 => Gls Tokens per Sec:     1448 || Batch Translation Loss:   0.218351 => Txt Tokens per Sec:     4014 || Lr: 0.000100
2024-02-02 23:03:25,176 Epoch 295: Total Training Recognition Loss 0.06  Total Training Translation Loss 18.69 
2024-02-02 23:03:25,176 EPOCH 296
2024-02-02 23:03:28,805 [Epoch: 296 Step: 00039300] Batch Recognition Loss:   0.000304 => Gls Tokens per Sec:     1434 || Batch Translation Loss:   0.075193 => Txt Tokens per Sec:     3903 || Lr: 0.000100
2024-02-02 23:03:32,818 Epoch 296: Total Training Recognition Loss 0.05  Total Training Translation Loss 15.60 
2024-02-02 23:03:32,818 EPOCH 297
2024-02-02 23:03:34,516 [Epoch: 297 Step: 00039400] Batch Recognition Loss:   0.000387 => Gls Tokens per Sec:     1509 || Batch Translation Loss:   0.716622 => Txt Tokens per Sec:     4312 || Lr: 0.000100
2024-02-02 23:03:40,405 [Epoch: 297 Step: 00039500] Batch Recognition Loss:   0.000350 => Gls Tokens per Sec:     1357 || Batch Translation Loss:   0.115104 => Txt Tokens per Sec:     3732 || Lr: 0.000100
2024-02-02 23:03:40,445 Epoch 297: Total Training Recognition Loss 0.05  Total Training Translation Loss 17.18 
2024-02-02 23:03:40,445 EPOCH 298
2024-02-02 23:03:46,169 [Epoch: 298 Step: 00039600] Batch Recognition Loss:   0.000317 => Gls Tokens per Sec:     1384 || Batch Translation Loss:   0.041563 => Txt Tokens per Sec:     3840 || Lr: 0.000100
2024-02-02 23:03:48,075 Epoch 298: Total Training Recognition Loss 0.05  Total Training Translation Loss 16.83 
2024-02-02 23:03:48,075 EPOCH 299
2024-02-02 23:03:51,770 [Epoch: 299 Step: 00039700] Batch Recognition Loss:   0.000382 => Gls Tokens per Sec:     1429 || Batch Translation Loss:   0.093284 => Txt Tokens per Sec:     4105 || Lr: 0.000100
2024-02-02 23:03:55,538 Epoch 299: Total Training Recognition Loss 0.04  Total Training Translation Loss 17.76 
2024-02-02 23:03:55,539 EPOCH 300
2024-02-02 23:03:57,398 [Epoch: 300 Step: 00039800] Batch Recognition Loss:   0.000342 => Gls Tokens per Sec:     1421 || Batch Translation Loss:   0.034634 => Txt Tokens per Sec:     3979 || Lr: 0.000100
2024-02-02 23:04:03,350 [Epoch: 300 Step: 00039900] Batch Recognition Loss:   0.000321 => Gls Tokens per Sec:     1343 || Batch Translation Loss:   1.685457 => Txt Tokens per Sec:     3716 || Lr: 0.000100
2024-02-02 23:04:03,351 Epoch 300: Total Training Recognition Loss 0.05  Total Training Translation Loss 11.23 
2024-02-02 23:04:03,351 EPOCH 301
2024-02-02 23:04:08,988 [Epoch: 301 Step: 00040000] Batch Recognition Loss:   0.000272 => Gls Tokens per Sec:     1419 || Batch Translation Loss:   0.058308 => Txt Tokens per Sec:     3940 || Lr: 0.000100
2024-02-02 23:04:20,479 Validation result at epoch 301, step    40000: duration: 11.4904s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00302	Translation Loss: 94759.69531	PPL: 13129.38086
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.77	(BLEU-1: 11.08,	BLEU-2: 3.51,	BLEU-3: 1.53,	BLEU-4: 0.77)
	CHRF 17.09	ROUGE 9.21
2024-02-02 23:04:20,480 Logging Recognition and Translation Outputs
2024-02-02 23:04:20,480 ========================================================================================================================
2024-02-02 23:04:20,480 Logging Sequence: 123_104.00
2024-02-02 23:04:20,481 	Gloss Reference :	A B+C+D+E
2024-02-02 23:04:20,481 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:04:20,481 	Gloss Alignment :	         
2024-02-02 23:04:20,481 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:04:20,482 	Text Reference  :	*** the       car was presented to *** ****** *** the  former india cricketer from an    unknown person
2024-02-02 23:04:20,483 	Text Hypothesis :	now similarly on  his way       to win neeraj has been proved that  it        is   named as      well  
2024-02-02 23:04:20,483 	Text Alignment  :	I   S         S   S   S            I   I      I   S    S      S     S         S    S     S       S     
2024-02-02 23:04:20,483 ========================================================================================================================
2024-02-02 23:04:20,483 Logging Sequence: 107_23.00
2024-02-02 23:04:20,483 	Gloss Reference :	A B+C+D+E
2024-02-02 23:04:20,483 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:04:20,483 	Gloss Alignment :	         
2024-02-02 23:04:20,483 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:04:20,484 	Text Reference  :	and viktor lilov who   is *** also from   the usa  
2024-02-02 23:04:20,484 	Text Hypothesis :	*** ****** ***** which is why or   around 8   crore
2024-02-02 23:04:20,484 	Text Alignment  :	D   D      D     S        I   S    S      S   S    
2024-02-02 23:04:20,484 ========================================================================================================================
2024-02-02 23:04:20,484 Logging Sequence: 134_212.00
2024-02-02 23:04:20,485 	Gloss Reference :	A B+C+D+E
2024-02-02 23:04:20,485 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:04:20,485 	Gloss Alignment :	         
2024-02-02 23:04:20,485 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:04:20,486 	Text Reference  :	******* dhanush said that he *** ** ****** practises little yoga
2024-02-02 23:04:20,486 	Text Hypothesis :	indians love    him  that he may be played at        the    way 
2024-02-02 23:04:20,486 	Text Alignment  :	I       S       S            I   I  I      S         S      S   
2024-02-02 23:04:20,486 ========================================================================================================================
2024-02-02 23:04:20,486 Logging Sequence: 165_577.00
2024-02-02 23:04:20,486 	Gloss Reference :	A B+C+D+E
2024-02-02 23:04:20,487 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:04:20,487 	Gloss Alignment :	         
2024-02-02 23:04:20,487 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:04:20,488 	Text Reference  :	**** then after 28    years india won    the     world cup   again in  2011   
2024-02-02 23:04:20,488 	Text Hypothesis :	pant made his   lucky so    he    always carried a     video to    her victory
2024-02-02 23:04:20,488 	Text Alignment  :	I    S    S     S     S     S     S      S       S     S     S     S   S      
2024-02-02 23:04:20,488 ========================================================================================================================
2024-02-02 23:04:20,488 Logging Sequence: 88_142.00
2024-02-02 23:04:20,489 	Gloss Reference :	A B+C+D+E
2024-02-02 23:04:20,489 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:04:20,489 	Gloss Alignment :	         
2024-02-02 23:04:20,489 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:04:20,489 	Text Reference  :	******* this    is  because the police does not       do    anything
2024-02-02 23:04:20,489 	Text Hypothesis :	notably rosario has become  a   threat to   celebrate their victory 
2024-02-02 23:04:20,489 	Text Alignment  :	I       S       S   S       S   S      S    S         S     S       
2024-02-02 23:04:20,490 ========================================================================================================================
2024-02-02 23:04:22,524 Epoch 301: Total Training Recognition Loss 0.05  Total Training Translation Loss 15.76 
2024-02-02 23:04:22,525 EPOCH 302
2024-02-02 23:04:26,516 [Epoch: 302 Step: 00040100] Batch Recognition Loss:   0.000271 => Gls Tokens per Sec:     1343 || Batch Translation Loss:   0.036806 => Txt Tokens per Sec:     3812 || Lr: 0.000050
2024-02-02 23:04:30,186 Epoch 302: Total Training Recognition Loss 0.04  Total Training Translation Loss 17.01 
2024-02-02 23:04:30,186 EPOCH 303
2024-02-02 23:04:32,190 [Epoch: 303 Step: 00040200] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     1358 || Batch Translation Loss:   0.048187 => Txt Tokens per Sec:     3830 || Lr: 0.000050
2024-02-02 23:04:37,873 Epoch 303: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.38 
2024-02-02 23:04:37,873 EPOCH 304
2024-02-02 23:04:37,920 [Epoch: 304 Step: 00040300] Batch Recognition Loss:   0.000303 => Gls Tokens per Sec:     1756 || Batch Translation Loss:   0.030464 => Txt Tokens per Sec:     4609 || Lr: 0.000050
2024-02-02 23:04:43,586 [Epoch: 304 Step: 00040400] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     1412 || Batch Translation Loss:   0.016960 => Txt Tokens per Sec:     3926 || Lr: 0.000050
2024-02-02 23:04:45,415 Epoch 304: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.02 
2024-02-02 23:04:45,415 EPOCH 305
2024-02-02 23:04:48,787 [Epoch: 305 Step: 00040500] Batch Recognition Loss:   0.000399 => Gls Tokens per Sec:     1614 || Batch Translation Loss:   0.013277 => Txt Tokens per Sec:     4449 || Lr: 0.000050
2024-02-02 23:04:52,372 Epoch 305: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.56 
2024-02-02 23:04:52,373 EPOCH 306
2024-02-02 23:04:54,153 [Epoch: 306 Step: 00040600] Batch Recognition Loss:   0.000280 => Gls Tokens per Sec:     1575 || Batch Translation Loss:   0.026521 => Txt Tokens per Sec:     4091 || Lr: 0.000050
2024-02-02 23:05:00,162 Epoch 306: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.08 
2024-02-02 23:05:00,163 EPOCH 307
2024-02-02 23:05:00,316 [Epoch: 307 Step: 00040700] Batch Recognition Loss:   0.000116 => Gls Tokens per Sec:     1062 || Batch Translation Loss:   0.010755 => Txt Tokens per Sec:     3212 || Lr: 0.000050
2024-02-02 23:05:05,928 [Epoch: 307 Step: 00040800] Batch Recognition Loss:   0.000371 => Gls Tokens per Sec:     1425 || Batch Translation Loss:   0.174131 => Txt Tokens per Sec:     3977 || Lr: 0.000050
2024-02-02 23:05:07,705 Epoch 307: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.58 
2024-02-02 23:05:07,705 EPOCH 308
2024-02-02 23:05:11,719 [Epoch: 308 Step: 00040900] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:     1375 || Batch Translation Loss:   0.025713 => Txt Tokens per Sec:     3835 || Lr: 0.000050
2024-02-02 23:05:15,126 Epoch 308: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.23 
2024-02-02 23:05:15,126 EPOCH 309
2024-02-02 23:05:17,078 [Epoch: 309 Step: 00041000] Batch Recognition Loss:   0.000422 => Gls Tokens per Sec:     1476 || Batch Translation Loss:   0.008089 => Txt Tokens per Sec:     3976 || Lr: 0.000050
2024-02-02 23:05:22,873 Epoch 309: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.80 
2024-02-02 23:05:22,873 EPOCH 310
2024-02-02 23:05:23,016 [Epoch: 310 Step: 00041100] Batch Recognition Loss:   0.000293 => Gls Tokens per Sec:     1689 || Batch Translation Loss:   0.008134 => Txt Tokens per Sec:     3989 || Lr: 0.000050
2024-02-02 23:05:28,855 [Epoch: 310 Step: 00041200] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     1368 || Batch Translation Loss:   0.017398 => Txt Tokens per Sec:     3818 || Lr: 0.000050
2024-02-02 23:05:30,464 Epoch 310: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.21 
2024-02-02 23:05:30,464 EPOCH 311
2024-02-02 23:05:34,231 [Epoch: 311 Step: 00041300] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:     1487 || Batch Translation Loss:   0.014467 => Txt Tokens per Sec:     4228 || Lr: 0.000050
2024-02-02 23:05:37,921 Epoch 311: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.67 
2024-02-02 23:05:37,922 EPOCH 312
2024-02-02 23:05:40,071 [Epoch: 312 Step: 00041400] Batch Recognition Loss:   0.000922 => Gls Tokens per Sec:     1378 || Batch Translation Loss:   0.036837 => Txt Tokens per Sec:     3930 || Lr: 0.000050
2024-02-02 23:05:45,598 Epoch 312: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.69 
2024-02-02 23:05:45,599 EPOCH 313
2024-02-02 23:05:45,821 [Epoch: 313 Step: 00041500] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:     1449 || Batch Translation Loss:   0.029584 => Txt Tokens per Sec:     4279 || Lr: 0.000050
2024-02-02 23:05:51,556 [Epoch: 313 Step: 00041600] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     1395 || Batch Translation Loss:   0.009653 => Txt Tokens per Sec:     3855 || Lr: 0.000050
2024-02-02 23:05:53,198 Epoch 313: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.42 
2024-02-02 23:05:53,198 EPOCH 314
2024-02-02 23:05:57,175 [Epoch: 314 Step: 00041700] Batch Recognition Loss:   0.000375 => Gls Tokens per Sec:     1429 || Batch Translation Loss:   0.020661 => Txt Tokens per Sec:     3904 || Lr: 0.000050
2024-02-02 23:06:00,649 Epoch 314: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.48 
2024-02-02 23:06:00,650 EPOCH 315
2024-02-02 23:06:02,655 [Epoch: 315 Step: 00041800] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     1517 || Batch Translation Loss:   0.091209 => Txt Tokens per Sec:     4194 || Lr: 0.000050
2024-02-02 23:06:08,412 Epoch 315: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.22 
2024-02-02 23:06:08,412 EPOCH 316
2024-02-02 23:06:08,637 [Epoch: 316 Step: 00041900] Batch Recognition Loss:   0.000230 => Gls Tokens per Sec:     1786 || Batch Translation Loss:   0.030527 => Txt Tokens per Sec:     4598 || Lr: 0.000050
2024-02-02 23:06:14,365 [Epoch: 316 Step: 00042000] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:     1397 || Batch Translation Loss:   0.017780 => Txt Tokens per Sec:     3895 || Lr: 0.000050
2024-02-02 23:06:25,427 Validation result at epoch 316, step    42000: duration: 11.0605s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00187	Translation Loss: 95887.79688	PPL: 14698.44629
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.77	(BLEU-1: 11.37,	BLEU-2: 3.38,	BLEU-3: 1.40,	BLEU-4: 0.77)
	CHRF 17.25	ROUGE 9.41
2024-02-02 23:06:25,428 Logging Recognition and Translation Outputs
2024-02-02 23:06:25,429 ========================================================================================================================
2024-02-02 23:06:25,429 Logging Sequence: 81_8.00
2024-02-02 23:06:25,429 	Gloss Reference :	A B+C+D+E
2024-02-02 23:06:25,429 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:06:25,429 	Gloss Alignment :	         
2024-02-02 23:06:25,430 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:06:25,431 	Text Reference  :	have been   involved in a huge controversy in  connection to real estate developer amrapali group        since   last 7   years  
2024-02-02 23:06:25,431 	Text Hypothesis :	the  tagged dhoni    in * the  posts       and appealed   to **** ****** him       to       disassociate himself from the builder
2024-02-02 23:06:25,432 	Text Alignment  :	S    S      S           D S    S           S   S             D    D      S         S        S            S       S    S   S      
2024-02-02 23:06:25,432 ========================================================================================================================
2024-02-02 23:06:25,432 Logging Sequence: 148_239.00
2024-02-02 23:06:25,432 	Gloss Reference :	A B+C+D+E
2024-02-02 23:06:25,432 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:06:25,432 	Gloss Alignment :	         
2024-02-02 23:06:25,432 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:06:25,434 	Text Reference  :	the     ground staff  were very happy  and  thanked the bowler for  his     kind gesture
2024-02-02 23:06:25,434 	Text Hypothesis :	however when   sharma lost his  wicket what a       sri lanka  were shocked and  this   
2024-02-02 23:06:25,434 	Text Alignment  :	S       S      S      S    S    S      S    S       S   S      S    S       S    S      
2024-02-02 23:06:25,434 ========================================================================================================================
2024-02-02 23:06:25,434 Logging Sequence: 165_8.00
2024-02-02 23:06:25,434 	Gloss Reference :	A B+C+D+E
2024-02-02 23:06:25,434 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:06:25,435 	Gloss Alignment :	         
2024-02-02 23:06:25,435 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:06:25,436 	Text Reference  :	** ** *** ***** ** *** ****** however many        don't believe in it   it       varies among people 
2024-02-02 23:06:25,436 	Text Hypothesis :	it is not known as the middle east    respiratory and   neeraj  he will continue to     be    amazing
2024-02-02 23:06:25,436 	Text Alignment  :	I  I  I   I     I  I   I      S       S           S     S       S  S    S        S      S     S      
2024-02-02 23:06:25,436 ========================================================================================================================
2024-02-02 23:06:25,436 Logging Sequence: 93_93.00
2024-02-02 23:06:25,436 	Gloss Reference :	A B+C+D+E
2024-02-02 23:06:25,437 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:06:25,437 	Gloss Alignment :	         
2024-02-02 23:06:25,437 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:06:25,437 	Text Reference  :	**** ***** rooney was    at   the ****** club   as    well   
2024-02-02 23:06:25,437 	Text Hypothesis :	they would be     played from the points during their matches
2024-02-02 23:06:25,438 	Text Alignment  :	I    I     S      S      S        I      S      S     S      
2024-02-02 23:06:25,438 ========================================================================================================================
2024-02-02 23:06:25,438 Logging Sequence: 96_129.00
2024-02-02 23:06:25,438 	Gloss Reference :	A B+C+D+E
2024-02-02 23:06:25,438 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:06:25,438 	Gloss Alignment :	         
2024-02-02 23:06:25,438 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:06:25,439 	Text Reference  :	** ***** **** *** **** *** **** **** viewers were very stressed
2024-02-02 23:06:25,439 	Text Hypothesis :	we don't know who will win will have to      wait and  watch   
2024-02-02 23:06:25,439 	Text Alignment  :	I  I     I    I   I    I   I    I    S       S    S    S       
2024-02-02 23:06:25,439 ========================================================================================================================
2024-02-02 23:06:27,016 Epoch 316: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.09 
2024-02-02 23:06:27,016 EPOCH 317
2024-02-02 23:06:30,909 [Epoch: 317 Step: 00042100] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     1480 || Batch Translation Loss:   0.016752 => Txt Tokens per Sec:     4154 || Lr: 0.000050
2024-02-02 23:06:34,360 Epoch 317: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.04 
2024-02-02 23:06:34,360 EPOCH 318
2024-02-02 23:06:36,700 [Epoch: 318 Step: 00042200] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:     1335 || Batch Translation Loss:   0.015479 => Txt Tokens per Sec:     3875 || Lr: 0.000050
2024-02-02 23:06:42,182 Epoch 318: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.17 
2024-02-02 23:06:42,183 EPOCH 319
2024-02-02 23:06:42,495 [Epoch: 319 Step: 00042300] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     1548 || Batch Translation Loss:   0.031342 => Txt Tokens per Sec:     4306 || Lr: 0.000050
2024-02-02 23:06:48,146 [Epoch: 319 Step: 00042400] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     1416 || Batch Translation Loss:   0.013397 => Txt Tokens per Sec:     3883 || Lr: 0.000050
2024-02-02 23:06:49,775 Epoch 319: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.52 
2024-02-02 23:06:49,775 EPOCH 320
2024-02-02 23:06:53,906 [Epoch: 320 Step: 00042500] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     1415 || Batch Translation Loss:   0.016936 => Txt Tokens per Sec:     3968 || Lr: 0.000050
2024-02-02 23:06:57,163 Epoch 320: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.15 
2024-02-02 23:06:57,164 EPOCH 321
2024-02-02 23:06:59,546 [Epoch: 321 Step: 00042600] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:     1344 || Batch Translation Loss:   0.018546 => Txt Tokens per Sec:     3788 || Lr: 0.000050
2024-02-02 23:07:04,904 Epoch 321: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.58 
2024-02-02 23:07:04,904 EPOCH 322
2024-02-02 23:07:05,237 [Epoch: 322 Step: 00042700] Batch Recognition Loss:   0.000265 => Gls Tokens per Sec:     1689 || Batch Translation Loss:   0.034023 => Txt Tokens per Sec:     4182 || Lr: 0.000050
2024-02-02 23:07:11,016 [Epoch: 322 Step: 00042800] Batch Recognition Loss:   0.000273 => Gls Tokens per Sec:     1384 || Batch Translation Loss:   0.018909 => Txt Tokens per Sec:     3876 || Lr: 0.000050
2024-02-02 23:07:12,455 Epoch 322: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.45 
2024-02-02 23:07:12,455 EPOCH 323
2024-02-02 23:07:16,624 [Epoch: 323 Step: 00042900] Batch Recognition Loss:   0.000401 => Gls Tokens per Sec:     1421 || Batch Translation Loss:   0.013921 => Txt Tokens per Sec:     3979 || Lr: 0.000050
2024-02-02 23:07:20,170 Epoch 323: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.66 
2024-02-02 23:07:20,171 EPOCH 324
2024-02-02 23:07:22,454 [Epoch: 324 Step: 00043000] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:     1437 || Batch Translation Loss:   0.018603 => Txt Tokens per Sec:     4009 || Lr: 0.000050
2024-02-02 23:07:27,568 Epoch 324: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.86 
2024-02-02 23:07:27,569 EPOCH 325
2024-02-02 23:07:27,999 [Epoch: 325 Step: 00043100] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     1495 || Batch Translation Loss:   0.024268 => Txt Tokens per Sec:     3680 || Lr: 0.000050
2024-02-02 23:07:33,765 [Epoch: 325 Step: 00043200] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:     1387 || Batch Translation Loss:   0.017591 => Txt Tokens per Sec:     3872 || Lr: 0.000050
2024-02-02 23:07:35,336 Epoch 325: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.01 
2024-02-02 23:07:35,337 EPOCH 326
2024-02-02 23:07:39,451 [Epoch: 326 Step: 00043300] Batch Recognition Loss:   0.000245 => Gls Tokens per Sec:     1458 || Batch Translation Loss:   0.018178 => Txt Tokens per Sec:     4008 || Lr: 0.000050
2024-02-02 23:07:42,859 Epoch 326: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.80 
2024-02-02 23:07:42,859 EPOCH 327
2024-02-02 23:07:45,057 [Epoch: 327 Step: 00043400] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     1529 || Batch Translation Loss:   0.034853 => Txt Tokens per Sec:     4306 || Lr: 0.000050
2024-02-02 23:07:50,276 Epoch 327: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.00 
2024-02-02 23:07:50,276 EPOCH 328
2024-02-02 23:07:50,728 [Epoch: 328 Step: 00043500] Batch Recognition Loss:   0.000296 => Gls Tokens per Sec:     1597 || Batch Translation Loss:   0.020671 => Txt Tokens per Sec:     3960 || Lr: 0.000050
2024-02-02 23:07:56,446 [Epoch: 328 Step: 00043600] Batch Recognition Loss:   0.000213 => Gls Tokens per Sec:     1399 || Batch Translation Loss:   0.013131 => Txt Tokens per Sec:     3877 || Lr: 0.000050
2024-02-02 23:07:57,966 Epoch 328: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.45 
2024-02-02 23:07:57,966 EPOCH 329
2024-02-02 23:08:01,938 [Epoch: 329 Step: 00043700] Batch Recognition Loss:   0.000180 => Gls Tokens per Sec:     1531 || Batch Translation Loss:   0.025839 => Txt Tokens per Sec:     4219 || Lr: 0.000050
2024-02-02 23:08:05,138 Epoch 329: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.04 
2024-02-02 23:08:05,138 EPOCH 330
2024-02-02 23:08:07,644 [Epoch: 330 Step: 00043800] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     1374 || Batch Translation Loss:   0.033448 => Txt Tokens per Sec:     3910 || Lr: 0.000050
2024-02-02 23:08:12,905 Epoch 330: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.51 
2024-02-02 23:08:12,906 EPOCH 331
2024-02-02 23:08:13,456 [Epoch: 331 Step: 00043900] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     1457 || Batch Translation Loss:   0.007636 => Txt Tokens per Sec:     4078 || Lr: 0.000050
2024-02-02 23:08:19,201 [Epoch: 331 Step: 00044000] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     1392 || Batch Translation Loss:   0.022176 => Txt Tokens per Sec:     3887 || Lr: 0.000050
2024-02-02 23:08:29,488 Validation result at epoch 331, step    44000: duration: 10.2859s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00213	Translation Loss: 95081.02344	PPL: 13558.41797
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.62	(BLEU-1: 10.64,	BLEU-2: 3.08,	BLEU-3: 1.19,	BLEU-4: 0.62)
	CHRF 16.62	ROUGE 9.22
2024-02-02 23:08:29,490 Logging Recognition and Translation Outputs
2024-02-02 23:08:29,490 ========================================================================================================================
2024-02-02 23:08:29,490 Logging Sequence: 117_29.00
2024-02-02 23:08:29,490 	Gloss Reference :	A B+C+D+E
2024-02-02 23:08:29,490 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:08:29,491 	Gloss Alignment :	         
2024-02-02 23:08:29,491 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:08:29,492 	Text Reference  :	however england was unable to  reach the target they    were all out   lost by  66     runs  
2024-02-02 23:08:29,492 	Text Hypothesis :	******* ******* *** when   csk lost  the match  because of   the first time and hardik pandya
2024-02-02 23:08:29,492 	Text Alignment  :	D       D       D   S      S   S         S      S       S    S   S     S    S   S      S     
2024-02-02 23:08:29,492 ========================================================================================================================
2024-02-02 23:08:29,493 Logging Sequence: 84_176.00
2024-02-02 23:08:29,493 	Gloss Reference :	A B+C+D+E
2024-02-02 23:08:29,493 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:08:29,493 	Gloss Alignment :	         
2024-02-02 23:08:29,493 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:08:29,494 	Text Reference  :	******** ** germany's nancy faeser who   attended the ***** **** ******* game    in   doha   against japan said
2024-02-02 23:08:29,494 	Text Hypothesis :	shocking to won       the   silver medal in       the world deaf cricket matches were unable to      tell  you 
2024-02-02 23:08:29,495 	Text Alignment  :	I        I  S         S     S      S     S            I     I    I       S       S    S      S       S     S   
2024-02-02 23:08:29,495 ========================================================================================================================
2024-02-02 23:08:29,495 Logging Sequence: 172_98.00
2024-02-02 23:08:29,495 	Gloss Reference :	A B+C+D+E
2024-02-02 23:08:29,495 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:08:29,495 	Gloss Alignment :	         
2024-02-02 23:08:29,495 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:08:29,496 	Text Reference  :	***** since 700   pm     it       kept    raining the intensity plunged around 915 pm  
2024-02-02 23:08:29,496 	Text Hypothesis :	while the   match starts anywhere between 730     and sri       lanka   were   in  time
2024-02-02 23:08:29,497 	Text Alignment  :	I     S     S     S      S        S       S       S   S         S       S      S   S   
2024-02-02 23:08:29,497 ========================================================================================================================
2024-02-02 23:08:29,497 Logging Sequence: 135_92.00
2024-02-02 23:08:29,497 	Gloss Reference :	A B+C+D+E
2024-02-02 23:08:29,497 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:08:29,497 	Gloss Alignment :	         
2024-02-02 23:08:29,497 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:08:29,498 	Text Reference  :	she   wrote     that half had   already been    raised by  the family's online fundraiser
2024-02-02 23:08:29,498 	Text Hypothesis :	these countries lost the  match in      javelin throw  was the ******** tokyo  olympics  
2024-02-02 23:08:29,499 	Text Alignment  :	S     S         S    S    S     S       S       S      S       D        S      S         
2024-02-02 23:08:29,499 ========================================================================================================================
2024-02-02 23:08:29,499 Logging Sequence: 180_332.00
2024-02-02 23:08:29,499 	Gloss Reference :	A B+C+D+E
2024-02-02 23:08:29,499 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:08:29,499 	Gloss Alignment :	         
2024-02-02 23:08:29,499 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:08:29,501 	Text Reference  :	did       i  eat     roti made   of shilajit that  i       got  energy to assault so many  girls  
2024-02-02 23:08:29,501 	Text Hypothesis :	according to rumours the  police he is       seven matches that she    be held    in seven matches
2024-02-02 23:08:29,501 	Text Alignment  :	S         S  S       S    S      S  S        S     S       S    S      S  S       S  S     S      
2024-02-02 23:08:29,501 ========================================================================================================================
2024-02-02 23:08:30,794 Epoch 331: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.62 
2024-02-02 23:08:30,795 EPOCH 332
2024-02-02 23:08:34,982 [Epoch: 332 Step: 00044100] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:     1471 || Batch Translation Loss:   0.034698 => Txt Tokens per Sec:     4073 || Lr: 0.000050
2024-02-02 23:08:38,102 Epoch 332: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.43 
2024-02-02 23:08:38,103 EPOCH 333
2024-02-02 23:08:40,458 [Epoch: 333 Step: 00044200] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:     1495 || Batch Translation Loss:   0.015719 => Txt Tokens per Sec:     4287 || Lr: 0.000050
2024-02-02 23:08:45,729 Epoch 333: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.39 
2024-02-02 23:08:45,729 EPOCH 334
2024-02-02 23:08:46,270 [Epoch: 334 Step: 00044300] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:     1630 || Batch Translation Loss:   0.015553 => Txt Tokens per Sec:     4185 || Lr: 0.000050
2024-02-02 23:08:51,841 [Epoch: 334 Step: 00044400] Batch Recognition Loss:   0.000113 => Gls Tokens per Sec:     1436 || Batch Translation Loss:   0.007963 => Txt Tokens per Sec:     4026 || Lr: 0.000050
2024-02-02 23:08:53,065 Epoch 334: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.46 
2024-02-02 23:08:53,065 EPOCH 335
2024-02-02 23:08:57,459 [Epoch: 335 Step: 00044500] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     1421 || Batch Translation Loss:   0.013714 => Txt Tokens per Sec:     3922 || Lr: 0.000050
2024-02-02 23:09:00,484 Epoch 335: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.34 
2024-02-02 23:09:00,484 EPOCH 336
2024-02-02 23:09:02,662 [Epoch: 336 Step: 00044600] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     1654 || Batch Translation Loss:   0.027275 => Txt Tokens per Sec:     4674 || Lr: 0.000050
2024-02-02 23:09:07,840 Epoch 336: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.69 
2024-02-02 23:09:07,840 EPOCH 337
2024-02-02 23:09:08,565 [Epoch: 337 Step: 00044700] Batch Recognition Loss:   0.000276 => Gls Tokens per Sec:     1325 || Batch Translation Loss:   0.025802 => Txt Tokens per Sec:     3831 || Lr: 0.000050
2024-02-02 23:09:14,232 [Epoch: 337 Step: 00044800] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:     1412 || Batch Translation Loss:   0.089013 => Txt Tokens per Sec:     3867 || Lr: 0.000050
2024-02-02 23:09:15,467 Epoch 337: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.67 
2024-02-02 23:09:15,468 EPOCH 338
2024-02-02 23:09:19,770 [Epoch: 338 Step: 00044900] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:     1469 || Batch Translation Loss:   0.024677 => Txt Tokens per Sec:     4130 || Lr: 0.000050
2024-02-02 23:09:22,689 Epoch 338: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.70 
2024-02-02 23:09:22,689 EPOCH 339
2024-02-02 23:09:25,350 [Epoch: 339 Step: 00045000] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     1383 || Batch Translation Loss:   0.012817 => Txt Tokens per Sec:     3867 || Lr: 0.000050
2024-02-02 23:09:30,375 Epoch 339: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.40 
2024-02-02 23:09:30,375 EPOCH 340
2024-02-02 23:09:31,055 [Epoch: 340 Step: 00045100] Batch Recognition Loss:   0.000373 => Gls Tokens per Sec:     1532 || Batch Translation Loss:   0.030985 => Txt Tokens per Sec:     4614 || Lr: 0.000050
2024-02-02 23:09:36,823 [Epoch: 340 Step: 00045200] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     1387 || Batch Translation Loss:   0.014996 => Txt Tokens per Sec:     3783 || Lr: 0.000050
2024-02-02 23:09:37,997 Epoch 340: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.11 
2024-02-02 23:09:37,997 EPOCH 341
2024-02-02 23:09:42,151 [Epoch: 341 Step: 00045300] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     1541 || Batch Translation Loss:   0.018815 => Txt Tokens per Sec:     4205 || Lr: 0.000050
2024-02-02 23:09:45,119 Epoch 341: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.48 
2024-02-02 23:09:45,119 EPOCH 342
2024-02-02 23:09:47,685 [Epoch: 342 Step: 00045400] Batch Recognition Loss:   0.000254 => Gls Tokens per Sec:     1466 || Batch Translation Loss:   0.071987 => Txt Tokens per Sec:     4005 || Lr: 0.000050
2024-02-02 23:09:52,515 Epoch 342: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.73 
2024-02-02 23:09:52,516 EPOCH 343
2024-02-02 23:09:53,295 [Epoch: 343 Step: 00045500] Batch Recognition Loss:   0.000243 => Gls Tokens per Sec:     1440 || Batch Translation Loss:   0.013136 => Txt Tokens per Sec:     3913 || Lr: 0.000050
2024-02-02 23:09:58,939 [Epoch: 343 Step: 00045600] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:     1417 || Batch Translation Loss:   0.021744 => Txt Tokens per Sec:     3914 || Lr: 0.000050
2024-02-02 23:10:00,015 Epoch 343: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.72 
2024-02-02 23:10:00,015 EPOCH 344
2024-02-02 23:10:04,768 [Epoch: 344 Step: 00045700] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     1364 || Batch Translation Loss:   0.013214 => Txt Tokens per Sec:     3792 || Lr: 0.000050
2024-02-02 23:10:07,623 Epoch 344: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.50 
2024-02-02 23:10:07,623 EPOCH 345
2024-02-02 23:10:09,885 [Epoch: 345 Step: 00045800] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:     1698 || Batch Translation Loss:   0.028949 => Txt Tokens per Sec:     4766 || Lr: 0.000050
2024-02-02 23:10:14,907 Epoch 345: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.08 
2024-02-02 23:10:14,908 EPOCH 346
2024-02-02 23:10:15,756 [Epoch: 346 Step: 00045900] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     1418 || Batch Translation Loss:   0.027100 => Txt Tokens per Sec:     4421 || Lr: 0.000050
2024-02-02 23:10:21,381 [Epoch: 346 Step: 00046000] Batch Recognition Loss:   0.000354 => Gls Tokens per Sec:     1421 || Batch Translation Loss:   0.014049 => Txt Tokens per Sec:     3868 || Lr: 0.000050
2024-02-02 23:10:32,243 Validation result at epoch 346, step    46000: duration: 10.8615s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00204	Translation Loss: 92997.01562	PPL: 11006.22461
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.72	(BLEU-1: 10.48,	BLEU-2: 2.91,	BLEU-3: 1.23,	BLEU-4: 0.72)
	CHRF 16.81	ROUGE 9.02
2024-02-02 23:10:32,244 Logging Recognition and Translation Outputs
2024-02-02 23:10:32,244 ========================================================================================================================
2024-02-02 23:10:32,244 Logging Sequence: 126_121.00
2024-02-02 23:10:32,244 	Gloss Reference :	A B+C+D+E
2024-02-02 23:10:32,245 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:10:32,245 	Gloss Alignment :	         
2024-02-02 23:10:32,245 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:10:32,245 	Text Reference  :	everyone was very happy by his victory
2024-02-02 23:10:32,245 	Text Hypothesis :	why      was **** ***** ** *** then   
2024-02-02 23:10:32,245 	Text Alignment  :	S            D    D     D  D   S      
2024-02-02 23:10:32,245 ========================================================================================================================
2024-02-02 23:10:32,246 Logging Sequence: 73_79.00
2024-02-02 23:10:32,246 	Gloss Reference :	A B+C+D+E
2024-02-02 23:10:32,246 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:10:32,246 	Gloss Alignment :	         
2024-02-02 23:10:32,246 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:10:32,248 	Text Reference  :	raina resturant has  food  from the rich spices of   north india to      the aromatic curries of south     india
2024-02-02 23:10:32,248 	Text Hypothesis :	the   pm        modi along with the **** ****** menu is    raita whereas the ******** ******* ** costliest dish 
2024-02-02 23:10:32,248 	Text Alignment  :	S     S         S    S     S        D    D      S    S     S     S           D        D       D  S         S    
2024-02-02 23:10:32,248 ========================================================================================================================
2024-02-02 23:10:32,248 Logging Sequence: 95_152.00
2024-02-02 23:10:32,248 	Gloss Reference :	A B+C+D+E
2024-02-02 23:10:32,248 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:10:32,248 	Gloss Alignment :	         
2024-02-02 23:10:32,249 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:10:32,249 	Text Reference  :	******* *** *** **** *** ***** how strange
2024-02-02 23:10:32,249 	Text Hypothesis :	england won the toss and chose to  bowl   
2024-02-02 23:10:32,249 	Text Alignment  :	I       I   I   I    I   I     S   S      
2024-02-02 23:10:32,249 ========================================================================================================================
2024-02-02 23:10:32,249 Logging Sequence: 135_39.00
2024-02-02 23:10:32,250 	Gloss Reference :	A B+C+D+E
2024-02-02 23:10:32,250 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:10:32,250 	Gloss Alignment :	         
2024-02-02 23:10:32,250 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:10:32,251 	Text Reference  :	who   needs   to  travel   from poland to stanford university in ******* california
2024-02-02 23:10:32,251 	Text Hypothesis :	after reading the training and  take   a  silver   medal      in javelin throw     
2024-02-02 23:10:32,251 	Text Alignment  :	S     S       S   S        S    S      S  S        S             I       S         
2024-02-02 23:10:32,251 ========================================================================================================================
2024-02-02 23:10:32,251 Logging Sequence: 87_2.00
2024-02-02 23:10:32,251 	Gloss Reference :	A B+C+D+E
2024-02-02 23:10:32,252 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:10:32,252 	Gloss Alignment :	         
2024-02-02 23:10:32,252 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:10:32,253 	Text Reference  :	cricketer gautam gambhir's jealousy against ms  dhoni   and   virat kohli   has been     increasing day by       day     
2024-02-02 23:10:32,253 	Text Hypothesis :	********* ****** ********* ******** ******* the supreme court then  ordered a   forensic audit      of  amrapali builders
2024-02-02 23:10:32,253 	Text Alignment  :	D         D      D         D        D       S   S       S     S     S       S   S        S          S   S        S       
2024-02-02 23:10:32,253 ========================================================================================================================
2024-02-02 23:10:33,271 Epoch 346: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.28 
2024-02-02 23:10:33,271 EPOCH 347
2024-02-02 23:10:38,024 [Epoch: 347 Step: 00046100] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     1380 || Batch Translation Loss:   0.055475 => Txt Tokens per Sec:     3824 || Lr: 0.000050
2024-02-02 23:10:40,915 Epoch 347: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.06 
2024-02-02 23:10:40,916 EPOCH 348
2024-02-02 23:10:43,451 [Epoch: 348 Step: 00046200] Batch Recognition Loss:   0.000189 => Gls Tokens per Sec:     1547 || Batch Translation Loss:   0.022396 => Txt Tokens per Sec:     4401 || Lr: 0.000050
2024-02-02 23:10:48,345 Epoch 348: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.80 
2024-02-02 23:10:48,346 EPOCH 349
2024-02-02 23:10:49,235 [Epoch: 349 Step: 00046300] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     1441 || Batch Translation Loss:   0.015190 => Txt Tokens per Sec:     3972 || Lr: 0.000050
2024-02-02 23:10:55,099 [Epoch: 349 Step: 00046400] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:     1363 || Batch Translation Loss:   0.017428 => Txt Tokens per Sec:     3780 || Lr: 0.000050
2024-02-02 23:10:56,081 Epoch 349: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.37 
2024-02-02 23:10:56,081 EPOCH 350
2024-02-02 23:11:00,622 [Epoch: 350 Step: 00046500] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     1462 || Batch Translation Loss:   0.038326 => Txt Tokens per Sec:     4006 || Lr: 0.000050
2024-02-02 23:11:03,527 Epoch 350: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.63 
2024-02-02 23:11:03,527 EPOCH 351
2024-02-02 23:11:06,151 [Epoch: 351 Step: 00046600] Batch Recognition Loss:   0.000208 => Gls Tokens per Sec:     1525 || Batch Translation Loss:   0.036411 => Txt Tokens per Sec:     4217 || Lr: 0.000050
2024-02-02 23:11:11,067 Epoch 351: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.79 
2024-02-02 23:11:11,068 EPOCH 352
2024-02-02 23:11:11,957 [Epoch: 352 Step: 00046700] Batch Recognition Loss:   0.000112 => Gls Tokens per Sec:     1530 || Batch Translation Loss:   0.020470 => Txt Tokens per Sec:     4084 || Lr: 0.000050
2024-02-02 23:11:17,777 [Epoch: 352 Step: 00046800] Batch Recognition Loss:   0.000435 => Gls Tokens per Sec:     1373 || Batch Translation Loss:   0.027189 => Txt Tokens per Sec:     3851 || Lr: 0.000050
2024-02-02 23:11:18,607 Epoch 352: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.14 
2024-02-02 23:11:18,608 EPOCH 353
2024-02-02 23:11:23,222 [Epoch: 353 Step: 00046900] Batch Recognition Loss:   0.000306 => Gls Tokens per Sec:     1457 || Batch Translation Loss:   0.021521 => Txt Tokens per Sec:     4091 || Lr: 0.000050
2024-02-02 23:11:25,947 Epoch 353: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.03 
2024-02-02 23:11:25,947 EPOCH 354
2024-02-02 23:11:28,702 [Epoch: 354 Step: 00047000] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:     1481 || Batch Translation Loss:   0.015188 => Txt Tokens per Sec:     4199 || Lr: 0.000050
2024-02-02 23:11:33,689 Epoch 354: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.11 
2024-02-02 23:11:33,690 EPOCH 355
2024-02-02 23:11:34,676 [Epoch: 355 Step: 00047100] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:     1464 || Batch Translation Loss:   0.013725 => Txt Tokens per Sec:     4039 || Lr: 0.000050
2024-02-02 23:11:40,368 [Epoch: 355 Step: 00047200] Batch Recognition Loss:   0.000117 => Gls Tokens per Sec:     1405 || Batch Translation Loss:   0.016391 => Txt Tokens per Sec:     3903 || Lr: 0.000050
2024-02-02 23:11:41,246 Epoch 355: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.29 
2024-02-02 23:11:41,247 EPOCH 356
2024-02-02 23:11:46,152 [Epoch: 356 Step: 00047300] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     1387 || Batch Translation Loss:   0.034322 => Txt Tokens per Sec:     3896 || Lr: 0.000050
2024-02-02 23:11:48,957 Epoch 356: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.41 
2024-02-02 23:11:48,958 EPOCH 357
2024-02-02 23:11:51,991 [Epoch: 357 Step: 00047400] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     1372 || Batch Translation Loss:   0.023463 => Txt Tokens per Sec:     3920 || Lr: 0.000050
2024-02-02 23:11:56,603 Epoch 357: Total Training Recognition Loss 0.04  Total Training Translation Loss 11.92 
2024-02-02 23:11:56,604 EPOCH 358
2024-02-02 23:11:57,756 [Epoch: 358 Step: 00047500] Batch Recognition Loss:   0.000364 => Gls Tokens per Sec:     1320 || Batch Translation Loss:   0.063485 => Txt Tokens per Sec:     3839 || Lr: 0.000050
2024-02-02 23:12:03,716 [Epoch: 358 Step: 00047600] Batch Recognition Loss:   0.000313 => Gls Tokens per Sec:     1341 || Batch Translation Loss:   0.070934 => Txt Tokens per Sec:     3703 || Lr: 0.000050
2024-02-02 23:12:04,500 Epoch 358: Total Training Recognition Loss 0.04  Total Training Translation Loss 8.85 
2024-02-02 23:12:04,500 EPOCH 359
2024-02-02 23:12:09,353 [Epoch: 359 Step: 00047700] Batch Recognition Loss:   0.000241 => Gls Tokens per Sec:     1418 || Batch Translation Loss:   0.038916 => Txt Tokens per Sec:     3869 || Lr: 0.000050
2024-02-02 23:12:12,029 Epoch 359: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.86 
2024-02-02 23:12:12,030 EPOCH 360
2024-02-02 23:12:14,738 [Epoch: 360 Step: 00047800] Batch Recognition Loss:   0.000202 => Gls Tokens per Sec:     1567 || Batch Translation Loss:   0.011647 => Txt Tokens per Sec:     4390 || Lr: 0.000050
2024-02-02 23:12:19,254 Epoch 360: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.81 
2024-02-02 23:12:19,255 EPOCH 361
2024-02-02 23:12:20,362 [Epoch: 361 Step: 00047900] Batch Recognition Loss:   0.000491 => Gls Tokens per Sec:     1447 || Batch Translation Loss:   0.074171 => Txt Tokens per Sec:     4204 || Lr: 0.000050
2024-02-02 23:12:26,280 [Epoch: 361 Step: 00048000] Batch Recognition Loss:   0.000393 => Gls Tokens per Sec:     1350 || Batch Translation Loss:   0.043134 => Txt Tokens per Sec:     3711 || Lr: 0.000050
2024-02-02 23:12:37,039 Validation result at epoch 361, step    48000: duration: 10.7572s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00271	Translation Loss: 93026.85156	PPL: 11039.13770
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.86	(BLEU-1: 10.95,	BLEU-2: 3.38,	BLEU-3: 1.52,	BLEU-4: 0.86)
	CHRF 17.14	ROUGE 9.39
2024-02-02 23:12:37,040 Logging Recognition and Translation Outputs
2024-02-02 23:12:37,040 ========================================================================================================================
2024-02-02 23:12:37,040 Logging Sequence: 88_159.00
2024-02-02 23:12:37,040 	Gloss Reference :	A B+C+D+E
2024-02-02 23:12:37,040 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:12:37,041 	Gloss Alignment :	         
2024-02-02 23:12:37,041 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:12:37,042 	Text Reference  :	however he  often  comes to        the **** ***** town to  meet his relatives
2024-02-02 23:12:37,042 	Text Hypothesis :	however the police never announced the fifa world cup  for held in  qatar    
2024-02-02 23:12:37,042 	Text Alignment  :	        S   S      S     S             I    I     S    S   S    S   S        
2024-02-02 23:12:37,042 ========================================================================================================================
2024-02-02 23:12:37,043 Logging Sequence: 180_53.00
2024-02-02 23:12:37,043 	Gloss Reference :	A B+C+D+E
2024-02-02 23:12:37,043 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:12:37,043 	Gloss Alignment :	         
2024-02-02 23:12:37,043 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:12:37,044 	Text Reference  :	the      protest is  against singh   again     
2024-02-02 23:12:37,044 	Text Hypothesis :	everyone usually the supreme court's directions
2024-02-02 23:12:37,044 	Text Alignment  :	S        S       S   S       S       S         
2024-02-02 23:12:37,044 ========================================================================================================================
2024-02-02 23:12:37,044 Logging Sequence: 163_30.00
2024-02-02 23:12:37,044 	Gloss Reference :	A B+C+D+E
2024-02-02 23:12:37,044 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:12:37,045 	Gloss Alignment :	         
2024-02-02 23:12:37,045 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:12:37,046 	Text Reference  :	**** they   never permitted anyone to   reveal her  face
2024-02-02 23:12:37,046 	Text Hypothesis :	many former and   anushka   sharma have been   very cute
2024-02-02 23:12:37,046 	Text Alignment  :	I    S      S     S         S      S    S      S    S   
2024-02-02 23:12:37,046 ========================================================================================================================
2024-02-02 23:12:37,046 Logging Sequence: 51_110.00
2024-02-02 23:12:37,047 	Gloss Reference :	A B+C+D+E
2024-02-02 23:12:37,047 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:12:37,047 	Gloss Alignment :	         
2024-02-02 23:12:37,047 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:12:37,049 	Text Reference  :	**** ** * *** ***** *** **** ** the     aussies were very  happy with their victory
2024-02-02 23:12:37,049 	Text Hypothesis :	this is a t20 world cup will be captain for     the  first time  in   the   world  
2024-02-02 23:12:37,049 	Text Alignment  :	I    I  I I   I     I   I    I  S       S       S    S     S     S    S     S      
2024-02-02 23:12:37,049 ========================================================================================================================
2024-02-02 23:12:37,049 Logging Sequence: 70_249.00
2024-02-02 23:12:37,050 	Gloss Reference :	A B+C+D+E
2024-02-02 23:12:37,050 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:12:37,050 	Gloss Alignment :	         
2024-02-02 23:12:37,050 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:12:37,051 	Text Reference  :	***** ** ******* ** ***** ** ** **** ******** have a     look at *** this video 
2024-02-02 23:12:37,051 	Text Hypothesis :	there is nothing to worry he is fine 'eriksen is   awake and  at the next season
2024-02-02 23:12:37,052 	Text Alignment  :	I     I  I       I  I     I  I  I    I        S    S     S       I   S    S     
2024-02-02 23:12:37,052 ========================================================================================================================
2024-02-02 23:12:37,818 Epoch 361: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.61 
2024-02-02 23:12:37,819 EPOCH 362
2024-02-02 23:12:42,457 [Epoch: 362 Step: 00048100] Batch Recognition Loss:   0.000207 => Gls Tokens per Sec:     1501 || Batch Translation Loss:   0.029595 => Txt Tokens per Sec:     4156 || Lr: 0.000050
2024-02-02 23:12:45,279 Epoch 362: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.00 
2024-02-02 23:12:45,279 EPOCH 363
2024-02-02 23:12:48,041 [Epoch: 363 Step: 00048200] Batch Recognition Loss:   0.000079 => Gls Tokens per Sec:     1565 || Batch Translation Loss:   0.020405 => Txt Tokens per Sec:     4292 || Lr: 0.000050
2024-02-02 23:12:52,786 Epoch 363: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.33 
2024-02-02 23:12:52,787 EPOCH 364
2024-02-02 23:12:53,992 [Epoch: 364 Step: 00048300] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:     1396 || Batch Translation Loss:   0.027855 => Txt Tokens per Sec:     3848 || Lr: 0.000050
2024-02-02 23:12:59,817 [Epoch: 364 Step: 00048400] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:     1374 || Batch Translation Loss:   0.022263 => Txt Tokens per Sec:     3782 || Lr: 0.000050
2024-02-02 23:13:00,732 Epoch 364: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.65 
2024-02-02 23:13:00,733 EPOCH 365
2024-02-02 23:13:05,556 [Epoch: 365 Step: 00048500] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:     1460 || Batch Translation Loss:   0.145362 => Txt Tokens per Sec:     4082 || Lr: 0.000050
2024-02-02 23:13:07,959 Epoch 365: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.64 
2024-02-02 23:13:07,959 EPOCH 366
2024-02-02 23:13:10,942 [Epoch: 366 Step: 00048600] Batch Recognition Loss:   0.000250 => Gls Tokens per Sec:     1476 || Batch Translation Loss:   0.016106 => Txt Tokens per Sec:     3957 || Lr: 0.000050
2024-02-02 23:13:15,574 Epoch 366: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.09 
2024-02-02 23:13:15,574 EPOCH 367
2024-02-02 23:13:16,796 [Epoch: 367 Step: 00048700] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     1441 || Batch Translation Loss:   0.009800 => Txt Tokens per Sec:     4145 || Lr: 0.000050
2024-02-02 23:13:22,309 [Epoch: 367 Step: 00048800] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     1449 || Batch Translation Loss:   0.018230 => Txt Tokens per Sec:     3979 || Lr: 0.000050
2024-02-02 23:13:22,949 Epoch 367: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.29 
2024-02-02 23:13:22,949 EPOCH 368
2024-02-02 23:13:27,795 [Epoch: 368 Step: 00048900] Batch Recognition Loss:   0.000238 => Gls Tokens per Sec:     1470 || Batch Translation Loss:   0.049312 => Txt Tokens per Sec:     4030 || Lr: 0.000050
2024-02-02 23:13:30,597 Epoch 368: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.85 
2024-02-02 23:13:30,598 EPOCH 369
2024-02-02 23:13:33,589 [Epoch: 369 Step: 00049000] Batch Recognition Loss:   0.000180 => Gls Tokens per Sec:     1498 || Batch Translation Loss:   0.051676 => Txt Tokens per Sec:     4227 || Lr: 0.000050
2024-02-02 23:13:38,004 Epoch 369: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.33 
2024-02-02 23:13:38,005 EPOCH 370
2024-02-02 23:13:39,411 [Epoch: 370 Step: 00049100] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:     1310 || Batch Translation Loss:   0.014660 => Txt Tokens per Sec:     3551 || Lr: 0.000050
2024-02-02 23:13:45,408 [Epoch: 370 Step: 00049200] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:     1334 || Batch Translation Loss:   0.028362 => Txt Tokens per Sec:     3725 || Lr: 0.000050
2024-02-02 23:13:46,026 Epoch 370: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.98 
2024-02-02 23:13:46,026 EPOCH 371
2024-02-02 23:13:51,065 [Epoch: 371 Step: 00049300] Batch Recognition Loss:   0.000283 => Gls Tokens per Sec:     1429 || Batch Translation Loss:   0.062465 => Txt Tokens per Sec:     3988 || Lr: 0.000050
2024-02-02 23:13:53,463 Epoch 371: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.02 
2024-02-02 23:13:53,463 EPOCH 372
2024-02-02 23:13:56,566 [Epoch: 372 Step: 00049400] Batch Recognition Loss:   0.000320 => Gls Tokens per Sec:     1470 || Batch Translation Loss:   0.029164 => Txt Tokens per Sec:     3948 || Lr: 0.000050
2024-02-02 23:14:01,111 Epoch 372: Total Training Recognition Loss 0.04  Total Training Translation Loss 11.51 
2024-02-02 23:14:01,112 EPOCH 373
2024-02-02 23:14:02,547 [Epoch: 373 Step: 00049500] Batch Recognition Loss:   0.000410 => Gls Tokens per Sec:     1340 || Batch Translation Loss:   0.048643 => Txt Tokens per Sec:     3739 || Lr: 0.000050
2024-02-02 23:14:08,423 [Epoch: 373 Step: 00049600] Batch Recognition Loss:   0.000340 => Gls Tokens per Sec:     1362 || Batch Translation Loss:   0.037852 => Txt Tokens per Sec:     3801 || Lr: 0.000050
2024-02-02 23:14:08,985 Epoch 373: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.16 
2024-02-02 23:14:08,985 EPOCH 374
2024-02-02 23:14:13,761 [Epoch: 374 Step: 00049700] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     1525 || Batch Translation Loss:   0.030338 => Txt Tokens per Sec:     4233 || Lr: 0.000050
2024-02-02 23:14:15,957 Epoch 374: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.44 
2024-02-02 23:14:15,957 EPOCH 375
2024-02-02 23:14:19,283 [Epoch: 375 Step: 00049800] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:     1396 || Batch Translation Loss:   0.021955 => Txt Tokens per Sec:     3840 || Lr: 0.000050
2024-02-02 23:14:23,786 Epoch 375: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.62 
2024-02-02 23:14:23,787 EPOCH 376
2024-02-02 23:14:25,262 [Epoch: 376 Step: 00049900] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:     1356 || Batch Translation Loss:   0.061809 => Txt Tokens per Sec:     3704 || Lr: 0.000050
2024-02-02 23:14:30,943 [Epoch: 376 Step: 00050000] Batch Recognition Loss:   0.000152 => Gls Tokens per Sec:     1407 || Batch Translation Loss:   0.021000 => Txt Tokens per Sec:     3899 || Lr: 0.000050
2024-02-02 23:14:41,845 Validation result at epoch 376, step    50000: duration: 10.9012s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00215	Translation Loss: 92036.02344	PPL: 9997.09277
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.87	(BLEU-1: 11.20,	BLEU-2: 3.39,	BLEU-3: 1.46,	BLEU-4: 0.87)
	CHRF 16.92	ROUGE 9.52
2024-02-02 23:14:41,846 Logging Recognition and Translation Outputs
2024-02-02 23:14:41,846 ========================================================================================================================
2024-02-02 23:14:41,846 Logging Sequence: 59_58.00
2024-02-02 23:14:41,846 	Gloss Reference :	A B+C+D+E
2024-02-02 23:14:41,847 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:14:41,847 	Gloss Alignment :	         
2024-02-02 23:14:41,847 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:14:41,848 	Text Reference  :	to fix   the  damage they did     not       have a     lot  of   time
2024-02-02 23:14:41,848 	Text Hypothesis :	** isn't that such   a    strange statement how  could only here you 
2024-02-02 23:14:41,848 	Text Alignment  :	D  S     S    S      S    S       S         S    S     S    S    S   
2024-02-02 23:14:41,848 ========================================================================================================================
2024-02-02 23:14:41,848 Logging Sequence: 165_2.00
2024-02-02 23:14:41,849 	Gloss Reference :	A B+C+D+E
2024-02-02 23:14:41,849 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:14:41,849 	Gloss Alignment :	         
2024-02-02 23:14:41,849 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:14:41,850 	Text Reference  :	many people believe in superstitions and think it   brings good luck   and         bad luck  
2024-02-02 23:14:41,850 	Text Hypothesis :	**** ****** ******* ** ************* he  has   also become the  indian performance in  sports
2024-02-02 23:14:41,850 	Text Alignment  :	D    D      D       D  D             S   S     S    S      S    S      S           S   S     
2024-02-02 23:14:41,850 ========================================================================================================================
2024-02-02 23:14:41,850 Logging Sequence: 58_147.00
2024-02-02 23:14:41,850 	Gloss Reference :	A B+C+D+E
2024-02-02 23:14:41,850 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:14:41,851 	Gloss Alignment :	         
2024-02-02 23:14:41,851 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:14:41,852 	Text Reference  :	the women's cricket team grabbed gold by  beating sri    lanka in the finals what a historic win  
2024-02-02 23:14:41,852 	Text Hypothesis :	*** ******* ******* **** ******* who  won the     silver medal in the ****** **** * asian    games
2024-02-02 23:14:41,852 	Text Alignment  :	D   D       D       D    D       S    S   S       S      S            D      D    D S        S    
2024-02-02 23:14:41,852 ========================================================================================================================
2024-02-02 23:14:41,852 Logging Sequence: 81_139.00
2024-02-02 23:14:41,852 	Gloss Reference :	A B+C+D+E
2024-02-02 23:14:41,853 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:14:41,853 	Gloss Alignment :	         
2024-02-02 23:14:41,853 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:14:41,854 	Text Reference  :	in 2017 the case was filed first in delhi high    court by rhiti sports   management on       behalf of     dhoni       
2024-02-02 23:14:41,854 	Text Hypothesis :	** on   the **** *** ***** ***** ** ***** supreme court ** ***** canceled the        builder' real   estate registration
2024-02-02 23:14:41,854 	Text Alignment  :	D  S        D    D   D     D     D  D     S             D  D     S        S          S        S      S      S           
2024-02-02 23:14:41,854 ========================================================================================================================
2024-02-02 23:14:41,854 Logging Sequence: 125_72.00
2024-02-02 23:14:41,855 	Gloss Reference :	A B+C+D+E
2024-02-02 23:14:41,855 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:14:41,855 	Gloss Alignment :	         
2024-02-02 23:14:41,855 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:14:41,856 	Text Reference  :	some said the pakistani javelineer had milicious intentions of  tampering with     the     javelin out of    jealousy
2024-02-02 23:14:41,856 	Text Hypothesis :	**** **** *** ********* ********** *** ********* ********** and included  talented players such    as  rahul dravid  
2024-02-02 23:14:41,856 	Text Alignment  :	D    D    D   D         D          D   D         D          S   S         S        S       S       S   S     S       
2024-02-02 23:14:41,856 ========================================================================================================================
2024-02-02 23:14:42,353 Epoch 376: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.03 
2024-02-02 23:14:42,353 EPOCH 377
2024-02-02 23:14:47,613 [Epoch: 377 Step: 00050100] Batch Recognition Loss:   0.000096 => Gls Tokens per Sec:     1399 || Batch Translation Loss:   0.020221 => Txt Tokens per Sec:     3909 || Lr: 0.000050
2024-02-02 23:14:49,870 Epoch 377: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.58 
2024-02-02 23:14:49,870 EPOCH 378
2024-02-02 23:14:52,929 [Epoch: 378 Step: 00050200] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     1543 || Batch Translation Loss:   0.028728 => Txt Tokens per Sec:     4316 || Lr: 0.000050
2024-02-02 23:14:56,909 Epoch 378: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.61 
2024-02-02 23:14:56,909 EPOCH 379
2024-02-02 23:14:58,286 [Epoch: 379 Step: 00050300] Batch Recognition Loss:   0.000232 => Gls Tokens per Sec:     1513 || Batch Translation Loss:   0.017289 => Txt Tokens per Sec:     4366 || Lr: 0.000050
2024-02-02 23:15:04,118 [Epoch: 379 Step: 00050400] Batch Recognition Loss:   0.000284 => Gls Tokens per Sec:     1370 || Batch Translation Loss:   0.048274 => Txt Tokens per Sec:     3755 || Lr: 0.000050
2024-02-02 23:15:04,622 Epoch 379: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.46 
2024-02-02 23:15:04,623 EPOCH 380
2024-02-02 23:15:09,999 [Epoch: 380 Step: 00050500] Batch Recognition Loss:   0.000383 => Gls Tokens per Sec:     1384 || Batch Translation Loss:   0.040967 => Txt Tokens per Sec:     3821 || Lr: 0.000050
2024-02-02 23:15:12,232 Epoch 380: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.14 
2024-02-02 23:15:12,232 EPOCH 381
2024-02-02 23:15:15,387 [Epoch: 381 Step: 00050600] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     1522 || Batch Translation Loss:   0.005116 => Txt Tokens per Sec:     4239 || Lr: 0.000050
2024-02-02 23:15:19,641 Epoch 381: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.28 
2024-02-02 23:15:19,642 EPOCH 382
2024-02-02 23:15:21,056 [Epoch: 382 Step: 00050700] Batch Recognition Loss:   0.000275 => Gls Tokens per Sec:     1529 || Batch Translation Loss:   0.025978 => Txt Tokens per Sec:     4103 || Lr: 0.000050
2024-02-02 23:15:26,918 [Epoch: 382 Step: 00050800] Batch Recognition Loss:   0.000268 => Gls Tokens per Sec:     1363 || Batch Translation Loss:   0.010695 => Txt Tokens per Sec:     3837 || Lr: 0.000050
2024-02-02 23:15:27,278 Epoch 382: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.89 
2024-02-02 23:15:27,278 EPOCH 383
2024-02-02 23:15:32,543 [Epoch: 383 Step: 00050900] Batch Recognition Loss:   0.000285 => Gls Tokens per Sec:     1428 || Batch Translation Loss:   0.017716 => Txt Tokens per Sec:     3980 || Lr: 0.000050
2024-02-02 23:15:34,840 Epoch 383: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.18 
2024-02-02 23:15:34,841 EPOCH 384
2024-02-02 23:15:38,171 [Epoch: 384 Step: 00051000] Batch Recognition Loss:   0.000285 => Gls Tokens per Sec:     1466 || Batch Translation Loss:   0.044694 => Txt Tokens per Sec:     4129 || Lr: 0.000050
2024-02-02 23:15:42,159 Epoch 384: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.48 
2024-02-02 23:15:42,159 EPOCH 385
2024-02-02 23:15:43,822 [Epoch: 385 Step: 00051100] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     1348 || Batch Translation Loss:   0.032139 => Txt Tokens per Sec:     3779 || Lr: 0.000050
2024-02-02 23:15:49,748 [Epoch: 385 Step: 00051200] Batch Recognition Loss:   0.000267 => Gls Tokens per Sec:     1348 || Batch Translation Loss:   0.043352 => Txt Tokens per Sec:     3743 || Lr: 0.000050
2024-02-02 23:15:49,993 Epoch 385: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.29 
2024-02-02 23:15:49,993 EPOCH 386
2024-02-02 23:15:55,168 [Epoch: 386 Step: 00051300] Batch Recognition Loss:   0.000275 => Gls Tokens per Sec:     1469 || Batch Translation Loss:   0.022968 => Txt Tokens per Sec:     4032 || Lr: 0.000050
2024-02-02 23:15:57,247 Epoch 386: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.13 
2024-02-02 23:15:57,247 EPOCH 387
2024-02-02 23:16:00,828 [Epoch: 387 Step: 00051400] Batch Recognition Loss:   0.000242 => Gls Tokens per Sec:     1386 || Batch Translation Loss:   0.023756 => Txt Tokens per Sec:     3904 || Lr: 0.000050
2024-02-02 23:16:04,914 Epoch 387: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.41 
2024-02-02 23:16:04,915 EPOCH 388
2024-02-02 23:16:06,612 [Epoch: 388 Step: 00051500] Batch Recognition Loss:   0.000230 => Gls Tokens per Sec:     1367 || Batch Translation Loss:   0.018032 => Txt Tokens per Sec:     3950 || Lr: 0.000050
2024-02-02 23:16:12,505 [Epoch: 388 Step: 00051600] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     1356 || Batch Translation Loss:   0.031699 => Txt Tokens per Sec:     3721 || Lr: 0.000050
2024-02-02 23:16:12,722 Epoch 388: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.19 
2024-02-02 23:16:12,722 EPOCH 389
2024-02-02 23:16:18,180 [Epoch: 389 Step: 00051700] Batch Recognition Loss:   0.000220 => Gls Tokens per Sec:     1407 || Batch Translation Loss:   0.035912 => Txt Tokens per Sec:     3916 || Lr: 0.000050
2024-02-02 23:16:20,156 Epoch 389: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.61 
2024-02-02 23:16:20,157 EPOCH 390
2024-02-02 23:16:23,554 [Epoch: 390 Step: 00051800] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:     1484 || Batch Translation Loss:   0.017938 => Txt Tokens per Sec:     4081 || Lr: 0.000050
2024-02-02 23:16:27,473 Epoch 390: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.88 
2024-02-02 23:16:27,474 EPOCH 391
2024-02-02 23:16:29,201 [Epoch: 391 Step: 00051900] Batch Recognition Loss:   0.000209 => Gls Tokens per Sec:     1389 || Batch Translation Loss:   0.018403 => Txt Tokens per Sec:     3701 || Lr: 0.000050
2024-02-02 23:16:35,024 [Epoch: 391 Step: 00052000] Batch Recognition Loss:   0.000538 => Gls Tokens per Sec:     1374 || Batch Translation Loss:   0.038094 => Txt Tokens per Sec:     3851 || Lr: 0.000050
2024-02-02 23:16:45,988 Validation result at epoch 391, step    52000: duration: 10.9643s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00303	Translation Loss: 92271.69531	PPL: 10235.66113
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.93	(BLEU-1: 10.39,	BLEU-2: 3.57,	BLEU-3: 1.68,	BLEU-4: 0.93)
	CHRF 16.65	ROUGE 9.08
2024-02-02 23:16:45,990 Logging Recognition and Translation Outputs
2024-02-02 23:16:45,990 ========================================================================================================================
2024-02-02 23:16:45,990 Logging Sequence: 87_229.00
2024-02-02 23:16:45,991 	Gloss Reference :	A B+C+D+E
2024-02-02 23:16:45,991 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:16:45,991 	Gloss Alignment :	         
2024-02-02 23:16:45,991 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:16:45,992 	Text Reference  :	it was not  against dhoni *** or   kohli    
2024-02-02 23:16:45,992 	Text Hypothesis :	** *** then ms      dhoni has been postponed
2024-02-02 23:16:45,992 	Text Alignment  :	D  D   S    S             I   S    S        
2024-02-02 23:16:45,992 ========================================================================================================================
2024-02-02 23:16:45,992 Logging Sequence: 134_153.00
2024-02-02 23:16:45,992 	Gloss Reference :	A B+C+D+E
2024-02-02 23:16:45,992 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:16:45,992 	Gloss Alignment :	         
2024-02-02 23:16:45,993 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:16:45,995 	Text Reference  :	pm modi in his interaction said that deaf athletes must       fight for  their goals       and never give up  despite the  losses
2024-02-02 23:16:45,995 	Text Hypothesis :	pm modi ** *** told        him  that yoga and      meditation would help him   concentrate and will  help him win     more medals
2024-02-02 23:16:45,995 	Text Alignment  :	        D  D   S           S         S    S        S          S     S    S     S               S     S    S   S       S    S     
2024-02-02 23:16:45,995 ========================================================================================================================
2024-02-02 23:16:45,995 Logging Sequence: 137_155.00
2024-02-02 23:16:45,996 	Gloss Reference :	A B+C+D+E
2024-02-02 23:16:45,996 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:16:45,996 	Gloss Alignment :	         
2024-02-02 23:16:45,996 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:16:45,997 	Text Reference  :	****** ****** *** an  extremely high    tax   named as       sin       tax will be   applied
2024-02-02 23:16:45,997 	Text Hypothesis :	indian team's win the match     between india and   pakistan qualified for the  next day    
2024-02-02 23:16:45,997 	Text Alignment  :	I      I      I   S   S         S       S     S     S        S         S   S    S    S      
2024-02-02 23:16:45,997 ========================================================================================================================
2024-02-02 23:16:45,997 Logging Sequence: 59_18.00
2024-02-02 23:16:45,998 	Gloss Reference :	A B+C+D+E
2024-02-02 23:16:45,998 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:16:45,998 	Gloss Alignment :	         
2024-02-02 23:16:45,998 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:16:45,999 	Text Reference  :	**** *** ********** 27-year-old jessica fox      from australia won    a   bronze a       gold medal in  canoeing
2024-02-02 23:16:45,999 	Text Hypothesis :	well the organisers of          the     olympics in   tokyo     handed out 60000  condoms to   all   the athletes
2024-02-02 23:16:45,999 	Text Alignment  :	I    I   I          S           S       S        S    S         S      S   S      S       S    S     S   S       
2024-02-02 23:16:46,000 ========================================================================================================================
2024-02-02 23:16:46,000 Logging Sequence: 173_103.00
2024-02-02 23:16:46,000 	Gloss Reference :	A B+C+D+E
2024-02-02 23:16:46,000 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:16:46,000 	Gloss Alignment :	         
2024-02-02 23:16:46,000 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:16:46,001 	Text Reference  :	** **** ***** **** *************** **** *** **** *** **** these rumours are  absolutely *** rubbish
2024-02-02 23:16:46,001 	Text Hypothesis :	as they would have unintentionally hurt the does not want for   the     team absolutely all rounder
2024-02-02 23:16:46,001 	Text Alignment  :	I  I    I     I    I               I    I   I    I   I    S     S       S               I   S      
2024-02-02 23:16:46,001 ========================================================================================================================
2024-02-02 23:16:46,252 Epoch 391: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.64 
2024-02-02 23:16:46,252 EPOCH 392
2024-02-02 23:16:51,619 [Epoch: 392 Step: 00052100] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     1446 || Batch Translation Loss:   0.013770 => Txt Tokens per Sec:     4035 || Lr: 0.000050
2024-02-02 23:16:53,516 Epoch 392: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.93 
2024-02-02 23:16:53,516 EPOCH 393
2024-02-02 23:16:56,842 [Epoch: 393 Step: 00052200] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     1540 || Batch Translation Loss:   0.106130 => Txt Tokens per Sec:     4233 || Lr: 0.000050
2024-02-02 23:17:00,726 Epoch 393: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.74 
2024-02-02 23:17:00,726 EPOCH 394
2024-02-02 23:17:02,313 [Epoch: 394 Step: 00052300] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:     1564 || Batch Translation Loss:   0.026541 => Txt Tokens per Sec:     4287 || Lr: 0.000050
2024-02-02 23:17:08,170 [Epoch: 394 Step: 00052400] Batch Recognition Loss:   0.000116 => Gls Tokens per Sec:     1365 || Batch Translation Loss:   0.145609 => Txt Tokens per Sec:     3804 || Lr: 0.000050
2024-02-02 23:17:08,289 Epoch 394: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.48 
2024-02-02 23:17:08,289 EPOCH 395
2024-02-02 23:17:13,726 [Epoch: 395 Step: 00052500] Batch Recognition Loss:   0.000122 => Gls Tokens per Sec:     1442 || Batch Translation Loss:   0.036854 => Txt Tokens per Sec:     3976 || Lr: 0.000050
2024-02-02 23:17:15,756 Epoch 395: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.01 
2024-02-02 23:17:15,757 EPOCH 396
2024-02-02 23:17:19,124 [Epoch: 396 Step: 00052600] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:     1544 || Batch Translation Loss:   0.022626 => Txt Tokens per Sec:     4157 || Lr: 0.000050
2024-02-02 23:17:23,026 Epoch 396: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.49 
2024-02-02 23:17:23,026 EPOCH 397
2024-02-02 23:17:24,675 [Epoch: 397 Step: 00052700] Batch Recognition Loss:   0.000435 => Gls Tokens per Sec:     1553 || Batch Translation Loss:   0.020986 => Txt Tokens per Sec:     4342 || Lr: 0.000050
2024-02-02 23:17:30,570 [Epoch: 397 Step: 00052800] Batch Recognition Loss:   0.000424 => Gls Tokens per Sec:     1356 || Batch Translation Loss:   0.032160 => Txt Tokens per Sec:     3762 || Lr: 0.000050
2024-02-02 23:17:30,625 Epoch 397: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.35 
2024-02-02 23:17:30,625 EPOCH 398
2024-02-02 23:17:36,325 [Epoch: 398 Step: 00052900] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:     1390 || Batch Translation Loss:   0.017541 => Txt Tokens per Sec:     3870 || Lr: 0.000050
2024-02-02 23:17:38,326 Epoch 398: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.76 
2024-02-02 23:17:38,326 EPOCH 399
2024-02-02 23:17:42,100 [Epoch: 399 Step: 00053000] Batch Recognition Loss:   0.000225 => Gls Tokens per Sec:     1400 || Batch Translation Loss:   0.022213 => Txt Tokens per Sec:     3937 || Lr: 0.000050
2024-02-02 23:17:46,317 Epoch 399: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.37 
2024-02-02 23:17:46,318 EPOCH 400
2024-02-02 23:17:48,391 [Epoch: 400 Step: 00053100] Batch Recognition Loss:   0.000246 => Gls Tokens per Sec:     1274 || Batch Translation Loss:   0.010896 => Txt Tokens per Sec:     3634 || Lr: 0.000050
2024-02-02 23:17:54,336 [Epoch: 400 Step: 00053200] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:     1344 || Batch Translation Loss:   0.018438 => Txt Tokens per Sec:     3697 || Lr: 0.000050
2024-02-02 23:17:54,337 Epoch 400: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.74 
2024-02-02 23:17:54,337 EPOCH 401
2024-02-02 23:17:59,861 [Epoch: 401 Step: 00053300] Batch Recognition Loss:   0.000209 => Gls Tokens per Sec:     1449 || Batch Translation Loss:   0.031672 => Txt Tokens per Sec:     4047 || Lr: 0.000050
2024-02-02 23:18:01,599 Epoch 401: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.70 
2024-02-02 23:18:01,599 EPOCH 402
2024-02-02 23:18:05,242 [Epoch: 402 Step: 00053400] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:     1472 || Batch Translation Loss:   0.064473 => Txt Tokens per Sec:     3996 || Lr: 0.000050
2024-02-02 23:18:09,190 Epoch 402: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.73 
2024-02-02 23:18:09,191 EPOCH 403
2024-02-02 23:18:11,022 [Epoch: 403 Step: 00053500] Batch Recognition Loss:   0.000247 => Gls Tokens per Sec:     1485 || Batch Translation Loss:   0.023875 => Txt Tokens per Sec:     4095 || Lr: 0.000050
2024-02-02 23:18:16,802 Epoch 403: Total Training Recognition Loss 0.04  Total Training Translation Loss 8.26 
2024-02-02 23:18:16,803 EPOCH 404
2024-02-02 23:18:16,865 [Epoch: 404 Step: 00053600] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:     1285 || Batch Translation Loss:   0.039274 => Txt Tokens per Sec:     3808 || Lr: 0.000050
2024-02-02 23:18:22,658 [Epoch: 404 Step: 00053700] Batch Recognition Loss:   0.000334 => Gls Tokens per Sec:     1381 || Batch Translation Loss:   0.022249 => Txt Tokens per Sec:     3822 || Lr: 0.000050
2024-02-02 23:18:24,573 Epoch 404: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.40 
2024-02-02 23:18:24,573 EPOCH 405
2024-02-02 23:18:28,205 [Epoch: 405 Step: 00053800] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:     1498 || Batch Translation Loss:   0.042372 => Txt Tokens per Sec:     4167 || Lr: 0.000050
2024-02-02 23:18:31,879 Epoch 405: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.35 
2024-02-02 23:18:31,880 EPOCH 406
2024-02-02 23:18:33,689 [Epoch: 406 Step: 00053900] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:     1548 || Batch Translation Loss:   0.020951 => Txt Tokens per Sec:     4211 || Lr: 0.000050
2024-02-02 23:18:38,920 Epoch 406: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.78 
2024-02-02 23:18:38,920 EPOCH 407
2024-02-02 23:18:39,020 [Epoch: 407 Step: 00054000] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     1633 || Batch Translation Loss:   0.014887 => Txt Tokens per Sec:     3653 || Lr: 0.000050
2024-02-02 23:18:50,004 Validation result at epoch 407, step    54000: duration: 10.9840s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00246	Translation Loss: 91757.78906	PPL: 9722.57910
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.75	(BLEU-1: 10.52,	BLEU-2: 3.06,	BLEU-3: 1.29,	BLEU-4: 0.75)
	CHRF 16.68	ROUGE 9.12
2024-02-02 23:18:50,005 Logging Recognition and Translation Outputs
2024-02-02 23:18:50,006 ========================================================================================================================
2024-02-02 23:18:50,006 Logging Sequence: 130_139.00
2024-02-02 23:18:50,006 	Gloss Reference :	A B+C+D+E
2024-02-02 23:18:50,006 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:18:50,006 	Gloss Alignment :	         
2024-02-02 23:18:50,006 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:18:50,009 	Text Reference  :	he shared a picture of  a little pouch he knit for his olympic gold   medal with uk flag on  one  side   and **** japanese flag on      the     other
2024-02-02 23:18:50,009 	Text Hypothesis :	he ****** * ******* won a ****** ***** ** **** *** *** ******* bronze medal **** ** at   the 2012 london and 2016 rio      de   janeiro olympic games
2024-02-02 23:18:50,009 	Text Alignment  :	   D      D D       S     D      D     D  D    D   D   D       S            D    D  S    S   S    S          I    S        S    S       S       S    
2024-02-02 23:18:50,009 ========================================================================================================================
2024-02-02 23:18:50,009 Logging Sequence: 148_155.00
2024-02-02 23:18:50,009 	Gloss Reference :	A B+C+D+E
2024-02-02 23:18:50,010 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:18:50,010 	Gloss Alignment :	         
2024-02-02 23:18:50,010 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:18:50,011 	Text Reference  :	****** ********* ***** **** india     won           the     match with     263 balls     remaining and without losing        any     wicket    
2024-02-02 23:18:50,011 	Text Hypothesis :	indian badminton world deaf badminton championships between india pakistan is  currently at        the dubai   international cricket tournament
2024-02-02 23:18:50,011 	Text Alignment  :	I      I         I     I    S         S             S       S     S        S   S         S         S   S       S             S       S         
2024-02-02 23:18:50,011 ========================================================================================================================
2024-02-02 23:18:50,012 Logging Sequence: 126_99.00
2024-02-02 23:18:50,012 	Gloss Reference :	A B+C+D+E
2024-02-02 23:18:50,012 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:18:50,012 	Gloss Alignment :	         
2024-02-02 23:18:50,012 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:18:50,013 	Text Reference  :	** ** ** he          dedicated the   medal to sprinter milkha singh
2024-02-02 23:18:50,013 	Text Hypothesis :	it is an interesting history   about rs    1  crore    cash   prize
2024-02-02 23:18:50,013 	Text Alignment  :	I  I  I  S           S         S     S     S  S        S      S    
2024-02-02 23:18:50,013 ========================================================================================================================
2024-02-02 23:18:50,013 Logging Sequence: 149_77.00
2024-02-02 23:18:50,013 	Gloss Reference :	A B+C+D+E
2024-02-02 23:18:50,014 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:18:50,014 	Gloss Alignment :	         
2024-02-02 23:18:50,014 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:18:50,015 	Text Reference  :	and arrested danushka for alleged sexual assault of a   29  year old woman whose    name has not **** been disclosed
2024-02-02 23:18:50,015 	Text Hypothesis :	*** ******** ******** *** ******* ****** ******* ** but due to   the covid pandemic they did not miss the  match    
2024-02-02 23:18:50,015 	Text Alignment  :	D   D        D        D   D       D      D       D  S   S   S    S   S     S        S    S       I    S    S        
2024-02-02 23:18:50,016 ========================================================================================================================
2024-02-02 23:18:50,016 Logging Sequence: 168_15.00
2024-02-02 23:18:50,016 	Gloss Reference :	A B+C+D+E
2024-02-02 23:18:50,016 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:18:50,016 	Gloss Alignment :	         
2024-02-02 23:18:50,016 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:18:50,017 	Text Reference  :	when in public the ****** couple are always   approached for     photographys and  autographs
2024-02-02 23:18:50,017 	Text Hypothesis :	**** ** ****** the indian team   was jubliant with       winning the          asia cup       
2024-02-02 23:18:50,017 	Text Alignment  :	D    D  D          I      S      S   S        S          S       S            S    S         
2024-02-02 23:18:50,018 ========================================================================================================================
2024-02-02 23:18:55,931 [Epoch: 407 Step: 00054100] Batch Recognition Loss:   0.000262 => Gls Tokens per Sec:     1350 || Batch Translation Loss:   0.032650 => Txt Tokens per Sec:     3755 || Lr: 0.000050
2024-02-02 23:18:57,676 Epoch 407: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.71 
2024-02-02 23:18:57,676 EPOCH 408
2024-02-02 23:19:01,303 [Epoch: 408 Step: 00054200] Batch Recognition Loss:   0.000213 => Gls Tokens per Sec:     1522 || Batch Translation Loss:   0.025799 => Txt Tokens per Sec:     4243 || Lr: 0.000050
2024-02-02 23:19:04,882 Epoch 408: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.66 
2024-02-02 23:19:04,882 EPOCH 409
2024-02-02 23:19:06,833 [Epoch: 409 Step: 00054300] Batch Recognition Loss:   0.000191 => Gls Tokens per Sec:     1477 || Batch Translation Loss:   0.018932 => Txt Tokens per Sec:     4188 || Lr: 0.000050
2024-02-02 23:19:12,361 Epoch 409: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.67 
2024-02-02 23:19:12,361 EPOCH 410
2024-02-02 23:19:12,505 [Epoch: 410 Step: 00054400] Batch Recognition Loss:   0.000191 => Gls Tokens per Sec:     1678 || Batch Translation Loss:   0.038702 => Txt Tokens per Sec:     4804 || Lr: 0.000050
2024-02-02 23:19:18,066 [Epoch: 410 Step: 00054500] Batch Recognition Loss:   0.000292 => Gls Tokens per Sec:     1438 || Batch Translation Loss:   0.027603 => Txt Tokens per Sec:     4015 || Lr: 0.000050
2024-02-02 23:19:19,710 Epoch 410: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.36 
2024-02-02 23:19:19,710 EPOCH 411
2024-02-02 23:19:23,346 [Epoch: 411 Step: 00054600] Batch Recognition Loss:   0.000322 => Gls Tokens per Sec:     1541 || Batch Translation Loss:   0.035608 => Txt Tokens per Sec:     4270 || Lr: 0.000050
2024-02-02 23:19:27,146 Epoch 411: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.59 
2024-02-02 23:19:27,147 EPOCH 412
2024-02-02 23:19:29,048 [Epoch: 412 Step: 00054700] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:     1558 || Batch Translation Loss:   0.284450 => Txt Tokens per Sec:     4076 || Lr: 0.000050
2024-02-02 23:19:34,646 Epoch 412: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.44 
2024-02-02 23:19:34,647 EPOCH 413
2024-02-02 23:19:34,812 [Epoch: 413 Step: 00054800] Batch Recognition Loss:   0.000386 => Gls Tokens per Sec:     1932 || Batch Translation Loss:   0.038554 => Txt Tokens per Sec:     4903 || Lr: 0.000050
2024-02-02 23:19:39,766 [Epoch: 413 Step: 00054900] Batch Recognition Loss:   0.000163 => Gls Tokens per Sec:     1615 || Batch Translation Loss:   0.013712 => Txt Tokens per Sec:     4534 || Lr: 0.000050
2024-02-02 23:19:41,201 Epoch 413: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.43 
2024-02-02 23:19:41,201 EPOCH 414
2024-02-02 23:19:44,590 [Epoch: 414 Step: 00055000] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:     1677 || Batch Translation Loss:   0.022378 => Txt Tokens per Sec:     4658 || Lr: 0.000050
2024-02-02 23:19:47,696 Epoch 414: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.34 
2024-02-02 23:19:47,696 EPOCH 415
2024-02-02 23:19:49,665 [Epoch: 415 Step: 00055100] Batch Recognition Loss:   0.000193 => Gls Tokens per Sec:     1545 || Batch Translation Loss:   0.066406 => Txt Tokens per Sec:     4315 || Lr: 0.000050
2024-02-02 23:19:54,420 Epoch 415: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.78 
2024-02-02 23:19:54,420 EPOCH 416
2024-02-02 23:19:54,700 [Epoch: 416 Step: 00055200] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:     1434 || Batch Translation Loss:   0.026620 => Txt Tokens per Sec:     4366 || Lr: 0.000050
2024-02-02 23:20:00,454 [Epoch: 416 Step: 00055300] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:     1389 || Batch Translation Loss:   0.030158 => Txt Tokens per Sec:     3862 || Lr: 0.000050
2024-02-02 23:20:02,054 Epoch 416: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.93 
2024-02-02 23:20:02,055 EPOCH 417
2024-02-02 23:20:05,805 [Epoch: 417 Step: 00055400] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     1536 || Batch Translation Loss:   0.010313 => Txt Tokens per Sec:     4134 || Lr: 0.000050
2024-02-02 23:20:09,023 Epoch 417: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.50 
2024-02-02 23:20:09,023 EPOCH 418
2024-02-02 23:20:10,917 [Epoch: 418 Step: 00055500] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     1647 || Batch Translation Loss:   0.010102 => Txt Tokens per Sec:     4370 || Lr: 0.000050
2024-02-02 23:20:15,615 Epoch 418: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.07 
2024-02-02 23:20:15,615 EPOCH 419
2024-02-02 23:20:16,049 [Epoch: 419 Step: 00055600] Batch Recognition Loss:   0.000440 => Gls Tokens per Sec:     1107 || Batch Translation Loss:   0.015009 => Txt Tokens per Sec:     2902 || Lr: 0.000050
2024-02-02 23:20:21,834 [Epoch: 419 Step: 00055700] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:     1381 || Batch Translation Loss:   0.039432 => Txt Tokens per Sec:     3792 || Lr: 0.000050
2024-02-02 23:20:23,398 Epoch 419: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.11 
2024-02-02 23:20:23,399 EPOCH 420
2024-02-02 23:20:27,507 [Epoch: 420 Step: 00055800] Batch Recognition Loss:   0.000175 => Gls Tokens per Sec:     1422 || Batch Translation Loss:   0.015659 => Txt Tokens per Sec:     4025 || Lr: 0.000050
2024-02-02 23:20:30,798 Epoch 420: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.68 
2024-02-02 23:20:30,799 EPOCH 421
2024-02-02 23:20:33,019 [Epoch: 421 Step: 00055900] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     1441 || Batch Translation Loss:   0.063473 => Txt Tokens per Sec:     4045 || Lr: 0.000050
2024-02-02 23:20:38,431 Epoch 421: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.08 
2024-02-02 23:20:38,431 EPOCH 422
2024-02-02 23:20:38,964 [Epoch: 422 Step: 00056000] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     1055 || Batch Translation Loss:   0.019152 => Txt Tokens per Sec:     2989 || Lr: 0.000050
2024-02-02 23:20:49,572 Validation result at epoch 422, step    56000: duration: 10.6078s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00232	Translation Loss: 90740.52344	PPL: 8781.55762
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.96	(BLEU-1: 10.89,	BLEU-2: 3.64,	BLEU-3: 1.66,	BLEU-4: 0.96)
	CHRF 17.23	ROUGE 9.18
2024-02-02 23:20:49,573 Logging Recognition and Translation Outputs
2024-02-02 23:20:49,573 ========================================================================================================================
2024-02-02 23:20:49,573 Logging Sequence: 122_110.00
2024-02-02 23:20:49,573 	Gloss Reference :	A B+C+D+E
2024-02-02 23:20:49,574 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:20:49,574 	Gloss Alignment :	         
2024-02-02 23:20:49,574 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:20:49,574 	Text Reference  :	now that i     achieved my  dream and       secured a silver medal  
2024-02-02 23:20:49,575 	Text Hypothesis :	*** **** after her      win an    overjoyed with    a huge   problem
2024-02-02 23:20:49,575 	Text Alignment  :	D   D    S     S        S   S     S         S         S      S      
2024-02-02 23:20:49,575 ========================================================================================================================
2024-02-02 23:20:49,575 Logging Sequence: 161_111.00
2024-02-02 23:20:49,575 	Gloss Reference :	A B+C+D+E
2024-02-02 23:20:49,575 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:20:49,575 	Gloss Alignment :	         
2024-02-02 23:20:49,576 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:20:49,577 	Text Reference  :	*** his       last   game  as    captain was *** ******** ****** the  cape   town test  in south africa in jan 2022
2024-02-02 23:20:49,577 	Text Hypothesis :	the rajasthan royals owner whose name    was not revealed didn't slap taylor 2-4  times in anger but    it was held
2024-02-02 23:20:49,578 	Text Alignment  :	I   S         S      S     S     S           I   I        I      S    S      S    S        S     S      S  S   S   
2024-02-02 23:20:49,578 ========================================================================================================================
2024-02-02 23:20:49,578 Logging Sequence: 136_79.00
2024-02-02 23:20:49,578 	Gloss Reference :	A B+C+D+E
2024-02-02 23:20:49,578 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:20:49,578 	Gloss Alignment :	         
2024-02-02 23:20:49,578 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:20:49,580 	Text Reference  :	with this win   sindhu became the first indian woman to  win two         individual olympic medals
2024-02-02 23:20:49,580 	Text Hypothesis :	**** **** sadly sindhu lost   the ***** match  and   she was heartbroken over       the     loss  
2024-02-02 23:20:49,580 	Text Alignment  :	D    D    S            S          D     S      S     S   S   S           S          S       S     
2024-02-02 23:20:49,580 ========================================================================================================================
2024-02-02 23:20:49,580 Logging Sequence: 166_335.00
2024-02-02 23:20:49,580 	Gloss Reference :	A B+C+D+E
2024-02-02 23:20:49,580 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:20:49,580 	Gloss Alignment :	         
2024-02-02 23:20:49,581 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:20:49,582 	Text Reference  :	* ** the **** ****** **** ****** **** **** second world test championship is   scheduled from    june 2021 to    30  april 2023
2024-02-02 23:20:49,583 	Text Hypothesis :	2 of the best indian team bowled very well were   raj   bawa who          took 5         wickets and  ravi kumar who is    more
2024-02-02 23:20:49,583 	Text Alignment  :	I I      I    I      I    I      I    I    S      S     S    S            S    S         S       S    S    S     S   S     S   
2024-02-02 23:20:49,583 ========================================================================================================================
2024-02-02 23:20:49,583 Logging Sequence: 95_152.00
2024-02-02 23:20:49,583 	Gloss Reference :	A B+C+D+E
2024-02-02 23:20:49,583 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:20:49,583 	Gloss Alignment :	         
2024-02-02 23:20:49,583 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:20:49,584 	Text Reference  :	******** ************* how strange
2024-02-02 23:20:49,584 	Text Hypothesis :	everyone congratulated the couple 
2024-02-02 23:20:49,584 	Text Alignment  :	I        I             S   S      
2024-02-02 23:20:49,584 ========================================================================================================================
2024-02-02 23:20:55,645 [Epoch: 422 Step: 00056100] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     1317 || Batch Translation Loss:   0.024345 => Txt Tokens per Sec:     3665 || Lr: 0.000050
2024-02-02 23:20:57,110 Epoch 422: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.14 
2024-02-02 23:20:57,110 EPOCH 423
2024-02-02 23:21:01,063 [Epoch: 423 Step: 00056200] Batch Recognition Loss:   0.000287 => Gls Tokens per Sec:     1498 || Batch Translation Loss:   0.023814 => Txt Tokens per Sec:     4210 || Lr: 0.000050
2024-02-02 23:21:04,354 Epoch 423: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.16 
2024-02-02 23:21:04,354 EPOCH 424
2024-02-02 23:21:06,632 [Epoch: 424 Step: 00056300] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     1441 || Batch Translation Loss:   0.019283 => Txt Tokens per Sec:     3990 || Lr: 0.000050
2024-02-02 23:21:12,062 Epoch 424: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.29 
2024-02-02 23:21:12,063 EPOCH 425
2024-02-02 23:21:12,556 [Epoch: 425 Step: 00056400] Batch Recognition Loss:   0.000408 => Gls Tokens per Sec:     1302 || Batch Translation Loss:   0.040668 => Txt Tokens per Sec:     3706 || Lr: 0.000050
2024-02-02 23:21:18,276 [Epoch: 425 Step: 00056500] Batch Recognition Loss:   0.000273 => Gls Tokens per Sec:     1399 || Batch Translation Loss:   0.183920 => Txt Tokens per Sec:     3910 || Lr: 0.000050
2024-02-02 23:21:19,670 Epoch 425: Total Training Recognition Loss 0.04  Total Training Translation Loss 10.95 
2024-02-02 23:21:19,670 EPOCH 426
2024-02-02 23:21:23,389 [Epoch: 426 Step: 00056600] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     1614 || Batch Translation Loss:   0.024077 => Txt Tokens per Sec:     4407 || Lr: 0.000050
2024-02-02 23:21:26,226 Epoch 426: Total Training Recognition Loss 0.04  Total Training Translation Loss 11.48 
2024-02-02 23:21:26,227 EPOCH 427
2024-02-02 23:21:28,303 [Epoch: 427 Step: 00056700] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:     1619 || Batch Translation Loss:   0.059763 => Txt Tokens per Sec:     4617 || Lr: 0.000050
2024-02-02 23:21:33,389 Epoch 427: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.50 
2024-02-02 23:21:33,390 EPOCH 428
2024-02-02 23:21:33,968 [Epoch: 428 Step: 00056800] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     1248 || Batch Translation Loss:   0.026286 => Txt Tokens per Sec:     3589 || Lr: 0.000050
2024-02-02 23:21:39,612 [Epoch: 428 Step: 00056900] Batch Recognition Loss:   0.000238 => Gls Tokens per Sec:     1418 || Batch Translation Loss:   0.039921 => Txt Tokens per Sec:     3924 || Lr: 0.000050
2024-02-02 23:21:40,941 Epoch 428: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.99 
2024-02-02 23:21:40,941 EPOCH 429
2024-02-02 23:21:45,360 [Epoch: 429 Step: 00057000] Batch Recognition Loss:   0.000240 => Gls Tokens per Sec:     1376 || Batch Translation Loss:   0.036364 => Txt Tokens per Sec:     3840 || Lr: 0.000050
2024-02-02 23:21:48,417 Epoch 429: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.96 
2024-02-02 23:21:48,417 EPOCH 430
2024-02-02 23:21:50,593 [Epoch: 430 Step: 00057100] Batch Recognition Loss:   0.000264 => Gls Tokens per Sec:     1582 || Batch Translation Loss:   0.013582 => Txt Tokens per Sec:     4611 || Lr: 0.000050
2024-02-02 23:21:55,765 Epoch 430: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.26 
2024-02-02 23:21:55,766 EPOCH 431
2024-02-02 23:21:56,249 [Epoch: 431 Step: 00057200] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:     1658 || Batch Translation Loss:   0.013513 => Txt Tokens per Sec:     4351 || Lr: 0.000050
2024-02-02 23:22:02,326 [Epoch: 431 Step: 00057300] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:     1315 || Batch Translation Loss:   0.009245 => Txt Tokens per Sec:     3679 || Lr: 0.000050
2024-02-02 23:22:03,576 Epoch 431: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.58 
2024-02-02 23:22:03,576 EPOCH 432
2024-02-02 23:22:07,693 [Epoch: 432 Step: 00057400] Batch Recognition Loss:   0.000131 => Gls Tokens per Sec:     1497 || Batch Translation Loss:   0.009184 => Txt Tokens per Sec:     4167 || Lr: 0.000050
2024-02-02 23:22:10,800 Epoch 432: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.89 
2024-02-02 23:22:10,800 EPOCH 433
2024-02-02 23:22:13,406 [Epoch: 433 Step: 00057500] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:     1352 || Batch Translation Loss:   0.674213 => Txt Tokens per Sec:     3737 || Lr: 0.000050
2024-02-02 23:22:18,539 Epoch 433: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.66 
2024-02-02 23:22:18,539 EPOCH 434
2024-02-02 23:22:19,173 [Epoch: 434 Step: 00057600] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:     1390 || Batch Translation Loss:   0.020691 => Txt Tokens per Sec:     4125 || Lr: 0.000050
2024-02-02 23:22:24,719 [Epoch: 434 Step: 00057700] Batch Recognition Loss:   0.000178 => Gls Tokens per Sec:     1443 || Batch Translation Loss:   0.022389 => Txt Tokens per Sec:     3943 || Lr: 0.000050
2024-02-02 23:22:26,030 Epoch 434: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.70 
2024-02-02 23:22:26,030 EPOCH 435
2024-02-02 23:22:30,416 [Epoch: 435 Step: 00057800] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:     1423 || Batch Translation Loss:   0.055873 => Txt Tokens per Sec:     3973 || Lr: 0.000050
2024-02-02 23:22:33,358 Epoch 435: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.56 
2024-02-02 23:22:33,358 EPOCH 436
2024-02-02 23:22:35,606 [Epoch: 436 Step: 00057900] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:     1602 || Batch Translation Loss:   0.022253 => Txt Tokens per Sec:     4346 || Lr: 0.000050
2024-02-02 23:22:40,784 Epoch 436: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.73 
2024-02-02 23:22:40,784 EPOCH 437
2024-02-02 23:22:41,397 [Epoch: 437 Step: 00058000] Batch Recognition Loss:   0.000278 => Gls Tokens per Sec:     1569 || Batch Translation Loss:   0.023679 => Txt Tokens per Sec:     4149 || Lr: 0.000050
2024-02-02 23:22:52,631 Validation result at epoch 437, step    58000: duration: 11.2342s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00245	Translation Loss: 91475.87500	PPL: 9452.13086
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.96	(BLEU-1: 10.82,	BLEU-2: 3.56,	BLEU-3: 1.66,	BLEU-4: 0.96)
	CHRF 16.88	ROUGE 9.29
2024-02-02 23:22:52,632 Logging Recognition and Translation Outputs
2024-02-02 23:22:52,633 ========================================================================================================================
2024-02-02 23:22:52,633 Logging Sequence: 180_138.00
2024-02-02 23:22:52,633 	Gloss Reference :	A B+C+D+E
2024-02-02 23:22:52,633 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:22:52,633 	Gloss Alignment :	         
2024-02-02 23:22:52,633 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:22:52,635 	Text Reference  :	ioa president p t usha constituted a    seven-member panel which included world champions from various sports    to  inquire into   the allegations
2024-02-02 23:22:52,635 	Text Hypothesis :	*** ********* * * **** singh       also said         that  he    has      full  faith     in   the     judiciary and will    accpet the verdict    
2024-02-02 23:22:52,635 	Text Alignment  :	D   D         D D D    S           S    S            S     S     S        S     S         S    S       S         S   S       S          S          
2024-02-02 23:22:52,636 ========================================================================================================================
2024-02-02 23:22:52,636 Logging Sequence: 128_189.00
2024-02-02 23:22:52,636 	Gloss Reference :	A B+C+D+E
2024-02-02 23:22:52,636 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:22:52,636 	Gloss Alignment :	         
2024-02-02 23:22:52,636 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:22:52,637 	Text Reference  :	*** meanwhile some funny incidents happened during the ***** match
2024-02-02 23:22:52,637 	Text Hypothesis :	for the       icc  world cup       2022     at     the first time 
2024-02-02 23:22:52,637 	Text Alignment  :	I   S         S    S     S         S        S          I     S    
2024-02-02 23:22:52,637 ========================================================================================================================
2024-02-02 23:22:52,637 Logging Sequence: 165_523.00
2024-02-02 23:22:52,638 	Gloss Reference :	A B+C+D+E
2024-02-02 23:22:52,638 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:22:52,638 	Gloss Alignment :	         
2024-02-02 23:22:52,638 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:22:52,639 	Text Reference  :	as he believed that his team might lose if he  takes off    his batting pads      
2024-02-02 23:22:52,639 	Text Hypothesis :	** ** ******** **** and they will  lose ** one of    vamika in  the     tournament
2024-02-02 23:22:52,639 	Text Alignment  :	D  D  D        D    S   S    S          D  S   S     S      S   S       S         
2024-02-02 23:22:52,639 ========================================================================================================================
2024-02-02 23:22:52,640 Logging Sequence: 145_168.00
2024-02-02 23:22:52,640 	Gloss Reference :	A B+C+D+E
2024-02-02 23:22:52,640 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:22:52,640 	Gloss Alignment :	         
2024-02-02 23:22:52,640 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:22:52,641 	Text Reference  :	the decision  has devastated sameeha and     her parents
2024-02-02 23:22:52,641 	Text Hypothesis :	the committee has ********** gained  control 5   years  
2024-02-02 23:22:52,641 	Text Alignment  :	    S             D          S       S       S   S      
2024-02-02 23:22:52,641 ========================================================================================================================
2024-02-02 23:22:52,641 Logging Sequence: 92_123.00
2024-02-02 23:22:52,641 	Gloss Reference :	A B+C+D+E
2024-02-02 23:22:52,641 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:22:52,641 	Gloss Alignment :	         
2024-02-02 23:22:52,642 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:22:52,643 	Text Reference  :	a heated argument also took place between members of the family and the    two    men 
2024-02-02 23:22:52,643 	Text Hypothesis :	* and    is       also **** ***** a       report  by the ****** *** police learnt that
2024-02-02 23:22:52,643 	Text Alignment  :	D S      S             D    D     S       S       S      D      D   S      S      S   
2024-02-02 23:22:52,643 ========================================================================================================================
2024-02-02 23:22:58,695 [Epoch: 437 Step: 00058100] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     1319 || Batch Translation Loss:   0.029109 => Txt Tokens per Sec:     3676 || Lr: 0.000050
2024-02-02 23:22:59,918 Epoch 437: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.49 
2024-02-02 23:22:59,918 EPOCH 438
2024-02-02 23:23:04,118 [Epoch: 438 Step: 00058200] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     1505 || Batch Translation Loss:   0.021982 => Txt Tokens per Sec:     4235 || Lr: 0.000050
2024-02-02 23:23:06,861 Epoch 438: Total Training Recognition Loss 0.03  Total Training Translation Loss 13.36 
2024-02-02 23:23:06,861 EPOCH 439
2024-02-02 23:23:09,149 [Epoch: 439 Step: 00058300] Batch Recognition Loss:   0.000302 => Gls Tokens per Sec:     1609 || Batch Translation Loss:   0.033469 => Txt Tokens per Sec:     4464 || Lr: 0.000050
2024-02-02 23:23:14,289 Epoch 439: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.84 
2024-02-02 23:23:14,289 EPOCH 440
2024-02-02 23:23:14,973 [Epoch: 440 Step: 00058400] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:     1523 || Batch Translation Loss:   0.024607 => Txt Tokens per Sec:     3698 || Lr: 0.000050
2024-02-02 23:23:20,621 [Epoch: 440 Step: 00058500] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:     1417 || Batch Translation Loss:   0.012691 => Txt Tokens per Sec:     3993 || Lr: 0.000050
2024-02-02 23:23:21,809 Epoch 440: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.54 
2024-02-02 23:23:21,809 EPOCH 441
2024-02-02 23:23:26,351 [Epoch: 441 Step: 00058600] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     1409 || Batch Translation Loss:   0.010022 => Txt Tokens per Sec:     3970 || Lr: 0.000050
2024-02-02 23:23:29,251 Epoch 441: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.82 
2024-02-02 23:23:29,251 EPOCH 442
2024-02-02 23:23:31,599 [Epoch: 442 Step: 00058700] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:     1602 || Batch Translation Loss:   0.032036 => Txt Tokens per Sec:     4637 || Lr: 0.000050
2024-02-02 23:23:35,978 Epoch 442: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.01 
2024-02-02 23:23:35,978 EPOCH 443
2024-02-02 23:23:36,693 [Epoch: 443 Step: 00058800] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     1569 || Batch Translation Loss:   0.028375 => Txt Tokens per Sec:     4309 || Lr: 0.000050
2024-02-02 23:23:42,774 [Epoch: 443 Step: 00058900] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     1314 || Batch Translation Loss:   0.022400 => Txt Tokens per Sec:     3638 || Lr: 0.000050
2024-02-02 23:23:43,833 Epoch 443: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.18 
2024-02-02 23:23:43,834 EPOCH 444
2024-02-02 23:23:48,273 [Epoch: 444 Step: 00059000] Batch Recognition Loss:   0.000235 => Gls Tokens per Sec:     1460 || Batch Translation Loss:   0.022572 => Txt Tokens per Sec:     4013 || Lr: 0.000050
2024-02-02 23:23:51,130 Epoch 444: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.15 
2024-02-02 23:23:51,130 EPOCH 445
2024-02-02 23:23:53,800 [Epoch: 445 Step: 00059100] Batch Recognition Loss:   0.000248 => Gls Tokens per Sec:     1439 || Batch Translation Loss:   0.018212 => Txt Tokens per Sec:     4032 || Lr: 0.000050
2024-02-02 23:23:58,648 Epoch 445: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.29 
2024-02-02 23:23:58,649 EPOCH 446
2024-02-02 23:23:59,448 [Epoch: 446 Step: 00059200] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:     1505 || Batch Translation Loss:   0.035621 => Txt Tokens per Sec:     3998 || Lr: 0.000050
2024-02-02 23:24:05,144 [Epoch: 446 Step: 00059300] Batch Recognition Loss:   0.000333 => Gls Tokens per Sec:     1403 || Batch Translation Loss:   0.039420 => Txt Tokens per Sec:     3896 || Lr: 0.000050
2024-02-02 23:24:06,214 Epoch 446: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.83 
2024-02-02 23:24:06,214 EPOCH 447
2024-02-02 23:24:10,755 [Epoch: 447 Step: 00059400] Batch Recognition Loss:   0.000131 => Gls Tokens per Sec:     1445 || Batch Translation Loss:   0.022233 => Txt Tokens per Sec:     3927 || Lr: 0.000050
2024-02-02 23:24:13,747 Epoch 447: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.11 
2024-02-02 23:24:13,747 EPOCH 448
2024-02-02 23:24:16,602 [Epoch: 448 Step: 00059500] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     1373 || Batch Translation Loss:   0.022724 => Txt Tokens per Sec:     3864 || Lr: 0.000050
2024-02-02 23:24:21,496 Epoch 448: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.42 
2024-02-02 23:24:21,496 EPOCH 449
2024-02-02 23:24:22,578 [Epoch: 449 Step: 00059600] Batch Recognition Loss:   0.000296 => Gls Tokens per Sec:     1184 || Batch Translation Loss:   0.085614 => Txt Tokens per Sec:     3512 || Lr: 0.000050
2024-02-02 23:24:28,237 [Epoch: 449 Step: 00059700] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     1414 || Batch Translation Loss:   0.026341 => Txt Tokens per Sec:     3881 || Lr: 0.000050
2024-02-02 23:24:29,259 Epoch 449: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.85 
2024-02-02 23:24:29,259 EPOCH 450
2024-02-02 23:24:33,861 [Epoch: 450 Step: 00059800] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     1443 || Batch Translation Loss:   0.026915 => Txt Tokens per Sec:     4012 || Lr: 0.000050
2024-02-02 23:24:36,406 Epoch 450: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.78 
2024-02-02 23:24:36,406 EPOCH 451
2024-02-02 23:24:38,791 [Epoch: 451 Step: 00059900] Batch Recognition Loss:   0.000245 => Gls Tokens per Sec:     1678 || Batch Translation Loss:   0.026621 => Txt Tokens per Sec:     4701 || Lr: 0.000050
2024-02-02 23:24:42,857 Epoch 451: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.86 
2024-02-02 23:24:42,858 EPOCH 452
2024-02-02 23:24:43,696 [Epoch: 452 Step: 00060000] Batch Recognition Loss:   0.000197 => Gls Tokens per Sec:     1625 || Batch Translation Loss:   0.237538 => Txt Tokens per Sec:     4741 || Lr: 0.000050
2024-02-02 23:24:54,746 Validation result at epoch 452, step    60000: duration: 11.0502s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00288	Translation Loss: 92106.32812	PPL: 10067.67285
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.72	(BLEU-1: 10.61,	BLEU-2: 3.21,	BLEU-3: 1.38,	BLEU-4: 0.72)
	CHRF 16.56	ROUGE 9.17
2024-02-02 23:24:54,747 Logging Recognition and Translation Outputs
2024-02-02 23:24:54,747 ========================================================================================================================
2024-02-02 23:24:54,747 Logging Sequence: 179_269.00
2024-02-02 23:24:54,747 	Gloss Reference :	A B+C+D+E
2024-02-02 23:24:54,747 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:24:54,747 	Gloss Alignment :	         
2024-02-02 23:24:54,748 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:24:54,748 	Text Reference  :	the ban would mean she can't compete in any    national or  other domestic events 
2024-02-02 23:24:54,748 	Text Hypothesis :	*** *** ***** **** *** ***** ******* ** vinesh phogat   got her   schedule changed
2024-02-02 23:24:54,749 	Text Alignment  :	D   D   D     D    D   D     D       D  S      S        S   S     S        S      
2024-02-02 23:24:54,749 ========================================================================================================================
2024-02-02 23:24:54,749 Logging Sequence: 94_253.00
2024-02-02 23:24:54,749 	Gloss Reference :	A B+C+D+E
2024-02-02 23:24:54,749 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:24:54,749 	Gloss Alignment :	         
2024-02-02 23:24:54,749 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:24:54,751 	Text Reference  :	however some tickets will be    kept aside       for   physical sale      at    the stadiums a few days prior to   the match    
2024-02-02 23:24:54,751 	Text Hypothesis :	******* for  the     sri  lanka vs   afghanistan match is       currently asked him with     a *** **** ***** huge fan following
2024-02-02 23:24:54,751 	Text Alignment  :	D       S    S       S    S     S    S           S     S        S         S     S   S          D   D    D     S    S   S        
2024-02-02 23:24:54,752 ========================================================================================================================
2024-02-02 23:24:54,752 Logging Sequence: 114_201.00
2024-02-02 23:24:54,752 	Gloss Reference :	A B+C+D+E
2024-02-02 23:24:54,752 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:24:54,752 	Gloss Alignment :	         
2024-02-02 23:24:54,752 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:24:54,753 	Text Reference  :	********** **** ** *** ********* *** **** this is    his first time winning the copa
2024-02-02 23:24:54,753 	Text Hypothesis :	vengsarkar said he was extremely fit when i    asked him about his  routine he  said
2024-02-02 23:24:54,753 	Text Alignment  :	I          I    I  I   I         I   I    S    S     S   S     S    S       S   S   
2024-02-02 23:24:54,753 ========================================================================================================================
2024-02-02 23:24:54,753 Logging Sequence: 118_104.00
2024-02-02 23:24:54,754 	Gloss Reference :	A B+C+D+E
2024-02-02 23:24:54,754 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:24:54,754 	Gloss Alignment :	         
2024-02-02 23:24:54,754 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:24:54,755 	Text Reference  :	kylian mbappã strong performance in  the   match was  greatly appreciated
2024-02-02 23:24:54,755 	Text Hypothesis :	after  the    tweet  he          was given a     sigh of      relief     
2024-02-02 23:24:54,755 	Text Alignment  :	S      S      S      S           S   S     S     S    S       S          
2024-02-02 23:24:54,755 ========================================================================================================================
2024-02-02 23:24:54,755 Logging Sequence: 144_74.00
2024-02-02 23:24:54,755 	Gloss Reference :	A B+C+D+E
2024-02-02 23:24:54,756 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:24:54,756 	Gloss Alignment :	         
2024-02-02 23:24:54,756 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:24:54,756 	Text Reference  :	** ***** isn't that amazing
2024-02-02 23:24:54,756 	Text Hypothesis :	so these are   the  rules  
2024-02-02 23:24:54,756 	Text Alignment  :	I  I     S     S    S      
2024-02-02 23:24:54,756 ========================================================================================================================
2024-02-02 23:25:00,635 [Epoch: 452 Step: 00060100] Batch Recognition Loss:   0.000197 => Gls Tokens per Sec:     1358 || Batch Translation Loss:   0.027463 => Txt Tokens per Sec:     3729 || Lr: 0.000050
2024-02-02 23:25:01,575 Epoch 452: Total Training Recognition Loss 0.03  Total Training Translation Loss 11.01 
2024-02-02 23:25:01,575 EPOCH 453
2024-02-02 23:25:06,453 [Epoch: 453 Step: 00060200] Batch Recognition Loss:   0.000270 => Gls Tokens per Sec:     1378 || Batch Translation Loss:   0.041208 => Txt Tokens per Sec:     3881 || Lr: 0.000050
2024-02-02 23:25:09,289 Epoch 453: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.19 
2024-02-02 23:25:09,289 EPOCH 454
2024-02-02 23:25:12,157 [Epoch: 454 Step: 00060300] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:     1423 || Batch Translation Loss:   0.012264 => Txt Tokens per Sec:     3927 || Lr: 0.000050
2024-02-02 23:25:16,549 Epoch 454: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.26 
2024-02-02 23:25:16,550 EPOCH 455
2024-02-02 23:25:17,425 [Epoch: 455 Step: 00060400] Batch Recognition Loss:   0.000338 => Gls Tokens per Sec:     1646 || Batch Translation Loss:   0.029037 => Txt Tokens per Sec:     4482 || Lr: 0.000050
2024-02-02 23:25:22,765 [Epoch: 455 Step: 00060500] Batch Recognition Loss:   0.000121 => Gls Tokens per Sec:     1498 || Batch Translation Loss:   0.020547 => Txt Tokens per Sec:     4174 || Lr: 0.000050
2024-02-02 23:25:23,790 Epoch 455: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.97 
2024-02-02 23:25:23,791 EPOCH 456
2024-02-02 23:25:28,578 [Epoch: 456 Step: 00060600] Batch Recognition Loss:   0.000274 => Gls Tokens per Sec:     1421 || Batch Translation Loss:   0.017007 => Txt Tokens per Sec:     3927 || Lr: 0.000050
2024-02-02 23:25:31,299 Epoch 456: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.09 
2024-02-02 23:25:31,299 EPOCH 457
2024-02-02 23:25:34,346 [Epoch: 457 Step: 00060700] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     1366 || Batch Translation Loss:   0.049255 => Txt Tokens per Sec:     3989 || Lr: 0.000050
2024-02-02 23:25:38,963 Epoch 457: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.58 
2024-02-02 23:25:38,964 EPOCH 458
2024-02-02 23:25:40,103 [Epoch: 458 Step: 00060800] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:     1337 || Batch Translation Loss:   0.016394 => Txt Tokens per Sec:     4048 || Lr: 0.000050
2024-02-02 23:25:45,858 [Epoch: 458 Step: 00060900] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:     1390 || Batch Translation Loss:   0.025538 => Txt Tokens per Sec:     3832 || Lr: 0.000050
2024-02-02 23:25:46,625 Epoch 458: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.99 
2024-02-02 23:25:46,625 EPOCH 459
2024-02-02 23:25:51,282 [Epoch: 459 Step: 00061000] Batch Recognition Loss:   0.000213 => Gls Tokens per Sec:     1478 || Batch Translation Loss:   0.046653 => Txt Tokens per Sec:     4081 || Lr: 0.000050
2024-02-02 23:25:53,682 Epoch 459: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.65 
2024-02-02 23:25:53,682 EPOCH 460
2024-02-02 23:25:56,142 [Epoch: 460 Step: 00061100] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     1724 || Batch Translation Loss:   0.023301 => Txt Tokens per Sec:     4659 || Lr: 0.000050
2024-02-02 23:26:00,253 Epoch 460: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.13 
2024-02-02 23:26:00,253 EPOCH 461
2024-02-02 23:26:01,377 [Epoch: 461 Step: 00061200] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     1425 || Batch Translation Loss:   0.031572 => Txt Tokens per Sec:     3785 || Lr: 0.000050
2024-02-02 23:26:06,930 [Epoch: 461 Step: 00061300] Batch Recognition Loss:   0.000303 => Gls Tokens per Sec:     1439 || Batch Translation Loss:   0.041100 => Txt Tokens per Sec:     4053 || Lr: 0.000050
2024-02-02 23:26:07,607 Epoch 461: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.06 
2024-02-02 23:26:07,608 EPOCH 462
2024-02-02 23:26:12,565 [Epoch: 462 Step: 00061400] Batch Recognition Loss:   0.000278 => Gls Tokens per Sec:     1404 || Batch Translation Loss:   0.050296 => Txt Tokens per Sec:     3956 || Lr: 0.000050
2024-02-02 23:26:15,253 Epoch 462: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.73 
2024-02-02 23:26:15,253 EPOCH 463
2024-02-02 23:26:18,045 [Epoch: 463 Step: 00061500] Batch Recognition Loss:   0.000240 => Gls Tokens per Sec:     1548 || Batch Translation Loss:   0.034407 => Txt Tokens per Sec:     4317 || Lr: 0.000050
2024-02-02 23:26:21,967 Epoch 463: Total Training Recognition Loss 0.04  Total Training Translation Loss 13.30 
2024-02-02 23:26:21,968 EPOCH 464
2024-02-02 23:26:23,178 [Epoch: 464 Step: 00061600] Batch Recognition Loss:   0.000447 => Gls Tokens per Sec:     1388 || Batch Translation Loss:   0.029076 => Txt Tokens per Sec:     4103 || Lr: 0.000050
2024-02-02 23:26:28,376 [Epoch: 464 Step: 00061700] Batch Recognition Loss:   0.000304 => Gls Tokens per Sec:     1538 || Batch Translation Loss:   0.041506 => Txt Tokens per Sec:     4230 || Lr: 0.000050
2024-02-02 23:26:29,027 Epoch 464: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.21 
2024-02-02 23:26:29,027 EPOCH 465
2024-02-02 23:26:34,159 [Epoch: 465 Step: 00061800] Batch Recognition Loss:   0.000386 => Gls Tokens per Sec:     1372 || Batch Translation Loss:   0.022701 => Txt Tokens per Sec:     3839 || Lr: 0.000050
2024-02-02 23:26:36,642 Epoch 465: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.12 
2024-02-02 23:26:36,643 EPOCH 466
2024-02-02 23:26:39,777 [Epoch: 466 Step: 00061900] Batch Recognition Loss:   0.000151 => Gls Tokens per Sec:     1404 || Batch Translation Loss:   0.010135 => Txt Tokens per Sec:     3951 || Lr: 0.000050
2024-02-02 23:26:44,110 Epoch 466: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.02 
2024-02-02 23:26:44,112 EPOCH 467
2024-02-02 23:26:45,402 [Epoch: 467 Step: 00062000] Batch Recognition Loss:   0.000265 => Gls Tokens per Sec:     1364 || Batch Translation Loss:   0.173248 => Txt Tokens per Sec:     3990 || Lr: 0.000050
2024-02-02 23:26:56,360 Validation result at epoch 467, step    62000: duration: 10.9580s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00282	Translation Loss: 90866.87500	PPL: 8893.29492
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.93	(BLEU-1: 10.81,	BLEU-2: 3.45,	BLEU-3: 1.60,	BLEU-4: 0.93)
	CHRF 17.01	ROUGE 9.21
2024-02-02 23:26:56,361 Logging Recognition and Translation Outputs
2024-02-02 23:26:56,361 ========================================================================================================================
2024-02-02 23:26:56,361 Logging Sequence: 87_52.00
2024-02-02 23:26:56,361 	Gloss Reference :	A B+C+D+E
2024-02-02 23:26:56,362 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:26:56,362 	Gloss Alignment :	         
2024-02-02 23:26:56,362 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:26:56,363 	Text Reference  :	that is     when gambhir walked into bat   and  rescued india with   his brilliant 97      runs  
2024-02-02 23:26:56,363 	Text Hypothesis :	csk  scored 37   runs    in     20   overs they won     6     medals in  the       various groups
2024-02-02 23:26:56,363 	Text Alignment  :	S    S      S    S       S      S    S     S    S       S     S      S   S         S       S     
2024-02-02 23:26:56,364 ========================================================================================================================
2024-02-02 23:26:56,364 Logging Sequence: 85_2.00
2024-02-02 23:26:56,364 	Gloss Reference :	A B+C+D+E
2024-02-02 23:26:56,364 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:26:56,364 	Gloss Alignment :	         
2024-02-02 23:26:56,364 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:26:56,365 	Text Reference  :	** *** andrew symonds is      one   of      the finest all       rounders in the history of australian cricket
2024-02-02 23:26:56,366 	Text Hypothesis :	he was also   a       violent clash betweem the afghan pakistani fans     in *** ******* ** ********** india  
2024-02-02 23:26:56,366 	Text Alignment  :	I  I   S      S       S       S     S           S      S         S           D   D       D  D          S      
2024-02-02 23:26:56,366 ========================================================================================================================
2024-02-02 23:26:56,366 Logging Sequence: 51_110.00
2024-02-02 23:26:56,366 	Gloss Reference :	A B+C+D+E
2024-02-02 23:26:56,366 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:26:56,366 	Gloss Alignment :	         
2024-02-02 23:26:56,366 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:26:56,367 	Text Reference  :	the aussies were very happy with their   victory
2024-02-02 23:26:56,367 	Text Hypothesis :	*** ******* **** they own   the  lucknow team   
2024-02-02 23:26:56,367 	Text Alignment  :	D   D       D    S    S     S    S       S      
2024-02-02 23:26:56,367 ========================================================================================================================
2024-02-02 23:26:56,368 Logging Sequence: 72_59.00
2024-02-02 23:26:56,368 	Gloss Reference :	A B+C+D+E
2024-02-02 23:26:56,368 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:26:56,368 	Gloss Alignment :	         
2024-02-02 23:26:56,368 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:26:56,370 	Text Reference  :	*** *** after that sapna and *** * **** shobit started arguing and misbehaving with    the ***** ****** *** *** ******* ** cricketer
2024-02-02 23:26:56,370 	Text Hypothesis :	you all know  that kohli and put a very few    days    back    the argument    however the exact reason was not present to see      
2024-02-02 23:26:56,370 	Text Alignment  :	I   I   S          S         I   I I    S      S       S       S   S           S           I     I      I   I   I       I  S        
2024-02-02 23:26:56,370 ========================================================================================================================
2024-02-02 23:26:56,371 Logging Sequence: 122_184.00
2024-02-02 23:26:56,371 	Gloss Reference :	A B+C+D+E
2024-02-02 23:26:56,371 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:26:56,371 	Gloss Alignment :	         
2024-02-02 23:26:56,371 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:26:56,372 	Text Reference  :	are playing exceptionally well and keeping hopes of  further olympic medals alive
2024-02-02 23:26:56,372 	Text Hypothesis :	*** the     kiwis         won  the world   cup   for held    in      uae    oman 
2024-02-02 23:26:56,372 	Text Alignment  :	D   S       S             S    S   S       S     S   S       S       S      S    
2024-02-02 23:26:56,372 ========================================================================================================================
2024-02-02 23:27:02,143 [Epoch: 467 Step: 00062100] Batch Recognition Loss:   0.000297 => Gls Tokens per Sec:     1384 || Batch Translation Loss:   0.018143 => Txt Tokens per Sec:     3769 || Lr: 0.000050
2024-02-02 23:27:02,827 Epoch 467: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.48 
2024-02-02 23:27:02,827 EPOCH 468
2024-02-02 23:27:07,744 [Epoch: 468 Step: 00062200] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:     1448 || Batch Translation Loss:   0.088728 => Txt Tokens per Sec:     3985 || Lr: 0.000050
2024-02-02 23:27:10,396 Epoch 468: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.79 
2024-02-02 23:27:10,397 EPOCH 469
2024-02-02 23:27:13,365 [Epoch: 469 Step: 00062300] Batch Recognition Loss:   0.000289 => Gls Tokens per Sec:     1510 || Batch Translation Loss:   0.023660 => Txt Tokens per Sec:     4247 || Lr: 0.000050
2024-02-02 23:27:17,844 Epoch 469: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.47 
2024-02-02 23:27:17,844 EPOCH 470
2024-02-02 23:27:19,201 [Epoch: 470 Step: 00062400] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:     1357 || Batch Translation Loss:   0.027903 => Txt Tokens per Sec:     3995 || Lr: 0.000050
2024-02-02 23:27:24,867 [Epoch: 470 Step: 00062500] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:     1410 || Batch Translation Loss:   0.042568 => Txt Tokens per Sec:     3830 || Lr: 0.000050
2024-02-02 23:27:25,473 Epoch 470: Total Training Recognition Loss 0.03  Total Training Translation Loss 9.06 
2024-02-02 23:27:25,473 EPOCH 471
2024-02-02 23:27:30,730 [Epoch: 471 Step: 00062600] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     1370 || Batch Translation Loss:   0.039525 => Txt Tokens per Sec:     3793 || Lr: 0.000050
2024-02-02 23:27:33,213 Epoch 471: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.94 
2024-02-02 23:27:33,213 EPOCH 472
2024-02-02 23:27:36,388 [Epoch: 472 Step: 00062700] Batch Recognition Loss:   0.000359 => Gls Tokens per Sec:     1437 || Batch Translation Loss:   0.097631 => Txt Tokens per Sec:     4001 || Lr: 0.000050
2024-02-02 23:27:40,604 Epoch 472: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.95 
2024-02-02 23:27:40,604 EPOCH 473
2024-02-02 23:27:41,706 [Epoch: 473 Step: 00062800] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     1742 || Batch Translation Loss:   0.024096 => Txt Tokens per Sec:     4747 || Lr: 0.000050
2024-02-02 23:27:47,619 [Epoch: 473 Step: 00062900] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:     1353 || Batch Translation Loss:   1.279346 => Txt Tokens per Sec:     3766 || Lr: 0.000050
2024-02-02 23:27:48,173 Epoch 473: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.64 
2024-02-02 23:27:48,173 EPOCH 474
2024-02-02 23:27:53,164 [Epoch: 474 Step: 00063000] Batch Recognition Loss:   0.000202 => Gls Tokens per Sec:     1459 || Batch Translation Loss:   0.020628 => Txt Tokens per Sec:     4017 || Lr: 0.000050
2024-02-02 23:27:55,589 Epoch 474: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.00 
2024-02-02 23:27:55,589 EPOCH 475
2024-02-02 23:27:58,592 [Epoch: 475 Step: 00063100] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:     1545 || Batch Translation Loss:   0.009498 => Txt Tokens per Sec:     4257 || Lr: 0.000050
2024-02-02 23:28:02,726 Epoch 475: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.07 
2024-02-02 23:28:02,727 EPOCH 476
2024-02-02 23:28:04,179 [Epoch: 476 Step: 00063200] Batch Recognition Loss:   0.000220 => Gls Tokens per Sec:     1379 || Batch Translation Loss:   0.032825 => Txt Tokens per Sec:     3688 || Lr: 0.000050
2024-02-02 23:28:10,008 [Epoch: 476 Step: 00063300] Batch Recognition Loss:   0.000266 => Gls Tokens per Sec:     1371 || Batch Translation Loss:   0.030347 => Txt Tokens per Sec:     3847 || Lr: 0.000050
2024-02-02 23:28:10,462 Epoch 476: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.44 
2024-02-02 23:28:10,462 EPOCH 477
2024-02-02 23:28:15,696 [Epoch: 477 Step: 00063400] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:     1406 || Batch Translation Loss:   0.009480 => Txt Tokens per Sec:     3899 || Lr: 0.000050
2024-02-02 23:28:17,806 Epoch 477: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.45 
2024-02-02 23:28:17,807 EPOCH 478
2024-02-02 23:28:20,789 [Epoch: 478 Step: 00063500] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:     1584 || Batch Translation Loss:   0.011081 => Txt Tokens per Sec:     4392 || Lr: 0.000050
2024-02-02 23:28:25,030 Epoch 478: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.72 
2024-02-02 23:28:25,030 EPOCH 479
2024-02-02 23:28:26,265 [Epoch: 479 Step: 00063600] Batch Recognition Loss:   0.000114 => Gls Tokens per Sec:     1687 || Batch Translation Loss:   0.010889 => Txt Tokens per Sec:     4423 || Lr: 0.000050
2024-02-02 23:28:32,123 [Epoch: 479 Step: 00063700] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     1364 || Batch Translation Loss:   0.037396 => Txt Tokens per Sec:     3870 || Lr: 0.000050
2024-02-02 23:28:32,473 Epoch 479: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.90 
2024-02-02 23:28:32,473 EPOCH 480
2024-02-02 23:28:37,773 [Epoch: 480 Step: 00063800] Batch Recognition Loss:   0.000180 => Gls Tokens per Sec:     1404 || Batch Translation Loss:   0.026762 => Txt Tokens per Sec:     3908 || Lr: 0.000050
2024-02-02 23:28:39,955 Epoch 480: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.50 
2024-02-02 23:28:39,955 EPOCH 481
2024-02-02 23:28:43,330 [Epoch: 481 Step: 00063900] Batch Recognition Loss:   0.000374 => Gls Tokens per Sec:     1423 || Batch Translation Loss:   0.076372 => Txt Tokens per Sec:     4037 || Lr: 0.000050
2024-02-02 23:28:47,200 Epoch 481: Total Training Recognition Loss 0.04  Total Training Translation Loss 12.17 
2024-02-02 23:28:47,200 EPOCH 482
2024-02-02 23:28:48,622 [Epoch: 482 Step: 00064000] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     1520 || Batch Translation Loss:   0.028335 => Txt Tokens per Sec:     4293 || Lr: 0.000050
2024-02-02 23:28:59,449 Validation result at epoch 482, step    64000: duration: 10.8276s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00327	Translation Loss: 92121.04688	PPL: 10082.50879
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.40	(BLEU-1: 9.91,	BLEU-2: 2.85,	BLEU-3: 0.98,	BLEU-4: 0.40)
	CHRF 16.39	ROUGE 8.73
2024-02-02 23:28:59,450 Logging Recognition and Translation Outputs
2024-02-02 23:28:59,450 ========================================================================================================================
2024-02-02 23:28:59,450 Logging Sequence: 72_2.00
2024-02-02 23:28:59,451 	Gloss Reference :	A B+C+D+E
2024-02-02 23:28:59,451 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:28:59,451 	Gloss Alignment :	         
2024-02-02 23:28:59,451 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:28:59,454 	Text Reference  :	***** star  india batter     prithvi shaw and friend ashish yadav went to   a 5 star  hotel for     dinner on 15   february early morning
2024-02-02 23:28:59,454 	Text Hypothesis :	later mirza was   witnessing bad     form in  her    career and   she  took a * break from  playing tennis in 2017 for      a     surgery
2024-02-02 23:28:59,454 	Text Alignment  :	I     S     S     S          S       S    S   S      S      S     S    S      D S     S     S       S      S  S    S        S     S      
2024-02-02 23:28:59,454 ========================================================================================================================
2024-02-02 23:28:59,454 Logging Sequence: 138_257.00
2024-02-02 23:28:59,454 	Gloss Reference :	A B+C+D+E
2024-02-02 23:28:59,454 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:28:59,455 	Gloss Alignment :	         
2024-02-02 23:28:59,455 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:28:59,456 	Text Reference  :	*** ****** ** ****** have      vehemently condemned the    racial attacks towards the *** three players
2024-02-02 23:28:59,456 	Text Hypothesis :	the finals of junior wimbledon which      is        tennis for    players below   the age of    18     
2024-02-02 23:28:59,456 	Text Alignment  :	I   I      I  I      S         S          S         S      S      S       S           I   S     S      
2024-02-02 23:28:59,456 ========================================================================================================================
2024-02-02 23:28:59,456 Logging Sequence: 170_134.00
2024-02-02 23:28:59,456 	Gloss Reference :	A B+C+D+E
2024-02-02 23:28:59,456 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:28:59,456 	Gloss Alignment :	         
2024-02-02 23:28:59,457 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:28:59,459 	Text Reference  :	taylor felt that the owner at the back of the mind was upset that he     had  paid    8    crores but  taylor failed to    score a **** run 
2024-02-02 23:28:59,459 	Text Hypothesis :	****** **** **** *** ***** ** *** **** ** *** **** *** ***** now  people were shocked with this   news and    what   about him   a good luck
2024-02-02 23:28:59,459 	Text Alignment  :	D      D    D    D   D     D  D   D    D  D   D    D   D     S    S      S    S       S    S      S    S      S      S     S       I    S   
2024-02-02 23:28:59,459 ========================================================================================================================
2024-02-02 23:28:59,460 Logging Sequence: 70_39.00
2024-02-02 23:28:59,460 	Gloss Reference :	A B+C+D+E
2024-02-02 23:28:59,460 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:28:59,460 	Gloss Alignment :	         
2024-02-02 23:28:59,460 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:28:59,461 	Text Reference  :	well the tournament called euro 2020 when it    is        being held in    2021       
2024-02-02 23:28:59,461 	Text Hypothesis :	**** *** ********** ****** **** **** **** since pathirana was   the  first tie-breaker
2024-02-02 23:28:59,461 	Text Alignment  :	D    D   D          D      D    D    D    S     S         S     S    S     S          
2024-02-02 23:28:59,461 ========================================================================================================================
2024-02-02 23:28:59,461 Logging Sequence: 124_172.00
2024-02-02 23:28:59,461 	Gloss Reference :	A B+C+D+E
2024-02-02 23:28:59,462 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:28:59,462 	Gloss Alignment :	         
2024-02-02 23:28:59,462 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:28:59,463 	Text Reference  :	**** ********* ****** ******* the ipl   matches   will be   held     only in  pune mumbai at 25  capacity
2024-02-02 23:28:59,463 	Text Hypothesis :	bcci president sourav ganguly and board secretary jay  shah welcomed the  two new  teams  to the ipl     
2024-02-02 23:28:59,463 	Text Alignment  :	I    I         I      I       S   S     S         S    S    S        S    S   S    S      S  S   S       
2024-02-02 23:28:59,464 ========================================================================================================================
2024-02-02 23:29:05,316 [Epoch: 482 Step: 00064100] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     1362 || Batch Translation Loss:   0.028210 => Txt Tokens per Sec:     3759 || Lr: 0.000050
2024-02-02 23:29:05,687 Epoch 482: Total Training Recognition Loss 0.04  Total Training Translation Loss 10.81 
2024-02-02 23:29:05,687 EPOCH 483
2024-02-02 23:29:10,984 [Epoch: 483 Step: 00064200] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     1420 || Batch Translation Loss:   0.036780 => Txt Tokens per Sec:     3962 || Lr: 0.000050
2024-02-02 23:29:13,095 Epoch 483: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.74 
2024-02-02 23:29:13,096 EPOCH 484
2024-02-02 23:29:16,100 [Epoch: 484 Step: 00064300] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:     1625 || Batch Translation Loss:   0.062385 => Txt Tokens per Sec:     4586 || Lr: 0.000050
2024-02-02 23:29:20,048 Epoch 484: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.58 
2024-02-02 23:29:20,049 EPOCH 485
2024-02-02 23:29:21,420 [Epoch: 485 Step: 00064400] Batch Recognition Loss:   0.000261 => Gls Tokens per Sec:     1635 || Batch Translation Loss:   0.047972 => Txt Tokens per Sec:     4250 || Lr: 0.000050
2024-02-02 23:29:27,317 [Epoch: 485 Step: 00064500] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:     1355 || Batch Translation Loss:   0.027890 => Txt Tokens per Sec:     3835 || Lr: 0.000050
2024-02-02 23:29:27,590 Epoch 485: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.92 
2024-02-02 23:29:27,590 EPOCH 486
2024-02-02 23:29:32,956 [Epoch: 486 Step: 00064600] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     1417 || Batch Translation Loss:   0.020057 => Txt Tokens per Sec:     3948 || Lr: 0.000050
2024-02-02 23:29:35,159 Epoch 486: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.09 
2024-02-02 23:29:35,159 EPOCH 487
2024-02-02 23:29:38,237 [Epoch: 487 Step: 00064700] Batch Recognition Loss:   0.000271 => Gls Tokens per Sec:     1612 || Batch Translation Loss:   0.022128 => Txt Tokens per Sec:     4497 || Lr: 0.000050
2024-02-02 23:29:42,290 Epoch 487: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.05 
2024-02-02 23:29:42,291 EPOCH 488
2024-02-02 23:29:43,944 [Epoch: 488 Step: 00064800] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     1405 || Batch Translation Loss:   0.017545 => Txt Tokens per Sec:     4167 || Lr: 0.000050
2024-02-02 23:29:49,627 [Epoch: 488 Step: 00064900] Batch Recognition Loss:   0.000203 => Gls Tokens per Sec:     1406 || Batch Translation Loss:   0.020931 => Txt Tokens per Sec:     3831 || Lr: 0.000050
2024-02-02 23:29:49,837 Epoch 488: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.35 
2024-02-02 23:29:49,837 EPOCH 489
2024-02-02 23:29:55,217 [Epoch: 489 Step: 00065000] Batch Recognition Loss:   0.000207 => Gls Tokens per Sec:     1428 || Batch Translation Loss:   0.403035 => Txt Tokens per Sec:     3935 || Lr: 0.000050
2024-02-02 23:29:57,398 Epoch 489: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.44 
2024-02-02 23:29:57,399 EPOCH 490
2024-02-02 23:30:00,608 [Epoch: 490 Step: 00065100] Batch Recognition Loss:   0.000152 => Gls Tokens per Sec:     1571 || Batch Translation Loss:   0.012276 => Txt Tokens per Sec:     4137 || Lr: 0.000050
2024-02-02 23:30:04,752 Epoch 490: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.99 
2024-02-02 23:30:04,752 EPOCH 491
2024-02-02 23:30:06,458 [Epoch: 491 Step: 00065200] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:     1408 || Batch Translation Loss:   0.025604 => Txt Tokens per Sec:     3789 || Lr: 0.000050
2024-02-02 23:30:12,289 [Epoch: 491 Step: 00065300] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     1370 || Batch Translation Loss:   0.031853 => Txt Tokens per Sec:     3849 || Lr: 0.000050
2024-02-02 23:30:12,478 Epoch 491: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.17 
2024-02-02 23:30:12,479 EPOCH 492
2024-02-02 23:30:18,015 [Epoch: 492 Step: 00065400] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     1402 || Batch Translation Loss:   0.014632 => Txt Tokens per Sec:     3902 || Lr: 0.000050
2024-02-02 23:30:20,122 Epoch 492: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.40 
2024-02-02 23:30:20,122 EPOCH 493
2024-02-02 23:30:23,353 [Epoch: 493 Step: 00065500] Batch Recognition Loss:   0.000319 => Gls Tokens per Sec:     1585 || Batch Translation Loss:   0.020252 => Txt Tokens per Sec:     4336 || Lr: 0.000050
2024-02-02 23:30:27,176 Epoch 493: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.32 
2024-02-02 23:30:27,176 EPOCH 494
2024-02-02 23:30:28,806 [Epoch: 494 Step: 00065600] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:     1524 || Batch Translation Loss:   0.020686 => Txt Tokens per Sec:     4244 || Lr: 0.000050
2024-02-02 23:30:34,732 [Epoch: 494 Step: 00065700] Batch Recognition Loss:   0.000305 => Gls Tokens per Sec:     1348 || Batch Translation Loss:   0.021172 => Txt Tokens per Sec:     3737 || Lr: 0.000050
2024-02-02 23:30:34,835 Epoch 494: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.48 
2024-02-02 23:30:34,835 EPOCH 495
2024-02-02 23:30:40,405 [Epoch: 495 Step: 00065800] Batch Recognition Loss:   0.000197 => Gls Tokens per Sec:     1408 || Batch Translation Loss:   0.105398 => Txt Tokens per Sec:     3912 || Lr: 0.000050
2024-02-02 23:30:42,268 Epoch 495: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.15 
2024-02-02 23:30:42,269 EPOCH 496
2024-02-02 23:30:46,047 [Epoch: 496 Step: 00065900] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:     1376 || Batch Translation Loss:   0.014948 => Txt Tokens per Sec:     3788 || Lr: 0.000050
2024-02-02 23:30:49,629 Epoch 496: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.49 
2024-02-02 23:30:49,629 EPOCH 497
2024-02-02 23:30:51,315 [Epoch: 497 Step: 00066000] Batch Recognition Loss:   0.000297 => Gls Tokens per Sec:     1519 || Batch Translation Loss:   0.052411 => Txt Tokens per Sec:     4368 || Lr: 0.000050
2024-02-02 23:31:02,179 Validation result at epoch 497, step    66000: duration: 10.8628s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00391	Translation Loss: 91384.39062	PPL: 9365.98926
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.74	(BLEU-1: 10.09,	BLEU-2: 3.01,	BLEU-3: 1.28,	BLEU-4: 0.74)
	CHRF 16.51	ROUGE 8.72
2024-02-02 23:31:02,180 Logging Recognition and Translation Outputs
2024-02-02 23:31:02,180 ========================================================================================================================
2024-02-02 23:31:02,180 Logging Sequence: 144_42.00
2024-02-02 23:31:02,180 	Gloss Reference :	A B+C+D+E
2024-02-02 23:31:02,180 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:31:02,180 	Gloss Alignment :	         
2024-02-02 23:31:02,181 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:31:02,181 	Text Reference  :	******* *** mumal was    overjoyed with     the  response
2024-02-02 23:31:02,181 	Text Hypothesis :	despite the delay people were      watching this match   
2024-02-02 23:31:02,181 	Text Alignment  :	I       I   S     S      S         S        S    S       
2024-02-02 23:31:02,181 ========================================================================================================================
2024-02-02 23:31:02,182 Logging Sequence: 116_162.00
2024-02-02 23:31:02,182 	Gloss Reference :	A B+C+D+E
2024-02-02 23:31:02,182 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:31:02,182 	Gloss Alignment :	         
2024-02-02 23:31:02,182 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:31:02,183 	Text Reference  :	turned out  the  video was shared on    social media by a      staff at the hotel
2024-02-02 23:31:02,183 	Text Hypothesis :	****** pant said that  he  can    never forget these 2  people ever  in his life 
2024-02-02 23:31:02,184 	Text Alignment  :	D      S    S    S     S   S      S     S      S     S  S      S     S  S   S    
2024-02-02 23:31:02,184 ========================================================================================================================
2024-02-02 23:31:02,184 Logging Sequence: 168_78.00
2024-02-02 23:31:02,184 	Gloss Reference :	A B+C+D+E
2024-02-02 23:31:02,184 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:31:02,184 	Gloss Alignment :	         
2024-02-02 23:31:02,184 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:31:02,186 	Text Reference  :	once she   posted a     picture eating at   a table but you could only see the back of   vamika's head     
2024-02-02 23:31:02,186 	Text Hypothesis :	**** virat kohli  actor anushka sharma have a ***** *** *** ***** **** *** *** **** huge fan      following
2024-02-02 23:31:02,186 	Text Alignment  :	D    S     S      S     S       S      S      D     D   D   D     D    D   D   D    S    S        S        
2024-02-02 23:31:02,186 ========================================================================================================================
2024-02-02 23:31:02,186 Logging Sequence: 118_223.00
2024-02-02 23:31:02,186 	Gloss Reference :	A B+C+D+E
2024-02-02 23:31:02,186 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:31:02,187 	Gloss Alignment :	         
2024-02-02 23:31:02,187 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:31:02,187 	Text Reference  :	her outfits were designed by the ****** ****** **** luxury company louis   vuitton 
2024-02-02 23:31:02,188 	Text Hypothesis :	it  is      not  known    as the indian team's goal from   the     penalty shootout
2024-02-02 23:31:02,188 	Text Alignment  :	S   S       S    S        S      I      I      I    S      S       S       S       
2024-02-02 23:31:02,188 ========================================================================================================================
2024-02-02 23:31:02,188 Logging Sequence: 132_125.00
2024-02-02 23:31:02,188 	Gloss Reference :	A B+C+D+E
2024-02-02 23:31:02,188 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 23:31:02,188 	Gloss Alignment :	         
2024-02-02 23:31:02,189 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 23:31:02,189 	Text Reference  :	'œusman is muslim and does not    drink alcohol because of  his religious beliefs
2024-02-02 23:31:02,190 	Text Hypothesis :	******* ** one    day on   social media was     playing for his ********* mother 
2024-02-02 23:31:02,190 	Text Alignment  :	D       D  S      S   S    S      S     S       S       S       D         S      
2024-02-02 23:31:02,190 ========================================================================================================================
2024-02-02 23:31:02,194 Training ended since there were no improvements inthe last learning rate step: 0.000050
2024-02-02 23:31:02,194 Best validation result at step    14000:   0.99 eval_metric.
2024-02-02 23:31:28,243 ------------------------------------------------------------
2024-02-02 23:31:28,244 [DEV] partition [RECOGNITION] experiment [BW]: 1
2024-02-02 23:31:39,550 finished in 11.3062s 
2024-02-02 23:31:39,551 ************************************************************
2024-02-02 23:31:39,551 [DEV] partition [RECOGNITION] results:
	New Best CTC Decode Beam Size: 1
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
2024-02-02 23:31:39,551 ************************************************************
2024-02-02 23:31:39,551 ------------------------------------------------------------
2024-02-02 23:31:39,551 [DEV] partition [RECOGNITION] experiment [BW]: 2
2024-02-02 23:31:50,778 finished in 11.2273s 
2024-02-02 23:31:50,779 ------------------------------------------------------------
2024-02-02 23:31:50,779 [DEV] partition [RECOGNITION] experiment [BW]: 3
2024-02-02 23:32:01,676 finished in 10.8968s 
2024-02-02 23:32:01,677 ------------------------------------------------------------
2024-02-02 23:32:01,677 [DEV] partition [RECOGNITION] experiment [BW]: 4
2024-02-02 23:32:12,465 finished in 10.7882s 
2024-02-02 23:32:12,465 ------------------------------------------------------------
2024-02-02 23:32:12,466 [DEV] partition [RECOGNITION] experiment [BW]: 5
2024-02-02 23:32:22,946 finished in 10.4800s 
2024-02-02 23:32:22,946 ------------------------------------------------------------
2024-02-02 23:32:22,947 [DEV] partition [RECOGNITION] experiment [BW]: 6
2024-02-02 23:32:33,483 finished in 10.5349s 
2024-02-02 23:32:33,483 ------------------------------------------------------------
2024-02-02 23:32:33,483 [DEV] partition [RECOGNITION] experiment [BW]: 7
2024-02-02 23:32:44,141 finished in 10.6567s 
2024-02-02 23:32:44,141 ------------------------------------------------------------
2024-02-02 23:32:44,141 [DEV] partition [RECOGNITION] experiment [BW]: 8
2024-02-02 23:32:54,747 finished in 10.6062s 
2024-02-02 23:32:54,747 ------------------------------------------------------------
2024-02-02 23:32:54,748 [DEV] partition [RECOGNITION] experiment [BW]: 9
2024-02-02 23:33:05,651 finished in 10.9034s 
2024-02-02 23:33:05,652 ------------------------------------------------------------
2024-02-02 23:33:05,652 [DEV] partition [RECOGNITION] experiment [BW]: 10
2024-02-02 23:33:16,603 finished in 10.9510s 
2024-02-02 23:33:16,603 ============================================================
2024-02-02 23:33:27,007 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 1 and Alpha: -1
	BLEU-4 0.99	(BLEU-1: 11.00,	BLEU-2: 3.71,	BLEU-3: 1.74,	BLEU-4: 0.99)
	CHRF 17.18	ROUGE 9.42
2024-02-02 23:33:27,008 ------------------------------------------------------------
2024-02-02 23:40:09,781 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 5 and Alpha: 3
	BLEU-4 1.01	(BLEU-1: 10.90,	BLEU-2: 3.64,	BLEU-3: 1.70,	BLEU-4: 1.01)
	CHRF 16.93	ROUGE 9.34
2024-02-02 23:40:09,782 ------------------------------------------------------------
2024-02-02 23:50:30,141 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 10 and Alpha: 4
	BLEU-4 1.01	(BLEU-1: 10.95,	BLEU-2: 3.66,	BLEU-3: 1.70,	BLEU-4: 1.01)
	CHRF 17.09	ROUGE 9.41
2024-02-02 23:50:30,142 ------------------------------------------------------------
2024-02-02 23:50:49,884 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 10 and Alpha: 5
	BLEU-4 1.01	(BLEU-1: 10.96,	BLEU-2: 3.67,	BLEU-3: 1.71,	BLEU-4: 1.01)
	CHRF 17.10	ROUGE 9.42
2024-02-02 23:50:49,885 ------------------------------------------------------------
2024-02-02 23:50:49,885 ************************************************************
2024-02-02 23:50:49,885 [DEV] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 1
	Best Translation Beam Size: 10 and Alpha: 5
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 1.01	(BLEU-1: 10.96,	BLEU-2: 3.67,	BLEU-3: 1.71,	BLEU-4: 1.01)
	CHRF 17.10	ROUGE 9.42
2024-02-02 23:50:49,885 ************************************************************
2024-02-02 23:51:09,481 [TEST] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 1
	Best Translation Beam Size: 10 and Alpha: 5
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.84	(BLEU-1: 10.49,	BLEU-2: 3.42,	BLEU-3: 1.51,	BLEU-4: 0.84)
	CHRF 17.11	ROUGE 8.71
2024-02-02 23:51:09,482 ************************************************************
