2024-02-02 09:34:18,554 Hello! This is Joey-NMT.
2024-02-02 09:34:18,563 Total params: 25639944
2024-02-02 09:34:18,564 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'gloss_output_layer.bias', 'gloss_output_layer.weight', 'sgn_embed.ln.bias', 'sgn_embed.ln.weight', 'sgn_embed.norm.norm.bias', 'sgn_embed.norm.norm.weight', 'txt_embed.norm.norm.bias', 'txt_embed.norm.norm.weight']
2024-02-02 09:34:19,698 cfg.name                           : sign_experiment
2024-02-02 09:34:19,698 cfg.data.data_path                 : ./data/Sports_dataset/0/
2024-02-02 09:34:19,698 cfg.data.version                   : phoenix_2014_trans
2024-02-02 09:34:19,698 cfg.data.sgn                       : sign
2024-02-02 09:34:19,698 cfg.data.txt                       : text
2024-02-02 09:34:19,698 cfg.data.gls                       : gloss
2024-02-02 09:34:19,699 cfg.data.train                     : excel_data.train
2024-02-02 09:34:19,699 cfg.data.dev                       : excel_data.dev
2024-02-02 09:34:19,699 cfg.data.test                      : excel_data.test
2024-02-02 09:34:19,699 cfg.data.feature_size              : 2560
2024-02-02 09:34:19,699 cfg.data.level                     : word
2024-02-02 09:34:19,699 cfg.data.txt_lowercase             : True
2024-02-02 09:34:19,699 cfg.data.max_sent_length           : 500
2024-02-02 09:34:19,699 cfg.data.random_train_subset       : -1
2024-02-02 09:34:19,699 cfg.data.random_dev_subset         : -1
2024-02-02 09:34:19,700 cfg.testing.recognition_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-02 09:34:19,700 cfg.testing.translation_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-02 09:34:19,700 cfg.testing.translation_beam_alphas : [-1, 0, 1, 2, 3, 4, 5]
2024-02-02 09:34:19,700 cfg.training.reset_best_ckpt       : False
2024-02-02 09:34:19,700 cfg.training.reset_scheduler       : False
2024-02-02 09:34:19,700 cfg.training.reset_optimizer       : False
2024-02-02 09:34:19,700 cfg.training.random_seed           : 42
2024-02-02 09:34:19,700 cfg.training.model_dir             : ./sign_sample_model/fold0/32head/128batch
2024-02-02 09:34:19,700 cfg.training.recognition_loss_weight : 1.0
2024-02-02 09:34:19,701 cfg.training.translation_loss_weight : 1.0
2024-02-02 09:34:19,701 cfg.training.eval_metric           : bleu
2024-02-02 09:34:19,701 cfg.training.optimizer             : adam
2024-02-02 09:34:19,701 cfg.training.learning_rate         : 0.0001
2024-02-02 09:34:19,701 cfg.training.batch_size            : 128
2024-02-02 09:34:19,701 cfg.training.num_valid_log         : 5
2024-02-02 09:34:19,701 cfg.training.epochs                : 50000
2024-02-02 09:34:19,701 cfg.training.early_stopping_metric : eval_metric
2024-02-02 09:34:19,702 cfg.training.batch_type            : sentence
2024-02-02 09:34:19,702 cfg.training.translation_normalization : batch
2024-02-02 09:34:19,702 cfg.training.eval_recognition_beam_size : 1
2024-02-02 09:34:19,702 cfg.training.eval_translation_beam_size : 1
2024-02-02 09:34:19,702 cfg.training.eval_translation_beam_alpha : -1
2024-02-02 09:34:19,702 cfg.training.overwrite             : True
2024-02-02 09:34:19,702 cfg.training.shuffle               : True
2024-02-02 09:34:19,702 cfg.training.use_cuda              : True
2024-02-02 09:34:19,702 cfg.training.translation_max_output_length : 40
2024-02-02 09:34:19,703 cfg.training.keep_last_ckpts       : 1
2024-02-02 09:34:19,703 cfg.training.batch_multiplier      : 1
2024-02-02 09:34:19,703 cfg.training.logging_freq          : 100
2024-02-02 09:34:19,703 cfg.training.validation_freq       : 2000
2024-02-02 09:34:19,703 cfg.training.betas                 : [0.9, 0.998]
2024-02-02 09:34:19,703 cfg.training.scheduling            : plateau
2024-02-02 09:34:19,703 cfg.training.learning_rate_min     : 1e-08
2024-02-02 09:34:19,703 cfg.training.weight_decay          : 0.0001
2024-02-02 09:34:19,703 cfg.training.patience              : 12
2024-02-02 09:34:19,704 cfg.training.decrease_factor       : 0.5
2024-02-02 09:34:19,704 cfg.training.label_smoothing       : 0.0
2024-02-02 09:34:19,704 cfg.model.initializer              : xavier
2024-02-02 09:34:19,704 cfg.model.bias_initializer         : zeros
2024-02-02 09:34:19,704 cfg.model.init_gain                : 1.0
2024-02-02 09:34:19,704 cfg.model.embed_initializer        : xavier
2024-02-02 09:34:19,704 cfg.model.embed_init_gain          : 1.0
2024-02-02 09:34:19,704 cfg.model.tied_softmax             : True
2024-02-02 09:34:19,704 cfg.model.encoder.type             : transformer
2024-02-02 09:34:19,705 cfg.model.encoder.num_layers       : 3
2024-02-02 09:34:19,705 cfg.model.encoder.num_heads        : 32
2024-02-02 09:34:19,705 cfg.model.encoder.embeddings.embedding_dim : 512
2024-02-02 09:34:19,705 cfg.model.encoder.embeddings.scale : False
2024-02-02 09:34:19,705 cfg.model.encoder.embeddings.dropout : 0.1
2024-02-02 09:34:19,705 cfg.model.encoder.embeddings.norm_type : batch
2024-02-02 09:34:19,705 cfg.model.encoder.embeddings.activation_type : softsign
2024-02-02 09:34:19,705 cfg.model.encoder.hidden_size      : 512
2024-02-02 09:34:19,705 cfg.model.encoder.ff_size          : 2048
2024-02-02 09:34:19,706 cfg.model.encoder.dropout          : 0.1
2024-02-02 09:34:19,706 cfg.model.decoder.type             : transformer
2024-02-02 09:34:19,706 cfg.model.decoder.num_layers       : 3
2024-02-02 09:34:19,706 cfg.model.decoder.num_heads        : 32
2024-02-02 09:34:19,706 cfg.model.decoder.embeddings.embedding_dim : 512
2024-02-02 09:34:19,706 cfg.model.decoder.embeddings.scale : False
2024-02-02 09:34:19,706 cfg.model.decoder.embeddings.dropout : 0.1
2024-02-02 09:34:19,706 cfg.model.decoder.embeddings.norm_type : batch
2024-02-02 09:34:19,707 cfg.model.decoder.embeddings.activation_type : softsign
2024-02-02 09:34:19,707 cfg.model.decoder.hidden_size      : 512
2024-02-02 09:34:19,707 cfg.model.decoder.ff_size          : 2048
2024-02-02 09:34:19,707 cfg.model.decoder.dropout          : 0.1
2024-02-02 09:34:19,707 Data set sizes: 
	train 2126,
	valid 708,
	test 706
2024-02-02 09:34:19,707 First training example:
	[GLS] A B C D E
	[TXT] although new zealand was disappointed to faltered at the finals against australia they did well throughout the tournament
2024-02-02 09:34:19,707 First 10 words (gls): (0) <si> (1) <unk> (2) <pad> (3) A (4) B (5) C (6) D (7) E
2024-02-02 09:34:19,707 First 10 words (txt): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) and (6) to (7) a (8) in (9) of
2024-02-02 09:34:19,708 Number of unique glosses (types): 8
2024-02-02 09:34:19,708 Number of unique words (types): 4397
2024-02-02 09:34:19,708 SignModel(
	encoder=TransformerEncoder(num_layers=3, num_heads=32),
	decoder=TransformerDecoder(num_layers=3, num_heads=32),
	sgn_embed=SpatialEmbeddings(embedding_dim=512, input_size=2560),
	txt_embed=Embeddings(embedding_dim=512, vocab_size=4397))
2024-02-02 09:34:19,711 EPOCH 1
2024-02-02 09:34:29,288 Epoch   1: Total Training Recognition Loss 118.16  Total Training Translation Loss 1801.04 
2024-02-02 09:34:29,289 EPOCH 2
2024-02-02 09:34:36,150 Epoch   2: Total Training Recognition Loss 31.04  Total Training Translation Loss 1637.42 
2024-02-02 09:34:36,150 EPOCH 3
2024-02-02 09:34:43,382 Epoch   3: Total Training Recognition Loss 13.94  Total Training Translation Loss 1558.06 
2024-02-02 09:34:43,383 EPOCH 4
2024-02-02 09:34:52,267 Epoch   4: Total Training Recognition Loss 7.50  Total Training Translation Loss 1521.35 
2024-02-02 09:34:52,268 EPOCH 5
2024-02-02 09:35:00,552 Epoch   5: Total Training Recognition Loss 3.80  Total Training Translation Loss 1504.49 
2024-02-02 09:35:00,552 EPOCH 6
2024-02-02 09:35:07,112 [Epoch: 006 Step: 00000100] Batch Recognition Loss:   0.078185 => Gls Tokens per Sec:     1426 || Batch Translation Loss: 115.870621 => Txt Tokens per Sec:     3933 || Lr: 0.000100
2024-02-02 09:35:07,814 Epoch   6: Total Training Recognition Loss 1.77  Total Training Translation Loss 1493.21 
2024-02-02 09:35:07,815 EPOCH 7
2024-02-02 09:35:14,208 Epoch   7: Total Training Recognition Loss 0.89  Total Training Translation Loss 1480.19 
2024-02-02 09:35:14,209 EPOCH 8
2024-02-02 09:35:20,939 Epoch   8: Total Training Recognition Loss 0.53  Total Training Translation Loss 1462.61 
2024-02-02 09:35:20,940 EPOCH 9
2024-02-02 09:35:27,489 Epoch   9: Total Training Recognition Loss 0.37  Total Training Translation Loss 1439.56 
2024-02-02 09:35:27,490 EPOCH 10
2024-02-02 09:35:34,151 Epoch  10: Total Training Recognition Loss 0.28  Total Training Translation Loss 1408.54 
2024-02-02 09:35:34,152 EPOCH 11
2024-02-02 09:35:40,753 Epoch  11: Total Training Recognition Loss 0.22  Total Training Translation Loss 1377.16 
2024-02-02 09:35:40,754 EPOCH 12
2024-02-02 09:35:46,217 [Epoch: 012 Step: 00000200] Batch Recognition Loss:   0.009744 => Gls Tokens per Sec:     1478 || Batch Translation Loss: 100.577057 => Txt Tokens per Sec:     4126 || Lr: 0.000100
2024-02-02 09:35:47,267 Epoch  12: Total Training Recognition Loss 0.19  Total Training Translation Loss 1347.89 
2024-02-02 09:35:47,268 EPOCH 13
2024-02-02 09:35:54,040 Epoch  13: Total Training Recognition Loss 0.17  Total Training Translation Loss 1315.40 
2024-02-02 09:35:54,040 EPOCH 14
2024-02-02 09:36:00,691 Epoch  14: Total Training Recognition Loss 0.16  Total Training Translation Loss 1287.33 
2024-02-02 09:36:00,691 EPOCH 15
2024-02-02 09:36:07,318 Epoch  15: Total Training Recognition Loss 0.15  Total Training Translation Loss 1257.44 
2024-02-02 09:36:07,318 EPOCH 16
2024-02-02 09:36:13,904 Epoch  16: Total Training Recognition Loss 0.15  Total Training Translation Loss 1230.49 
2024-02-02 09:36:13,904 EPOCH 17
2024-02-02 09:36:20,494 Epoch  17: Total Training Recognition Loss 0.13  Total Training Translation Loss 1205.62 
2024-02-02 09:36:20,495 EPOCH 18
2024-02-02 09:36:23,422 [Epoch: 018 Step: 00000300] Batch Recognition Loss:   0.008241 => Gls Tokens per Sec:     2407 || Batch Translation Loss:  90.899498 => Txt Tokens per Sec:     6449 || Lr: 0.000100
2024-02-02 09:36:27,107 Epoch  18: Total Training Recognition Loss 0.14  Total Training Translation Loss 1178.80 
2024-02-02 09:36:27,107 EPOCH 19
2024-02-02 09:36:33,720 Epoch  19: Total Training Recognition Loss 0.14  Total Training Translation Loss 1155.36 
2024-02-02 09:36:33,721 EPOCH 20
2024-02-02 09:36:40,198 Epoch  20: Total Training Recognition Loss 0.15  Total Training Translation Loss 1137.33 
2024-02-02 09:36:40,199 EPOCH 21
2024-02-02 09:36:46,808 Epoch  21: Total Training Recognition Loss 0.17  Total Training Translation Loss 1122.87 
2024-02-02 09:36:46,808 EPOCH 22
2024-02-02 09:36:53,338 Epoch  22: Total Training Recognition Loss 0.16  Total Training Translation Loss 1091.42 
2024-02-02 09:36:53,338 EPOCH 23
2024-02-02 09:37:00,048 Epoch  23: Total Training Recognition Loss 0.15  Total Training Translation Loss 1066.45 
2024-02-02 09:37:00,049 EPOCH 24
2024-02-02 09:37:04,798 [Epoch: 024 Step: 00000400] Batch Recognition Loss:   0.005637 => Gls Tokens per Sec:     1161 || Batch Translation Loss:  63.268379 => Txt Tokens per Sec:     3547 || Lr: 0.000100
2024-02-02 09:37:06,526 Epoch  24: Total Training Recognition Loss 0.16  Total Training Translation Loss 1047.83 
2024-02-02 09:37:06,526 EPOCH 25
2024-02-02 09:37:13,174 Epoch  25: Total Training Recognition Loss 0.15  Total Training Translation Loss 1027.05 
2024-02-02 09:37:13,175 EPOCH 26
2024-02-02 09:37:19,425 Epoch  26: Total Training Recognition Loss 0.20  Total Training Translation Loss 997.39 
2024-02-02 09:37:19,425 EPOCH 27
2024-02-02 09:37:26,072 Epoch  27: Total Training Recognition Loss 0.15  Total Training Translation Loss 990.13 
2024-02-02 09:37:26,072 EPOCH 28
2024-02-02 09:37:32,646 Epoch  28: Total Training Recognition Loss 0.18  Total Training Translation Loss 960.94 
2024-02-02 09:37:32,647 EPOCH 29
2024-02-02 09:37:39,405 Epoch  29: Total Training Recognition Loss 0.18  Total Training Translation Loss 935.69 
2024-02-02 09:37:39,406 EPOCH 30
2024-02-02 09:37:41,239 [Epoch: 030 Step: 00000500] Batch Recognition Loss:   0.015788 => Gls Tokens per Sec:     2444 || Batch Translation Loss:  70.010872 => Txt Tokens per Sec:     6826 || Lr: 0.000100
2024-02-02 09:37:45,974 Epoch  30: Total Training Recognition Loss 0.16  Total Training Translation Loss 917.73 
2024-02-02 09:37:45,975 EPOCH 31
2024-02-02 09:37:52,386 Epoch  31: Total Training Recognition Loss 0.19  Total Training Translation Loss 897.44 
2024-02-02 09:37:52,387 EPOCH 32
2024-02-02 09:37:59,161 Epoch  32: Total Training Recognition Loss 0.18  Total Training Translation Loss 881.66 
2024-02-02 09:37:59,161 EPOCH 33
2024-02-02 09:38:05,917 Epoch  33: Total Training Recognition Loss 0.19  Total Training Translation Loss 864.00 
2024-02-02 09:38:05,917 EPOCH 34
2024-02-02 09:38:12,351 Epoch  34: Total Training Recognition Loss 0.19  Total Training Translation Loss 856.09 
2024-02-02 09:38:12,351 EPOCH 35
2024-02-02 09:38:19,045 Epoch  35: Total Training Recognition Loss 0.23  Total Training Translation Loss 832.34 
2024-02-02 09:38:19,046 EPOCH 36
2024-02-02 09:38:22,634 [Epoch: 036 Step: 00000600] Batch Recognition Loss:   0.009054 => Gls Tokens per Sec:      822 || Batch Translation Loss:  50.857498 => Txt Tokens per Sec:     2484 || Lr: 0.000100
2024-02-02 09:38:25,661 Epoch  36: Total Training Recognition Loss 0.22  Total Training Translation Loss 810.72 
2024-02-02 09:38:25,661 EPOCH 37
2024-02-02 09:38:32,211 Epoch  37: Total Training Recognition Loss 0.23  Total Training Translation Loss 794.22 
2024-02-02 09:38:32,211 EPOCH 38
2024-02-02 09:38:38,928 Epoch  38: Total Training Recognition Loss 0.22  Total Training Translation Loss 779.25 
2024-02-02 09:38:38,929 EPOCH 39
2024-02-02 09:38:45,388 Epoch  39: Total Training Recognition Loss 0.23  Total Training Translation Loss 761.33 
2024-02-02 09:38:45,389 EPOCH 40
2024-02-02 09:38:52,122 Epoch  40: Total Training Recognition Loss 0.26  Total Training Translation Loss 739.54 
2024-02-02 09:38:52,122 EPOCH 41
2024-02-02 09:38:58,842 Epoch  41: Total Training Recognition Loss 0.25  Total Training Translation Loss 728.81 
2024-02-02 09:38:58,843 EPOCH 42
2024-02-02 09:38:59,559 [Epoch: 042 Step: 00000700] Batch Recognition Loss:   0.017775 => Gls Tokens per Sec:     2688 || Batch Translation Loss:  21.009197 => Txt Tokens per Sec:     7058 || Lr: 0.000100
2024-02-02 09:39:05,627 Epoch  42: Total Training Recognition Loss 0.28  Total Training Translation Loss 709.80 
2024-02-02 09:39:05,627 EPOCH 43
2024-02-02 09:39:12,168 Epoch  43: Total Training Recognition Loss 0.27  Total Training Translation Loss 694.99 
2024-02-02 09:39:12,169 EPOCH 44
2024-02-02 09:39:18,522 Epoch  44: Total Training Recognition Loss 0.32  Total Training Translation Loss 674.50 
2024-02-02 09:39:18,523 EPOCH 45
2024-02-02 09:39:25,269 Epoch  45: Total Training Recognition Loss 0.29  Total Training Translation Loss 662.51 
2024-02-02 09:39:25,270 EPOCH 46
2024-02-02 09:39:32,046 Epoch  46: Total Training Recognition Loss 0.29  Total Training Translation Loss 646.40 
2024-02-02 09:39:32,046 EPOCH 47
2024-02-02 09:39:38,661 Epoch  47: Total Training Recognition Loss 0.33  Total Training Translation Loss 628.15 
2024-02-02 09:39:38,662 EPOCH 48
2024-02-02 09:39:38,874 [Epoch: 048 Step: 00000800] Batch Recognition Loss:   0.015050 => Gls Tokens per Sec:     3033 || Batch Translation Loss:  36.261951 => Txt Tokens per Sec:     8289 || Lr: 0.000100
2024-02-02 09:39:45,169 Epoch  48: Total Training Recognition Loss 0.30  Total Training Translation Loss 611.53 
2024-02-02 09:39:45,170 EPOCH 49
2024-02-02 09:39:51,844 Epoch  49: Total Training Recognition Loss 0.30  Total Training Translation Loss 600.21 
2024-02-02 09:39:51,845 EPOCH 50
2024-02-02 09:39:58,339 Epoch  50: Total Training Recognition Loss 0.32  Total Training Translation Loss 581.84 
2024-02-02 09:39:58,339 EPOCH 51
2024-02-02 09:40:05,094 Epoch  51: Total Training Recognition Loss 0.31  Total Training Translation Loss 569.26 
2024-02-02 09:40:05,095 EPOCH 52
2024-02-02 09:40:11,783 Epoch  52: Total Training Recognition Loss 0.37  Total Training Translation Loss 554.00 
2024-02-02 09:40:11,784 EPOCH 53
2024-02-02 09:40:17,937 [Epoch: 053 Step: 00000900] Batch Recognition Loss:   0.014952 => Gls Tokens per Sec:     1624 || Batch Translation Loss:  27.069624 => Txt Tokens per Sec:     4469 || Lr: 0.000100
2024-02-02 09:40:18,287 Epoch  53: Total Training Recognition Loss 0.35  Total Training Translation Loss 545.20 
2024-02-02 09:40:18,287 EPOCH 54
2024-02-02 09:40:24,818 Epoch  54: Total Training Recognition Loss 0.38  Total Training Translation Loss 532.51 
2024-02-02 09:40:24,819 EPOCH 55
2024-02-02 09:40:31,308 Epoch  55: Total Training Recognition Loss 0.36  Total Training Translation Loss 522.19 
2024-02-02 09:40:31,308 EPOCH 56
2024-02-02 09:40:38,069 Epoch  56: Total Training Recognition Loss 0.37  Total Training Translation Loss 506.34 
2024-02-02 09:40:38,069 EPOCH 57
2024-02-02 09:40:44,666 Epoch  57: Total Training Recognition Loss 0.38  Total Training Translation Loss 487.63 
2024-02-02 09:40:44,667 EPOCH 58
2024-02-02 09:40:51,230 Epoch  58: Total Training Recognition Loss 0.35  Total Training Translation Loss 471.92 
2024-02-02 09:40:51,230 EPOCH 59
2024-02-02 09:40:56,926 [Epoch: 059 Step: 00001000] Batch Recognition Loss:   0.015786 => Gls Tokens per Sec:     1530 || Batch Translation Loss:  24.821310 => Txt Tokens per Sec:     4296 || Lr: 0.000100
2024-02-02 09:40:57,649 Epoch  59: Total Training Recognition Loss 0.35  Total Training Translation Loss 455.97 
2024-02-02 09:40:57,650 EPOCH 60
2024-02-02 09:41:04,427 Epoch  60: Total Training Recognition Loss 0.36  Total Training Translation Loss 439.90 
2024-02-02 09:41:04,427 EPOCH 61
2024-02-02 09:41:11,153 Epoch  61: Total Training Recognition Loss 0.38  Total Training Translation Loss 429.46 
2024-02-02 09:41:11,153 EPOCH 62
2024-02-02 09:41:17,792 Epoch  62: Total Training Recognition Loss 0.36  Total Training Translation Loss 417.45 
2024-02-02 09:41:17,793 EPOCH 63
2024-02-02 09:41:24,171 Epoch  63: Total Training Recognition Loss 0.39  Total Training Translation Loss 410.18 
2024-02-02 09:41:24,172 EPOCH 64
2024-02-02 09:41:30,650 Epoch  64: Total Training Recognition Loss 0.36  Total Training Translation Loss 392.65 
2024-02-02 09:41:30,650 EPOCH 65
2024-02-02 09:41:35,791 [Epoch: 065 Step: 00001100] Batch Recognition Loss:   0.015246 => Gls Tokens per Sec:     1446 || Batch Translation Loss:  22.792015 => Txt Tokens per Sec:     3863 || Lr: 0.000100
2024-02-02 09:41:37,241 Epoch  65: Total Training Recognition Loss 0.35  Total Training Translation Loss 376.74 
2024-02-02 09:41:37,241 EPOCH 66
2024-02-02 09:41:43,941 Epoch  66: Total Training Recognition Loss 0.40  Total Training Translation Loss 369.14 
2024-02-02 09:41:43,942 EPOCH 67
2024-02-02 09:41:50,519 Epoch  67: Total Training Recognition Loss 0.37  Total Training Translation Loss 356.26 
2024-02-02 09:41:50,519 EPOCH 68
2024-02-02 09:41:56,925 Epoch  68: Total Training Recognition Loss 0.38  Total Training Translation Loss 345.57 
2024-02-02 09:41:56,925 EPOCH 69
2024-02-02 09:42:03,634 Epoch  69: Total Training Recognition Loss 0.38  Total Training Translation Loss 332.52 
2024-02-02 09:42:03,634 EPOCH 70
2024-02-02 09:42:10,278 Epoch  70: Total Training Recognition Loss 0.37  Total Training Translation Loss 324.25 
2024-02-02 09:42:10,278 EPOCH 71
2024-02-02 09:42:13,051 [Epoch: 071 Step: 00001200] Batch Recognition Loss:   0.018532 => Gls Tokens per Sec:     2309 || Batch Translation Loss:  11.914307 => Txt Tokens per Sec:     6185 || Lr: 0.000100
2024-02-02 09:42:16,863 Epoch  71: Total Training Recognition Loss 0.37  Total Training Translation Loss 313.48 
2024-02-02 09:42:16,864 EPOCH 72
2024-02-02 09:42:23,256 Epoch  72: Total Training Recognition Loss 0.35  Total Training Translation Loss 302.31 
2024-02-02 09:42:23,257 EPOCH 73
2024-02-02 09:42:29,851 Epoch  73: Total Training Recognition Loss 0.37  Total Training Translation Loss 296.63 
2024-02-02 09:42:29,851 EPOCH 74
2024-02-02 09:42:36,380 Epoch  74: Total Training Recognition Loss 0.37  Total Training Translation Loss 279.95 
2024-02-02 09:42:36,380 EPOCH 75
2024-02-02 09:42:43,126 Epoch  75: Total Training Recognition Loss 0.36  Total Training Translation Loss 270.05 
2024-02-02 09:42:43,127 EPOCH 76
2024-02-02 09:42:49,691 Epoch  76: Total Training Recognition Loss 0.37  Total Training Translation Loss 261.06 
2024-02-02 09:42:49,691 EPOCH 77
2024-02-02 09:42:54,076 [Epoch: 077 Step: 00001300] Batch Recognition Loss:   0.014514 => Gls Tokens per Sec:     1111 || Batch Translation Loss:  16.925461 => Txt Tokens per Sec:     3210 || Lr: 0.000100
2024-02-02 09:42:56,490 Epoch  77: Total Training Recognition Loss 0.39  Total Training Translation Loss 256.52 
2024-02-02 09:42:56,490 EPOCH 78
2024-02-02 09:43:02,781 Epoch  78: Total Training Recognition Loss 0.35  Total Training Translation Loss 240.67 
2024-02-02 09:43:02,781 EPOCH 79
2024-02-02 09:43:09,436 Epoch  79: Total Training Recognition Loss 0.33  Total Training Translation Loss 228.29 
2024-02-02 09:43:09,436 EPOCH 80
2024-02-02 09:43:16,138 Epoch  80: Total Training Recognition Loss 0.34  Total Training Translation Loss 217.87 
2024-02-02 09:43:16,139 EPOCH 81
2024-02-02 09:43:22,782 Epoch  81: Total Training Recognition Loss 0.33  Total Training Translation Loss 209.69 
2024-02-02 09:43:22,782 EPOCH 82
2024-02-02 09:43:29,211 Epoch  82: Total Training Recognition Loss 0.33  Total Training Translation Loss 203.71 
2024-02-02 09:43:29,212 EPOCH 83
2024-02-02 09:43:30,658 [Epoch: 083 Step: 00001400] Batch Recognition Loss:   0.026537 => Gls Tokens per Sec:     2657 || Batch Translation Loss:  14.046933 => Txt Tokens per Sec:     6590 || Lr: 0.000100
2024-02-02 09:43:35,852 Epoch  83: Total Training Recognition Loss 0.31  Total Training Translation Loss 197.64 
2024-02-02 09:43:35,852 EPOCH 84
2024-02-02 09:43:42,317 Epoch  84: Total Training Recognition Loss 0.33  Total Training Translation Loss 189.66 
2024-02-02 09:43:42,318 EPOCH 85
2024-02-02 09:43:49,118 Epoch  85: Total Training Recognition Loss 0.31  Total Training Translation Loss 185.51 
2024-02-02 09:43:49,119 EPOCH 86
2024-02-02 09:43:55,866 Epoch  86: Total Training Recognition Loss 0.34  Total Training Translation Loss 179.91 
2024-02-02 09:43:55,866 EPOCH 87
2024-02-02 09:44:02,335 Epoch  87: Total Training Recognition Loss 0.34  Total Training Translation Loss 170.39 
2024-02-02 09:44:02,335 EPOCH 88
2024-02-02 09:44:08,570 Epoch  88: Total Training Recognition Loss 0.31  Total Training Translation Loss 162.99 
2024-02-02 09:44:08,571 EPOCH 89
2024-02-02 09:44:11,821 [Epoch: 089 Step: 00001500] Batch Recognition Loss:   0.025319 => Gls Tokens per Sec:      711 || Batch Translation Loss:  12.547327 => Txt Tokens per Sec:     2288 || Lr: 0.000100
2024-02-02 09:44:15,284 Epoch  89: Total Training Recognition Loss 0.30  Total Training Translation Loss 154.52 
2024-02-02 09:44:15,284 EPOCH 90
2024-02-02 09:44:21,996 Epoch  90: Total Training Recognition Loss 0.30  Total Training Translation Loss 148.87 
2024-02-02 09:44:21,997 EPOCH 91
2024-02-02 09:44:29,017 Epoch  91: Total Training Recognition Loss 0.29  Total Training Translation Loss 141.97 
2024-02-02 09:44:29,017 EPOCH 92
2024-02-02 09:44:35,651 Epoch  92: Total Training Recognition Loss 0.27  Total Training Translation Loss 135.16 
2024-02-02 09:44:35,651 EPOCH 93
2024-02-02 09:44:41,841 Epoch  93: Total Training Recognition Loss 0.29  Total Training Translation Loss 130.72 
2024-02-02 09:44:41,841 EPOCH 94
2024-02-02 09:44:48,512 Epoch  94: Total Training Recognition Loss 0.27  Total Training Translation Loss 124.47 
2024-02-02 09:44:48,512 EPOCH 95
2024-02-02 09:44:49,169 [Epoch: 095 Step: 00001600] Batch Recognition Loss:   0.015994 => Gls Tokens per Sec:     1954 || Batch Translation Loss:   8.098663 => Txt Tokens per Sec:     5794 || Lr: 0.000100
2024-02-02 09:44:55,276 Epoch  95: Total Training Recognition Loss 0.26  Total Training Translation Loss 118.31 
2024-02-02 09:44:55,276 EPOCH 96
2024-02-02 09:45:01,985 Epoch  96: Total Training Recognition Loss 0.25  Total Training Translation Loss 112.79 
2024-02-02 09:45:01,985 EPOCH 97
2024-02-02 09:45:08,674 Epoch  97: Total Training Recognition Loss 0.26  Total Training Translation Loss 109.53 
2024-02-02 09:45:08,674 EPOCH 98
2024-02-02 09:45:15,034 Epoch  98: Total Training Recognition Loss 0.23  Total Training Translation Loss 103.93 
2024-02-02 09:45:15,035 EPOCH 99
2024-02-02 09:45:21,754 Epoch  99: Total Training Recognition Loss 0.24  Total Training Translation Loss 98.75 
2024-02-02 09:45:21,755 EPOCH 100
2024-02-02 09:45:28,498 [Epoch: 100 Step: 00001700] Batch Recognition Loss:   0.011631 => Gls Tokens per Sec:     1577 || Batch Translation Loss:   5.106503 => Txt Tokens per Sec:     4378 || Lr: 0.000100
2024-02-02 09:45:28,499 Epoch 100: Total Training Recognition Loss 0.24  Total Training Translation Loss 98.72 
2024-02-02 09:45:28,499 EPOCH 101
2024-02-02 09:45:34,973 Epoch 101: Total Training Recognition Loss 0.24  Total Training Translation Loss 94.53 
2024-02-02 09:45:34,974 EPOCH 102
2024-02-02 09:45:41,421 Epoch 102: Total Training Recognition Loss 0.23  Total Training Translation Loss 94.43 
2024-02-02 09:45:41,421 EPOCH 103
2024-02-02 09:45:47,864 Epoch 103: Total Training Recognition Loss 0.26  Total Training Translation Loss 90.35 
2024-02-02 09:45:47,864 EPOCH 104
2024-02-02 09:45:54,449 Epoch 104: Total Training Recognition Loss 0.23  Total Training Translation Loss 85.82 
2024-02-02 09:45:54,449 EPOCH 105
2024-02-02 09:46:01,125 Epoch 105: Total Training Recognition Loss 0.23  Total Training Translation Loss 79.66 
2024-02-02 09:46:01,125 EPOCH 106
2024-02-02 09:46:07,265 [Epoch: 106 Step: 00001800] Batch Recognition Loss:   0.012129 => Gls Tokens per Sec:     1523 || Batch Translation Loss:   2.099618 => Txt Tokens per Sec:     4228 || Lr: 0.000100
2024-02-02 09:46:07,765 Epoch 106: Total Training Recognition Loss 0.22  Total Training Translation Loss 76.91 
2024-02-02 09:46:07,766 EPOCH 107
2024-02-02 09:46:14,438 Epoch 107: Total Training Recognition Loss 0.20  Total Training Translation Loss 73.89 
2024-02-02 09:46:14,438 EPOCH 108
2024-02-02 09:46:21,026 Epoch 108: Total Training Recognition Loss 0.21  Total Training Translation Loss 69.88 
2024-02-02 09:46:21,026 EPOCH 109
2024-02-02 09:46:27,636 Epoch 109: Total Training Recognition Loss 0.20  Total Training Translation Loss 67.66 
2024-02-02 09:46:27,636 EPOCH 110
2024-02-02 09:46:34,388 Epoch 110: Total Training Recognition Loss 0.18  Total Training Translation Loss 64.15 
2024-02-02 09:46:34,389 EPOCH 111
2024-02-02 09:46:41,154 Epoch 111: Total Training Recognition Loss 0.19  Total Training Translation Loss 61.49 
2024-02-02 09:46:41,155 EPOCH 112
2024-02-02 09:46:44,848 [Epoch: 112 Step: 00001900] Batch Recognition Loss:   0.010913 => Gls Tokens per Sec:     2253 || Batch Translation Loss:   3.926379 => Txt Tokens per Sec:     6174 || Lr: 0.000100
2024-02-02 09:46:47,815 Epoch 112: Total Training Recognition Loss 0.18  Total Training Translation Loss 60.23 
2024-02-02 09:46:47,815 EPOCH 113
2024-02-02 09:46:54,574 Epoch 113: Total Training Recognition Loss 0.18  Total Training Translation Loss 57.85 
2024-02-02 09:46:54,574 EPOCH 114
2024-02-02 09:47:01,277 Epoch 114: Total Training Recognition Loss 0.18  Total Training Translation Loss 55.74 
2024-02-02 09:47:01,278 EPOCH 115
2024-02-02 09:47:07,943 Epoch 115: Total Training Recognition Loss 0.17  Total Training Translation Loss 54.07 
2024-02-02 09:47:07,944 EPOCH 116
2024-02-02 09:47:14,455 Epoch 116: Total Training Recognition Loss 0.16  Total Training Translation Loss 52.40 
2024-02-02 09:47:14,456 EPOCH 117
2024-02-02 09:47:21,299 Epoch 117: Total Training Recognition Loss 0.15  Total Training Translation Loss 50.60 
2024-02-02 09:47:21,300 EPOCH 118
2024-02-02 09:47:25,922 [Epoch: 118 Step: 00002000] Batch Recognition Loss:   0.008102 => Gls Tokens per Sec:     1469 || Batch Translation Loss:   2.053139 => Txt Tokens per Sec:     3832 || Lr: 0.000100
2024-02-02 09:48:10,558 Hooray! New best validation result [eval_metric]!
2024-02-02 09:48:10,559 Saving new checkpoint.
2024-02-02 09:48:10,806 Validation result at epoch 118, step     2000: duration: 44.8836s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.01689	Translation Loss: 73564.42188	PPL: 1574.34656
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.69	(BLEU-1: 11.52,	BLEU-2: 3.85,	BLEU-3: 1.59,	BLEU-4: 0.69)
	CHRF 16.38	ROUGE 10.20
2024-02-02 09:48:10,806 Logging Recognition and Translation Outputs
2024-02-02 09:48:10,807 ========================================================================================================================
2024-02-02 09:48:10,807 Logging Sequence: 182_115.00
2024-02-02 09:48:10,807 	Gloss Reference :	A B+C+D+E
2024-02-02 09:48:10,807 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 09:48:10,807 	Gloss Alignment :	         
2024-02-02 09:48:10,807 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 09:48:10,810 	Text Reference  :	fans are unclear whether yuvraj will  be   returning to  play      test match odi or   in  t20 leagues from  february 2022 
2024-02-02 09:48:10,810 	Text Hypothesis :	**** *** it      is      not    known that they      are spreading many goals and then get a   huge    round of       pride
2024-02-02 09:48:10,810 	Text Alignment  :	D    D   S       S       S      S     S    S         S   S         S    S     S   S    S   S   S       S     S        S    
2024-02-02 09:48:10,810 ========================================================================================================================
2024-02-02 09:48:10,810 Logging Sequence: 140_120.00
2024-02-02 09:48:10,810 	Gloss Reference :	A B+C+D+E
2024-02-02 09:48:10,810 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 09:48:10,811 	Gloss Alignment :	         
2024-02-02 09:48:10,811 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 09:48:10,813 	Text Reference  :	but why so   it is  because pant is a talented player and **** it  will  help encouraging the  youth   of **** uttarakhand toward sports
2024-02-02 09:48:10,813 	Text Hypothesis :	he  has been a  lot of      pant ** * ******** ****** and made the world cup  but         also because of pant is          a      native
2024-02-02 09:48:10,813 	Text Alignment  :	S   S   S    S  S   S            D  D D        D          I    S   S     S    S           S    S          I    S           S      S     
2024-02-02 09:48:10,814 ========================================================================================================================
2024-02-02 09:48:10,814 Logging Sequence: 85_36.00
2024-02-02 09:48:10,814 	Gloss Reference :	A B+C+D+E
2024-02-02 09:48:10,814 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 09:48:10,814 	Gloss Alignment :	         
2024-02-02 09:48:10,814 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 09:48:10,815 	Text Reference  :	symonds has scored 2 centuries in 26 tests that he      played for his ******* **** country
2024-02-02 09:48:10,815 	Text Hypothesis :	he      has ****** * ********* ** ** been  a    strange player for his symonds also played 
2024-02-02 09:48:10,816 	Text Alignment  :	S           D      D D         D  D  S     S    S       S              I       I    S      
2024-02-02 09:48:10,816 ========================================================================================================================
2024-02-02 09:48:10,816 Logging Sequence: 164_100.00
2024-02-02 09:48:10,816 	Gloss Reference :	A B+C+D+E
2024-02-02 09:48:10,816 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 09:48:10,816 	Gloss Alignment :	         
2024-02-02 09:48:10,816 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 09:48:10,819 	Text Reference  :	the tv    rights for       broadcasting ipl   matches in     india for          the next 5 years went to  star india for rs 23575 crore
2024-02-02 09:48:10,819 	Text Hypothesis :	*** group a      consisted of           these these   rights of    broadcasting the **** * ipl   date has a    total of  rs 6     balls
2024-02-02 09:48:10,819 	Text Alignment  :	D   S     S      S         S            S     S       S      S     S                D    D S     S    S   S    S     S      S     S    
2024-02-02 09:48:10,819 ========================================================================================================================
2024-02-02 09:48:10,819 Logging Sequence: 76_79.00
2024-02-02 09:48:10,819 	Gloss Reference :	A B+C+D+E
2024-02-02 09:48:10,819 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 09:48:10,820 	Gloss Alignment :	         
2024-02-02 09:48:10,820 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 09:48:10,821 	Text Reference  :	** ** ********* ***** ** *** speaking to    ani csk  ceo kasi  viswanathan said
2024-02-02 09:48:10,821 	Text Hypothesis :	it is currently known as the ipl      there was held in  dubai on          12th
2024-02-02 09:48:10,821 	Text Alignment  :	I  I  I         I     I  I   S        S     S   S    S   S     S           S   
2024-02-02 09:48:10,821 ========================================================================================================================
2024-02-02 09:48:12,943 Epoch 118: Total Training Recognition Loss 0.15  Total Training Translation Loss 49.85 
2024-02-02 09:48:12,943 EPOCH 119
2024-02-02 09:48:19,879 Epoch 119: Total Training Recognition Loss 0.16  Total Training Translation Loss 49.14 
2024-02-02 09:48:19,880 EPOCH 120
2024-02-02 09:48:26,601 Epoch 120: Total Training Recognition Loss 0.17  Total Training Translation Loss 46.26 
2024-02-02 09:48:26,601 EPOCH 121
2024-02-02 09:48:33,298 Epoch 121: Total Training Recognition Loss 0.15  Total Training Translation Loss 44.30 
2024-02-02 09:48:33,299 EPOCH 122
2024-02-02 09:48:39,911 Epoch 122: Total Training Recognition Loss 0.15  Total Training Translation Loss 42.37 
2024-02-02 09:48:39,912 EPOCH 123
2024-02-02 09:48:46,846 Epoch 123: Total Training Recognition Loss 0.14  Total Training Translation Loss 41.61 
2024-02-02 09:48:46,846 EPOCH 124
2024-02-02 09:48:48,726 [Epoch: 124 Step: 00002100] Batch Recognition Loss:   0.006989 => Gls Tokens per Sec:     3068 || Batch Translation Loss:   1.992237 => Txt Tokens per Sec:     7564 || Lr: 0.000100
2024-02-02 09:48:53,557 Epoch 124: Total Training Recognition Loss 0.13  Total Training Translation Loss 39.69 
2024-02-02 09:48:53,557 EPOCH 125
2024-02-02 09:49:00,429 Epoch 125: Total Training Recognition Loss 0.13  Total Training Translation Loss 38.58 
2024-02-02 09:49:00,430 EPOCH 126
2024-02-02 09:49:07,144 Epoch 126: Total Training Recognition Loss 0.13  Total Training Translation Loss 37.71 
2024-02-02 09:49:07,145 EPOCH 127
2024-02-02 09:49:13,991 Epoch 127: Total Training Recognition Loss 0.13  Total Training Translation Loss 36.18 
2024-02-02 09:49:13,991 EPOCH 128
2024-02-02 09:49:20,399 Epoch 128: Total Training Recognition Loss 0.14  Total Training Translation Loss 35.56 
2024-02-02 09:49:20,399 EPOCH 129
2024-02-02 09:49:26,690 Epoch 129: Total Training Recognition Loss 0.13  Total Training Translation Loss 33.71 
2024-02-02 09:49:26,691 EPOCH 130
2024-02-02 09:49:28,563 [Epoch: 130 Step: 00002200] Batch Recognition Loss:   0.007573 => Gls Tokens per Sec:     2395 || Batch Translation Loss:   2.166969 => Txt Tokens per Sec:     6758 || Lr: 0.000100
2024-02-02 09:49:33,482 Epoch 130: Total Training Recognition Loss 0.12  Total Training Translation Loss 33.01 
2024-02-02 09:49:33,483 EPOCH 131
2024-02-02 09:49:40,266 Epoch 131: Total Training Recognition Loss 0.12  Total Training Translation Loss 34.40 
2024-02-02 09:49:40,267 EPOCH 132
2024-02-02 09:49:46,968 Epoch 132: Total Training Recognition Loss 0.13  Total Training Translation Loss 32.64 
2024-02-02 09:49:46,969 EPOCH 133
2024-02-02 09:49:53,805 Epoch 133: Total Training Recognition Loss 0.13  Total Training Translation Loss 31.47 
2024-02-02 09:49:53,806 EPOCH 134
2024-02-02 09:50:00,678 Epoch 134: Total Training Recognition Loss 0.12  Total Training Translation Loss 30.90 
2024-02-02 09:50:00,679 EPOCH 135
2024-02-02 09:50:07,444 Epoch 135: Total Training Recognition Loss 0.12  Total Training Translation Loss 29.11 
2024-02-02 09:50:07,444 EPOCH 136
2024-02-02 09:50:08,420 [Epoch: 136 Step: 00002300] Batch Recognition Loss:   0.006485 => Gls Tokens per Sec:     3281 || Batch Translation Loss:   1.887433 => Txt Tokens per Sec:     8034 || Lr: 0.000100
2024-02-02 09:50:14,198 Epoch 136: Total Training Recognition Loss 0.11  Total Training Translation Loss 28.35 
2024-02-02 09:50:14,198 EPOCH 137
2024-02-02 09:50:20,925 Epoch 137: Total Training Recognition Loss 0.11  Total Training Translation Loss 27.29 
2024-02-02 09:50:20,925 EPOCH 138
2024-02-02 09:50:27,665 Epoch 138: Total Training Recognition Loss 0.11  Total Training Translation Loss 26.07 
2024-02-02 09:50:27,666 EPOCH 139
2024-02-02 09:50:34,530 Epoch 139: Total Training Recognition Loss 0.10  Total Training Translation Loss 26.16 
2024-02-02 09:50:34,531 EPOCH 140
2024-02-02 09:50:41,486 Epoch 140: Total Training Recognition Loss 0.10  Total Training Translation Loss 26.04 
2024-02-02 09:50:41,486 EPOCH 141
2024-02-02 09:50:48,511 Epoch 141: Total Training Recognition Loss 0.11  Total Training Translation Loss 24.75 
2024-02-02 09:50:48,512 EPOCH 142
2024-02-02 09:50:48,997 [Epoch: 142 Step: 00002400] Batch Recognition Loss:   0.004919 => Gls Tokens per Sec:     3967 || Batch Translation Loss:   1.211427 => Txt Tokens per Sec:     9236 || Lr: 0.000100
2024-02-02 09:50:55,831 Epoch 142: Total Training Recognition Loss 0.10  Total Training Translation Loss 24.29 
2024-02-02 09:50:55,832 EPOCH 143
2024-02-02 09:51:02,917 Epoch 143: Total Training Recognition Loss 0.09  Total Training Translation Loss 23.60 
2024-02-02 09:51:02,918 EPOCH 144
2024-02-02 09:51:09,650 Epoch 144: Total Training Recognition Loss 0.09  Total Training Translation Loss 23.53 
2024-02-02 09:51:09,651 EPOCH 145
2024-02-02 09:51:16,527 Epoch 145: Total Training Recognition Loss 0.10  Total Training Translation Loss 22.81 
2024-02-02 09:51:16,528 EPOCH 146
2024-02-02 09:51:23,290 Epoch 146: Total Training Recognition Loss 0.09  Total Training Translation Loss 22.06 
2024-02-02 09:51:23,290 EPOCH 147
2024-02-02 09:51:29,968 Epoch 147: Total Training Recognition Loss 0.09  Total Training Translation Loss 21.09 
2024-02-02 09:51:29,969 EPOCH 148
2024-02-02 09:51:30,267 [Epoch: 148 Step: 00002500] Batch Recognition Loss:   0.005842 => Gls Tokens per Sec:     2155 || Batch Translation Loss:   1.245297 => Txt Tokens per Sec:     6199 || Lr: 0.000100
2024-02-02 09:51:36,699 Epoch 148: Total Training Recognition Loss 0.09  Total Training Translation Loss 20.04 
2024-02-02 09:51:36,700 EPOCH 149
2024-02-02 09:51:43,583 Epoch 149: Total Training Recognition Loss 0.09  Total Training Translation Loss 19.66 
2024-02-02 09:51:43,584 EPOCH 150
2024-02-02 09:51:50,359 Epoch 150: Total Training Recognition Loss 0.09  Total Training Translation Loss 18.86 
2024-02-02 09:51:50,360 EPOCH 151
2024-02-02 09:51:57,120 Epoch 151: Total Training Recognition Loss 0.09  Total Training Translation Loss 18.37 
2024-02-02 09:51:57,121 EPOCH 152
2024-02-02 09:52:03,871 Epoch 152: Total Training Recognition Loss 0.08  Total Training Translation Loss 18.30 
2024-02-02 09:52:03,872 EPOCH 153
2024-02-02 09:52:10,336 [Epoch: 153 Step: 00002600] Batch Recognition Loss:   0.005949 => Gls Tokens per Sec:     1546 || Batch Translation Loss:   1.413093 => Txt Tokens per Sec:     4329 || Lr: 0.000100
2024-02-02 09:52:10,496 Epoch 153: Total Training Recognition Loss 0.08  Total Training Translation Loss 18.60 
2024-02-02 09:52:10,497 EPOCH 154
2024-02-02 09:52:17,416 Epoch 154: Total Training Recognition Loss 0.08  Total Training Translation Loss 17.78 
2024-02-02 09:52:17,417 EPOCH 155
2024-02-02 09:52:24,106 Epoch 155: Total Training Recognition Loss 0.08  Total Training Translation Loss 17.11 
2024-02-02 09:52:24,107 EPOCH 156
2024-02-02 09:52:31,003 Epoch 156: Total Training Recognition Loss 0.07  Total Training Translation Loss 16.98 
2024-02-02 09:52:31,004 EPOCH 157
2024-02-02 09:52:37,676 Epoch 157: Total Training Recognition Loss 0.08  Total Training Translation Loss 16.85 
2024-02-02 09:52:37,676 EPOCH 158
2024-02-02 09:52:44,436 Epoch 158: Total Training Recognition Loss 0.08  Total Training Translation Loss 17.25 
2024-02-02 09:52:44,437 EPOCH 159
2024-02-02 09:52:50,596 [Epoch: 159 Step: 00002700] Batch Recognition Loss:   0.004349 => Gls Tokens per Sec:     1414 || Batch Translation Loss:   1.053790 => Txt Tokens per Sec:     4032 || Lr: 0.000100
2024-02-02 09:52:51,248 Epoch 159: Total Training Recognition Loss 0.08  Total Training Translation Loss 16.32 
2024-02-02 09:52:51,248 EPOCH 160
2024-02-02 09:52:58,074 Epoch 160: Total Training Recognition Loss 0.07  Total Training Translation Loss 15.88 
2024-02-02 09:52:58,074 EPOCH 161
2024-02-02 09:53:04,874 Epoch 161: Total Training Recognition Loss 0.08  Total Training Translation Loss 15.04 
2024-02-02 09:53:04,874 EPOCH 162
2024-02-02 09:53:11,555 Epoch 162: Total Training Recognition Loss 0.07  Total Training Translation Loss 14.94 
2024-02-02 09:53:11,555 EPOCH 163
2024-02-02 09:53:18,462 Epoch 163: Total Training Recognition Loss 0.07  Total Training Translation Loss 15.25 
2024-02-02 09:53:18,463 EPOCH 164
2024-02-02 09:53:25,192 Epoch 164: Total Training Recognition Loss 0.07  Total Training Translation Loss 14.62 
2024-02-02 09:53:25,193 EPOCH 165
2024-02-02 09:53:30,784 [Epoch: 165 Step: 00002800] Batch Recognition Loss:   0.005677 => Gls Tokens per Sec:     1329 || Batch Translation Loss:   1.050083 => Txt Tokens per Sec:     3740 || Lr: 0.000100
2024-02-02 09:53:32,016 Epoch 165: Total Training Recognition Loss 0.07  Total Training Translation Loss 14.72 
2024-02-02 09:53:32,017 EPOCH 166
2024-02-02 09:53:38,237 Epoch 166: Total Training Recognition Loss 0.06  Total Training Translation Loss 13.98 
2024-02-02 09:53:38,237 EPOCH 167
2024-02-02 09:53:44,602 Epoch 167: Total Training Recognition Loss 0.07  Total Training Translation Loss 13.95 
2024-02-02 09:53:44,602 EPOCH 168
2024-02-02 09:53:51,642 Epoch 168: Total Training Recognition Loss 0.06  Total Training Translation Loss 13.42 
2024-02-02 09:53:51,643 EPOCH 169
2024-02-02 09:53:58,227 Epoch 169: Total Training Recognition Loss 0.06  Total Training Translation Loss 12.73 
2024-02-02 09:53:58,228 EPOCH 170
2024-02-02 09:54:04,869 Epoch 170: Total Training Recognition Loss 0.06  Total Training Translation Loss 12.47 
2024-02-02 09:54:04,869 EPOCH 171
2024-02-02 09:54:09,848 [Epoch: 171 Step: 00002900] Batch Recognition Loss:   0.004139 => Gls Tokens per Sec:     1235 || Batch Translation Loss:   0.835839 => Txt Tokens per Sec:     3513 || Lr: 0.000100
2024-02-02 09:54:11,571 Epoch 171: Total Training Recognition Loss 0.07  Total Training Translation Loss 12.19 
2024-02-02 09:54:11,572 EPOCH 172
2024-02-02 09:54:18,341 Epoch 172: Total Training Recognition Loss 0.06  Total Training Translation Loss 12.09 
2024-02-02 09:54:18,342 EPOCH 173
2024-02-02 09:54:25,127 Epoch 173: Total Training Recognition Loss 0.06  Total Training Translation Loss 11.82 
2024-02-02 09:54:25,127 EPOCH 174
2024-02-02 09:54:31,775 Epoch 174: Total Training Recognition Loss 0.05  Total Training Translation Loss 11.83 
2024-02-02 09:54:31,775 EPOCH 175
2024-02-02 09:54:38,504 Epoch 175: Total Training Recognition Loss 0.06  Total Training Translation Loss 11.85 
2024-02-02 09:54:38,505 EPOCH 176
2024-02-02 09:54:45,441 Epoch 176: Total Training Recognition Loss 0.06  Total Training Translation Loss 11.75 
2024-02-02 09:54:45,441 EPOCH 177
2024-02-02 09:54:50,217 [Epoch: 177 Step: 00003000] Batch Recognition Loss:   0.002769 => Gls Tokens per Sec:     1020 || Batch Translation Loss:   0.698015 => Txt Tokens per Sec:     3005 || Lr: 0.000100
2024-02-02 09:54:52,275 Epoch 177: Total Training Recognition Loss 0.06  Total Training Translation Loss 11.46 
2024-02-02 09:54:52,275 EPOCH 178
2024-02-02 09:54:59,070 Epoch 178: Total Training Recognition Loss 0.05  Total Training Translation Loss 10.85 
2024-02-02 09:54:59,070 EPOCH 179
2024-02-02 09:55:06,100 Epoch 179: Total Training Recognition Loss 0.05  Total Training Translation Loss 10.34 
2024-02-02 09:55:06,100 EPOCH 180
2024-02-02 09:55:12,729 Epoch 180: Total Training Recognition Loss 0.05  Total Training Translation Loss 10.07 
2024-02-02 09:55:12,729 EPOCH 181
2024-02-02 09:55:19,566 Epoch 181: Total Training Recognition Loss 0.05  Total Training Translation Loss 10.19 
2024-02-02 09:55:19,567 EPOCH 182
2024-02-02 09:55:25,597 Epoch 182: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.81 
2024-02-02 09:55:25,597 EPOCH 183
2024-02-02 09:55:29,766 [Epoch: 183 Step: 00003100] Batch Recognition Loss:   0.003854 => Gls Tokens per Sec:      861 || Batch Translation Loss:   0.782876 => Txt Tokens per Sec:     2636 || Lr: 0.000100
2024-02-02 09:55:32,476 Epoch 183: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.60 
2024-02-02 09:55:32,477 EPOCH 184
2024-02-02 09:55:39,429 Epoch 184: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.64 
2024-02-02 09:55:39,429 EPOCH 185
2024-02-02 09:55:46,073 Epoch 185: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.53 
2024-02-02 09:55:46,074 EPOCH 186
2024-02-02 09:55:52,900 Epoch 186: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.51 
2024-02-02 09:55:52,901 EPOCH 187
2024-02-02 09:55:59,635 Epoch 187: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.50 
2024-02-02 09:55:59,635 EPOCH 188
2024-02-02 09:56:06,585 Epoch 188: Total Training Recognition Loss 0.04  Total Training Translation Loss 9.31 
2024-02-02 09:56:06,586 EPOCH 189
2024-02-02 09:56:07,916 [Epoch: 189 Step: 00003200] Batch Recognition Loss:   0.002330 => Gls Tokens per Sec:     1926 || Batch Translation Loss:   0.443942 => Txt Tokens per Sec:     5139 || Lr: 0.000100
2024-02-02 09:56:13,473 Epoch 189: Total Training Recognition Loss 0.04  Total Training Translation Loss 9.20 
2024-02-02 09:56:13,473 EPOCH 190
2024-02-02 09:56:20,306 Epoch 190: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.61 
2024-02-02 09:56:20,307 EPOCH 191
2024-02-02 09:56:26,579 Epoch 191: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.12 
2024-02-02 09:56:26,580 EPOCH 192
2024-02-02 09:56:33,332 Epoch 192: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.67 
2024-02-02 09:56:33,332 EPOCH 193
2024-02-02 09:56:40,283 Epoch 193: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.52 
2024-02-02 09:56:40,284 EPOCH 194
2024-02-02 09:56:47,005 Epoch 194: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.62 
2024-02-02 09:56:47,006 EPOCH 195
2024-02-02 09:56:47,287 [Epoch: 195 Step: 00003300] Batch Recognition Loss:   0.002142 => Gls Tokens per Sec:     4571 || Batch Translation Loss:   0.477497 => Txt Tokens per Sec:    11200 || Lr: 0.000100
2024-02-02 09:56:53,714 Epoch 195: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.62 
2024-02-02 09:56:53,715 EPOCH 196
2024-02-02 09:57:00,295 Epoch 196: Total Training Recognition Loss 0.05  Total Training Translation Loss 8.88 
2024-02-02 09:57:00,295 EPOCH 197
2024-02-02 09:57:07,249 Epoch 197: Total Training Recognition Loss 0.04  Total Training Translation Loss 8.78 
2024-02-02 09:57:07,250 EPOCH 198
2024-02-02 09:57:13,699 Epoch 198: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.12 
2024-02-02 09:57:13,699 EPOCH 199
2024-02-02 09:57:20,598 Epoch 199: Total Training Recognition Loss 0.05  Total Training Translation Loss 8.84 
2024-02-02 09:57:20,598 EPOCH 200
2024-02-02 09:57:27,318 [Epoch: 200 Step: 00003400] Batch Recognition Loss:   0.002551 => Gls Tokens per Sec:     1582 || Batch Translation Loss:   0.528909 => Txt Tokens per Sec:     4392 || Lr: 0.000100
2024-02-02 09:57:27,319 Epoch 200: Total Training Recognition Loss 0.05  Total Training Translation Loss 8.33 
2024-02-02 09:57:27,319 EPOCH 201
2024-02-02 09:57:34,112 Epoch 201: Total Training Recognition Loss 0.05  Total Training Translation Loss 8.13 
2024-02-02 09:57:34,112 EPOCH 202
2024-02-02 09:57:40,484 Epoch 202: Total Training Recognition Loss 0.05  Total Training Translation Loss 7.70 
2024-02-02 09:57:40,485 EPOCH 203
2024-02-02 09:57:47,242 Epoch 203: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.54 
2024-02-02 09:57:47,243 EPOCH 204
2024-02-02 09:57:54,132 Epoch 204: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.05 
2024-02-02 09:57:54,133 EPOCH 205
2024-02-02 09:58:01,122 Epoch 205: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.11 
2024-02-02 09:58:01,123 EPOCH 206
2024-02-02 09:58:07,387 [Epoch: 206 Step: 00003500] Batch Recognition Loss:   0.002748 => Gls Tokens per Sec:     1493 || Batch Translation Loss:   0.563364 => Txt Tokens per Sec:     4132 || Lr: 0.000100
2024-02-02 09:58:07,883 Epoch 206: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.77 
2024-02-02 09:58:07,883 EPOCH 207
2024-02-02 09:58:14,621 Epoch 207: Total Training Recognition Loss 0.04  Total Training Translation Loss 8.47 
2024-02-02 09:58:14,621 EPOCH 208
2024-02-02 09:58:21,395 Epoch 208: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.93 
2024-02-02 09:58:21,396 EPOCH 209
2024-02-02 09:58:28,042 Epoch 209: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.32 
2024-02-02 09:58:28,043 EPOCH 210
2024-02-02 09:58:35,184 Epoch 210: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.15 
2024-02-02 09:58:35,185 EPOCH 211
2024-02-02 09:58:42,040 Epoch 211: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.27 
2024-02-02 09:58:42,040 EPOCH 212
2024-02-02 09:58:45,752 [Epoch: 212 Step: 00003600] Batch Recognition Loss:   0.002282 => Gls Tokens per Sec:     2243 || Batch Translation Loss:   0.508220 => Txt Tokens per Sec:     6167 || Lr: 0.000100
2024-02-02 09:58:48,807 Epoch 212: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.85 
2024-02-02 09:58:48,807 EPOCH 213
2024-02-02 09:58:55,610 Epoch 213: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.86 
2024-02-02 09:58:55,611 EPOCH 214
2024-02-02 09:59:02,560 Epoch 214: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.21 
2024-02-02 09:59:02,560 EPOCH 215
2024-02-02 09:59:09,158 Epoch 215: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.32 
2024-02-02 09:59:09,159 EPOCH 216
2024-02-02 09:59:16,237 Epoch 216: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.11 
2024-02-02 09:59:16,238 EPOCH 217
2024-02-02 09:59:23,027 Epoch 217: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.26 
2024-02-02 09:59:23,027 EPOCH 218
2024-02-02 09:59:27,415 [Epoch: 218 Step: 00003700] Batch Recognition Loss:   0.002151 => Gls Tokens per Sec:     1548 || Batch Translation Loss:   0.545890 => Txt Tokens per Sec:     4218 || Lr: 0.000100
2024-02-02 09:59:29,477 Epoch 218: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.80 
2024-02-02 09:59:29,478 EPOCH 219
2024-02-02 09:59:36,299 Epoch 219: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.39 
2024-02-02 09:59:36,300 EPOCH 220
2024-02-02 09:59:43,036 Epoch 220: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.00 
2024-02-02 09:59:43,036 EPOCH 221
2024-02-02 09:59:49,912 Epoch 221: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.92 
2024-02-02 09:59:49,913 EPOCH 222
2024-02-02 09:59:56,722 Epoch 222: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.97 
2024-02-02 09:59:56,722 EPOCH 223
2024-02-02 10:00:03,450 Epoch 223: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.99 
2024-02-02 10:00:03,451 EPOCH 224
2024-02-02 10:00:07,681 [Epoch: 224 Step: 00003800] Batch Recognition Loss:   0.001222 => Gls Tokens per Sec:     1303 || Batch Translation Loss:   0.256978 => Txt Tokens per Sec:     3582 || Lr: 0.000100
2024-02-02 10:00:10,251 Epoch 224: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.71 
2024-02-02 10:00:10,252 EPOCH 225
2024-02-02 10:00:17,267 Epoch 225: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.05 
2024-02-02 10:00:17,268 EPOCH 226
2024-02-02 10:00:24,023 Epoch 226: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.82 
2024-02-02 10:00:24,024 EPOCH 227
2024-02-02 10:00:30,804 Epoch 227: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.35 
2024-02-02 10:00:30,805 EPOCH 228
2024-02-02 10:00:37,335 Epoch 228: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.28 
2024-02-02 10:00:37,335 EPOCH 229
2024-02-02 10:00:43,873 Epoch 229: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.27 
2024-02-02 10:00:43,874 EPOCH 230
2024-02-02 10:00:46,056 [Epoch: 230 Step: 00003900] Batch Recognition Loss:   0.001956 => Gls Tokens per Sec:     2054 || Batch Translation Loss:   0.139467 => Txt Tokens per Sec:     5580 || Lr: 0.000100
2024-02-02 10:00:50,960 Epoch 230: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.21 
2024-02-02 10:00:50,961 EPOCH 231
2024-02-02 10:00:57,791 Epoch 231: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.58 
2024-02-02 10:00:57,791 EPOCH 232
2024-02-02 10:01:04,808 Epoch 232: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.91 
2024-02-02 10:01:04,809 EPOCH 233
2024-02-02 10:01:11,624 Epoch 233: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.51 
2024-02-02 10:01:11,625 EPOCH 234
2024-02-02 10:01:17,775 Epoch 234: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.19 
2024-02-02 10:01:17,776 EPOCH 235
2024-02-02 10:01:23,711 Epoch 235: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.83 
2024-02-02 10:01:23,711 EPOCH 236
2024-02-02 10:01:24,727 [Epoch: 236 Step: 00004000] Batch Recognition Loss:   0.001285 => Gls Tokens per Sec:     3150 || Batch Translation Loss:   0.215372 => Txt Tokens per Sec:     8238 || Lr: 0.000100
2024-02-02 10:01:55,283 Hooray! New best validation result [eval_metric]!
2024-02-02 10:01:55,284 Saving new checkpoint.
2024-02-02 10:01:55,566 Validation result at epoch 236, step     4000: duration: 30.8381s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00195	Translation Loss: 82366.17188	PPL: 3798.59619
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.96	(BLEU-1: 12.88,	BLEU-2: 4.33,	BLEU-3: 1.89,	BLEU-4: 0.96)
	CHRF 18.10	ROUGE 10.71
2024-02-02 10:01:55,567 Logging Recognition and Translation Outputs
2024-02-02 10:01:55,567 ========================================================================================================================
2024-02-02 10:01:55,568 Logging Sequence: 133_173.00
2024-02-02 10:01:55,568 	Gloss Reference :	A B+C+D+E
2024-02-02 10:01:55,568 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 10:01:55,568 	Gloss Alignment :	         
2024-02-02 10:01:55,568 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 10:01:55,571 	Text Reference  :	** according to       sources the   leaders of the two   countries are   set to      join the ******* *** ** **** *** commentary panel as  well 
2024-02-02 10:01:55,571 	Text Hypothesis :	on 12th      february 2023    there was     a  t20 match between   india and gujarat at   the stadium let me tell you can        watch the world
2024-02-02 10:01:55,571 	Text Alignment  :	I  S         S        S       S     S       S  S   S     S         S     S   S       S        I       I   I  I    I   S          S     S   S    
2024-02-02 10:01:55,571 ========================================================================================================================
2024-02-02 10:01:55,572 Logging Sequence: 83_33.00
2024-02-02 10:01:55,572 	Gloss Reference :	A B+C+D+E
2024-02-02 10:01:55,572 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 10:01:55,572 	Gloss Alignment :	         
2024-02-02 10:01:55,572 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 10:01:55,574 	Text Reference  :	*** ******* ****** ** a ** **** *** ***** ** football match   lasts for  two    equal halves of  45      minutes
2024-02-02 10:01:55,574 	Text Hypothesis :	the denmark person is a 29 year old staff to the      denmark team  with people seem  to     the denmark team   
2024-02-02 10:01:55,574 	Text Alignment  :	I   I       I      I    I  I    I   I     I  S        S       S     S    S      S     S      S   S       S      
2024-02-02 10:01:55,574 ========================================================================================================================
2024-02-02 10:01:55,574 Logging Sequence: 68_147.00
2024-02-02 10:01:55,574 	Gloss Reference :	A B+C+D+E
2024-02-02 10:01:55,574 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 10:01:55,574 	Gloss Alignment :	         
2024-02-02 10:01:55,575 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 10:01:55,576 	Text Reference  :	remember the 2007 t20 world cup    amid  a    lot   of  sledging by     english players
2024-02-02 10:01:55,576 	Text Hypothesis :	******** *** **** *** while stuart broad gave dhoni for the      bowler smashed sixes  
2024-02-02 10:01:55,576 	Text Alignment  :	D        D   D    D   S     S      S     S    S     S   S        S      S       S      
2024-02-02 10:01:55,576 ========================================================================================================================
2024-02-02 10:01:55,576 Logging Sequence: 165_8.00
2024-02-02 10:01:55,576 	Gloss Reference :	A B+C+D+E
2024-02-02 10:01:55,576 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 10:01:55,577 	Gloss Alignment :	         
2024-02-02 10:01:55,577 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 10:01:55,577 	Text Reference  :	however many don't believe in     it   it    varies among people
2024-02-02 10:01:55,577 	Text Hypothesis :	******* **** they  are     caught that after his    bag   medals
2024-02-02 10:01:55,578 	Text Alignment  :	D       D    S     S       S      S    S     S      S     S     
2024-02-02 10:01:55,578 ========================================================================================================================
2024-02-02 10:01:55,578 Logging Sequence: 119_71.00
2024-02-02 10:01:55,578 	Gloss Reference :	A B+C+D+E
2024-02-02 10:01:55,578 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 10:01:55,578 	Gloss Alignment :	         
2024-02-02 10:01:55,578 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 10:01:55,580 	Text Reference  :	the special gold devices have each player' names   and ***** jersey numbers next   to the     camera
2024-02-02 10:01:55,580 	Text Hypothesis :	*** idesign gold ******* is   a    violent clashes and messi got    the     reason of idesign gold  
2024-02-02 10:01:55,580 	Text Alignment  :	D   S            D       S    S    S       S           I     S      S       S      S  S       S     
2024-02-02 10:01:55,580 ========================================================================================================================
2024-02-02 10:02:01,412 Epoch 236: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.69 
2024-02-02 10:02:01,412 EPOCH 237
2024-02-02 10:02:08,158 Epoch 237: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.80 
2024-02-02 10:02:08,159 EPOCH 238
2024-02-02 10:02:14,806 Epoch 238: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.88 
2024-02-02 10:02:14,806 EPOCH 239
2024-02-02 10:02:21,590 Epoch 239: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.88 
2024-02-02 10:02:21,591 EPOCH 240
2024-02-02 10:02:28,217 Epoch 240: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.77 
2024-02-02 10:02:28,218 EPOCH 241
2024-02-02 10:02:34,859 Epoch 241: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.55 
2024-02-02 10:02:34,859 EPOCH 242
2024-02-02 10:02:35,291 [Epoch: 242 Step: 00004100] Batch Recognition Loss:   0.001017 => Gls Tokens per Sec:     4444 || Batch Translation Loss:   0.229453 => Txt Tokens per Sec:    10947 || Lr: 0.000100
2024-02-02 10:02:41,669 Epoch 242: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.30 
2024-02-02 10:02:41,669 EPOCH 243
2024-02-02 10:02:48,463 Epoch 243: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.57 
2024-02-02 10:02:48,464 EPOCH 244
2024-02-02 10:02:54,940 Epoch 244: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.28 
2024-02-02 10:02:54,941 EPOCH 245
2024-02-02 10:03:01,807 Epoch 245: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.30 
2024-02-02 10:03:01,807 EPOCH 246
2024-02-02 10:03:08,664 Epoch 246: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.51 
2024-02-02 10:03:08,665 EPOCH 247
2024-02-02 10:03:15,491 Epoch 247: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.19 
2024-02-02 10:03:15,492 EPOCH 248
2024-02-02 10:03:16,059 [Epoch: 248 Step: 00004200] Batch Recognition Loss:   0.002619 => Gls Tokens per Sec:     1135 || Batch Translation Loss:   0.411332 => Txt Tokens per Sec:     4016 || Lr: 0.000100
2024-02-02 10:03:22,609 Epoch 248: Total Training Recognition Loss 0.05  Total Training Translation Loss 9.95 
2024-02-02 10:03:22,609 EPOCH 249
2024-02-02 10:03:29,301 Epoch 249: Total Training Recognition Loss 0.08  Total Training Translation Loss 20.24 
2024-02-02 10:03:29,302 EPOCH 250
2024-02-02 10:03:36,111 Epoch 250: Total Training Recognition Loss 0.07  Total Training Translation Loss 13.68 
2024-02-02 10:03:36,112 EPOCH 251
2024-02-02 10:03:43,057 Epoch 251: Total Training Recognition Loss 0.06  Total Training Translation Loss 11.64 
2024-02-02 10:03:43,058 EPOCH 252
2024-02-02 10:03:49,963 Epoch 252: Total Training Recognition Loss 0.07  Total Training Translation Loss 8.83 
2024-02-02 10:03:49,963 EPOCH 253
2024-02-02 10:03:56,531 [Epoch: 253 Step: 00004300] Batch Recognition Loss:   0.002694 => Gls Tokens per Sec:     1521 || Batch Translation Loss:   0.449053 => Txt Tokens per Sec:     4226 || Lr: 0.000100
2024-02-02 10:03:56,748 Epoch 253: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.39 
2024-02-02 10:03:56,748 EPOCH 254
2024-02-02 10:04:03,671 Epoch 254: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.73 
2024-02-02 10:04:03,671 EPOCH 255
2024-02-02 10:04:10,393 Epoch 255: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.30 
2024-02-02 10:04:10,394 EPOCH 256
2024-02-02 10:04:17,269 Epoch 256: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.66 
2024-02-02 10:04:17,269 EPOCH 257
2024-02-02 10:04:24,151 Epoch 257: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.44 
2024-02-02 10:04:24,152 EPOCH 258
2024-02-02 10:04:31,005 Epoch 258: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.27 
2024-02-02 10:04:31,006 EPOCH 259
2024-02-02 10:04:36,502 [Epoch: 259 Step: 00004400] Batch Recognition Loss:   0.001368 => Gls Tokens per Sec:     1585 || Batch Translation Loss:   0.258118 => Txt Tokens per Sec:     4301 || Lr: 0.000100
2024-02-02 10:04:37,587 Epoch 259: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.28 
2024-02-02 10:04:37,587 EPOCH 260
2024-02-02 10:04:44,162 Epoch 260: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.04 
2024-02-02 10:04:44,162 EPOCH 261
2024-02-02 10:04:50,889 Epoch 261: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.78 
2024-02-02 10:04:50,889 EPOCH 262
2024-02-02 10:04:57,718 Epoch 262: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.89 
2024-02-02 10:04:57,719 EPOCH 263
2024-02-02 10:05:04,498 Epoch 263: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.92 
2024-02-02 10:05:04,499 EPOCH 264
2024-02-02 10:05:11,193 Epoch 264: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.81 
2024-02-02 10:05:11,194 EPOCH 265
2024-02-02 10:05:15,206 [Epoch: 265 Step: 00004500] Batch Recognition Loss:   0.001099 => Gls Tokens per Sec:     1915 || Batch Translation Loss:   0.188761 => Txt Tokens per Sec:     5609 || Lr: 0.000100
2024-02-02 10:05:18,164 Epoch 265: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.70 
2024-02-02 10:05:18,165 EPOCH 266
2024-02-02 10:05:25,115 Epoch 266: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.53 
2024-02-02 10:05:25,115 EPOCH 267
2024-02-02 10:05:32,307 Epoch 267: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.47 
2024-02-02 10:05:32,307 EPOCH 268
2024-02-02 10:05:39,181 Epoch 268: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.58 
2024-02-02 10:05:39,181 EPOCH 269
2024-02-02 10:05:45,844 Epoch 269: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.38 
2024-02-02 10:05:45,845 EPOCH 270
2024-02-02 10:05:52,626 Epoch 270: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.46 
2024-02-02 10:05:52,626 EPOCH 271
2024-02-02 10:05:57,243 [Epoch: 271 Step: 00004600] Batch Recognition Loss:   0.001876 => Gls Tokens per Sec:     1333 || Batch Translation Loss:   0.086897 => Txt Tokens per Sec:     3637 || Lr: 0.000100
2024-02-02 10:05:59,434 Epoch 271: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.40 
2024-02-02 10:05:59,435 EPOCH 272
2024-02-02 10:06:06,461 Epoch 272: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.52 
2024-02-02 10:06:06,461 EPOCH 273
2024-02-02 10:06:13,243 Epoch 273: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.00 
2024-02-02 10:06:13,243 EPOCH 274
2024-02-02 10:06:20,066 Epoch 274: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.46 
2024-02-02 10:06:20,067 EPOCH 275
2024-02-02 10:06:26,631 Epoch 275: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.10 
2024-02-02 10:06:26,631 EPOCH 276
2024-02-02 10:06:33,523 Epoch 276: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.07 
2024-02-02 10:06:33,524 EPOCH 277
2024-02-02 10:06:35,696 [Epoch: 277 Step: 00004700] Batch Recognition Loss:   0.000857 => Gls Tokens per Sec:     2359 || Batch Translation Loss:   0.117618 => Txt Tokens per Sec:     6468 || Lr: 0.000100
2024-02-02 10:06:40,196 Epoch 277: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.90 
2024-02-02 10:06:40,196 EPOCH 278
2024-02-02 10:06:47,100 Epoch 278: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.05 
2024-02-02 10:06:47,101 EPOCH 279
2024-02-02 10:06:53,825 Epoch 279: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.76 
2024-02-02 10:06:53,826 EPOCH 280
2024-02-02 10:07:00,499 Epoch 280: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.96 
2024-02-02 10:07:00,500 EPOCH 281
2024-02-02 10:07:07,218 Epoch 281: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.90 
2024-02-02 10:07:07,218 EPOCH 282
2024-02-02 10:07:14,104 Epoch 282: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.14 
2024-02-02 10:07:14,104 EPOCH 283
2024-02-02 10:07:15,266 [Epoch: 283 Step: 00004800] Batch Recognition Loss:   0.000776 => Gls Tokens per Sec:     3307 || Batch Translation Loss:   0.174599 => Txt Tokens per Sec:     8547 || Lr: 0.000100
2024-02-02 10:07:20,674 Epoch 283: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.42 
2024-02-02 10:07:20,675 EPOCH 284
2024-02-02 10:07:27,596 Epoch 284: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.99 
2024-02-02 10:07:27,597 EPOCH 285
2024-02-02 10:07:34,257 Epoch 285: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.83 
2024-02-02 10:07:34,258 EPOCH 286
2024-02-02 10:07:40,926 Epoch 286: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.91 
2024-02-02 10:07:40,926 EPOCH 287
2024-02-02 10:07:47,648 Epoch 287: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.04 
2024-02-02 10:07:47,648 EPOCH 288
2024-02-02 10:07:54,646 Epoch 288: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.99 
2024-02-02 10:07:54,647 EPOCH 289
2024-02-02 10:07:56,030 [Epoch: 289 Step: 00004900] Batch Recognition Loss:   0.000769 => Gls Tokens per Sec:     1852 || Batch Translation Loss:   0.164114 => Txt Tokens per Sec:     5405 || Lr: 0.000100
2024-02-02 10:08:01,498 Epoch 289: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.33 
2024-02-02 10:08:01,498 EPOCH 290
2024-02-02 10:08:08,256 Epoch 290: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.13 
2024-02-02 10:08:08,257 EPOCH 291
2024-02-02 10:08:15,094 Epoch 291: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.93 
2024-02-02 10:08:15,095 EPOCH 292
2024-02-02 10:08:21,658 Epoch 292: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.94 
2024-02-02 10:08:21,659 EPOCH 293
2024-02-02 10:08:28,549 Epoch 293: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.29 
2024-02-02 10:08:28,550 EPOCH 294
2024-02-02 10:08:35,261 Epoch 294: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.94 
2024-02-02 10:08:35,262 EPOCH 295
2024-02-02 10:08:35,836 [Epoch: 295 Step: 00005000] Batch Recognition Loss:   0.001010 => Gls Tokens per Sec:     2232 || Batch Translation Loss:   0.181288 => Txt Tokens per Sec:     6554 || Lr: 0.000100
2024-02-02 10:08:42,157 Epoch 295: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.60 
2024-02-02 10:08:42,157 EPOCH 296
2024-02-02 10:08:49,049 Epoch 296: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.37 
2024-02-02 10:08:49,050 EPOCH 297
2024-02-02 10:08:55,737 Epoch 297: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.54 
2024-02-02 10:08:55,738 EPOCH 298
2024-02-02 10:09:02,312 Epoch 298: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.61 
2024-02-02 10:09:02,313 EPOCH 299
2024-02-02 10:09:08,882 Epoch 299: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.48 
2024-02-02 10:09:08,882 EPOCH 300
2024-02-02 10:09:15,615 [Epoch: 300 Step: 00005100] Batch Recognition Loss:   0.000815 => Gls Tokens per Sec:     1579 || Batch Translation Loss:   0.116123 => Txt Tokens per Sec:     4384 || Lr: 0.000100
2024-02-02 10:09:15,616 Epoch 300: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.57 
2024-02-02 10:09:15,616 EPOCH 301
2024-02-02 10:09:22,516 Epoch 301: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.90 
2024-02-02 10:09:22,516 EPOCH 302
2024-02-02 10:09:29,336 Epoch 302: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.18 
2024-02-02 10:09:29,337 EPOCH 303
2024-02-02 10:09:36,082 Epoch 303: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.34 
2024-02-02 10:09:36,083 EPOCH 304
2024-02-02 10:09:42,533 Epoch 304: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.56 
2024-02-02 10:09:42,534 EPOCH 305
2024-02-02 10:09:49,191 Epoch 305: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.72 
2024-02-02 10:09:49,192 EPOCH 306
2024-02-02 10:09:55,479 [Epoch: 306 Step: 00005200] Batch Recognition Loss:   0.000929 => Gls Tokens per Sec:     1487 || Batch Translation Loss:   0.141444 => Txt Tokens per Sec:     4150 || Lr: 0.000100
2024-02-02 10:09:55,862 Epoch 306: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.90 
2024-02-02 10:09:55,862 EPOCH 307
2024-02-02 10:10:02,024 Epoch 307: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.93 
2024-02-02 10:10:02,024 EPOCH 308
2024-02-02 10:10:08,724 Epoch 308: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.74 
2024-02-02 10:10:08,725 EPOCH 309
2024-02-02 10:10:15,596 Epoch 309: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.84 
2024-02-02 10:10:15,596 EPOCH 310
2024-02-02 10:10:22,267 Epoch 310: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.77 
2024-02-02 10:10:22,269 EPOCH 311
2024-02-02 10:10:28,888 Epoch 311: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.14 
2024-02-02 10:10:28,889 EPOCH 312
2024-02-02 10:10:32,541 [Epoch: 312 Step: 00005300] Batch Recognition Loss:   0.001154 => Gls Tokens per Sec:     2279 || Batch Translation Loss:   0.394829 => Txt Tokens per Sec:     6201 || Lr: 0.000100
2024-02-02 10:10:35,684 Epoch 312: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.39 
2024-02-02 10:10:35,685 EPOCH 313
2024-02-02 10:10:42,650 Epoch 313: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.18 
2024-02-02 10:10:42,651 EPOCH 314
2024-02-02 10:10:49,466 Epoch 314: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.92 
2024-02-02 10:10:49,467 EPOCH 315
2024-02-02 10:10:56,235 Epoch 315: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.95 
2024-02-02 10:10:56,235 EPOCH 316
2024-02-02 10:11:03,006 Epoch 316: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.58 
2024-02-02 10:11:03,007 EPOCH 317
2024-02-02 10:11:09,885 Epoch 317: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.04 
2024-02-02 10:11:09,886 EPOCH 318
2024-02-02 10:11:14,880 [Epoch: 318 Step: 00005400] Batch Recognition Loss:   0.001096 => Gls Tokens per Sec:     1359 || Batch Translation Loss:   0.180741 => Txt Tokens per Sec:     3740 || Lr: 0.000100
2024-02-02 10:11:16,756 Epoch 318: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.99 
2024-02-02 10:11:16,756 EPOCH 319
2024-02-02 10:11:23,529 Epoch 319: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.56 
2024-02-02 10:11:23,530 EPOCH 320
2024-02-02 10:11:30,418 Epoch 320: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.25 
2024-02-02 10:11:30,419 EPOCH 321
2024-02-02 10:11:37,052 Epoch 321: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.45 
2024-02-02 10:11:37,053 EPOCH 322
2024-02-02 10:11:43,828 Epoch 322: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.31 
2024-02-02 10:11:43,828 EPOCH 323
2024-02-02 10:11:50,452 Epoch 323: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.44 
2024-02-02 10:11:50,452 EPOCH 324
2024-02-02 10:11:52,749 [Epoch: 324 Step: 00005500] Batch Recognition Loss:   0.000697 => Gls Tokens per Sec:     2510 || Batch Translation Loss:   0.164021 => Txt Tokens per Sec:     6654 || Lr: 0.000100
2024-02-02 10:11:57,194 Epoch 324: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.78 
2024-02-02 10:11:57,194 EPOCH 325
2024-02-02 10:12:04,056 Epoch 325: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.04 
2024-02-02 10:12:04,057 EPOCH 326
2024-02-02 10:12:10,695 Epoch 326: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.95 
2024-02-02 10:12:10,696 EPOCH 327
2024-02-02 10:12:17,274 Epoch 327: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.00 
2024-02-02 10:12:17,274 EPOCH 328
2024-02-02 10:12:24,356 Epoch 328: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.74 
2024-02-02 10:12:24,357 EPOCH 329
2024-02-02 10:12:30,916 Epoch 329: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.83 
2024-02-02 10:12:30,917 EPOCH 330
2024-02-02 10:12:32,662 [Epoch: 330 Step: 00005600] Batch Recognition Loss:   0.000759 => Gls Tokens per Sec:     2567 || Batch Translation Loss:   0.160821 => Txt Tokens per Sec:     7373 || Lr: 0.000100
2024-02-02 10:12:37,367 Epoch 330: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.73 
2024-02-02 10:12:37,368 EPOCH 331
2024-02-02 10:12:44,349 Epoch 331: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.37 
2024-02-02 10:12:44,349 EPOCH 332
2024-02-02 10:12:50,957 Epoch 332: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.34 
2024-02-02 10:12:50,958 EPOCH 333
2024-02-02 10:12:57,846 Epoch 333: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.55 
2024-02-02 10:12:57,847 EPOCH 334
2024-02-02 10:13:05,031 Epoch 334: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.77 
2024-02-02 10:13:05,032 EPOCH 335
2024-02-02 10:13:11,791 Epoch 335: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.01 
2024-02-02 10:13:11,792 EPOCH 336
2024-02-02 10:13:13,596 [Epoch: 336 Step: 00005700] Batch Recognition Loss:   0.000648 => Gls Tokens per Sec:     1776 || Batch Translation Loss:   0.181509 => Txt Tokens per Sec:     5378 || Lr: 0.000100
2024-02-02 10:13:18,557 Epoch 336: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.17 
2024-02-02 10:13:18,558 EPOCH 337
2024-02-02 10:13:25,475 Epoch 337: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.00 
2024-02-02 10:13:25,476 EPOCH 338
2024-02-02 10:13:32,322 Epoch 338: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.59 
2024-02-02 10:13:32,323 EPOCH 339
2024-02-02 10:13:39,372 Epoch 339: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.23 
2024-02-02 10:13:39,372 EPOCH 340
2024-02-02 10:13:46,182 Epoch 340: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.86 
2024-02-02 10:13:46,183 EPOCH 341
2024-02-02 10:13:52,909 Epoch 341: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.80 
2024-02-02 10:13:52,910 EPOCH 342
2024-02-02 10:13:55,446 [Epoch: 342 Step: 00005800] Batch Recognition Loss:   0.000485 => Gls Tokens per Sec:      659 || Batch Translation Loss:   0.065082 => Txt Tokens per Sec:     1520 || Lr: 0.000100
2024-02-02 10:13:59,732 Epoch 342: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.92 
2024-02-02 10:13:59,732 EPOCH 343
2024-02-02 10:14:06,709 Epoch 343: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.07 
2024-02-02 10:14:06,709 EPOCH 344
2024-02-02 10:14:13,605 Epoch 344: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.80 
2024-02-02 10:14:13,606 EPOCH 345
2024-02-02 10:14:20,510 Epoch 345: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.75 
2024-02-02 10:14:20,510 EPOCH 346
2024-02-02 10:14:27,568 Epoch 346: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.94 
2024-02-02 10:14:27,569 EPOCH 347
2024-02-02 10:14:34,596 Epoch 347: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.89 
2024-02-02 10:14:34,597 EPOCH 348
2024-02-02 10:14:35,086 [Epoch: 348 Step: 00005900] Batch Recognition Loss:   0.000639 => Gls Tokens per Sec:     1311 || Batch Translation Loss:   0.106512 => Txt Tokens per Sec:     4621 || Lr: 0.000100
2024-02-02 10:14:41,344 Epoch 348: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.91 
2024-02-02 10:14:41,345 EPOCH 349
2024-02-02 10:14:48,265 Epoch 349: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.76 
2024-02-02 10:14:48,266 EPOCH 350
2024-02-02 10:14:55,080 Epoch 350: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.02 
2024-02-02 10:14:55,081 EPOCH 351
2024-02-02 10:15:02,080 Epoch 351: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.92 
2024-02-02 10:15:02,081 EPOCH 352
2024-02-02 10:15:08,963 Epoch 352: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.01 
2024-02-02 10:15:08,963 EPOCH 353
2024-02-02 10:15:15,242 [Epoch: 353 Step: 00006000] Batch Recognition Loss:   0.000631 => Gls Tokens per Sec:     1591 || Batch Translation Loss:   0.082095 => Txt Tokens per Sec:     4339 || Lr: 0.000100
2024-02-02 10:15:45,295 Validation result at epoch 353, step     6000: duration: 30.0523s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00065	Translation Loss: 89157.75000	PPL: 7495.22070
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.81	(BLEU-1: 12.39,	BLEU-2: 4.27,	BLEU-3: 1.64,	BLEU-4: 0.81)
	CHRF 18.00	ROUGE 10.53
2024-02-02 10:15:45,296 Logging Recognition and Translation Outputs
2024-02-02 10:15:45,296 ========================================================================================================================
2024-02-02 10:15:45,297 Logging Sequence: 89_111.00
2024-02-02 10:15:45,297 	Gloss Reference :	A B+C+D+E
2024-02-02 10:15:45,297 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 10:15:45,297 	Gloss Alignment :	         
2024-02-02 10:15:45,297 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 10:15:45,298 	Text Reference  :	** however selectors never selected me for the team *** ***** **** * *****
2024-02-02 10:15:45,298 	Text Hypothesis :	it is      not       known if       a  tie the team was touch with 6 balls
2024-02-02 10:15:45,299 	Text Alignment  :	I  S       S         S     S        S  S            I   I     I    I I    
2024-02-02 10:15:45,299 ========================================================================================================================
2024-02-02 10:15:45,299 Logging Sequence: 137_23.00
2024-02-02 10:15:45,299 	Gloss Reference :	A B+C+D+E
2024-02-02 10:15:45,299 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 10:15:45,299 	Gloss Alignment :	         
2024-02-02 10:15:45,300 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 10:15:45,301 	Text Reference  :	******** fan  from around the world are in      qatar for the fifa world   cup   
2024-02-02 10:15:45,301 	Text Hypothesis :	football fans from around the world are excited to    see the **** current season
2024-02-02 10:15:45,301 	Text Alignment  :	I        S                              S       S     S       D    S       S     
2024-02-02 10:15:45,301 ========================================================================================================================
2024-02-02 10:15:45,301 Logging Sequence: 128_145.00
2024-02-02 10:15:45,301 	Gloss Reference :	A B+C+D+E
2024-02-02 10:15:45,302 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 10:15:45,302 	Gloss Alignment :	         
2024-02-02 10:15:45,302 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 10:15:45,302 	Text Reference  :	icc   also uploaded a   video of  the  same
2024-02-02 10:15:45,302 	Text Hypothesis :	about the  two      men and   met with this
2024-02-02 10:15:45,303 	Text Alignment  :	S     S    S        S   S     S   S    S   
2024-02-02 10:15:45,303 ========================================================================================================================
2024-02-02 10:15:45,303 Logging Sequence: 165_192.00
2024-02-02 10:15:45,303 	Gloss Reference :	A B+C+D+E
2024-02-02 10:15:45,303 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 10:15:45,303 	Gloss Alignment :	         
2024-02-02 10:15:45,303 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 10:15:45,304 	Text Reference  :	3 ravichandran ashwin believes   that **** his bag ** *** ****** **** is lucky
2024-02-02 10:15:45,304 	Text Hypothesis :	* it           is     disgusting that such a   bag of the indian team by this 
2024-02-02 10:15:45,305 	Text Alignment  :	D S            S      S               I    S       I  I   I      I    S  S    
2024-02-02 10:15:45,305 ========================================================================================================================
2024-02-02 10:15:45,305 Logging Sequence: 180_494.00
2024-02-02 10:15:45,305 	Gloss Reference :	A B+C+D+E
2024-02-02 10:15:45,305 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 10:15:45,305 	Gloss Alignment :	         
2024-02-02 10:15:45,305 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 10:15:45,307 	Text Reference  :	******* **** *** the   women  wrestlers ***** spoke angrily against    the       police  and  the     controversy in    front of the    media  
2024-02-02 10:15:45,307 	Text Hypothesis :	however they say seven female wrestlers filed a     sexual  harassment complaint against brij bhushan sharan      singh at    cp police station
2024-02-02 10:15:45,308 	Text Alignment  :	I       I    I   S     S                I     S     S       S          S         S       S    S       S           S     S     S  S      S      
2024-02-02 10:15:45,308 ========================================================================================================================
2024-02-02 10:15:45,816 Epoch 353: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.92 
2024-02-02 10:15:45,816 EPOCH 354
2024-02-02 10:15:52,729 Epoch 354: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.83 
2024-02-02 10:15:52,730 EPOCH 355
2024-02-02 10:15:59,392 Epoch 355: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.73 
2024-02-02 10:15:59,393 EPOCH 356
2024-02-02 10:16:05,730 Epoch 356: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.08 
2024-02-02 10:16:05,731 EPOCH 357
2024-02-02 10:16:12,605 Epoch 357: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.14 
2024-02-02 10:16:12,606 EPOCH 358
2024-02-02 10:16:19,490 Epoch 358: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.12 
2024-02-02 10:16:19,491 EPOCH 359
2024-02-02 10:16:25,843 [Epoch: 359 Step: 00006100] Batch Recognition Loss:   0.000421 => Gls Tokens per Sec:     1372 || Batch Translation Loss:   0.129057 => Txt Tokens per Sec:     3847 || Lr: 0.000100
2024-02-02 10:16:26,456 Epoch 359: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.31 
2024-02-02 10:16:26,457 EPOCH 360
2024-02-02 10:16:33,068 Epoch 360: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.40 
2024-02-02 10:16:33,069 EPOCH 361
2024-02-02 10:16:39,835 Epoch 361: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.08 
2024-02-02 10:16:39,836 EPOCH 362
2024-02-02 10:16:46,405 Epoch 362: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.87 
2024-02-02 10:16:46,406 EPOCH 363
2024-02-02 10:16:53,451 Epoch 363: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.13 
2024-02-02 10:16:53,452 EPOCH 364
2024-02-02 10:17:00,378 Epoch 364: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.25 
2024-02-02 10:17:00,378 EPOCH 365
2024-02-02 10:17:03,553 [Epoch: 365 Step: 00006200] Batch Recognition Loss:   0.000617 => Gls Tokens per Sec:     2420 || Batch Translation Loss:   0.061730 => Txt Tokens per Sec:     6674 || Lr: 0.000100
2024-02-02 10:17:07,047 Epoch 365: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.31 
2024-02-02 10:17:07,047 EPOCH 366
2024-02-02 10:17:13,931 Epoch 366: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.63 
2024-02-02 10:17:13,932 EPOCH 367
2024-02-02 10:17:20,406 Epoch 367: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.24 
2024-02-02 10:17:20,406 EPOCH 368
2024-02-02 10:17:27,323 Epoch 368: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.22 
2024-02-02 10:17:27,324 EPOCH 369
2024-02-02 10:17:34,235 Epoch 369: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.69 
2024-02-02 10:17:34,236 EPOCH 370
2024-02-02 10:17:40,822 Epoch 370: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.35 
2024-02-02 10:17:40,823 EPOCH 371
2024-02-02 10:17:43,520 [Epoch: 371 Step: 00006300] Batch Recognition Loss:   0.000743 => Gls Tokens per Sec:     2374 || Batch Translation Loss:   0.162990 => Txt Tokens per Sec:     6414 || Lr: 0.000100
2024-02-02 10:17:47,617 Epoch 371: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.72 
2024-02-02 10:17:47,618 EPOCH 372
2024-02-02 10:17:54,278 Epoch 372: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.26 
2024-02-02 10:17:54,279 EPOCH 373
2024-02-02 10:18:00,982 Epoch 373: Total Training Recognition Loss 0.05  Total Training Translation Loss 14.61 
2024-02-02 10:18:00,983 EPOCH 374
2024-02-02 10:18:07,502 Epoch 374: Total Training Recognition Loss 0.05  Total Training Translation Loss 11.23 
2024-02-02 10:18:07,503 EPOCH 375
2024-02-02 10:18:14,374 Epoch 375: Total Training Recognition Loss 0.04  Total Training Translation Loss 7.04 
2024-02-02 10:18:14,374 EPOCH 376
2024-02-02 10:18:21,251 Epoch 376: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.01 
2024-02-02 10:18:21,251 EPOCH 377
2024-02-02 10:18:23,182 [Epoch: 377 Step: 00006400] Batch Recognition Loss:   0.000990 => Gls Tokens per Sec:     2654 || Batch Translation Loss:   0.188042 => Txt Tokens per Sec:     7484 || Lr: 0.000100
2024-02-02 10:18:27,844 Epoch 377: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.77 
2024-02-02 10:18:27,845 EPOCH 378
2024-02-02 10:18:34,497 Epoch 378: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.23 
2024-02-02 10:18:34,498 EPOCH 379
2024-02-02 10:18:41,265 Epoch 379: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.86 
2024-02-02 10:18:41,266 EPOCH 380
2024-02-02 10:18:47,776 Epoch 380: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.75 
2024-02-02 10:18:47,777 EPOCH 381
2024-02-02 10:18:53,737 Epoch 381: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.62 
2024-02-02 10:18:53,737 EPOCH 382
2024-02-02 10:19:00,291 Epoch 382: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.56 
2024-02-02 10:19:00,292 EPOCH 383
2024-02-02 10:19:01,579 [Epoch: 383 Step: 00006500] Batch Recognition Loss:   0.000547 => Gls Tokens per Sec:     2985 || Batch Translation Loss:   0.127508 => Txt Tokens per Sec:     8287 || Lr: 0.000100
2024-02-02 10:19:07,288 Epoch 383: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.42 
2024-02-02 10:19:07,288 EPOCH 384
2024-02-02 10:19:14,094 Epoch 384: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.34 
2024-02-02 10:19:14,095 EPOCH 385
2024-02-02 10:19:20,831 Epoch 385: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-02 10:19:20,832 EPOCH 386
2024-02-02 10:19:27,514 Epoch 386: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.22 
2024-02-02 10:19:27,514 EPOCH 387
2024-02-02 10:19:34,249 Epoch 387: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.38 
2024-02-02 10:19:34,250 EPOCH 388
2024-02-02 10:19:41,151 Epoch 388: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.41 
2024-02-02 10:19:41,152 EPOCH 389
2024-02-02 10:19:42,279 [Epoch: 389 Step: 00006600] Batch Recognition Loss:   0.000573 => Gls Tokens per Sec:     2274 || Batch Translation Loss:   0.064758 => Txt Tokens per Sec:     6653 || Lr: 0.000100
2024-02-02 10:19:47,984 Epoch 389: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-02 10:19:47,985 EPOCH 390
2024-02-02 10:19:54,579 Epoch 390: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-02 10:19:54,579 EPOCH 391
2024-02-02 10:20:00,895 Epoch 391: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.33 
2024-02-02 10:20:00,896 EPOCH 392
2024-02-02 10:20:07,909 Epoch 392: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-02 10:20:07,910 EPOCH 393
2024-02-02 10:20:14,856 Epoch 393: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-02 10:20:14,857 EPOCH 394
2024-02-02 10:20:21,508 Epoch 394: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-02 10:20:21,508 EPOCH 395
2024-02-02 10:20:21,829 [Epoch: 395 Step: 00006700] Batch Recognition Loss:   0.000429 => Gls Tokens per Sec:     3995 || Batch Translation Loss:   0.034483 => Txt Tokens per Sec:    10017 || Lr: 0.000100
2024-02-02 10:20:27,935 Epoch 395: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-02 10:20:27,936 EPOCH 396
2024-02-02 10:20:34,765 Epoch 396: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-02 10:20:34,766 EPOCH 397
2024-02-02 10:20:41,424 Epoch 397: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-02 10:20:41,424 EPOCH 398
2024-02-02 10:20:48,336 Epoch 398: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-02 10:20:48,336 EPOCH 399
2024-02-02 10:20:55,124 Epoch 399: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.19 
2024-02-02 10:20:55,124 EPOCH 400
2024-02-02 10:21:01,789 [Epoch: 400 Step: 00006800] Batch Recognition Loss:   0.000340 => Gls Tokens per Sec:     1595 || Batch Translation Loss:   0.048013 => Txt Tokens per Sec:     4429 || Lr: 0.000100
2024-02-02 10:21:01,789 Epoch 400: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-02 10:21:01,789 EPOCH 401
2024-02-02 10:21:08,997 Epoch 401: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.37 
2024-02-02 10:21:08,997 EPOCH 402
2024-02-02 10:21:15,632 Epoch 402: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.44 
2024-02-02 10:21:15,632 EPOCH 403
2024-02-02 10:21:22,472 Epoch 403: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.55 
2024-02-02 10:21:22,472 EPOCH 404
2024-02-02 10:21:29,254 Epoch 404: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.40 
2024-02-02 10:21:29,254 EPOCH 405
2024-02-02 10:21:36,049 Epoch 405: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-02 10:21:36,050 EPOCH 406
2024-02-02 10:21:42,278 [Epoch: 406 Step: 00006900] Batch Recognition Loss:   0.000437 => Gls Tokens per Sec:     1501 || Batch Translation Loss:   0.061252 => Txt Tokens per Sec:     4113 || Lr: 0.000100
2024-02-02 10:21:42,871 Epoch 406: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.15 
2024-02-02 10:21:42,872 EPOCH 407
2024-02-02 10:21:49,391 Epoch 407: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-02 10:21:49,391 EPOCH 408
2024-02-02 10:21:55,363 Epoch 408: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-02 10:21:55,363 EPOCH 409
2024-02-02 10:22:01,340 Epoch 409: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.40 
2024-02-02 10:22:01,340 EPOCH 410
2024-02-02 10:22:07,316 Epoch 410: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-02 10:22:07,316 EPOCH 411
2024-02-02 10:22:14,106 Epoch 411: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.36 
2024-02-02 10:22:14,107 EPOCH 412
2024-02-02 10:22:19,763 [Epoch: 412 Step: 00007000] Batch Recognition Loss:   0.000273 => Gls Tokens per Sec:     1427 || Batch Translation Loss:   0.034171 => Txt Tokens per Sec:     3988 || Lr: 0.000100
2024-02-02 10:22:20,764 Epoch 412: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.27 
2024-02-02 10:22:20,765 EPOCH 413
2024-02-02 10:22:27,494 Epoch 413: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.51 
2024-02-02 10:22:27,495 EPOCH 414
2024-02-02 10:22:34,172 Epoch 414: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.34 
2024-02-02 10:22:34,173 EPOCH 415
2024-02-02 10:22:40,873 Epoch 415: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.11 
2024-02-02 10:22:40,874 EPOCH 416
2024-02-02 10:22:47,546 Epoch 416: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.40 
2024-02-02 10:22:47,547 EPOCH 417
2024-02-02 10:22:54,185 Epoch 417: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.12 
2024-02-02 10:22:54,186 EPOCH 418
2024-02-02 10:22:59,018 [Epoch: 418 Step: 00007100] Batch Recognition Loss:   0.000428 => Gls Tokens per Sec:     1406 || Batch Translation Loss:   0.132729 => Txt Tokens per Sec:     3830 || Lr: 0.000100
2024-02-02 10:23:00,943 Epoch 418: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.32 
2024-02-02 10:23:00,943 EPOCH 419
2024-02-02 10:23:07,617 Epoch 419: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.25 
2024-02-02 10:23:07,617 EPOCH 420
2024-02-02 10:23:14,462 Epoch 420: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.52 
2024-02-02 10:23:14,463 EPOCH 421
2024-02-02 10:23:21,175 Epoch 421: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.35 
2024-02-02 10:23:21,176 EPOCH 422
2024-02-02 10:23:27,892 Epoch 422: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.45 
2024-02-02 10:23:27,893 EPOCH 423
2024-02-02 10:23:34,923 Epoch 423: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.16 
2024-02-02 10:23:34,923 EPOCH 424
2024-02-02 10:23:37,205 [Epoch: 424 Step: 00007200] Batch Recognition Loss:   0.000348 => Gls Tokens per Sec:     2526 || Batch Translation Loss:   0.112091 => Txt Tokens per Sec:     6925 || Lr: 0.000100
2024-02-02 10:23:41,629 Epoch 424: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.78 
2024-02-02 10:23:41,630 EPOCH 425
2024-02-02 10:23:48,261 Epoch 425: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.72 
2024-02-02 10:23:48,262 EPOCH 426
2024-02-02 10:23:55,010 Epoch 426: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.69 
2024-02-02 10:23:55,010 EPOCH 427
2024-02-02 10:24:01,803 Epoch 427: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.31 
2024-02-02 10:24:01,803 EPOCH 428
2024-02-02 10:24:08,575 Epoch 428: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.31 
2024-02-02 10:24:08,575 EPOCH 429
2024-02-02 10:24:15,092 Epoch 429: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.22 
2024-02-02 10:24:15,093 EPOCH 430
2024-02-02 10:24:18,935 [Epoch: 430 Step: 00007300] Batch Recognition Loss:   0.000397 => Gls Tokens per Sec:     1101 || Batch Translation Loss:   0.113325 => Txt Tokens per Sec:     3096 || Lr: 0.000100
2024-02-02 10:24:21,909 Epoch 430: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.28 
2024-02-02 10:24:21,909 EPOCH 431
2024-02-02 10:24:28,543 Epoch 431: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.30 
2024-02-02 10:24:28,543 EPOCH 432
2024-02-02 10:24:35,428 Epoch 432: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.48 
2024-02-02 10:24:35,429 EPOCH 433
2024-02-02 10:24:42,134 Epoch 433: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.76 
2024-02-02 10:24:42,134 EPOCH 434
2024-02-02 10:24:48,758 Epoch 434: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.83 
2024-02-02 10:24:48,759 EPOCH 435
2024-02-02 10:24:55,356 Epoch 435: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.95 
2024-02-02 10:24:55,356 EPOCH 436
2024-02-02 10:24:56,248 [Epoch: 436 Step: 00007400] Batch Recognition Loss:   0.000322 => Gls Tokens per Sec:     3590 || Batch Translation Loss:   0.089727 => Txt Tokens per Sec:     8745 || Lr: 0.000100
2024-02-02 10:25:01,948 Epoch 436: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.91 
2024-02-02 10:25:01,949 EPOCH 437
2024-02-02 10:25:08,859 Epoch 437: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.60 
2024-02-02 10:25:08,859 EPOCH 438
2024-02-02 10:25:15,726 Epoch 438: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.49 
2024-02-02 10:25:15,726 EPOCH 439
2024-02-02 10:25:22,474 Epoch 439: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.66 
2024-02-02 10:25:22,475 EPOCH 440
2024-02-02 10:25:29,613 Epoch 440: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.83 
2024-02-02 10:25:29,613 EPOCH 441
2024-02-02 10:25:36,535 Epoch 441: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.78 
2024-02-02 10:25:36,536 EPOCH 442
2024-02-02 10:25:39,265 [Epoch: 442 Step: 00007500] Batch Recognition Loss:   0.000375 => Gls Tokens per Sec:      612 || Batch Translation Loss:   0.067654 => Txt Tokens per Sec:     1872 || Lr: 0.000100
2024-02-02 10:25:43,148 Epoch 442: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.78 
2024-02-02 10:25:43,149 EPOCH 443
2024-02-02 10:25:49,850 Epoch 443: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.55 
2024-02-02 10:25:49,850 EPOCH 444
2024-02-02 10:25:56,749 Epoch 444: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.57 
2024-02-02 10:25:56,750 EPOCH 445
2024-02-02 10:26:03,366 Epoch 445: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.52 
2024-02-02 10:26:03,367 EPOCH 446
2024-02-02 10:26:09,925 Epoch 446: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.51 
2024-02-02 10:26:09,925 EPOCH 447
2024-02-02 10:26:16,382 Epoch 447: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.37 
2024-02-02 10:26:16,382 EPOCH 448
2024-02-02 10:26:16,543 [Epoch: 448 Step: 00007600] Batch Recognition Loss:   0.000277 => Gls Tokens per Sec:     4000 || Batch Translation Loss:   0.040442 => Txt Tokens per Sec:     9588 || Lr: 0.000100
2024-02-02 10:26:23,075 Epoch 448: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.44 
2024-02-02 10:26:23,075 EPOCH 449
2024-02-02 10:26:29,992 Epoch 449: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.76 
2024-02-02 10:26:29,992 EPOCH 450
2024-02-02 10:26:36,622 Epoch 450: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.53 
2024-02-02 10:26:36,622 EPOCH 451
2024-02-02 10:26:43,166 Epoch 451: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.33 
2024-02-02 10:26:43,167 EPOCH 452
2024-02-02 10:26:49,832 Epoch 452: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.42 
2024-02-02 10:26:49,833 EPOCH 453
2024-02-02 10:26:54,348 [Epoch: 453 Step: 00007700] Batch Recognition Loss:   0.000325 => Gls Tokens per Sec:     2269 || Batch Translation Loss:   0.051515 => Txt Tokens per Sec:     6228 || Lr: 0.000100
2024-02-02 10:26:56,651 Epoch 453: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.77 
2024-02-02 10:26:56,651 EPOCH 454
2024-02-02 10:27:03,392 Epoch 454: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.58 
2024-02-02 10:27:03,393 EPOCH 455
2024-02-02 10:27:10,198 Epoch 455: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.83 
2024-02-02 10:27:10,198 EPOCH 456
2024-02-02 10:27:16,749 Epoch 456: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.98 
2024-02-02 10:27:16,749 EPOCH 457
2024-02-02 10:27:23,250 Epoch 457: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.89 
2024-02-02 10:27:23,251 EPOCH 458
2024-02-02 10:27:29,887 Epoch 458: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.03 
2024-02-02 10:27:29,887 EPOCH 459
2024-02-02 10:27:35,795 [Epoch: 459 Step: 00007800] Batch Recognition Loss:   0.000614 => Gls Tokens per Sec:     1474 || Batch Translation Loss:   0.193054 => Txt Tokens per Sec:     4063 || Lr: 0.000100
2024-02-02 10:27:36,769 Epoch 459: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.74 
2024-02-02 10:27:36,769 EPOCH 460
2024-02-02 10:27:43,234 Epoch 460: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-02 10:27:43,235 EPOCH 461
2024-02-02 10:27:50,001 Epoch 461: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.60 
2024-02-02 10:27:50,002 EPOCH 462
2024-02-02 10:27:56,405 Epoch 462: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.50 
2024-02-02 10:27:56,405 EPOCH 463
2024-02-02 10:28:03,287 Epoch 463: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-02 10:28:03,288 EPOCH 464
2024-02-02 10:28:10,012 Epoch 464: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.44 
2024-02-02 10:28:10,013 EPOCH 465
2024-02-02 10:28:13,215 [Epoch: 465 Step: 00007900] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:     2400 || Batch Translation Loss:   0.054320 => Txt Tokens per Sec:     6563 || Lr: 0.000100
2024-02-02 10:28:16,875 Epoch 465: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.76 
2024-02-02 10:28:16,875 EPOCH 466
2024-02-02 10:28:23,622 Epoch 466: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.66 
2024-02-02 10:28:23,623 EPOCH 467
2024-02-02 10:28:30,343 Epoch 467: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.87 
2024-02-02 10:28:30,344 EPOCH 468
2024-02-02 10:28:37,182 Epoch 468: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.42 
2024-02-02 10:28:37,183 EPOCH 469
2024-02-02 10:28:43,992 Epoch 469: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.28 
2024-02-02 10:28:43,992 EPOCH 470
2024-02-02 10:28:50,777 Epoch 470: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.17 
2024-02-02 10:28:50,777 EPOCH 471
2024-02-02 10:28:55,565 [Epoch: 471 Step: 00008000] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     1285 || Batch Translation Loss:   0.059231 => Txt Tokens per Sec:     3602 || Lr: 0.000100
2024-02-02 10:29:25,921 Hooray! New best validation result [eval_metric]!
2024-02-02 10:29:25,922 Saving new checkpoint.
2024-02-02 10:29:26,197 Validation result at epoch 471, step     8000: duration: 30.6302s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00030	Translation Loss: 92136.25781	PPL: 10097.87598
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.96	(BLEU-1: 11.87,	BLEU-2: 3.98,	BLEU-3: 1.72,	BLEU-4: 0.96)
	CHRF 17.76	ROUGE 10.10
2024-02-02 10:29:26,199 Logging Recognition and Translation Outputs
2024-02-02 10:29:26,199 ========================================================================================================================
2024-02-02 10:29:26,199 Logging Sequence: 88_57.00
2024-02-02 10:29:26,199 	Gloss Reference :	A B+C+D+E
2024-02-02 10:29:26,200 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 10:29:26,200 	Gloss Alignment :	         
2024-02-02 10:29:26,200 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 10:29:26,202 	Text Reference  :	which   stated  messi we're  waiting for  you     to   come here      you  will be finished when    you come
2024-02-02 10:29:26,202 	Text Hypothesis :	notably rosario has   become the     most violent city in   argentina with 250  to 300      murders in  2022
2024-02-02 10:29:26,202 	Text Alignment  :	S       S       S     S      S       S    S       S    S    S         S    S    S  S        S       S   S   
2024-02-02 10:29:26,202 ========================================================================================================================
2024-02-02 10:29:26,202 Logging Sequence: 171_142.00
2024-02-02 10:29:26,203 	Gloss Reference :	A B+C+D+E
2024-02-02 10:29:26,203 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 10:29:26,203 	Gloss Alignment :	         
2024-02-02 10:29:26,203 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 10:29:26,204 	Text Reference  :	this  decision on     dhoni made      a  significant impact as     pathirana claimed two   tough wickets 
2024-02-02 10:29:26,204 	Text Hypothesis :	since the      couple were  residents of mumbai      the    finals against   each    other for   covid-19
2024-02-02 10:29:26,204 	Text Alignment  :	S     S        S      S     S         S  S           S      S      S         S       S     S     S       
2024-02-02 10:29:26,205 ========================================================================================================================
2024-02-02 10:29:26,205 Logging Sequence: 125_207.00
2024-02-02 10:29:26,205 	Gloss Reference :	A B+C+D+E
2024-02-02 10:29:26,205 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 10:29:26,205 	Gloss Alignment :	         
2024-02-02 10:29:26,205 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 10:29:26,207 	Text Reference  :	he  had not  practised since he   returned and he        had  also fallen sick
2024-02-02 10:29:26,207 	Text Hypothesis :	you all know the       bcci  were supposed to  australia with the  first  time
2024-02-02 10:29:26,207 	Text Alignment  :	S   S   S    S         S     S    S        S   S         S    S    S      S   
2024-02-02 10:29:26,207 ========================================================================================================================
2024-02-02 10:29:26,207 Logging Sequence: 68_230.00
2024-02-02 10:29:26,207 	Gloss Reference :	A B+C+D+E
2024-02-02 10:29:26,207 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 10:29:26,207 	Gloss Alignment :	         
2024-02-02 10:29:26,208 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 10:29:26,209 	Text Reference  :	*** let     us        know   what  you think in   the ***** ********** *** **** comments below
2024-02-02 10:29:26,209 	Text Hypothesis :	new zealand cricketer suresh raina did not   find the brand ambassador and also called   dhoni
2024-02-02 10:29:26,209 	Text Alignment  :	I   S       S         S      S     S   S     S        I     I          I   I    S        S    
2024-02-02 10:29:26,209 ========================================================================================================================
2024-02-02 10:29:26,209 Logging Sequence: 126_82.00
2024-02-02 10:29:26,209 	Gloss Reference :	A B+C+D+E
2024-02-02 10:29:26,209 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 10:29:26,210 	Gloss Alignment :	         
2024-02-02 10:29:26,210 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 10:29:26,211 	Text Reference  :	neeraj also dedicated his gold medal to former indian olympians who     came  close to  winning medals  
2024-02-02 10:29:26,211 	Text Hypothesis :	****** he   won       a   gold medal ** ****** ****** in        javelin throw at    the 2012    olympics
2024-02-02 10:29:26,211 	Text Alignment  :	D      S    S         S              D  D      D      S         S       S     S     S   S       S       
2024-02-02 10:29:26,211 ========================================================================================================================
2024-02-02 10:29:28,284 Epoch 471: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.16 
2024-02-02 10:29:28,284 EPOCH 472
2024-02-02 10:29:35,153 Epoch 472: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-02 10:29:35,154 EPOCH 473
2024-02-02 10:29:41,985 Epoch 473: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-02 10:29:41,986 EPOCH 474
2024-02-02 10:29:48,812 Epoch 474: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.41 
2024-02-02 10:29:48,812 EPOCH 475
2024-02-02 10:29:55,697 Epoch 475: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.29 
2024-02-02 10:29:55,697 EPOCH 476
2024-02-02 10:30:02,511 Epoch 476: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.41 
2024-02-02 10:30:02,511 EPOCH 477
2024-02-02 10:30:05,105 [Epoch: 477 Step: 00008100] Batch Recognition Loss:   0.000318 => Gls Tokens per Sec:     1975 || Batch Translation Loss:   0.054520 => Txt Tokens per Sec:     5916 || Lr: 0.000100
2024-02-02 10:30:09,383 Epoch 477: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.20 
2024-02-02 10:30:09,383 EPOCH 478
2024-02-02 10:30:16,057 Epoch 478: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-02 10:30:16,058 EPOCH 479
2024-02-02 10:30:22,783 Epoch 479: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-02 10:30:22,783 EPOCH 480
2024-02-02 10:30:29,670 Epoch 480: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.68 
2024-02-02 10:30:29,671 EPOCH 481
2024-02-02 10:30:35,881 Epoch 481: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.51 
2024-02-02 10:30:35,881 EPOCH 482
2024-02-02 10:30:42,286 Epoch 482: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.46 
2024-02-02 10:30:42,287 EPOCH 483
2024-02-02 10:30:44,066 [Epoch: 483 Step: 00008200] Batch Recognition Loss:   0.000658 => Gls Tokens per Sec:     2160 || Batch Translation Loss:   0.213823 => Txt Tokens per Sec:     5812 || Lr: 0.000100
2024-02-02 10:30:49,030 Epoch 483: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.40 
2024-02-02 10:30:49,031 EPOCH 484
2024-02-02 10:30:55,610 Epoch 484: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.56 
2024-02-02 10:30:55,611 EPOCH 485
2024-02-02 10:31:02,415 Epoch 485: Total Training Recognition Loss 0.01  Total Training Translation Loss 6.89 
2024-02-02 10:31:02,416 EPOCH 486
2024-02-02 10:31:09,215 Epoch 486: Total Training Recognition Loss 0.03  Total Training Translation Loss 7.03 
2024-02-02 10:31:09,216 EPOCH 487
2024-02-02 10:31:16,087 Epoch 487: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.19 
2024-02-02 10:31:16,087 EPOCH 488
2024-02-02 10:31:22,743 Epoch 488: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.96 
2024-02-02 10:31:22,743 EPOCH 489
2024-02-02 10:31:23,636 [Epoch: 489 Step: 00008300] Batch Recognition Loss:   0.000837 => Gls Tokens per Sec:     2870 || Batch Translation Loss:   0.138547 => Txt Tokens per Sec:     7339 || Lr: 0.000100
2024-02-02 10:31:29,464 Epoch 489: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.95 
2024-02-02 10:31:29,464 EPOCH 490
2024-02-02 10:31:36,511 Epoch 490: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.85 
2024-02-02 10:31:36,512 EPOCH 491
2024-02-02 10:31:43,404 Epoch 491: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.38 
2024-02-02 10:31:43,404 EPOCH 492
2024-02-02 10:31:50,152 Epoch 492: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.33 
2024-02-02 10:31:50,153 EPOCH 493
2024-02-02 10:31:56,836 Epoch 493: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-02 10:31:56,837 EPOCH 494
2024-02-02 10:32:03,572 Epoch 494: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-02 10:32:03,572 EPOCH 495
2024-02-02 10:32:04,091 [Epoch: 495 Step: 00008400] Batch Recognition Loss:   0.000340 => Gls Tokens per Sec:     2471 || Batch Translation Loss:   0.050310 => Txt Tokens per Sec:     6772 || Lr: 0.000100
2024-02-02 10:32:10,390 Epoch 495: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-02 10:32:10,390 EPOCH 496
2024-02-02 10:32:17,125 Epoch 496: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-02 10:32:17,126 EPOCH 497
2024-02-02 10:32:24,050 Epoch 497: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-02 10:32:24,051 EPOCH 498
2024-02-02 10:32:30,892 Epoch 498: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.94 
2024-02-02 10:32:30,893 EPOCH 499
2024-02-02 10:32:37,493 Epoch 499: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-02 10:32:37,494 EPOCH 500
2024-02-02 10:32:44,445 [Epoch: 500 Step: 00008500] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:     1529 || Batch Translation Loss:   0.049710 => Txt Tokens per Sec:     4246 || Lr: 0.000100
2024-02-02 10:32:44,446 Epoch 500: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.13 
2024-02-02 10:32:44,446 EPOCH 501
2024-02-02 10:32:51,031 Epoch 501: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.16 
2024-02-02 10:32:51,032 EPOCH 502
2024-02-02 10:32:57,611 Epoch 502: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.03 
2024-02-02 10:32:57,611 EPOCH 503
2024-02-02 10:33:04,446 Epoch 503: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.15 
2024-02-02 10:33:04,446 EPOCH 504
2024-02-02 10:33:11,231 Epoch 504: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.98 
2024-02-02 10:33:11,232 EPOCH 505
2024-02-02 10:33:17,826 Epoch 505: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.16 
2024-02-02 10:33:17,826 EPOCH 506
2024-02-02 10:33:21,844 [Epoch: 506 Step: 00008600] Batch Recognition Loss:   0.000265 => Gls Tokens per Sec:     2391 || Batch Translation Loss:   0.029422 => Txt Tokens per Sec:     6437 || Lr: 0.000100
2024-02-02 10:33:24,669 Epoch 506: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.03 
2024-02-02 10:33:24,669 EPOCH 507
2024-02-02 10:33:31,436 Epoch 507: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.98 
2024-02-02 10:33:31,437 EPOCH 508
2024-02-02 10:33:38,273 Epoch 508: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-02 10:33:38,273 EPOCH 509
2024-02-02 10:33:44,985 Epoch 509: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-02 10:33:44,986 EPOCH 510
2024-02-02 10:33:51,903 Epoch 510: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-02 10:33:51,903 EPOCH 511
2024-02-02 10:33:58,524 Epoch 511: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.71 
2024-02-02 10:33:58,525 EPOCH 512
2024-02-02 10:34:01,948 [Epoch: 512 Step: 00008700] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     2431 || Batch Translation Loss:   0.097201 => Txt Tokens per Sec:     6736 || Lr: 0.000100
2024-02-02 10:34:05,136 Epoch 512: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.80 
2024-02-02 10:34:05,136 EPOCH 513
2024-02-02 10:34:11,918 Epoch 513: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 10:34:11,919 EPOCH 514
2024-02-02 10:34:18,918 Epoch 514: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-02 10:34:18,919 EPOCH 515
2024-02-02 10:34:25,625 Epoch 515: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 10:34:25,626 EPOCH 516
2024-02-02 10:34:32,633 Epoch 516: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-02 10:34:32,634 EPOCH 517
2024-02-02 10:34:39,344 Epoch 517: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-02 10:34:39,345 EPOCH 518
2024-02-02 10:34:43,746 [Epoch: 518 Step: 00008800] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     1543 || Batch Translation Loss:   0.028677 => Txt Tokens per Sec:     4009 || Lr: 0.000100
2024-02-02 10:34:45,983 Epoch 518: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-02 10:34:45,983 EPOCH 519
2024-02-02 10:34:52,470 Epoch 519: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.78 
2024-02-02 10:34:52,471 EPOCH 520
2024-02-02 10:34:59,220 Epoch 520: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.97 
2024-02-02 10:34:59,221 EPOCH 521
2024-02-02 10:35:05,992 Epoch 521: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-02 10:35:05,992 EPOCH 522
2024-02-02 10:35:12,924 Epoch 522: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-02 10:35:12,925 EPOCH 523
2024-02-02 10:35:19,713 Epoch 523: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.89 
2024-02-02 10:35:19,714 EPOCH 524
2024-02-02 10:35:24,719 [Epoch: 524 Step: 00008900] Batch Recognition Loss:   0.000191 => Gls Tokens per Sec:     1101 || Batch Translation Loss:   0.130639 => Txt Tokens per Sec:     3292 || Lr: 0.000100
2024-02-02 10:35:26,365 Epoch 524: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.90 
2024-02-02 10:35:26,366 EPOCH 525
2024-02-02 10:35:32,870 Epoch 525: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-02 10:35:32,871 EPOCH 526
2024-02-02 10:35:39,904 Epoch 526: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.99 
2024-02-02 10:35:39,904 EPOCH 527
2024-02-02 10:35:46,665 Epoch 527: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.84 
2024-02-02 10:35:46,665 EPOCH 528
2024-02-02 10:35:53,561 Epoch 528: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.14 
2024-02-02 10:35:53,562 EPOCH 529
2024-02-02 10:36:00,480 Epoch 529: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.30 
2024-02-02 10:36:00,481 EPOCH 530
2024-02-02 10:36:04,515 [Epoch: 530 Step: 00009000] Batch Recognition Loss:   0.000213 => Gls Tokens per Sec:     1049 || Batch Translation Loss:   0.135948 => Txt Tokens per Sec:     2909 || Lr: 0.000100
2024-02-02 10:36:07,303 Epoch 530: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.50 
2024-02-02 10:36:07,303 EPOCH 531
2024-02-02 10:36:13,621 Epoch 531: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.57 
2024-02-02 10:36:13,622 EPOCH 532
2024-02-02 10:36:20,492 Epoch 532: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.83 
2024-02-02 10:36:20,493 EPOCH 533
2024-02-02 10:36:27,131 Epoch 533: Total Training Recognition Loss 0.00  Total Training Translation Loss 2.20 
2024-02-02 10:36:27,131 EPOCH 534
2024-02-02 10:36:33,870 Epoch 534: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.31 
2024-02-02 10:36:33,871 EPOCH 535
2024-02-02 10:36:40,557 Epoch 535: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.73 
2024-02-02 10:36:40,558 EPOCH 536
2024-02-02 10:36:42,187 [Epoch: 536 Step: 00009100] Batch Recognition Loss:   0.000365 => Gls Tokens per Sec:     1964 || Batch Translation Loss:   0.088477 => Txt Tokens per Sec:     5985 || Lr: 0.000100
2024-02-02 10:36:47,112 Epoch 536: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.66 
2024-02-02 10:36:47,113 EPOCH 537
2024-02-02 10:36:53,887 Epoch 537: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.25 
2024-02-02 10:36:53,888 EPOCH 538
2024-02-02 10:37:00,615 Epoch 538: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.40 
2024-02-02 10:37:00,615 EPOCH 539
2024-02-02 10:37:07,608 Epoch 539: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.29 
2024-02-02 10:37:07,608 EPOCH 540
2024-02-02 10:37:14,554 Epoch 540: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.92 
2024-02-02 10:37:14,555 EPOCH 541
2024-02-02 10:37:21,233 Epoch 541: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.59 
2024-02-02 10:37:21,233 EPOCH 542
2024-02-02 10:37:24,419 [Epoch: 542 Step: 00009200] Batch Recognition Loss:   0.000245 => Gls Tokens per Sec:      524 || Batch Translation Loss:   0.034615 => Txt Tokens per Sec:     1843 || Lr: 0.000100
2024-02-02 10:37:27,948 Epoch 542: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.37 
2024-02-02 10:37:27,949 EPOCH 543
2024-02-02 10:37:34,533 Epoch 543: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.08 
2024-02-02 10:37:34,533 EPOCH 544
2024-02-02 10:37:41,286 Epoch 544: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-02 10:37:41,287 EPOCH 545
2024-02-02 10:37:48,095 Epoch 545: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-02 10:37:48,096 EPOCH 546
2024-02-02 10:37:54,950 Epoch 546: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.31 
2024-02-02 10:37:54,951 EPOCH 547
2024-02-02 10:38:01,657 Epoch 547: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.28 
2024-02-02 10:38:01,658 EPOCH 548
2024-02-02 10:38:01,808 [Epoch: 548 Step: 00009300] Batch Recognition Loss:   0.000317 => Gls Tokens per Sec:     4295 || Batch Translation Loss:   0.061438 => Txt Tokens per Sec:     9819 || Lr: 0.000100
2024-02-02 10:38:08,416 Epoch 548: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.30 
2024-02-02 10:38:08,416 EPOCH 549
2024-02-02 10:38:15,400 Epoch 549: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.05 
2024-02-02 10:38:15,401 EPOCH 550
2024-02-02 10:38:22,351 Epoch 550: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-02 10:38:22,352 EPOCH 551
2024-02-02 10:38:29,047 Epoch 551: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.96 
2024-02-02 10:38:29,048 EPOCH 552
2024-02-02 10:38:35,578 Epoch 552: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.02 
2024-02-02 10:38:35,579 EPOCH 553
2024-02-02 10:38:41,929 [Epoch: 553 Step: 00009400] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     1574 || Batch Translation Loss:   0.093959 => Txt Tokens per Sec:     4325 || Lr: 0.000100
2024-02-02 10:38:42,565 Epoch 553: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.07 
2024-02-02 10:38:42,565 EPOCH 554
2024-02-02 10:38:49,229 Epoch 554: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.89 
2024-02-02 10:38:49,230 EPOCH 555
2024-02-02 10:38:55,819 Epoch 555: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.99 
2024-02-02 10:38:55,820 EPOCH 556
2024-02-02 10:39:02,455 Epoch 556: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.79 
2024-02-02 10:39:02,456 EPOCH 557
2024-02-02 10:39:09,382 Epoch 557: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.79 
2024-02-02 10:39:09,383 EPOCH 558
2024-02-02 10:39:16,003 Epoch 558: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-02 10:39:16,004 EPOCH 559
2024-02-02 10:39:21,858 [Epoch: 559 Step: 00009500] Batch Recognition Loss:   0.000226 => Gls Tokens per Sec:     1488 || Batch Translation Loss:   0.047877 => Txt Tokens per Sec:     4011 || Lr: 0.000100
2024-02-02 10:39:22,844 Epoch 559: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.77 
2024-02-02 10:39:22,844 EPOCH 560
2024-02-02 10:39:28,812 Epoch 560: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-02 10:39:28,812 EPOCH 561
2024-02-02 10:39:35,557 Epoch 561: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.93 
2024-02-02 10:39:35,557 EPOCH 562
2024-02-02 10:39:41,992 Epoch 562: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.05 
2024-02-02 10:39:41,992 EPOCH 563
2024-02-02 10:39:48,756 Epoch 563: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.98 
2024-02-02 10:39:48,756 EPOCH 564
2024-02-02 10:39:55,861 Epoch 564: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.83 
2024-02-02 10:39:55,862 EPOCH 565
2024-02-02 10:39:59,050 [Epoch: 565 Step: 00009600] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:     2410 || Batch Translation Loss:   0.031693 => Txt Tokens per Sec:     6740 || Lr: 0.000100
2024-02-02 10:40:02,488 Epoch 565: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.01 
2024-02-02 10:40:02,489 EPOCH 566
2024-02-02 10:40:09,230 Epoch 566: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.11 
2024-02-02 10:40:09,231 EPOCH 567
2024-02-02 10:40:15,867 Epoch 567: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.17 
2024-02-02 10:40:15,868 EPOCH 568
2024-02-02 10:40:22,514 Epoch 568: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.29 
2024-02-02 10:40:22,515 EPOCH 569
2024-02-02 10:40:29,273 Epoch 569: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.56 
2024-02-02 10:40:29,273 EPOCH 570
2024-02-02 10:40:36,026 Epoch 570: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.56 
2024-02-02 10:40:36,026 EPOCH 571
2024-02-02 10:40:41,692 [Epoch: 571 Step: 00009700] Batch Recognition Loss:   0.000515 => Gls Tokens per Sec:     1086 || Batch Translation Loss:   0.089149 => Txt Tokens per Sec:     3387 || Lr: 0.000100
2024-02-02 10:40:42,893 Epoch 571: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.40 
2024-02-02 10:40:42,893 EPOCH 572
2024-02-02 10:40:49,743 Epoch 572: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.63 
2024-02-02 10:40:49,743 EPOCH 573
2024-02-02 10:40:56,394 Epoch 573: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.36 
2024-02-02 10:40:56,394 EPOCH 574
2024-02-02 10:41:02,957 Epoch 574: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.56 
2024-02-02 10:41:02,958 EPOCH 575
2024-02-02 10:41:09,496 Epoch 575: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.38 
2024-02-02 10:41:09,496 EPOCH 576
2024-02-02 10:41:16,454 Epoch 576: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.54 
2024-02-02 10:41:16,454 EPOCH 577
2024-02-02 10:41:18,396 [Epoch: 577 Step: 00009800] Batch Recognition Loss:   0.000254 => Gls Tokens per Sec:     2639 || Batch Translation Loss:   0.078486 => Txt Tokens per Sec:     6630 || Lr: 0.000100
2024-02-02 10:41:23,228 Epoch 577: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-02 10:41:23,228 EPOCH 578
2024-02-02 10:41:30,105 Epoch 578: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.44 
2024-02-02 10:41:30,105 EPOCH 579
2024-02-02 10:41:36,717 Epoch 579: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.45 
2024-02-02 10:41:36,717 EPOCH 580
2024-02-02 10:41:43,267 Epoch 580: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.49 
2024-02-02 10:41:43,268 EPOCH 581
2024-02-02 10:41:50,233 Epoch 581: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.44 
2024-02-02 10:41:50,234 EPOCH 582
2024-02-02 10:41:56,941 Epoch 582: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.29 
2024-02-02 10:41:56,941 EPOCH 583
2024-02-02 10:42:00,587 [Epoch: 583 Step: 00009900] Batch Recognition Loss:   0.000303 => Gls Tokens per Sec:      985 || Batch Translation Loss:   0.057311 => Txt Tokens per Sec:     2614 || Lr: 0.000100
2024-02-02 10:42:03,785 Epoch 583: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.72 
2024-02-02 10:42:03,785 EPOCH 584
2024-02-02 10:42:10,611 Epoch 584: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.61 
2024-02-02 10:42:10,612 EPOCH 585
2024-02-02 10:42:17,544 Epoch 585: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.54 
2024-02-02 10:42:17,545 EPOCH 586
2024-02-02 10:42:24,390 Epoch 586: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.28 
2024-02-02 10:42:24,391 EPOCH 587
2024-02-02 10:42:31,168 Epoch 587: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.10 
2024-02-02 10:42:31,168 EPOCH 588
2024-02-02 10:42:37,742 Epoch 588: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.02 
2024-02-02 10:42:37,743 EPOCH 589
2024-02-02 10:42:38,553 [Epoch: 589 Step: 00010000] Batch Recognition Loss:   0.000242 => Gls Tokens per Sec:     3171 || Batch Translation Loss:   0.033036 => Txt Tokens per Sec:     7980 || Lr: 0.000100
2024-02-02 10:43:08,883 Hooray! New best validation result [eval_metric]!
2024-02-02 10:43:08,884 Saving new checkpoint.
2024-02-02 10:43:09,174 Validation result at epoch 589, step    10000: duration: 30.6204s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00026	Translation Loss: 94945.04688	PPL: 13375.17773
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.99	(BLEU-1: 11.21,	BLEU-2: 3.83,	BLEU-3: 1.78,	BLEU-4: 0.99)
	CHRF 17.26	ROUGE 9.58
2024-02-02 10:43:09,175 Logging Recognition and Translation Outputs
2024-02-02 10:43:09,175 ========================================================================================================================
2024-02-02 10:43:09,175 Logging Sequence: 159_139.00
2024-02-02 10:43:09,175 	Gloss Reference :	A B+C+D+E
2024-02-02 10:43:09,176 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 10:43:09,176 	Gloss Alignment :	         
2024-02-02 10:43:09,176 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 10:43:09,177 	Text Reference  :	***** he  took time and finally was ready for the asia cup where he   scored the ****** century   
2024-02-02 10:43:09,177 	Text Hypothesis :	dhoni are all  out  and ******* *** ***** *** did she  get a     duck in     the sports tournament
2024-02-02 10:43:09,178 	Text Alignment  :	I     S   S    S        D       D   D     D   S   S    S   S     S    S          I      S         
2024-02-02 10:43:09,178 ========================================================================================================================
2024-02-02 10:43:09,178 Logging Sequence: 159_159.00
2024-02-02 10:43:09,178 	Gloss Reference :	A B+C+D+E
2024-02-02 10:43:09,178 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 10:43:09,178 	Gloss Alignment :	         
2024-02-02 10:43:09,178 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 10:43:09,181 	Text Reference  :	he said  it  wasn't easy the mind has to be    focussed and he is glad that he is back in form with the asia cup century
2024-02-02 10:43:09,181 	Text Hypothesis :	** kohli was out    of   the **** *** ** first time     and ** ** **** **** ** ** **** ** **** **** csk will be  played 
2024-02-02 10:43:09,181 	Text Alignment  :	D  S     S   S      S        D    D   D  S     S            D  D  D    D    D  D  D    D  D    D    S   S    S   S      
2024-02-02 10:43:09,181 ========================================================================================================================
2024-02-02 10:43:09,181 Logging Sequence: 103_8.00
2024-02-02 10:43:09,181 	Gloss Reference :	A B+C+D+E
2024-02-02 10:43:09,181 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 10:43:09,181 	Gloss Alignment :	         
2024-02-02 10:43:09,182 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 10:43:09,183 	Text Reference  :	************ ***** ********* were         going on  in      birmingham england from 28th july to 8th  august 2022 
2024-02-02 10:43:09,183 	Text Hypothesis :	commonwealth games encourage independence from  the british empire     games   and  will have to wait for    india
2024-02-02 10:43:09,183 	Text Alignment  :	I            I     I         S            S     S   S       S          S       S    S    S       S    S      S    
2024-02-02 10:43:09,183 ========================================================================================================================
2024-02-02 10:43:09,184 Logging Sequence: 164_546.00
2024-02-02 10:43:09,184 	Gloss Reference :	A B+C+D+E
2024-02-02 10:43:09,184 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 10:43:09,184 	Gloss Alignment :	         
2024-02-02 10:43:09,184 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 10:43:09,185 	Text Reference  :	***** *** reliance has    turned out      to **** **** ****** be  the strongest company
2024-02-02 10:43:09,185 	Text Hypothesis :	after the match    people are    expected to have been rights for the next      season 
2024-02-02 10:43:09,185 	Text Alignment  :	I     I   S        S      S      S           I    I    I      S       S         S      
2024-02-02 10:43:09,185 ========================================================================================================================
2024-02-02 10:43:09,185 Logging Sequence: 132_173.00
2024-02-02 10:43:09,186 	Gloss Reference :	A B+C+D+E
2024-02-02 10:43:09,186 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 10:43:09,186 	Gloss Alignment :	         
2024-02-02 10:43:09,186 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 10:43:09,187 	Text Reference  :	** **** ******** ** *** *** ** ***** ** ** usman is   australia' first muslim player 
2024-02-02 10:43:09,187 	Text Hypothesis :	he will continue at the age of kohli as he will  have to         wait  and    special
2024-02-02 10:43:09,187 	Text Alignment  :	I  I    I        I  I   I   I  I     I  I  S     S    S          S     S      S      
2024-02-02 10:43:09,187 ========================================================================================================================
2024-02-02 10:43:15,405 Epoch 589: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.94 
2024-02-02 10:43:15,405 EPOCH 590
2024-02-02 10:43:22,069 Epoch 590: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.89 
2024-02-02 10:43:22,070 EPOCH 591
2024-02-02 10:43:28,548 Epoch 591: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.88 
2024-02-02 10:43:28,549 EPOCH 592
2024-02-02 10:43:35,322 Epoch 592: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.90 
2024-02-02 10:43:35,323 EPOCH 593
2024-02-02 10:43:42,121 Epoch 593: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.06 
2024-02-02 10:43:42,122 EPOCH 594
2024-02-02 10:43:49,090 Epoch 594: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-02 10:43:49,091 EPOCH 595
2024-02-02 10:43:49,832 [Epoch: 595 Step: 00010100] Batch Recognition Loss:   0.000553 => Gls Tokens per Sec:     1729 || Batch Translation Loss:   0.081340 => Txt Tokens per Sec:     5286 || Lr: 0.000100
2024-02-02 10:43:55,927 Epoch 595: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.15 
2024-02-02 10:43:55,928 EPOCH 596
2024-02-02 10:44:02,635 Epoch 596: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.13 
2024-02-02 10:44:02,636 EPOCH 597
2024-02-02 10:44:09,298 Epoch 597: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.97 
2024-02-02 10:44:09,299 EPOCH 598
2024-02-02 10:44:16,074 Epoch 598: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-02 10:44:16,074 EPOCH 599
2024-02-02 10:44:22,845 Epoch 599: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-02 10:44:22,846 EPOCH 600
2024-02-02 10:44:29,880 [Epoch: 600 Step: 00010200] Batch Recognition Loss:   0.000285 => Gls Tokens per Sec:     1512 || Batch Translation Loss:   0.048343 => Txt Tokens per Sec:     4196 || Lr: 0.000100
2024-02-02 10:44:29,880 Epoch 600: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.44 
2024-02-02 10:44:29,880 EPOCH 601
2024-02-02 10:44:36,813 Epoch 601: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.39 
2024-02-02 10:44:36,813 EPOCH 602
2024-02-02 10:44:43,688 Epoch 602: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.60 
2024-02-02 10:44:43,688 EPOCH 603
2024-02-02 10:44:50,600 Epoch 603: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-02 10:44:50,601 EPOCH 604
2024-02-02 10:44:57,278 Epoch 604: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.81 
2024-02-02 10:44:57,279 EPOCH 605
2024-02-02 10:45:03,933 Epoch 605: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.71 
2024-02-02 10:45:03,934 EPOCH 606
2024-02-02 10:45:10,274 [Epoch: 606 Step: 00010300] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     1475 || Batch Translation Loss:   0.043104 => Txt Tokens per Sec:     4066 || Lr: 0.000100
2024-02-02 10:45:10,797 Epoch 606: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-02 10:45:10,798 EPOCH 607
2024-02-02 10:45:17,735 Epoch 607: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.87 
2024-02-02 10:45:17,736 EPOCH 608
2024-02-02 10:45:24,479 Epoch 608: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.01 
2024-02-02 10:45:24,479 EPOCH 609
2024-02-02 10:45:31,129 Epoch 609: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.96 
2024-02-02 10:45:31,130 EPOCH 610
2024-02-02 10:45:37,968 Epoch 610: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.79 
2024-02-02 10:45:37,968 EPOCH 611
2024-02-02 10:45:44,091 Epoch 611: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.96 
2024-02-02 10:45:44,091 EPOCH 612
2024-02-02 10:45:49,492 [Epoch: 612 Step: 00010400] Batch Recognition Loss:   0.000288 => Gls Tokens per Sec:     1495 || Batch Translation Loss:   0.022598 => Txt Tokens per Sec:     4060 || Lr: 0.000100
2024-02-02 10:45:51,081 Epoch 612: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-02 10:45:51,081 EPOCH 613
2024-02-02 10:45:57,942 Epoch 613: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-02 10:45:57,943 EPOCH 614
2024-02-02 10:46:04,645 Epoch 614: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-02 10:46:04,646 EPOCH 615
2024-02-02 10:46:11,401 Epoch 615: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-02 10:46:11,401 EPOCH 616
2024-02-02 10:46:17,892 Epoch 616: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.83 
2024-02-02 10:46:17,893 EPOCH 617
2024-02-02 10:46:24,788 Epoch 617: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.01 
2024-02-02 10:46:24,788 EPOCH 618
2024-02-02 10:46:27,367 [Epoch: 618 Step: 00010500] Batch Recognition Loss:   0.000291 => Gls Tokens per Sec:     2732 || Batch Translation Loss:   0.017422 => Txt Tokens per Sec:     7169 || Lr: 0.000100
2024-02-02 10:46:31,760 Epoch 618: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.10 
2024-02-02 10:46:31,760 EPOCH 619
2024-02-02 10:46:38,379 Epoch 619: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.14 
2024-02-02 10:46:38,379 EPOCH 620
2024-02-02 10:46:45,182 Epoch 620: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.01 
2024-02-02 10:46:45,183 EPOCH 621
2024-02-02 10:46:52,066 Epoch 621: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.28 
2024-02-02 10:46:52,067 EPOCH 622
2024-02-02 10:46:58,999 Epoch 622: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.31 
2024-02-02 10:46:58,999 EPOCH 623
2024-02-02 10:47:05,667 Epoch 623: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.51 
2024-02-02 10:47:05,667 EPOCH 624
2024-02-02 10:47:07,629 [Epoch: 624 Step: 00010600] Batch Recognition Loss:   0.000284 => Gls Tokens per Sec:     2939 || Batch Translation Loss:   0.094084 => Txt Tokens per Sec:     7911 || Lr: 0.000100
2024-02-02 10:47:12,249 Epoch 624: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.71 
2024-02-02 10:47:12,250 EPOCH 625
2024-02-02 10:47:19,135 Epoch 625: Total Training Recognition Loss 0.00  Total Training Translation Loss 2.35 
2024-02-02 10:47:19,136 EPOCH 626
2024-02-02 10:47:25,798 Epoch 626: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.39 
2024-02-02 10:47:25,798 EPOCH 627
2024-02-02 10:47:32,706 Epoch 627: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.90 
2024-02-02 10:47:32,707 EPOCH 628
2024-02-02 10:47:39,196 Epoch 628: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.70 
2024-02-02 10:47:39,196 EPOCH 629
2024-02-02 10:47:45,786 Epoch 629: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.91 
2024-02-02 10:47:45,787 EPOCH 630
2024-02-02 10:47:49,667 [Epoch: 630 Step: 00010700] Batch Recognition Loss:   0.000338 => Gls Tokens per Sec:     1090 || Batch Translation Loss:   0.095992 => Txt Tokens per Sec:     3000 || Lr: 0.000100
2024-02-02 10:47:52,720 Epoch 630: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.51 
2024-02-02 10:47:52,720 EPOCH 631
2024-02-02 10:47:59,468 Epoch 631: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.70 
2024-02-02 10:47:59,469 EPOCH 632
2024-02-02 10:48:06,357 Epoch 632: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.55 
2024-02-02 10:48:06,357 EPOCH 633
2024-02-02 10:48:13,270 Epoch 633: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.45 
2024-02-02 10:48:13,271 EPOCH 634
2024-02-02 10:48:20,039 Epoch 634: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.01 
2024-02-02 10:48:20,039 EPOCH 635
2024-02-02 10:48:26,899 Epoch 635: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-02 10:48:26,899 EPOCH 636
2024-02-02 10:48:29,900 [Epoch: 636 Step: 00010800] Batch Recognition Loss:   0.000419 => Gls Tokens per Sec:      984 || Batch Translation Loss:   0.039150 => Txt Tokens per Sec:     2435 || Lr: 0.000100
2024-02-02 10:48:33,778 Epoch 636: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.26 
2024-02-02 10:48:33,779 EPOCH 637
2024-02-02 10:48:40,439 Epoch 637: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.09 
2024-02-02 10:48:40,439 EPOCH 638
2024-02-02 10:48:47,064 Epoch 638: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.92 
2024-02-02 10:48:47,064 EPOCH 639
2024-02-02 10:48:53,976 Epoch 639: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.78 
2024-02-02 10:48:53,977 EPOCH 640
2024-02-02 10:49:00,830 Epoch 640: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.93 
2024-02-02 10:49:00,830 EPOCH 641
2024-02-02 10:49:07,429 Epoch 641: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-02 10:49:07,429 EPOCH 642
2024-02-02 10:49:08,255 [Epoch: 642 Step: 00010900] Batch Recognition Loss:   0.000267 => Gls Tokens per Sec:     2327 || Batch Translation Loss:   0.047046 => Txt Tokens per Sec:     6561 || Lr: 0.000100
2024-02-02 10:49:13,915 Epoch 642: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.85 
2024-02-02 10:49:13,915 EPOCH 643
2024-02-02 10:49:20,515 Epoch 643: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.86 
2024-02-02 10:49:20,515 EPOCH 644
2024-02-02 10:49:27,438 Epoch 644: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.77 
2024-02-02 10:49:27,439 EPOCH 645
2024-02-02 10:49:34,200 Epoch 645: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-02 10:49:34,200 EPOCH 646
2024-02-02 10:49:40,719 Epoch 646: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.91 
2024-02-02 10:49:40,720 EPOCH 647
2024-02-02 10:49:47,539 Epoch 647: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.50 
2024-02-02 10:49:47,540 EPOCH 648
2024-02-02 10:49:47,829 [Epoch: 648 Step: 00011000] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     2215 || Batch Translation Loss:   0.061805 => Txt Tokens per Sec:     6460 || Lr: 0.000100
2024-02-02 10:49:54,246 Epoch 648: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.13 
2024-02-02 10:49:54,246 EPOCH 649
2024-02-02 10:50:00,937 Epoch 649: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-02 10:50:00,937 EPOCH 650
2024-02-02 10:50:06,928 Epoch 650: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.44 
2024-02-02 10:50:06,929 EPOCH 651
2024-02-02 10:50:13,424 Epoch 651: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.48 
2024-02-02 10:50:13,425 EPOCH 652
2024-02-02 10:50:20,298 Epoch 652: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.99 
2024-02-02 10:50:20,298 EPOCH 653
2024-02-02 10:50:26,461 [Epoch: 653 Step: 00011100] Batch Recognition Loss:   0.000781 => Gls Tokens per Sec:     1621 || Batch Translation Loss:   0.861705 => Txt Tokens per Sec:     4492 || Lr: 0.000100
2024-02-02 10:50:26,766 Epoch 653: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.13 
2024-02-02 10:50:26,766 EPOCH 654
2024-02-02 10:50:33,679 Epoch 654: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.18 
2024-02-02 10:50:33,679 EPOCH 655
2024-02-02 10:50:40,504 Epoch 655: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.21 
2024-02-02 10:50:40,504 EPOCH 656
2024-02-02 10:50:47,345 Epoch 656: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.77 
2024-02-02 10:50:47,346 EPOCH 657
2024-02-02 10:50:54,189 Epoch 657: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.28 
2024-02-02 10:50:54,190 EPOCH 658
2024-02-02 10:51:00,715 Epoch 658: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-02 10:51:00,716 EPOCH 659
2024-02-02 10:51:06,869 [Epoch: 659 Step: 00011200] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:     1416 || Batch Translation Loss:   0.044502 => Txt Tokens per Sec:     3950 || Lr: 0.000100
2024-02-02 10:51:07,897 Epoch 659: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-02 10:51:07,898 EPOCH 660
2024-02-02 10:51:14,737 Epoch 660: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-02 10:51:14,738 EPOCH 661
2024-02-02 10:51:21,266 Epoch 661: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-02 10:51:21,266 EPOCH 662
2024-02-02 10:51:27,985 Epoch 662: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.71 
2024-02-02 10:51:27,986 EPOCH 663
2024-02-02 10:51:34,678 Epoch 663: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-02 10:51:34,678 EPOCH 664
2024-02-02 10:51:41,178 Epoch 664: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-02 10:51:41,179 EPOCH 665
2024-02-02 10:51:46,830 [Epoch: 665 Step: 00011300] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     1315 || Batch Translation Loss:   0.024808 => Txt Tokens per Sec:     3750 || Lr: 0.000100
2024-02-02 10:51:48,221 Epoch 665: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 10:51:48,222 EPOCH 666
2024-02-02 10:51:54,798 Epoch 666: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 10:51:54,798 EPOCH 667
2024-02-02 10:52:01,686 Epoch 667: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 10:52:01,687 EPOCH 668
2024-02-02 10:52:08,528 Epoch 668: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 10:52:08,528 EPOCH 669
2024-02-02 10:52:15,394 Epoch 669: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 10:52:15,395 EPOCH 670
2024-02-02 10:52:21,793 Epoch 670: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 10:52:21,794 EPOCH 671
2024-02-02 10:52:26,673 [Epoch: 671 Step: 00011400] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:     1261 || Batch Translation Loss:   0.026586 => Txt Tokens per Sec:     3608 || Lr: 0.000100
2024-02-02 10:52:28,566 Epoch 671: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 10:52:28,566 EPOCH 672
2024-02-02 10:52:35,222 Epoch 672: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-02 10:52:35,223 EPOCH 673
2024-02-02 10:52:41,898 Epoch 673: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-02 10:52:41,899 EPOCH 674
2024-02-02 10:52:48,481 Epoch 674: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-02 10:52:48,482 EPOCH 675
2024-02-02 10:52:55,246 Epoch 675: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.86 
2024-02-02 10:52:55,247 EPOCH 676
2024-02-02 10:53:01,982 Epoch 676: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.86 
2024-02-02 10:53:01,982 EPOCH 677
2024-02-02 10:53:04,198 [Epoch: 677 Step: 00011500] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     2312 || Batch Translation Loss:   0.077776 => Txt Tokens per Sec:     6124 || Lr: 0.000100
2024-02-02 10:53:08,816 Epoch 677: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.97 
2024-02-02 10:53:08,816 EPOCH 678
2024-02-02 10:53:15,421 Epoch 678: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.36 
2024-02-02 10:53:15,422 EPOCH 679
2024-02-02 10:53:22,456 Epoch 679: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.45 
2024-02-02 10:53:22,457 EPOCH 680
2024-02-02 10:53:28,963 Epoch 680: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.32 
2024-02-02 10:53:28,964 EPOCH 681
2024-02-02 10:53:35,572 Epoch 681: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.10 
2024-02-02 10:53:35,573 EPOCH 682
2024-02-02 10:53:42,435 Epoch 682: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.16 
2024-02-02 10:53:42,435 EPOCH 683
2024-02-02 10:53:44,109 [Epoch: 683 Step: 00011600] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     2296 || Batch Translation Loss:   0.158524 => Txt Tokens per Sec:     6567 || Lr: 0.000100
2024-02-02 10:53:49,295 Epoch 683: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.31 
2024-02-02 10:53:49,295 EPOCH 684
2024-02-02 10:53:56,140 Epoch 684: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.48 
2024-02-02 10:53:56,141 EPOCH 685
2024-02-02 10:54:02,573 Epoch 685: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.55 
2024-02-02 10:54:02,574 EPOCH 686
2024-02-02 10:54:08,595 Epoch 686: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.05 
2024-02-02 10:54:08,596 EPOCH 687
2024-02-02 10:54:15,455 Epoch 687: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.15 
2024-02-02 10:54:15,455 EPOCH 688
2024-02-02 10:54:22,404 Epoch 688: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.18 
2024-02-02 10:54:22,404 EPOCH 689
2024-02-02 10:54:25,757 [Epoch: 689 Step: 00011700] Batch Recognition Loss:   0.000238 => Gls Tokens per Sec:      689 || Batch Translation Loss:   0.045426 => Txt Tokens per Sec:     2094 || Lr: 0.000100
2024-02-02 10:54:29,253 Epoch 689: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.40 
2024-02-02 10:54:29,253 EPOCH 690
2024-02-02 10:54:35,957 Epoch 690: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-02 10:54:35,958 EPOCH 691
2024-02-02 10:54:42,558 Epoch 691: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.21 
2024-02-02 10:54:42,559 EPOCH 692
2024-02-02 10:54:49,384 Epoch 692: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-02 10:54:49,385 EPOCH 693
2024-02-02 10:54:56,045 Epoch 693: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.14 
2024-02-02 10:54:56,046 EPOCH 694
2024-02-02 10:55:02,900 Epoch 694: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.11 
2024-02-02 10:55:02,901 EPOCH 695
2024-02-02 10:55:03,561 [Epoch: 695 Step: 00011800] Batch Recognition Loss:   0.000178 => Gls Tokens per Sec:     1942 || Batch Translation Loss:   0.023937 => Txt Tokens per Sec:     5094 || Lr: 0.000100
2024-02-02 10:55:09,789 Epoch 695: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.93 
2024-02-02 10:55:09,789 EPOCH 696
2024-02-02 10:55:16,647 Epoch 696: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-02 10:55:16,647 EPOCH 697
2024-02-02 10:55:23,409 Epoch 697: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.76 
2024-02-02 10:55:23,409 EPOCH 698
2024-02-02 10:55:30,296 Epoch 698: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-02 10:55:30,297 EPOCH 699
2024-02-02 10:55:36,895 Epoch 699: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 10:55:36,896 EPOCH 700
2024-02-02 10:55:43,578 [Epoch: 700 Step: 00011900] Batch Recognition Loss:   0.000178 => Gls Tokens per Sec:     1591 || Batch Translation Loss:   0.019793 => Txt Tokens per Sec:     4417 || Lr: 0.000100
2024-02-02 10:55:43,579 Epoch 700: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 10:55:43,579 EPOCH 701
2024-02-02 10:55:50,527 Epoch 701: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 10:55:50,528 EPOCH 702
2024-02-02 10:55:57,358 Epoch 702: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-02 10:55:57,359 EPOCH 703
2024-02-02 10:56:04,147 Epoch 703: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-02 10:56:04,148 EPOCH 704
2024-02-02 10:56:10,840 Epoch 704: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 10:56:10,841 EPOCH 705
2024-02-02 10:56:17,702 Epoch 705: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-02 10:56:17,703 EPOCH 706
2024-02-02 10:56:23,978 [Epoch: 706 Step: 00012000] Batch Recognition Loss:   0.000225 => Gls Tokens per Sec:     1491 || Batch Translation Loss:   0.086286 => Txt Tokens per Sec:     4174 || Lr: 0.000100
2024-02-02 10:56:54,536 Hooray! New best validation result [eval_metric]!
2024-02-02 10:56:54,537 Saving new checkpoint.
2024-02-02 10:56:54,826 Validation result at epoch 706, step    12000: duration: 30.8478s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00021	Translation Loss: 95682.77344	PPL: 14399.94629
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 1.06	(BLEU-1: 11.73,	BLEU-2: 3.85,	BLEU-3: 1.85,	BLEU-4: 1.06)
	CHRF 17.62	ROUGE 10.14
2024-02-02 10:56:54,828 Logging Recognition and Translation Outputs
2024-02-02 10:56:54,828 ========================================================================================================================
2024-02-02 10:56:54,829 Logging Sequence: 177_50.00
2024-02-02 10:56:54,829 	Gloss Reference :	A B+C+D+E
2024-02-02 10:56:54,829 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 10:56:54,830 	Gloss Alignment :	         
2024-02-02 10:56:54,830 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 10:56:54,832 	Text Reference  :	a similar reward    of rs      50000 was announced for information against his    associate ajay kumar   
2024-02-02 10:56:54,832 	Text Hypothesis :	* but     according to rumours the   ipl may       be  held        in      search of        the  wrestler
2024-02-02 10:56:54,832 	Text Alignment  :	D S       S         S  S       S     S   S         S   S           S       S      S         S    S       
2024-02-02 10:56:54,832 ========================================================================================================================
2024-02-02 10:56:54,832 Logging Sequence: 122_86.00
2024-02-02 10:56:54,832 	Gloss Reference :	A B+C+D+E
2024-02-02 10:56:54,833 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 10:56:54,833 	Gloss Alignment :	         
2024-02-02 10:56:54,833 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 10:56:54,834 	Text Reference  :	** **** after winning chanu spoke to the **** ***** ** ***** *** media and said    
2024-02-02 10:56:54,834 	Text Hypothesis :	he said that  he      is    part  in the 2012 going on chanu are now   and handsome
2024-02-02 10:56:54,835 	Text Alignment  :	I  I    S     S       S     S     S      I    I     I  I     I   S         S       
2024-02-02 10:56:54,835 ========================================================================================================================
2024-02-02 10:56:54,835 Logging Sequence: 165_27.00
2024-02-02 10:56:54,835 	Gloss Reference :	A B+C+D+E
2024-02-02 10:56:54,835 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 10:56:54,836 	Gloss Alignment :	         
2024-02-02 10:56:54,836 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 10:56:54,837 	Text Reference  :	so then they change their routes some people     believe in  this while some don't
2024-02-02 10:56:54,837 	Text Hypothesis :	** **** **** ****** ***** it     is   disgusting that    his bag  did   not  agree
2024-02-02 10:56:54,837 	Text Alignment  :	D  D    D    D      D     S      S    S          S       S   S    S     S    S    
2024-02-02 10:56:54,837 ========================================================================================================================
2024-02-02 10:56:54,837 Logging Sequence: 70_65.00
2024-02-02 10:56:54,838 	Gloss Reference :	A B+C+D+E
2024-02-02 10:56:54,838 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 10:56:54,838 	Gloss Alignment :	         
2024-02-02 10:56:54,838 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 10:56:54,839 	Text Reference  :	during the press   conference a   table was  placed in front of the   media   
2024-02-02 10:56:54,839 	Text Hypothesis :	this   is  because of         the euro  2020 is     in ***** ** tokyo olympics
2024-02-02 10:56:54,839 	Text Alignment  :	S      S   S       S          S   S     S    S         D     D  S     S       
2024-02-02 10:56:54,840 ========================================================================================================================
2024-02-02 10:56:54,840 Logging Sequence: 149_65.00
2024-02-02 10:56:54,841 	Gloss Reference :	A B+C+D+E
2024-02-02 10:56:54,841 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 10:56:54,841 	Gloss Alignment :	         
2024-02-02 10:56:54,841 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 10:56:54,844 	Text Reference  :	at 6am on 6th november 2022  the     police reached  sri lankan team's      hotel in sydney  australia's central business district cbd 
2024-02-02 10:56:54,844 	Text Hypothesis :	** *** ** *** the      woman alleged that   danushka had sexual intercourse with  a  without his         consent which    means    rape
2024-02-02 10:56:54,844 	Text Alignment  :	D  D   D  D   S        S     S       S      S        S   S      S           S     S  S       S           S       S        S        S   
2024-02-02 10:56:54,844 ========================================================================================================================
2024-02-02 10:56:55,341 Epoch 706: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-02 10:56:55,342 EPOCH 707
2024-02-02 10:57:02,157 Epoch 707: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-02 10:57:02,157 EPOCH 708
2024-02-02 10:57:08,994 Epoch 708: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-02 10:57:08,995 EPOCH 709
2024-02-02 10:57:15,705 Epoch 709: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-02 10:57:15,706 EPOCH 710
2024-02-02 10:57:22,632 Epoch 710: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.80 
2024-02-02 10:57:22,633 EPOCH 711
2024-02-02 10:57:29,585 Epoch 711: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.82 
2024-02-02 10:57:29,586 EPOCH 712
2024-02-02 10:57:35,698 [Epoch: 712 Step: 00012100] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:     1321 || Batch Translation Loss:   0.023603 => Txt Tokens per Sec:     3818 || Lr: 0.000100
2024-02-02 10:57:36,663 Epoch 712: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.79 
2024-02-02 10:57:36,663 EPOCH 713
2024-02-02 10:57:43,089 Epoch 713: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.85 
2024-02-02 10:57:43,089 EPOCH 714
2024-02-02 10:57:49,910 Epoch 714: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.23 
2024-02-02 10:57:49,910 EPOCH 715
2024-02-02 10:57:56,656 Epoch 715: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.22 
2024-02-02 10:57:56,657 EPOCH 716
2024-02-02 10:58:03,364 Epoch 716: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.06 
2024-02-02 10:58:03,364 EPOCH 717
2024-02-02 10:58:10,153 Epoch 717: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.88 
2024-02-02 10:58:10,154 EPOCH 718
2024-02-02 10:58:15,324 [Epoch: 718 Step: 00012200] Batch Recognition Loss:   0.000279 => Gls Tokens per Sec:     1313 || Batch Translation Loss:   0.033015 => Txt Tokens per Sec:     3890 || Lr: 0.000100
2024-02-02 10:58:16,706 Epoch 718: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.98 
2024-02-02 10:58:16,706 EPOCH 719
2024-02-02 10:58:23,401 Epoch 719: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.28 
2024-02-02 10:58:23,402 EPOCH 720
2024-02-02 10:58:30,223 Epoch 720: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.49 
2024-02-02 10:58:30,224 EPOCH 721
2024-02-02 10:58:37,119 Epoch 721: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.55 
2024-02-02 10:58:37,120 EPOCH 722
2024-02-02 10:58:43,882 Epoch 722: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.44 
2024-02-02 10:58:43,882 EPOCH 723
2024-02-02 10:58:50,686 Epoch 723: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.31 
2024-02-02 10:58:50,687 EPOCH 724
2024-02-02 10:58:55,370 [Epoch: 724 Step: 00012300] Batch Recognition Loss:   0.000295 => Gls Tokens per Sec:     1177 || Batch Translation Loss:   0.073008 => Txt Tokens per Sec:     3291 || Lr: 0.000100
2024-02-02 10:58:57,307 Epoch 724: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-02 10:58:57,308 EPOCH 725
2024-02-02 10:59:04,087 Epoch 725: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.81 
2024-02-02 10:59:04,087 EPOCH 726
2024-02-02 10:59:10,905 Epoch 726: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-02 10:59:10,906 EPOCH 727
2024-02-02 10:59:17,816 Epoch 727: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-02 10:59:17,816 EPOCH 728
2024-02-02 10:59:23,820 Epoch 728: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-02 10:59:23,820 EPOCH 729
2024-02-02 10:59:30,501 Epoch 729: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.79 
2024-02-02 10:59:30,502 EPOCH 730
2024-02-02 10:59:34,444 [Epoch: 730 Step: 00012400] Batch Recognition Loss:   0.000282 => Gls Tokens per Sec:     1073 || Batch Translation Loss:   0.025999 => Txt Tokens per Sec:     2985 || Lr: 0.000100
2024-02-02 10:59:37,423 Epoch 730: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.76 
2024-02-02 10:59:37,424 EPOCH 731
2024-02-02 10:59:44,376 Epoch 731: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.03 
2024-02-02 10:59:44,376 EPOCH 732
2024-02-02 10:59:51,006 Epoch 732: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.17 
2024-02-02 10:59:51,007 EPOCH 733
2024-02-02 10:59:57,577 Epoch 733: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.69 
2024-02-02 10:59:57,578 EPOCH 734
2024-02-02 11:00:04,067 Epoch 734: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.25 
2024-02-02 11:00:04,068 EPOCH 735
2024-02-02 11:00:11,045 Epoch 735: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.14 
2024-02-02 11:00:11,046 EPOCH 736
2024-02-02 11:00:12,489 [Epoch: 736 Step: 00012500] Batch Recognition Loss:   0.000365 => Gls Tokens per Sec:     2219 || Batch Translation Loss:   0.078626 => Txt Tokens per Sec:     5900 || Lr: 0.000100
2024-02-02 11:00:17,926 Epoch 736: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.96 
2024-02-02 11:00:17,927 EPOCH 737
2024-02-02 11:00:24,748 Epoch 737: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.13 
2024-02-02 11:00:24,748 EPOCH 738
2024-02-02 11:00:31,415 Epoch 738: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.15 
2024-02-02 11:00:31,416 EPOCH 739
2024-02-02 11:00:38,291 Epoch 739: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.14 
2024-02-02 11:00:38,292 EPOCH 740
2024-02-02 11:00:44,986 Epoch 740: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.50 
2024-02-02 11:00:44,987 EPOCH 741
2024-02-02 11:00:51,593 Epoch 741: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.47 
2024-02-02 11:00:51,593 EPOCH 742
2024-02-02 11:00:52,108 [Epoch: 742 Step: 00012600] Batch Recognition Loss:   0.000587 => Gls Tokens per Sec:     3743 || Batch Translation Loss:   0.332020 => Txt Tokens per Sec:     9495 || Lr: 0.000100
2024-02-02 11:00:58,439 Epoch 742: Total Training Recognition Loss 0.01  Total Training Translation Loss 11.65 
2024-02-02 11:00:58,440 EPOCH 743
2024-02-02 11:01:05,243 Epoch 743: Total Training Recognition Loss 0.05  Total Training Translation Loss 7.42 
2024-02-02 11:01:05,243 EPOCH 744
2024-02-02 11:01:11,689 Epoch 744: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.31 
2024-02-02 11:01:11,690 EPOCH 745
2024-02-02 11:01:18,621 Epoch 745: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.63 
2024-02-02 11:01:18,622 EPOCH 746
2024-02-02 11:01:25,372 Epoch 746: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-02 11:01:25,372 EPOCH 747
2024-02-02 11:01:32,137 Epoch 747: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-02 11:01:32,137 EPOCH 748
2024-02-02 11:01:32,276 [Epoch: 748 Step: 00012700] Batch Recognition Loss:   0.000305 => Gls Tokens per Sec:     4672 || Batch Translation Loss:   0.052753 => Txt Tokens per Sec:    10810 || Lr: 0.000100
2024-02-02 11:01:38,842 Epoch 748: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-02 11:01:38,843 EPOCH 749
2024-02-02 11:01:45,502 Epoch 749: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-02 11:01:45,502 EPOCH 750
2024-02-02 11:01:52,381 Epoch 750: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-02 11:01:52,382 EPOCH 751
2024-02-02 11:01:59,156 Epoch 751: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 11:01:59,157 EPOCH 752
2024-02-02 11:02:05,916 Epoch 752: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 11:02:05,917 EPOCH 753
2024-02-02 11:02:12,322 [Epoch: 753 Step: 00012800] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     1560 || Batch Translation Loss:   0.018270 => Txt Tokens per Sec:     4292 || Lr: 0.000100
2024-02-02 11:02:12,671 Epoch 753: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 11:02:12,671 EPOCH 754
2024-02-02 11:02:19,622 Epoch 754: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 11:02:19,623 EPOCH 755
2024-02-02 11:02:26,268 Epoch 755: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 11:02:26,269 EPOCH 756
2024-02-02 11:02:32,815 Epoch 756: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 11:02:32,815 EPOCH 757
2024-02-02 11:02:39,605 Epoch 757: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 11:02:39,606 EPOCH 758
2024-02-02 11:02:46,324 Epoch 758: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 11:02:46,324 EPOCH 759
2024-02-02 11:02:52,468 [Epoch: 759 Step: 00012900] Batch Recognition Loss:   0.000221 => Gls Tokens per Sec:     1418 || Batch Translation Loss:   0.023018 => Txt Tokens per Sec:     3905 || Lr: 0.000100
2024-02-02 11:02:53,366 Epoch 759: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 11:02:53,367 EPOCH 760
2024-02-02 11:02:59,834 Epoch 760: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 11:02:59,834 EPOCH 761
2024-02-02 11:03:06,280 Epoch 761: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 11:03:06,281 EPOCH 762
2024-02-02 11:03:13,011 Epoch 762: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 11:03:13,011 EPOCH 763
2024-02-02 11:03:19,914 Epoch 763: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 11:03:19,915 EPOCH 764
2024-02-02 11:03:26,597 Epoch 764: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 11:03:26,598 EPOCH 765
2024-02-02 11:03:31,881 [Epoch: 765 Step: 00013000] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:     1407 || Batch Translation Loss:   0.019432 => Txt Tokens per Sec:     3880 || Lr: 0.000100
2024-02-02 11:03:33,427 Epoch 765: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 11:03:33,428 EPOCH 766
2024-02-02 11:03:40,237 Epoch 766: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 11:03:40,238 EPOCH 767
2024-02-02 11:03:46,982 Epoch 767: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 11:03:46,982 EPOCH 768
2024-02-02 11:03:53,673 Epoch 768: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 11:03:53,673 EPOCH 769
2024-02-02 11:04:00,359 Epoch 769: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 11:04:00,360 EPOCH 770
2024-02-02 11:04:07,031 Epoch 770: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 11:04:07,032 EPOCH 771
2024-02-02 11:04:11,955 [Epoch: 771 Step: 00013100] Batch Recognition Loss:   0.000151 => Gls Tokens per Sec:     1250 || Batch Translation Loss:   0.020584 => Txt Tokens per Sec:     3467 || Lr: 0.000100
2024-02-02 11:04:13,851 Epoch 771: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-02 11:04:13,851 EPOCH 772
2024-02-02 11:04:20,724 Epoch 772: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.82 
2024-02-02 11:04:20,724 EPOCH 773
2024-02-02 11:04:27,562 Epoch 773: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.84 
2024-02-02 11:04:27,563 EPOCH 774
2024-02-02 11:04:34,035 Epoch 774: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.03 
2024-02-02 11:04:34,035 EPOCH 775
2024-02-02 11:04:40,866 Epoch 775: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.40 
2024-02-02 11:04:40,867 EPOCH 776
2024-02-02 11:04:47,479 Epoch 776: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.08 
2024-02-02 11:04:47,480 EPOCH 777
2024-02-02 11:04:51,852 [Epoch: 777 Step: 00013200] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:     1114 || Batch Translation Loss:   0.028385 => Txt Tokens per Sec:     3022 || Lr: 0.000100
2024-02-02 11:04:54,445 Epoch 777: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-02 11:04:54,445 EPOCH 778
2024-02-02 11:05:01,420 Epoch 778: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.10 
2024-02-02 11:05:01,421 EPOCH 779
2024-02-02 11:05:08,362 Epoch 779: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-02 11:05:08,363 EPOCH 780
2024-02-02 11:05:15,235 Epoch 780: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.91 
2024-02-02 11:05:15,236 EPOCH 781
2024-02-02 11:05:22,004 Epoch 781: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.87 
2024-02-02 11:05:22,005 EPOCH 782
2024-02-02 11:05:28,934 Epoch 782: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-02 11:05:28,935 EPOCH 783
2024-02-02 11:05:30,497 [Epoch: 783 Step: 00013300] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:     2458 || Batch Translation Loss:   0.057557 => Txt Tokens per Sec:     6753 || Lr: 0.000100
2024-02-02 11:05:35,940 Epoch 783: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-02 11:05:35,941 EPOCH 784
2024-02-02 11:05:43,110 Epoch 784: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 11:05:43,110 EPOCH 785
2024-02-02 11:05:49,969 Epoch 785: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 11:05:49,969 EPOCH 786
2024-02-02 11:05:57,043 Epoch 786: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-02 11:05:57,043 EPOCH 787
2024-02-02 11:06:04,034 Epoch 787: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-02 11:06:04,035 EPOCH 788
2024-02-02 11:06:10,842 Epoch 788: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-02 11:06:10,842 EPOCH 789
2024-02-02 11:06:11,561 [Epoch: 789 Step: 00013400] Batch Recognition Loss:   0.000180 => Gls Tokens per Sec:     3565 || Batch Translation Loss:   0.030370 => Txt Tokens per Sec:     8533 || Lr: 0.000100
2024-02-02 11:06:17,470 Epoch 789: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-02 11:06:17,471 EPOCH 790
2024-02-02 11:06:24,244 Epoch 790: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-02 11:06:24,245 EPOCH 791
2024-02-02 11:06:31,129 Epoch 791: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.71 
2024-02-02 11:06:31,130 EPOCH 792
2024-02-02 11:06:37,876 Epoch 792: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 11:06:37,876 EPOCH 793
2024-02-02 11:06:44,518 Epoch 793: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 11:06:44,519 EPOCH 794
2024-02-02 11:06:51,486 Epoch 794: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.96 
2024-02-02 11:06:51,487 EPOCH 795
2024-02-02 11:06:51,944 [Epoch: 795 Step: 00013500] Batch Recognition Loss:   0.000169 => Gls Tokens per Sec:     2807 || Batch Translation Loss:   0.079458 => Txt Tokens per Sec:     7671 || Lr: 0.000100
2024-02-02 11:06:58,404 Epoch 795: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.28 
2024-02-02 11:06:58,405 EPOCH 796
2024-02-02 11:07:05,213 Epoch 796: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.25 
2024-02-02 11:07:05,214 EPOCH 797
2024-02-02 11:07:11,619 Epoch 797: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.03 
2024-02-02 11:07:11,619 EPOCH 798
2024-02-02 11:07:18,351 Epoch 798: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.05 
2024-02-02 11:07:18,352 EPOCH 799
2024-02-02 11:07:25,142 Epoch 799: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.01 
2024-02-02 11:07:25,142 EPOCH 800
2024-02-02 11:07:31,909 [Epoch: 800 Step: 00013600] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     1571 || Batch Translation Loss:   0.130436 => Txt Tokens per Sec:     4361 || Lr: 0.000100
2024-02-02 11:07:31,910 Epoch 800: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.25 
2024-02-02 11:07:31,910 EPOCH 801
2024-02-02 11:07:38,293 Epoch 801: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.62 
2024-02-02 11:07:38,293 EPOCH 802
2024-02-02 11:07:45,212 Epoch 802: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-02 11:07:45,212 EPOCH 803
2024-02-02 11:07:52,013 Epoch 803: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.45 
2024-02-02 11:07:52,013 EPOCH 804
2024-02-02 11:07:58,600 Epoch 804: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.35 
2024-02-02 11:07:58,601 EPOCH 805
2024-02-02 11:08:05,127 Epoch 805: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-02 11:08:05,128 EPOCH 806
2024-02-02 11:08:11,556 [Epoch: 806 Step: 00013700] Batch Recognition Loss:   0.000307 => Gls Tokens per Sec:     1455 || Batch Translation Loss:   0.046850 => Txt Tokens per Sec:     4048 || Lr: 0.000100
2024-02-02 11:08:12,072 Epoch 806: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.24 
2024-02-02 11:08:12,072 EPOCH 807
2024-02-02 11:08:19,137 Epoch 807: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.34 
2024-02-02 11:08:19,137 EPOCH 808
2024-02-02 11:08:26,105 Epoch 808: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-02 11:08:26,106 EPOCH 809
2024-02-02 11:08:32,974 Epoch 809: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-02 11:08:32,974 EPOCH 810
2024-02-02 11:08:39,830 Epoch 810: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.92 
2024-02-02 11:08:39,830 EPOCH 811
2024-02-02 11:08:46,330 Epoch 811: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.09 
2024-02-02 11:08:46,331 EPOCH 812
2024-02-02 11:08:51,733 [Epoch: 812 Step: 00013800] Batch Recognition Loss:   0.000274 => Gls Tokens per Sec:     1494 || Batch Translation Loss:   0.124334 => Txt Tokens per Sec:     4137 || Lr: 0.000100
2024-02-02 11:08:53,203 Epoch 812: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.86 
2024-02-02 11:08:53,204 EPOCH 813
2024-02-02 11:09:00,114 Epoch 813: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.56 
2024-02-02 11:09:00,115 EPOCH 814
2024-02-02 11:09:07,077 Epoch 814: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.47 
2024-02-02 11:09:07,078 EPOCH 815
2024-02-02 11:09:13,860 Epoch 815: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.73 
2024-02-02 11:09:13,861 EPOCH 816
2024-02-02 11:09:20,588 Epoch 816: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-02 11:09:20,589 EPOCH 817
2024-02-02 11:09:27,139 Epoch 817: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.81 
2024-02-02 11:09:27,139 EPOCH 818
2024-02-02 11:09:30,069 [Epoch: 818 Step: 00013900] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     2406 || Batch Translation Loss:   0.033906 => Txt Tokens per Sec:     6610 || Lr: 0.000100
2024-02-02 11:09:33,851 Epoch 818: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-02 11:09:33,852 EPOCH 819
2024-02-02 11:09:40,436 Epoch 819: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 11:09:40,436 EPOCH 820
2024-02-02 11:09:47,474 Epoch 820: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-02 11:09:47,474 EPOCH 821
2024-02-02 11:09:54,339 Epoch 821: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.07 
2024-02-02 11:09:54,340 EPOCH 822
2024-02-02 11:10:00,977 Epoch 822: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.91 
2024-02-02 11:10:00,977 EPOCH 823
2024-02-02 11:10:07,758 Epoch 823: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.14 
2024-02-02 11:10:07,758 EPOCH 824
2024-02-02 11:10:10,421 [Epoch: 824 Step: 00014000] Batch Recognition Loss:   0.000191 => Gls Tokens per Sec:     2163 || Batch Translation Loss:   0.085916 => Txt Tokens per Sec:     6209 || Lr: 0.000100
2024-02-02 11:10:40,689 Validation result at epoch 824, step    14000: duration: 30.2665s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00025	Translation Loss: 94254.34375	PPL: 12481.93164
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.70	(BLEU-1: 10.79,	BLEU-2: 3.48,	BLEU-3: 1.41,	BLEU-4: 0.70)
	CHRF 17.28	ROUGE 9.23
2024-02-02 11:10:40,690 Logging Recognition and Translation Outputs
2024-02-02 11:10:40,690 ========================================================================================================================
2024-02-02 11:10:40,690 Logging Sequence: 141_40.00
2024-02-02 11:10:40,691 	Gloss Reference :	A B+C+D+E
2024-02-02 11:10:40,691 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 11:10:40,691 	Gloss Alignment :	         
2024-02-02 11:10:40,691 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 11:10:40,692 	Text Reference  :	got infected with  covid-19 he  was quarantined and could not take part in      the warmup match
2024-02-02 11:10:40,692 	Text Hypothesis :	*** ******** india had      won the toss        and ***** *** **** **** decided to  bowl   first
2024-02-02 11:10:40,693 	Text Alignment  :	D   D        S     S        S   S   S               D     D   D    D    S       S   S      S    
2024-02-02 11:10:40,693 ========================================================================================================================
2024-02-02 11:10:40,693 Logging Sequence: 117_37.00
2024-02-02 11:10:40,693 	Gloss Reference :	A B+C+D+E
2024-02-02 11:10:40,693 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 11:10:40,693 	Gloss Alignment :	         
2024-02-02 11:10:40,694 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 11:10:40,694 	Text Reference  :	shikhar dhawan put up a    wonderful performance scoring 98     runs
2024-02-02 11:10:40,694 	Text Hypothesis :	******* ****** *** on 23rd september 2022        he      scored 3175
2024-02-02 11:10:40,694 	Text Alignment  :	D       D      D   S  S    S         S           S       S      S   
2024-02-02 11:10:40,695 ========================================================================================================================
2024-02-02 11:10:40,695 Logging Sequence: 64_13.00
2024-02-02 11:10:40,695 	Gloss Reference :	A B+C+D+E
2024-02-02 11:10:40,695 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 11:10:40,695 	Gloss Alignment :	         
2024-02-02 11:10:40,695 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 11:10:40,697 	Text Reference  :	*** arrangements were made to  move all the ipl matches to the ******* *** wankhede stadium in     mumbai
2024-02-02 11:10:40,697 	Text Hypothesis :	for the          bcci has  not move *** *** ipl ******* to the country has banned   all     indian team  
2024-02-02 11:10:40,697 	Text Alignment  :	I   S            S    S    S        D   D       D              I       I   S        S       S      S     
2024-02-02 11:10:40,697 ========================================================================================================================
2024-02-02 11:10:40,697 Logging Sequence: 98_121.00
2024-02-02 11:10:40,697 	Gloss Reference :	A B+C+D+E
2024-02-02 11:10:40,697 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 11:10:40,698 	Gloss Alignment :	         
2024-02-02 11:10:40,698 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 11:10:40,699 	Text Reference  :	so then england legends and bangladesh legends were added to    the     tournament
2024-02-02 11:10:40,699 	Text Hypothesis :	** **** out     of      the tournament still   a    huge  fight between india     
2024-02-02 11:10:40,699 	Text Alignment  :	D  D    S       S       S   S          S       S    S     S     S       S         
2024-02-02 11:10:40,699 ========================================================================================================================
2024-02-02 11:10:40,699 Logging Sequence: 179_414.00
2024-02-02 11:10:40,699 	Gloss Reference :	A B+C+D+E
2024-02-02 11:10:40,700 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 11:10:40,700 	Gloss Alignment :	         
2024-02-02 11:10:40,700 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 11:10:40,701 	Text Reference  :	we could not    travel to      delhi as    there     was a lockdown in our home town     haryana
2024-02-02 11:10:40,701 	Text Hypothesis :	** ***** people were   stunned over  their behaviour for a ******** ** *** sai  official twitter
2024-02-02 11:10:40,701 	Text Alignment  :	D  D     S      S      S       S     S     S         S     D        D  D   S    S        S      
2024-02-02 11:10:40,701 ========================================================================================================================
2024-02-02 11:10:44,645 Epoch 824: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.23 
2024-02-02 11:10:44,645 EPOCH 825
2024-02-02 11:10:51,651 Epoch 825: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.21 
2024-02-02 11:10:51,652 EPOCH 826
2024-02-02 11:10:58,405 Epoch 826: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.92 
2024-02-02 11:10:58,406 EPOCH 827
2024-02-02 11:11:05,232 Epoch 827: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.85 
2024-02-02 11:11:05,233 EPOCH 828
2024-02-02 11:11:11,943 Epoch 828: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.85 
2024-02-02 11:11:11,944 EPOCH 829
2024-02-02 11:11:18,986 Epoch 829: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-02 11:11:18,987 EPOCH 830
2024-02-02 11:11:20,722 [Epoch: 830 Step: 00014100] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     2584 || Batch Translation Loss:   0.033703 => Txt Tokens per Sec:     6710 || Lr: 0.000100
2024-02-02 11:11:25,776 Epoch 830: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-02 11:11:25,776 EPOCH 831
2024-02-02 11:11:32,655 Epoch 831: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-02 11:11:32,656 EPOCH 832
2024-02-02 11:11:39,613 Epoch 832: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 11:11:39,613 EPOCH 833
2024-02-02 11:11:46,394 Epoch 833: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-02 11:11:46,395 EPOCH 834
2024-02-02 11:11:53,246 Epoch 834: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.14 
2024-02-02 11:11:53,247 EPOCH 835
2024-02-02 11:11:59,783 Epoch 835: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.01 
2024-02-02 11:11:59,784 EPOCH 836
2024-02-02 11:12:02,885 [Epoch: 836 Step: 00014200] Batch Recognition Loss:   0.000197 => Gls Tokens per Sec:      952 || Batch Translation Loss:   0.041770 => Txt Tokens per Sec:     2385 || Lr: 0.000100
2024-02-02 11:12:06,465 Epoch 836: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.22 
2024-02-02 11:12:06,465 EPOCH 837
2024-02-02 11:12:12,441 Epoch 837: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.30 
2024-02-02 11:12:12,441 EPOCH 838
2024-02-02 11:12:19,410 Epoch 838: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.90 
2024-02-02 11:12:19,411 EPOCH 839
2024-02-02 11:12:26,224 Epoch 839: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.97 
2024-02-02 11:12:26,224 EPOCH 840
2024-02-02 11:12:33,077 Epoch 840: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.00 
2024-02-02 11:12:33,078 EPOCH 841
2024-02-02 11:12:39,806 Epoch 841: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.22 
2024-02-02 11:12:39,807 EPOCH 842
2024-02-02 11:12:40,689 [Epoch: 842 Step: 00014300] Batch Recognition Loss:   0.000314 => Gls Tokens per Sec:     2179 || Batch Translation Loss:   0.085147 => Txt Tokens per Sec:     6070 || Lr: 0.000100
2024-02-02 11:12:46,296 Epoch 842: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.18 
2024-02-02 11:12:46,296 EPOCH 843
2024-02-02 11:12:52,908 Epoch 843: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.03 
2024-02-02 11:12:52,909 EPOCH 844
2024-02-02 11:12:59,576 Epoch 844: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.07 
2024-02-02 11:12:59,577 EPOCH 845
2024-02-02 11:13:06,384 Epoch 845: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.79 
2024-02-02 11:13:06,384 EPOCH 846
2024-02-02 11:13:13,243 Epoch 846: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.11 
2024-02-02 11:13:13,244 EPOCH 847
2024-02-02 11:13:20,035 Epoch 847: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.91 
2024-02-02 11:13:20,036 EPOCH 848
2024-02-02 11:13:20,588 [Epoch: 848 Step: 00014400] Batch Recognition Loss:   0.000373 => Gls Tokens per Sec:     1159 || Batch Translation Loss:   0.050442 => Txt Tokens per Sec:     3754 || Lr: 0.000100
2024-02-02 11:13:26,871 Epoch 848: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.34 
2024-02-02 11:13:26,872 EPOCH 849
2024-02-02 11:13:33,706 Epoch 849: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.47 
2024-02-02 11:13:33,707 EPOCH 850
2024-02-02 11:13:40,671 Epoch 850: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.22 
2024-02-02 11:13:40,671 EPOCH 851
2024-02-02 11:13:47,715 Epoch 851: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.30 
2024-02-02 11:13:47,716 EPOCH 852
2024-02-02 11:13:54,529 Epoch 852: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.32 
2024-02-02 11:13:54,530 EPOCH 853
2024-02-02 11:14:00,575 [Epoch: 853 Step: 00014500] Batch Recognition Loss:   0.000268 => Gls Tokens per Sec:     1653 || Batch Translation Loss:   0.174139 => Txt Tokens per Sec:     4515 || Lr: 0.000100
2024-02-02 11:14:01,146 Epoch 853: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.00 
2024-02-02 11:14:01,147 EPOCH 854
2024-02-02 11:14:07,842 Epoch 854: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.90 
2024-02-02 11:14:07,843 EPOCH 855
2024-02-02 11:14:14,481 Epoch 855: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.12 
2024-02-02 11:14:14,481 EPOCH 856
2024-02-02 11:14:21,450 Epoch 856: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.05 
2024-02-02 11:14:21,450 EPOCH 857
2024-02-02 11:14:28,121 Epoch 857: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.70 
2024-02-02 11:14:28,122 EPOCH 858
2024-02-02 11:14:34,692 Epoch 858: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-02 11:14:34,692 EPOCH 859
2024-02-02 11:14:40,615 [Epoch: 859 Step: 00014600] Batch Recognition Loss:   0.000228 => Gls Tokens per Sec:     1471 || Batch Translation Loss:   0.024340 => Txt Tokens per Sec:     4107 || Lr: 0.000100
2024-02-02 11:14:41,297 Epoch 859: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 11:14:41,298 EPOCH 860
2024-02-02 11:14:47,971 Epoch 860: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 11:14:47,972 EPOCH 861
2024-02-02 11:14:54,696 Epoch 861: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-02 11:14:54,697 EPOCH 862
2024-02-02 11:15:01,520 Epoch 862: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-02 11:15:01,521 EPOCH 863
2024-02-02 11:15:08,123 Epoch 863: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-02 11:15:08,124 EPOCH 864
2024-02-02 11:15:14,812 Epoch 864: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 11:15:14,813 EPOCH 865
2024-02-02 11:15:20,383 [Epoch: 865 Step: 00014700] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     1334 || Batch Translation Loss:   0.047416 => Txt Tokens per Sec:     3781 || Lr: 0.000100
2024-02-02 11:15:21,609 Epoch 865: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 11:15:21,610 EPOCH 866
2024-02-02 11:15:28,442 Epoch 866: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 11:15:28,442 EPOCH 867
2024-02-02 11:15:35,364 Epoch 867: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-02 11:15:35,364 EPOCH 868
2024-02-02 11:15:42,051 Epoch 868: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-02 11:15:42,051 EPOCH 869
2024-02-02 11:15:48,823 Epoch 869: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-02 11:15:48,824 EPOCH 870
2024-02-02 11:15:55,757 Epoch 870: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-02 11:15:55,757 EPOCH 871
2024-02-02 11:15:58,208 [Epoch: 871 Step: 00014800] Batch Recognition Loss:   0.000304 => Gls Tokens per Sec:     2612 || Batch Translation Loss:   0.054144 => Txt Tokens per Sec:     7298 || Lr: 0.000100
2024-02-02 11:16:02,378 Epoch 871: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-02 11:16:02,378 EPOCH 872
2024-02-02 11:16:09,087 Epoch 872: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.81 
2024-02-02 11:16:09,087 EPOCH 873
2024-02-02 11:16:16,162 Epoch 873: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.00 
2024-02-02 11:16:16,162 EPOCH 874
2024-02-02 11:16:23,006 Epoch 874: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.33 
2024-02-02 11:16:23,006 EPOCH 875
2024-02-02 11:16:29,594 Epoch 875: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.40 
2024-02-02 11:16:29,594 EPOCH 876
2024-02-02 11:16:36,102 Epoch 876: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.15 
2024-02-02 11:16:36,103 EPOCH 877
2024-02-02 11:16:40,788 [Epoch: 877 Step: 00014900] Batch Recognition Loss:   0.000358 => Gls Tokens per Sec:     1040 || Batch Translation Loss:   0.047218 => Txt Tokens per Sec:     3138 || Lr: 0.000100
2024-02-02 11:16:43,017 Epoch 877: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-02 11:16:43,018 EPOCH 878
2024-02-02 11:16:49,743 Epoch 878: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.79 
2024-02-02 11:16:49,743 EPOCH 879
2024-02-02 11:16:56,709 Epoch 879: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.80 
2024-02-02 11:16:56,709 EPOCH 880
2024-02-02 11:17:03,476 Epoch 880: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.80 
2024-02-02 11:17:03,476 EPOCH 881
2024-02-02 11:17:10,063 Epoch 881: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.77 
2024-02-02 11:17:10,063 EPOCH 882
2024-02-02 11:17:16,832 Epoch 882: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.87 
2024-02-02 11:17:16,832 EPOCH 883
2024-02-02 11:17:18,434 [Epoch: 883 Step: 00015000] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     2397 || Batch Translation Loss:   0.037096 => Txt Tokens per Sec:     6761 || Lr: 0.000100
2024-02-02 11:17:23,544 Epoch 883: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.21 
2024-02-02 11:17:23,545 EPOCH 884
2024-02-02 11:17:30,272 Epoch 884: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.47 
2024-02-02 11:17:30,273 EPOCH 885
2024-02-02 11:17:37,211 Epoch 885: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-02 11:17:37,211 EPOCH 886
2024-02-02 11:17:43,962 Epoch 886: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.30 
2024-02-02 11:17:43,963 EPOCH 887
2024-02-02 11:17:50,756 Epoch 887: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.93 
2024-02-02 11:17:50,757 EPOCH 888
2024-02-02 11:17:57,439 Epoch 888: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.01 
2024-02-02 11:17:57,440 EPOCH 889
2024-02-02 11:17:58,466 [Epoch: 889 Step: 00015100] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:     2500 || Batch Translation Loss:   0.038137 => Txt Tokens per Sec:     7038 || Lr: 0.000100
2024-02-02 11:18:04,141 Epoch 889: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.05 
2024-02-02 11:18:04,142 EPOCH 890
2024-02-02 11:18:10,980 Epoch 890: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.87 
2024-02-02 11:18:10,981 EPOCH 891
2024-02-02 11:18:17,777 Epoch 891: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.03 
2024-02-02 11:18:17,777 EPOCH 892
2024-02-02 11:18:24,458 Epoch 892: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-02 11:18:24,459 EPOCH 893
2024-02-02 11:18:31,261 Epoch 893: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.03 
2024-02-02 11:18:31,261 EPOCH 894
2024-02-02 11:18:37,826 Epoch 894: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-02 11:18:37,827 EPOCH 895
2024-02-02 11:18:40,576 [Epoch: 895 Step: 00015200] Batch Recognition Loss:   0.000258 => Gls Tokens per Sec:      375 || Batch Translation Loss:   0.066406 => Txt Tokens per Sec:     1258 || Lr: 0.000100
2024-02-02 11:18:44,607 Epoch 895: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.91 
2024-02-02 11:18:44,608 EPOCH 896
2024-02-02 11:18:51,520 Epoch 896: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-02 11:18:51,520 EPOCH 897
2024-02-02 11:18:58,316 Epoch 897: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-02 11:18:58,316 EPOCH 898
2024-02-02 11:19:04,964 Epoch 898: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.80 
2024-02-02 11:19:04,965 EPOCH 899
2024-02-02 11:19:11,672 Epoch 899: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.85 
2024-02-02 11:19:11,673 EPOCH 900
2024-02-02 11:19:18,516 [Epoch: 900 Step: 00015300] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     1554 || Batch Translation Loss:   0.027534 => Txt Tokens per Sec:     4313 || Lr: 0.000100
2024-02-02 11:19:18,516 Epoch 900: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.02 
2024-02-02 11:19:18,517 EPOCH 901
2024-02-02 11:19:24,933 Epoch 901: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-02 11:19:24,934 EPOCH 902
2024-02-02 11:19:31,813 Epoch 902: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-02 11:19:31,813 EPOCH 903
2024-02-02 11:19:38,405 Epoch 903: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-02 11:19:38,405 EPOCH 904
2024-02-02 11:19:45,254 Epoch 904: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-02 11:19:45,254 EPOCH 905
2024-02-02 11:19:52,056 Epoch 905: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 11:19:52,057 EPOCH 906
2024-02-02 11:19:57,544 [Epoch: 906 Step: 00015400] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     1704 || Batch Translation Loss:   0.019712 => Txt Tokens per Sec:     4595 || Lr: 0.000100
2024-02-02 11:19:58,567 Epoch 906: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-02 11:19:58,567 EPOCH 907
2024-02-02 11:20:05,481 Epoch 907: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 11:20:05,482 EPOCH 908
2024-02-02 11:20:12,188 Epoch 908: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.70 
2024-02-02 11:20:12,189 EPOCH 909
2024-02-02 11:20:18,710 Epoch 909: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 11:20:18,710 EPOCH 910
2024-02-02 11:20:25,690 Epoch 910: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-02 11:20:25,690 EPOCH 911
2024-02-02 11:20:32,531 Epoch 911: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.80 
2024-02-02 11:20:32,531 EPOCH 912
2024-02-02 11:20:38,533 [Epoch: 912 Step: 00015500] Batch Recognition Loss:   0.000193 => Gls Tokens per Sec:     1345 || Batch Translation Loss:   0.032184 => Txt Tokens per Sec:     3752 || Lr: 0.000100
2024-02-02 11:20:39,415 Epoch 912: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.98 
2024-02-02 11:20:39,416 EPOCH 913
2024-02-02 11:20:45,987 Epoch 913: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.92 
2024-02-02 11:20:45,988 EPOCH 914
2024-02-02 11:20:52,723 Epoch 914: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.98 
2024-02-02 11:20:52,724 EPOCH 915
2024-02-02 11:20:59,362 Epoch 915: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-02 11:20:59,362 EPOCH 916
2024-02-02 11:21:06,297 Epoch 916: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.88 
2024-02-02 11:21:06,297 EPOCH 917
2024-02-02 11:21:13,155 Epoch 917: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.88 
2024-02-02 11:21:13,156 EPOCH 918
2024-02-02 11:21:18,315 [Epoch: 918 Step: 00015600] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:     1317 || Batch Translation Loss:   0.062169 => Txt Tokens per Sec:     3658 || Lr: 0.000100
2024-02-02 11:21:19,908 Epoch 918: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.85 
2024-02-02 11:21:19,908 EPOCH 919
2024-02-02 11:21:26,097 Epoch 919: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.71 
2024-02-02 11:21:26,098 EPOCH 920
2024-02-02 11:21:32,798 Epoch 920: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-02 11:21:32,799 EPOCH 921
2024-02-02 11:21:39,546 Epoch 921: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 11:21:39,547 EPOCH 922
2024-02-02 11:21:46,223 Epoch 922: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-02 11:21:46,223 EPOCH 923
2024-02-02 11:21:52,865 Epoch 923: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.76 
2024-02-02 11:21:52,865 EPOCH 924
2024-02-02 11:21:55,243 [Epoch: 924 Step: 00015700] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     2422 || Batch Translation Loss:   0.033063 => Txt Tokens per Sec:     7073 || Lr: 0.000100
2024-02-02 11:21:59,364 Epoch 924: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.30 
2024-02-02 11:21:59,365 EPOCH 925
2024-02-02 11:22:06,250 Epoch 925: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.47 
2024-02-02 11:22:06,250 EPOCH 926
2024-02-02 11:22:12,864 Epoch 926: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.08 
2024-02-02 11:22:12,865 EPOCH 927
2024-02-02 11:22:19,270 Epoch 927: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-02 11:22:19,271 EPOCH 928
2024-02-02 11:22:26,041 Epoch 928: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.43 
2024-02-02 11:22:26,041 EPOCH 929
2024-02-02 11:22:32,861 Epoch 929: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.47 
2024-02-02 11:22:32,861 EPOCH 930
2024-02-02 11:22:34,648 [Epoch: 930 Step: 00015800] Batch Recognition Loss:   0.000272 => Gls Tokens per Sec:     2509 || Batch Translation Loss:   0.056569 => Txt Tokens per Sec:     6662 || Lr: 0.000100
2024-02-02 11:22:39,578 Epoch 930: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.58 
2024-02-02 11:22:39,579 EPOCH 931
2024-02-02 11:22:46,371 Epoch 931: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.38 
2024-02-02 11:22:46,372 EPOCH 932
2024-02-02 11:22:52,939 Epoch 932: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-02 11:22:52,940 EPOCH 933
2024-02-02 11:22:59,731 Epoch 933: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-02 11:22:59,731 EPOCH 934
2024-02-02 11:23:06,524 Epoch 934: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-02 11:23:06,525 EPOCH 935
2024-02-02 11:23:13,660 Epoch 935: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-02 11:23:13,661 EPOCH 936
2024-02-02 11:23:17,159 [Epoch: 936 Step: 00015900] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:      844 || Batch Translation Loss:   0.017220 => Txt Tokens per Sec:     2500 || Lr: 0.000100
2024-02-02 11:23:20,326 Epoch 936: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-02 11:23:20,326 EPOCH 937
2024-02-02 11:23:27,158 Epoch 937: Total Training Recognition Loss 0.00  Total Training Translation Loss 5.23 
2024-02-02 11:23:27,159 EPOCH 938
2024-02-02 11:23:33,944 Epoch 938: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.61 
2024-02-02 11:23:33,945 EPOCH 939
2024-02-02 11:23:40,755 Epoch 939: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.10 
2024-02-02 11:23:40,756 EPOCH 940
2024-02-02 11:23:47,543 Epoch 940: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-02 11:23:47,543 EPOCH 941
2024-02-02 11:23:54,292 Epoch 941: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-02 11:23:54,293 EPOCH 942
2024-02-02 11:23:55,083 [Epoch: 942 Step: 00016000] Batch Recognition Loss:   0.000506 => Gls Tokens per Sec:     2434 || Batch Translation Loss:   0.071849 => Txt Tokens per Sec:     6907 || Lr: 0.000100
2024-02-02 11:24:25,492 Validation result at epoch 942, step    16000: duration: 30.4086s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00044	Translation Loss: 94963.22656	PPL: 13399.52441
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.77	(BLEU-1: 11.34,	BLEU-2: 3.53,	BLEU-3: 1.45,	BLEU-4: 0.77)
	CHRF 17.19	ROUGE 9.65
2024-02-02 11:24:25,493 Logging Recognition and Translation Outputs
2024-02-02 11:24:25,494 ========================================================================================================================
2024-02-02 11:24:25,494 Logging Sequence: 147_132.00
2024-02-02 11:24:25,495 	Gloss Reference :	A B+C+D+E
2024-02-02 11:24:25,495 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 11:24:25,495 	Gloss Alignment :	         
2024-02-02 11:24:25,495 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 11:24:25,497 	Text Reference  :	** i  can not ***** ** *** **** *** earlier i     used to ** have    fun in   gymnastics
2024-02-02 11:24:25,497 	Text Hypothesis :	if it is  not known as the same but she     would have to be stopped her from them      
2024-02-02 11:24:25,497 	Text Alignment  :	I  S  S       I     I  I   I    I   S       S     S       I  S       S   S    S         
2024-02-02 11:24:25,497 ========================================================================================================================
2024-02-02 11:24:25,497 Logging Sequence: 116_162.00
2024-02-02 11:24:25,497 	Gloss Reference :	A B+C+D+E
2024-02-02 11:24:25,498 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 11:24:25,498 	Gloss Alignment :	         
2024-02-02 11:24:25,498 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 11:24:25,499 	Text Reference  :	turned out  the     video was shared on social media by   a staff   at   the  hotel
2024-02-02 11:24:25,499 	Text Hypothesis :	on     31st october 2022  he  shared a  video  along with a caption that went viral
2024-02-02 11:24:25,500 	Text Alignment  :	S      S    S       S     S          S  S      S     S      S       S    S    S    
2024-02-02 11:24:25,500 ========================================================================================================================
2024-02-02 11:24:25,500 Logging Sequence: 73_79.00
2024-02-02 11:24:25,500 	Gloss Reference :	A B+C+D+E
2024-02-02 11:24:25,500 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 11:24:25,500 	Gloss Alignment :	         
2024-02-02 11:24:25,501 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 11:24:25,502 	Text Reference  :	raina resturant has food from the rich spices    of   north india to     the aromatic curries of south india     
2024-02-02 11:24:25,502 	Text Hypothesis :	***** ********* *** **** **** on  23rd september 2023 raina has   opened the ******** ******* ** ***** restaurant
2024-02-02 11:24:25,502 	Text Alignment  :	D     D         D   D    D    S   S    S         S    S     S     S          D        D       D  D     S         
2024-02-02 11:24:25,503 ========================================================================================================================
2024-02-02 11:24:25,503 Logging Sequence: 165_523.00
2024-02-02 11:24:25,503 	Gloss Reference :	A B+C+D+E
2024-02-02 11:24:25,503 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 11:24:25,504 	Gloss Alignment :	         
2024-02-02 11:24:25,504 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 11:24:25,505 	Text Reference  :	***** *** *** *** **** as  he     believed that his    team might lose if he   takes off  his  batting pads
2024-02-02 11:24:25,505 	Text Hypothesis :	india had won the toss and choose to       bowl indian team ***** with 5  boys and   they have left    pad 
2024-02-02 11:24:25,506 	Text Alignment  :	I     I   I   I   I    S   S      S        S    S           D     S    S  S    S     S    S    S       S   
2024-02-02 11:24:25,506 ========================================================================================================================
2024-02-02 11:24:25,506 Logging Sequence: 125_72.00
2024-02-02 11:24:25,506 	Gloss Reference :	A B+C+D+E
2024-02-02 11:24:25,506 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 11:24:25,506 	Gloss Alignment :	         
2024-02-02 11:24:25,506 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 11:24:25,507 	Text Reference  :	some said the pakistani javelineer had milicious intentions of  tampering  with      the javelin out of   jealousy
2024-02-02 11:24:25,507 	Text Hypothesis :	**** **** *** ********* ********** *** ********* india      has completely dominated the ******* *** gold medal   
2024-02-02 11:24:25,507 	Text Alignment  :	D    D    D   D         D          D   D         S          S   S          S             D       D   S    S       
2024-02-02 11:24:25,508 ========================================================================================================================
2024-02-02 11:24:31,512 Epoch 942: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-02 11:24:31,512 EPOCH 943
2024-02-02 11:24:37,606 Epoch 943: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-02 11:24:37,606 EPOCH 944
2024-02-02 11:24:44,125 Epoch 944: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-02 11:24:44,126 EPOCH 945
2024-02-02 11:24:50,985 Epoch 945: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 11:24:50,985 EPOCH 946
2024-02-02 11:24:57,692 Epoch 946: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 11:24:57,693 EPOCH 947
2024-02-02 11:25:04,544 Epoch 947: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 11:25:04,545 EPOCH 948
2024-02-02 11:25:04,905 [Epoch: 948 Step: 00016100] Batch Recognition Loss:   0.000226 => Gls Tokens per Sec:     1783 || Batch Translation Loss:   0.018606 => Txt Tokens per Sec:     5733 || Lr: 0.000100
2024-02-02 11:25:11,279 Epoch 948: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 11:25:11,279 EPOCH 949
2024-02-02 11:25:18,170 Epoch 949: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 11:25:18,171 EPOCH 950
2024-02-02 11:25:24,933 Epoch 950: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 11:25:24,933 EPOCH 951
2024-02-02 11:25:31,763 Epoch 951: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 11:25:31,763 EPOCH 952
2024-02-02 11:25:38,216 Epoch 952: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 11:25:38,217 EPOCH 953
2024-02-02 11:25:44,730 [Epoch: 953 Step: 00016200] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     1534 || Batch Translation Loss:   0.025491 => Txt Tokens per Sec:     4290 || Lr: 0.000100
2024-02-02 11:25:44,983 Epoch 953: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 11:25:44,984 EPOCH 954
2024-02-02 11:25:51,733 Epoch 954: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 11:25:51,733 EPOCH 955
2024-02-02 11:25:58,762 Epoch 955: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 11:25:58,762 EPOCH 956
2024-02-02 11:26:05,589 Epoch 956: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 11:26:05,590 EPOCH 957
2024-02-02 11:26:11,963 Epoch 957: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 11:26:11,964 EPOCH 958
2024-02-02 11:26:19,074 Epoch 958: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 11:26:19,075 EPOCH 959
2024-02-02 11:26:22,938 [Epoch: 959 Step: 00016300] Batch Recognition Loss:   0.000226 => Gls Tokens per Sec:     2320 || Batch Translation Loss:   0.010287 => Txt Tokens per Sec:     6200 || Lr: 0.000100
2024-02-02 11:26:25,972 Epoch 959: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 11:26:25,972 EPOCH 960
2024-02-02 11:26:32,181 Epoch 960: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 11:26:32,182 EPOCH 961
2024-02-02 11:26:38,650 Epoch 961: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 11:26:38,650 EPOCH 962
2024-02-02 11:26:45,365 Epoch 962: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-02 11:26:45,366 EPOCH 963
2024-02-02 11:26:51,892 Epoch 963: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-02 11:26:51,893 EPOCH 964
2024-02-02 11:26:58,857 Epoch 964: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 11:26:58,858 EPOCH 965
2024-02-02 11:27:04,283 [Epoch: 965 Step: 00016400] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     1370 || Batch Translation Loss:   0.023707 => Txt Tokens per Sec:     3884 || Lr: 0.000100
2024-02-02 11:27:05,550 Epoch 965: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 11:27:05,551 EPOCH 966
2024-02-02 11:27:12,421 Epoch 966: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 11:27:12,422 EPOCH 967
2024-02-02 11:27:19,211 Epoch 967: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 11:27:19,211 EPOCH 968
2024-02-02 11:27:26,012 Epoch 968: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-02 11:27:26,013 EPOCH 969
2024-02-02 11:27:32,706 Epoch 969: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-02 11:27:32,707 EPOCH 970
2024-02-02 11:27:39,406 Epoch 970: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-02 11:27:39,407 EPOCH 971
2024-02-02 11:27:42,009 [Epoch: 971 Step: 00016500] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     2461 || Batch Translation Loss:   0.022961 => Txt Tokens per Sec:     6657 || Lr: 0.000100
2024-02-02 11:27:46,204 Epoch 971: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-02 11:27:46,204 EPOCH 972
2024-02-02 11:27:52,206 Epoch 972: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.25 
2024-02-02 11:27:52,206 EPOCH 973
2024-02-02 11:27:58,778 Epoch 973: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-02 11:27:58,779 EPOCH 974
2024-02-02 11:28:05,495 Epoch 974: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.70 
2024-02-02 11:28:05,496 EPOCH 975
2024-02-02 11:28:12,184 Epoch 975: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-02 11:28:12,185 EPOCH 976
2024-02-02 11:28:19,145 Epoch 976: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.06 
2024-02-02 11:28:19,146 EPOCH 977
2024-02-02 11:28:23,754 [Epoch: 977 Step: 00016600] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:     1057 || Batch Translation Loss:   0.044949 => Txt Tokens per Sec:     3078 || Lr: 0.000100
2024-02-02 11:28:26,049 Epoch 977: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.03 
2024-02-02 11:28:26,049 EPOCH 978
2024-02-02 11:28:32,978 Epoch 978: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.21 
2024-02-02 11:28:32,978 EPOCH 979
2024-02-02 11:28:39,927 Epoch 979: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.97 
2024-02-02 11:28:39,928 EPOCH 980
2024-02-02 11:28:46,684 Epoch 980: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.12 
2024-02-02 11:28:46,685 EPOCH 981
2024-02-02 11:28:53,350 Epoch 981: Total Training Recognition Loss 0.00  Total Training Translation Loss 2.71 
2024-02-02 11:28:53,350 EPOCH 982
2024-02-02 11:29:00,075 Epoch 982: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.53 
2024-02-02 11:29:00,075 EPOCH 983
2024-02-02 11:29:01,623 [Epoch: 983 Step: 00016700] Batch Recognition Loss:   0.000952 => Gls Tokens per Sec:     2483 || Batch Translation Loss:   0.149254 => Txt Tokens per Sec:     7015 || Lr: 0.000100
2024-02-02 11:29:06,727 Epoch 983: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.03 
2024-02-02 11:29:06,727 EPOCH 984
2024-02-02 11:29:13,776 Epoch 984: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.17 
2024-02-02 11:29:13,777 EPOCH 985
2024-02-02 11:29:20,549 Epoch 985: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.46 
2024-02-02 11:29:20,550 EPOCH 986
2024-02-02 11:29:26,828 Epoch 986: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.19 
2024-02-02 11:29:26,828 EPOCH 987
2024-02-02 11:29:32,799 Epoch 987: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.87 
2024-02-02 11:29:32,799 EPOCH 988
2024-02-02 11:29:39,801 Epoch 988: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-02 11:29:39,801 EPOCH 989
2024-02-02 11:29:40,754 [Epoch: 989 Step: 00016800] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     2689 || Batch Translation Loss:   0.045789 => Txt Tokens per Sec:     6904 || Lr: 0.000100
2024-02-02 11:29:46,782 Epoch 989: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-02 11:29:46,783 EPOCH 990
2024-02-02 11:29:53,656 Epoch 990: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 11:29:53,657 EPOCH 991
2024-02-02 11:30:00,658 Epoch 991: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-02 11:30:00,658 EPOCH 992
2024-02-02 11:30:07,377 Epoch 992: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 11:30:07,378 EPOCH 993
2024-02-02 11:30:13,887 Epoch 993: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 11:30:13,888 EPOCH 994
2024-02-02 11:30:20,560 Epoch 994: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-02 11:30:20,561 EPOCH 995
2024-02-02 11:30:23,163 [Epoch: 995 Step: 00016900] Batch Recognition Loss:   0.000189 => Gls Tokens per Sec:      396 || Batch Translation Loss:   0.025664 => Txt Tokens per Sec:     1289 || Lr: 0.000100
2024-02-02 11:30:27,370 Epoch 995: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 11:30:27,371 EPOCH 996
2024-02-02 11:30:34,245 Epoch 996: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 11:30:34,245 EPOCH 997
2024-02-02 11:30:41,197 Epoch 997: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-02 11:30:41,198 EPOCH 998
2024-02-02 11:30:47,930 Epoch 998: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 11:30:47,931 EPOCH 999
2024-02-02 11:30:54,708 Epoch 999: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 11:30:54,709 EPOCH 1000
2024-02-02 11:31:01,551 [Epoch: 1000 Step: 00017000] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:     1554 || Batch Translation Loss:   0.019776 => Txt Tokens per Sec:     4314 || Lr: 0.000100
2024-02-02 11:31:01,552 Epoch 1000: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 11:31:01,552 EPOCH 1001
2024-02-02 11:31:08,162 Epoch 1001: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 11:31:08,162 EPOCH 1002
2024-02-02 11:31:14,986 Epoch 1002: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 11:31:14,987 EPOCH 1003
2024-02-02 11:31:21,992 Epoch 1003: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 11:31:21,993 EPOCH 1004
2024-02-02 11:31:28,839 Epoch 1004: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 11:31:28,839 EPOCH 1005
2024-02-02 11:31:35,338 Epoch 1005: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 11:31:35,338 EPOCH 1006
2024-02-02 11:31:41,401 [Epoch: 1006 Step: 00017100] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:     1542 || Batch Translation Loss:   0.010178 => Txt Tokens per Sec:     4185 || Lr: 0.000100
2024-02-02 11:31:42,291 Epoch 1006: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 11:31:42,291 EPOCH 1007
2024-02-02 11:31:48,845 Epoch 1007: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 11:31:48,845 EPOCH 1008
2024-02-02 11:31:55,730 Epoch 1008: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 11:31:55,730 EPOCH 1009
2024-02-02 11:32:02,610 Epoch 1009: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 11:32:02,611 EPOCH 1010
2024-02-02 11:32:09,314 Epoch 1010: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 11:32:09,315 EPOCH 1011
2024-02-02 11:32:16,048 Epoch 1011: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 11:32:16,049 EPOCH 1012
2024-02-02 11:32:19,387 [Epoch: 1012 Step: 00017200] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:     2494 || Batch Translation Loss:   0.009780 => Txt Tokens per Sec:     6800 || Lr: 0.000100
2024-02-02 11:32:22,688 Epoch 1012: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 11:32:22,689 EPOCH 1013
2024-02-02 11:32:29,489 Epoch 1013: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-02 11:32:29,490 EPOCH 1014
2024-02-02 11:32:36,491 Epoch 1014: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-02 11:32:36,491 EPOCH 1015
2024-02-02 11:32:43,236 Epoch 1015: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-02 11:32:43,237 EPOCH 1016
2024-02-02 11:32:49,682 Epoch 1016: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-02 11:32:49,683 EPOCH 1017
2024-02-02 11:32:56,344 Epoch 1017: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.76 
2024-02-02 11:32:56,345 EPOCH 1018
2024-02-02 11:33:01,533 [Epoch: 1018 Step: 00017300] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     1309 || Batch Translation Loss:   0.041386 => Txt Tokens per Sec:     3655 || Lr: 0.000100
2024-02-02 11:33:03,639 Epoch 1018: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-02 11:33:03,639 EPOCH 1019
2024-02-02 11:33:10,401 Epoch 1019: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.22 
2024-02-02 11:33:10,402 EPOCH 1020
2024-02-02 11:33:17,368 Epoch 1020: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.16 
2024-02-02 11:33:17,368 EPOCH 1021
2024-02-02 11:33:24,190 Epoch 1021: Total Training Recognition Loss 0.03  Total Training Translation Loss 13.44 
2024-02-02 11:33:24,190 EPOCH 1022
2024-02-02 11:33:31,062 Epoch 1022: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.66 
2024-02-02 11:33:31,062 EPOCH 1023
2024-02-02 11:33:37,803 Epoch 1023: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.66 
2024-02-02 11:33:37,804 EPOCH 1024
2024-02-02 11:33:40,794 [Epoch: 1024 Step: 00017400] Batch Recognition Loss:   0.000641 => Gls Tokens per Sec:     1927 || Batch Translation Loss:   0.057524 => Txt Tokens per Sec:     5396 || Lr: 0.000100
2024-02-02 11:33:44,908 Epoch 1024: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.52 
2024-02-02 11:33:44,908 EPOCH 1025
2024-02-02 11:33:51,878 Epoch 1025: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-02 11:33:51,878 EPOCH 1026
2024-02-02 11:33:58,448 Epoch 1026: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-02 11:33:58,448 EPOCH 1027
2024-02-02 11:34:04,998 Epoch 1027: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-02 11:34:04,999 EPOCH 1028
2024-02-02 11:34:12,007 Epoch 1028: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 11:34:12,008 EPOCH 1029
2024-02-02 11:34:18,824 Epoch 1029: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 11:34:18,825 EPOCH 1030
2024-02-02 11:34:20,894 [Epoch: 1030 Step: 00017500] Batch Recognition Loss:   0.000262 => Gls Tokens per Sec:     2168 || Batch Translation Loss:   0.021754 => Txt Tokens per Sec:     5915 || Lr: 0.000100
2024-02-02 11:34:25,591 Epoch 1030: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 11:34:25,591 EPOCH 1031
2024-02-02 11:34:32,284 Epoch 1031: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 11:34:32,285 EPOCH 1032
2024-02-02 11:34:39,146 Epoch 1032: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 11:34:39,146 EPOCH 1033
2024-02-02 11:34:45,949 Epoch 1033: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 11:34:45,950 EPOCH 1034
2024-02-02 11:34:52,764 Epoch 1034: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 11:34:52,765 EPOCH 1035
2024-02-02 11:34:59,405 Epoch 1035: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 11:34:59,406 EPOCH 1036
2024-02-02 11:35:03,040 [Epoch: 1036 Step: 00017600] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:      812 || Batch Translation Loss:   0.011964 => Txt Tokens per Sec:     2490 || Lr: 0.000100
2024-02-02 11:35:05,668 Epoch 1036: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 11:35:05,668 EPOCH 1037
2024-02-02 11:35:12,538 Epoch 1037: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 11:35:12,538 EPOCH 1038
2024-02-02 11:35:19,389 Epoch 1038: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 11:35:19,389 EPOCH 1039
2024-02-02 11:35:25,988 Epoch 1039: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 11:35:25,989 EPOCH 1040
2024-02-02 11:35:32,776 Epoch 1040: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 11:35:32,777 EPOCH 1041
2024-02-02 11:35:39,471 Epoch 1041: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 11:35:39,472 EPOCH 1042
2024-02-02 11:35:40,511 [Epoch: 1042 Step: 00017700] Batch Recognition Loss:   0.000157 => Gls Tokens per Sec:     1850 || Batch Translation Loss:   0.019777 => Txt Tokens per Sec:     5387 || Lr: 0.000100
2024-02-02 11:35:46,361 Epoch 1042: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 11:35:46,361 EPOCH 1043
2024-02-02 11:35:53,251 Epoch 1043: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 11:35:53,251 EPOCH 1044
2024-02-02 11:36:00,238 Epoch 1044: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 11:36:00,239 EPOCH 1045
2024-02-02 11:36:06,832 Epoch 1045: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 11:36:06,833 EPOCH 1046
2024-02-02 11:36:13,515 Epoch 1046: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 11:36:13,516 EPOCH 1047
2024-02-02 11:36:20,119 Epoch 1047: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 11:36:20,120 EPOCH 1048
2024-02-02 11:36:20,672 [Epoch: 1048 Step: 00017800] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:     1162 || Batch Translation Loss:   0.019729 => Txt Tokens per Sec:     4113 || Lr: 0.000100
2024-02-02 11:36:26,956 Epoch 1048: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 11:36:26,957 EPOCH 1049
2024-02-02 11:36:33,843 Epoch 1049: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 11:36:33,844 EPOCH 1050
2024-02-02 11:36:40,607 Epoch 1050: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 11:36:40,607 EPOCH 1051
2024-02-02 11:36:47,330 Epoch 1051: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 11:36:47,331 EPOCH 1052
2024-02-02 11:36:53,889 Epoch 1052: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 11:36:53,889 EPOCH 1053
2024-02-02 11:37:00,611 [Epoch: 1053 Step: 00017900] Batch Recognition Loss:   0.000121 => Gls Tokens per Sec:     1486 || Batch Translation Loss:   0.015697 => Txt Tokens per Sec:     4178 || Lr: 0.000100
2024-02-02 11:37:00,763 Epoch 1053: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 11:37:00,763 EPOCH 1054
2024-02-02 11:37:07,562 Epoch 1054: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 11:37:07,563 EPOCH 1055
2024-02-02 11:37:14,270 Epoch 1055: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 11:37:14,270 EPOCH 1056
2024-02-02 11:37:20,916 Epoch 1056: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 11:37:20,916 EPOCH 1057
2024-02-02 11:37:27,428 Epoch 1057: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 11:37:27,429 EPOCH 1058
2024-02-02 11:37:34,205 Epoch 1058: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 11:37:34,206 EPOCH 1059
2024-02-02 11:37:40,535 [Epoch: 1059 Step: 00018000] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     1377 || Batch Translation Loss:   0.031489 => Txt Tokens per Sec:     3930 || Lr: 0.000100
2024-02-02 11:38:12,226 Validation result at epoch 1059, step    18000: duration: 31.6891s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00020	Translation Loss: 93988.82812	PPL: 12154.64941
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.83	(BLEU-1: 11.47,	BLEU-2: 3.49,	BLEU-3: 1.55,	BLEU-4: 0.83)
	CHRF 17.33	ROUGE 9.79
2024-02-02 11:38:12,227 Logging Recognition and Translation Outputs
2024-02-02 11:38:12,227 ========================================================================================================================
2024-02-02 11:38:12,228 Logging Sequence: 155_119.00
2024-02-02 11:38:12,228 	Gloss Reference :	A B+C+D+E
2024-02-02 11:38:12,228 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 11:38:12,228 	Gloss Alignment :	         
2024-02-02 11:38:12,229 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 11:38:12,231 	Text Reference  :	a    report said   that the **** taliban wanted icc     to   replace the ***** ******* *** ***** afghan flag    with its own 
2024-02-02 11:38:12,231 	Text Hypothesis :	they would  decide if   the bcci to      play   against they lost    the match however icc would 8      wickets in   the game
2024-02-02 11:38:12,231 	Text Alignment  :	S    S      S      S        I    S       S      S       S    S           I     I       I   I     S      S       S    S   S   
2024-02-02 11:38:12,231 ========================================================================================================================
2024-02-02 11:38:12,232 Logging Sequence: 153_43.00
2024-02-02 11:38:12,232 	Gloss Reference :	A B+C+D+E
2024-02-02 11:38:12,232 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 11:38:12,232 	Gloss Alignment :	         
2024-02-02 11:38:12,233 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 11:38:12,233 	Text Reference  :	these   runs were all because of      hardik pandya and    virat  kohli
2024-02-02 11:38:12,233 	Text Hypothesis :	however they lost the match   leaving on     the    caught indian team 
2024-02-02 11:38:12,234 	Text Alignment  :	S       S    S    S   S       S       S      S      S      S      S    
2024-02-02 11:38:12,234 ========================================================================================================================
2024-02-02 11:38:12,234 Logging Sequence: 150_35.00
2024-02-02 11:38:12,234 	Gloss Reference :	A B+C+D+E
2024-02-02 11:38:12,234 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 11:38:12,234 	Gloss Alignment :	         
2024-02-02 11:38:12,234 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 11:38:12,235 	Text Reference  :	********* ** * *** ** ****** wow    india   football team   is   really strong
2024-02-02 11:38:12,235 	Text Hypothesis :	according to a lot of hardik pandya because he       looked like a      lot   
2024-02-02 11:38:12,235 	Text Alignment  :	I         I  I I   I  I      S      S       S        S      S    S      S     
2024-02-02 11:38:12,235 ========================================================================================================================
2024-02-02 11:38:12,236 Logging Sequence: 146_154.00
2024-02-02 11:38:12,236 	Gloss Reference :	A B+C+D+E
2024-02-02 11:38:12,236 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 11:38:12,236 	Gloss Alignment :	         
2024-02-02 11:38:12,236 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 11:38:12,237 	Text Reference  :	bwf said that testing protocols have been implemented to       ensure the      health  and safety of     all participants
2024-02-02 11:38:12,237 	Text Hypothesis :	*** **** **** ******* ********* **** and  was         consoled by     denmark' captain and ****** scored 95  runs        
2024-02-02 11:38:12,237 	Text Alignment  :	D   D    D    D       D         D    S    S           S        S      S        S           D      S      S   S           
2024-02-02 11:38:12,238 ========================================================================================================================
2024-02-02 11:38:12,238 Logging Sequence: 76_79.00
2024-02-02 11:38:12,238 	Gloss Reference :	A B+C+D+E
2024-02-02 11:38:12,238 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 11:38:12,238 	Gloss Alignment :	         
2024-02-02 11:38:12,238 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 11:38:12,239 	Text Reference  :	*** speaking to       ani csk       ceo   kasi viswanathan said
2024-02-02 11:38:12,239 	Text Hypothesis :	the t20      worldcup is  currently going in   dubai       oman
2024-02-02 11:38:12,239 	Text Alignment  :	I   S        S        S   S         S     S    S           S   
2024-02-02 11:38:12,239 ========================================================================================================================
2024-02-02 11:38:12,722 Epoch 1059: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 11:38:12,722 EPOCH 1060
2024-02-02 11:38:19,348 Epoch 1060: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-02 11:38:19,348 EPOCH 1061
2024-02-02 11:38:26,097 Epoch 1061: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.06 
2024-02-02 11:38:26,097 EPOCH 1062
2024-02-02 11:38:32,980 Epoch 1062: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.52 
2024-02-02 11:38:32,981 EPOCH 1063
2024-02-02 11:38:39,481 Epoch 1063: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.58 
2024-02-02 11:38:39,481 EPOCH 1064
2024-02-02 11:38:46,242 Epoch 1064: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.48 
2024-02-02 11:38:46,243 EPOCH 1065
2024-02-02 11:38:51,622 [Epoch: 1065 Step: 00018100] Batch Recognition Loss:   0.000775 => Gls Tokens per Sec:     1382 || Batch Translation Loss:   0.195314 => Txt Tokens per Sec:     3931 || Lr: 0.000100
2024-02-02 11:38:52,991 Epoch 1065: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.80 
2024-02-02 11:38:52,991 EPOCH 1066
2024-02-02 11:38:59,586 Epoch 1066: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.35 
2024-02-02 11:38:59,586 EPOCH 1067
2024-02-02 11:39:06,289 Epoch 1067: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.38 
2024-02-02 11:39:06,290 EPOCH 1068
2024-02-02 11:39:12,973 Epoch 1068: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.40 
2024-02-02 11:39:12,974 EPOCH 1069
2024-02-02 11:39:19,757 Epoch 1069: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-02 11:39:19,757 EPOCH 1070
2024-02-02 11:39:26,441 Epoch 1070: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.19 
2024-02-02 11:39:26,441 EPOCH 1071
2024-02-02 11:39:31,543 [Epoch: 1071 Step: 00018200] Batch Recognition Loss:   0.000297 => Gls Tokens per Sec:     1206 || Batch Translation Loss:   0.061755 => Txt Tokens per Sec:     3551 || Lr: 0.000100
2024-02-02 11:39:33,062 Epoch 1071: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.95 
2024-02-02 11:39:33,062 EPOCH 1072
2024-02-02 11:39:38,984 Epoch 1072: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.89 
2024-02-02 11:39:38,984 EPOCH 1073
2024-02-02 11:39:45,543 Epoch 1073: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-02 11:39:45,544 EPOCH 1074
2024-02-02 11:39:52,453 Epoch 1074: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-02 11:39:52,453 EPOCH 1075
2024-02-02 11:39:59,251 Epoch 1075: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-02 11:39:59,252 EPOCH 1076
2024-02-02 11:40:05,836 Epoch 1076: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-02 11:40:05,837 EPOCH 1077
2024-02-02 11:40:07,354 [Epoch: 1077 Step: 00018300] Batch Recognition Loss:   0.000252 => Gls Tokens per Sec:     3377 || Batch Translation Loss:   0.029483 => Txt Tokens per Sec:     8301 || Lr: 0.000100
2024-02-02 11:40:12,551 Epoch 1077: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.96 
2024-02-02 11:40:12,551 EPOCH 1078
2024-02-02 11:40:19,259 Epoch 1078: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.35 
2024-02-02 11:40:19,260 EPOCH 1079
2024-02-02 11:40:26,150 Epoch 1079: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-02 11:40:26,150 EPOCH 1080
2024-02-02 11:40:32,907 Epoch 1080: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.00 
2024-02-02 11:40:32,908 EPOCH 1081
2024-02-02 11:40:39,763 Epoch 1081: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.78 
2024-02-02 11:40:39,763 EPOCH 1082
2024-02-02 11:40:46,650 Epoch 1082: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-02 11:40:46,651 EPOCH 1083
2024-02-02 11:40:48,169 [Epoch: 1083 Step: 00018400] Batch Recognition Loss:   0.000166 => Gls Tokens per Sec:     2530 || Batch Translation Loss:   0.016983 => Txt Tokens per Sec:     7278 || Lr: 0.000100
2024-02-02 11:40:53,315 Epoch 1083: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-02 11:40:53,316 EPOCH 1084
2024-02-02 11:41:00,068 Epoch 1084: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-02 11:41:00,068 EPOCH 1085
2024-02-02 11:41:06,780 Epoch 1085: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 11:41:06,780 EPOCH 1086
2024-02-02 11:41:13,688 Epoch 1086: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 11:41:13,688 EPOCH 1087
2024-02-02 11:41:20,463 Epoch 1087: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 11:41:20,464 EPOCH 1088
2024-02-02 11:41:27,255 Epoch 1088: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 11:41:27,256 EPOCH 1089
2024-02-02 11:41:30,141 [Epoch: 1089 Step: 00018500] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:      801 || Batch Translation Loss:   0.015661 => Txt Tokens per Sec:     2052 || Lr: 0.000100
2024-02-02 11:41:34,113 Epoch 1089: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-02 11:41:34,114 EPOCH 1090
2024-02-02 11:41:40,855 Epoch 1090: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 11:41:40,856 EPOCH 1091
2024-02-02 11:41:47,625 Epoch 1091: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 11:41:47,626 EPOCH 1092
2024-02-02 11:41:54,637 Epoch 1092: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 11:41:54,637 EPOCH 1093
2024-02-02 11:42:01,291 Epoch 1093: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 11:42:01,292 EPOCH 1094
2024-02-02 11:42:07,950 Epoch 1094: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 11:42:07,950 EPOCH 1095
2024-02-02 11:42:08,389 [Epoch: 1095 Step: 00018600] Batch Recognition Loss:   0.000105 => Gls Tokens per Sec:     2922 || Batch Translation Loss:   0.015408 => Txt Tokens per Sec:     8037 || Lr: 0.000100
2024-02-02 11:42:14,383 Epoch 1095: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 11:42:14,383 EPOCH 1096
2024-02-02 11:42:21,197 Epoch 1096: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 11:42:21,197 EPOCH 1097
2024-02-02 11:42:27,919 Epoch 1097: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 11:42:27,920 EPOCH 1098
2024-02-02 11:42:34,487 Epoch 1098: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 11:42:34,487 EPOCH 1099
2024-02-02 11:42:41,138 Epoch 1099: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 11:42:41,138 EPOCH 1100
2024-02-02 11:42:47,633 [Epoch: 1100 Step: 00018700] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:     1637 || Batch Translation Loss:   0.017866 => Txt Tokens per Sec:     4544 || Lr: 0.000100
2024-02-02 11:42:47,634 Epoch 1100: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 11:42:47,634 EPOCH 1101
2024-02-02 11:42:54,404 Epoch 1101: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 11:42:54,404 EPOCH 1102
2024-02-02 11:43:01,251 Epoch 1102: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 11:43:01,251 EPOCH 1103
2024-02-02 11:43:08,158 Epoch 1103: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 11:43:08,159 EPOCH 1104
2024-02-02 11:43:14,976 Epoch 1104: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 11:43:14,977 EPOCH 1105
2024-02-02 11:43:21,865 Epoch 1105: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 11:43:21,866 EPOCH 1106
2024-02-02 11:43:25,919 [Epoch: 1106 Step: 00018800] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:     2369 || Batch Translation Loss:   0.009128 => Txt Tokens per Sec:     6564 || Lr: 0.000100
2024-02-02 11:43:28,468 Epoch 1106: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 11:43:28,469 EPOCH 1107
2024-02-02 11:43:35,314 Epoch 1107: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 11:43:35,315 EPOCH 1108
2024-02-02 11:43:42,104 Epoch 1108: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 11:43:42,105 EPOCH 1109
2024-02-02 11:43:48,891 Epoch 1109: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 11:43:48,892 EPOCH 1110
2024-02-02 11:43:55,596 Epoch 1110: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-02 11:43:55,597 EPOCH 1111
2024-02-02 11:44:02,001 Epoch 1111: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-02 11:44:02,002 EPOCH 1112
2024-02-02 11:44:05,656 [Epoch: 1112 Step: 00018900] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     2278 || Batch Translation Loss:   0.044264 => Txt Tokens per Sec:     6126 || Lr: 0.000100
2024-02-02 11:44:08,950 Epoch 1112: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.89 
2024-02-02 11:44:08,950 EPOCH 1113
2024-02-02 11:44:15,802 Epoch 1113: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.05 
2024-02-02 11:44:15,803 EPOCH 1114
2024-02-02 11:44:22,576 Epoch 1114: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.31 
2024-02-02 11:44:22,577 EPOCH 1115
2024-02-02 11:44:29,023 Epoch 1115: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.34 
2024-02-02 11:44:29,024 EPOCH 1116
2024-02-02 11:44:35,942 Epoch 1116: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.57 
2024-02-02 11:44:35,943 EPOCH 1117
2024-02-02 11:44:42,899 Epoch 1117: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.65 
2024-02-02 11:44:42,900 EPOCH 1118
2024-02-02 11:44:47,794 [Epoch: 1118 Step: 00019000] Batch Recognition Loss:   0.000369 => Gls Tokens per Sec:     1388 || Batch Translation Loss:   0.095604 => Txt Tokens per Sec:     3744 || Lr: 0.000100
2024-02-02 11:44:49,572 Epoch 1118: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.24 
2024-02-02 11:44:49,572 EPOCH 1119
2024-02-02 11:44:56,400 Epoch 1119: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.31 
2024-02-02 11:44:56,401 EPOCH 1120
2024-02-02 11:45:03,179 Epoch 1120: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.93 
2024-02-02 11:45:03,180 EPOCH 1121
2024-02-02 11:45:09,916 Epoch 1121: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.51 
2024-02-02 11:45:09,916 EPOCH 1122
2024-02-02 11:45:16,493 Epoch 1122: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-02 11:45:16,494 EPOCH 1123
2024-02-02 11:45:23,243 Epoch 1123: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-02 11:45:23,243 EPOCH 1124
2024-02-02 11:45:25,618 [Epoch: 1124 Step: 00019100] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:     2426 || Batch Translation Loss:   0.050309 => Txt Tokens per Sec:     6483 || Lr: 0.000100
2024-02-02 11:45:30,175 Epoch 1124: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.82 
2024-02-02 11:45:30,175 EPOCH 1125
2024-02-02 11:45:37,006 Epoch 1125: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.91 
2024-02-02 11:45:37,007 EPOCH 1126
2024-02-02 11:45:43,888 Epoch 1126: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-02 11:45:43,889 EPOCH 1127
2024-02-02 11:45:50,129 Epoch 1127: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-02 11:45:50,130 EPOCH 1128
2024-02-02 11:45:57,019 Epoch 1128: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.85 
2024-02-02 11:45:57,020 EPOCH 1129
2024-02-02 11:46:03,780 Epoch 1129: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.78 
2024-02-02 11:46:03,780 EPOCH 1130
2024-02-02 11:46:05,604 [Epoch: 1130 Step: 00019200] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     2459 || Batch Translation Loss:   0.012674 => Txt Tokens per Sec:     6424 || Lr: 0.000100
2024-02-02 11:46:10,616 Epoch 1130: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.10 
2024-02-02 11:46:10,616 EPOCH 1131
2024-02-02 11:46:17,469 Epoch 1131: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-02 11:46:17,469 EPOCH 1132
2024-02-02 11:46:23,797 Epoch 1132: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-02 11:46:23,797 EPOCH 1133
2024-02-02 11:46:30,723 Epoch 1133: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-02 11:46:30,724 EPOCH 1134
2024-02-02 11:46:37,551 Epoch 1134: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 11:46:37,551 EPOCH 1135
2024-02-02 11:46:44,375 Epoch 1135: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 11:46:44,375 EPOCH 1136
2024-02-02 11:46:46,006 [Epoch: 1136 Step: 00019300] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:     1963 || Batch Translation Loss:   0.025726 => Txt Tokens per Sec:     5738 || Lr: 0.000100
2024-02-02 11:46:51,278 Epoch 1136: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 11:46:51,279 EPOCH 1137
2024-02-02 11:46:58,327 Epoch 1137: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-02 11:46:58,327 EPOCH 1138
2024-02-02 11:47:05,143 Epoch 1138: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 11:47:05,143 EPOCH 1139
2024-02-02 11:47:11,988 Epoch 1139: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-02 11:47:11,988 EPOCH 1140
2024-02-02 11:47:18,409 Epoch 1140: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 11:47:18,409 EPOCH 1141
2024-02-02 11:47:25,378 Epoch 1141: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 11:47:25,378 EPOCH 1142
2024-02-02 11:47:26,141 [Epoch: 1142 Step: 00019400] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     2520 || Batch Translation Loss:   0.017871 => Txt Tokens per Sec:     6268 || Lr: 0.000100
2024-02-02 11:47:32,346 Epoch 1142: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 11:47:32,346 EPOCH 1143
2024-02-02 11:47:39,228 Epoch 1143: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 11:47:39,228 EPOCH 1144
2024-02-02 11:47:45,946 Epoch 1144: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 11:47:45,946 EPOCH 1145
2024-02-02 11:47:52,534 Epoch 1145: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 11:47:52,534 EPOCH 1146
2024-02-02 11:47:59,375 Epoch 1146: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-02 11:47:59,375 EPOCH 1147
2024-02-02 11:48:06,240 Epoch 1147: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 11:48:06,240 EPOCH 1148
2024-02-02 11:48:06,384 [Epoch: 1148 Step: 00019500] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:     4475 || Batch Translation Loss:   0.015637 => Txt Tokens per Sec:    11503 || Lr: 0.000100
2024-02-02 11:48:13,140 Epoch 1148: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-02 11:48:13,141 EPOCH 1149
2024-02-02 11:48:19,976 Epoch 1149: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.76 
2024-02-02 11:48:19,977 EPOCH 1150
2024-02-02 11:48:26,563 Epoch 1150: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.80 
2024-02-02 11:48:26,564 EPOCH 1151
2024-02-02 11:48:33,375 Epoch 1151: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 11:48:33,376 EPOCH 1152
2024-02-02 11:48:40,072 Epoch 1152: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-02 11:48:40,072 EPOCH 1153
2024-02-02 11:48:46,846 [Epoch: 1153 Step: 00019600] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     1475 || Batch Translation Loss:   0.084457 => Txt Tokens per Sec:     4212 || Lr: 0.000100
2024-02-02 11:48:46,952 Epoch 1153: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.77 
2024-02-02 11:48:46,952 EPOCH 1154
2024-02-02 11:48:53,843 Epoch 1154: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.23 
2024-02-02 11:48:53,844 EPOCH 1155
2024-02-02 11:49:00,655 Epoch 1155: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.44 
2024-02-02 11:49:00,656 EPOCH 1156
2024-02-02 11:49:07,444 Epoch 1156: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.11 
2024-02-02 11:49:07,445 EPOCH 1157
2024-02-02 11:49:14,158 Epoch 1157: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.93 
2024-02-02 11:49:14,158 EPOCH 1158
2024-02-02 11:49:20,912 Epoch 1158: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.01 
2024-02-02 11:49:20,913 EPOCH 1159
2024-02-02 11:49:26,993 [Epoch: 1159 Step: 00019700] Batch Recognition Loss:   0.000254 => Gls Tokens per Sec:     1433 || Batch Translation Loss:   0.120590 => Txt Tokens per Sec:     4005 || Lr: 0.000100
2024-02-02 11:49:27,828 Epoch 1159: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.30 
2024-02-02 11:49:27,828 EPOCH 1160
2024-02-02 11:49:34,791 Epoch 1160: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.28 
2024-02-02 11:49:34,792 EPOCH 1161
2024-02-02 11:49:41,513 Epoch 1161: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.36 
2024-02-02 11:49:41,514 EPOCH 1162
2024-02-02 11:49:48,338 Epoch 1162: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.31 
2024-02-02 11:49:48,339 EPOCH 1163
2024-02-02 11:49:55,234 Epoch 1163: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.05 
2024-02-02 11:49:55,234 EPOCH 1164
2024-02-02 11:50:01,829 Epoch 1164: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.77 
2024-02-02 11:50:01,829 EPOCH 1165
2024-02-02 11:50:07,803 [Epoch: 1165 Step: 00019800] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:     1244 || Batch Translation Loss:   0.034888 => Txt Tokens per Sec:     3623 || Lr: 0.000100
2024-02-02 11:50:08,703 Epoch 1165: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-02 11:50:08,703 EPOCH 1166
2024-02-02 11:50:15,482 Epoch 1166: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.76 
2024-02-02 11:50:15,482 EPOCH 1167
2024-02-02 11:50:22,153 Epoch 1167: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.03 
2024-02-02 11:50:22,154 EPOCH 1168
2024-02-02 11:50:29,013 Epoch 1168: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.99 
2024-02-02 11:50:29,013 EPOCH 1169
2024-02-02 11:50:35,803 Epoch 1169: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.05 
2024-02-02 11:50:35,804 EPOCH 1170
2024-02-02 11:50:42,606 Epoch 1170: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.19 
2024-02-02 11:50:42,606 EPOCH 1171
2024-02-02 11:50:45,310 [Epoch: 1171 Step: 00019900] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:     2368 || Batch Translation Loss:   0.036719 => Txt Tokens per Sec:     6595 || Lr: 0.000100
2024-02-02 11:50:49,122 Epoch 1171: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.70 
2024-02-02 11:50:49,122 EPOCH 1172
2024-02-02 11:50:56,013 Epoch 1172: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.71 
2024-02-02 11:50:56,013 EPOCH 1173
2024-02-02 11:51:02,444 Epoch 1173: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-02 11:51:02,445 EPOCH 1174
2024-02-02 11:51:09,331 Epoch 1174: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-02 11:51:09,332 EPOCH 1175
2024-02-02 11:51:16,317 Epoch 1175: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-02 11:51:16,317 EPOCH 1176
2024-02-02 11:51:23,443 Epoch 1176: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-02 11:51:23,444 EPOCH 1177
2024-02-02 11:51:25,648 [Epoch: 1177 Step: 00020000] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     2324 || Batch Translation Loss:   0.099952 => Txt Tokens per Sec:     5978 || Lr: 0.000100
2024-02-02 11:51:58,151 Validation result at epoch 1177, step    20000: duration: 32.5023s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00036	Translation Loss: 94831.17188	PPL: 13223.62695
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.64	(BLEU-1: 11.62,	BLEU-2: 3.51,	BLEU-3: 1.33,	BLEU-4: 0.64)
	CHRF 17.22	ROUGE 9.72
2024-02-02 11:51:58,152 Logging Recognition and Translation Outputs
2024-02-02 11:51:58,153 ========================================================================================================================
2024-02-02 11:51:58,153 Logging Sequence: 174_121.00
2024-02-02 11:51:58,153 	Gloss Reference :	A B+C+D+E
2024-02-02 11:51:58,153 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 11:51:58,153 	Gloss Alignment :	         
2024-02-02 11:51:58,154 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 11:51:58,155 	Text Reference  :	*** ****** *** ******** * **** *** ********* ** ****** there was  a  strong   competition and   a     difficult auction for the    5     franchise owners
2024-02-02 11:51:58,156 	Text Hypothesis :	the couple are enjoying a huge fan following in sports there were an argument between     their world cup       match   and forget about rs        250   
2024-02-02 11:51:58,156 	Text Alignment  :	I   I      I   I        I I    I   I         I  I            S    S  S        S           S     S     S         S       S   S      S     S         S     
2024-02-02 11:51:58,156 ========================================================================================================================
2024-02-02 11:51:58,156 Logging Sequence: 170_24.00
2024-02-02 11:51:58,156 	Gloss Reference :	A B+C+D+E
2024-02-02 11:51:58,157 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 11:51:58,157 	Gloss Alignment :	         
2024-02-02 11:51:58,157 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 11:51:58,157 	Text Reference  :	let me tell you about it
2024-02-02 11:51:58,157 	Text Hypothesis :	let me tell you about it
2024-02-02 11:51:58,157 	Text Alignment  :	                        
2024-02-02 11:51:58,158 ========================================================================================================================
2024-02-02 11:51:58,158 Logging Sequence: 73_79.00
2024-02-02 11:51:58,158 	Gloss Reference :	A B+C+D+E
2024-02-02 11:51:58,158 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 11:51:58,158 	Gloss Alignment :	         
2024-02-02 11:51:58,159 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 11:51:58,160 	Text Reference  :	raina resturant has food from   the rich spices of north india      to the aromatic curries of south     india
2024-02-02 11:51:58,160 	Text Hypothesis :	when  it        has **** opened the **** ****** ** ***** restaurant in the ******** ******* ** beautiful city 
2024-02-02 11:51:58,160 	Text Alignment  :	S     S             D    S          D    D      D  D     S          S      D        D       D  S         S    
2024-02-02 11:51:58,160 ========================================================================================================================
2024-02-02 11:51:58,160 Logging Sequence: 140_2.00
2024-02-02 11:51:58,160 	Gloss Reference :	A B+C+D+E
2024-02-02 11:51:58,161 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 11:51:58,161 	Gloss Alignment :	         
2024-02-02 11:51:58,161 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 11:51:58,162 	Text Reference  :	indian batsman-wicket keeper rishabh pant has    outstanding skills in     cricket
2024-02-02 11:51:58,162 	Text Hypothesis :	****** ************** for    india   had  become viral       on     social media  
2024-02-02 11:51:58,162 	Text Alignment  :	D      D              S      S       S    S      S           S      S      S      
2024-02-02 11:51:58,162 ========================================================================================================================
2024-02-02 11:51:58,162 Logging Sequence: 81_470.00
2024-02-02 11:51:58,162 	Gloss Reference :	A B+C+D+E
2024-02-02 11:51:58,162 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 11:51:58,163 	Gloss Alignment :	         
2024-02-02 11:51:58,163 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 11:51:58,164 	Text Reference  :	********** or you don't  know if   you    do let  us    know      in the comments
2024-02-02 11:51:58,164 	Text Hypothesis :	arbitrator is a   person who  will listen to each other countries in the team    
2024-02-02 11:51:58,164 	Text Alignment  :	I          S  S   S      S    S    S      S  S    S     S                S       
2024-02-02 11:51:58,164 ========================================================================================================================
2024-02-02 11:52:03,010 Epoch 1177: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-02 11:52:03,011 EPOCH 1178
2024-02-02 11:52:09,869 Epoch 1178: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-02 11:52:09,870 EPOCH 1179
2024-02-02 11:52:16,479 Epoch 1179: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.87 
2024-02-02 11:52:16,479 EPOCH 1180
2024-02-02 11:52:22,470 Epoch 1180: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.92 
2024-02-02 11:52:22,470 EPOCH 1181
2024-02-02 11:52:28,442 Epoch 1181: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.10 
2024-02-02 11:52:28,443 EPOCH 1182
2024-02-02 11:52:35,199 Epoch 1182: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.38 
2024-02-02 11:52:35,199 EPOCH 1183
2024-02-02 11:52:37,428 [Epoch: 1183 Step: 00020100] Batch Recognition Loss:   0.000292 => Gls Tokens per Sec:     1724 || Batch Translation Loss:   0.045048 => Txt Tokens per Sec:     5434 || Lr: 0.000100
2024-02-02 11:52:42,098 Epoch 1183: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.79 
2024-02-02 11:52:42,099 EPOCH 1184
2024-02-02 11:52:48,777 Epoch 1184: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.77 
2024-02-02 11:52:48,777 EPOCH 1185
2024-02-02 11:52:55,356 Epoch 1185: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.94 
2024-02-02 11:52:55,356 EPOCH 1186
2024-02-02 11:53:02,314 Epoch 1186: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-02 11:53:02,314 EPOCH 1187
2024-02-02 11:53:09,085 Epoch 1187: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 11:53:09,086 EPOCH 1188
2024-02-02 11:53:15,310 Epoch 1188: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 11:53:15,310 EPOCH 1189
2024-02-02 11:53:18,372 [Epoch: 1189 Step: 00020200] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:      755 || Batch Translation Loss:   0.017471 => Txt Tokens per Sec:     2000 || Lr: 0.000100
2024-02-02 11:53:22,398 Epoch 1189: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 11:53:22,399 EPOCH 1190
2024-02-02 11:53:29,231 Epoch 1190: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 11:53:29,232 EPOCH 1191
2024-02-02 11:53:35,907 Epoch 1191: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 11:53:35,908 EPOCH 1192
2024-02-02 11:53:42,494 Epoch 1192: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 11:53:42,495 EPOCH 1193
2024-02-02 11:53:49,159 Epoch 1193: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 11:53:49,160 EPOCH 1194
2024-02-02 11:53:55,888 Epoch 1194: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 11:53:55,889 EPOCH 1195
2024-02-02 11:53:56,771 [Epoch: 1195 Step: 00020300] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     1451 || Batch Translation Loss:   0.028070 => Txt Tokens per Sec:     4273 || Lr: 0.000100
2024-02-02 11:54:02,787 Epoch 1195: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 11:54:02,787 EPOCH 1196
2024-02-02 11:54:09,606 Epoch 1196: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 11:54:09,607 EPOCH 1197
2024-02-02 11:54:16,357 Epoch 1197: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 11:54:16,358 EPOCH 1198
2024-02-02 11:54:23,139 Epoch 1198: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 11:54:23,140 EPOCH 1199
2024-02-02 11:54:29,805 Epoch 1199: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 11:54:29,805 EPOCH 1200
2024-02-02 11:54:36,147 [Epoch: 1200 Step: 00020400] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     1677 || Batch Translation Loss:   0.021654 => Txt Tokens per Sec:     4654 || Lr: 0.000100
2024-02-02 11:54:36,148 Epoch 1200: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 11:54:36,148 EPOCH 1201
2024-02-02 11:54:42,959 Epoch 1201: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-02 11:54:42,960 EPOCH 1202
2024-02-02 11:54:50,128 Epoch 1202: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-02 11:54:50,129 EPOCH 1203
2024-02-02 11:54:56,851 Epoch 1203: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 11:54:56,851 EPOCH 1204
2024-02-02 11:55:03,338 Epoch 1204: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 11:55:03,338 EPOCH 1205
2024-02-02 11:55:10,146 Epoch 1205: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.30 
2024-02-02 11:55:10,147 EPOCH 1206
2024-02-02 11:55:16,327 [Epoch: 1206 Step: 00020500] Batch Recognition Loss:   0.000221 => Gls Tokens per Sec:     1513 || Batch Translation Loss:   0.020837 => Txt Tokens per Sec:     4230 || Lr: 0.000100
2024-02-02 11:55:16,790 Epoch 1206: Total Training Recognition Loss 0.00  Total Training Translation Loss 3.02 
2024-02-02 11:55:16,791 EPOCH 1207
2024-02-02 11:55:23,874 Epoch 1207: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.03 
2024-02-02 11:55:23,875 EPOCH 1208
2024-02-02 11:55:30,900 Epoch 1208: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.40 
2024-02-02 11:55:30,901 EPOCH 1209
2024-02-02 11:55:37,692 Epoch 1209: Total Training Recognition Loss 0.02  Total Training Translation Loss 7.57 
2024-02-02 11:55:37,692 EPOCH 1210
2024-02-02 11:55:44,116 Epoch 1210: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.53 
2024-02-02 11:55:44,117 EPOCH 1211
2024-02-02 11:55:50,974 Epoch 1211: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.36 
2024-02-02 11:55:50,974 EPOCH 1212
2024-02-02 11:55:56,424 [Epoch: 1212 Step: 00020600] Batch Recognition Loss:   0.000970 => Gls Tokens per Sec:     1481 || Batch Translation Loss:   0.082412 => Txt Tokens per Sec:     4123 || Lr: 0.000100
2024-02-02 11:55:57,695 Epoch 1212: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.90 
2024-02-02 11:55:57,695 EPOCH 1213
2024-02-02 11:56:04,654 Epoch 1213: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-02 11:56:04,654 EPOCH 1214
2024-02-02 11:56:11,497 Epoch 1214: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-02 11:56:11,498 EPOCH 1215
2024-02-02 11:56:18,343 Epoch 1215: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-02 11:56:18,343 EPOCH 1216
2024-02-02 11:56:25,100 Epoch 1216: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 11:56:25,101 EPOCH 1217
2024-02-02 11:56:31,664 Epoch 1217: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 11:56:31,665 EPOCH 1218
2024-02-02 11:56:36,401 [Epoch: 1218 Step: 00020700] Batch Recognition Loss:   0.000327 => Gls Tokens per Sec:     1434 || Batch Translation Loss:   0.029728 => Txt Tokens per Sec:     3781 || Lr: 0.000100
2024-02-02 11:56:38,405 Epoch 1218: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 11:56:38,405 EPOCH 1219
2024-02-02 11:56:45,308 Epoch 1219: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 11:56:45,308 EPOCH 1220
2024-02-02 11:56:52,059 Epoch 1220: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 11:56:52,059 EPOCH 1221
2024-02-02 11:56:58,773 Epoch 1221: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 11:56:58,773 EPOCH 1222
2024-02-02 11:57:05,658 Epoch 1222: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 11:57:05,658 EPOCH 1223
2024-02-02 11:57:12,179 Epoch 1223: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 11:57:12,180 EPOCH 1224
2024-02-02 11:57:14,614 [Epoch: 1224 Step: 00020800] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:     2369 || Batch Translation Loss:   0.016819 => Txt Tokens per Sec:     6212 || Lr: 0.000100
2024-02-02 11:57:18,998 Epoch 1224: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 11:57:18,998 EPOCH 1225
2024-02-02 11:57:25,770 Epoch 1225: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 11:57:25,771 EPOCH 1226
2024-02-02 11:57:32,458 Epoch 1226: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 11:57:32,458 EPOCH 1227
2024-02-02 11:57:39,306 Epoch 1227: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 11:57:39,307 EPOCH 1228
2024-02-02 11:57:46,074 Epoch 1228: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 11:57:46,074 EPOCH 1229
2024-02-02 11:57:52,782 Epoch 1229: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 11:57:52,783 EPOCH 1230
2024-02-02 11:57:54,938 [Epoch: 1230 Step: 00020900] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:     2080 || Batch Translation Loss:   0.013931 => Txt Tokens per Sec:     5769 || Lr: 0.000100
2024-02-02 11:57:59,846 Epoch 1230: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 11:57:59,846 EPOCH 1231
2024-02-02 11:58:06,738 Epoch 1231: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 11:58:06,739 EPOCH 1232
2024-02-02 11:58:13,468 Epoch 1232: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 11:58:13,469 EPOCH 1233
2024-02-02 11:58:19,958 Epoch 1233: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 11:58:19,959 EPOCH 1234
2024-02-02 11:58:26,575 Epoch 1234: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 11:58:26,576 EPOCH 1235
2024-02-02 11:58:33,453 Epoch 1235: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 11:58:33,453 EPOCH 1236
2024-02-02 11:58:34,334 [Epoch: 1236 Step: 00021000] Batch Recognition Loss:   0.000117 => Gls Tokens per Sec:     3635 || Batch Translation Loss:   0.013462 => Txt Tokens per Sec:     8480 || Lr: 0.000100
2024-02-02 11:58:40,261 Epoch 1236: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 11:58:40,262 EPOCH 1237
2024-02-02 11:58:46,969 Epoch 1237: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 11:58:46,969 EPOCH 1238
2024-02-02 11:58:53,529 Epoch 1238: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 11:58:53,530 EPOCH 1239
2024-02-02 11:59:00,353 Epoch 1239: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 11:59:00,353 EPOCH 1240
2024-02-02 11:59:07,202 Epoch 1240: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 11:59:07,202 EPOCH 1241
2024-02-02 11:59:14,003 Epoch 1241: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 11:59:14,004 EPOCH 1242
2024-02-02 11:59:14,490 [Epoch: 1242 Step: 00021100] Batch Recognition Loss:   0.000166 => Gls Tokens per Sec:     3963 || Batch Translation Loss:   0.010645 => Txt Tokens per Sec:     9670 || Lr: 0.000100
2024-02-02 11:59:20,360 Epoch 1242: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 11:59:20,360 EPOCH 1243
2024-02-02 11:59:27,418 Epoch 1243: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 11:59:27,419 EPOCH 1244
2024-02-02 11:59:34,089 Epoch 1244: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 11:59:34,090 EPOCH 1245
2024-02-02 11:59:40,924 Epoch 1245: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-02 11:59:40,924 EPOCH 1246
2024-02-02 11:59:47,838 Epoch 1246: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-02 11:59:47,839 EPOCH 1247
2024-02-02 11:59:54,722 Epoch 1247: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.71 
2024-02-02 11:59:54,723 EPOCH 1248
2024-02-02 11:59:57,049 [Epoch: 1248 Step: 00021200] Batch Recognition Loss:   0.000296 => Gls Tokens per Sec:      168 || Batch Translation Loss:   0.033306 => Txt Tokens per Sec:      600 || Lr: 0.000100
2024-02-02 12:00:01,345 Epoch 1248: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.87 
2024-02-02 12:00:01,345 EPOCH 1249
2024-02-02 12:00:08,103 Epoch 1249: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 12:00:08,104 EPOCH 1250
2024-02-02 12:00:14,935 Epoch 1250: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-02 12:00:14,936 EPOCH 1251
2024-02-02 12:00:21,687 Epoch 1251: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-02 12:00:21,688 EPOCH 1252
2024-02-02 12:00:28,250 Epoch 1252: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-02 12:00:28,251 EPOCH 1253
2024-02-02 12:00:34,821 [Epoch: 1253 Step: 00021300] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     1521 || Batch Translation Loss:   0.051750 => Txt Tokens per Sec:     4216 || Lr: 0.000100
2024-02-02 12:00:35,087 Epoch 1253: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 12:00:35,087 EPOCH 1254
2024-02-02 12:00:41,795 Epoch 1254: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 12:00:41,795 EPOCH 1255
2024-02-02 12:00:48,643 Epoch 1255: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-02 12:00:48,644 EPOCH 1256
2024-02-02 12:00:55,499 Epoch 1256: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-02 12:00:55,500 EPOCH 1257
2024-02-02 12:01:02,513 Epoch 1257: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 12:01:02,514 EPOCH 1258
2024-02-02 12:01:09,079 Epoch 1258: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 12:01:09,080 EPOCH 1259
2024-02-02 12:01:14,873 [Epoch: 1259 Step: 00021400] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     1504 || Batch Translation Loss:   0.024268 => Txt Tokens per Sec:     4132 || Lr: 0.000100
2024-02-02 12:01:15,850 Epoch 1259: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 12:01:15,851 EPOCH 1260
2024-02-02 12:01:22,587 Epoch 1260: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 12:01:22,587 EPOCH 1261
2024-02-02 12:01:29,508 Epoch 1261: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-02 12:01:29,508 EPOCH 1262
2024-02-02 12:01:36,520 Epoch 1262: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 12:01:36,521 EPOCH 1263
2024-02-02 12:01:43,219 Epoch 1263: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-02 12:01:43,220 EPOCH 1264
2024-02-02 12:01:49,993 Epoch 1264: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 12:01:49,993 EPOCH 1265
2024-02-02 12:01:55,191 [Epoch: 1265 Step: 00021500] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:     1430 || Batch Translation Loss:   0.021110 => Txt Tokens per Sec:     3947 || Lr: 0.000100
2024-02-02 12:01:56,662 Epoch 1265: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.80 
2024-02-02 12:01:56,663 EPOCH 1266
2024-02-02 12:02:03,566 Epoch 1266: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.99 
2024-02-02 12:02:03,567 EPOCH 1267
2024-02-02 12:02:10,554 Epoch 1267: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.66 
2024-02-02 12:02:10,555 EPOCH 1268
2024-02-02 12:02:17,295 Epoch 1268: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.64 
2024-02-02 12:02:17,296 EPOCH 1269
2024-02-02 12:02:24,105 Epoch 1269: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.63 
2024-02-02 12:02:24,106 EPOCH 1270
2024-02-02 12:02:30,858 Epoch 1270: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-02 12:02:30,859 EPOCH 1271
2024-02-02 12:02:33,099 [Epoch: 1271 Step: 00021600] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     2858 || Batch Translation Loss:   0.034720 => Txt Tokens per Sec:     7578 || Lr: 0.000100
2024-02-02 12:02:37,471 Epoch 1271: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.02 
2024-02-02 12:02:37,472 EPOCH 1272
2024-02-02 12:02:44,305 Epoch 1272: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.90 
2024-02-02 12:02:44,305 EPOCH 1273
2024-02-02 12:02:51,156 Epoch 1273: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.81 
2024-02-02 12:02:51,156 EPOCH 1274
2024-02-02 12:02:57,802 Epoch 1274: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-02 12:02:57,803 EPOCH 1275
2024-02-02 12:03:04,339 Epoch 1275: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-02 12:03:04,339 EPOCH 1276
2024-02-02 12:03:11,034 Epoch 1276: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-02 12:03:11,034 EPOCH 1277
2024-02-02 12:03:15,326 [Epoch: 1277 Step: 00021700] Batch Recognition Loss:   0.000232 => Gls Tokens per Sec:     1135 || Batch Translation Loss:   0.022669 => Txt Tokens per Sec:     3188 || Lr: 0.000100
2024-02-02 12:03:17,803 Epoch 1277: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 12:03:17,804 EPOCH 1278
2024-02-02 12:03:24,685 Epoch 1278: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 12:03:24,686 EPOCH 1279
2024-02-02 12:03:31,551 Epoch 1279: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-02 12:03:31,552 EPOCH 1280
2024-02-02 12:03:38,187 Epoch 1280: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-02 12:03:38,188 EPOCH 1281
2024-02-02 12:03:44,950 Epoch 1281: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 12:03:44,950 EPOCH 1282
2024-02-02 12:03:51,427 Epoch 1282: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 12:03:51,427 EPOCH 1283
2024-02-02 12:03:55,470 [Epoch: 1283 Step: 00021800] Batch Recognition Loss:   0.000246 => Gls Tokens per Sec:      888 || Batch Translation Loss:   0.015049 => Txt Tokens per Sec:     2584 || Lr: 0.000100
2024-02-02 12:03:58,396 Epoch 1283: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 12:03:58,396 EPOCH 1284
2024-02-02 12:04:05,187 Epoch 1284: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 12:04:05,187 EPOCH 1285
2024-02-02 12:04:12,032 Epoch 1285: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 12:04:12,033 EPOCH 1286
2024-02-02 12:04:18,796 Epoch 1286: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 12:04:18,797 EPOCH 1287
2024-02-02 12:04:25,966 Epoch 1287: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 12:04:25,967 EPOCH 1288
2024-02-02 12:04:32,736 Epoch 1288: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 12:04:32,736 EPOCH 1289
2024-02-02 12:04:34,064 [Epoch: 1289 Step: 00021900] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:     1931 || Batch Translation Loss:   0.019032 => Txt Tokens per Sec:     5200 || Lr: 0.000100
2024-02-02 12:04:39,666 Epoch 1289: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 12:04:39,666 EPOCH 1290
2024-02-02 12:04:46,378 Epoch 1290: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-02 12:04:46,379 EPOCH 1291
2024-02-02 12:04:53,026 Epoch 1291: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-02 12:04:53,026 EPOCH 1292
2024-02-02 12:04:59,687 Epoch 1292: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 12:04:59,688 EPOCH 1293
2024-02-02 12:05:06,350 Epoch 1293: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.78 
2024-02-02 12:05:06,350 EPOCH 1294
2024-02-02 12:05:13,274 Epoch 1294: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-02 12:05:13,274 EPOCH 1295
2024-02-02 12:05:15,854 [Epoch: 1295 Step: 00022000] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:      399 || Batch Translation Loss:   0.023812 => Txt Tokens per Sec:     1199 || Lr: 0.000100
2024-02-02 12:05:48,126 Validation result at epoch 1295, step    22000: duration: 32.2705s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00034	Translation Loss: 95237.64062	PPL: 13772.58789
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.57	(BLEU-1: 12.02,	BLEU-2: 3.64,	BLEU-3: 1.36,	BLEU-4: 0.57)
	CHRF 17.61	ROUGE 9.95
2024-02-02 12:05:48,127 Logging Recognition and Translation Outputs
2024-02-02 12:05:48,127 ========================================================================================================================
2024-02-02 12:05:48,127 Logging Sequence: 146_56.00
2024-02-02 12:05:48,128 	Gloss Reference :	A B+C+D+E
2024-02-02 12:05:48,128 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 12:05:48,128 	Gloss Alignment :	         
2024-02-02 12:05:48,128 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 12:05:48,130 	Text Reference  :	when the players go back to the hotel as per rules all          of    them have to undergo rtpcr test  for covid-19 ** ** ******* everyday
2024-02-02 12:05:48,130 	Text Hypothesis :	**** *** ******* ** **** ** the ***** ** *** team  representing india is   set  to ******* ***** leave for covid-19 on 18 january 2022    
2024-02-02 12:05:48,130 	Text Alignment  :	D    D   D       D  D    D      D     D  D   S     S            S     S    S       D       D     S                  I  I  I       S       
2024-02-02 12:05:48,130 ========================================================================================================================
2024-02-02 12:05:48,130 Logging Sequence: 118_338.00
2024-02-02 12:05:48,131 	Gloss Reference :	A B+C+D+E
2024-02-02 12:05:48,131 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 12:05:48,131 	Gloss Alignment :	         
2024-02-02 12:05:48,131 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 12:05:48,132 	Text Reference  :	this is why *** ***** **** *** ****** ******* ** even  messi wore it     
2024-02-02 12:05:48,132 	Text Hypothesis :	**** so why are aware that the entire support as there was   no   problem
2024-02-02 12:05:48,132 	Text Alignment  :	D    S      I   I     I    I   I      I       I  S     S     S    S      
2024-02-02 12:05:48,132 ========================================================================================================================
2024-02-02 12:05:48,132 Logging Sequence: 66_61.00
2024-02-02 12:05:48,132 	Gloss Reference :	A B+C+D+E
2024-02-02 12:05:48,133 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 12:05:48,133 	Gloss Alignment :	         
2024-02-02 12:05:48,133 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 12:05:48,134 	Text Reference  :	* ** **** instead of  returning back to      his homeland because of *** *** ***** ** **** *** his    injury
2024-02-02 12:05:48,134 	Text Hypothesis :	i am sure you     all must      have enjoyed the current  season  of ipl and would be from the afghan team  
2024-02-02 12:05:48,134 	Text Alignment  :	I I  I    S       S   S         S    S       S   S        S          I   I   I     I  I    I   S      S     
2024-02-02 12:05:48,135 ========================================================================================================================
2024-02-02 12:05:48,135 Logging Sequence: 81_278.00
2024-02-02 12:05:48,135 	Gloss Reference :	A B+C+D+E
2024-02-02 12:05:48,135 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 12:05:48,135 	Gloss Alignment :	         
2024-02-02 12:05:48,135 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 12:05:48,138 	Text Reference  :	of this amrapali group paid rs 3570 crore the remaining rs    652  crore was   paid by      amrapali sapphire developers a  subsidiary of   amrapali group
2024-02-02 12:05:48,138 	Text Hypothesis :	** **** ******** ***** **** ** now  with  the ********* first time a     court then lifting the      supreme  court      of his        wife sakshi   dhoni
2024-02-02 12:05:48,138 	Text Alignment  :	D  D    D        D     D    D  S    S         D         S     S    S     S     S    S       S        S        S          S  S          S    S        S    
2024-02-02 12:05:48,138 ========================================================================================================================
2024-02-02 12:05:48,138 Logging Sequence: 162_125.00
2024-02-02 12:05:48,138 	Gloss Reference :	A B+C+D+E
2024-02-02 12:05:48,138 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 12:05:48,138 	Gloss Alignment :	         
2024-02-02 12:05:48,139 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 12:05:48,140 	Text Reference  :	**** **** ********* **** ***** ** in response  to this    kohli received many hate comments on   social        media
2024-02-02 12:05:48,140 	Text Hypothesis :	fans have dedicated this match as a  pakistani to chasing from  the      team was  very     hard unfortunately they 
2024-02-02 12:05:48,140 	Text Alignment  :	I    I    I         I    I     I  S  S            S       S     S        S    S    S        S    S             S    
2024-02-02 12:05:48,140 ========================================================================================================================
2024-02-02 12:05:52,482 Epoch 1295: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-02 12:05:52,483 EPOCH 1296
2024-02-02 12:05:59,322 Epoch 1296: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.70 
2024-02-02 12:05:59,323 EPOCH 1297
2024-02-02 12:06:05,989 Epoch 1297: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-02 12:06:05,989 EPOCH 1298
2024-02-02 12:06:12,800 Epoch 1298: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 12:06:12,801 EPOCH 1299
2024-02-02 12:06:19,231 Epoch 1299: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 12:06:19,231 EPOCH 1300
2024-02-02 12:06:25,203 [Epoch: 1300 Step: 00022100] Batch Recognition Loss:   0.000115 => Gls Tokens per Sec:     1780 || Batch Translation Loss:   0.011328 => Txt Tokens per Sec:     4942 || Lr: 0.000100
2024-02-02 12:06:25,203 Epoch 1300: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 12:06:25,204 EPOCH 1301
2024-02-02 12:06:31,915 Epoch 1301: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 12:06:31,916 EPOCH 1302
2024-02-02 12:06:38,879 Epoch 1302: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 12:06:38,880 EPOCH 1303
2024-02-02 12:06:45,711 Epoch 1303: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-02 12:06:45,712 EPOCH 1304
2024-02-02 12:06:52,528 Epoch 1304: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-02 12:06:52,529 EPOCH 1305
2024-02-02 12:06:59,416 Epoch 1305: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-02 12:06:59,417 EPOCH 1306
2024-02-02 12:07:05,756 [Epoch: 1306 Step: 00022200] Batch Recognition Loss:   0.000230 => Gls Tokens per Sec:     1475 || Batch Translation Loss:   0.024865 => Txt Tokens per Sec:     4133 || Lr: 0.000100
2024-02-02 12:07:06,308 Epoch 1306: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-02 12:07:06,308 EPOCH 1307
2024-02-02 12:07:13,059 Epoch 1307: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-02 12:07:13,059 EPOCH 1308
2024-02-02 12:07:20,123 Epoch 1308: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-02 12:07:20,124 EPOCH 1309
2024-02-02 12:07:27,067 Epoch 1309: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.83 
2024-02-02 12:07:27,067 EPOCH 1310
2024-02-02 12:07:33,936 Epoch 1310: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.92 
2024-02-02 12:07:33,937 EPOCH 1311
2024-02-02 12:07:40,768 Epoch 1311: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.20 
2024-02-02 12:07:40,769 EPOCH 1312
2024-02-02 12:07:45,921 [Epoch: 1312 Step: 00022300] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     1566 || Batch Translation Loss:   0.043025 => Txt Tokens per Sec:     4215 || Lr: 0.000100
2024-02-02 12:07:47,498 Epoch 1312: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.88 
2024-02-02 12:07:47,498 EPOCH 1313
2024-02-02 12:07:54,122 Epoch 1313: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.50 
2024-02-02 12:07:54,122 EPOCH 1314
2024-02-02 12:08:00,963 Epoch 1314: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.30 
2024-02-02 12:08:00,963 EPOCH 1315
2024-02-02 12:08:07,399 Epoch 1315: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-02 12:08:07,399 EPOCH 1316
2024-02-02 12:08:14,222 Epoch 1316: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.27 
2024-02-02 12:08:14,223 EPOCH 1317
2024-02-02 12:08:21,168 Epoch 1317: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-02 12:08:21,168 EPOCH 1318
2024-02-02 12:08:24,408 [Epoch: 1318 Step: 00022400] Batch Recognition Loss:   0.000324 => Gls Tokens per Sec:     2173 || Batch Translation Loss:   0.112321 => Txt Tokens per Sec:     5996 || Lr: 0.000100
2024-02-02 12:08:27,903 Epoch 1318: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.33 
2024-02-02 12:08:27,904 EPOCH 1319
2024-02-02 12:08:34,655 Epoch 1319: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.39 
2024-02-02 12:08:34,655 EPOCH 1320
2024-02-02 12:08:41,222 Epoch 1320: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.50 
2024-02-02 12:08:41,223 EPOCH 1321
2024-02-02 12:08:48,076 Epoch 1321: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.49 
2024-02-02 12:08:48,076 EPOCH 1322
2024-02-02 12:08:54,915 Epoch 1322: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.12 
2024-02-02 12:08:54,916 EPOCH 1323
2024-02-02 12:09:01,664 Epoch 1323: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.92 
2024-02-02 12:09:01,665 EPOCH 1324
2024-02-02 12:09:03,758 [Epoch: 1324 Step: 00022500] Batch Recognition Loss:   0.000316 => Gls Tokens per Sec:     2753 || Batch Translation Loss:   0.057790 => Txt Tokens per Sec:     7502 || Lr: 0.000100
2024-02-02 12:09:08,300 Epoch 1324: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.00 
2024-02-02 12:09:08,301 EPOCH 1325
2024-02-02 12:09:15,005 Epoch 1325: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.21 
2024-02-02 12:09:15,006 EPOCH 1326
2024-02-02 12:09:21,816 Epoch 1326: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.30 
2024-02-02 12:09:21,817 EPOCH 1327
2024-02-02 12:09:28,720 Epoch 1327: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.00 
2024-02-02 12:09:28,721 EPOCH 1328
2024-02-02 12:09:35,357 Epoch 1328: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-02 12:09:35,357 EPOCH 1329
2024-02-02 12:09:42,255 Epoch 1329: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-02 12:09:42,255 EPOCH 1330
2024-02-02 12:09:46,110 [Epoch: 1330 Step: 00022600] Batch Recognition Loss:   0.000380 => Gls Tokens per Sec:     1098 || Batch Translation Loss:   0.036364 => Txt Tokens per Sec:     3096 || Lr: 0.000100
2024-02-02 12:09:48,962 Epoch 1330: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-02 12:09:48,962 EPOCH 1331
2024-02-02 12:09:55,550 Epoch 1331: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 12:09:55,550 EPOCH 1332
2024-02-02 12:10:02,493 Epoch 1332: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-02 12:10:02,493 EPOCH 1333
2024-02-02 12:10:08,999 Epoch 1333: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 12:10:09,000 EPOCH 1334
2024-02-02 12:10:15,888 Epoch 1334: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 12:10:15,889 EPOCH 1335
2024-02-02 12:10:22,453 Epoch 1335: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 12:10:22,454 EPOCH 1336
2024-02-02 12:10:24,724 [Epoch: 1336 Step: 00022700] Batch Recognition Loss:   0.000348 => Gls Tokens per Sec:     1410 || Batch Translation Loss:   0.035250 => Txt Tokens per Sec:     4613 || Lr: 0.000100
2024-02-02 12:10:29,387 Epoch 1336: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 12:10:29,387 EPOCH 1337
2024-02-02 12:10:36,310 Epoch 1337: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 12:10:36,311 EPOCH 1338
2024-02-02 12:10:43,230 Epoch 1338: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-02 12:10:43,231 EPOCH 1339
2024-02-02 12:10:49,801 Epoch 1339: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.99 
2024-02-02 12:10:49,801 EPOCH 1340
2024-02-02 12:10:56,722 Epoch 1340: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.66 
2024-02-02 12:10:56,722 EPOCH 1341
2024-02-02 12:11:03,598 Epoch 1341: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-02 12:11:03,599 EPOCH 1342
2024-02-02 12:11:04,553 [Epoch: 1342 Step: 00022800] Batch Recognition Loss:   0.000307 => Gls Tokens per Sec:     2014 || Batch Translation Loss:   0.061312 => Txt Tokens per Sec:     6068 || Lr: 0.000100
2024-02-02 12:11:10,408 Epoch 1342: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-02 12:11:10,408 EPOCH 1343
2024-02-02 12:11:17,195 Epoch 1343: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 12:11:17,196 EPOCH 1344
2024-02-02 12:11:23,994 Epoch 1344: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-02 12:11:23,995 EPOCH 1345
2024-02-02 12:11:30,702 Epoch 1345: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-02 12:11:30,703 EPOCH 1346
2024-02-02 12:11:37,554 Epoch 1346: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 12:11:37,555 EPOCH 1347
2024-02-02 12:11:44,490 Epoch 1347: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 12:11:44,490 EPOCH 1348
2024-02-02 12:11:44,634 [Epoch: 1348 Step: 00022900] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     4476 || Batch Translation Loss:   0.012609 => Txt Tokens per Sec:    11434 || Lr: 0.000100
2024-02-02 12:11:50,865 Epoch 1348: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 12:11:50,866 EPOCH 1349
2024-02-02 12:11:57,719 Epoch 1349: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 12:11:57,719 EPOCH 1350
2024-02-02 12:12:04,593 Epoch 1350: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-02 12:12:04,594 EPOCH 1351
2024-02-02 12:12:11,442 Epoch 1351: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 12:12:11,443 EPOCH 1352
2024-02-02 12:12:18,283 Epoch 1352: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 12:12:18,284 EPOCH 1353
2024-02-02 12:12:24,913 [Epoch: 1353 Step: 00023000] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:     1507 || Batch Translation Loss:   0.016819 => Txt Tokens per Sec:     4203 || Lr: 0.000100
2024-02-02 12:12:25,084 Epoch 1353: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 12:12:25,084 EPOCH 1354
2024-02-02 12:12:31,883 Epoch 1354: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 12:12:31,883 EPOCH 1355
2024-02-02 12:12:38,578 Epoch 1355: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 12:12:38,578 EPOCH 1356
2024-02-02 12:12:45,292 Epoch 1356: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.87 
2024-02-02 12:12:45,292 EPOCH 1357
2024-02-02 12:12:52,194 Epoch 1357: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-02 12:12:52,194 EPOCH 1358
2024-02-02 12:12:59,131 Epoch 1358: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.71 
2024-02-02 12:12:59,132 EPOCH 1359
2024-02-02 12:13:05,043 [Epoch: 1359 Step: 00023100] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     1474 || Batch Translation Loss:   0.034578 => Txt Tokens per Sec:     3986 || Lr: 0.000100
2024-02-02 12:13:06,095 Epoch 1359: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.94 
2024-02-02 12:13:06,095 EPOCH 1360
2024-02-02 12:13:12,734 Epoch 1360: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.04 
2024-02-02 12:13:12,734 EPOCH 1361
2024-02-02 12:13:19,326 Epoch 1361: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-02 12:13:19,326 EPOCH 1362
2024-02-02 12:13:26,166 Epoch 1362: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.86 
2024-02-02 12:13:26,166 EPOCH 1363
2024-02-02 12:13:32,403 Epoch 1363: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.79 
2024-02-02 12:13:32,404 EPOCH 1364
2024-02-02 12:13:39,215 Epoch 1364: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.76 
2024-02-02 12:13:39,215 EPOCH 1365
2024-02-02 12:13:44,252 [Epoch: 1365 Step: 00023200] Batch Recognition Loss:   0.000279 => Gls Tokens per Sec:     1476 || Batch Translation Loss:   0.041752 => Txt Tokens per Sec:     4006 || Lr: 0.000100
2024-02-02 12:13:46,116 Epoch 1365: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-02 12:13:46,116 EPOCH 1366
2024-02-02 12:13:52,785 Epoch 1366: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-02 12:13:52,785 EPOCH 1367
2024-02-02 12:13:59,775 Epoch 1367: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-02 12:13:59,776 EPOCH 1368
2024-02-02 12:14:06,669 Epoch 1368: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-02 12:14:06,670 EPOCH 1369
2024-02-02 12:14:13,312 Epoch 1369: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.98 
2024-02-02 12:14:13,313 EPOCH 1370
2024-02-02 12:14:19,950 Epoch 1370: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.91 
2024-02-02 12:14:19,950 EPOCH 1371
2024-02-02 12:14:22,718 [Epoch: 1371 Step: 00023300] Batch Recognition Loss:   0.000224 => Gls Tokens per Sec:     2313 || Batch Translation Loss:   0.057234 => Txt Tokens per Sec:     6425 || Lr: 0.000100
2024-02-02 12:14:26,511 Epoch 1371: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-02 12:14:26,511 EPOCH 1372
2024-02-02 12:14:33,448 Epoch 1372: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-02 12:14:33,448 EPOCH 1373
2024-02-02 12:14:40,376 Epoch 1373: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.99 
2024-02-02 12:14:40,377 EPOCH 1374
2024-02-02 12:14:47,263 Epoch 1374: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-02 12:14:47,263 EPOCH 1375
2024-02-02 12:14:53,902 Epoch 1375: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-02 12:14:53,902 EPOCH 1376
2024-02-02 12:15:00,650 Epoch 1376: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.99 
2024-02-02 12:15:00,651 EPOCH 1377
2024-02-02 12:15:03,134 [Epoch: 1377 Step: 00023400] Batch Recognition Loss:   0.000213 => Gls Tokens per Sec:     2063 || Batch Translation Loss:   0.044677 => Txt Tokens per Sec:     6025 || Lr: 0.000100
2024-02-02 12:15:07,493 Epoch 1377: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.70 
2024-02-02 12:15:07,493 EPOCH 1378
2024-02-02 12:15:14,228 Epoch 1378: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-02 12:15:14,228 EPOCH 1379
2024-02-02 12:15:21,103 Epoch 1379: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 12:15:21,103 EPOCH 1380
2024-02-02 12:15:27,754 Epoch 1380: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-02 12:15:27,754 EPOCH 1381
2024-02-02 12:15:34,700 Epoch 1381: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.82 
2024-02-02 12:15:34,700 EPOCH 1382
2024-02-02 12:15:41,508 Epoch 1382: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.81 
2024-02-02 12:15:41,508 EPOCH 1383
2024-02-02 12:15:45,724 [Epoch: 1383 Step: 00023500] Batch Recognition Loss:   0.000270 => Gls Tokens per Sec:      852 || Batch Translation Loss:   0.043453 => Txt Tokens per Sec:     2650 || Lr: 0.000100
2024-02-02 12:15:48,370 Epoch 1383: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.91 
2024-02-02 12:15:48,371 EPOCH 1384
2024-02-02 12:15:55,154 Epoch 1384: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.91 
2024-02-02 12:15:55,154 EPOCH 1385
2024-02-02 12:16:01,995 Epoch 1385: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.95 
2024-02-02 12:16:01,996 EPOCH 1386
2024-02-02 12:16:08,976 Epoch 1386: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-02 12:16:08,977 EPOCH 1387
2024-02-02 12:16:16,068 Epoch 1387: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-02 12:16:16,068 EPOCH 1388
2024-02-02 12:16:22,980 Epoch 1388: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-02 12:16:22,980 EPOCH 1389
2024-02-02 12:16:24,174 [Epoch: 1389 Step: 00023600] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     2146 || Batch Translation Loss:   0.025355 => Txt Tokens per Sec:     5985 || Lr: 0.000100
2024-02-02 12:16:29,844 Epoch 1389: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 12:16:29,844 EPOCH 1390
2024-02-02 12:16:36,722 Epoch 1390: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 12:16:36,723 EPOCH 1391
2024-02-02 12:16:43,607 Epoch 1391: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 12:16:43,608 EPOCH 1392
2024-02-02 12:16:50,426 Epoch 1392: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 12:16:50,427 EPOCH 1393
2024-02-02 12:16:57,405 Epoch 1393: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 12:16:57,405 EPOCH 1394
2024-02-02 12:17:04,160 Epoch 1394: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 12:17:04,160 EPOCH 1395
2024-02-02 12:17:06,918 [Epoch: 1395 Step: 00023700] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:      374 || Batch Translation Loss:   0.032034 => Txt Tokens per Sec:     1187 || Lr: 0.000100
2024-02-02 12:17:11,190 Epoch 1395: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-02 12:17:11,191 EPOCH 1396
2024-02-02 12:17:18,069 Epoch 1396: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-02 12:17:18,069 EPOCH 1397
2024-02-02 12:17:25,041 Epoch 1397: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 12:17:25,042 EPOCH 1398
2024-02-02 12:17:31,868 Epoch 1398: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 12:17:31,868 EPOCH 1399
2024-02-02 12:17:38,643 Epoch 1399: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-02 12:17:38,644 EPOCH 1400
2024-02-02 12:17:45,545 [Epoch: 1400 Step: 00023800] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     1541 || Batch Translation Loss:   0.029794 => Txt Tokens per Sec:     4277 || Lr: 0.000100
2024-02-02 12:17:45,546 Epoch 1400: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-02 12:17:45,546 EPOCH 1401
2024-02-02 12:17:52,330 Epoch 1401: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 12:17:52,331 EPOCH 1402
2024-02-02 12:17:59,263 Epoch 1402: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 12:17:59,263 EPOCH 1403
2024-02-02 12:18:06,216 Epoch 1403: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-02 12:18:06,217 EPOCH 1404
2024-02-02 12:18:13,093 Epoch 1404: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.16 
2024-02-02 12:18:13,094 EPOCH 1405
2024-02-02 12:18:20,029 Epoch 1405: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.25 
2024-02-02 12:18:20,030 EPOCH 1406
2024-02-02 12:18:24,377 [Epoch: 1406 Step: 00023900] Batch Recognition Loss:   0.000830 => Gls Tokens per Sec:     2209 || Batch Translation Loss:   0.120002 => Txt Tokens per Sec:     6121 || Lr: 0.000100
2024-02-02 12:18:26,972 Epoch 1406: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.78 
2024-02-02 12:18:26,973 EPOCH 1407
2024-02-02 12:18:33,943 Epoch 1407: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.78 
2024-02-02 12:18:33,944 EPOCH 1408
2024-02-02 12:18:40,687 Epoch 1408: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.52 
2024-02-02 12:18:40,687 EPOCH 1409
2024-02-02 12:18:47,647 Epoch 1409: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.49 
2024-02-02 12:18:47,648 EPOCH 1410
2024-02-02 12:18:54,619 Epoch 1410: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.98 
2024-02-02 12:18:54,620 EPOCH 1411
2024-02-02 12:19:01,639 Epoch 1411: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.06 
2024-02-02 12:19:01,639 EPOCH 1412
2024-02-02 12:19:05,085 [Epoch: 1412 Step: 00024000] Batch Recognition Loss:   0.000807 => Gls Tokens per Sec:     2416 || Batch Translation Loss:   0.232331 => Txt Tokens per Sec:     6643 || Lr: 0.000100
2024-02-02 12:19:35,405 Validation result at epoch 1412, step    24000: duration: 30.3196s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00126	Translation Loss: 92963.03125	PPL: 10968.85742
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.59	(BLEU-1: 10.43,	BLEU-2: 3.18,	BLEU-3: 1.19,	BLEU-4: 0.59)
	CHRF 16.94	ROUGE 8.80
2024-02-02 12:19:35,406 Logging Recognition and Translation Outputs
2024-02-02 12:19:35,406 ========================================================================================================================
2024-02-02 12:19:35,407 Logging Sequence: 169_165.00
2024-02-02 12:19:35,407 	Gloss Reference :	A B+C+D+E
2024-02-02 12:19:35,407 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 12:19:35,407 	Gloss Alignment :	         
2024-02-02 12:19:35,407 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 12:19:35,409 	Text Reference  :	** the indian government was       outraged by          the incident and  these changes were undone by     wikipedia
2024-02-02 12:19:35,409 	Text Hypothesis :	do you know   that       wikipedia provides information on  celebs   like their height  age  must   ensure male     
2024-02-02 12:19:35,409 	Text Alignment  :	I  S   S      S          S         S        S           S   S        S    S     S       S    S      S      S        
2024-02-02 12:19:35,409 ========================================================================================================================
2024-02-02 12:19:35,409 Logging Sequence: 175_60.00
2024-02-02 12:19:35,410 	Gloss Reference :	A B+C+D+E
2024-02-02 12:19:35,410 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 12:19:35,410 	Gloss Alignment :	         
2024-02-02 12:19:35,410 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 12:19:35,411 	Text Reference  :	that is    how       india bagged 9   medals in the ****** ****** *** ***** *** **** youth tournament
2024-02-02 12:19:35,411 	Text Hypothesis :	**** hence pakistan' ended at     the fall   of the eighth wicket and india has left the   match     
2024-02-02 12:19:35,411 	Text Alignment  :	D    S     S         S     S      S   S      S      I      I      I   I     I   I    S     S         
2024-02-02 12:19:35,412 ========================================================================================================================
2024-02-02 12:19:35,412 Logging Sequence: 61_255.00
2024-02-02 12:19:35,412 	Gloss Reference :	A B+C+D+E
2024-02-02 12:19:35,412 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 12:19:35,412 	Gloss Alignment :	         
2024-02-02 12:19:35,412 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 12:19:35,413 	Text Reference  :	*** *** **** *** in       2011 we  decided to     marry and      informed our families
2024-02-02 12:19:35,413 	Text Hypothesis :	she has said her marriage is   not majorly during the   business class    on  twitter 
2024-02-02 12:19:35,414 	Text Alignment  :	I   I   I    I   S        S    S   S       S      S     S        S        S   S       
2024-02-02 12:19:35,414 ========================================================================================================================
2024-02-02 12:19:35,414 Logging Sequence: 173_39.00
2024-02-02 12:19:35,414 	Gloss Reference :	A B+C+D+E
2024-02-02 12:19:35,414 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 12:19:35,414 	Gloss Alignment :	         
2024-02-02 12:19:35,415 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 12:19:35,415 	Text Reference  :	**** ****** *** kohli will step        down as     india' captain 
2024-02-02 12:19:35,415 	Text Hypothesis :	when taylor too was   not  comfortable and  joined pune   warriors
2024-02-02 12:19:35,415 	Text Alignment  :	I    I      I   S     S    S           S    S      S      S       
2024-02-02 12:19:35,415 ========================================================================================================================
2024-02-02 12:19:35,416 Logging Sequence: 172_82.00
2024-02-02 12:19:35,416 	Gloss Reference :	A B+C+D+E
2024-02-02 12:19:35,416 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 12:19:35,416 	Gloss Alignment :	         
2024-02-02 12:19:35,416 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 12:19:35,418 	Text Reference  :	you all know     that the toss was about to start   at  700 pm  but  it started raining at around 630 pm     
2024-02-02 12:19:35,418 	Text Hypothesis :	*** *** remember that *** **** *** ***** ** amazing win you can find a  tickets will    be runner an  amazing
2024-02-02 12:19:35,418 	Text Alignment  :	D   D   S             D   D    D   D     D  S       S   S   S   S    S  S       S       S  S      S   S      
2024-02-02 12:19:35,418 ========================================================================================================================
2024-02-02 12:19:38,472 Epoch 1412: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.11 
2024-02-02 12:19:38,472 EPOCH 1413
2024-02-02 12:19:45,437 Epoch 1413: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.48 
2024-02-02 12:19:45,438 EPOCH 1414
2024-02-02 12:19:52,344 Epoch 1414: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-02 12:19:52,345 EPOCH 1415
2024-02-02 12:19:58,897 Epoch 1415: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-02 12:19:58,898 EPOCH 1416
2024-02-02 12:20:05,744 Epoch 1416: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 12:20:05,745 EPOCH 1417
2024-02-02 12:20:12,291 Epoch 1417: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 12:20:12,291 EPOCH 1418
2024-02-02 12:20:17,720 [Epoch: 1418 Step: 00024100] Batch Recognition Loss:   0.000241 => Gls Tokens per Sec:     1251 || Batch Translation Loss:   0.020614 => Txt Tokens per Sec:     3815 || Lr: 0.000100
2024-02-02 12:20:18,991 Epoch 1418: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 12:20:18,991 EPOCH 1419
2024-02-02 12:20:25,774 Epoch 1419: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 12:20:25,775 EPOCH 1420
2024-02-02 12:20:32,469 Epoch 1420: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 12:20:32,470 EPOCH 1421
2024-02-02 12:20:39,307 Epoch 1421: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 12:20:39,308 EPOCH 1422
2024-02-02 12:20:46,158 Epoch 1422: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 12:20:46,158 EPOCH 1423
2024-02-02 12:20:52,937 Epoch 1423: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 12:20:52,938 EPOCH 1424
2024-02-02 12:20:57,595 [Epoch: 1424 Step: 00024200] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     1183 || Batch Translation Loss:   0.029340 => Txt Tokens per Sec:     3222 || Lr: 0.000100
2024-02-02 12:20:59,816 Epoch 1424: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 12:20:59,816 EPOCH 1425
2024-02-02 12:21:06,500 Epoch 1425: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 12:21:06,500 EPOCH 1426
2024-02-02 12:21:13,220 Epoch 1426: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 12:21:13,221 EPOCH 1427
2024-02-02 12:21:20,170 Epoch 1427: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-02 12:21:20,171 EPOCH 1428
2024-02-02 12:21:27,074 Epoch 1428: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 12:21:27,075 EPOCH 1429
2024-02-02 12:21:33,583 Epoch 1429: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 12:21:33,583 EPOCH 1430
2024-02-02 12:21:35,045 [Epoch: 1430 Step: 00024300] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:     3067 || Batch Translation Loss:   0.010309 => Txt Tokens per Sec:     7880 || Lr: 0.000100
2024-02-02 12:21:40,559 Epoch 1430: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 12:21:40,559 EPOCH 1431
2024-02-02 12:21:47,194 Epoch 1431: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 12:21:47,194 EPOCH 1432
2024-02-02 12:21:54,140 Epoch 1432: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 12:21:54,140 EPOCH 1433
2024-02-02 12:22:00,753 Epoch 1433: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 12:22:00,753 EPOCH 1434
2024-02-02 12:22:07,303 Epoch 1434: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 12:22:07,304 EPOCH 1435
2024-02-02 12:22:14,258 Epoch 1435: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 12:22:14,258 EPOCH 1436
2024-02-02 12:22:15,557 [Epoch: 1436 Step: 00024400] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:     2465 || Batch Translation Loss:   0.012789 => Txt Tokens per Sec:     6597 || Lr: 0.000100
2024-02-02 12:22:20,957 Epoch 1436: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-02 12:22:20,958 EPOCH 1437
2024-02-02 12:22:26,911 Epoch 1437: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 12:22:26,912 EPOCH 1438
2024-02-02 12:22:33,537 Epoch 1438: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 12:22:33,537 EPOCH 1439
2024-02-02 12:22:40,267 Epoch 1439: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-02 12:22:40,268 EPOCH 1440
2024-02-02 12:22:46,963 Epoch 1440: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 12:22:46,963 EPOCH 1441
2024-02-02 12:22:53,696 Epoch 1441: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 12:22:53,696 EPOCH 1442
2024-02-02 12:22:54,482 [Epoch: 1442 Step: 00024500] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     2446 || Batch Translation Loss:   0.021610 => Txt Tokens per Sec:     6886 || Lr: 0.000100
2024-02-02 12:23:00,170 Epoch 1442: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 12:23:00,170 EPOCH 1443
2024-02-02 12:23:07,011 Epoch 1443: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 12:23:07,012 EPOCH 1444
2024-02-02 12:23:13,854 Epoch 1444: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 12:23:13,855 EPOCH 1445
2024-02-02 12:23:20,053 Epoch 1445: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 12:23:20,053 EPOCH 1446
2024-02-02 12:23:26,396 Epoch 1446: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 12:23:26,396 EPOCH 1447
2024-02-02 12:23:33,293 Epoch 1447: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.88 
2024-02-02 12:23:33,293 EPOCH 1448
2024-02-02 12:23:33,400 [Epoch: 1448 Step: 00024600] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     6038 || Batch Translation Loss:   0.030712 => Txt Tokens per Sec:     9321 || Lr: 0.000100
2024-02-02 12:23:39,941 Epoch 1448: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.17 
2024-02-02 12:23:39,942 EPOCH 1449
2024-02-02 12:23:46,742 Epoch 1449: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.37 
2024-02-02 12:23:46,742 EPOCH 1450
2024-02-02 12:23:53,712 Epoch 1450: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.06 
2024-02-02 12:23:53,713 EPOCH 1451
2024-02-02 12:24:00,526 Epoch 1451: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.10 
2024-02-02 12:24:00,526 EPOCH 1452
2024-02-02 12:24:07,258 Epoch 1452: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.18 
2024-02-02 12:24:07,259 EPOCH 1453
2024-02-02 12:24:11,600 [Epoch: 1453 Step: 00024700] Batch Recognition Loss:   0.000209 => Gls Tokens per Sec:     2359 || Batch Translation Loss:   0.038890 => Txt Tokens per Sec:     6477 || Lr: 0.000100
2024-02-02 12:24:14,062 Epoch 1453: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.95 
2024-02-02 12:24:14,063 EPOCH 1454
2024-02-02 12:24:20,931 Epoch 1454: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-02 12:24:20,932 EPOCH 1455
2024-02-02 12:24:27,709 Epoch 1455: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-02 12:24:27,709 EPOCH 1456
2024-02-02 12:24:34,639 Epoch 1456: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-02 12:24:34,640 EPOCH 1457
2024-02-02 12:24:41,313 Epoch 1457: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-02 12:24:41,313 EPOCH 1458
2024-02-02 12:24:48,049 Epoch 1458: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-02 12:24:48,049 EPOCH 1459
2024-02-02 12:24:52,108 [Epoch: 1459 Step: 00024800] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     2208 || Batch Translation Loss:   0.038393 => Txt Tokens per Sec:     6249 || Lr: 0.000100
2024-02-02 12:24:54,811 Epoch 1459: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.94 
2024-02-02 12:24:54,812 EPOCH 1460
2024-02-02 12:25:01,716 Epoch 1460: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.85 
2024-02-02 12:25:01,717 EPOCH 1461
2024-02-02 12:25:08,517 Epoch 1461: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.87 
2024-02-02 12:25:08,518 EPOCH 1462
2024-02-02 12:25:15,433 Epoch 1462: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.80 
2024-02-02 12:25:15,434 EPOCH 1463
2024-02-02 12:25:22,088 Epoch 1463: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-02 12:25:22,088 EPOCH 1464
2024-02-02 12:25:28,551 Epoch 1464: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 12:25:28,551 EPOCH 1465
2024-02-02 12:25:31,285 [Epoch: 1465 Step: 00024900] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     2810 || Batch Translation Loss:   0.009292 => Txt Tokens per Sec:     7750 || Lr: 0.000100
2024-02-02 12:25:34,468 Epoch 1465: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.86 
2024-02-02 12:25:34,468 EPOCH 1466
2024-02-02 12:25:41,279 Epoch 1466: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-02 12:25:41,279 EPOCH 1467
2024-02-02 12:25:48,164 Epoch 1467: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.96 
2024-02-02 12:25:48,165 EPOCH 1468
2024-02-02 12:25:54,587 Epoch 1468: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.99 
2024-02-02 12:25:54,587 EPOCH 1469
2024-02-02 12:26:01,588 Epoch 1469: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.00 
2024-02-02 12:26:01,588 EPOCH 1470
2024-02-02 12:26:08,428 Epoch 1470: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.32 
2024-02-02 12:26:08,429 EPOCH 1471
2024-02-02 12:26:13,679 [Epoch: 1471 Step: 00025000] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     1172 || Batch Translation Loss:   0.040214 => Txt Tokens per Sec:     3457 || Lr: 0.000100
2024-02-02 12:26:15,249 Epoch 1471: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.54 
2024-02-02 12:26:15,249 EPOCH 1472
2024-02-02 12:26:21,216 Epoch 1472: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.51 
2024-02-02 12:26:21,216 EPOCH 1473
2024-02-02 12:26:28,252 Epoch 1473: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.37 
2024-02-02 12:26:28,252 EPOCH 1474
2024-02-02 12:26:35,085 Epoch 1474: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.78 
2024-02-02 12:26:35,086 EPOCH 1475
2024-02-02 12:26:42,126 Epoch 1475: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-02 12:26:42,126 EPOCH 1476
2024-02-02 12:26:49,060 Epoch 1476: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 12:26:49,060 EPOCH 1477
2024-02-02 12:26:53,339 [Epoch: 1477 Step: 00025100] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:     1138 || Batch Translation Loss:   0.033858 => Txt Tokens per Sec:     3047 || Lr: 0.000100
2024-02-02 12:26:55,893 Epoch 1477: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-02 12:26:55,893 EPOCH 1478
2024-02-02 12:27:02,327 Epoch 1478: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 12:27:02,328 EPOCH 1479
2024-02-02 12:27:09,142 Epoch 1479: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 12:27:09,143 EPOCH 1480
2024-02-02 12:27:15,962 Epoch 1480: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 12:27:15,963 EPOCH 1481
2024-02-02 12:27:22,832 Epoch 1481: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 12:27:22,833 EPOCH 1482
2024-02-02 12:27:29,542 Epoch 1482: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 12:27:29,542 EPOCH 1483
2024-02-02 12:27:33,482 [Epoch: 1483 Step: 00025200] Batch Recognition Loss:   0.000293 => Gls Tokens per Sec:      912 || Batch Translation Loss:   0.012448 => Txt Tokens per Sec:     2750 || Lr: 0.000100
2024-02-02 12:27:36,310 Epoch 1483: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 12:27:36,310 EPOCH 1484
2024-02-02 12:27:43,035 Epoch 1484: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 12:27:43,035 EPOCH 1485
2024-02-02 12:27:49,729 Epoch 1485: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 12:27:49,729 EPOCH 1486
2024-02-02 12:27:56,745 Epoch 1486: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 12:27:56,745 EPOCH 1487
2024-02-02 12:28:03,636 Epoch 1487: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 12:28:03,636 EPOCH 1488
2024-02-02 12:28:10,436 Epoch 1488: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 12:28:10,437 EPOCH 1489
2024-02-02 12:28:11,585 [Epoch: 1489 Step: 00025300] Batch Recognition Loss:   0.000148 => Gls Tokens per Sec:     2232 || Batch Translation Loss:   0.015406 => Txt Tokens per Sec:     6711 || Lr: 0.000100
2024-02-02 12:28:17,291 Epoch 1489: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 12:28:17,292 EPOCH 1490
2024-02-02 12:28:23,914 Epoch 1490: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 12:28:23,915 EPOCH 1491
2024-02-02 12:28:30,711 Epoch 1491: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 12:28:30,712 EPOCH 1492
2024-02-02 12:28:37,598 Epoch 1492: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 12:28:37,598 EPOCH 1493
2024-02-02 12:28:44,487 Epoch 1493: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 12:28:44,487 EPOCH 1494
2024-02-02 12:28:51,287 Epoch 1494: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-02 12:28:51,287 EPOCH 1495
2024-02-02 12:28:52,065 [Epoch: 1495 Step: 00025400] Batch Recognition Loss:   0.000189 => Gls Tokens per Sec:     1649 || Batch Translation Loss:   0.020534 => Txt Tokens per Sec:     4948 || Lr: 0.000100
2024-02-02 12:28:58,017 Epoch 1495: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-02 12:28:58,018 EPOCH 1496
2024-02-02 12:29:04,733 Epoch 1496: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.78 
2024-02-02 12:29:04,733 EPOCH 1497
2024-02-02 12:29:11,620 Epoch 1497: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.16 
2024-02-02 12:29:11,621 EPOCH 1498
2024-02-02 12:29:18,441 Epoch 1498: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.85 
2024-02-02 12:29:18,442 EPOCH 1499
2024-02-02 12:29:25,220 Epoch 1499: Total Training Recognition Loss 0.06  Total Training Translation Loss 5.96 
2024-02-02 12:29:25,221 EPOCH 1500
2024-02-02 12:29:32,276 [Epoch: 1500 Step: 00025500] Batch Recognition Loss:   0.000349 => Gls Tokens per Sec:     1507 || Batch Translation Loss:   0.083977 => Txt Tokens per Sec:     4183 || Lr: 0.000100
2024-02-02 12:29:32,277 Epoch 1500: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.40 
2024-02-02 12:29:32,277 EPOCH 1501
2024-02-02 12:29:38,898 Epoch 1501: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.54 
2024-02-02 12:29:38,898 EPOCH 1502
2024-02-02 12:29:45,389 Epoch 1502: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-02 12:29:45,390 EPOCH 1503
2024-02-02 12:29:52,206 Epoch 1503: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-02 12:29:52,207 EPOCH 1504
2024-02-02 12:29:59,087 Epoch 1504: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-02 12:29:59,088 EPOCH 1505
2024-02-02 12:30:05,818 Epoch 1505: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-02 12:30:05,819 EPOCH 1506
2024-02-02 12:30:11,935 [Epoch: 1506 Step: 00025600] Batch Recognition Loss:   0.000300 => Gls Tokens per Sec:     1529 || Batch Translation Loss:   0.018154 => Txt Tokens per Sec:     4242 || Lr: 0.000100
2024-02-02 12:30:12,543 Epoch 1506: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-02 12:30:12,544 EPOCH 1507
2024-02-02 12:30:19,245 Epoch 1507: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 12:30:19,245 EPOCH 1508
2024-02-02 12:30:25,997 Epoch 1508: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 12:30:25,997 EPOCH 1509
2024-02-02 12:30:32,715 Epoch 1509: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 12:30:32,715 EPOCH 1510
2024-02-02 12:30:39,169 Epoch 1510: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 12:30:39,170 EPOCH 1511
2024-02-02 12:30:45,810 Epoch 1511: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 12:30:45,810 EPOCH 1512
2024-02-02 12:30:51,566 [Epoch: 1512 Step: 00025700] Batch Recognition Loss:   0.000246 => Gls Tokens per Sec:     1402 || Batch Translation Loss:   0.019088 => Txt Tokens per Sec:     3908 || Lr: 0.000100
2024-02-02 12:30:52,754 Epoch 1512: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 12:30:52,754 EPOCH 1513
2024-02-02 12:30:59,689 Epoch 1513: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 12:30:59,689 EPOCH 1514
2024-02-02 12:31:06,488 Epoch 1514: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 12:31:06,489 EPOCH 1515
2024-02-02 12:31:13,182 Epoch 1515: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 12:31:13,183 EPOCH 1516
2024-02-02 12:31:19,906 Epoch 1516: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 12:31:19,907 EPOCH 1517
2024-02-02 12:31:26,729 Epoch 1517: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 12:31:26,729 EPOCH 1518
2024-02-02 12:31:30,325 [Epoch: 1518 Step: 00025800] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     1959 || Batch Translation Loss:   0.010533 => Txt Tokens per Sec:     5691 || Lr: 0.000100
2024-02-02 12:31:33,602 Epoch 1518: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 12:31:33,602 EPOCH 1519
2024-02-02 12:31:40,708 Epoch 1519: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 12:31:40,709 EPOCH 1520
2024-02-02 12:31:47,390 Epoch 1520: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 12:31:47,390 EPOCH 1521
2024-02-02 12:31:54,431 Epoch 1521: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 12:31:54,432 EPOCH 1522
2024-02-02 12:32:01,433 Epoch 1522: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 12:32:01,434 EPOCH 1523
2024-02-02 12:32:08,350 Epoch 1523: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 12:32:08,351 EPOCH 1524
2024-02-02 12:32:11,174 [Epoch: 1524 Step: 00025900] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     2041 || Batch Translation Loss:   0.015459 => Txt Tokens per Sec:     5748 || Lr: 0.000100
2024-02-02 12:32:15,175 Epoch 1524: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 12:32:15,175 EPOCH 1525
2024-02-02 12:32:22,010 Epoch 1525: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 12:32:22,011 EPOCH 1526
2024-02-02 12:32:28,691 Epoch 1526: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 12:32:28,691 EPOCH 1527
2024-02-02 12:32:35,291 Epoch 1527: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 12:32:35,291 EPOCH 1528
2024-02-02 12:32:42,191 Epoch 1528: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 12:32:42,191 EPOCH 1529
2024-02-02 12:32:48,971 Epoch 1529: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 12:32:48,972 EPOCH 1530
2024-02-02 12:32:52,457 [Epoch: 1530 Step: 00026000] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:     1214 || Batch Translation Loss:   0.011250 => Txt Tokens per Sec:     3322 || Lr: 0.000100
2024-02-02 12:33:24,868 Validation result at epoch 1530, step    26000: duration: 32.4120s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00032	Translation Loss: 94322.03906	PPL: 12566.77051
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.86	(BLEU-1: 10.99,	BLEU-2: 3.47,	BLEU-3: 1.52,	BLEU-4: 0.86)
	CHRF 17.47	ROUGE 9.32
2024-02-02 12:33:24,869 Logging Recognition and Translation Outputs
2024-02-02 12:33:24,870 ========================================================================================================================
2024-02-02 12:33:24,870 Logging Sequence: 130_139.00
2024-02-02 12:33:24,870 	Gloss Reference :	A B+C+D+E
2024-02-02 12:33:24,870 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 12:33:24,872 	Gloss Alignment :	         
2024-02-02 12:33:24,872 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 12:33:24,874 	Text Reference  :	he shared a picture of     a little pouch he knit for his olympic gold medal with uk   flag on one side     and **** japanese flag on      the     other
2024-02-02 12:33:24,874 	Text Hypothesis :	he ****** * ******* played a ****** ***** ** **** *** *** ******* **** diver and  took part in the audience and 2016 rio      de   janeiro olympic games
2024-02-02 12:33:24,874 	Text Alignment  :	   D      D D       S        D      D     D  D    D   D   D       D    S     S    S    S    S  S   S            I    S        S    S       S       S    
2024-02-02 12:33:24,874 ========================================================================================================================
2024-02-02 12:33:24,874 Logging Sequence: 72_194.00
2024-02-02 12:33:24,874 	Gloss Reference :	A B+C+D+E
2024-02-02 12:33:24,874 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 12:33:24,874 	Gloss Alignment :	         
2024-02-02 12:33:24,876 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 12:33:24,877 	Text Reference  :	** shah told her      to **** do  what she    wants and filed a   police complaint against her    
2024-02-02 12:33:24,877 	Text Hypothesis :	as they were supposed to book the same colony he    was from  the video  of        9       minutes
2024-02-02 12:33:24,877 	Text Alignment  :	I  S    S    S           I    S   S    S      S     S   S     S   S      S         S       S      
2024-02-02 12:33:24,878 ========================================================================================================================
2024-02-02 12:33:24,878 Logging Sequence: 69_177.00
2024-02-02 12:33:24,878 	Gloss Reference :	A B+C+D+E
2024-02-02 12:33:24,878 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 12:33:24,878 	Gloss Alignment :	         
2024-02-02 12:33:24,878 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 12:33:24,879 	Text Reference  :	he said 'i will continue playing i know it's about time i   retire   i  also have a   knee condition
2024-02-02 12:33:24,879 	Text Hypothesis :	** **** ** **** ******** ******* * **** **** it    was  not selected to play the  ipl next season   
2024-02-02 12:33:24,879 	Text Alignment  :	D  D    D  D    D        D       D D    D    S     S    S   S        S  S    S    S   S    S        
2024-02-02 12:33:24,879 ========================================================================================================================
2024-02-02 12:33:24,879 Logging Sequence: 95_118.00
2024-02-02 12:33:24,879 	Gloss Reference :	A B+C+D+E
2024-02-02 12:33:24,879 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 12:33:24,879 	Gloss Alignment :	         
2024-02-02 12:33:24,879 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 12:33:24,881 	Text Reference  :	**** **** the      game    was stopped strangely due to   excessive sunlight
2024-02-02 12:33:24,881 	Text Hypothesis :	have been shocking ronaldo for the     pitch     and play a         lot     
2024-02-02 12:33:24,881 	Text Alignment  :	I    I    S        S       S   S       S         S   S    S         S       
2024-02-02 12:33:24,881 ========================================================================================================================
2024-02-02 12:33:24,882 Logging Sequence: 112_8.00
2024-02-02 12:33:24,882 	Gloss Reference :	A B+C+D+E
2024-02-02 12:33:24,882 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 12:33:24,882 	Gloss Alignment :	         
2024-02-02 12:33:24,882 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 12:33:24,883 	Text Reference  :	before there were 8 teams such as      mumbai indians delhi capitals punjab kings etc  and      now  there   will be 10   teams in 2022
2024-02-02 12:33:24,883 	Text Hypothesis :	****** ***** **** * ***** **** however the    bcci    has   been     made   the   same narendra modi stadium will ** home for   3  days
2024-02-02 12:33:24,884 	Text Alignment  :	D      D     D    D D     D    S       S      S       S     S        S      S     S    S        S    S            D  S    S     S  S   
2024-02-02 12:33:24,884 ========================================================================================================================
2024-02-02 12:33:28,064 Epoch 1530: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 12:33:28,064 EPOCH 1531
2024-02-02 12:33:35,094 Epoch 1531: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 12:33:35,095 EPOCH 1532
2024-02-02 12:33:41,957 Epoch 1532: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 12:33:41,957 EPOCH 1533
2024-02-02 12:33:48,248 Epoch 1533: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 12:33:48,249 EPOCH 1534
2024-02-02 12:33:55,035 Epoch 1534: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-02 12:33:55,036 EPOCH 1535
2024-02-02 12:34:01,981 Epoch 1535: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 12:34:01,982 EPOCH 1536
2024-02-02 12:34:03,376 [Epoch: 1536 Step: 00026100] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:     2296 || Batch Translation Loss:   0.015612 => Txt Tokens per Sec:     6482 || Lr: 0.000100
2024-02-02 12:34:08,759 Epoch 1536: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 12:34:08,759 EPOCH 1537
2024-02-02 12:34:15,447 Epoch 1537: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 12:34:15,448 EPOCH 1538
2024-02-02 12:34:22,395 Epoch 1538: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 12:34:22,395 EPOCH 1539
2024-02-02 12:34:29,130 Epoch 1539: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 12:34:29,131 EPOCH 1540
2024-02-02 12:34:35,680 Epoch 1540: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 12:34:35,681 EPOCH 1541
2024-02-02 12:34:42,332 Epoch 1541: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 12:34:42,333 EPOCH 1542
2024-02-02 12:34:42,964 [Epoch: 1542 Step: 00026200] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:     3052 || Batch Translation Loss:   0.008529 => Txt Tokens per Sec:     7052 || Lr: 0.000100
2024-02-02 12:34:49,138 Epoch 1542: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.70 
2024-02-02 12:34:49,138 EPOCH 1543
2024-02-02 12:34:56,085 Epoch 1543: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-02 12:34:56,085 EPOCH 1544
2024-02-02 12:35:02,697 Epoch 1544: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-02 12:35:02,698 EPOCH 1545
2024-02-02 12:35:09,198 Epoch 1545: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.09 
2024-02-02 12:35:09,199 EPOCH 1546
2024-02-02 12:35:15,886 Epoch 1546: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.99 
2024-02-02 12:35:15,886 EPOCH 1547
2024-02-02 12:35:22,740 Epoch 1547: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.51 
2024-02-02 12:35:22,741 EPOCH 1548
2024-02-02 12:35:22,870 [Epoch: 1548 Step: 00026300] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:     5000 || Batch Translation Loss:   0.030705 => Txt Tokens per Sec:    11141 || Lr: 0.000100
2024-02-02 12:35:29,595 Epoch 1548: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.27 
2024-02-02 12:35:29,596 EPOCH 1549
2024-02-02 12:35:36,539 Epoch 1549: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.46 
2024-02-02 12:35:36,540 EPOCH 1550
2024-02-02 12:35:42,945 Epoch 1550: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.45 
2024-02-02 12:35:42,945 EPOCH 1551
2024-02-02 12:35:49,665 Epoch 1551: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.31 
2024-02-02 12:35:49,666 EPOCH 1552
2024-02-02 12:35:56,299 Epoch 1552: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-02 12:35:56,300 EPOCH 1553
2024-02-02 12:36:03,092 [Epoch: 1553 Step: 00026400] Batch Recognition Loss:   0.000367 => Gls Tokens per Sec:     1471 || Batch Translation Loss:   0.052980 => Txt Tokens per Sec:     4135 || Lr: 0.000100
2024-02-02 12:36:03,236 Epoch 1553: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.30 
2024-02-02 12:36:03,236 EPOCH 1554
2024-02-02 12:36:09,981 Epoch 1554: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.94 
2024-02-02 12:36:09,982 EPOCH 1555
2024-02-02 12:36:16,608 Epoch 1555: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.97 
2024-02-02 12:36:16,609 EPOCH 1556
2024-02-02 12:36:23,453 Epoch 1556: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.82 
2024-02-02 12:36:23,453 EPOCH 1557
2024-02-02 12:36:30,533 Epoch 1557: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-02 12:36:30,534 EPOCH 1558
2024-02-02 12:36:37,331 Epoch 1558: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-02 12:36:37,331 EPOCH 1559
2024-02-02 12:36:43,238 [Epoch: 1559 Step: 00026500] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     1475 || Batch Translation Loss:   0.024428 => Txt Tokens per Sec:     4094 || Lr: 0.000100
2024-02-02 12:36:44,020 Epoch 1559: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-02 12:36:44,020 EPOCH 1560
2024-02-02 12:36:50,687 Epoch 1560: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-02 12:36:50,688 EPOCH 1561
2024-02-02 12:36:57,559 Epoch 1561: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.90 
2024-02-02 12:36:57,559 EPOCH 1562
2024-02-02 12:37:04,358 Epoch 1562: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-02 12:37:04,359 EPOCH 1563
2024-02-02 12:37:11,083 Epoch 1563: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-02 12:37:11,083 EPOCH 1564
2024-02-02 12:37:17,952 Epoch 1564: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 12:37:17,952 EPOCH 1565
2024-02-02 12:37:23,182 [Epoch: 1565 Step: 00026600] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     1421 || Batch Translation Loss:   0.021569 => Txt Tokens per Sec:     3944 || Lr: 0.000100
2024-02-02 12:37:24,677 Epoch 1565: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 12:37:24,678 EPOCH 1566
2024-02-02 12:37:31,412 Epoch 1566: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 12:37:31,412 EPOCH 1567
2024-02-02 12:37:38,343 Epoch 1567: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 12:37:38,343 EPOCH 1568
2024-02-02 12:37:45,166 Epoch 1568: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 12:37:45,167 EPOCH 1569
2024-02-02 12:37:51,908 Epoch 1569: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 12:37:51,909 EPOCH 1570
2024-02-02 12:37:58,652 Epoch 1570: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-02 12:37:58,652 EPOCH 1571
2024-02-02 12:38:03,395 [Epoch: 1571 Step: 00026700] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:     1297 || Batch Translation Loss:   0.013524 => Txt Tokens per Sec:     3686 || Lr: 0.000100
2024-02-02 12:38:05,185 Epoch 1571: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-02 12:38:05,185 EPOCH 1572
2024-02-02 12:38:11,722 Epoch 1572: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-02 12:38:11,722 EPOCH 1573
2024-02-02 12:38:18,537 Epoch 1573: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-02 12:38:18,537 EPOCH 1574
2024-02-02 12:38:25,634 Epoch 1574: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 12:38:25,635 EPOCH 1575
2024-02-02 12:38:32,571 Epoch 1575: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 12:38:32,571 EPOCH 1576
2024-02-02 12:38:39,350 Epoch 1576: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 12:38:39,350 EPOCH 1577
2024-02-02 12:38:41,192 [Epoch: 1577 Step: 00026800] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     2781 || Batch Translation Loss:   0.024399 => Txt Tokens per Sec:     7377 || Lr: 0.000100
2024-02-02 12:38:45,993 Epoch 1577: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 12:38:45,993 EPOCH 1578
2024-02-02 12:38:52,799 Epoch 1578: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 12:38:52,800 EPOCH 1579
2024-02-02 12:38:59,547 Epoch 1579: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 12:38:59,548 EPOCH 1580
2024-02-02 12:39:06,142 Epoch 1580: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 12:39:06,142 EPOCH 1581
2024-02-02 12:39:12,792 Epoch 1581: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-02 12:39:12,793 EPOCH 1582
2024-02-02 12:39:19,558 Epoch 1582: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.90 
2024-02-02 12:39:19,558 EPOCH 1583
2024-02-02 12:39:21,628 [Epoch: 1583 Step: 00026900] Batch Recognition Loss:   0.000361 => Gls Tokens per Sec:     1856 || Batch Translation Loss:   0.049532 => Txt Tokens per Sec:     5164 || Lr: 0.000100
2024-02-02 12:39:26,422 Epoch 1583: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.33 
2024-02-02 12:39:26,423 EPOCH 1584
2024-02-02 12:39:33,145 Epoch 1584: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.99 
2024-02-02 12:39:33,146 EPOCH 1585
2024-02-02 12:39:39,891 Epoch 1585: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.43 
2024-02-02 12:39:39,892 EPOCH 1586
2024-02-02 12:39:46,689 Epoch 1586: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.16 
2024-02-02 12:39:46,690 EPOCH 1587
2024-02-02 12:39:53,316 Epoch 1587: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-02 12:39:53,316 EPOCH 1588
2024-02-02 12:40:00,293 Epoch 1588: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.19 
2024-02-02 12:40:00,293 EPOCH 1589
2024-02-02 12:40:03,068 [Epoch: 1589 Step: 00027000] Batch Recognition Loss:   0.000538 => Gls Tokens per Sec:      833 || Batch Translation Loss:   0.025609 => Txt Tokens per Sec:     2277 || Lr: 0.000100
2024-02-02 12:40:07,048 Epoch 1589: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-02 12:40:07,049 EPOCH 1590
2024-02-02 12:40:13,383 Epoch 1590: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-02 12:40:13,384 EPOCH 1591
2024-02-02 12:40:20,319 Epoch 1591: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.70 
2024-02-02 12:40:20,320 EPOCH 1592
2024-02-02 12:40:27,191 Epoch 1592: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-02 12:40:27,192 EPOCH 1593
2024-02-02 12:40:34,063 Epoch 1593: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 12:40:34,063 EPOCH 1594
2024-02-02 12:40:40,634 Epoch 1594: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 12:40:40,634 EPOCH 1595
2024-02-02 12:40:41,050 [Epoch: 1595 Step: 00027100] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:     3084 || Batch Translation Loss:   0.011483 => Txt Tokens per Sec:     7566 || Lr: 0.000100
2024-02-02 12:40:47,337 Epoch 1595: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 12:40:47,337 EPOCH 1596
2024-02-02 12:40:53,992 Epoch 1596: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 12:40:53,992 EPOCH 1597
2024-02-02 12:41:00,838 Epoch 1597: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 12:41:00,838 EPOCH 1598
2024-02-02 12:41:07,716 Epoch 1598: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 12:41:07,717 EPOCH 1599
2024-02-02 12:41:14,419 Epoch 1599: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-02 12:41:14,420 EPOCH 1600
2024-02-02 12:41:21,222 [Epoch: 1600 Step: 00027200] Batch Recognition Loss:   0.000261 => Gls Tokens per Sec:     1563 || Batch Translation Loss:   0.067965 => Txt Tokens per Sec:     4339 || Lr: 0.000100
2024-02-02 12:41:21,223 Epoch 1600: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-02 12:41:21,223 EPOCH 1601
2024-02-02 12:41:27,983 Epoch 1601: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.01 
2024-02-02 12:41:27,983 EPOCH 1602
2024-02-02 12:41:34,673 Epoch 1602: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.81 
2024-02-02 12:41:34,673 EPOCH 1603
2024-02-02 12:41:41,586 Epoch 1603: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-02 12:41:41,587 EPOCH 1604
2024-02-02 12:41:47,924 Epoch 1604: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-02 12:41:47,925 EPOCH 1605
2024-02-02 12:41:54,721 Epoch 1605: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.01 
2024-02-02 12:41:54,721 EPOCH 1606
2024-02-02 12:42:01,050 [Epoch: 1606 Step: 00027300] Batch Recognition Loss:   0.000473 => Gls Tokens per Sec:     1477 || Batch Translation Loss:   0.099791 => Txt Tokens per Sec:     4144 || Lr: 0.000100
2024-02-02 12:42:01,476 Epoch 1606: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-02 12:42:01,476 EPOCH 1607
2024-02-02 12:42:08,191 Epoch 1607: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-02 12:42:08,192 EPOCH 1608
2024-02-02 12:42:15,162 Epoch 1608: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-02 12:42:15,163 EPOCH 1609
2024-02-02 12:42:21,868 Epoch 1609: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 12:42:21,869 EPOCH 1610
2024-02-02 12:42:28,579 Epoch 1610: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 12:42:28,579 EPOCH 1611
2024-02-02 12:42:35,288 Epoch 1611: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-02 12:42:35,288 EPOCH 1612
2024-02-02 12:42:40,774 [Epoch: 1612 Step: 00027400] Batch Recognition Loss:   0.000231 => Gls Tokens per Sec:     1471 || Batch Translation Loss:   0.037500 => Txt Tokens per Sec:     3988 || Lr: 0.000100
2024-02-02 12:42:41,968 Epoch 1612: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-02 12:42:41,969 EPOCH 1613
2024-02-02 12:42:48,729 Epoch 1613: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 12:42:48,730 EPOCH 1614
2024-02-02 12:42:55,423 Epoch 1614: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 12:42:55,424 EPOCH 1615
2024-02-02 12:43:01,972 Epoch 1615: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 12:43:01,973 EPOCH 1616
2024-02-02 12:43:08,770 Epoch 1616: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-02 12:43:08,771 EPOCH 1617
2024-02-02 12:43:15,469 Epoch 1617: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-02 12:43:15,470 EPOCH 1618
2024-02-02 12:43:20,771 [Epoch: 1618 Step: 00027500] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     1281 || Batch Translation Loss:   0.018531 => Txt Tokens per Sec:     3590 || Lr: 0.000100
2024-02-02 12:43:22,371 Epoch 1618: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 12:43:22,371 EPOCH 1619
2024-02-02 12:43:29,187 Epoch 1619: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 12:43:29,187 EPOCH 1620
2024-02-02 12:43:35,824 Epoch 1620: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-02 12:43:35,825 EPOCH 1621
2024-02-02 12:43:42,535 Epoch 1621: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 12:43:42,535 EPOCH 1622
2024-02-02 12:43:49,448 Epoch 1622: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-02 12:43:49,448 EPOCH 1623
2024-02-02 12:43:56,238 Epoch 1623: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.95 
2024-02-02 12:43:56,238 EPOCH 1624
2024-02-02 12:44:00,930 [Epoch: 1624 Step: 00027600] Batch Recognition Loss:   0.000163 => Gls Tokens per Sec:     1174 || Batch Translation Loss:   0.033847 => Txt Tokens per Sec:     3413 || Lr: 0.000100
2024-02-02 12:44:02,997 Epoch 1624: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-02 12:44:02,997 EPOCH 1625
2024-02-02 12:44:09,530 Epoch 1625: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.84 
2024-02-02 12:44:09,530 EPOCH 1626
2024-02-02 12:44:16,135 Epoch 1626: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 12:44:16,136 EPOCH 1627
2024-02-02 12:44:22,958 Epoch 1627: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-02 12:44:22,958 EPOCH 1628
2024-02-02 12:44:29,733 Epoch 1628: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.76 
2024-02-02 12:44:29,733 EPOCH 1629
2024-02-02 12:44:36,534 Epoch 1629: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.91 
2024-02-02 12:44:36,535 EPOCH 1630
2024-02-02 12:44:40,649 [Epoch: 1630 Step: 00027700] Batch Recognition Loss:   0.000338 => Gls Tokens per Sec:     1028 || Batch Translation Loss:   0.049027 => Txt Tokens per Sec:     2972 || Lr: 0.000100
2024-02-02 12:44:43,213 Epoch 1630: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.22 
2024-02-02 12:44:43,213 EPOCH 1631
2024-02-02 12:44:49,740 Epoch 1631: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.00 
2024-02-02 12:44:49,741 EPOCH 1632
2024-02-02 12:44:56,430 Epoch 1632: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-02 12:44:56,430 EPOCH 1633
2024-02-02 12:45:03,350 Epoch 1633: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-02 12:45:03,351 EPOCH 1634
2024-02-02 12:45:10,146 Epoch 1634: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 12:45:10,146 EPOCH 1635
2024-02-02 12:45:16,840 Epoch 1635: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 12:45:16,841 EPOCH 1636
2024-02-02 12:45:18,114 [Epoch: 1636 Step: 00027800] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:     2517 || Batch Translation Loss:   0.027088 => Txt Tokens per Sec:     6745 || Lr: 0.000100
2024-02-02 12:45:23,729 Epoch 1636: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 12:45:23,730 EPOCH 1637
2024-02-02 12:45:30,273 Epoch 1637: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 12:45:30,273 EPOCH 1638
2024-02-02 12:45:37,198 Epoch 1638: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-02 12:45:37,199 EPOCH 1639
2024-02-02 12:45:44,100 Epoch 1639: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 12:45:44,101 EPOCH 1640
2024-02-02 12:45:50,874 Epoch 1640: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.12 
2024-02-02 12:45:50,875 EPOCH 1641
2024-02-02 12:45:57,502 Epoch 1641: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.09 
2024-02-02 12:45:57,503 EPOCH 1642
2024-02-02 12:46:00,784 [Epoch: 1642 Step: 00027900] Batch Recognition Loss:   0.000654 => Gls Tokens per Sec:      509 || Batch Translation Loss:   0.101794 => Txt Tokens per Sec:     1669 || Lr: 0.000100
2024-02-02 12:46:04,256 Epoch 1642: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.04 
2024-02-02 12:46:04,256 EPOCH 1643
2024-02-02 12:46:11,095 Epoch 1643: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.01 
2024-02-02 12:46:11,096 EPOCH 1644
2024-02-02 12:46:17,962 Epoch 1644: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.59 
2024-02-02 12:46:17,962 EPOCH 1645
2024-02-02 12:46:24,543 Epoch 1645: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.37 
2024-02-02 12:46:24,543 EPOCH 1646
2024-02-02 12:46:31,171 Epoch 1646: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.53 
2024-02-02 12:46:31,172 EPOCH 1647
2024-02-02 12:46:37,993 Epoch 1647: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-02 12:46:37,993 EPOCH 1648
2024-02-02 12:46:38,439 [Epoch: 1648 Step: 00028000] Batch Recognition Loss:   0.000406 => Gls Tokens per Sec:     1438 || Batch Translation Loss:   0.065189 => Txt Tokens per Sec:     4629 || Lr: 0.000100
2024-02-02 12:47:08,524 Validation result at epoch 1648, step    28000: duration: 30.0857s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00061	Translation Loss: 95695.08594	PPL: 14417.70020
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.61	(BLEU-1: 10.27,	BLEU-2: 3.06,	BLEU-3: 1.19,	BLEU-4: 0.61)
	CHRF 17.09	ROUGE 8.83
2024-02-02 12:47:08,525 Logging Recognition and Translation Outputs
2024-02-02 12:47:08,525 ========================================================================================================================
2024-02-02 12:47:08,526 Logging Sequence: 67_98.00
2024-02-02 12:47:08,526 	Gloss Reference :	A B+C+D+E
2024-02-02 12:47:08,527 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 12:47:08,527 	Gloss Alignment :	         
2024-02-02 12:47:08,527 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 12:47:08,528 	Text Reference  :	it saddens me to    see people suffering and dying due     to ******** lack of   oxygen
2024-02-02 12:47:08,528 	Text Hypothesis :	** ******* ** india won the    toss      and ***** decided to withdraw in   test series
2024-02-02 12:47:08,528 	Text Alignment  :	D  D       D  S     S   S      S             D     S          I        S    S    S     
2024-02-02 12:47:08,528 ========================================================================================================================
2024-02-02 12:47:08,529 Logging Sequence: 157_83.00
2024-02-02 12:47:08,529 	Gloss Reference :	A B+C+D+E
2024-02-02 12:47:08,529 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 12:47:08,529 	Gloss Alignment :	         
2024-02-02 12:47:08,529 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 12:47:08,531 	Text Reference  :	also when you eat sandwich at a streetside hawker     or  stall  the     sandwich maker will    first apply     butter with a  knife
2024-02-02 12:47:08,531 	Text Hypothesis :	**** **** *** *** ******** ** * the        spectators had booked tickets through  paytm however no    e-tickets were   sent to them 
2024-02-02 12:47:08,531 	Text Alignment  :	D    D    D   D   D        D  D S          S          S   S      S       S        S     S       S     S         S      S    S  S    
2024-02-02 12:47:08,531 ========================================================================================================================
2024-02-02 12:47:08,531 Logging Sequence: 76_35.00
2024-02-02 12:47:08,532 	Gloss Reference :	A B+C+D+E
2024-02-02 12:47:08,532 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 12:47:08,532 	Gloss Alignment :	         
2024-02-02 12:47:08,532 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 12:47:08,533 	Text Reference  :	bcci president sourav ganguly along with board   secretary jay shah
2024-02-02 12:47:08,533 	Text Hypothesis :	**** ********* ****** ******* in    all  because dhoni     was said
2024-02-02 12:47:08,533 	Text Alignment  :	D    D         D      D       S     S    S       S         S   S   
2024-02-02 12:47:08,533 ========================================================================================================================
2024-02-02 12:47:08,533 Logging Sequence: 139_180.00
2024-02-02 12:47:08,533 	Gloss Reference :	A B+C+D+E
2024-02-02 12:47:08,533 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 12:47:08,534 	Gloss Alignment :	         
2024-02-02 12:47:08,534 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 12:47:08,534 	Text Reference  :	**** ** *** ***** *** ***** **** **** **** ******** ***** netherlands also    faced similar riots
2024-02-02 12:47:08,534 	Text Hypothesis :	this is why after the right time they were stressed there are         several such  a       viral
2024-02-02 12:47:08,535 	Text Alignment  :	I    I  I   I     I   I     I    I    I    I        I     S           S       S     S       S    
2024-02-02 12:47:08,535 ========================================================================================================================
2024-02-02 12:47:08,535 Logging Sequence: 98_87.00
2024-02-02 12:47:08,535 	Gloss Reference :	A B+C+D+E
2024-02-02 12:47:08,535 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 12:47:08,535 	Gloss Alignment :	         
2024-02-02 12:47:08,535 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 12:47:08,537 	Text Reference  :	instead of     starting afresh in 2021 the organizers opted to   resume with   the     previous edition
2024-02-02 12:47:08,537 	Text Hypothesis :	he      gifted each     one    of her  cup holding    we    will be     defeat against her      house  
2024-02-02 12:47:08,537 	Text Alignment  :	S       S      S        S      S  S    S   S          S     S    S      S      S       S        S      
2024-02-02 12:47:08,537 ========================================================================================================================
2024-02-02 12:47:15,038 Epoch 1648: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-02 12:47:15,038 EPOCH 1649
2024-02-02 12:47:21,839 Epoch 1649: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-02 12:47:21,839 EPOCH 1650
2024-02-02 12:47:28,832 Epoch 1650: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-02 12:47:28,833 EPOCH 1651
2024-02-02 12:47:35,648 Epoch 1651: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 12:47:35,649 EPOCH 1652
2024-02-02 12:47:42,404 Epoch 1652: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 12:47:42,405 EPOCH 1653
2024-02-02 12:47:48,635 [Epoch: 1653 Step: 00028100] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:     1604 || Batch Translation Loss:   0.015874 => Txt Tokens per Sec:     4377 || Lr: 0.000100
2024-02-02 12:47:49,121 Epoch 1653: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 12:47:49,121 EPOCH 1654
2024-02-02 12:47:55,802 Epoch 1654: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 12:47:55,803 EPOCH 1655
2024-02-02 12:48:02,505 Epoch 1655: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 12:48:02,506 EPOCH 1656
2024-02-02 12:48:09,370 Epoch 1656: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 12:48:09,371 EPOCH 1657
2024-02-02 12:48:16,168 Epoch 1657: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 12:48:16,168 EPOCH 1658
2024-02-02 12:48:22,539 Epoch 1658: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 12:48:22,540 EPOCH 1659
2024-02-02 12:48:28,156 [Epoch: 1659 Step: 00028200] Batch Recognition Loss:   0.000197 => Gls Tokens per Sec:     1552 || Batch Translation Loss:   0.017164 => Txt Tokens per Sec:     4209 || Lr: 0.000100
2024-02-02 12:48:29,124 Epoch 1659: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 12:48:29,124 EPOCH 1660
2024-02-02 12:48:35,105 Epoch 1660: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 12:48:35,105 EPOCH 1661
2024-02-02 12:48:41,898 Epoch 1661: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 12:48:41,899 EPOCH 1662
2024-02-02 12:48:48,625 Epoch 1662: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 12:48:48,626 EPOCH 1663
2024-02-02 12:48:55,449 Epoch 1663: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 12:48:55,450 EPOCH 1664
2024-02-02 12:49:02,199 Epoch 1664: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 12:49:02,200 EPOCH 1665
2024-02-02 12:49:07,537 [Epoch: 1665 Step: 00028300] Batch Recognition Loss:   0.000166 => Gls Tokens per Sec:     1393 || Batch Translation Loss:   0.013028 => Txt Tokens per Sec:     3984 || Lr: 0.000100
2024-02-02 12:49:08,994 Epoch 1665: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 12:49:08,995 EPOCH 1666
2024-02-02 12:49:15,644 Epoch 1666: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 12:49:15,644 EPOCH 1667
2024-02-02 12:49:22,552 Epoch 1667: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 12:49:22,553 EPOCH 1668
2024-02-02 12:49:29,386 Epoch 1668: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 12:49:29,387 EPOCH 1669
2024-02-02 12:49:36,366 Epoch 1669: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 12:49:36,367 EPOCH 1670
2024-02-02 12:49:43,339 Epoch 1670: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 12:49:43,339 EPOCH 1671
2024-02-02 12:49:48,054 [Epoch: 1671 Step: 00028400] Batch Recognition Loss:   0.000122 => Gls Tokens per Sec:     1305 || Batch Translation Loss:   0.012254 => Txt Tokens per Sec:     3690 || Lr: 0.000100
2024-02-02 12:49:50,124 Epoch 1671: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 12:49:50,124 EPOCH 1672
2024-02-02 12:49:56,529 Epoch 1672: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 12:49:56,529 EPOCH 1673
2024-02-02 12:50:03,361 Epoch 1673: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 12:50:03,362 EPOCH 1674
2024-02-02 12:50:10,254 Epoch 1674: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 12:50:10,255 EPOCH 1675
2024-02-02 12:50:17,050 Epoch 1675: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 12:50:17,051 EPOCH 1676
2024-02-02 12:50:23,518 Epoch 1676: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 12:50:23,518 EPOCH 1677
2024-02-02 12:50:27,467 [Epoch: 1677 Step: 00028500] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:     1233 || Batch Translation Loss:   0.008470 => Txt Tokens per Sec:     3404 || Lr: 0.000100
2024-02-02 12:50:30,506 Epoch 1677: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 12:50:30,507 EPOCH 1678
2024-02-02 12:50:37,054 Epoch 1678: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 12:50:37,055 EPOCH 1679
2024-02-02 12:50:44,214 Epoch 1679: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 12:50:44,215 EPOCH 1680
2024-02-02 12:50:51,018 Epoch 1680: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 12:50:51,019 EPOCH 1681
2024-02-02 12:50:57,704 Epoch 1681: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 12:50:57,704 EPOCH 1682
2024-02-02 12:51:04,262 Epoch 1682: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 12:51:04,262 EPOCH 1683
2024-02-02 12:51:05,579 [Epoch: 1683 Step: 00028600] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     2917 || Batch Translation Loss:   0.033139 => Txt Tokens per Sec:     7531 || Lr: 0.000100
2024-02-02 12:51:11,040 Epoch 1683: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 12:51:11,040 EPOCH 1684
2024-02-02 12:51:17,999 Epoch 1684: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 12:51:18,000 EPOCH 1685
2024-02-02 12:51:24,815 Epoch 1685: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-02 12:51:24,816 EPOCH 1686
2024-02-02 12:51:31,603 Epoch 1686: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.81 
2024-02-02 12:51:31,604 EPOCH 1687
2024-02-02 12:51:38,430 Epoch 1687: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.88 
2024-02-02 12:51:38,431 EPOCH 1688
2024-02-02 12:51:45,048 Epoch 1688: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.06 
2024-02-02 12:51:45,049 EPOCH 1689
2024-02-02 12:51:46,020 [Epoch: 1689 Step: 00028700] Batch Recognition Loss:   0.000380 => Gls Tokens per Sec:     2636 || Batch Translation Loss:   0.075393 => Txt Tokens per Sec:     7310 || Lr: 0.000100
2024-02-02 12:51:51,655 Epoch 1689: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.11 
2024-02-02 12:51:51,656 EPOCH 1690
2024-02-02 12:51:58,402 Epoch 1690: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-02 12:51:58,403 EPOCH 1691
2024-02-02 12:52:04,670 Epoch 1691: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.44 
2024-02-02 12:52:04,671 EPOCH 1692
2024-02-02 12:52:10,646 Epoch 1692: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.30 
2024-02-02 12:52:10,646 EPOCH 1693
2024-02-02 12:52:17,438 Epoch 1693: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.17 
2024-02-02 12:52:17,438 EPOCH 1694
2024-02-02 12:52:24,120 Epoch 1694: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.05 
2024-02-02 12:52:24,121 EPOCH 1695
2024-02-02 12:52:24,375 [Epoch: 1695 Step: 00028800] Batch Recognition Loss:   0.000189 => Gls Tokens per Sec:     5057 || Batch Translation Loss:   0.045796 => Txt Tokens per Sec:    10419 || Lr: 0.000100
2024-02-02 12:52:30,769 Epoch 1695: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.80 
2024-02-02 12:52:30,769 EPOCH 1696
2024-02-02 12:52:37,402 Epoch 1696: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-02 12:52:37,403 EPOCH 1697
2024-02-02 12:52:44,065 Epoch 1697: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-02 12:52:44,065 EPOCH 1698
2024-02-02 12:52:50,573 Epoch 1698: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.71 
2024-02-02 12:52:50,574 EPOCH 1699
2024-02-02 12:52:57,457 Epoch 1699: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-02 12:52:57,457 EPOCH 1700
2024-02-02 12:53:04,348 [Epoch: 1700 Step: 00028900] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:     1543 || Batch Translation Loss:   0.025893 => Txt Tokens per Sec:     4284 || Lr: 0.000100
2024-02-02 12:53:04,348 Epoch 1700: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-02 12:53:04,348 EPOCH 1701
2024-02-02 12:53:10,933 Epoch 1701: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.12 
2024-02-02 12:53:10,933 EPOCH 1702
2024-02-02 12:53:17,567 Epoch 1702: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.31 
2024-02-02 12:53:17,567 EPOCH 1703
2024-02-02 12:53:24,415 Epoch 1703: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-02 12:53:24,416 EPOCH 1704
2024-02-02 12:53:31,247 Epoch 1704: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-02 12:53:31,248 EPOCH 1705
2024-02-02 12:53:37,937 Epoch 1705: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 12:53:37,938 EPOCH 1706
2024-02-02 12:53:44,320 [Epoch: 1706 Step: 00029000] Batch Recognition Loss:   0.000180 => Gls Tokens per Sec:     1465 || Batch Translation Loss:   0.024441 => Txt Tokens per Sec:     4102 || Lr: 0.000100
2024-02-02 12:53:44,779 Epoch 1706: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 12:53:44,779 EPOCH 1707
2024-02-02 12:53:51,221 Epoch 1707: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 12:53:51,221 EPOCH 1708
2024-02-02 12:53:57,943 Epoch 1708: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 12:53:57,944 EPOCH 1709
2024-02-02 12:54:04,252 Epoch 1709: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 12:54:04,252 EPOCH 1710
2024-02-02 12:54:10,221 Epoch 1710: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-02 12:54:10,222 EPOCH 1711
2024-02-02 12:54:17,187 Epoch 1711: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-02 12:54:17,188 EPOCH 1712
2024-02-02 12:54:20,894 [Epoch: 1712 Step: 00029100] Batch Recognition Loss:   0.000209 => Gls Tokens per Sec:     2246 || Batch Translation Loss:   0.021440 => Txt Tokens per Sec:     6097 || Lr: 0.000100
2024-02-02 12:54:24,048 Epoch 1712: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-02 12:54:24,048 EPOCH 1713
2024-02-02 12:54:30,937 Epoch 1713: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.71 
2024-02-02 12:54:30,938 EPOCH 1714
2024-02-02 12:54:37,508 Epoch 1714: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.99 
2024-02-02 12:54:37,508 EPOCH 1715
2024-02-02 12:54:44,089 Epoch 1715: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-02 12:54:44,090 EPOCH 1716
2024-02-02 12:54:50,856 Epoch 1716: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.21 
2024-02-02 12:54:50,856 EPOCH 1717
2024-02-02 12:54:57,785 Epoch 1717: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-02 12:54:57,786 EPOCH 1718
2024-02-02 12:55:03,328 [Epoch: 1718 Step: 00029200] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:     1226 || Batch Translation Loss:   0.058224 => Txt Tokens per Sec:     3498 || Lr: 0.000100
2024-02-02 12:55:04,706 Epoch 1718: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.95 
2024-02-02 12:55:04,706 EPOCH 1719
2024-02-02 12:55:11,566 Epoch 1719: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-02 12:55:11,566 EPOCH 1720
2024-02-02 12:55:18,086 Epoch 1720: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-02 12:55:18,087 EPOCH 1721
2024-02-02 12:55:24,936 Epoch 1721: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 12:55:24,937 EPOCH 1722
2024-02-02 12:55:31,659 Epoch 1722: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 12:55:31,659 EPOCH 1723
2024-02-02 12:55:38,602 Epoch 1723: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 12:55:38,603 EPOCH 1724
2024-02-02 12:55:40,592 [Epoch: 1724 Step: 00029300] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     2898 || Batch Translation Loss:   0.033549 => Txt Tokens per Sec:     8030 || Lr: 0.000100
2024-02-02 12:55:45,349 Epoch 1724: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 12:55:45,349 EPOCH 1725
2024-02-02 12:55:52,220 Epoch 1725: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 12:55:52,221 EPOCH 1726
2024-02-02 12:55:58,923 Epoch 1726: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 12:55:58,924 EPOCH 1727
2024-02-02 12:56:05,729 Epoch 1727: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 12:56:05,730 EPOCH 1728
2024-02-02 12:56:12,796 Epoch 1728: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 12:56:12,796 EPOCH 1729
2024-02-02 12:56:19,709 Epoch 1729: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 12:56:19,710 EPOCH 1730
2024-02-02 12:56:23,433 [Epoch: 1730 Step: 00029400] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:     1136 || Batch Translation Loss:   0.019724 => Txt Tokens per Sec:     3154 || Lr: 0.000100
2024-02-02 12:56:26,228 Epoch 1730: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 12:56:26,228 EPOCH 1731
2024-02-02 12:56:32,809 Epoch 1731: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 12:56:32,809 EPOCH 1732
2024-02-02 12:56:39,656 Epoch 1732: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 12:56:39,656 EPOCH 1733
2024-02-02 12:56:46,575 Epoch 1733: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 12:56:46,575 EPOCH 1734
2024-02-02 12:56:53,474 Epoch 1734: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 12:56:53,475 EPOCH 1735
2024-02-02 12:57:00,256 Epoch 1735: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 12:57:00,257 EPOCH 1736
2024-02-02 12:57:01,968 [Epoch: 1736 Step: 00029500] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     1872 || Batch Translation Loss:   0.015434 => Txt Tokens per Sec:     5644 || Lr: 0.000100
2024-02-02 12:57:06,902 Epoch 1736: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 12:57:06,902 EPOCH 1737
2024-02-02 12:57:13,464 Epoch 1737: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-02 12:57:13,465 EPOCH 1738
2024-02-02 12:57:20,429 Epoch 1738: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-02 12:57:20,430 EPOCH 1739
2024-02-02 12:57:27,073 Epoch 1739: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-02 12:57:27,074 EPOCH 1740
2024-02-02 12:57:33,912 Epoch 1740: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 12:57:33,913 EPOCH 1741
2024-02-02 12:57:40,422 Epoch 1741: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 12:57:40,422 EPOCH 1742
2024-02-02 12:57:41,307 [Epoch: 1742 Step: 00029600] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:     2173 || Batch Translation Loss:   0.022032 => Txt Tokens per Sec:     5821 || Lr: 0.000100
2024-02-02 12:57:47,216 Epoch 1742: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 12:57:47,217 EPOCH 1743
2024-02-02 12:57:54,043 Epoch 1743: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 12:57:54,043 EPOCH 1744
2024-02-02 12:58:00,989 Epoch 1744: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-02 12:58:00,990 EPOCH 1745
2024-02-02 12:58:07,937 Epoch 1745: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-02 12:58:07,938 EPOCH 1746
2024-02-02 12:58:14,610 Epoch 1746: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-02 12:58:14,611 EPOCH 1747
2024-02-02 12:58:21,407 Epoch 1747: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-02 12:58:21,408 EPOCH 1748
2024-02-02 12:58:21,570 [Epoch: 1748 Step: 00029700] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:     3975 || Batch Translation Loss:   0.027603 => Txt Tokens per Sec:    10161 || Lr: 0.000100
2024-02-02 12:58:28,110 Epoch 1748: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-02 12:58:28,111 EPOCH 1749
2024-02-02 12:58:34,976 Epoch 1749: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.86 
2024-02-02 12:58:34,977 EPOCH 1750
2024-02-02 12:58:41,761 Epoch 1750: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.08 
2024-02-02 12:58:41,761 EPOCH 1751
2024-02-02 12:58:48,150 Epoch 1751: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.93 
2024-02-02 12:58:48,151 EPOCH 1752
2024-02-02 12:58:55,171 Epoch 1752: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-02 12:58:55,172 EPOCH 1753
2024-02-02 12:59:01,811 [Epoch: 1753 Step: 00029800] Batch Recognition Loss:   0.000488 => Gls Tokens per Sec:     1505 || Batch Translation Loss:   0.077919 => Txt Tokens per Sec:     4169 || Lr: 0.000100
2024-02-02 12:59:02,079 Epoch 1753: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.59 
2024-02-02 12:59:02,079 EPOCH 1754
2024-02-02 12:59:08,857 Epoch 1754: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.83 
2024-02-02 12:59:08,857 EPOCH 1755
2024-02-02 12:59:15,531 Epoch 1755: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.46 
2024-02-02 12:59:15,531 EPOCH 1756
2024-02-02 12:59:22,455 Epoch 1756: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.44 
2024-02-02 12:59:22,456 EPOCH 1757
2024-02-02 12:59:29,259 Epoch 1757: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.67 
2024-02-02 12:59:29,259 EPOCH 1758
2024-02-02 12:59:35,950 Epoch 1758: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.22 
2024-02-02 12:59:35,951 EPOCH 1759
2024-02-02 12:59:42,343 [Epoch: 1759 Step: 00029900] Batch Recognition Loss:   0.000267 => Gls Tokens per Sec:     1363 || Batch Translation Loss:   0.071073 => Txt Tokens per Sec:     3879 || Lr: 0.000100
2024-02-02 12:59:42,857 Epoch 1759: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.84 
2024-02-02 12:59:42,857 EPOCH 1760
2024-02-02 12:59:49,498 Epoch 1760: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-02 12:59:49,499 EPOCH 1761
2024-02-02 12:59:56,361 Epoch 1761: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-02 12:59:56,362 EPOCH 1762
2024-02-02 13:00:03,215 Epoch 1762: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-02 13:00:03,215 EPOCH 1763
2024-02-02 13:00:09,921 Epoch 1763: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-02 13:00:09,921 EPOCH 1764
2024-02-02 13:00:16,805 Epoch 1764: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 13:00:16,806 EPOCH 1765
2024-02-02 13:00:21,826 [Epoch: 1765 Step: 00030000] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:     1481 || Batch Translation Loss:   0.012332 => Txt Tokens per Sec:     3977 || Lr: 0.000100
2024-02-02 13:00:51,703 Validation result at epoch 1765, step    30000: duration: 29.8779s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00041	Translation Loss: 97500.51562	PPL: 17272.68555
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.62	(BLEU-1: 10.53,	BLEU-2: 3.12,	BLEU-3: 1.28,	BLEU-4: 0.62)
	CHRF 16.97	ROUGE 8.99
2024-02-02 13:00:51,705 Logging Recognition and Translation Outputs
2024-02-02 13:00:51,705 ========================================================================================================================
2024-02-02 13:00:51,705 Logging Sequence: 165_502.00
2024-02-02 13:00:51,705 	Gloss Reference :	A B+C+D+E
2024-02-02 13:00:51,706 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 13:00:51,706 	Gloss Alignment :	         
2024-02-02 13:00:51,706 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 13:00:51,708 	Text Reference  :	tendulkar would sit in  the pavilion wearing both  his  batting pads even after he got   out  
2024-02-02 13:00:51,708 	Text Hypothesis :	when      he    was out to  play     the     match they played  well the  teams in their match
2024-02-02 13:00:51,708 	Text Alignment  :	S         S     S   S   S   S        S       S     S    S       S    S    S     S  S     S    
2024-02-02 13:00:51,708 ========================================================================================================================
2024-02-02 13:00:51,708 Logging Sequence: 127_57.00
2024-02-02 13:00:51,708 	Gloss Reference :	A B+C+D+E
2024-02-02 13:00:51,708 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 13:00:51,709 	Gloss Alignment :	         
2024-02-02 13:00:51,709 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 13:00:51,710 	Text Reference  :	till date india had won only 2 medals at the championships which like the     olympics is     the highest level championship
2024-02-02 13:00:51,710 	Text Hypothesis :	**** **** india *** won **** * ****** ** the ************* toss  and  decided to       become the ******* ***** match       
2024-02-02 13:00:51,710 	Text Alignment  :	D    D          D       D    D D      D      D             S     S    S       S        S          D       D     S           
2024-02-02 13:00:51,710 ========================================================================================================================
2024-02-02 13:00:51,711 Logging Sequence: 169_10.00
2024-02-02 13:00:51,711 	Gloss Reference :	A B+C+D+E
2024-02-02 13:00:51,711 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 13:00:51,711 	Gloss Alignment :	         
2024-02-02 13:00:51,711 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 13:00:51,713 	Text Reference  :	the 18th over was bowled by   ravi    bishnoi with khushdil shah   and asif ali  on      the   crease
2024-02-02 13:00:51,713 	Text Hypothesis :	*** **** you  are aware  that earlier ie      a    test     series on  6th  team chennai super kings 
2024-02-02 13:00:51,713 	Text Alignment  :	D   D    S    S   S      S    S       S       S    S        S      S   S    S    S       S     S     
2024-02-02 13:00:51,713 ========================================================================================================================
2024-02-02 13:00:51,713 Logging Sequence: 64_89.00
2024-02-02 13:00:51,713 	Gloss Reference :	A B+C+D+E
2024-02-02 13:00:51,714 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 13:00:51,714 	Gloss Alignment :	         
2024-02-02 13:00:51,714 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 13:00:51,715 	Text Reference  :	but this can not go on amidst the rising cases human lives need to be  safeguarded
2024-02-02 13:00:51,715 	Text Hypothesis :	*** **** *** *** ** ** ****** and is     ipl   am    very  well in any spectators 
2024-02-02 13:00:51,715 	Text Alignment  :	D   D    D   D   D  D  D      S   S      S     S     S     S    S  S   S          
2024-02-02 13:00:51,715 ========================================================================================================================
2024-02-02 13:00:51,715 Logging Sequence: 166_261.00
2024-02-02 13:00:51,715 	Gloss Reference :	A B+C+D+E
2024-02-02 13:00:51,716 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 13:00:51,716 	Gloss Alignment :	         
2024-02-02 13:00:51,716 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 13:00:51,717 	Text Reference  :	********** *** *** for    all  organizational matters and    the ****** **** ***** **** * **** *** schedule 
2024-02-02 13:00:51,717 	Text Hypothesis :	federation and the series they were           also    become the series they would also a huge fan following
2024-02-02 13:00:51,717 	Text Alignment  :	I          I   I   S      S    S              S       S          I      I    I     I    I I    I   S        
2024-02-02 13:00:51,717 ========================================================================================================================
2024-02-02 13:00:53,692 Epoch 1765: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 13:00:53,693 EPOCH 1766
2024-02-02 13:01:00,277 Epoch 1766: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 13:01:00,277 EPOCH 1767
2024-02-02 13:01:07,011 Epoch 1767: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 13:01:07,012 EPOCH 1768
2024-02-02 13:01:13,532 Epoch 1768: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 13:01:13,533 EPOCH 1769
2024-02-02 13:01:20,374 Epoch 1769: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-02 13:01:20,375 EPOCH 1770
2024-02-02 13:01:27,055 Epoch 1770: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 13:01:27,055 EPOCH 1771
2024-02-02 13:01:29,841 [Epoch: 1771 Step: 00030100] Batch Recognition Loss:   0.000219 => Gls Tokens per Sec:     2299 || Batch Translation Loss:   0.030570 => Txt Tokens per Sec:     6438 || Lr: 0.000100
2024-02-02 13:01:33,856 Epoch 1771: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 13:01:33,856 EPOCH 1772
2024-02-02 13:01:40,604 Epoch 1772: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 13:01:40,604 EPOCH 1773
2024-02-02 13:01:47,298 Epoch 1773: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 13:01:47,299 EPOCH 1774
2024-02-02 13:01:54,276 Epoch 1774: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 13:01:54,277 EPOCH 1775
2024-02-02 13:02:01,391 Epoch 1775: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 13:02:01,391 EPOCH 1776
2024-02-02 13:02:08,320 Epoch 1776: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 13:02:08,321 EPOCH 1777
2024-02-02 13:02:10,738 [Epoch: 1777 Step: 00030200] Batch Recognition Loss:   0.000207 => Gls Tokens per Sec:     2119 || Batch Translation Loss:   0.056792 => Txt Tokens per Sec:     6289 || Lr: 0.000100
2024-02-02 13:02:14,776 Epoch 1777: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-02 13:02:14,776 EPOCH 1778
2024-02-02 13:02:21,460 Epoch 1778: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.96 
2024-02-02 13:02:21,460 EPOCH 1779
2024-02-02 13:02:28,461 Epoch 1779: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.94 
2024-02-02 13:02:28,462 EPOCH 1780
2024-02-02 13:02:35,238 Epoch 1780: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-02 13:02:35,239 EPOCH 1781
2024-02-02 13:02:42,121 Epoch 1781: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.22 
2024-02-02 13:02:42,122 EPOCH 1782
2024-02-02 13:02:48,631 Epoch 1782: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.31 
2024-02-02 13:02:48,631 EPOCH 1783
2024-02-02 13:02:49,990 [Epoch: 1783 Step: 00030300] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:     2828 || Batch Translation Loss:   0.072712 => Txt Tokens per Sec:     7636 || Lr: 0.000100
2024-02-02 13:02:55,217 Epoch 1783: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-02 13:02:55,218 EPOCH 1784
2024-02-02 13:03:02,246 Epoch 1784: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.94 
2024-02-02 13:03:02,246 EPOCH 1785
2024-02-02 13:03:08,996 Epoch 1785: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-02 13:03:08,997 EPOCH 1786
2024-02-02 13:03:15,753 Epoch 1786: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-02 13:03:15,754 EPOCH 1787
2024-02-02 13:03:22,341 Epoch 1787: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-02 13:03:22,341 EPOCH 1788
2024-02-02 13:03:29,278 Epoch 1788: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 13:03:29,279 EPOCH 1789
2024-02-02 13:03:32,672 [Epoch: 1789 Step: 00030400] Batch Recognition Loss:   0.000315 => Gls Tokens per Sec:      681 || Batch Translation Loss:   0.018109 => Txt Tokens per Sec:     1974 || Lr: 0.000100
2024-02-02 13:03:36,162 Epoch 1789: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 13:03:36,162 EPOCH 1790
2024-02-02 13:03:42,573 Epoch 1790: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 13:03:42,574 EPOCH 1791
2024-02-02 13:03:49,317 Epoch 1791: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 13:03:49,317 EPOCH 1792
2024-02-02 13:03:56,103 Epoch 1792: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 13:03:56,103 EPOCH 1793
2024-02-02 13:04:03,078 Epoch 1793: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 13:04:03,078 EPOCH 1794
2024-02-02 13:04:09,790 Epoch 1794: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 13:04:09,790 EPOCH 1795
2024-02-02 13:04:10,526 [Epoch: 1795 Step: 00030500] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:     1743 || Batch Translation Loss:   0.021875 => Txt Tokens per Sec:     5550 || Lr: 0.000100
2024-02-02 13:04:16,229 Epoch 1795: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 13:04:16,229 EPOCH 1796
2024-02-02 13:04:23,254 Epoch 1796: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-02 13:04:23,255 EPOCH 1797
2024-02-02 13:04:29,745 Epoch 1797: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 13:04:29,745 EPOCH 1798
2024-02-02 13:04:36,669 Epoch 1798: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 13:04:36,670 EPOCH 1799
2024-02-02 13:04:43,537 Epoch 1799: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 13:04:43,538 EPOCH 1800
2024-02-02 13:04:50,256 [Epoch: 1800 Step: 00030600] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     1582 || Batch Translation Loss:   0.018762 => Txt Tokens per Sec:     4393 || Lr: 0.000100
2024-02-02 13:04:50,256 Epoch 1800: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 13:04:50,257 EPOCH 1801
2024-02-02 13:04:57,034 Epoch 1801: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.21 
2024-02-02 13:04:57,035 EPOCH 1802
2024-02-02 13:05:04,021 Epoch 1802: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.74 
2024-02-02 13:05:04,022 EPOCH 1803
2024-02-02 13:05:10,722 Epoch 1803: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.11 
2024-02-02 13:05:10,722 EPOCH 1804
2024-02-02 13:05:17,699 Epoch 1804: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.30 
2024-02-02 13:05:17,700 EPOCH 1805
2024-02-02 13:05:24,469 Epoch 1805: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-02 13:05:24,469 EPOCH 1806
2024-02-02 13:05:30,987 [Epoch: 1806 Step: 00030700] Batch Recognition Loss:   0.000228 => Gls Tokens per Sec:     1435 || Batch Translation Loss:   0.042252 => Txt Tokens per Sec:     4033 || Lr: 0.000100
2024-02-02 13:05:31,347 Epoch 1806: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.81 
2024-02-02 13:05:31,347 EPOCH 1807
2024-02-02 13:05:37,953 Epoch 1807: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-02 13:05:37,954 EPOCH 1808
2024-02-02 13:05:44,612 Epoch 1808: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-02 13:05:44,612 EPOCH 1809
2024-02-02 13:05:51,818 Epoch 1809: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.89 
2024-02-02 13:05:51,818 EPOCH 1810
2024-02-02 13:05:59,273 Epoch 1810: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-02 13:05:59,274 EPOCH 1811
2024-02-02 13:06:06,054 Epoch 1811: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.80 
2024-02-02 13:06:06,055 EPOCH 1812
2024-02-02 13:06:11,209 [Epoch: 1812 Step: 00030800] Batch Recognition Loss:   0.000389 => Gls Tokens per Sec:     1566 || Batch Translation Loss:   0.021694 => Txt Tokens per Sec:     4285 || Lr: 0.000100
2024-02-02 13:06:12,485 Epoch 1812: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-02 13:06:12,485 EPOCH 1813
2024-02-02 13:06:19,405 Epoch 1813: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-02 13:06:19,406 EPOCH 1814
2024-02-02 13:06:26,091 Epoch 1814: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 13:06:26,091 EPOCH 1815
2024-02-02 13:06:33,019 Epoch 1815: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 13:06:33,020 EPOCH 1816
2024-02-02 13:06:39,695 Epoch 1816: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 13:06:39,696 EPOCH 1817
2024-02-02 13:06:46,485 Epoch 1817: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 13:06:46,485 EPOCH 1818
2024-02-02 13:06:50,981 [Epoch: 1818 Step: 00030900] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:     1511 || Batch Translation Loss:   0.013507 => Txt Tokens per Sec:     3939 || Lr: 0.000100
2024-02-02 13:06:53,311 Epoch 1818: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 13:06:53,311 EPOCH 1819
2024-02-02 13:06:59,877 Epoch 1819: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 13:06:59,878 EPOCH 1820
2024-02-02 13:07:06,763 Epoch 1820: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 13:07:06,764 EPOCH 1821
2024-02-02 13:07:13,527 Epoch 1821: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 13:07:13,528 EPOCH 1822
2024-02-02 13:07:20,101 Epoch 1822: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 13:07:20,102 EPOCH 1823
2024-02-02 13:07:26,565 Epoch 1823: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 13:07:26,566 EPOCH 1824
2024-02-02 13:07:30,801 [Epoch: 1824 Step: 00031000] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:     1301 || Batch Translation Loss:   0.020122 => Txt Tokens per Sec:     3430 || Lr: 0.000100
2024-02-02 13:07:33,344 Epoch 1824: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-02 13:07:33,345 EPOCH 1825
2024-02-02 13:07:40,263 Epoch 1825: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 13:07:40,264 EPOCH 1826
2024-02-02 13:07:46,993 Epoch 1826: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 13:07:46,994 EPOCH 1827
2024-02-02 13:07:53,509 Epoch 1827: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 13:07:53,509 EPOCH 1828
2024-02-02 13:08:00,247 Epoch 1828: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 13:08:00,248 EPOCH 1829
2024-02-02 13:08:06,961 Epoch 1829: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 13:08:06,962 EPOCH 1830
2024-02-02 13:08:11,156 [Epoch: 1830 Step: 00031100] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:     1009 || Batch Translation Loss:   0.012970 => Txt Tokens per Sec:     2886 || Lr: 0.000100
2024-02-02 13:08:13,845 Epoch 1830: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-02 13:08:13,845 EPOCH 1831
2024-02-02 13:08:20,619 Epoch 1831: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.83 
2024-02-02 13:08:20,620 EPOCH 1832
2024-02-02 13:08:27,300 Epoch 1832: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.77 
2024-02-02 13:08:27,300 EPOCH 1833
2024-02-02 13:08:33,978 Epoch 1833: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-02 13:08:33,978 EPOCH 1834
2024-02-02 13:08:40,435 Epoch 1834: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-02 13:08:40,436 EPOCH 1835
2024-02-02 13:08:47,199 Epoch 1835: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-02 13:08:47,199 EPOCH 1836
2024-02-02 13:08:48,445 [Epoch: 1836 Step: 00031200] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:     2571 || Batch Translation Loss:   0.011767 => Txt Tokens per Sec:     6679 || Lr: 0.000100
2024-02-02 13:08:54,051 Epoch 1836: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 13:08:54,051 EPOCH 1837
2024-02-02 13:09:00,798 Epoch 1837: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 13:09:00,799 EPOCH 1838
2024-02-02 13:09:07,545 Epoch 1838: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 13:09:07,546 EPOCH 1839
2024-02-02 13:09:14,418 Epoch 1839: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 13:09:14,419 EPOCH 1840
2024-02-02 13:09:21,601 Epoch 1840: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 13:09:21,602 EPOCH 1841
2024-02-02 13:09:28,273 Epoch 1841: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 13:09:28,273 EPOCH 1842
2024-02-02 13:09:30,991 [Epoch: 1842 Step: 00031300] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:      615 || Batch Translation Loss:   0.018402 => Txt Tokens per Sec:     1872 || Lr: 0.000100
2024-02-02 13:09:34,404 Epoch 1842: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 13:09:34,404 EPOCH 1843
2024-02-02 13:09:41,560 Epoch 1843: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 13:09:41,561 EPOCH 1844
2024-02-02 13:09:48,263 Epoch 1844: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 13:09:48,263 EPOCH 1845
2024-02-02 13:09:54,923 Epoch 1845: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-02 13:09:54,924 EPOCH 1846
2024-02-02 13:10:01,582 Epoch 1846: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.79 
2024-02-02 13:10:01,582 EPOCH 1847
2024-02-02 13:10:08,426 Epoch 1847: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.21 
2024-02-02 13:10:08,426 EPOCH 1848
2024-02-02 13:10:08,661 [Epoch: 1848 Step: 00031400] Batch Recognition Loss:   0.000363 => Gls Tokens per Sec:     2754 || Batch Translation Loss:   0.074904 => Txt Tokens per Sec:     7325 || Lr: 0.000100
2024-02-02 13:10:15,134 Epoch 1848: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-02 13:10:15,135 EPOCH 1849
2024-02-02 13:10:22,129 Epoch 1849: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.37 
2024-02-02 13:10:22,130 EPOCH 1850
2024-02-02 13:10:28,966 Epoch 1850: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.29 
2024-02-02 13:10:28,966 EPOCH 1851
2024-02-02 13:10:35,698 Epoch 1851: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.54 
2024-02-02 13:10:35,698 EPOCH 1852
2024-02-02 13:10:42,499 Epoch 1852: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.70 
2024-02-02 13:10:42,500 EPOCH 1853
2024-02-02 13:10:49,085 [Epoch: 1853 Step: 00031500] Batch Recognition Loss:   0.000663 => Gls Tokens per Sec:     1517 || Batch Translation Loss:   0.077582 => Txt Tokens per Sec:     4203 || Lr: 0.000100
2024-02-02 13:10:49,431 Epoch 1853: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.69 
2024-02-02 13:10:49,432 EPOCH 1854
2024-02-02 13:10:56,141 Epoch 1854: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.56 
2024-02-02 13:10:56,141 EPOCH 1855
2024-02-02 13:11:03,083 Epoch 1855: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-02 13:11:03,084 EPOCH 1856
2024-02-02 13:11:09,770 Epoch 1856: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-02 13:11:09,771 EPOCH 1857
2024-02-02 13:11:16,375 Epoch 1857: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-02 13:11:16,376 EPOCH 1858
2024-02-02 13:11:23,067 Epoch 1858: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-02 13:11:23,068 EPOCH 1859
2024-02-02 13:11:28,749 [Epoch: 1859 Step: 00031600] Batch Recognition Loss:   0.000220 => Gls Tokens per Sec:     1534 || Batch Translation Loss:   0.017745 => Txt Tokens per Sec:     4137 || Lr: 0.000100
2024-02-02 13:11:29,853 Epoch 1859: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 13:11:29,853 EPOCH 1860
2024-02-02 13:11:36,851 Epoch 1860: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 13:11:36,851 EPOCH 1861
2024-02-02 13:11:43,587 Epoch 1861: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 13:11:43,588 EPOCH 1862
2024-02-02 13:11:50,284 Epoch 1862: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 13:11:50,285 EPOCH 1863
2024-02-02 13:11:57,008 Epoch 1863: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 13:11:57,008 EPOCH 1864
2024-02-02 13:12:03,920 Epoch 1864: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 13:12:03,920 EPOCH 1865
2024-02-02 13:12:09,972 [Epoch: 1865 Step: 00031700] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:     1228 || Batch Translation Loss:   0.014544 => Txt Tokens per Sec:     3675 || Lr: 0.000100
2024-02-02 13:12:10,800 Epoch 1865: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 13:12:10,800 EPOCH 1866
2024-02-02 13:12:17,658 Epoch 1866: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 13:12:17,659 EPOCH 1867
2024-02-02 13:12:24,335 Epoch 1867: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 13:12:24,335 EPOCH 1868
2024-02-02 13:12:30,800 Epoch 1868: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 13:12:30,800 EPOCH 1869
2024-02-02 13:12:37,639 Epoch 1869: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 13:12:37,640 EPOCH 1870
2024-02-02 13:12:44,465 Epoch 1870: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 13:12:44,465 EPOCH 1871
2024-02-02 13:12:49,336 [Epoch: 1871 Step: 00031800] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     1263 || Batch Translation Loss:   0.012416 => Txt Tokens per Sec:     3436 || Lr: 0.000100
2024-02-02 13:12:51,280 Epoch 1871: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 13:12:51,281 EPOCH 1872
2024-02-02 13:12:58,105 Epoch 1872: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 13:12:58,105 EPOCH 1873
2024-02-02 13:13:04,665 Epoch 1873: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 13:13:04,665 EPOCH 1874
2024-02-02 13:13:11,284 Epoch 1874: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 13:13:11,285 EPOCH 1875
2024-02-02 13:13:18,183 Epoch 1875: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 13:13:18,184 EPOCH 1876
2024-02-02 13:13:24,433 Epoch 1876: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 13:13:24,434 EPOCH 1877
2024-02-02 13:13:26,438 [Epoch: 1877 Step: 00031900] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:     2556 || Batch Translation Loss:   0.023291 => Txt Tokens per Sec:     6755 || Lr: 0.000100
2024-02-02 13:13:31,252 Epoch 1877: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 13:13:31,252 EPOCH 1878
2024-02-02 13:13:38,052 Epoch 1878: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 13:13:38,053 EPOCH 1879
2024-02-02 13:13:44,887 Epoch 1879: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 13:13:44,888 EPOCH 1880
2024-02-02 13:13:51,781 Epoch 1880: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 13:13:51,781 EPOCH 1881
2024-02-02 13:13:58,283 Epoch 1881: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 13:13:58,283 EPOCH 1882
2024-02-02 13:14:05,007 Epoch 1882: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 13:14:05,007 EPOCH 1883
2024-02-02 13:14:08,597 [Epoch: 1883 Step: 00032000] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:     1001 || Batch Translation Loss:   0.013494 => Txt Tokens per Sec:     2615 || Lr: 0.000100
2024-02-02 13:14:39,921 Validation result at epoch 1883, step    32000: duration: 31.3237s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00029	Translation Loss: 97026.72656	PPL: 16472.86719
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.69	(BLEU-1: 11.11,	BLEU-2: 3.40,	BLEU-3: 1.39,	BLEU-4: 0.69)
	CHRF 17.06	ROUGE 9.56
2024-02-02 13:14:39,922 Logging Recognition and Translation Outputs
2024-02-02 13:14:39,922 ========================================================================================================================
2024-02-02 13:14:39,923 Logging Sequence: 86_11.00
2024-02-02 13:14:39,923 	Gloss Reference :	A B+C+D+E
2024-02-02 13:14:39,923 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 13:14:39,924 	Gloss Alignment :	         
2024-02-02 13:14:39,924 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 13:14:39,924 	Text Reference  :	he *** ***** ** ******* ** ** was 66      years old  
2024-02-02 13:14:39,924 	Text Hypothesis :	he was taken to because of it was decided to    start
2024-02-02 13:14:39,924 	Text Alignment  :	   I   I     I  I       I  I      S       S     S    
2024-02-02 13:14:39,925 ========================================================================================================================
2024-02-02 13:14:39,925 Logging Sequence: 67_16.00
2024-02-02 13:14:39,925 	Gloss Reference :	A B+C+D+E
2024-02-02 13:14:39,925 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 13:14:39,925 	Gloss Alignment :	         
2024-02-02 13:14:39,925 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 13:14:39,926 	Text Reference  :	***** ***** ** ** *** ** to  help india's fight  against the  covid-19 pandemic
2024-02-02 13:14:39,926 	Text Hypothesis :	there would be no one of his wife between neeraj has     been an       accident
2024-02-02 13:14:39,927 	Text Alignment  :	I     I     I  I  I   I  S   S    S       S      S       S    S        S       
2024-02-02 13:14:39,927 ========================================================================================================================
2024-02-02 13:14:39,927 Logging Sequence: 69_177.00
2024-02-02 13:14:39,927 	Gloss Reference :	A B+C+D+E
2024-02-02 13:14:39,927 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 13:14:39,927 	Gloss Alignment :	         
2024-02-02 13:14:39,927 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 13:14:39,929 	Text Reference  :	he said 'i will continue playing i    know it's   about time i      retire i  also have a      knee    condition
2024-02-02 13:14:39,929 	Text Hypothesis :	** **** ** **** ******** ******* when the  couple saw   a    single ball   is csk  and  mumbai indians mi       
2024-02-02 13:14:39,929 	Text Alignment  :	D  D    D  D    D        D       S    S    S      S     S    S      S      S  S    S    S      S       S        
2024-02-02 13:14:39,929 ========================================================================================================================
2024-02-02 13:14:39,929 Logging Sequence: 165_615.00
2024-02-02 13:14:39,929 	Gloss Reference :	A B+C+D+E
2024-02-02 13:14:39,930 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 13:14:39,930 	Gloss Alignment :	         
2024-02-02 13:14:39,930 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 13:14:39,930 	Text Reference  :	****** ******* we  defeated pakistan too  
2024-02-02 13:14:39,930 	Text Hypothesis :	people trolled him for      some     sleep
2024-02-02 13:14:39,930 	Text Alignment  :	I      I       S   S        S        S    
2024-02-02 13:14:39,931 ========================================================================================================================
2024-02-02 13:14:39,931 Logging Sequence: 61_5.00
2024-02-02 13:14:39,931 	Gloss Reference :	A B+C+D+E
2024-02-02 13:14:39,931 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 13:14:39,931 	Gloss Alignment :	         
2024-02-02 13:14:39,931 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 13:14:39,932 	Text Reference  :	they rivalry is seen the     most during       india pakistan cricket matches
2024-02-02 13:14:39,932 	Text Hypothesis :	**** ******* ** **** however no   confirmation has   been     made    yet    
2024-02-02 13:14:39,932 	Text Alignment  :	D    D       D  D    S       S    S            S     S        S       S      
2024-02-02 13:14:39,932 ========================================================================================================================
2024-02-02 13:14:43,325 Epoch 1883: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 13:14:43,326 EPOCH 1884
2024-02-02 13:14:49,876 Epoch 1884: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 13:14:49,877 EPOCH 1885
2024-02-02 13:14:56,793 Epoch 1885: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 13:14:56,794 EPOCH 1886
2024-02-02 13:15:03,386 Epoch 1886: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-02 13:15:03,386 EPOCH 1887
2024-02-02 13:15:10,333 Epoch 1887: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 13:15:10,333 EPOCH 1888
2024-02-02 13:15:17,110 Epoch 1888: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 13:15:17,110 EPOCH 1889
2024-02-02 13:15:18,116 [Epoch: 1889 Step: 00032100] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     2546 || Batch Translation Loss:   0.019060 => Txt Tokens per Sec:     6831 || Lr: 0.000100
2024-02-02 13:15:23,764 Epoch 1889: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 13:15:23,764 EPOCH 1890
2024-02-02 13:15:30,426 Epoch 1890: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 13:15:30,427 EPOCH 1891
2024-02-02 13:15:37,105 Epoch 1891: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-02 13:15:37,106 EPOCH 1892
2024-02-02 13:15:43,918 Epoch 1892: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.89 
2024-02-02 13:15:43,918 EPOCH 1893
2024-02-02 13:15:50,727 Epoch 1893: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.94 
2024-02-02 13:15:50,727 EPOCH 1894
2024-02-02 13:15:57,443 Epoch 1894: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-02 13:15:57,443 EPOCH 1895
2024-02-02 13:15:57,765 [Epoch: 1895 Step: 00032200] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     3992 || Batch Translation Loss:   0.028947 => Txt Tokens per Sec:     9301 || Lr: 0.000100
2024-02-02 13:16:04,329 Epoch 1895: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-02 13:16:04,330 EPOCH 1896
2024-02-02 13:16:11,289 Epoch 1896: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-02 13:16:11,290 EPOCH 1897
2024-02-02 13:16:17,775 Epoch 1897: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.01 
2024-02-02 13:16:17,776 EPOCH 1898
2024-02-02 13:16:24,683 Epoch 1898: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 13:16:24,684 EPOCH 1899
2024-02-02 13:16:31,516 Epoch 1899: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-02 13:16:31,517 EPOCH 1900
2024-02-02 13:16:38,300 [Epoch: 1900 Step: 00032300] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     1568 || Batch Translation Loss:   0.065477 => Txt Tokens per Sec:     4351 || Lr: 0.000100
2024-02-02 13:16:38,300 Epoch 1900: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-02 13:16:38,301 EPOCH 1901
2024-02-02 13:16:45,137 Epoch 1901: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-02 13:16:45,138 EPOCH 1902
2024-02-02 13:16:51,977 Epoch 1902: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 13:16:51,977 EPOCH 1903
2024-02-02 13:16:58,818 Epoch 1903: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-02 13:16:58,818 EPOCH 1904
2024-02-02 13:17:05,630 Epoch 1904: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.83 
2024-02-02 13:17:05,630 EPOCH 1905
2024-02-02 13:17:12,192 Epoch 1905: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.14 
2024-02-02 13:17:12,193 EPOCH 1906
2024-02-02 13:17:18,322 [Epoch: 1906 Step: 00032400] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     1526 || Batch Translation Loss:   0.043784 => Txt Tokens per Sec:     4246 || Lr: 0.000100
2024-02-02 13:17:19,060 Epoch 1906: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.17 
2024-02-02 13:17:19,061 EPOCH 1907
2024-02-02 13:17:25,575 Epoch 1907: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.03 
2024-02-02 13:17:25,576 EPOCH 1908
2024-02-02 13:17:32,477 Epoch 1908: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-02 13:17:32,477 EPOCH 1909
2024-02-02 13:17:39,236 Epoch 1909: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-02 13:17:39,237 EPOCH 1910
2024-02-02 13:17:45,852 Epoch 1910: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-02 13:17:45,853 EPOCH 1911
2024-02-02 13:17:52,400 Epoch 1911: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-02 13:17:52,401 EPOCH 1912
2024-02-02 13:17:58,231 [Epoch: 1912 Step: 00032500] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     1384 || Batch Translation Loss:   0.037605 => Txt Tokens per Sec:     3945 || Lr: 0.000100
2024-02-02 13:17:59,251 Epoch 1912: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-02 13:17:59,252 EPOCH 1913
2024-02-02 13:18:06,229 Epoch 1913: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-02 13:18:06,230 EPOCH 1914
2024-02-02 13:18:12,937 Epoch 1914: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 13:18:12,938 EPOCH 1915
2024-02-02 13:18:19,527 Epoch 1915: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 13:18:19,528 EPOCH 1916
2024-02-02 13:18:26,380 Epoch 1916: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 13:18:26,381 EPOCH 1917
2024-02-02 13:18:32,987 Epoch 1917: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-02 13:18:32,988 EPOCH 1918
2024-02-02 13:18:38,298 [Epoch: 1918 Step: 00032600] Batch Recognition Loss:   0.000327 => Gls Tokens per Sec:     1279 || Batch Translation Loss:   0.047476 => Txt Tokens per Sec:     3547 || Lr: 0.000100
2024-02-02 13:18:39,877 Epoch 1918: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.19 
2024-02-02 13:18:39,877 EPOCH 1919
2024-02-02 13:18:46,636 Epoch 1919: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.87 
2024-02-02 13:18:46,637 EPOCH 1920
2024-02-02 13:18:53,392 Epoch 1920: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-02 13:18:53,393 EPOCH 1921
2024-02-02 13:19:00,134 Epoch 1921: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-02 13:19:00,135 EPOCH 1922
2024-02-02 13:19:06,669 Epoch 1922: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 13:19:06,670 EPOCH 1923
2024-02-02 13:19:13,517 Epoch 1923: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 13:19:13,517 EPOCH 1924
2024-02-02 13:19:15,722 [Epoch: 1924 Step: 00032700] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     2614 || Batch Translation Loss:   0.022704 => Txt Tokens per Sec:     7161 || Lr: 0.000100
2024-02-02 13:19:20,285 Epoch 1924: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 13:19:20,286 EPOCH 1925
2024-02-02 13:19:27,124 Epoch 1925: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 13:19:27,125 EPOCH 1926
2024-02-02 13:19:33,872 Epoch 1926: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 13:19:33,872 EPOCH 1927
2024-02-02 13:19:40,525 Epoch 1927: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 13:19:40,526 EPOCH 1928
2024-02-02 13:19:47,226 Epoch 1928: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 13:19:47,226 EPOCH 1929
2024-02-02 13:19:54,131 Epoch 1929: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 13:19:54,131 EPOCH 1930
2024-02-02 13:19:57,930 [Epoch: 1930 Step: 00032800] Batch Recognition Loss:   0.000193 => Gls Tokens per Sec:     1114 || Batch Translation Loss:   0.011555 => Txt Tokens per Sec:     3088 || Lr: 0.000100
2024-02-02 13:20:00,823 Epoch 1930: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 13:20:00,823 EPOCH 1931
2024-02-02 13:20:07,454 Epoch 1931: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 13:20:07,454 EPOCH 1932
2024-02-02 13:20:14,471 Epoch 1932: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 13:20:14,472 EPOCH 1933
2024-02-02 13:20:21,086 Epoch 1933: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 13:20:21,086 EPOCH 1934
2024-02-02 13:20:27,648 Epoch 1934: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-02 13:20:27,649 EPOCH 1935
2024-02-02 13:20:34,540 Epoch 1935: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 13:20:34,541 EPOCH 1936
2024-02-02 13:20:36,036 [Epoch: 1936 Step: 00032900] Batch Recognition Loss:   0.000192 => Gls Tokens per Sec:     2141 || Batch Translation Loss:   0.045367 => Txt Tokens per Sec:     5657 || Lr: 0.000100
2024-02-02 13:20:41,551 Epoch 1936: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-02 13:20:41,552 EPOCH 1937
2024-02-02 13:20:48,310 Epoch 1937: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-02 13:20:48,310 EPOCH 1938
2024-02-02 13:20:55,212 Epoch 1938: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.78 
2024-02-02 13:20:55,212 EPOCH 1939
2024-02-02 13:21:02,080 Epoch 1939: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-02 13:21:02,080 EPOCH 1940
2024-02-02 13:21:08,538 Epoch 1940: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-02 13:21:08,539 EPOCH 1941
2024-02-02 13:21:15,226 Epoch 1941: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-02 13:21:15,227 EPOCH 1942
2024-02-02 13:21:15,894 [Epoch: 1942 Step: 00033000] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     2883 || Batch Translation Loss:   0.032672 => Txt Tokens per Sec:     7496 || Lr: 0.000100
2024-02-02 13:21:21,958 Epoch 1942: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.82 
2024-02-02 13:21:21,958 EPOCH 1943
2024-02-02 13:21:28,919 Epoch 1943: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.84 
2024-02-02 13:21:28,920 EPOCH 1944
2024-02-02 13:21:35,688 Epoch 1944: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-02 13:21:35,689 EPOCH 1945
2024-02-02 13:21:42,332 Epoch 1945: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-02 13:21:42,333 EPOCH 1946
2024-02-02 13:21:48,979 Epoch 1946: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.91 
2024-02-02 13:21:48,979 EPOCH 1947
2024-02-02 13:21:55,546 Epoch 1947: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-02 13:21:55,546 EPOCH 1948
2024-02-02 13:21:56,178 [Epoch: 1948 Step: 00033100] Batch Recognition Loss:   0.000302 => Gls Tokens per Sec:     1013 || Batch Translation Loss:   0.052712 => Txt Tokens per Sec:     3267 || Lr: 0.000100
2024-02-02 13:22:02,541 Epoch 1948: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-02 13:22:02,542 EPOCH 1949
2024-02-02 13:22:09,294 Epoch 1949: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-02 13:22:09,294 EPOCH 1950
2024-02-02 13:22:16,062 Epoch 1950: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-02 13:22:16,063 EPOCH 1951
2024-02-02 13:22:22,580 Epoch 1951: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-02 13:22:22,581 EPOCH 1952
2024-02-02 13:22:29,679 Epoch 1952: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-02 13:22:29,680 EPOCH 1953
2024-02-02 13:22:36,261 [Epoch: 1953 Step: 00033200] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     1518 || Batch Translation Loss:   0.022158 => Txt Tokens per Sec:     4218 || Lr: 0.000100
2024-02-02 13:22:36,478 Epoch 1953: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 13:22:36,478 EPOCH 1954
2024-02-02 13:22:42,687 Epoch 1954: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-02 13:22:42,688 EPOCH 1955
2024-02-02 13:22:49,623 Epoch 1955: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 13:22:49,623 EPOCH 1956
2024-02-02 13:22:56,194 Epoch 1956: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-02 13:22:56,194 EPOCH 1957
2024-02-02 13:23:02,803 Epoch 1957: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.97 
2024-02-02 13:23:02,804 EPOCH 1958
2024-02-02 13:23:09,522 Epoch 1958: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.44 
2024-02-02 13:23:09,523 EPOCH 1959
2024-02-02 13:23:15,389 [Epoch: 1959 Step: 00033300] Batch Recognition Loss:   0.000320 => Gls Tokens per Sec:     1485 || Batch Translation Loss:   0.158818 => Txt Tokens per Sec:     4138 || Lr: 0.000100
2024-02-02 13:23:16,163 Epoch 1959: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.66 
2024-02-02 13:23:16,164 EPOCH 1960
2024-02-02 13:23:22,917 Epoch 1960: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.18 
2024-02-02 13:23:22,917 EPOCH 1961
2024-02-02 13:23:29,465 Epoch 1961: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.22 
2024-02-02 13:23:29,466 EPOCH 1962
2024-02-02 13:23:36,276 Epoch 1962: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-02 13:23:36,276 EPOCH 1963
2024-02-02 13:23:43,048 Epoch 1963: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-02 13:23:43,049 EPOCH 1964
2024-02-02 13:23:49,583 Epoch 1964: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-02 13:23:49,583 EPOCH 1965
2024-02-02 13:23:55,362 [Epoch: 1965 Step: 00033400] Batch Recognition Loss:   0.000210 => Gls Tokens per Sec:     1286 || Batch Translation Loss:   0.034973 => Txt Tokens per Sec:     3782 || Lr: 0.000100
2024-02-02 13:23:56,242 Epoch 1965: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 13:23:56,242 EPOCH 1966
2024-02-02 13:24:02,820 Epoch 1966: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-02 13:24:02,821 EPOCH 1967
2024-02-02 13:24:09,730 Epoch 1967: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-02 13:24:09,731 EPOCH 1968
2024-02-02 13:24:16,689 Epoch 1968: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-02 13:24:16,689 EPOCH 1969
2024-02-02 13:24:22,964 Epoch 1969: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 13:24:22,964 EPOCH 1970
2024-02-02 13:24:29,599 Epoch 1970: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 13:24:29,600 EPOCH 1971
2024-02-02 13:24:32,913 [Epoch: 1971 Step: 00033500] Batch Recognition Loss:   0.000238 => Gls Tokens per Sec:     1932 || Batch Translation Loss:   0.028883 => Txt Tokens per Sec:     5597 || Lr: 0.000100
2024-02-02 13:24:36,506 Epoch 1971: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-02 13:24:36,506 EPOCH 1972
2024-02-02 13:24:43,150 Epoch 1972: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 13:24:43,151 EPOCH 1973
2024-02-02 13:24:49,803 Epoch 1973: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 13:24:49,804 EPOCH 1974
2024-02-02 13:24:56,658 Epoch 1974: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-02 13:24:56,659 EPOCH 1975
2024-02-02 13:25:03,365 Epoch 1975: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-02 13:25:03,366 EPOCH 1976
2024-02-02 13:25:10,103 Epoch 1976: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-02 13:25:10,103 EPOCH 1977
2024-02-02 13:25:12,084 [Epoch: 1977 Step: 00033600] Batch Recognition Loss:   0.000274 => Gls Tokens per Sec:     2586 || Batch Translation Loss:   0.061935 => Txt Tokens per Sec:     7524 || Lr: 0.000100
2024-02-02 13:25:16,140 Epoch 1977: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 13:25:16,141 EPOCH 1978
2024-02-02 13:25:22,706 Epoch 1978: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 13:25:22,707 EPOCH 1979
2024-02-02 13:25:29,490 Epoch 1979: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 13:25:29,491 EPOCH 1980
2024-02-02 13:25:36,159 Epoch 1980: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 13:25:36,160 EPOCH 1981
2024-02-02 13:25:42,987 Epoch 1981: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 13:25:42,988 EPOCH 1982
2024-02-02 13:25:49,897 Epoch 1982: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 13:25:49,897 EPOCH 1983
2024-02-02 13:25:51,521 [Epoch: 1983 Step: 00033700] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:     2365 || Batch Translation Loss:   0.015637 => Txt Tokens per Sec:     6684 || Lr: 0.000100
2024-02-02 13:25:56,667 Epoch 1983: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 13:25:56,667 EPOCH 1984
2024-02-02 13:26:03,356 Epoch 1984: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 13:26:03,357 EPOCH 1985
2024-02-02 13:26:10,004 Epoch 1985: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 13:26:10,006 EPOCH 1986
2024-02-02 13:26:16,838 Epoch 1986: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 13:26:16,839 EPOCH 1987
2024-02-02 13:26:23,951 Epoch 1987: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-02 13:26:23,952 EPOCH 1988
2024-02-02 13:26:30,653 Epoch 1988: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-02 13:26:30,653 EPOCH 1989
2024-02-02 13:26:31,670 [Epoch: 1989 Step: 00033800] Batch Recognition Loss:   0.000166 => Gls Tokens per Sec:     2520 || Batch Translation Loss:   0.015134 => Txt Tokens per Sec:     7035 || Lr: 0.000100
2024-02-02 13:26:37,395 Epoch 1989: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-02 13:26:37,395 EPOCH 1990
2024-02-02 13:26:44,327 Epoch 1990: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-02 13:26:44,328 EPOCH 1991
2024-02-02 13:26:50,889 Epoch 1991: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-02 13:26:50,889 EPOCH 1992
2024-02-02 13:26:57,860 Epoch 1992: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.00 
2024-02-02 13:26:57,861 EPOCH 1993
2024-02-02 13:27:04,516 Epoch 1993: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.87 
2024-02-02 13:27:04,516 EPOCH 1994
2024-02-02 13:27:11,242 Epoch 1994: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-02 13:27:11,243 EPOCH 1995
2024-02-02 13:27:11,881 [Epoch: 1995 Step: 00033900] Batch Recognition Loss:   0.000361 => Gls Tokens per Sec:     2009 || Batch Translation Loss:   0.054720 => Txt Tokens per Sec:     6363 || Lr: 0.000100
2024-02-02 13:27:17,754 Epoch 1995: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-02 13:27:17,754 EPOCH 1996
2024-02-02 13:27:24,358 Epoch 1996: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.90 
2024-02-02 13:27:24,359 EPOCH 1997
2024-02-02 13:27:31,143 Epoch 1997: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-02 13:27:31,144 EPOCH 1998
2024-02-02 13:27:37,761 Epoch 1998: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.50 
2024-02-02 13:27:37,762 EPOCH 1999
2024-02-02 13:27:44,499 Epoch 1999: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.24 
2024-02-02 13:27:44,500 EPOCH 2000
2024-02-02 13:27:51,329 [Epoch: 2000 Step: 00034000] Batch Recognition Loss:   0.000814 => Gls Tokens per Sec:     1557 || Batch Translation Loss:   0.063708 => Txt Tokens per Sec:     4321 || Lr: 0.000100
2024-02-02 13:28:23,290 Validation result at epoch 2000, step    34000: duration: 31.9596s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00159	Translation Loss: 96410.71875	PPL: 15488.06934
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.55	(BLEU-1: 9.62,	BLEU-2: 2.91,	BLEU-3: 1.18,	BLEU-4: 0.55)
	CHRF 16.29	ROUGE 8.55
2024-02-02 13:28:23,291 Logging Recognition and Translation Outputs
2024-02-02 13:28:23,291 ========================================================================================================================
2024-02-02 13:28:23,291 Logging Sequence: 92_199.00
2024-02-02 13:28:23,292 	Gloss Reference :	A B+C+D+E
2024-02-02 13:28:23,292 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 13:28:23,292 	Gloss Alignment :	         
2024-02-02 13:28:23,292 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 13:28:23,293 	Text Reference  :	people on social media said    that  
2024-02-02 13:28:23,293 	Text Hypothesis :	****** ** and    was   amazing player
2024-02-02 13:28:23,293 	Text Alignment  :	D      D  S      S     S       S     
2024-02-02 13:28:23,293 ========================================================================================================================
2024-02-02 13:28:23,293 Logging Sequence: 109_64.00
2024-02-02 13:28:23,294 	Gloss Reference :	A B+C+D+E
2024-02-02 13:28:23,294 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 13:28:23,294 	Gloss Alignment :	         
2024-02-02 13:28:23,294 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 13:28:23,295 	Text Reference  :	** the 2 players as    well    as the ****** entire kkr  team have been quarantined
2024-02-02 13:28:23,295 	Text Hypothesis :	on the * ******* third offence in the season a      fine of   rs   30   lakh       
2024-02-02 13:28:23,295 	Text Alignment  :	I      D D       S     S       S      I      S      S    S    S    S    S          
2024-02-02 13:28:23,296 ========================================================================================================================
2024-02-02 13:28:23,296 Logging Sequence: 84_108.00
2024-02-02 13:28:23,296 	Gloss Reference :	A B+C+D+E
2024-02-02 13:28:23,296 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 13:28:23,296 	Gloss Alignment :	         
2024-02-02 13:28:23,296 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 13:28:23,298 	Text Reference  :	so in order to show their protest they  covered their mouth  in the photos which then   went viral    
2024-02-02 13:28:23,298 	Text Hypothesis :	** ** ***** ** the  news  went    viral and     the   number of the ****** ***** people were surprised
2024-02-02 13:28:23,298 	Text Alignment  :	D  D  D     D  S    S     S       S     S       S     S      S      D      D     S      S    S        
2024-02-02 13:28:23,298 ========================================================================================================================
2024-02-02 13:28:23,298 Logging Sequence: 115_24.00
2024-02-02 13:28:23,298 	Gloss Reference :	A B+C+D+E
2024-02-02 13:28:23,298 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 13:28:23,299 	Gloss Alignment :	         
2024-02-02 13:28:23,299 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 13:28:23,299 	Text Reference  :	bumrah also did not  participate in       the  5    match   t20 series
2024-02-02 13:28:23,299 	Text Hypothesis :	****** **** *** this was         reported that deaf cricket was always
2024-02-02 13:28:23,300 	Text Alignment  :	D      D    D   S    S           S        S    S    S       S   S     
2024-02-02 13:28:23,300 ========================================================================================================================
2024-02-02 13:28:23,300 Logging Sequence: 96_129.00
2024-02-02 13:28:23,300 	Gloss Reference :	A B+C+D+E
2024-02-02 13:28:23,300 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 13:28:23,300 	Gloss Alignment :	         
2024-02-02 13:28:23,300 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 13:28:23,301 	Text Reference  :	** ** ** **** **** *** ** **** viewers were very stressed
2024-02-02 13:28:23,301 	Text Hypothesis :	it is no hard work who is have to      wait and  watch   
2024-02-02 13:28:23,301 	Text Alignment  :	I  I  I  I    I    I   I  I    S       S    S    S       
2024-02-02 13:28:23,301 ========================================================================================================================
2024-02-02 13:28:23,305 Epoch 2000: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-02 13:28:23,305 EPOCH 2001
2024-02-02 13:28:30,155 Epoch 2001: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-02 13:28:30,156 EPOCH 2002
2024-02-02 13:28:36,780 Epoch 2002: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-02 13:28:36,780 EPOCH 2003
2024-02-02 13:28:43,400 Epoch 2003: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-02 13:28:43,400 EPOCH 2004
2024-02-02 13:28:50,474 Epoch 2004: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 13:28:50,474 EPOCH 2005
2024-02-02 13:28:57,574 Epoch 2005: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 13:28:57,575 EPOCH 2006
2024-02-02 13:29:03,788 [Epoch: 2006 Step: 00034100] Batch Recognition Loss:   0.000224 => Gls Tokens per Sec:     1505 || Batch Translation Loss:   0.024594 => Txt Tokens per Sec:     4142 || Lr: 0.000100
2024-02-02 13:29:04,449 Epoch 2006: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 13:29:04,449 EPOCH 2007
2024-02-02 13:29:11,277 Epoch 2007: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 13:29:11,278 EPOCH 2008
2024-02-02 13:29:18,159 Epoch 2008: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 13:29:18,160 EPOCH 2009
2024-02-02 13:29:24,830 Epoch 2009: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 13:29:24,830 EPOCH 2010
2024-02-02 13:29:31,575 Epoch 2010: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-02 13:29:31,576 EPOCH 2011
2024-02-02 13:29:38,289 Epoch 2011: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-02 13:29:38,289 EPOCH 2012
2024-02-02 13:29:44,066 [Epoch: 2012 Step: 00034200] Batch Recognition Loss:   0.000115 => Gls Tokens per Sec:     1398 || Batch Translation Loss:   0.024073 => Txt Tokens per Sec:     3953 || Lr: 0.000100
2024-02-02 13:29:44,948 Epoch 2012: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-02 13:29:44,949 EPOCH 2013
2024-02-02 13:29:51,514 Epoch 2013: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 13:29:51,514 EPOCH 2014
2024-02-02 13:29:58,225 Epoch 2014: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 13:29:58,226 EPOCH 2015
2024-02-02 13:30:05,167 Epoch 2015: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 13:30:05,167 EPOCH 2016
2024-02-02 13:30:11,868 Epoch 2016: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 13:30:11,868 EPOCH 2017
2024-02-02 13:30:18,656 Epoch 2017: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-02 13:30:18,656 EPOCH 2018
2024-02-02 13:30:24,291 [Epoch: 2018 Step: 00034300] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     1205 || Batch Translation Loss:   0.019249 => Txt Tokens per Sec:     3712 || Lr: 0.000100
2024-02-02 13:30:25,459 Epoch 2018: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 13:30:25,459 EPOCH 2019
2024-02-02 13:30:32,274 Epoch 2019: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-02 13:30:32,274 EPOCH 2020
2024-02-02 13:30:39,132 Epoch 2020: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-02 13:30:39,132 EPOCH 2021
2024-02-02 13:30:45,757 Epoch 2021: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-02 13:30:45,757 EPOCH 2022
2024-02-02 13:30:52,458 Epoch 2022: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-02 13:30:52,458 EPOCH 2023
2024-02-02 13:30:58,916 Epoch 2023: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-02 13:30:58,917 EPOCH 2024
2024-02-02 13:31:02,766 [Epoch: 2024 Step: 00034400] Batch Recognition Loss:   0.000245 => Gls Tokens per Sec:     1432 || Batch Translation Loss:   0.063068 => Txt Tokens per Sec:     3617 || Lr: 0.000100
2024-02-02 13:31:05,697 Epoch 2024: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-02 13:31:05,698 EPOCH 2025
2024-02-02 13:31:12,564 Epoch 2025: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-02 13:31:12,565 EPOCH 2026
2024-02-02 13:31:19,358 Epoch 2026: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 13:31:19,358 EPOCH 2027
2024-02-02 13:31:25,954 Epoch 2027: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 13:31:25,954 EPOCH 2028
2024-02-02 13:31:32,883 Epoch 2028: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 13:31:32,884 EPOCH 2029
2024-02-02 13:31:39,361 Epoch 2029: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 13:31:39,361 EPOCH 2030
2024-02-02 13:31:43,771 [Epoch: 2030 Step: 00034500] Batch Recognition Loss:   0.000191 => Gls Tokens per Sec:      959 || Batch Translation Loss:   0.027861 => Txt Tokens per Sec:     2971 || Lr: 0.000100
2024-02-02 13:31:46,116 Epoch 2030: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 13:31:46,117 EPOCH 2031
2024-02-02 13:31:52,881 Epoch 2031: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 13:31:52,881 EPOCH 2032
2024-02-02 13:32:00,067 Epoch 2032: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 13:32:00,068 EPOCH 2033
2024-02-02 13:32:06,859 Epoch 2033: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.71 
2024-02-02 13:32:06,859 EPOCH 2034
2024-02-02 13:32:13,436 Epoch 2034: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-02 13:32:13,436 EPOCH 2035
2024-02-02 13:32:20,156 Epoch 2035: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 13:32:20,156 EPOCH 2036
2024-02-02 13:32:23,565 [Epoch: 2036 Step: 00034600] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:      866 || Batch Translation Loss:   0.009601 => Txt Tokens per Sec:     2523 || Lr: 0.000100
2024-02-02 13:32:26,563 Epoch 2036: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.70 
2024-02-02 13:32:26,563 EPOCH 2037
2024-02-02 13:32:32,528 Epoch 2037: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-02 13:32:32,529 EPOCH 2038
2024-02-02 13:32:39,183 Epoch 2038: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-02 13:32:39,184 EPOCH 2039
2024-02-02 13:32:46,005 Epoch 2039: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 13:32:46,006 EPOCH 2040
2024-02-02 13:32:52,689 Epoch 2040: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 13:32:52,689 EPOCH 2041
2024-02-02 13:32:59,443 Epoch 2041: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 13:32:59,444 EPOCH 2042
2024-02-02 13:33:00,601 [Epoch: 2042 Step: 00034700] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     1660 || Batch Translation Loss:   0.020158 => Txt Tokens per Sec:     4786 || Lr: 0.000100
2024-02-02 13:33:06,314 Epoch 2042: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 13:33:06,314 EPOCH 2043
2024-02-02 13:33:13,100 Epoch 2043: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 13:33:13,101 EPOCH 2044
2024-02-02 13:33:19,734 Epoch 2044: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 13:33:19,735 EPOCH 2045
2024-02-02 13:33:26,592 Epoch 2045: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-02 13:33:26,593 EPOCH 2046
2024-02-02 13:33:33,269 Epoch 2046: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-02 13:33:33,269 EPOCH 2047
2024-02-02 13:33:40,104 Epoch 2047: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-02 13:33:40,105 EPOCH 2048
2024-02-02 13:33:40,236 [Epoch: 2048 Step: 00034800] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:     4923 || Batch Translation Loss:   0.017788 => Txt Tokens per Sec:     9800 || Lr: 0.000100
2024-02-02 13:33:46,903 Epoch 2048: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.96 
2024-02-02 13:33:46,904 EPOCH 2049
2024-02-02 13:33:53,498 Epoch 2049: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.80 
2024-02-02 13:33:53,498 EPOCH 2050
2024-02-02 13:34:00,225 Epoch 2050: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-02 13:34:00,226 EPOCH 2051
2024-02-02 13:34:06,929 Epoch 2051: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-02 13:34:06,929 EPOCH 2052
2024-02-02 13:34:13,920 Epoch 2052: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-02 13:34:13,920 EPOCH 2053
2024-02-02 13:34:20,491 [Epoch: 2053 Step: 00034900] Batch Recognition Loss:   0.000326 => Gls Tokens per Sec:     1521 || Batch Translation Loss:   0.029530 => Txt Tokens per Sec:     4208 || Lr: 0.000100
2024-02-02 13:34:20,819 Epoch 2053: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-02 13:34:20,819 EPOCH 2054
2024-02-02 13:34:27,508 Epoch 2054: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 13:34:27,508 EPOCH 2055
2024-02-02 13:34:33,444 Epoch 2055: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-02 13:34:33,444 EPOCH 2056
2024-02-02 13:34:39,999 Epoch 2056: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 13:34:40,000 EPOCH 2057
2024-02-02 13:34:46,726 Epoch 2057: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 13:34:46,726 EPOCH 2058
2024-02-02 13:34:53,281 Epoch 2058: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 13:34:53,282 EPOCH 2059
2024-02-02 13:34:59,248 [Epoch: 2059 Step: 00035000] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     1460 || Batch Translation Loss:   0.020244 => Txt Tokens per Sec:     4011 || Lr: 0.000100
2024-02-02 13:35:00,286 Epoch 2059: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 13:35:00,286 EPOCH 2060
2024-02-02 13:35:07,001 Epoch 2060: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 13:35:07,002 EPOCH 2061
2024-02-02 13:35:13,863 Epoch 2061: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 13:35:13,863 EPOCH 2062
2024-02-02 13:35:20,420 Epoch 2062: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 13:35:20,420 EPOCH 2063
2024-02-02 13:35:27,265 Epoch 2063: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 13:35:27,266 EPOCH 2064
2024-02-02 13:35:33,958 Epoch 2064: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 13:35:33,958 EPOCH 2065
2024-02-02 13:35:39,288 [Epoch: 2065 Step: 00035100] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:     1394 || Batch Translation Loss:   0.011779 => Txt Tokens per Sec:     3851 || Lr: 0.000100
2024-02-02 13:35:40,650 Epoch 2065: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 13:35:40,651 EPOCH 2066
2024-02-02 13:35:47,240 Epoch 2066: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 13:35:47,241 EPOCH 2067
2024-02-02 13:35:53,977 Epoch 2067: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 13:35:53,978 EPOCH 2068
2024-02-02 13:36:00,876 Epoch 2068: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 13:36:00,877 EPOCH 2069
2024-02-02 13:36:07,694 Epoch 2069: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 13:36:07,694 EPOCH 2070
2024-02-02 13:36:14,333 Epoch 2070: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 13:36:14,334 EPOCH 2071
2024-02-02 13:36:19,196 [Epoch: 2071 Step: 00035200] Batch Recognition Loss:   0.000122 => Gls Tokens per Sec:     1265 || Batch Translation Loss:   0.012467 => Txt Tokens per Sec:     3628 || Lr: 0.000100
2024-02-02 13:36:20,999 Epoch 2071: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-02 13:36:21,000 EPOCH 2072
2024-02-02 13:36:27,636 Epoch 2072: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-02 13:36:27,637 EPOCH 2073
2024-02-02 13:36:34,501 Epoch 2073: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 13:36:34,502 EPOCH 2074
2024-02-02 13:36:41,408 Epoch 2074: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-02 13:36:41,409 EPOCH 2075
2024-02-02 13:36:48,159 Epoch 2075: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.78 
2024-02-02 13:36:48,160 EPOCH 2076
2024-02-02 13:36:54,718 Epoch 2076: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.82 
2024-02-02 13:36:54,719 EPOCH 2077
2024-02-02 13:36:59,069 [Epoch: 2077 Step: 00035300] Batch Recognition Loss:   0.001093 => Gls Tokens per Sec:     1120 || Batch Translation Loss:   0.045431 => Txt Tokens per Sec:     3114 || Lr: 0.000100
2024-02-02 13:37:01,414 Epoch 2077: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.53 
2024-02-02 13:37:01,414 EPOCH 2078
2024-02-02 13:37:08,222 Epoch 2078: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.75 
2024-02-02 13:37:08,223 EPOCH 2079
2024-02-02 13:37:14,901 Epoch 2079: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.85 
2024-02-02 13:37:14,901 EPOCH 2080
2024-02-02 13:37:21,413 Epoch 2080: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.04 
2024-02-02 13:37:21,413 EPOCH 2081
2024-02-02 13:37:28,252 Epoch 2081: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.68 
2024-02-02 13:37:28,253 EPOCH 2082
2024-02-02 13:37:35,004 Epoch 2082: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.16 
2024-02-02 13:37:35,004 EPOCH 2083
2024-02-02 13:37:36,584 [Epoch: 2083 Step: 00035400] Batch Recognition Loss:   0.000559 => Gls Tokens per Sec:     2431 || Batch Translation Loss:   0.119763 => Txt Tokens per Sec:     7084 || Lr: 0.000100
2024-02-02 13:37:41,516 Epoch 2083: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.56 
2024-02-02 13:37:41,517 EPOCH 2084
2024-02-02 13:37:48,328 Epoch 2084: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.46 
2024-02-02 13:37:48,329 EPOCH 2085
2024-02-02 13:37:55,175 Epoch 2085: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-02 13:37:55,175 EPOCH 2086
2024-02-02 13:38:02,090 Epoch 2086: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-02 13:38:02,091 EPOCH 2087
2024-02-02 13:38:08,709 Epoch 2087: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 13:38:08,709 EPOCH 2088
2024-02-02 13:38:15,604 Epoch 2088: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 13:38:15,605 EPOCH 2089
2024-02-02 13:38:16,390 [Epoch: 2089 Step: 00035500] Batch Recognition Loss:   0.000180 => Gls Tokens per Sec:     3264 || Batch Translation Loss:   0.015656 => Txt Tokens per Sec:     8294 || Lr: 0.000100
2024-02-02 13:38:21,979 Epoch 2089: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 13:38:21,980 EPOCH 2090
2024-02-02 13:38:28,812 Epoch 2090: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 13:38:28,813 EPOCH 2091
2024-02-02 13:38:35,661 Epoch 2091: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 13:38:35,661 EPOCH 2092
2024-02-02 13:38:42,487 Epoch 2092: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 13:38:42,488 EPOCH 2093
2024-02-02 13:38:49,246 Epoch 2093: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 13:38:49,247 EPOCH 2094
2024-02-02 13:38:56,170 Epoch 2094: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 13:38:56,171 EPOCH 2095
2024-02-02 13:38:56,912 [Epoch: 2095 Step: 00035600] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     1732 || Batch Translation Loss:   0.018025 => Txt Tokens per Sec:     5039 || Lr: 0.000100
2024-02-02 13:39:02,942 Epoch 2095: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 13:39:02,942 EPOCH 2096
2024-02-02 13:39:09,564 Epoch 2096: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 13:39:09,564 EPOCH 2097
2024-02-02 13:39:16,030 Epoch 2097: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 13:39:16,031 EPOCH 2098
2024-02-02 13:39:22,996 Epoch 2098: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 13:39:22,997 EPOCH 2099
2024-02-02 13:39:29,855 Epoch 2099: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 13:39:29,855 EPOCH 2100
2024-02-02 13:39:36,728 [Epoch: 2100 Step: 00035700] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:     1547 || Batch Translation Loss:   0.013829 => Txt Tokens per Sec:     4294 || Lr: 0.000100
2024-02-02 13:39:36,729 Epoch 2100: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 13:39:36,729 EPOCH 2101
2024-02-02 13:39:43,318 Epoch 2101: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 13:39:43,318 EPOCH 2102
2024-02-02 13:39:50,413 Epoch 2102: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 13:39:50,413 EPOCH 2103
2024-02-02 13:39:57,326 Epoch 2103: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 13:39:57,326 EPOCH 2104
2024-02-02 13:40:04,165 Epoch 2104: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 13:40:04,166 EPOCH 2105
2024-02-02 13:40:11,048 Epoch 2105: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 13:40:11,049 EPOCH 2106
2024-02-02 13:40:16,995 [Epoch: 2106 Step: 00035800] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:     1573 || Batch Translation Loss:   0.038728 => Txt Tokens per Sec:     4410 || Lr: 0.000100
2024-02-02 13:40:17,678 Epoch 2106: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 13:40:17,679 EPOCH 2107
2024-02-02 13:40:24,462 Epoch 2107: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 13:40:24,462 EPOCH 2108
2024-02-02 13:40:31,247 Epoch 2108: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 13:40:31,247 EPOCH 2109
2024-02-02 13:40:37,820 Epoch 2109: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 13:40:37,821 EPOCH 2110
2024-02-02 13:40:44,615 Epoch 2110: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 13:40:44,616 EPOCH 2111
2024-02-02 13:40:51,147 Epoch 2111: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 13:40:51,147 EPOCH 2112
2024-02-02 13:40:55,022 [Epoch: 2112 Step: 00035900] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     2147 || Batch Translation Loss:   0.024095 => Txt Tokens per Sec:     5977 || Lr: 0.000100
2024-02-02 13:40:57,906 Epoch 2112: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 13:40:57,907 EPOCH 2113
2024-02-02 13:41:04,746 Epoch 2113: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 13:41:04,747 EPOCH 2114
2024-02-02 13:41:11,587 Epoch 2114: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 13:41:11,588 EPOCH 2115
2024-02-02 13:41:18,194 Epoch 2115: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 13:41:18,195 EPOCH 2116
2024-02-02 13:41:24,972 Epoch 2116: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 13:41:24,973 EPOCH 2117
2024-02-02 13:41:31,757 Epoch 2117: Total Training Recognition Loss 0.00  Total Training Translation Loss 2.07 
2024-02-02 13:41:31,757 EPOCH 2118
2024-02-02 13:41:34,487 [Epoch: 2118 Step: 00036000] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:     2580 || Batch Translation Loss:   0.112235 => Txt Tokens per Sec:     6724 || Lr: 0.000100
2024-02-02 13:42:04,864 Validation result at epoch 2118, step    36000: duration: 30.3763s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00298	Translation Loss: 97964.39062	PPL: 18093.38867
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.54	(BLEU-1: 8.34,	BLEU-2: 2.52,	BLEU-3: 1.05,	BLEU-4: 0.54)
	CHRF 14.42	ROUGE 8.28
2024-02-02 13:42:04,865 Logging Recognition and Translation Outputs
2024-02-02 13:42:04,865 ========================================================================================================================
2024-02-02 13:42:04,866 Logging Sequence: 78_198.00
2024-02-02 13:42:04,866 	Gloss Reference :	A B+C+D+E
2024-02-02 13:42:04,866 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 13:42:04,867 	Gloss Alignment :	         
2024-02-02 13:42:04,867 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 13:42:04,867 	Text Reference  :	*** they      have been  flooded        with congratulations comments
2024-02-02 13:42:04,868 	Text Hypothesis :	and thousands of   other advertisements with the             matches 
2024-02-02 13:42:04,868 	Text Alignment  :	I   S         S    S     S                   S               S       
2024-02-02 13:42:04,868 ========================================================================================================================
2024-02-02 13:42:04,868 Logging Sequence: 145_216.00
2024-02-02 13:42:04,868 	Gloss Reference :	A B+C+D+E
2024-02-02 13:42:04,868 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 13:42:04,868 	Gloss Alignment :	         
2024-02-02 13:42:04,869 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 13:42:04,869 	Text Reference  :	asking him to include sameeha in  the world championship as she was a    talented athlete
2024-02-02 13:42:04,870 	Text Hypothesis :	****** *** ** which   is      why the ***** ************ ** *** *** team created  history
2024-02-02 13:42:04,870 	Text Alignment  :	D      D   D  S       S       S       D     D            D  D   D   S    S        S      
2024-02-02 13:42:04,870 ========================================================================================================================
2024-02-02 13:42:04,870 Logging Sequence: 70_137.00
2024-02-02 13:42:04,870 	Gloss Reference :	A B+C+D+E
2024-02-02 13:42:04,870 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 13:42:04,870 	Gloss Alignment :	         
2024-02-02 13:42:04,870 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 13:42:04,871 	Text Reference  :	the small gesture appeared to encourage people to  drink water instead of  aerated drinks  
2024-02-02 13:42:04,871 	Text Hypothesis :	*** ***** ******* ******** ** and       behind him win   he    helped  her by      covid-19
2024-02-02 13:42:04,872 	Text Alignment  :	D   D     D       D        D  S         S      S   S     S     S       S   S       S       
2024-02-02 13:42:04,872 ========================================================================================================================
2024-02-02 13:42:04,872 Logging Sequence: 119_20.00
2024-02-02 13:42:04,872 	Gloss Reference :	A B+C+D+E
2024-02-02 13:42:04,872 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 13:42:04,872 	Gloss Alignment :	         
2024-02-02 13:42:04,872 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 13:42:04,873 	Text Reference  :	messi intended to gift something to all the players and the    staff  to     special to  celebrate the moment   
2024-02-02 13:42:04,874 	Text Hypothesis :	***** ******** ** **** ********* ** *** the ******* *** indian team's defeat 2       men did       not announced
2024-02-02 13:42:04,874 	Text Alignment  :	D     D        D  D    D         D  D       D       D   S      S      S      S       S   S         S   S        
2024-02-02 13:42:04,874 ========================================================================================================================
2024-02-02 13:42:04,874 Logging Sequence: 106_15.00
2024-02-02 13:42:04,874 	Gloss Reference :	A B+C+D+E
2024-02-02 13:42:04,874 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 13:42:04,874 	Gloss Alignment :	         
2024-02-02 13:42:04,874 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 13:42:04,875 	Text Reference  :	but what about women's cricket earlier we  never spoke   about it     
2024-02-02 13:42:04,875 	Text Hypothesis :	*** **** ***** ******* ******* ******* and three formats of    matches
2024-02-02 13:42:04,875 	Text Alignment  :	D   D    D     D       D       D       S   S     S       S     S      
2024-02-02 13:42:04,875 ========================================================================================================================
2024-02-02 13:42:09,125 Epoch 2118: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.27 
2024-02-02 13:42:09,125 EPOCH 2119
2024-02-02 13:42:15,933 Epoch 2119: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.43 
2024-02-02 13:42:15,934 EPOCH 2120
2024-02-02 13:42:22,475 Epoch 2120: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.19 
2024-02-02 13:42:22,476 EPOCH 2121
2024-02-02 13:42:28,908 Epoch 2121: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-02 13:42:28,909 EPOCH 2122
2024-02-02 13:42:35,610 Epoch 2122: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-02 13:42:35,611 EPOCH 2123
2024-02-02 13:42:42,411 Epoch 2123: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 13:42:42,412 EPOCH 2124
2024-02-02 13:42:47,106 [Epoch: 2124 Step: 00036100] Batch Recognition Loss:   0.000253 => Gls Tokens per Sec:     1174 || Batch Translation Loss:   0.025772 => Txt Tokens per Sec:     3281 || Lr: 0.000100
2024-02-02 13:42:49,132 Epoch 2124: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 13:42:49,132 EPOCH 2125
2024-02-02 13:42:55,858 Epoch 2125: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 13:42:55,859 EPOCH 2126
2024-02-02 13:43:02,641 Epoch 2126: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 13:43:02,642 EPOCH 2127
2024-02-02 13:43:09,511 Epoch 2127: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 13:43:09,512 EPOCH 2128
2024-02-02 13:43:15,957 Epoch 2128: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-02 13:43:15,958 EPOCH 2129
2024-02-02 13:43:22,635 Epoch 2129: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 13:43:22,635 EPOCH 2130
2024-02-02 13:43:26,290 [Epoch: 2130 Step: 00036200] Batch Recognition Loss:   0.000129 => Gls Tokens per Sec:     1158 || Batch Translation Loss:   0.013937 => Txt Tokens per Sec:     3022 || Lr: 0.000100
2024-02-02 13:43:29,411 Epoch 2130: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 13:43:29,411 EPOCH 2131
2024-02-02 13:43:35,763 Epoch 2131: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 13:43:35,764 EPOCH 2132
2024-02-02 13:43:42,535 Epoch 2132: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 13:43:42,535 EPOCH 2133
2024-02-02 13:43:49,458 Epoch 2133: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 13:43:49,459 EPOCH 2134
2024-02-02 13:43:56,252 Epoch 2134: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 13:43:56,252 EPOCH 2135
2024-02-02 13:44:02,833 Epoch 2135: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 13:44:02,834 EPOCH 2136
2024-02-02 13:44:04,689 [Epoch: 2136 Step: 00036300] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     1726 || Batch Translation Loss:   0.013447 => Txt Tokens per Sec:     5167 || Lr: 0.000100
2024-02-02 13:44:09,751 Epoch 2136: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 13:44:09,751 EPOCH 2137
2024-02-02 13:44:16,568 Epoch 2137: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 13:44:16,568 EPOCH 2138
2024-02-02 13:44:23,268 Epoch 2138: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 13:44:23,269 EPOCH 2139
2024-02-02 13:44:30,054 Epoch 2139: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 13:44:30,055 EPOCH 2140
2024-02-02 13:44:36,663 Epoch 2140: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 13:44:36,664 EPOCH 2141
2024-02-02 13:44:43,348 Epoch 2141: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 13:44:43,349 EPOCH 2142
2024-02-02 13:44:46,042 [Epoch: 2142 Step: 00036400] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:      621 || Batch Translation Loss:   0.010126 => Txt Tokens per Sec:     1604 || Lr: 0.000100
2024-02-02 13:44:50,247 Epoch 2142: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 13:44:50,247 EPOCH 2143
2024-02-02 13:44:57,200 Epoch 2143: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 13:44:57,201 EPOCH 2144
2024-02-02 13:45:03,945 Epoch 2144: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 13:45:03,945 EPOCH 2145
2024-02-02 13:45:10,489 Epoch 2145: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 13:45:10,490 EPOCH 2146
2024-02-02 13:45:17,357 Epoch 2146: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 13:45:17,357 EPOCH 2147
2024-02-02 13:45:24,204 Epoch 2147: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 13:45:24,204 EPOCH 2148
2024-02-02 13:45:24,418 [Epoch: 2148 Step: 00036500] Batch Recognition Loss:   0.000148 => Gls Tokens per Sec:     3019 || Batch Translation Loss:   0.011494 => Txt Tokens per Sec:     9118 || Lr: 0.000100
2024-02-02 13:45:31,016 Epoch 2148: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 13:45:31,017 EPOCH 2149
2024-02-02 13:45:37,715 Epoch 2149: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 13:45:37,716 EPOCH 2150
2024-02-02 13:45:44,370 Epoch 2150: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 13:45:44,371 EPOCH 2151
2024-02-02 13:45:51,016 Epoch 2151: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 13:45:51,016 EPOCH 2152
2024-02-02 13:45:58,077 Epoch 2152: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 13:45:58,078 EPOCH 2153
2024-02-02 13:46:04,669 [Epoch: 2153 Step: 00036600] Batch Recognition Loss:   0.000106 => Gls Tokens per Sec:     1516 || Batch Translation Loss:   0.012472 => Txt Tokens per Sec:     4286 || Lr: 0.000100
2024-02-02 13:46:04,805 Epoch 2153: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 13:46:04,805 EPOCH 2154
2024-02-02 13:46:11,370 Epoch 2154: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 13:46:11,370 EPOCH 2155
2024-02-02 13:46:17,915 Epoch 2155: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 13:46:17,916 EPOCH 2156
2024-02-02 13:46:24,663 Epoch 2156: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 13:46:24,663 EPOCH 2157
2024-02-02 13:46:31,802 Epoch 2157: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 13:46:31,803 EPOCH 2158
2024-02-02 13:46:38,680 Epoch 2158: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 13:46:38,681 EPOCH 2159
2024-02-02 13:46:44,837 [Epoch: 2159 Step: 00036700] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     1415 || Batch Translation Loss:   0.023066 => Txt Tokens per Sec:     4066 || Lr: 0.000100
2024-02-02 13:46:45,390 Epoch 2159: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 13:46:45,390 EPOCH 2160
2024-02-02 13:46:52,137 Epoch 2160: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 13:46:52,138 EPOCH 2161
2024-02-02 13:46:58,783 Epoch 2161: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 13:46:58,784 EPOCH 2162
2024-02-02 13:47:05,703 Epoch 2162: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-02 13:47:05,704 EPOCH 2163
2024-02-02 13:47:12,408 Epoch 2163: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.33 
2024-02-02 13:47:12,408 EPOCH 2164
2024-02-02 13:47:18,915 Epoch 2164: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.54 
2024-02-02 13:47:18,916 EPOCH 2165
2024-02-02 13:47:21,699 [Epoch: 2165 Step: 00036800] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:     2760 || Batch Translation Loss:   0.050509 => Txt Tokens per Sec:     7314 || Lr: 0.000100
2024-02-02 13:47:25,512 Epoch 2165: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.17 
2024-02-02 13:47:25,513 EPOCH 2166
2024-02-02 13:47:32,220 Epoch 2166: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.73 
2024-02-02 13:47:32,220 EPOCH 2167
2024-02-02 13:47:39,240 Epoch 2167: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.64 
2024-02-02 13:47:39,240 EPOCH 2168
2024-02-02 13:47:46,095 Epoch 2168: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.13 
2024-02-02 13:47:46,096 EPOCH 2169
2024-02-02 13:47:52,843 Epoch 2169: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.47 
2024-02-02 13:47:52,844 EPOCH 2170
2024-02-02 13:47:59,187 Epoch 2170: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-02 13:47:59,187 EPOCH 2171
2024-02-02 13:48:02,366 [Epoch: 2171 Step: 00036900] Batch Recognition Loss:   0.000647 => Gls Tokens per Sec:     2014 || Batch Translation Loss:   0.048601 => Txt Tokens per Sec:     5844 || Lr: 0.000100
2024-02-02 13:48:06,092 Epoch 2171: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-02 13:48:06,092 EPOCH 2172
2024-02-02 13:48:12,819 Epoch 2172: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-02 13:48:12,819 EPOCH 2173
2024-02-02 13:48:19,585 Epoch 2173: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 13:48:19,586 EPOCH 2174
2024-02-02 13:48:26,203 Epoch 2174: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-02 13:48:26,204 EPOCH 2175
2024-02-02 13:48:32,898 Epoch 2175: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 13:48:32,899 EPOCH 2176
2024-02-02 13:48:39,685 Epoch 2176: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 13:48:39,685 EPOCH 2177
2024-02-02 13:48:44,174 [Epoch: 2177 Step: 00037000] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:     1085 || Batch Translation Loss:   0.018245 => Txt Tokens per Sec:     3009 || Lr: 0.000100
2024-02-02 13:48:46,538 Epoch 2177: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 13:48:46,538 EPOCH 2178
2024-02-02 13:48:53,361 Epoch 2178: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 13:48:53,362 EPOCH 2179
2024-02-02 13:49:00,140 Epoch 2179: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 13:49:00,141 EPOCH 2180
2024-02-02 13:49:06,922 Epoch 2180: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 13:49:06,923 EPOCH 2181
2024-02-02 13:49:13,817 Epoch 2181: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 13:49:13,817 EPOCH 2182
2024-02-02 13:49:20,629 Epoch 2182: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 13:49:20,630 EPOCH 2183
2024-02-02 13:49:22,170 [Epoch: 2183 Step: 00037100] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:     2495 || Batch Translation Loss:   0.012260 => Txt Tokens per Sec:     7159 || Lr: 0.000100
2024-02-02 13:49:27,033 Epoch 2183: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 13:49:27,033 EPOCH 2184
2024-02-02 13:49:33,758 Epoch 2184: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 13:49:33,759 EPOCH 2185
2024-02-02 13:49:40,821 Epoch 2185: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 13:49:40,822 EPOCH 2186
2024-02-02 13:49:47,832 Epoch 2186: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 13:49:47,833 EPOCH 2187
2024-02-02 13:49:54,511 Epoch 2187: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 13:49:54,512 EPOCH 2188
2024-02-02 13:50:01,127 Epoch 2188: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-02 13:50:01,128 EPOCH 2189
2024-02-02 13:50:02,496 [Epoch: 2189 Step: 00037200] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:     1873 || Batch Translation Loss:   0.017489 => Txt Tokens per Sec:     5685 || Lr: 0.000100
2024-02-02 13:50:07,993 Epoch 2189: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 13:50:07,994 EPOCH 2190
2024-02-02 13:50:14,781 Epoch 2190: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 13:50:14,782 EPOCH 2191
2024-02-02 13:50:21,639 Epoch 2191: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-02 13:50:21,640 EPOCH 2192
2024-02-02 13:50:28,142 Epoch 2192: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.76 
2024-02-02 13:50:28,143 EPOCH 2193
2024-02-02 13:50:34,833 Epoch 2193: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-02 13:50:34,834 EPOCH 2194
2024-02-02 13:50:41,407 Epoch 2194: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-02 13:50:41,408 EPOCH 2195
2024-02-02 13:50:41,793 [Epoch: 2195 Step: 00037300] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     3337 || Batch Translation Loss:   0.019842 => Txt Tokens per Sec:     8189 || Lr: 0.000100
2024-02-02 13:50:48,293 Epoch 2195: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.89 
2024-02-02 13:50:48,294 EPOCH 2196
2024-02-02 13:50:55,055 Epoch 2196: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-02 13:50:55,056 EPOCH 2197
2024-02-02 13:51:01,791 Epoch 2197: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-02 13:51:01,791 EPOCH 2198
2024-02-02 13:51:08,216 Epoch 2198: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-02 13:51:08,216 EPOCH 2199
2024-02-02 13:51:15,023 Epoch 2199: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.70 
2024-02-02 13:51:15,023 EPOCH 2200
2024-02-02 13:51:21,430 [Epoch: 2200 Step: 00037400] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:     1659 || Batch Translation Loss:   0.014029 => Txt Tokens per Sec:     4606 || Lr: 0.000100
2024-02-02 13:51:21,431 Epoch 2200: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.89 
2024-02-02 13:51:21,431 EPOCH 2201
2024-02-02 13:51:27,833 Epoch 2201: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-02 13:51:27,833 EPOCH 2202
2024-02-02 13:51:34,906 Epoch 2202: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-02 13:51:34,907 EPOCH 2203
2024-02-02 13:51:41,455 Epoch 2203: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-02 13:51:41,455 EPOCH 2204
2024-02-02 13:51:48,417 Epoch 2204: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 13:51:48,417 EPOCH 2205
2024-02-02 13:51:55,308 Epoch 2205: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 13:51:55,309 EPOCH 2206
2024-02-02 13:52:01,640 [Epoch: 2206 Step: 00037500] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     1477 || Batch Translation Loss:   0.044372 => Txt Tokens per Sec:     4152 || Lr: 0.000100
2024-02-02 13:52:02,024 Epoch 2206: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-02 13:52:02,025 EPOCH 2207
2024-02-02 13:52:08,343 Epoch 2207: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-02 13:52:08,344 EPOCH 2208
2024-02-02 13:52:14,288 Epoch 2208: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-02 13:52:14,288 EPOCH 2209
2024-02-02 13:52:20,264 Epoch 2209: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 13:52:20,264 EPOCH 2210
2024-02-02 13:52:26,581 Epoch 2210: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 13:52:26,581 EPOCH 2211
2024-02-02 13:52:33,415 Epoch 2211: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 13:52:33,415 EPOCH 2212
2024-02-02 13:52:36,839 [Epoch: 2212 Step: 00037600] Batch Recognition Loss:   0.000113 => Gls Tokens per Sec:     2430 || Batch Translation Loss:   0.007626 => Txt Tokens per Sec:     6485 || Lr: 0.000100
2024-02-02 13:52:40,185 Epoch 2212: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-02 13:52:40,186 EPOCH 2213
2024-02-02 13:52:47,305 Epoch 2213: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-02 13:52:47,306 EPOCH 2214
2024-02-02 13:52:54,112 Epoch 2214: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 13:52:54,112 EPOCH 2215
2024-02-02 13:53:01,304 Epoch 2215: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 13:53:01,304 EPOCH 2216
2024-02-02 13:53:08,179 Epoch 2216: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 13:53:08,180 EPOCH 2217
2024-02-02 13:53:15,008 Epoch 2217: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-02 13:53:15,008 EPOCH 2218
2024-02-02 13:53:19,891 [Epoch: 2218 Step: 00037700] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:     1391 || Batch Translation Loss:   0.046530 => Txt Tokens per Sec:     3789 || Lr: 0.000100
2024-02-02 13:53:21,953 Epoch 2218: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 13:53:21,954 EPOCH 2219
2024-02-02 13:53:28,827 Epoch 2219: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 13:53:28,828 EPOCH 2220
2024-02-02 13:53:35,664 Epoch 2220: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 13:53:35,665 EPOCH 2221
2024-02-02 13:53:42,514 Epoch 2221: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 13:53:42,515 EPOCH 2222
2024-02-02 13:53:49,275 Epoch 2222: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 13:53:49,276 EPOCH 2223
2024-02-02 13:53:55,768 Epoch 2223: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 13:53:55,769 EPOCH 2224
2024-02-02 13:54:00,360 [Epoch: 2224 Step: 00037800] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:     1200 || Batch Translation Loss:   0.016061 => Txt Tokens per Sec:     3238 || Lr: 0.000100
2024-02-02 13:54:02,901 Epoch 2224: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 13:54:02,901 EPOCH 2225
2024-02-02 13:54:09,478 Epoch 2225: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 13:54:09,478 EPOCH 2226
2024-02-02 13:54:16,227 Epoch 2226: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 13:54:16,228 EPOCH 2227
2024-02-02 13:54:23,019 Epoch 2227: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-02 13:54:23,019 EPOCH 2228
2024-02-02 13:54:29,597 Epoch 2228: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-02 13:54:29,598 EPOCH 2229
2024-02-02 13:54:36,193 Epoch 2229: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-02 13:54:36,194 EPOCH 2230
2024-02-02 13:54:40,358 [Epoch: 2230 Step: 00037900] Batch Recognition Loss:   0.000384 => Gls Tokens per Sec:     1016 || Batch Translation Loss:   0.026851 => Txt Tokens per Sec:     2905 || Lr: 0.000100
2024-02-02 13:54:43,072 Epoch 2230: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 13:54:43,072 EPOCH 2231
2024-02-02 13:54:50,031 Epoch 2231: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-02 13:54:50,032 EPOCH 2232
2024-02-02 13:54:56,871 Epoch 2232: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 13:54:56,872 EPOCH 2233
2024-02-02 13:55:03,509 Epoch 2233: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 13:55:03,509 EPOCH 2234
2024-02-02 13:55:10,312 Epoch 2234: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 13:55:10,313 EPOCH 2235
2024-02-02 13:55:16,984 Epoch 2235: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 13:55:16,984 EPOCH 2236
2024-02-02 13:55:20,612 [Epoch: 2236 Step: 00038000] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:      813 || Batch Translation Loss:   0.064436 => Txt Tokens per Sec:     2390 || Lr: 0.000100
2024-02-02 13:55:53,879 Validation result at epoch 2236, step    38000: duration: 33.2665s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00062	Translation Loss: 98347.78125	PPL: 18801.03516
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.52	(BLEU-1: 10.35,	BLEU-2: 3.03,	BLEU-3: 1.21,	BLEU-4: 0.52)
	CHRF 16.50	ROUGE 8.84
2024-02-02 13:55:53,880 Logging Recognition and Translation Outputs
2024-02-02 13:55:53,880 ========================================================================================================================
2024-02-02 13:55:53,881 Logging Sequence: 72_194.00
2024-02-02 13:55:53,881 	Gloss Reference :	A B+C+D+E
2024-02-02 13:55:53,881 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 13:55:53,881 	Gloss Alignment :	         
2024-02-02 13:55:53,881 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 13:55:53,883 	Text Reference  :	shah told her to   do   what     she   wants and filed a    police complaint against      her
2024-02-02 13:55:53,883 	Text Hypothesis :	**** **** *** they have captured their fifth ipl title both kept   their     relationship day
2024-02-02 13:55:53,883 	Text Alignment  :	D    D    D   S    S    S        S     S     S   S     S    S      S         S            S  
2024-02-02 13:55:53,884 ========================================================================================================================
2024-02-02 13:55:53,884 Logging Sequence: 108_59.00
2024-02-02 13:55:53,884 	Gloss Reference :	A B+C+D+E
2024-02-02 13:55:53,884 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 13:55:53,884 	Gloss Alignment :	         
2024-02-02 13:55:53,884 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 13:55:53,886 	Text Reference  :	ishan kishan   remained the biggest buy of ipl as mumbai indians paid a   whopping rs 1525 crore ***** ****** *** to      keep  him   
2024-02-02 13:55:53,886 	Text Hypothesis :	also  recently during   the ******* *** ** *** ** ****** ******* **** 1st may      rs 2000 crore rohit sharma and lucknow super giants
2024-02-02 13:55:53,886 	Text Alignment  :	S     S        S            D       D   D  D   D  D      D       D    S   S           S          I     I      I   S       S     S     
2024-02-02 13:55:53,886 ========================================================================================================================
2024-02-02 13:55:53,887 Logging Sequence: 109_10.00
2024-02-02 13:55:53,887 	Gloss Reference :	A B+C+D+E
2024-02-02 13:55:53,887 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 13:55:53,887 	Gloss Alignment :	         
2024-02-02 13:55:53,887 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 13:55:53,888 	Text Reference  :	****** *** ******* was  scheduled to be    played at  the     narendra  modi stadium in ahmedabad
2024-02-02 13:55:53,888 	Text Hypothesis :	before the matches were held      in india with   450 million followers on   may     be refunded 
2024-02-02 13:55:53,889 	Text Alignment  :	I      I   I       S    S         S  S     S      S   S       S         S    S       S  S        
2024-02-02 13:55:53,889 ========================================================================================================================
2024-02-02 13:55:53,889 Logging Sequence: 103_202.00
2024-02-02 13:55:53,889 	Gloss Reference :	A B+C+D+E
2024-02-02 13:55:53,889 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 13:55:53,889 	Gloss Alignment :	         
2024-02-02 13:55:53,889 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 13:55:53,891 	Text Reference  :	india in    total has won 61 medals including    22    gold medals 16  silver medals 23      bronze medals
2024-02-02 13:55:53,891 	Text Hypothesis :	since there was   the 3rd of the    commonwealth games and  kohli  etc on     the    british empire games 
2024-02-02 13:55:53,891 	Text Alignment  :	S     S     S     S   S   S  S      S            S     S    S      S   S      S      S       S      S     
2024-02-02 13:55:53,891 ========================================================================================================================
2024-02-02 13:55:53,892 Logging Sequence: 149_77.00
2024-02-02 13:55:53,892 	Gloss Reference :	A B+C+D+E
2024-02-02 13:55:53,892 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 13:55:53,892 	Gloss Alignment :	         
2024-02-02 13:55:53,892 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 13:55:53,893 	Text Reference  :	and arrested danushka for alleged sexual assault of a 29 year old woman whose       name has not     been disclosed
2024-02-02 13:55:53,893 	Text Hypothesis :	*** ******** ******** *** ******* ****** ******* ** * ** **** *** 6     individuals on   him cooking at   1230pm   
2024-02-02 13:55:53,893 	Text Alignment  :	D   D        D        D   D       D      D       D  D D  D    D   S     S           S    S   S       S    S        
2024-02-02 13:55:53,893 ========================================================================================================================
2024-02-02 13:55:57,114 Epoch 2236: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-02 13:55:57,114 EPOCH 2237
2024-02-02 13:56:03,886 Epoch 2237: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 13:56:03,886 EPOCH 2238
2024-02-02 13:56:10,935 Epoch 2238: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 13:56:10,936 EPOCH 2239
2024-02-02 13:56:17,897 Epoch 2239: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 13:56:17,898 EPOCH 2240
2024-02-02 13:56:24,556 Epoch 2240: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 13:56:24,557 EPOCH 2241
2024-02-02 13:56:31,378 Epoch 2241: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 13:56:31,378 EPOCH 2242
2024-02-02 13:56:32,274 [Epoch: 2242 Step: 00038100] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:     2144 || Batch Translation Loss:   0.014241 => Txt Tokens per Sec:     6418 || Lr: 0.000050
2024-02-02 13:56:38,200 Epoch 2242: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 13:56:38,200 EPOCH 2243
2024-02-02 13:56:44,915 Epoch 2243: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 13:56:44,916 EPOCH 2244
2024-02-02 13:56:51,720 Epoch 2244: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 13:56:51,720 EPOCH 2245
2024-02-02 13:56:58,341 Epoch 2245: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 13:56:58,341 EPOCH 2246
2024-02-02 13:57:05,182 Epoch 2246: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 13:57:05,182 EPOCH 2247
2024-02-02 13:57:12,024 Epoch 2247: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 13:57:12,025 EPOCH 2248
2024-02-02 13:57:12,172 [Epoch: 2248 Step: 00038200] Batch Recognition Loss:   0.000116 => Gls Tokens per Sec:     4384 || Batch Translation Loss:   0.009555 => Txt Tokens per Sec:    11459 || Lr: 0.000050
2024-02-02 13:57:18,802 Epoch 2248: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 13:57:18,802 EPOCH 2249
2024-02-02 13:57:25,574 Epoch 2249: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 13:57:25,575 EPOCH 2250
2024-02-02 13:57:32,257 Epoch 2250: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 13:57:32,257 EPOCH 2251
2024-02-02 13:57:38,972 Epoch 2251: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 13:57:38,972 EPOCH 2252
2024-02-02 13:57:45,866 Epoch 2252: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 13:57:45,866 EPOCH 2253
2024-02-02 13:57:51,910 [Epoch: 2253 Step: 00038300] Batch Recognition Loss:   0.000102 => Gls Tokens per Sec:     1653 || Batch Translation Loss:   0.006813 => Txt Tokens per Sec:     4546 || Lr: 0.000050
2024-02-02 13:57:52,446 Epoch 2253: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 13:57:52,446 EPOCH 2254
2024-02-02 13:57:59,218 Epoch 2254: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 13:57:59,219 EPOCH 2255
2024-02-02 13:58:05,939 Epoch 2255: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 13:58:05,939 EPOCH 2256
2024-02-02 13:58:12,705 Epoch 2256: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 13:58:12,706 EPOCH 2257
2024-02-02 13:58:19,210 Epoch 2257: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 13:58:19,210 EPOCH 2258
2024-02-02 13:58:25,865 Epoch 2258: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 13:58:25,866 EPOCH 2259
2024-02-02 13:58:31,106 [Epoch: 2259 Step: 00038400] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:     1663 || Batch Translation Loss:   0.009631 => Txt Tokens per Sec:     4521 || Lr: 0.000050
2024-02-02 13:58:32,560 Epoch 2259: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 13:58:32,560 EPOCH 2260
2024-02-02 13:58:39,215 Epoch 2260: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 13:58:39,216 EPOCH 2261
2024-02-02 13:58:46,175 Epoch 2261: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 13:58:46,175 EPOCH 2262
2024-02-02 13:58:52,909 Epoch 2262: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 13:58:52,910 EPOCH 2263
2024-02-02 13:58:59,567 Epoch 2263: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 13:58:59,568 EPOCH 2264
2024-02-02 13:59:06,159 Epoch 2264: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 13:59:06,160 EPOCH 2265
2024-02-02 13:59:11,407 [Epoch: 2265 Step: 00038500] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:     1416 || Batch Translation Loss:   0.012951 => Txt Tokens per Sec:     3994 || Lr: 0.000050
2024-02-02 13:59:12,782 Epoch 2265: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 13:59:12,783 EPOCH 2266
2024-02-02 13:59:19,686 Epoch 2266: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 13:59:19,686 EPOCH 2267
2024-02-02 13:59:26,493 Epoch 2267: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 13:59:26,494 EPOCH 2268
2024-02-02 13:59:33,393 Epoch 2268: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 13:59:33,393 EPOCH 2269
2024-02-02 13:59:39,887 Epoch 2269: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 13:59:39,887 EPOCH 2270
2024-02-02 13:59:46,560 Epoch 2270: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 13:59:46,561 EPOCH 2271
2024-02-02 13:59:51,767 [Epoch: 2271 Step: 00038600] Batch Recognition Loss:   0.000109 => Gls Tokens per Sec:     1182 || Batch Translation Loss:   0.009302 => Txt Tokens per Sec:     3407 || Lr: 0.000050
2024-02-02 13:59:53,549 Epoch 2271: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 13:59:53,549 EPOCH 2272
2024-02-02 14:00:00,345 Epoch 2272: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 14:00:00,345 EPOCH 2273
2024-02-02 14:00:07,341 Epoch 2273: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 14:00:07,341 EPOCH 2274
2024-02-02 14:00:13,914 Epoch 2274: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 14:00:13,914 EPOCH 2275
2024-02-02 14:00:20,504 Epoch 2275: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 14:00:20,505 EPOCH 2276
2024-02-02 14:00:27,471 Epoch 2276: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 14:00:27,472 EPOCH 2277
2024-02-02 14:00:31,509 [Epoch: 2277 Step: 00038700] Batch Recognition Loss:   0.000117 => Gls Tokens per Sec:     1207 || Batch Translation Loss:   0.010911 => Txt Tokens per Sec:     3349 || Lr: 0.000050
2024-02-02 14:00:34,119 Epoch 2277: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 14:00:34,119 EPOCH 2278
2024-02-02 14:00:40,676 Epoch 2278: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 14:00:40,677 EPOCH 2279
2024-02-02 14:00:47,414 Epoch 2279: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 14:00:47,415 EPOCH 2280
2024-02-02 14:00:54,042 Epoch 2280: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 14:00:54,042 EPOCH 2281
2024-02-02 14:01:01,107 Epoch 2281: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 14:01:01,107 EPOCH 2282
2024-02-02 14:01:07,858 Epoch 2282: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:01:07,859 EPOCH 2283
2024-02-02 14:01:09,309 [Epoch: 2283 Step: 00038800] Batch Recognition Loss:   0.000121 => Gls Tokens per Sec:     2650 || Batch Translation Loss:   0.023821 => Txt Tokens per Sec:     7885 || Lr: 0.000050
2024-02-02 14:01:14,162 Epoch 2283: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:01:14,163 EPOCH 2284
2024-02-02 14:01:20,832 Epoch 2284: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 14:01:20,832 EPOCH 2285
2024-02-02 14:01:27,851 Epoch 2285: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 14:01:27,851 EPOCH 2286
2024-02-02 14:01:34,545 Epoch 2286: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 14:01:34,546 EPOCH 2287
2024-02-02 14:01:41,261 Epoch 2287: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 14:01:41,262 EPOCH 2288
2024-02-02 14:01:48,064 Epoch 2288: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:01:48,065 EPOCH 2289
2024-02-02 14:01:51,271 [Epoch: 2289 Step: 00038900] Batch Recognition Loss:   0.000315 => Gls Tokens per Sec:      721 || Batch Translation Loss:   0.012275 => Txt Tokens per Sec:     2277 || Lr: 0.000050
2024-02-02 14:01:54,784 Epoch 2289: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:01:54,785 EPOCH 2290
2024-02-02 14:02:01,899 Epoch 2290: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:02:01,899 EPOCH 2291
2024-02-02 14:02:08,959 Epoch 2291: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:02:08,959 EPOCH 2292
2024-02-02 14:02:15,616 Epoch 2292: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:02:15,617 EPOCH 2293
2024-02-02 14:02:22,231 Epoch 2293: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:02:22,232 EPOCH 2294
2024-02-02 14:02:29,056 Epoch 2294: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 14:02:29,057 EPOCH 2295
2024-02-02 14:02:29,420 [Epoch: 2295 Step: 00039000] Batch Recognition Loss:   0.000103 => Gls Tokens per Sec:     3526 || Batch Translation Loss:   0.008021 => Txt Tokens per Sec:     9377 || Lr: 0.000050
2024-02-02 14:02:35,868 Epoch 2295: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 14:02:35,868 EPOCH 2296
2024-02-02 14:02:42,470 Epoch 2296: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 14:02:42,470 EPOCH 2297
2024-02-02 14:02:48,691 Epoch 2297: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 14:02:48,692 EPOCH 2298
2024-02-02 14:02:55,631 Epoch 2298: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 14:02:55,632 EPOCH 2299
2024-02-02 14:03:02,520 Epoch 2299: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 14:03:02,521 EPOCH 2300
2024-02-02 14:03:09,371 [Epoch: 2300 Step: 00039100] Batch Recognition Loss:   0.000099 => Gls Tokens per Sec:     1552 || Batch Translation Loss:   0.006677 => Txt Tokens per Sec:     4308 || Lr: 0.000050
2024-02-02 14:03:09,372 Epoch 2300: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 14:03:09,372 EPOCH 2301
2024-02-02 14:03:15,994 Epoch 2301: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:03:15,995 EPOCH 2302
2024-02-02 14:03:22,676 Epoch 2302: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:03:22,677 EPOCH 2303
2024-02-02 14:03:29,360 Epoch 2303: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:03:29,360 EPOCH 2304
2024-02-02 14:03:36,257 Epoch 2304: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:03:36,257 EPOCH 2305
2024-02-02 14:03:43,033 Epoch 2305: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 14:03:43,033 EPOCH 2306
2024-02-02 14:03:48,817 [Epoch: 2306 Step: 00039200] Batch Recognition Loss:   0.000115 => Gls Tokens per Sec:     1617 || Batch Translation Loss:   0.010965 => Txt Tokens per Sec:     4429 || Lr: 0.000050
2024-02-02 14:03:49,634 Epoch 2306: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 14:03:49,634 EPOCH 2307
2024-02-02 14:03:56,271 Epoch 2307: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 14:03:56,272 EPOCH 2308
2024-02-02 14:04:03,267 Epoch 2308: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:04:03,267 EPOCH 2309
2024-02-02 14:04:10,050 Epoch 2309: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 14:04:10,051 EPOCH 2310
2024-02-02 14:04:16,670 Epoch 2310: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 14:04:16,671 EPOCH 2311
2024-02-02 14:04:23,035 Epoch 2311: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:04:23,035 EPOCH 2312
2024-02-02 14:04:26,665 [Epoch: 2312 Step: 00039300] Batch Recognition Loss:   0.000111 => Gls Tokens per Sec:     2293 || Batch Translation Loss:   0.008758 => Txt Tokens per Sec:     6179 || Lr: 0.000050
2024-02-02 14:04:30,121 Epoch 2312: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 14:04:30,122 EPOCH 2313
2024-02-02 14:04:36,878 Epoch 2313: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:04:36,879 EPOCH 2314
2024-02-02 14:04:44,138 Epoch 2314: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:04:44,138 EPOCH 2315
2024-02-02 14:04:50,958 Epoch 2315: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:04:50,958 EPOCH 2316
2024-02-02 14:04:57,755 Epoch 2316: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:04:57,755 EPOCH 2317
2024-02-02 14:05:04,602 Epoch 2317: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 14:05:04,603 EPOCH 2318
2024-02-02 14:05:08,100 [Epoch: 2318 Step: 00039400] Batch Recognition Loss:   0.000148 => Gls Tokens per Sec:     2014 || Batch Translation Loss:   0.013489 => Txt Tokens per Sec:     5721 || Lr: 0.000050
2024-02-02 14:05:11,577 Epoch 2318: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 14:05:11,578 EPOCH 2319
2024-02-02 14:05:18,420 Epoch 2319: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 14:05:18,421 EPOCH 2320
2024-02-02 14:05:25,503 Epoch 2320: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:05:25,504 EPOCH 2321
2024-02-02 14:05:32,400 Epoch 2321: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 14:05:32,401 EPOCH 2322
2024-02-02 14:05:39,278 Epoch 2322: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 14:05:39,279 EPOCH 2323
2024-02-02 14:05:46,189 Epoch 2323: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 14:05:46,189 EPOCH 2324
2024-02-02 14:05:50,910 [Epoch: 2324 Step: 00039500] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     1168 || Batch Translation Loss:   0.019405 => Txt Tokens per Sec:     3306 || Lr: 0.000050
2024-02-02 14:05:53,043 Epoch 2324: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-02 14:05:53,044 EPOCH 2325
2024-02-02 14:05:59,788 Epoch 2325: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-02 14:05:59,789 EPOCH 2326
2024-02-02 14:06:07,008 Epoch 2326: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-02 14:06:07,008 EPOCH 2327
2024-02-02 14:06:13,888 Epoch 2327: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-02 14:06:13,889 EPOCH 2328
2024-02-02 14:06:20,730 Epoch 2328: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.96 
2024-02-02 14:06:20,731 EPOCH 2329
2024-02-02 14:06:27,576 Epoch 2329: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-02 14:06:27,577 EPOCH 2330
2024-02-02 14:06:31,449 [Epoch: 2330 Step: 00039600] Batch Recognition Loss:   0.000314 => Gls Tokens per Sec:     1093 || Batch Translation Loss:   0.049489 => Txt Tokens per Sec:     3026 || Lr: 0.000050
2024-02-02 14:06:34,437 Epoch 2330: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-02 14:06:34,437 EPOCH 2331
2024-02-02 14:06:41,343 Epoch 2331: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 14:06:41,343 EPOCH 2332
2024-02-02 14:06:48,237 Epoch 2332: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 14:06:48,238 EPOCH 2333
2024-02-02 14:06:55,251 Epoch 2333: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 14:06:55,251 EPOCH 2334
2024-02-02 14:07:02,237 Epoch 2334: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 14:07:02,238 EPOCH 2335
2024-02-02 14:07:09,081 Epoch 2335: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 14:07:09,081 EPOCH 2336
2024-02-02 14:07:10,992 [Epoch: 2336 Step: 00039700] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:     1676 || Batch Translation Loss:   0.016457 => Txt Tokens per Sec:     5235 || Lr: 0.000050
2024-02-02 14:07:15,956 Epoch 2336: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 14:07:15,956 EPOCH 2337
2024-02-02 14:07:22,713 Epoch 2337: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 14:07:22,714 EPOCH 2338
2024-02-02 14:07:29,513 Epoch 2338: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 14:07:29,514 EPOCH 2339
2024-02-02 14:07:36,513 Epoch 2339: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 14:07:36,513 EPOCH 2340
2024-02-02 14:07:43,249 Epoch 2340: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 14:07:43,250 EPOCH 2341
2024-02-02 14:07:50,103 Epoch 2341: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 14:07:50,104 EPOCH 2342
2024-02-02 14:07:53,257 [Epoch: 2342 Step: 00039800] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:      530 || Batch Translation Loss:   0.015288 => Txt Tokens per Sec:     1656 || Lr: 0.000050
2024-02-02 14:07:57,051 Epoch 2342: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:07:57,052 EPOCH 2343
2024-02-02 14:08:03,958 Epoch 2343: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 14:08:03,959 EPOCH 2344
2024-02-02 14:08:10,997 Epoch 2344: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:08:10,997 EPOCH 2345
2024-02-02 14:08:17,677 Epoch 2345: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:08:17,677 EPOCH 2346
2024-02-02 14:08:24,282 Epoch 2346: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 14:08:24,283 EPOCH 2347
2024-02-02 14:08:31,152 Epoch 2347: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 14:08:31,153 EPOCH 2348
2024-02-02 14:08:31,273 [Epoch: 2348 Step: 00039900] Batch Recognition Loss:   0.000117 => Gls Tokens per Sec:     5333 || Batch Translation Loss:   0.011461 => Txt Tokens per Sec:    10608 || Lr: 0.000050
2024-02-02 14:08:37,954 Epoch 2348: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 14:08:37,955 EPOCH 2349
2024-02-02 14:08:44,846 Epoch 2349: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 14:08:44,847 EPOCH 2350
2024-02-02 14:08:51,716 Epoch 2350: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 14:08:51,716 EPOCH 2351
2024-02-02 14:08:58,536 Epoch 2351: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:08:58,537 EPOCH 2352
2024-02-02 14:09:05,364 Epoch 2352: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 14:09:05,364 EPOCH 2353
2024-02-02 14:09:12,055 [Epoch: 2353 Step: 00040000] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     1494 || Batch Translation Loss:   0.015773 => Txt Tokens per Sec:     4153 || Lr: 0.000050
2024-02-02 14:09:44,361 Validation result at epoch 2353, step    40000: duration: 32.3049s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00052	Translation Loss: 97475.14844	PPL: 17228.89258
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.67	(BLEU-1: 9.96,	BLEU-2: 2.94,	BLEU-3: 1.21,	BLEU-4: 0.67)
	CHRF 16.74	ROUGE 8.60
2024-02-02 14:09:44,362 Logging Recognition and Translation Outputs
2024-02-02 14:09:44,362 ========================================================================================================================
2024-02-02 14:09:44,362 Logging Sequence: 123_104.00
2024-02-02 14:09:44,363 	Gloss Reference :	A B+C+D+E
2024-02-02 14:09:44,363 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 14:09:44,363 	Gloss Alignment :	         
2024-02-02 14:09:44,363 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 14:09:44,364 	Text Reference  :	the car was    presented to the former india cricketer from an        unknown  person
2024-02-02 14:09:44,364 	Text Hypothesis :	*** he  gifted each      on the ****** ***** jersey    a    three-day practice match 
2024-02-02 14:09:44,364 	Text Alignment  :	D   S   S      S         S      D      D     S         S    S         S        S     
2024-02-02 14:09:44,365 ========================================================================================================================
2024-02-02 14:09:44,365 Logging Sequence: 107_23.00
2024-02-02 14:09:44,365 	Gloss Reference :	A B+C+D+E
2024-02-02 14:09:44,365 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 14:09:44,365 	Gloss Alignment :	         
2024-02-02 14:09:44,365 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 14:09:44,366 	Text Reference  :	and viktor  lilov who     is   also from the     usa   
2024-02-02 14:09:44,366 	Text Hypothesis :	*** however the   matches were lost to   against sushil
2024-02-02 14:09:44,366 	Text Alignment  :	D   S       S     S       S    S    S    S       S     
2024-02-02 14:09:44,366 ========================================================================================================================
2024-02-02 14:09:44,366 Logging Sequence: 134_212.00
2024-02-02 14:09:44,367 	Gloss Reference :	A B+C+D+E
2024-02-02 14:09:44,367 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 14:09:44,367 	Gloss Alignment :	         
2024-02-02 14:09:44,367 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 14:09:44,368 	Text Reference  :	******* *** **** dhanush said that   he practises little yoga
2024-02-02 14:09:44,368 	Text Hypothesis :	however our baby girl    is   taking pm a         viral  it  
2024-02-02 14:09:44,368 	Text Alignment  :	I       I   I    S       S    S      S  S         S      S   
2024-02-02 14:09:44,368 ========================================================================================================================
2024-02-02 14:09:44,368 Logging Sequence: 165_577.00
2024-02-02 14:09:44,368 	Gloss Reference :	A B+C+D+E
2024-02-02 14:09:44,368 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 14:09:44,368 	Gloss Alignment :	         
2024-02-02 14:09:44,369 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 14:09:44,370 	Text Reference  :	****** **** ******* ** *** *** then after 28  years india won  the world cup again in  2011   
2024-02-02 14:09:44,370 	Text Hypothesis :	people were shocked to see him play it    2-3 years ***** back the ***** *** game  was playing
2024-02-02 14:09:44,370 	Text Alignment  :	I      I    I       I  I   I   S    S     S         D     S        D     D   S     S   S      
2024-02-02 14:09:44,370 ========================================================================================================================
2024-02-02 14:09:44,370 Logging Sequence: 88_142.00
2024-02-02 14:09:44,370 	Gloss Reference :	A B+C+D+E
2024-02-02 14:09:44,371 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 14:09:44,371 	Gloss Alignment :	         
2024-02-02 14:09:44,371 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 14:09:44,371 	Text Reference  :	this is       because the police does not   do  anything
2024-02-02 14:09:44,371 	Text Hypothesis :	the  shooters kept    the ****** note there and escaped 
2024-02-02 14:09:44,372 	Text Alignment  :	S    S        S           D      S    S     S   S       
2024-02-02 14:09:44,372 ========================================================================================================================
2024-02-02 14:09:44,606 Epoch 2353: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 14:09:44,606 EPOCH 2354
2024-02-02 14:09:51,473 Epoch 2354: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:09:51,473 EPOCH 2355
2024-02-02 14:09:58,361 Epoch 2355: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:09:58,361 EPOCH 2356
2024-02-02 14:10:04,943 Epoch 2356: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:10:04,944 EPOCH 2357
2024-02-02 14:10:11,834 Epoch 2357: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:10:11,835 EPOCH 2358
2024-02-02 14:10:18,552 Epoch 2358: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:10:18,553 EPOCH 2359
2024-02-02 14:10:24,259 [Epoch: 2359 Step: 00040100] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:     1526 || Batch Translation Loss:   0.014827 => Txt Tokens per Sec:     4142 || Lr: 0.000050
2024-02-02 14:10:25,343 Epoch 2359: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 14:10:25,343 EPOCH 2360
2024-02-02 14:10:31,937 Epoch 2360: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 14:10:31,938 EPOCH 2361
2024-02-02 14:10:38,791 Epoch 2361: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 14:10:38,792 EPOCH 2362
2024-02-02 14:10:45,645 Epoch 2362: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 14:10:45,645 EPOCH 2363
2024-02-02 14:10:52,412 Epoch 2363: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 14:10:52,413 EPOCH 2364
2024-02-02 14:10:58,539 Epoch 2364: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 14:10:58,539 EPOCH 2365
2024-02-02 14:11:02,152 [Epoch: 2365 Step: 00040200] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:     2128 || Batch Translation Loss:   0.018181 => Txt Tokens per Sec:     5824 || Lr: 0.000050
2024-02-02 14:11:05,505 Epoch 2365: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 14:11:05,505 EPOCH 2366
2024-02-02 14:11:12,320 Epoch 2366: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 14:11:12,320 EPOCH 2367
2024-02-02 14:11:19,134 Epoch 2367: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 14:11:19,134 EPOCH 2368
2024-02-02 14:11:25,731 Epoch 2368: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 14:11:25,731 EPOCH 2369
2024-02-02 14:11:32,697 Epoch 2369: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 14:11:32,697 EPOCH 2370
2024-02-02 14:11:39,386 Epoch 2370: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 14:11:39,386 EPOCH 2371
2024-02-02 14:11:44,538 [Epoch: 2371 Step: 00040300] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:     1194 || Batch Translation Loss:   0.024522 => Txt Tokens per Sec:     3411 || Lr: 0.000050
2024-02-02 14:11:46,424 Epoch 2371: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 14:11:46,424 EPOCH 2372
2024-02-02 14:11:53,248 Epoch 2372: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 14:11:53,249 EPOCH 2373
2024-02-02 14:12:00,007 Epoch 2373: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 14:12:00,008 EPOCH 2374
2024-02-02 14:12:06,470 Epoch 2374: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 14:12:06,470 EPOCH 2375
2024-02-02 14:12:13,190 Epoch 2375: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 14:12:13,191 EPOCH 2376
2024-02-02 14:12:20,227 Epoch 2376: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:12:20,227 EPOCH 2377
2024-02-02 14:12:22,253 [Epoch: 2377 Step: 00040400] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:     2530 || Batch Translation Loss:   0.017862 => Txt Tokens per Sec:     6618 || Lr: 0.000050
2024-02-02 14:12:26,801 Epoch 2377: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:12:26,801 EPOCH 2378
2024-02-02 14:12:33,252 Epoch 2378: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:12:33,253 EPOCH 2379
2024-02-02 14:12:40,137 Epoch 2379: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:12:40,137 EPOCH 2380
2024-02-02 14:12:46,877 Epoch 2380: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:12:46,877 EPOCH 2381
2024-02-02 14:12:53,638 Epoch 2381: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:12:53,639 EPOCH 2382
2024-02-02 14:13:00,639 Epoch 2382: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:13:00,639 EPOCH 2383
2024-02-02 14:13:04,465 [Epoch: 2383 Step: 00040500] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:      939 || Batch Translation Loss:   0.016906 => Txt Tokens per Sec:     2648 || Lr: 0.000050
2024-02-02 14:13:07,381 Epoch 2383: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:13:07,381 EPOCH 2384
2024-02-02 14:13:13,812 Epoch 2384: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:13:13,813 EPOCH 2385
2024-02-02 14:13:20,675 Epoch 2385: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:13:20,676 EPOCH 2386
2024-02-02 14:13:27,590 Epoch 2386: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 14:13:27,591 EPOCH 2387
2024-02-02 14:13:34,619 Epoch 2387: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 14:13:34,620 EPOCH 2388
2024-02-02 14:13:41,295 Epoch 2388: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:13:41,296 EPOCH 2389
2024-02-02 14:13:42,017 [Epoch: 2389 Step: 00040600] Batch Recognition Loss:   0.000125 => Gls Tokens per Sec:     3556 || Batch Translation Loss:   0.013814 => Txt Tokens per Sec:     8549 || Lr: 0.000050
2024-02-02 14:13:48,068 Epoch 2389: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:13:48,069 EPOCH 2390
2024-02-02 14:13:54,704 Epoch 2390: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:13:54,705 EPOCH 2391
2024-02-02 14:14:01,359 Epoch 2391: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 14:14:01,359 EPOCH 2392
2024-02-02 14:14:08,271 Epoch 2392: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 14:14:08,272 EPOCH 2393
2024-02-02 14:14:15,151 Epoch 2393: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 14:14:15,152 EPOCH 2394
2024-02-02 14:14:22,018 Epoch 2394: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 14:14:22,019 EPOCH 2395
2024-02-02 14:14:22,937 [Epoch: 2395 Step: 00040700] Batch Recognition Loss:   0.000151 => Gls Tokens per Sec:     1396 || Batch Translation Loss:   0.014986 => Txt Tokens per Sec:     4275 || Lr: 0.000050
2024-02-02 14:14:28,924 Epoch 2395: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 14:14:28,925 EPOCH 2396
2024-02-02 14:14:35,566 Epoch 2396: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 14:14:35,566 EPOCH 2397
2024-02-02 14:14:42,418 Epoch 2397: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 14:14:42,419 EPOCH 2398
2024-02-02 14:14:49,140 Epoch 2398: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 14:14:49,141 EPOCH 2399
2024-02-02 14:14:55,881 Epoch 2399: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:14:55,882 EPOCH 2400
2024-02-02 14:15:02,362 [Epoch: 2400 Step: 00040800] Batch Recognition Loss:   0.000115 => Gls Tokens per Sec:     1641 || Batch Translation Loss:   0.009990 => Txt Tokens per Sec:     4554 || Lr: 0.000050
2024-02-02 14:15:02,363 Epoch 2400: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:15:02,363 EPOCH 2401
2024-02-02 14:15:09,348 Epoch 2401: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 14:15:09,348 EPOCH 2402
2024-02-02 14:15:16,247 Epoch 2402: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 14:15:16,248 EPOCH 2403
2024-02-02 14:15:23,408 Epoch 2403: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 14:15:23,409 EPOCH 2404
2024-02-02 14:15:30,272 Epoch 2404: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-02 14:15:30,273 EPOCH 2405
2024-02-02 14:15:37,155 Epoch 2405: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 14:15:37,155 EPOCH 2406
2024-02-02 14:15:43,398 [Epoch: 2406 Step: 00040900] Batch Recognition Loss:   0.000249 => Gls Tokens per Sec:     1498 || Batch Translation Loss:   0.034935 => Txt Tokens per Sec:     4274 || Lr: 0.000050
2024-02-02 14:15:43,803 Epoch 2406: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 14:15:43,804 EPOCH 2407
2024-02-02 14:15:50,754 Epoch 2407: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-02 14:15:50,755 EPOCH 2408
2024-02-02 14:15:57,492 Epoch 2408: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-02 14:15:57,492 EPOCH 2409
2024-02-02 14:16:04,329 Epoch 2409: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 14:16:04,330 EPOCH 2410
2024-02-02 14:16:10,812 Epoch 2410: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 14:16:10,812 EPOCH 2411
2024-02-02 14:16:17,463 Epoch 2411: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 14:16:17,463 EPOCH 2412
2024-02-02 14:16:23,120 [Epoch: 2412 Step: 00041000] Batch Recognition Loss:   0.000189 => Gls Tokens per Sec:     1427 || Batch Translation Loss:   0.021878 => Txt Tokens per Sec:     3894 || Lr: 0.000050
2024-02-02 14:16:24,451 Epoch 2412: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 14:16:24,452 EPOCH 2413
2024-02-02 14:16:31,138 Epoch 2413: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 14:16:31,139 EPOCH 2414
2024-02-02 14:16:37,937 Epoch 2414: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 14:16:37,938 EPOCH 2415
2024-02-02 14:16:44,835 Epoch 2415: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 14:16:44,836 EPOCH 2416
2024-02-02 14:16:51,418 Epoch 2416: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 14:16:51,419 EPOCH 2417
2024-02-02 14:16:58,234 Epoch 2417: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-02 14:16:58,235 EPOCH 2418
2024-02-02 14:17:03,179 [Epoch: 2418 Step: 00041100] Batch Recognition Loss:   0.000295 => Gls Tokens per Sec:     1374 || Batch Translation Loss:   0.027613 => Txt Tokens per Sec:     3890 || Lr: 0.000050
2024-02-02 14:17:04,858 Epoch 2418: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 14:17:04,858 EPOCH 2419
2024-02-02 14:17:11,736 Epoch 2419: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 14:17:11,737 EPOCH 2420
2024-02-02 14:17:18,436 Epoch 2420: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 14:17:18,436 EPOCH 2421
2024-02-02 14:17:25,259 Epoch 2421: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 14:17:25,260 EPOCH 2422
2024-02-02 14:17:32,125 Epoch 2422: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 14:17:32,125 EPOCH 2423
2024-02-02 14:17:38,537 Epoch 2423: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 14:17:38,538 EPOCH 2424
2024-02-02 14:17:40,969 [Epoch: 2424 Step: 00041200] Batch Recognition Loss:   0.000258 => Gls Tokens per Sec:     2370 || Batch Translation Loss:   0.024107 => Txt Tokens per Sec:     6213 || Lr: 0.000050
2024-02-02 14:17:45,259 Epoch 2424: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 14:17:45,259 EPOCH 2425
2024-02-02 14:17:52,115 Epoch 2425: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 14:17:52,116 EPOCH 2426
2024-02-02 14:17:58,963 Epoch 2426: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 14:17:58,964 EPOCH 2427
2024-02-02 14:18:05,756 Epoch 2427: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 14:18:05,757 EPOCH 2428
2024-02-02 14:18:12,182 Epoch 2428: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:18:12,183 EPOCH 2429
2024-02-02 14:18:18,986 Epoch 2429: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:18:18,987 EPOCH 2430
2024-02-02 14:18:20,803 [Epoch: 2430 Step: 00041300] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     2469 || Batch Translation Loss:   0.015116 => Txt Tokens per Sec:     6500 || Lr: 0.000050
2024-02-02 14:18:25,558 Epoch 2430: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:18:25,558 EPOCH 2431
2024-02-02 14:18:32,520 Epoch 2431: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:18:32,521 EPOCH 2432
2024-02-02 14:18:39,236 Epoch 2432: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:18:39,237 EPOCH 2433
2024-02-02 14:18:45,520 Epoch 2433: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:18:45,521 EPOCH 2434
2024-02-02 14:18:52,223 Epoch 2434: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:18:52,224 EPOCH 2435
2024-02-02 14:18:59,145 Epoch 2435: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:18:59,146 EPOCH 2436
2024-02-02 14:19:00,128 [Epoch: 2436 Step: 00041400] Batch Recognition Loss:   0.000117 => Gls Tokens per Sec:     3268 || Batch Translation Loss:   0.010669 => Txt Tokens per Sec:     8240 || Lr: 0.000050
2024-02-02 14:19:05,942 Epoch 2436: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:19:05,942 EPOCH 2437
2024-02-02 14:19:12,234 Epoch 2437: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 14:19:12,235 EPOCH 2438
2024-02-02 14:19:18,642 Epoch 2438: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 14:19:18,643 EPOCH 2439
2024-02-02 14:19:25,507 Epoch 2439: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 14:19:25,508 EPOCH 2440
2024-02-02 14:19:32,006 Epoch 2440: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 14:19:32,006 EPOCH 2441
2024-02-02 14:19:39,080 Epoch 2441: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.90 
2024-02-02 14:19:39,080 EPOCH 2442
2024-02-02 14:19:39,954 [Epoch: 2442 Step: 00041500] Batch Recognition Loss:   0.000254 => Gls Tokens per Sec:     2201 || Batch Translation Loss:   0.043257 => Txt Tokens per Sec:     6101 || Lr: 0.000050
2024-02-02 14:19:45,793 Epoch 2442: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.40 
2024-02-02 14:19:45,794 EPOCH 2443
2024-02-02 14:19:52,471 Epoch 2443: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.47 
2024-02-02 14:19:52,471 EPOCH 2444
2024-02-02 14:19:59,115 Epoch 2444: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-02 14:19:59,115 EPOCH 2445
2024-02-02 14:20:05,775 Epoch 2445: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.93 
2024-02-02 14:20:05,775 EPOCH 2446
2024-02-02 14:20:12,708 Epoch 2446: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 14:20:12,708 EPOCH 2447
2024-02-02 14:20:19,503 Epoch 2447: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 14:20:19,503 EPOCH 2448
2024-02-02 14:20:19,759 [Epoch: 2448 Step: 00041600] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:     2510 || Batch Translation Loss:   0.019036 => Txt Tokens per Sec:     7251 || Lr: 0.000050
2024-02-02 14:20:26,222 Epoch 2448: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 14:20:26,223 EPOCH 2449
2024-02-02 14:20:33,101 Epoch 2449: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 14:20:33,102 EPOCH 2450
2024-02-02 14:20:39,744 Epoch 2450: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 14:20:39,745 EPOCH 2451
2024-02-02 14:20:46,669 Epoch 2451: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 14:20:46,670 EPOCH 2452
2024-02-02 14:20:53,535 Epoch 2452: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 14:20:53,535 EPOCH 2453
2024-02-02 14:21:00,100 [Epoch: 2453 Step: 00041700] Batch Recognition Loss:   0.000185 => Gls Tokens per Sec:     1522 || Batch Translation Loss:   0.016493 => Txt Tokens per Sec:     4240 || Lr: 0.000050
2024-02-02 14:21:00,272 Epoch 2453: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:21:00,272 EPOCH 2454
2024-02-02 14:21:06,902 Epoch 2454: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:21:06,903 EPOCH 2455
2024-02-02 14:21:13,652 Epoch 2455: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:21:13,653 EPOCH 2456
2024-02-02 14:21:20,473 Epoch 2456: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 14:21:20,474 EPOCH 2457
2024-02-02 14:21:27,144 Epoch 2457: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 14:21:27,145 EPOCH 2458
2024-02-02 14:21:34,007 Epoch 2458: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 14:21:34,007 EPOCH 2459
2024-02-02 14:21:39,986 [Epoch: 2459 Step: 00041800] Batch Recognition Loss:   0.000102 => Gls Tokens per Sec:     1457 || Batch Translation Loss:   0.010502 => Txt Tokens per Sec:     3935 || Lr: 0.000050
2024-02-02 14:21:40,899 Epoch 2459: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:21:40,899 EPOCH 2460
2024-02-02 14:21:47,673 Epoch 2460: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:21:47,673 EPOCH 2461
2024-02-02 14:21:53,733 Epoch 2461: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:21:53,733 EPOCH 2462
2024-02-02 14:22:00,562 Epoch 2462: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:22:00,562 EPOCH 2463
2024-02-02 14:22:07,110 Epoch 2463: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:22:07,111 EPOCH 2464
2024-02-02 14:22:13,570 Epoch 2464: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:22:13,570 EPOCH 2465
2024-02-02 14:22:19,166 [Epoch: 2465 Step: 00041900] Batch Recognition Loss:   0.000095 => Gls Tokens per Sec:     1328 || Batch Translation Loss:   0.008183 => Txt Tokens per Sec:     3706 || Lr: 0.000050
2024-02-02 14:22:20,378 Epoch 2465: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:22:20,378 EPOCH 2466
2024-02-02 14:22:27,136 Epoch 2466: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 14:22:27,137 EPOCH 2467
2024-02-02 14:22:33,877 Epoch 2467: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:22:33,878 EPOCH 2468
2024-02-02 14:22:40,449 Epoch 2468: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:22:40,450 EPOCH 2469
2024-02-02 14:22:47,305 Epoch 2469: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:22:47,305 EPOCH 2470
2024-02-02 14:22:54,202 Epoch 2470: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:22:54,203 EPOCH 2471
2024-02-02 14:22:57,551 [Epoch: 2471 Step: 00042000] Batch Recognition Loss:   0.000095 => Gls Tokens per Sec:     1912 || Batch Translation Loss:   0.005837 => Txt Tokens per Sec:     5439 || Lr: 0.000050
2024-02-02 14:23:28,054 Validation result at epoch 2471, step    42000: duration: 30.5014s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00049	Translation Loss: 98856.35938	PPL: 19782.65625
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.49	(BLEU-1: 9.99,	BLEU-2: 2.69,	BLEU-3: 1.01,	BLEU-4: 0.49)
	CHRF 16.39	ROUGE 8.46
2024-02-02 14:23:28,055 Logging Recognition and Translation Outputs
2024-02-02 14:23:28,055 ========================================================================================================================
2024-02-02 14:23:28,056 Logging Sequence: 81_8.00
2024-02-02 14:23:28,056 	Gloss Reference :	A B+C+D+E
2024-02-02 14:23:28,056 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 14:23:28,056 	Gloss Alignment :	         
2024-02-02 14:23:28,057 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 14:23:28,059 	Text Reference  :	have been involved in a huge controversy in connection to real   estate developer amrapali group since last 7   years         
2024-02-02 14:23:28,059 	Text Hypothesis :	**** **** ******** ** * **** *********** ** ********** he played 18     tests     226      odis  and   78   t20 internationals
2024-02-02 14:23:28,059 	Text Alignment  :	D    D    D        D  D D    D           D  D          S  S      S      S         S        S     S     S    S   S             
2024-02-02 14:23:28,059 ========================================================================================================================
2024-02-02 14:23:28,059 Logging Sequence: 148_239.00
2024-02-02 14:23:28,060 	Gloss Reference :	A B+C+D+E
2024-02-02 14:23:28,060 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 14:23:28,060 	Gloss Alignment :	         
2024-02-02 14:23:28,060 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 14:23:28,062 	Text Reference  :	******* *** the     ground staff were  very happy and ******** **** * *** ** thanked the bowler for   his kind gesture
2024-02-02 14:23:28,062 	Text Hypothesis :	however not kuldeep yadav  a     video just 111   and finished with a day in an      odi match  falls on  the  matter 
2024-02-02 14:23:28,062 	Text Alignment  :	I       I   S       S      S     S     S    S         I        I    I I   I  S       S   S      S     S   S    S      
2024-02-02 14:23:28,062 ========================================================================================================================
2024-02-02 14:23:28,062 Logging Sequence: 165_8.00
2024-02-02 14:23:28,063 	Gloss Reference :	A B+C+D+E
2024-02-02 14:23:28,063 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 14:23:28,063 	Gloss Alignment :	         
2024-02-02 14:23:28,063 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 14:23:28,064 	Text Reference  :	however many don't believe in     it  it      varies among people
2024-02-02 14:23:28,064 	Text Hypothesis :	******* **** ***** they    played the quarter final  as    well  
2024-02-02 14:23:28,064 	Text Alignment  :	D       D    D     S       S      S   S       S      S     S     
2024-02-02 14:23:28,064 ========================================================================================================================
2024-02-02 14:23:28,064 Logging Sequence: 93_93.00
2024-02-02 14:23:28,064 	Gloss Reference :	A B+C+D+E
2024-02-02 14:23:28,064 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 14:23:28,065 	Gloss Alignment :	         
2024-02-02 14:23:28,065 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 14:23:28,065 	Text Reference  :	****** **** rooney was at    the     club   as  well 
2024-02-02 14:23:28,065 	Text Hypothesis :	people were glued  to  their screens during the match
2024-02-02 14:23:28,066 	Text Alignment  :	I      I    S      S   S     S       S      S   S    
2024-02-02 14:23:28,066 ========================================================================================================================
2024-02-02 14:23:28,066 Logging Sequence: 96_129.00
2024-02-02 14:23:28,066 	Gloss Reference :	A B+C+D+E
2024-02-02 14:23:28,066 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 14:23:28,066 	Gloss Alignment :	         
2024-02-02 14:23:28,066 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 14:23:28,067 	Text Reference  :	*** **** *** viewers were very stressed
2024-02-02 14:23:28,067 	Text Hypothesis :	ish news had win     will be   wicket  
2024-02-02 14:23:28,067 	Text Alignment  :	I   I    I   S       S    S    S       
2024-02-02 14:23:28,067 ========================================================================================================================
2024-02-02 14:23:31,867 Epoch 2471: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:23:31,867 EPOCH 2472
2024-02-02 14:23:38,598 Epoch 2472: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 14:23:38,599 EPOCH 2473
2024-02-02 14:23:45,263 Epoch 2473: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:23:45,263 EPOCH 2474
2024-02-02 14:23:52,075 Epoch 2474: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 14:23:52,076 EPOCH 2475
2024-02-02 14:23:58,881 Epoch 2475: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:23:58,881 EPOCH 2476
2024-02-02 14:24:05,724 Epoch 2476: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 14:24:05,725 EPOCH 2477
2024-02-02 14:24:10,182 [Epoch: 2477 Step: 00042100] Batch Recognition Loss:   0.000152 => Gls Tokens per Sec:     1093 || Batch Translation Loss:   0.025164 => Txt Tokens per Sec:     3281 || Lr: 0.000050
2024-02-02 14:24:12,402 Epoch 2477: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 14:24:12,403 EPOCH 2478
2024-02-02 14:24:19,082 Epoch 2478: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 14:24:19,083 EPOCH 2479
2024-02-02 14:24:25,901 Epoch 2479: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:24:25,901 EPOCH 2480
2024-02-02 14:24:32,669 Epoch 2480: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:24:32,669 EPOCH 2481
2024-02-02 14:24:39,317 Epoch 2481: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 14:24:39,318 EPOCH 2482
2024-02-02 14:24:46,173 Epoch 2482: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 14:24:46,174 EPOCH 2483
2024-02-02 14:24:49,712 [Epoch: 2483 Step: 00042200] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     1015 || Batch Translation Loss:   0.017772 => Txt Tokens per Sec:     2960 || Lr: 0.000050
2024-02-02 14:24:53,081 Epoch 2483: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 14:24:53,081 EPOCH 2484
2024-02-02 14:24:59,774 Epoch 2484: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 14:24:59,774 EPOCH 2485
2024-02-02 14:25:06,715 Epoch 2485: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 14:25:06,716 EPOCH 2486
2024-02-02 14:25:13,493 Epoch 2486: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 14:25:13,493 EPOCH 2487
2024-02-02 14:25:20,275 Epoch 2487: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 14:25:20,276 EPOCH 2488
2024-02-02 14:25:27,264 Epoch 2488: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 14:25:27,265 EPOCH 2489
2024-02-02 14:25:28,261 [Epoch: 2489 Step: 00042300] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     2572 || Batch Translation Loss:   0.017083 => Txt Tokens per Sec:     6410 || Lr: 0.000050
2024-02-02 14:25:33,771 Epoch 2489: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 14:25:33,772 EPOCH 2490
2024-02-02 14:25:40,733 Epoch 2490: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 14:25:40,734 EPOCH 2491
2024-02-02 14:25:47,508 Epoch 2491: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:25:47,509 EPOCH 2492
2024-02-02 14:25:54,317 Epoch 2492: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 14:25:54,317 EPOCH 2493
2024-02-02 14:26:00,975 Epoch 2493: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 14:26:00,976 EPOCH 2494
2024-02-02 14:26:07,874 Epoch 2494: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 14:26:07,874 EPOCH 2495
2024-02-02 14:26:08,430 [Epoch: 2495 Step: 00042400] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:     2306 || Batch Translation Loss:   0.023430 => Txt Tokens per Sec:     7013 || Lr: 0.000050
2024-02-02 14:26:13,982 Epoch 2495: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 14:26:13,982 EPOCH 2496
2024-02-02 14:26:20,499 Epoch 2496: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 14:26:20,500 EPOCH 2497
2024-02-02 14:26:27,213 Epoch 2497: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 14:26:27,214 EPOCH 2498
2024-02-02 14:26:34,049 Epoch 2498: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 14:26:34,049 EPOCH 2499
2024-02-02 14:26:40,664 Epoch 2499: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 14:26:40,664 EPOCH 2500
2024-02-02 14:26:47,542 [Epoch: 2500 Step: 00042500] Batch Recognition Loss:   0.000129 => Gls Tokens per Sec:     1546 || Batch Translation Loss:   0.012519 => Txt Tokens per Sec:     4291 || Lr: 0.000050
2024-02-02 14:26:47,543 Epoch 2500: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:26:47,543 EPOCH 2501
2024-02-02 14:26:54,376 Epoch 2501: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:26:54,376 EPOCH 2502
2024-02-02 14:27:00,976 Epoch 2502: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 14:27:00,977 EPOCH 2503
2024-02-02 14:27:07,818 Epoch 2503: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 14:27:07,819 EPOCH 2504
2024-02-02 14:27:14,546 Epoch 2504: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.75 
2024-02-02 14:27:14,547 EPOCH 2505
2024-02-02 14:27:21,492 Epoch 2505: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-02 14:27:21,492 EPOCH 2506
2024-02-02 14:27:27,607 [Epoch: 2506 Step: 00042600] Batch Recognition Loss:   0.000271 => Gls Tokens per Sec:     1529 || Batch Translation Loss:   0.084491 => Txt Tokens per Sec:     4241 || Lr: 0.000050
2024-02-02 14:27:28,300 Epoch 2506: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.76 
2024-02-02 14:27:28,301 EPOCH 2507
2024-02-02 14:27:34,864 Epoch 2507: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-02 14:27:34,865 EPOCH 2508
2024-02-02 14:27:41,613 Epoch 2508: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-02 14:27:41,614 EPOCH 2509
2024-02-02 14:27:48,314 Epoch 2509: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 14:27:48,315 EPOCH 2510
2024-02-02 14:27:55,246 Epoch 2510: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 14:27:55,246 EPOCH 2511
2024-02-02 14:28:02,083 Epoch 2511: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 14:28:02,084 EPOCH 2512
2024-02-02 14:28:07,811 [Epoch: 2512 Step: 00042700] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     1410 || Batch Translation Loss:   0.017486 => Txt Tokens per Sec:     3998 || Lr: 0.000050
2024-02-02 14:28:08,859 Epoch 2512: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 14:28:08,859 EPOCH 2513
2024-02-02 14:28:15,164 Epoch 2513: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-02 14:28:15,164 EPOCH 2514
2024-02-02 14:28:21,241 Epoch 2514: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.83 
2024-02-02 14:28:21,241 EPOCH 2515
2024-02-02 14:28:28,088 Epoch 2515: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.02 
2024-02-02 14:28:28,089 EPOCH 2516
2024-02-02 14:28:34,901 Epoch 2516: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-02 14:28:34,901 EPOCH 2517
2024-02-02 14:28:41,777 Epoch 2517: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-02 14:28:41,777 EPOCH 2518
2024-02-02 14:28:44,835 [Epoch: 2518 Step: 00042800] Batch Recognition Loss:   0.000394 => Gls Tokens per Sec:     2303 || Batch Translation Loss:   0.028432 => Txt Tokens per Sec:     6870 || Lr: 0.000050
2024-02-02 14:28:48,049 Epoch 2518: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 14:28:48,049 EPOCH 2519
2024-02-02 14:28:54,796 Epoch 2519: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 14:28:54,796 EPOCH 2520
2024-02-02 14:29:01,611 Epoch 2520: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 14:29:01,612 EPOCH 2521
2024-02-02 14:29:08,592 Epoch 2521: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:29:08,593 EPOCH 2522
2024-02-02 14:29:15,363 Epoch 2522: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 14:29:15,364 EPOCH 2523
2024-02-02 14:29:21,821 Epoch 2523: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:29:21,821 EPOCH 2524
2024-02-02 14:29:24,313 [Epoch: 2524 Step: 00042900] Batch Recognition Loss:   0.000191 => Gls Tokens per Sec:     2312 || Batch Translation Loss:   0.018530 => Txt Tokens per Sec:     6369 || Lr: 0.000050
2024-02-02 14:29:28,493 Epoch 2524: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:29:28,493 EPOCH 2525
2024-02-02 14:29:35,363 Epoch 2525: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:29:35,364 EPOCH 2526
2024-02-02 14:29:42,085 Epoch 2526: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:29:42,086 EPOCH 2527
2024-02-02 14:29:48,849 Epoch 2527: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:29:48,850 EPOCH 2528
2024-02-02 14:29:55,316 Epoch 2528: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:29:55,317 EPOCH 2529
2024-02-02 14:30:02,132 Epoch 2529: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:30:02,132 EPOCH 2530
2024-02-02 14:30:05,846 [Epoch: 2530 Step: 00043000] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:     1139 || Batch Translation Loss:   0.013981 => Txt Tokens per Sec:     2985 || Lr: 0.000050
2024-02-02 14:30:08,798 Epoch 2530: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:30:08,799 EPOCH 2531
2024-02-02 14:30:15,436 Epoch 2531: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:30:15,437 EPOCH 2532
2024-02-02 14:30:22,015 Epoch 2532: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:30:22,015 EPOCH 2533
2024-02-02 14:30:28,867 Epoch 2533: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 14:30:28,867 EPOCH 2534
2024-02-02 14:30:35,393 Epoch 2534: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:30:35,394 EPOCH 2535
2024-02-02 14:30:42,001 Epoch 2535: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:30:42,001 EPOCH 2536
2024-02-02 14:30:43,456 [Epoch: 2536 Step: 00043100] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     2202 || Batch Translation Loss:   0.012274 => Txt Tokens per Sec:     6682 || Lr: 0.000050
2024-02-02 14:30:48,554 Epoch 2536: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:30:48,554 EPOCH 2537
2024-02-02 14:30:55,121 Epoch 2537: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:30:55,121 EPOCH 2538
2024-02-02 14:31:01,832 Epoch 2538: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:31:01,832 EPOCH 2539
2024-02-02 14:31:08,154 Epoch 2539: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:31:08,155 EPOCH 2540
2024-02-02 14:31:14,108 Epoch 2540: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:31:14,108 EPOCH 2541
2024-02-02 14:31:20,964 Epoch 2541: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:31:20,964 EPOCH 2542
2024-02-02 14:31:23,455 [Epoch: 2542 Step: 00043200] Batch Recognition Loss:   0.000105 => Gls Tokens per Sec:      671 || Batch Translation Loss:   0.012464 => Txt Tokens per Sec:     1643 || Lr: 0.000050
2024-02-02 14:31:27,004 Epoch 2542: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:31:27,005 EPOCH 2543
2024-02-02 14:31:32,986 Epoch 2543: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:31:32,986 EPOCH 2544
2024-02-02 14:31:39,410 Epoch 2544: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:31:39,410 EPOCH 2545
2024-02-02 14:31:46,307 Epoch 2545: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:31:46,308 EPOCH 2546
2024-02-02 14:31:52,994 Epoch 2546: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:31:52,995 EPOCH 2547
2024-02-02 14:31:59,644 Epoch 2547: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:31:59,645 EPOCH 2548
2024-02-02 14:31:59,907 [Epoch: 2548 Step: 00043300] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     2452 || Batch Translation Loss:   0.013103 => Txt Tokens per Sec:     7153 || Lr: 0.000050
2024-02-02 14:32:06,457 Epoch 2548: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:32:06,458 EPOCH 2549
2024-02-02 14:32:13,091 Epoch 2549: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:32:13,092 EPOCH 2550
2024-02-02 14:32:19,964 Epoch 2550: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:32:19,965 EPOCH 2551
2024-02-02 14:32:26,652 Epoch 2551: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:32:26,653 EPOCH 2552
2024-02-02 14:32:33,616 Epoch 2552: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 14:32:33,617 EPOCH 2553
2024-02-02 14:32:40,208 [Epoch: 2553 Step: 00043400] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:     1516 || Batch Translation Loss:   0.015965 => Txt Tokens per Sec:     4248 || Lr: 0.000050
2024-02-02 14:32:40,398 Epoch 2553: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 14:32:40,398 EPOCH 2554
2024-02-02 14:32:46,940 Epoch 2554: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 14:32:46,941 EPOCH 2555
2024-02-02 14:32:53,732 Epoch 2555: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 14:32:53,733 EPOCH 2556
2024-02-02 14:33:00,384 Epoch 2556: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 14:33:00,384 EPOCH 2557
2024-02-02 14:33:07,318 Epoch 2557: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 14:33:07,319 EPOCH 2558
2024-02-02 14:33:14,023 Epoch 2558: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 14:33:14,024 EPOCH 2559
2024-02-02 14:33:19,752 [Epoch: 2559 Step: 00043500] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     1521 || Batch Translation Loss:   0.023123 => Txt Tokens per Sec:     4236 || Lr: 0.000050
2024-02-02 14:33:20,812 Epoch 2559: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 14:33:20,812 EPOCH 2560
2024-02-02 14:33:27,409 Epoch 2560: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 14:33:27,409 EPOCH 2561
2024-02-02 14:33:34,425 Epoch 2561: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 14:33:34,426 EPOCH 2562
2024-02-02 14:33:41,231 Epoch 2562: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 14:33:41,231 EPOCH 2563
2024-02-02 14:33:47,816 Epoch 2563: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:33:47,817 EPOCH 2564
2024-02-02 14:33:54,653 Epoch 2564: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:33:54,654 EPOCH 2565
2024-02-02 14:34:00,168 [Epoch: 2565 Step: 00043600] Batch Recognition Loss:   0.000100 => Gls Tokens per Sec:     1348 || Batch Translation Loss:   0.008288 => Txt Tokens per Sec:     3868 || Lr: 0.000050
2024-02-02 14:34:01,365 Epoch 2565: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:34:01,365 EPOCH 2566
2024-02-02 14:34:08,234 Epoch 2566: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:34:08,235 EPOCH 2567
2024-02-02 14:34:15,105 Epoch 2567: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:34:15,106 EPOCH 2568
2024-02-02 14:34:21,763 Epoch 2568: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 14:34:21,764 EPOCH 2569
2024-02-02 14:34:28,427 Epoch 2569: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 14:34:28,427 EPOCH 2570
2024-02-02 14:34:34,846 Epoch 2570: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 14:34:34,846 EPOCH 2571
2024-02-02 14:34:39,797 [Epoch: 2571 Step: 00043700] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     1242 || Batch Translation Loss:   0.017401 => Txt Tokens per Sec:     3472 || Lr: 0.000050
2024-02-02 14:34:41,728 Epoch 2571: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 14:34:41,728 EPOCH 2572
2024-02-02 14:34:48,593 Epoch 2572: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 14:34:48,594 EPOCH 2573
2024-02-02 14:34:55,372 Epoch 2573: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 14:34:55,372 EPOCH 2574
2024-02-02 14:35:01,926 Epoch 2574: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 14:35:01,926 EPOCH 2575
2024-02-02 14:35:08,696 Epoch 2575: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 14:35:08,697 EPOCH 2576
2024-02-02 14:35:15,446 Epoch 2576: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 14:35:15,446 EPOCH 2577
2024-02-02 14:35:17,524 [Epoch: 2577 Step: 00043800] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:     2466 || Batch Translation Loss:   0.011945 => Txt Tokens per Sec:     6430 || Lr: 0.000050
2024-02-02 14:35:22,202 Epoch 2577: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 14:35:22,202 EPOCH 2578
2024-02-02 14:35:28,972 Epoch 2578: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 14:35:28,972 EPOCH 2579
2024-02-02 14:35:35,829 Epoch 2579: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 14:35:35,830 EPOCH 2580
2024-02-02 14:35:42,460 Epoch 2580: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 14:35:42,461 EPOCH 2581
2024-02-02 14:35:49,282 Epoch 2581: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 14:35:49,282 EPOCH 2582
2024-02-02 14:35:56,028 Epoch 2582: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 14:35:56,028 EPOCH 2583
2024-02-02 14:35:57,018 [Epoch: 2583 Step: 00043900] Batch Recognition Loss:   0.000157 => Gls Tokens per Sec:     3885 || Batch Translation Loss:   0.018976 => Txt Tokens per Sec:     9606 || Lr: 0.000050
2024-02-02 14:36:02,635 Epoch 2583: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 14:36:02,636 EPOCH 2584
2024-02-02 14:36:09,394 Epoch 2584: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 14:36:09,395 EPOCH 2585
2024-02-02 14:36:15,931 Epoch 2585: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:36:15,932 EPOCH 2586
2024-02-02 14:36:22,482 Epoch 2586: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:36:22,483 EPOCH 2587
2024-02-02 14:36:29,110 Epoch 2587: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 14:36:29,111 EPOCH 2588
2024-02-02 14:36:35,880 Epoch 2588: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:36:35,880 EPOCH 2589
2024-02-02 14:36:37,023 [Epoch: 2589 Step: 00044000] Batch Recognition Loss:   0.000157 => Gls Tokens per Sec:     2240 || Batch Translation Loss:   0.032227 => Txt Tokens per Sec:     6859 || Lr: 0.000050
2024-02-02 14:37:07,076 Validation result at epoch 2589, step    44000: duration: 30.0531s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00060	Translation Loss: 99933.85938	PPL: 22034.97461
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.31	(BLEU-1: 9.33,	BLEU-2: 2.66,	BLEU-3: 0.86,	BLEU-4: 0.31)
	CHRF 16.33	ROUGE 8.23
2024-02-02 14:37:07,077 Logging Recognition and Translation Outputs
2024-02-02 14:37:07,077 ========================================================================================================================
2024-02-02 14:37:07,077 Logging Sequence: 117_29.00
2024-02-02 14:37:07,078 	Gloss Reference :	A B+C+D+E
2024-02-02 14:37:07,078 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 14:37:07,078 	Gloss Alignment :	         
2024-02-02 14:37:07,078 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 14:37:07,080 	Text Reference  :	however england was unable to reach the   target they  were all      out lost   by    66 runs   
2024-02-02 14:37:07,080 	Text Hypothesis :	******* ******* it  is     a  tough medal since  india and  pakistan is  played every 4  wickets
2024-02-02 14:37:07,080 	Text Alignment  :	D       D       S   S      S  S     S     S      S     S    S        S   S      S     S  S      
2024-02-02 14:37:07,080 ========================================================================================================================
2024-02-02 14:37:07,080 Logging Sequence: 84_176.00
2024-02-02 14:37:07,081 	Gloss Reference :	A B+C+D+E
2024-02-02 14:37:07,081 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 14:37:07,081 	Gloss Alignment :	         
2024-02-02 14:37:07,081 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 14:37:07,082 	Text Reference  :	germany's nancy faeser who  attended the game in    doha    against japan said   
2024-02-02 14:37:07,082 	Text Hypothesis :	on        9th   april  2023 there    was a    match between india   and   everton
2024-02-02 14:37:07,082 	Text Alignment  :	S         S     S      S    S        S   S    S     S       S       S     S      
2024-02-02 14:37:07,082 ========================================================================================================================
2024-02-02 14:37:07,082 Logging Sequence: 172_98.00
2024-02-02 14:37:07,083 	Gloss Reference :	A B+C+D+E
2024-02-02 14:37:07,083 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 14:37:07,083 	Gloss Alignment :	         
2024-02-02 14:37:07,083 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 14:37:07,084 	Text Reference  :	since 700 pm    it  kept      raining the ******* ******* ***** ***** *** *********** intensity plunged     around 915     pm         
2024-02-02 14:37:07,084 	Text Hypothesis :	***** the match was scheduled at      the stadium chennai super kings had kickstarted the       blockbuster season against argentina's
2024-02-02 14:37:07,085 	Text Alignment  :	D     S   S     S   S         S           I       I       I     I     I   I           S         S           S      S       S          
2024-02-02 14:37:07,085 ========================================================================================================================
2024-02-02 14:37:07,085 Logging Sequence: 135_92.00
2024-02-02 14:37:07,085 	Gloss Reference :	A B+C+D+E
2024-02-02 14:37:07,085 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 14:37:07,085 	Gloss Alignment :	         
2024-02-02 14:37:07,085 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 14:37:07,087 	Text Reference  :	she ****** ** ***** ** wrote that     half had  already been   raised by the family's online fundraiser
2024-02-02 14:37:07,087 	Text Hypothesis :	she wanted to focus on her   training and  some heart   issues which  is why he       passed away      
2024-02-02 14:37:07,087 	Text Alignment  :	    I      I  I     I  S     S        S    S    S       S      S      S  S   S        S      S         
2024-02-02 14:37:07,087 ========================================================================================================================
2024-02-02 14:37:07,087 Logging Sequence: 180_332.00
2024-02-02 14:37:07,087 	Gloss Reference :	A B+C+D+E
2024-02-02 14:37:07,088 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 14:37:07,088 	Gloss Alignment :	         
2024-02-02 14:37:07,088 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 14:37:07,089 	Text Reference  :	did i eat roti made of  shilajit that i got energy   to    assault so   many girls
2024-02-02 14:37:07,089 	Text Hypothesis :	*** i am  sure you  all know     that * *** mahendra singh dhoni   will be   fined
2024-02-02 14:37:07,089 	Text Alignment  :	D     S   S    S    S   S             D D   S        S     S       S    S    S    
2024-02-02 14:37:07,089 ========================================================================================================================
2024-02-02 14:37:12,793 Epoch 2589: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:37:12,793 EPOCH 2590
2024-02-02 14:37:19,768 Epoch 2590: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:37:19,769 EPOCH 2591
2024-02-02 14:37:26,395 Epoch 2591: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 14:37:26,396 EPOCH 2592
2024-02-02 14:37:33,338 Epoch 2592: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:37:33,338 EPOCH 2593
2024-02-02 14:37:39,759 Epoch 2593: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 14:37:39,759 EPOCH 2594
2024-02-02 14:37:46,217 Epoch 2594: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 14:37:46,218 EPOCH 2595
2024-02-02 14:37:46,490 [Epoch: 2595 Step: 00044100] Batch Recognition Loss:   0.000092 => Gls Tokens per Sec:     4726 || Batch Translation Loss:   0.007892 => Txt Tokens per Sec:    10271 || Lr: 0.000050
2024-02-02 14:37:52,208 Epoch 2595: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 14:37:52,208 EPOCH 2596
2024-02-02 14:37:59,131 Epoch 2596: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 14:37:59,132 EPOCH 2597
2024-02-02 14:38:06,198 Epoch 2597: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:38:06,198 EPOCH 2598
2024-02-02 14:38:12,733 Epoch 2598: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:38:12,734 EPOCH 2599
2024-02-02 14:38:19,634 Epoch 2599: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 14:38:19,635 EPOCH 2600
2024-02-02 14:38:26,652 [Epoch: 2600 Step: 00044200] Batch Recognition Loss:   0.000099 => Gls Tokens per Sec:     1515 || Batch Translation Loss:   0.007281 => Txt Tokens per Sec:     4206 || Lr: 0.000050
2024-02-02 14:38:26,653 Epoch 2600: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 14:38:26,653 EPOCH 2601
2024-02-02 14:38:33,688 Epoch 2601: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 14:38:33,689 EPOCH 2602
2024-02-02 14:38:40,234 Epoch 2602: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 14:38:40,235 EPOCH 2603
2024-02-02 14:38:47,079 Epoch 2603: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 14:38:47,080 EPOCH 2604
2024-02-02 14:38:53,747 Epoch 2604: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-02 14:38:53,747 EPOCH 2605
2024-02-02 14:39:00,497 Epoch 2605: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 14:39:00,497 EPOCH 2606
2024-02-02 14:39:06,714 [Epoch: 2606 Step: 00044300] Batch Recognition Loss:   0.000207 => Gls Tokens per Sec:     1505 || Batch Translation Loss:   0.024855 => Txt Tokens per Sec:     4182 || Lr: 0.000050
2024-02-02 14:39:07,148 Epoch 2606: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 14:39:07,148 EPOCH 2607
2024-02-02 14:39:13,753 Epoch 2607: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-02 14:39:13,754 EPOCH 2608
2024-02-02 14:39:20,564 Epoch 2608: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 14:39:20,564 EPOCH 2609
2024-02-02 14:39:27,391 Epoch 2609: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 14:39:27,392 EPOCH 2610
2024-02-02 14:39:34,097 Epoch 2610: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 14:39:34,098 EPOCH 2611
2024-02-02 14:39:40,615 Epoch 2611: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 14:39:40,616 EPOCH 2612
2024-02-02 14:39:46,455 [Epoch: 2612 Step: 00044400] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:     1382 || Batch Translation Loss:   0.012203 => Txt Tokens per Sec:     3902 || Lr: 0.000050
2024-02-02 14:39:47,300 Epoch 2612: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 14:39:47,301 EPOCH 2613
2024-02-02 14:39:54,253 Epoch 2613: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:39:54,254 EPOCH 2614
2024-02-02 14:40:01,009 Epoch 2614: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 14:40:01,009 EPOCH 2615
2024-02-02 14:40:07,481 Epoch 2615: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 14:40:07,482 EPOCH 2616
2024-02-02 14:40:14,373 Epoch 2616: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:40:14,373 EPOCH 2617
2024-02-02 14:40:20,978 Epoch 2617: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 14:40:20,978 EPOCH 2618
2024-02-02 14:40:25,923 [Epoch: 2618 Step: 00044500] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     1374 || Batch Translation Loss:   0.030211 => Txt Tokens per Sec:     3706 || Lr: 0.000050
2024-02-02 14:40:27,874 Epoch 2618: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-02 14:40:27,875 EPOCH 2619
2024-02-02 14:40:34,563 Epoch 2619: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 14:40:34,564 EPOCH 2620
2024-02-02 14:40:41,035 Epoch 2620: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 14:40:41,036 EPOCH 2621
2024-02-02 14:40:47,961 Epoch 2621: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 14:40:47,962 EPOCH 2622
2024-02-02 14:40:54,811 Epoch 2622: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 14:40:54,811 EPOCH 2623
2024-02-02 14:41:01,571 Epoch 2623: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-02 14:41:01,572 EPOCH 2624
2024-02-02 14:41:06,072 [Epoch: 2624 Step: 00044600] Batch Recognition Loss:   0.000239 => Gls Tokens per Sec:     1225 || Batch Translation Loss:   0.048503 => Txt Tokens per Sec:     3559 || Lr: 0.000050
2024-02-02 14:41:08,185 Epoch 2624: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 14:41:08,185 EPOCH 2625
2024-02-02 14:41:14,853 Epoch 2625: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 14:41:14,853 EPOCH 2626
2024-02-02 14:41:21,336 Epoch 2626: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 14:41:21,336 EPOCH 2627
2024-02-02 14:41:28,248 Epoch 2627: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 14:41:28,249 EPOCH 2628
2024-02-02 14:41:35,269 Epoch 2628: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 14:41:35,270 EPOCH 2629
2024-02-02 14:41:42,187 Epoch 2629: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 14:41:42,187 EPOCH 2630
2024-02-02 14:41:43,760 [Epoch: 2630 Step: 00044700] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:     2850 || Batch Translation Loss:   0.015427 => Txt Tokens per Sec:     8058 || Lr: 0.000050
2024-02-02 14:41:48,674 Epoch 2630: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 14:41:48,675 EPOCH 2631
2024-02-02 14:41:55,599 Epoch 2631: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-02 14:41:55,600 EPOCH 2632
2024-02-02 14:42:02,324 Epoch 2632: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-02 14:42:02,324 EPOCH 2633
2024-02-02 14:42:08,774 Epoch 2633: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 14:42:08,774 EPOCH 2634
2024-02-02 14:42:15,364 Epoch 2634: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 14:42:15,365 EPOCH 2635
2024-02-02 14:42:21,641 Epoch 2635: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-02 14:42:21,641 EPOCH 2636
2024-02-02 14:42:24,830 [Epoch: 2636 Step: 00044800] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:      926 || Batch Translation Loss:   0.023866 => Txt Tokens per Sec:     2540 || Lr: 0.000050
2024-02-02 14:42:28,618 Epoch 2636: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-02 14:42:28,619 EPOCH 2637
2024-02-02 14:42:35,489 Epoch 2637: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 14:42:35,489 EPOCH 2638
2024-02-02 14:42:42,338 Epoch 2638: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-02 14:42:42,338 EPOCH 2639
2024-02-02 14:42:49,040 Epoch 2639: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-02 14:42:49,041 EPOCH 2640
2024-02-02 14:42:55,674 Epoch 2640: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 14:42:55,675 EPOCH 2641
2024-02-02 14:43:02,445 Epoch 2641: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 14:43:02,446 EPOCH 2642
2024-02-02 14:43:02,958 [Epoch: 2642 Step: 00044900] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:     3768 || Batch Translation Loss:   0.024203 => Txt Tokens per Sec:     8666 || Lr: 0.000050
2024-02-02 14:43:09,047 Epoch 2642: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 14:43:09,047 EPOCH 2643
2024-02-02 14:43:15,977 Epoch 2643: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 14:43:15,977 EPOCH 2644
2024-02-02 14:43:22,579 Epoch 2644: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 14:43:22,580 EPOCH 2645
2024-02-02 14:43:29,196 Epoch 2645: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 14:43:29,196 EPOCH 2646
2024-02-02 14:43:35,903 Epoch 2646: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 14:43:35,904 EPOCH 2647
2024-02-02 14:43:42,879 Epoch 2647: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 14:43:42,880 EPOCH 2648
2024-02-02 14:43:43,190 [Epoch: 2648 Step: 00045000] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     2078 || Batch Translation Loss:   0.017582 => Txt Tokens per Sec:     6669 || Lr: 0.000050
2024-02-02 14:43:49,743 Epoch 2648: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 14:43:49,743 EPOCH 2649
2024-02-02 14:43:56,406 Epoch 2649: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:43:56,406 EPOCH 2650
2024-02-02 14:44:02,849 Epoch 2650: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:44:02,850 EPOCH 2651
2024-02-02 14:44:09,522 Epoch 2651: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:44:09,523 EPOCH 2652
2024-02-02 14:44:16,356 Epoch 2652: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:44:16,356 EPOCH 2653
2024-02-02 14:44:22,860 [Epoch: 2653 Step: 00045100] Batch Recognition Loss:   0.000120 => Gls Tokens per Sec:     1536 || Batch Translation Loss:   0.015582 => Txt Tokens per Sec:     4286 || Lr: 0.000050
2024-02-02 14:44:23,024 Epoch 2653: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:44:23,024 EPOCH 2654
2024-02-02 14:44:29,593 Epoch 2654: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:44:29,594 EPOCH 2655
2024-02-02 14:44:36,431 Epoch 2655: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:44:36,431 EPOCH 2656
2024-02-02 14:44:43,273 Epoch 2656: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:44:43,274 EPOCH 2657
2024-02-02 14:44:50,381 Epoch 2657: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:44:50,381 EPOCH 2658
2024-02-02 14:44:57,182 Epoch 2658: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:44:57,182 EPOCH 2659
2024-02-02 14:45:00,817 [Epoch: 2659 Step: 00045200] Batch Recognition Loss:   0.000114 => Gls Tokens per Sec:     2466 || Batch Translation Loss:   0.012846 => Txt Tokens per Sec:     6952 || Lr: 0.000050
2024-02-02 14:45:03,487 Epoch 2659: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:45:03,488 EPOCH 2660
2024-02-02 14:45:10,402 Epoch 2660: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:45:10,402 EPOCH 2661
2024-02-02 14:45:17,191 Epoch 2661: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 14:45:17,192 EPOCH 2662
2024-02-02 14:45:23,808 Epoch 2662: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 14:45:23,809 EPOCH 2663
2024-02-02 14:45:30,233 Epoch 2663: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 14:45:30,234 EPOCH 2664
2024-02-02 14:45:37,227 Epoch 2664: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:45:37,228 EPOCH 2665
2024-02-02 14:45:40,963 [Epoch: 2665 Step: 00045300] Batch Recognition Loss:   0.000109 => Gls Tokens per Sec:     2057 || Batch Translation Loss:   0.019619 => Txt Tokens per Sec:     5803 || Lr: 0.000050
2024-02-02 14:45:44,176 Epoch 2665: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 14:45:44,176 EPOCH 2666
2024-02-02 14:45:51,071 Epoch 2666: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 14:45:51,071 EPOCH 2667
2024-02-02 14:45:57,814 Epoch 2667: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 14:45:57,815 EPOCH 2668
2024-02-02 14:46:04,402 Epoch 2668: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 14:46:04,403 EPOCH 2669
2024-02-02 14:46:11,002 Epoch 2669: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 14:46:11,002 EPOCH 2670
2024-02-02 14:46:17,841 Epoch 2670: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:46:17,842 EPOCH 2671
2024-02-02 14:46:22,383 [Epoch: 2671 Step: 00045400] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:     1355 || Batch Translation Loss:   0.016790 => Txt Tokens per Sec:     3756 || Lr: 0.000050
2024-02-02 14:46:24,482 Epoch 2671: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 14:46:24,482 EPOCH 2672
2024-02-02 14:46:31,180 Epoch 2672: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 14:46:31,180 EPOCH 2673
2024-02-02 14:46:37,863 Epoch 2673: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.84 
2024-02-02 14:46:37,864 EPOCH 2674
2024-02-02 14:46:44,945 Epoch 2674: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.68 
2024-02-02 14:46:44,945 EPOCH 2675
2024-02-02 14:46:51,766 Epoch 2675: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.76 
2024-02-02 14:46:51,766 EPOCH 2676
2024-02-02 14:46:58,527 Epoch 2676: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.30 
2024-02-02 14:46:58,527 EPOCH 2677
2024-02-02 14:47:02,807 [Epoch: 2677 Step: 00045500] Batch Recognition Loss:   0.000407 => Gls Tokens per Sec:     1138 || Batch Translation Loss:   0.029485 => Txt Tokens per Sec:     3326 || Lr: 0.000050
2024-02-02 14:47:05,257 Epoch 2677: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-02 14:47:05,258 EPOCH 2678
2024-02-02 14:47:12,257 Epoch 2678: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-02 14:47:12,258 EPOCH 2679
2024-02-02 14:47:19,450 Epoch 2679: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 14:47:19,450 EPOCH 2680
2024-02-02 14:47:26,174 Epoch 2680: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 14:47:26,174 EPOCH 2681
2024-02-02 14:47:33,166 Epoch 2681: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 14:47:33,166 EPOCH 2682
2024-02-02 14:47:40,015 Epoch 2682: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 14:47:40,016 EPOCH 2683
2024-02-02 14:47:41,937 [Epoch: 2683 Step: 00045600] Batch Recognition Loss:   0.000247 => Gls Tokens per Sec:     2001 || Batch Translation Loss:   0.021385 => Txt Tokens per Sec:     5969 || Lr: 0.000050
2024-02-02 14:47:46,692 Epoch 2683: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 14:47:46,693 EPOCH 2684
2024-02-02 14:47:53,384 Epoch 2684: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 14:47:53,384 EPOCH 2685
2024-02-02 14:48:00,272 Epoch 2685: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 14:48:00,272 EPOCH 2686
2024-02-02 14:48:06,969 Epoch 2686: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 14:48:06,970 EPOCH 2687
2024-02-02 14:48:13,962 Epoch 2687: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:48:13,963 EPOCH 2688
2024-02-02 14:48:20,369 Epoch 2688: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:48:20,370 EPOCH 2689
2024-02-02 14:48:21,734 [Epoch: 2689 Step: 00045700] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:     1878 || Batch Translation Loss:   0.021962 => Txt Tokens per Sec:     5627 || Lr: 0.000050
2024-02-02 14:48:26,964 Epoch 2689: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:48:26,964 EPOCH 2690
2024-02-02 14:48:33,922 Epoch 2690: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:48:33,923 EPOCH 2691
2024-02-02 14:48:40,715 Epoch 2691: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:48:40,715 EPOCH 2692
2024-02-02 14:48:47,464 Epoch 2692: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:48:47,465 EPOCH 2693
2024-02-02 14:48:54,317 Epoch 2693: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:48:54,317 EPOCH 2694
2024-02-02 14:49:01,085 Epoch 2694: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:49:01,085 EPOCH 2695
2024-02-02 14:49:01,487 [Epoch: 2695 Step: 00045800] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:     3200 || Batch Translation Loss:   0.012954 => Txt Tokens per Sec:     8653 || Lr: 0.000050
2024-02-02 14:49:07,770 Epoch 2695: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:49:07,770 EPOCH 2696
2024-02-02 14:49:14,376 Epoch 2696: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:49:14,377 EPOCH 2697
2024-02-02 14:49:20,858 Epoch 2697: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:49:20,858 EPOCH 2698
2024-02-02 14:49:27,436 Epoch 2698: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:49:27,437 EPOCH 2699
2024-02-02 14:49:34,222 Epoch 2699: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:49:34,223 EPOCH 2700
2024-02-02 14:49:41,168 [Epoch: 2700 Step: 00045900] Batch Recognition Loss:   0.000113 => Gls Tokens per Sec:     1531 || Batch Translation Loss:   0.012731 => Txt Tokens per Sec:     4250 || Lr: 0.000050
2024-02-02 14:49:41,169 Epoch 2700: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:49:41,169 EPOCH 2701
2024-02-02 14:49:48,132 Epoch 2701: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:49:48,132 EPOCH 2702
2024-02-02 14:49:54,903 Epoch 2702: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:49:54,903 EPOCH 2703
2024-02-02 14:50:01,498 Epoch 2703: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:50:01,498 EPOCH 2704
2024-02-02 14:50:08,378 Epoch 2704: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:50:08,378 EPOCH 2705
2024-02-02 14:50:15,020 Epoch 2705: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:50:15,021 EPOCH 2706
2024-02-02 14:50:21,037 [Epoch: 2706 Step: 00046000] Batch Recognition Loss:   0.000102 => Gls Tokens per Sec:     1554 || Batch Translation Loss:   0.007311 => Txt Tokens per Sec:     4225 || Lr: 0.000050
2024-02-02 14:50:51,018 Validation result at epoch 2706, step    46000: duration: 29.9801s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00058	Translation Loss: 100606.31250	PPL: 23568.78711
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.67	(BLEU-1: 10.39,	BLEU-2: 3.25,	BLEU-3: 1.35,	BLEU-4: 0.67)
	CHRF 16.86	ROUGE 8.94
2024-02-02 14:50:51,019 Logging Recognition and Translation Outputs
2024-02-02 14:50:51,019 ========================================================================================================================
2024-02-02 14:50:51,020 Logging Sequence: 126_121.00
2024-02-02 14:50:51,020 	Gloss Reference :	A B+C+D+E
2024-02-02 14:50:51,020 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 14:50:51,020 	Gloss Alignment :	         
2024-02-02 14:50:51,021 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 14:50:51,021 	Text Reference  :	everyone was very happy by his  victory
2024-02-02 14:50:51,021 	Text Hypothesis :	******** *** **** let   me tell you    
2024-02-02 14:50:51,021 	Text Alignment  :	D        D   D    S     S  S    S      
2024-02-02 14:50:51,021 ========================================================================================================================
2024-02-02 14:50:51,021 Logging Sequence: 73_79.00
2024-02-02 14:50:51,022 	Gloss Reference :	A B+C+D+E
2024-02-02 14:50:51,022 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 14:50:51,022 	Gloss Alignment :	         
2024-02-02 14:50:51,022 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 14:50:51,023 	Text Reference  :	raina resturant has food from  the rich spices of north india to   the    aromatic curries of    south  india
2024-02-02 14:50:51,024 	Text Hypothesis :	***** ********* *** **** there was a    total  of ***** ***** fury people blamed   from    their second loss 
2024-02-02 14:50:51,024 	Text Alignment  :	D     D         D   D    S     S   S    S         D     D     S    S      S        S       S     S      S    
2024-02-02 14:50:51,024 ========================================================================================================================
2024-02-02 14:50:51,024 Logging Sequence: 95_152.00
2024-02-02 14:50:51,024 	Gloss Reference :	A B+C+D+E
2024-02-02 14:50:51,024 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 14:50:51,024 	Gloss Alignment :	         
2024-02-02 14:50:51,024 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 14:50:51,025 	Text Reference  :	** *** *** *** how    strange
2024-02-02 14:50:51,025 	Text Hypothesis :	on 5th may the police learnt 
2024-02-02 14:50:51,025 	Text Alignment  :	I  I   I   I   S      S      
2024-02-02 14:50:51,025 ========================================================================================================================
2024-02-02 14:50:51,025 Logging Sequence: 135_39.00
2024-02-02 14:50:51,026 	Gloss Reference :	A B+C+D+E
2024-02-02 14:50:51,026 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 14:50:51,026 	Gloss Alignment :	         
2024-02-02 14:50:51,026 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 14:50:51,027 	Text Reference  :	who needs  to travel from poland to   stanford university in         california
2024-02-02 14:50:51,027 	Text Hypothesis :	he  wanted to step   down as     they want     to         dhanashree verma     
2024-02-02 14:50:51,027 	Text Alignment  :	S   S         S      S    S      S    S        S          S          S         
2024-02-02 14:50:51,027 ========================================================================================================================
2024-02-02 14:50:51,027 Logging Sequence: 87_2.00
2024-02-02 14:50:51,027 	Gloss Reference :	A B+C+D+E
2024-02-02 14:50:51,027 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 14:50:51,028 	Gloss Alignment :	         
2024-02-02 14:50:51,028 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 14:50:51,029 	Text Reference  :	cricketer gautam gambhir's jealousy against ms         dhoni     and    virat kohli has been increasing day   by day    
2024-02-02 14:50:51,029 	Text Hypothesis :	let       me     tell      you      how     australia' legendary bowler shane warne has left dhoni      about 3  formats
2024-02-02 14:50:51,029 	Text Alignment  :	S         S      S         S        S       S          S         S      S     S         S    S          S     S  S      
2024-02-02 14:50:51,030 ========================================================================================================================
2024-02-02 14:50:51,792 Epoch 2706: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:50:51,792 EPOCH 2707
2024-02-02 14:50:58,646 Epoch 2707: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:50:58,647 EPOCH 2708
2024-02-02 14:51:05,372 Epoch 2708: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 14:51:05,373 EPOCH 2709
2024-02-02 14:51:11,958 Epoch 2709: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 14:51:11,958 EPOCH 2710
2024-02-02 14:51:18,987 Epoch 2710: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 14:51:18,987 EPOCH 2711
2024-02-02 14:51:25,961 Epoch 2711: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 14:51:25,962 EPOCH 2712
2024-02-02 14:51:31,666 [Epoch: 2712 Step: 00046100] Batch Recognition Loss:   0.000117 => Gls Tokens per Sec:     1415 || Batch Translation Loss:   0.010469 => Txt Tokens per Sec:     3843 || Lr: 0.000050
2024-02-02 14:51:32,841 Epoch 2712: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 14:51:32,842 EPOCH 2713
2024-02-02 14:51:39,270 Epoch 2713: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 14:51:39,271 EPOCH 2714
2024-02-02 14:51:45,979 Epoch 2714: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 14:51:45,980 EPOCH 2715
2024-02-02 14:51:52,749 Epoch 2715: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 14:51:52,750 EPOCH 2716
2024-02-02 14:51:59,359 Epoch 2716: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 14:51:59,359 EPOCH 2717
2024-02-02 14:52:05,687 Epoch 2717: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:52:05,687 EPOCH 2718
2024-02-02 14:52:11,146 [Epoch: 2718 Step: 00046200] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:     1244 || Batch Translation Loss:   0.024959 => Txt Tokens per Sec:     3717 || Lr: 0.000050
2024-02-02 14:52:12,617 Epoch 2718: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:52:12,618 EPOCH 2719
2024-02-02 14:52:19,512 Epoch 2719: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:52:19,513 EPOCH 2720
2024-02-02 14:52:26,157 Epoch 2720: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:52:26,157 EPOCH 2721
2024-02-02 14:52:32,402 Epoch 2721: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:52:32,402 EPOCH 2722
2024-02-02 14:52:38,298 Epoch 2722: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:52:38,298 EPOCH 2723
2024-02-02 14:52:44,244 Epoch 2723: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:52:44,245 EPOCH 2724
2024-02-02 14:52:46,467 [Epoch: 2724 Step: 00046300] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:     2592 || Batch Translation Loss:   0.018740 => Txt Tokens per Sec:     6920 || Lr: 0.000050
2024-02-02 14:52:50,427 Epoch 2724: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:52:50,427 EPOCH 2725
2024-02-02 14:52:57,407 Epoch 2725: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:52:57,407 EPOCH 2726
2024-02-02 14:53:04,269 Epoch 2726: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:53:04,269 EPOCH 2727
2024-02-02 14:53:11,154 Epoch 2727: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:53:11,154 EPOCH 2728
2024-02-02 14:53:17,983 Epoch 2728: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:53:17,984 EPOCH 2729
2024-02-02 14:53:24,354 Epoch 2729: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:53:24,355 EPOCH 2730
2024-02-02 14:53:28,375 [Epoch: 2730 Step: 00046400] Batch Recognition Loss:   0.000117 => Gls Tokens per Sec:     1052 || Batch Translation Loss:   0.016147 => Txt Tokens per Sec:     2968 || Lr: 0.000050
2024-02-02 14:53:31,089 Epoch 2730: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:53:31,090 EPOCH 2731
2024-02-02 14:53:37,758 Epoch 2731: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:53:37,759 EPOCH 2732
2024-02-02 14:53:44,236 Epoch 2732: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:53:44,237 EPOCH 2733
2024-02-02 14:53:50,935 Epoch 2733: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:53:50,936 EPOCH 2734
2024-02-02 14:53:57,921 Epoch 2734: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 14:53:57,922 EPOCH 2735
2024-02-02 14:54:04,481 Epoch 2735: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 14:54:04,481 EPOCH 2736
2024-02-02 14:54:05,643 [Epoch: 2736 Step: 00046500] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:     2755 || Batch Translation Loss:   0.013724 => Txt Tokens per Sec:     7036 || Lr: 0.000050
2024-02-02 14:54:10,748 Epoch 2736: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 14:54:10,748 EPOCH 2737
2024-02-02 14:54:16,891 Epoch 2737: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 14:54:16,891 EPOCH 2738
2024-02-02 14:54:23,858 Epoch 2738: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 14:54:23,859 EPOCH 2739
2024-02-02 14:54:30,724 Epoch 2739: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-02 14:54:30,724 EPOCH 2740
2024-02-02 14:54:37,327 Epoch 2740: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-02 14:54:37,327 EPOCH 2741
2024-02-02 14:54:43,722 Epoch 2741: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 14:54:43,723 EPOCH 2742
2024-02-02 14:54:46,578 [Epoch: 2742 Step: 00046600] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:      585 || Batch Translation Loss:   0.030918 => Txt Tokens per Sec:     1662 || Lr: 0.000050
2024-02-02 14:54:50,624 Epoch 2742: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 14:54:50,625 EPOCH 2743
2024-02-02 14:54:57,279 Epoch 2743: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 14:54:57,279 EPOCH 2744
2024-02-02 14:55:04,163 Epoch 2744: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 14:55:04,164 EPOCH 2745
2024-02-02 14:55:10,835 Epoch 2745: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 14:55:10,835 EPOCH 2746
2024-02-02 14:55:17,635 Epoch 2746: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 14:55:17,636 EPOCH 2747
2024-02-02 14:55:24,272 Epoch 2747: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 14:55:24,273 EPOCH 2748
2024-02-02 14:55:24,673 [Epoch: 2748 Step: 00046700] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:     1604 || Batch Translation Loss:   0.024891 => Txt Tokens per Sec:     5133 || Lr: 0.000050
2024-02-02 14:55:31,049 Epoch 2748: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 14:55:31,049 EPOCH 2749
2024-02-02 14:55:37,824 Epoch 2749: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 14:55:37,825 EPOCH 2750
2024-02-02 14:55:44,445 Epoch 2750: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 14:55:44,446 EPOCH 2751
2024-02-02 14:55:51,184 Epoch 2751: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-02 14:55:51,185 EPOCH 2752
2024-02-02 14:55:57,780 Epoch 2752: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 14:55:57,780 EPOCH 2753
2024-02-02 14:56:04,455 [Epoch: 2753 Step: 00046800] Batch Recognition Loss:   0.000228 => Gls Tokens per Sec:     1497 || Batch Translation Loss:   0.043622 => Txt Tokens per Sec:     4131 || Lr: 0.000050
2024-02-02 14:56:04,700 Epoch 2753: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.76 
2024-02-02 14:56:04,700 EPOCH 2754
2024-02-02 14:56:11,469 Epoch 2754: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.15 
2024-02-02 14:56:11,469 EPOCH 2755
2024-02-02 14:56:18,100 Epoch 2755: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-02 14:56:18,100 EPOCH 2756
2024-02-02 14:56:24,806 Epoch 2756: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-02 14:56:24,806 EPOCH 2757
2024-02-02 14:56:32,040 Epoch 2757: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-02 14:56:32,041 EPOCH 2758
2024-02-02 14:56:38,859 Epoch 2758: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-02 14:56:38,860 EPOCH 2759
2024-02-02 14:56:44,850 [Epoch: 2759 Step: 00046900] Batch Recognition Loss:   0.000313 => Gls Tokens per Sec:     1454 || Batch Translation Loss:   0.028199 => Txt Tokens per Sec:     3987 || Lr: 0.000050
2024-02-02 14:56:45,650 Epoch 2759: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 14:56:45,650 EPOCH 2760
2024-02-02 14:56:52,324 Epoch 2760: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 14:56:52,325 EPOCH 2761
2024-02-02 14:56:58,948 Epoch 2761: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 14:56:58,949 EPOCH 2762
2024-02-02 14:57:05,787 Epoch 2762: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 14:57:05,788 EPOCH 2763
2024-02-02 14:57:11,822 Epoch 2763: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 14:57:11,823 EPOCH 2764
2024-02-02 14:57:18,613 Epoch 2764: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 14:57:18,613 EPOCH 2765
2024-02-02 14:57:23,987 [Epoch: 2765 Step: 00047000] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     1383 || Batch Translation Loss:   0.018661 => Txt Tokens per Sec:     4002 || Lr: 0.000050
2024-02-02 14:57:25,287 Epoch 2765: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 14:57:25,287 EPOCH 2766
2024-02-02 14:57:31,786 Epoch 2766: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:57:31,787 EPOCH 2767
2024-02-02 14:57:38,647 Epoch 2767: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 14:57:38,648 EPOCH 2768
2024-02-02 14:57:45,401 Epoch 2768: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 14:57:45,402 EPOCH 2769
2024-02-02 14:57:52,598 Epoch 2769: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 14:57:52,599 EPOCH 2770
2024-02-02 14:57:59,472 Epoch 2770: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:57:59,472 EPOCH 2771
2024-02-02 14:58:04,515 [Epoch: 2771 Step: 00047100] Batch Recognition Loss:   0.000115 => Gls Tokens per Sec:     1220 || Batch Translation Loss:   0.012577 => Txt Tokens per Sec:     3676 || Lr: 0.000050
2024-02-02 14:58:06,069 Epoch 2771: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:58:06,069 EPOCH 2772
2024-02-02 14:58:12,925 Epoch 2772: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:58:12,925 EPOCH 2773
2024-02-02 14:58:19,773 Epoch 2773: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:58:19,773 EPOCH 2774
2024-02-02 14:58:26,715 Epoch 2774: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 14:58:26,715 EPOCH 2775
2024-02-02 14:58:33,544 Epoch 2775: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:58:33,545 EPOCH 2776
2024-02-02 14:58:40,421 Epoch 2776: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:58:40,422 EPOCH 2777
2024-02-02 14:58:42,546 [Epoch: 2777 Step: 00047200] Batch Recognition Loss:   0.000116 => Gls Tokens per Sec:     2412 || Batch Translation Loss:   0.010433 => Txt Tokens per Sec:     6522 || Lr: 0.000050
2024-02-02 14:58:47,124 Epoch 2777: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:58:47,125 EPOCH 2778
2024-02-02 14:58:54,238 Epoch 2778: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:58:54,239 EPOCH 2779
2024-02-02 14:59:01,066 Epoch 2779: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:59:01,067 EPOCH 2780
2024-02-02 14:59:07,590 Epoch 2780: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:59:07,591 EPOCH 2781
2024-02-02 14:59:14,240 Epoch 2781: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:59:14,240 EPOCH 2782
2024-02-02 14:59:21,243 Epoch 2782: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:59:21,244 EPOCH 2783
2024-02-02 14:59:24,738 [Epoch: 2783 Step: 00047300] Batch Recognition Loss:   0.000116 => Gls Tokens per Sec:     1028 || Batch Translation Loss:   0.010785 => Txt Tokens per Sec:     3065 || Lr: 0.000050
2024-02-02 14:59:27,854 Epoch 2783: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:59:27,855 EPOCH 2784
2024-02-02 14:59:34,391 Epoch 2784: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 14:59:34,392 EPOCH 2785
2024-02-02 14:59:41,325 Epoch 2785: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 14:59:41,326 EPOCH 2786
2024-02-02 14:59:48,021 Epoch 2786: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 14:59:48,021 EPOCH 2787
2024-02-02 14:59:54,842 Epoch 2787: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 14:59:54,843 EPOCH 2788
2024-02-02 15:00:01,486 Epoch 2788: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 15:00:01,486 EPOCH 2789
2024-02-02 15:00:02,461 [Epoch: 2789 Step: 00047400] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:     2627 || Batch Translation Loss:   0.010176 => Txt Tokens per Sec:     7515 || Lr: 0.000050
2024-02-02 15:00:08,206 Epoch 2789: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 15:00:08,206 EPOCH 2790
2024-02-02 15:00:14,581 Epoch 2790: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 15:00:14,581 EPOCH 2791
2024-02-02 15:00:21,211 Epoch 2791: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.82 
2024-02-02 15:00:21,211 EPOCH 2792
2024-02-02 15:00:28,024 Epoch 2792: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-02 15:00:28,024 EPOCH 2793
2024-02-02 15:00:34,634 Epoch 2793: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-02 15:00:34,635 EPOCH 2794
2024-02-02 15:00:41,232 Epoch 2794: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.22 
2024-02-02 15:00:41,232 EPOCH 2795
2024-02-02 15:00:41,747 [Epoch: 2795 Step: 00047500] Batch Recognition Loss:   0.000357 => Gls Tokens per Sec:     2495 || Batch Translation Loss:   0.057048 => Txt Tokens per Sec:     6419 || Lr: 0.000050
2024-02-02 15:00:48,122 Epoch 2795: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-02 15:00:48,122 EPOCH 2796
2024-02-02 15:00:55,043 Epoch 2796: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-02 15:00:55,043 EPOCH 2797
2024-02-02 15:01:01,748 Epoch 2797: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 15:01:01,748 EPOCH 2798
2024-02-02 15:01:08,527 Epoch 2798: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 15:01:08,528 EPOCH 2799
2024-02-02 15:01:14,897 Epoch 2799: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 15:01:14,897 EPOCH 2800
2024-02-02 15:01:21,647 [Epoch: 2800 Step: 00047600] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     1575 || Batch Translation Loss:   0.029622 => Txt Tokens per Sec:     4373 || Lr: 0.000050
2024-02-02 15:01:21,647 Epoch 2800: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 15:01:21,647 EPOCH 2801
2024-02-02 15:01:28,488 Epoch 2801: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 15:01:28,488 EPOCH 2802
2024-02-02 15:01:35,299 Epoch 2802: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 15:01:35,300 EPOCH 2803
2024-02-02 15:01:41,947 Epoch 2803: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 15:01:41,948 EPOCH 2804
2024-02-02 15:01:48,531 Epoch 2804: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 15:01:48,532 EPOCH 2805
2024-02-02 15:01:55,402 Epoch 2805: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 15:01:55,402 EPOCH 2806
2024-02-02 15:02:01,533 [Epoch: 2806 Step: 00047700] Batch Recognition Loss:   0.000180 => Gls Tokens per Sec:     1526 || Batch Translation Loss:   0.017048 => Txt Tokens per Sec:     4284 || Lr: 0.000050
2024-02-02 15:02:02,082 Epoch 2806: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 15:02:02,082 EPOCH 2807
2024-02-02 15:02:08,647 Epoch 2807: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 15:02:08,648 EPOCH 2808
2024-02-02 15:02:15,207 Epoch 2808: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:02:15,207 EPOCH 2809
2024-02-02 15:02:22,366 Epoch 2809: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:02:22,367 EPOCH 2810
2024-02-02 15:02:29,281 Epoch 2810: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:02:29,282 EPOCH 2811
2024-02-02 15:02:35,842 Epoch 2811: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 15:02:35,843 EPOCH 2812
2024-02-02 15:02:41,390 [Epoch: 2812 Step: 00047800] Batch Recognition Loss:   0.000085 => Gls Tokens per Sec:     1455 || Batch Translation Loss:   0.005959 => Txt Tokens per Sec:     3929 || Lr: 0.000050
2024-02-02 15:02:42,603 Epoch 2812: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 15:02:42,603 EPOCH 2813
2024-02-02 15:02:49,154 Epoch 2813: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:02:49,154 EPOCH 2814
2024-02-02 15:02:55,737 Epoch 2814: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:02:55,737 EPOCH 2815
2024-02-02 15:03:02,465 Epoch 2815: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:03:02,466 EPOCH 2816
2024-02-02 15:03:09,017 Epoch 2816: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:03:09,017 EPOCH 2817
2024-02-02 15:03:15,851 Epoch 2817: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:03:15,851 EPOCH 2818
2024-02-02 15:03:18,455 [Epoch: 2818 Step: 00047900] Batch Recognition Loss:   0.000112 => Gls Tokens per Sec:     2704 || Batch Translation Loss:   0.011059 => Txt Tokens per Sec:     7171 || Lr: 0.000050
2024-02-02 15:03:22,452 Epoch 2818: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 15:03:22,453 EPOCH 2819
2024-02-02 15:03:29,374 Epoch 2819: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:03:29,375 EPOCH 2820
2024-02-02 15:03:36,031 Epoch 2820: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:03:36,032 EPOCH 2821
2024-02-02 15:03:42,825 Epoch 2821: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:03:42,825 EPOCH 2822
2024-02-02 15:03:49,296 Epoch 2822: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:03:49,297 EPOCH 2823
2024-02-02 15:03:56,025 Epoch 2823: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 15:03:56,025 EPOCH 2824
2024-02-02 15:03:58,478 [Epoch: 2824 Step: 00048000] Batch Recognition Loss:   0.000122 => Gls Tokens per Sec:     2349 || Batch Translation Loss:   0.013230 => Txt Tokens per Sec:     6214 || Lr: 0.000050
2024-02-02 15:04:29,050 Validation result at epoch 2824, step    48000: duration: 30.5713s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00058	Translation Loss: 101324.61719	PPL: 25325.31445
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.48	(BLEU-1: 9.91,	BLEU-2: 2.85,	BLEU-3: 1.08,	BLEU-4: 0.48)
	CHRF 16.59	ROUGE 8.56
2024-02-02 15:04:29,052 Logging Recognition and Translation Outputs
2024-02-02 15:04:29,052 ========================================================================================================================
2024-02-02 15:04:29,052 Logging Sequence: 88_159.00
2024-02-02 15:04:29,052 	Gloss Reference :	A B+C+D+E
2024-02-02 15:04:29,053 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 15:04:29,053 	Gloss Alignment :	         
2024-02-02 15:04:29,053 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 15:04:29,054 	Text Reference  :	*** *** ***** however he  often comes to the town  to     meet     his relatives
2024-02-02 15:04:29,054 	Text Hypothesis :	the two names are     the note  there is the mayor javkin attacked in  police   
2024-02-02 15:04:29,054 	Text Alignment  :	I   I   I     S       S   S     S     S      S     S      S        S   S        
2024-02-02 15:04:29,055 ========================================================================================================================
2024-02-02 15:04:29,055 Logging Sequence: 180_53.00
2024-02-02 15:04:29,055 	Gloss Reference :	A B+C+D+E
2024-02-02 15:04:29,055 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 15:04:29,055 	Gloss Alignment :	         
2024-02-02 15:04:29,055 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 15:04:29,056 	Text Reference  :	the protest is  against singh again
2024-02-02 15:04:29,056 	Text Hypothesis :	*** ******* let me      tell  you  
2024-02-02 15:04:29,056 	Text Alignment  :	D   D       S   S       S     S    
2024-02-02 15:04:29,056 ========================================================================================================================
2024-02-02 15:04:29,056 Logging Sequence: 163_30.00
2024-02-02 15:04:29,056 	Gloss Reference :	A B+C+D+E
2024-02-02 15:04:29,057 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 15:04:29,057 	Gloss Alignment :	         
2024-02-02 15:04:29,057 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 15:04:29,058 	Text Reference  :	***** *** they   never permitted anyone to ***** ***** **** reveal her  face 
2024-02-02 15:04:29,058 	Text Hypothesis :	after the number of    matches   love   to virat kohli made a      huge image
2024-02-02 15:04:29,058 	Text Alignment  :	I     I   S      S     S         S         I     I     I    S      S    S    
2024-02-02 15:04:29,058 ========================================================================================================================
2024-02-02 15:04:29,058 Logging Sequence: 51_110.00
2024-02-02 15:04:29,058 	Gloss Reference :	A B+C+D+E
2024-02-02 15:04:29,058 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 15:04:29,059 	Gloss Alignment :	         
2024-02-02 15:04:29,059 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 15:04:29,059 	Text Reference  :	the aussies were  very happy with    their victory
2024-02-02 15:04:29,059 	Text Hypothesis :	*** he      keeps a    new   zealand as    well   
2024-02-02 15:04:29,059 	Text Alignment  :	D   S       S     S    S     S       S     S      
2024-02-02 15:04:29,060 ========================================================================================================================
2024-02-02 15:04:29,060 Logging Sequence: 70_249.00
2024-02-02 15:04:29,060 	Gloss Reference :	A B+C+D+E
2024-02-02 15:04:29,060 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 15:04:29,060 	Gloss Alignment :	         
2024-02-02 15:04:29,060 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 15:04:29,061 	Text Reference  :	******* **** **** *** ***** ***** have   a    look at this video   
2024-02-02 15:04:29,061 	Text Hypothesis :	however even when the media asked sindhu then goes on the  olympics
2024-02-02 15:04:29,061 	Text Alignment  :	I       I    I    I   I     I     S      S    S    S  S    S       
2024-02-02 15:04:29,061 ========================================================================================================================
2024-02-02 15:04:33,667 Epoch 2824: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 15:04:33,667 EPOCH 2825
2024-02-02 15:04:40,423 Epoch 2825: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:04:40,423 EPOCH 2826
2024-02-02 15:04:47,259 Epoch 2826: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 15:04:47,260 EPOCH 2827
2024-02-02 15:04:54,071 Epoch 2827: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 15:04:54,071 EPOCH 2828
2024-02-02 15:05:01,234 Epoch 2828: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 15:05:01,235 EPOCH 2829
2024-02-02 15:05:08,117 Epoch 2829: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 15:05:08,118 EPOCH 2830
2024-02-02 15:05:11,568 [Epoch: 2830 Step: 00048100] Batch Recognition Loss:   0.000088 => Gls Tokens per Sec:     1226 || Batch Translation Loss:   0.006038 => Txt Tokens per Sec:     3001 || Lr: 0.000050
2024-02-02 15:05:14,956 Epoch 2830: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 15:05:14,957 EPOCH 2831
2024-02-02 15:05:21,759 Epoch 2831: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-02 15:05:21,759 EPOCH 2832
2024-02-02 15:05:28,662 Epoch 2832: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 15:05:28,663 EPOCH 2833
2024-02-02 15:05:35,361 Epoch 2833: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 15:05:35,362 EPOCH 2834
2024-02-02 15:05:42,236 Epoch 2834: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 15:05:42,236 EPOCH 2835
2024-02-02 15:05:48,933 Epoch 2835: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 15:05:48,933 EPOCH 2836
2024-02-02 15:05:50,563 [Epoch: 2836 Step: 00048200] Batch Recognition Loss:   0.000238 => Gls Tokens per Sec:     1965 || Batch Translation Loss:   0.031830 => Txt Tokens per Sec:     5201 || Lr: 0.000050
2024-02-02 15:05:55,653 Epoch 2836: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 15:05:55,653 EPOCH 2837
2024-02-02 15:06:02,564 Epoch 2837: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 15:06:02,565 EPOCH 2838
2024-02-02 15:06:09,389 Epoch 2838: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 15:06:09,390 EPOCH 2839
2024-02-02 15:06:16,546 Epoch 2839: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 15:06:16,547 EPOCH 2840
2024-02-02 15:06:23,261 Epoch 2840: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 15:06:23,261 EPOCH 2841
2024-02-02 15:06:29,966 Epoch 2841: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 15:06:29,966 EPOCH 2842
2024-02-02 15:06:30,627 [Epoch: 2842 Step: 00048300] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     2905 || Batch Translation Loss:   0.029088 => Txt Tokens per Sec:     7312 || Lr: 0.000050
2024-02-02 15:06:36,839 Epoch 2842: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 15:06:36,839 EPOCH 2843
2024-02-02 15:06:43,564 Epoch 2843: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 15:06:43,565 EPOCH 2844
2024-02-02 15:06:50,301 Epoch 2844: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 15:06:50,302 EPOCH 2845
2024-02-02 15:06:56,869 Epoch 2845: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 15:06:56,869 EPOCH 2846
2024-02-02 15:07:03,686 Epoch 2846: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 15:07:03,686 EPOCH 2847
2024-02-02 15:07:10,388 Epoch 2847: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-02 15:07:10,389 EPOCH 2848
2024-02-02 15:07:10,533 [Epoch: 2848 Step: 00048400] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     4476 || Batch Translation Loss:   0.014721 => Txt Tokens per Sec:    11616 || Lr: 0.000050
2024-02-02 15:07:16,852 Epoch 2848: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 15:07:16,852 EPOCH 2849
2024-02-02 15:07:23,587 Epoch 2849: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-02 15:07:23,588 EPOCH 2850
2024-02-02 15:07:30,220 Epoch 2850: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 15:07:30,220 EPOCH 2851
2024-02-02 15:07:37,147 Epoch 2851: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-02 15:07:37,148 EPOCH 2852
2024-02-02 15:07:43,375 Epoch 2852: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-02 15:07:43,376 EPOCH 2853
2024-02-02 15:07:50,018 [Epoch: 2853 Step: 00048500] Batch Recognition Loss:   0.000221 => Gls Tokens per Sec:     1504 || Batch Translation Loss:   0.030277 => Txt Tokens per Sec:     4250 || Lr: 0.000050
2024-02-02 15:07:50,166 Epoch 2853: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 15:07:50,167 EPOCH 2854
2024-02-02 15:07:56,998 Epoch 2854: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.80 
2024-02-02 15:07:56,999 EPOCH 2855
2024-02-02 15:08:03,931 Epoch 2855: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 15:08:03,931 EPOCH 2856
2024-02-02 15:08:09,935 Epoch 2856: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 15:08:09,935 EPOCH 2857
2024-02-02 15:08:16,520 Epoch 2857: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 15:08:16,521 EPOCH 2858
2024-02-02 15:08:23,266 Epoch 2858: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 15:08:23,267 EPOCH 2859
2024-02-02 15:08:28,607 [Epoch: 2859 Step: 00048600] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:     1631 || Batch Translation Loss:   0.026198 => Txt Tokens per Sec:     4417 || Lr: 0.000050
2024-02-02 15:08:29,724 Epoch 2859: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 15:08:29,725 EPOCH 2860
2024-02-02 15:08:36,522 Epoch 2860: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 15:08:36,523 EPOCH 2861
2024-02-02 15:08:43,255 Epoch 2861: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 15:08:43,255 EPOCH 2862
2024-02-02 15:08:49,823 Epoch 2862: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 15:08:49,824 EPOCH 2863
2024-02-02 15:08:56,278 Epoch 2863: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 15:08:56,278 EPOCH 2864
2024-02-02 15:09:02,251 Epoch 2864: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 15:09:02,251 EPOCH 2865
2024-02-02 15:09:07,858 [Epoch: 2865 Step: 00048700] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:     1325 || Batch Translation Loss:   0.012765 => Txt Tokens per Sec:     3819 || Lr: 0.000050
2024-02-02 15:09:09,135 Epoch 2865: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:09:09,136 EPOCH 2866
2024-02-02 15:09:15,910 Epoch 2866: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:09:15,911 EPOCH 2867
2024-02-02 15:09:22,465 Epoch 2867: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 15:09:22,466 EPOCH 2868
2024-02-02 15:09:29,268 Epoch 2868: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 15:09:29,269 EPOCH 2869
2024-02-02 15:09:35,969 Epoch 2869: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 15:09:35,970 EPOCH 2870
2024-02-02 15:09:43,054 Epoch 2870: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 15:09:43,054 EPOCH 2871
2024-02-02 15:09:47,331 [Epoch: 2871 Step: 00048800] Batch Recognition Loss:   0.000100 => Gls Tokens per Sec:     1439 || Batch Translation Loss:   0.011386 => Txt Tokens per Sec:     3886 || Lr: 0.000050
2024-02-02 15:09:49,792 Epoch 2871: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:09:49,792 EPOCH 2872
2024-02-02 15:09:56,267 Epoch 2872: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 15:09:56,268 EPOCH 2873
2024-02-02 15:10:02,990 Epoch 2873: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:10:02,990 EPOCH 2874
2024-02-02 15:10:09,709 Epoch 2874: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:10:09,709 EPOCH 2875
2024-02-02 15:10:16,522 Epoch 2875: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:10:16,522 EPOCH 2876
2024-02-02 15:10:23,284 Epoch 2876: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:10:23,285 EPOCH 2877
2024-02-02 15:10:25,441 [Epoch: 2877 Step: 00048900] Batch Recognition Loss:   0.000085 => Gls Tokens per Sec:     2376 || Batch Translation Loss:   0.005047 => Txt Tokens per Sec:     6656 || Lr: 0.000050
2024-02-02 15:10:29,917 Epoch 2877: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 15:10:29,917 EPOCH 2878
2024-02-02 15:10:36,392 Epoch 2878: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 15:10:36,392 EPOCH 2879
2024-02-02 15:10:43,322 Epoch 2879: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 15:10:43,323 EPOCH 2880
2024-02-02 15:10:49,957 Epoch 2880: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 15:10:49,958 EPOCH 2881
2024-02-02 15:10:56,433 Epoch 2881: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 15:10:56,434 EPOCH 2882
2024-02-02 15:11:02,925 Epoch 2882: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 15:11:02,926 EPOCH 2883
2024-02-02 15:11:04,815 [Epoch: 2883 Step: 00049000] Batch Recognition Loss:   0.000114 => Gls Tokens per Sec:     2033 || Batch Translation Loss:   0.010045 => Txt Tokens per Sec:     5771 || Lr: 0.000050
2024-02-02 15:11:09,799 Epoch 2883: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 15:11:09,800 EPOCH 2884
2024-02-02 15:11:16,594 Epoch 2884: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 15:11:16,595 EPOCH 2885
2024-02-02 15:11:23,253 Epoch 2885: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 15:11:23,254 EPOCH 2886
2024-02-02 15:11:29,900 Epoch 2886: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 15:11:29,900 EPOCH 2887
2024-02-02 15:11:36,889 Epoch 2887: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-02 15:11:36,890 EPOCH 2888
2024-02-02 15:11:43,783 Epoch 2888: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.88 
2024-02-02 15:11:43,784 EPOCH 2889
2024-02-02 15:11:44,664 [Epoch: 2889 Step: 00049100] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     2914 || Batch Translation Loss:   0.139860 => Txt Tokens per Sec:     7863 || Lr: 0.000050
2024-02-02 15:11:50,489 Epoch 2889: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.90 
2024-02-02 15:11:50,490 EPOCH 2890
2024-02-02 15:11:57,364 Epoch 2890: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-02 15:11:57,365 EPOCH 2891
2024-02-02 15:12:04,148 Epoch 2891: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-02 15:12:04,149 EPOCH 2892
2024-02-02 15:12:10,891 Epoch 2892: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 15:12:10,891 EPOCH 2893
2024-02-02 15:12:17,727 Epoch 2893: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 15:12:17,728 EPOCH 2894
2024-02-02 15:12:24,418 Epoch 2894: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 15:12:24,419 EPOCH 2895
2024-02-02 15:12:24,788 [Epoch: 2895 Step: 00049200] Batch Recognition Loss:   0.000113 => Gls Tokens per Sec:     3478 || Batch Translation Loss:   0.011879 => Txt Tokens per Sec:     8766 || Lr: 0.000050
2024-02-02 15:12:31,416 Epoch 2895: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 15:12:31,417 EPOCH 2896
2024-02-02 15:12:38,081 Epoch 2896: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 15:12:38,082 EPOCH 2897
2024-02-02 15:12:45,058 Epoch 2897: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 15:12:45,059 EPOCH 2898
2024-02-02 15:12:52,154 Epoch 2898: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 15:12:52,154 EPOCH 2899
2024-02-02 15:12:58,921 Epoch 2899: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 15:12:58,922 EPOCH 2900
2024-02-02 15:13:05,506 [Epoch: 2900 Step: 00049300] Batch Recognition Loss:   0.000117 => Gls Tokens per Sec:     1615 || Batch Translation Loss:   0.013643 => Txt Tokens per Sec:     4483 || Lr: 0.000050
2024-02-02 15:13:05,506 Epoch 2900: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 15:13:05,507 EPOCH 2901
2024-02-02 15:13:12,314 Epoch 2901: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-02 15:13:12,315 EPOCH 2902
2024-02-02 15:13:19,061 Epoch 2902: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-02 15:13:19,062 EPOCH 2903
2024-02-02 15:13:25,863 Epoch 2903: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-02 15:13:25,864 EPOCH 2904
2024-02-02 15:13:32,520 Epoch 2904: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 15:13:32,521 EPOCH 2905
2024-02-02 15:13:39,254 Epoch 2905: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 15:13:39,254 EPOCH 2906
2024-02-02 15:13:45,653 [Epoch: 2906 Step: 00049400] Batch Recognition Loss:   0.000113 => Gls Tokens per Sec:     1461 || Batch Translation Loss:   0.008496 => Txt Tokens per Sec:     4083 || Lr: 0.000050
2024-02-02 15:13:46,058 Epoch 2906: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 15:13:46,058 EPOCH 2907
2024-02-02 15:13:52,857 Epoch 2907: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 15:13:52,858 EPOCH 2908
2024-02-02 15:13:59,702 Epoch 2908: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 15:13:59,703 EPOCH 2909
2024-02-02 15:14:06,030 Epoch 2909: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 15:14:06,031 EPOCH 2910
2024-02-02 15:14:12,938 Epoch 2910: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 15:14:12,939 EPOCH 2911
2024-02-02 15:14:19,571 Epoch 2911: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:14:19,572 EPOCH 2912
2024-02-02 15:14:24,820 [Epoch: 2912 Step: 00049500] Batch Recognition Loss:   0.000114 => Gls Tokens per Sec:     1538 || Batch Translation Loss:   0.013408 => Txt Tokens per Sec:     4378 || Lr: 0.000050
2024-02-02 15:14:25,544 Epoch 2912: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 15:14:25,544 EPOCH 2913
2024-02-02 15:14:32,055 Epoch 2913: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:14:32,055 EPOCH 2914
2024-02-02 15:14:38,956 Epoch 2914: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 15:14:38,957 EPOCH 2915
2024-02-02 15:14:45,557 Epoch 2915: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:14:45,558 EPOCH 2916
2024-02-02 15:14:52,520 Epoch 2916: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:14:52,520 EPOCH 2917
2024-02-02 15:14:59,295 Epoch 2917: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:14:59,295 EPOCH 2918
2024-02-02 15:15:04,362 [Epoch: 2918 Step: 00049600] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     1340 || Batch Translation Loss:   0.010548 => Txt Tokens per Sec:     3601 || Lr: 0.000050
2024-02-02 15:15:06,176 Epoch 2918: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:15:06,176 EPOCH 2919
2024-02-02 15:15:12,330 Epoch 2919: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:15:12,330 EPOCH 2920
2024-02-02 15:15:18,389 Epoch 2920: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:15:18,389 EPOCH 2921
2024-02-02 15:15:25,480 Epoch 2921: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:15:25,480 EPOCH 2922
2024-02-02 15:15:32,445 Epoch 2922: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:15:32,445 EPOCH 2923
2024-02-02 15:15:39,047 Epoch 2923: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:15:39,047 EPOCH 2924
2024-02-02 15:15:40,905 [Epoch: 2924 Step: 00049700] Batch Recognition Loss:   0.000113 => Gls Tokens per Sec:     3103 || Batch Translation Loss:   0.011066 => Txt Tokens per Sec:     8032 || Lr: 0.000050
2024-02-02 15:15:45,793 Epoch 2924: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:15:45,794 EPOCH 2925
2024-02-02 15:15:52,517 Epoch 2925: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:15:52,518 EPOCH 2926
2024-02-02 15:15:59,203 Epoch 2926: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:15:59,204 EPOCH 2927
2024-02-02 15:16:05,905 Epoch 2927: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:16:05,905 EPOCH 2928
2024-02-02 15:16:12,850 Epoch 2928: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 15:16:12,851 EPOCH 2929
2024-02-02 15:16:19,682 Epoch 2929: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:16:19,683 EPOCH 2930
2024-02-02 15:16:21,336 [Epoch: 2930 Step: 00049800] Batch Recognition Loss:   0.000112 => Gls Tokens per Sec:     2712 || Batch Translation Loss:   0.016429 => Txt Tokens per Sec:     7445 || Lr: 0.000050
2024-02-02 15:16:25,891 Epoch 2930: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 15:16:25,892 EPOCH 2931
2024-02-02 15:16:32,738 Epoch 2931: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 15:16:32,739 EPOCH 2932
2024-02-02 15:16:39,596 Epoch 2932: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 15:16:39,597 EPOCH 2933
2024-02-02 15:16:46,409 Epoch 2933: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 15:16:46,410 EPOCH 2934
2024-02-02 15:16:53,158 Epoch 2934: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 15:16:53,159 EPOCH 2935
2024-02-02 15:17:00,017 Epoch 2935: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 15:17:00,017 EPOCH 2936
2024-02-02 15:17:01,455 [Epoch: 2936 Step: 00049900] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:     2227 || Batch Translation Loss:   0.041305 => Txt Tokens per Sec:     6207 || Lr: 0.000050
2024-02-02 15:17:06,807 Epoch 2936: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-02 15:17:06,808 EPOCH 2937
2024-02-02 15:17:13,596 Epoch 2937: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 15:17:13,596 EPOCH 2938
2024-02-02 15:17:20,448 Epoch 2938: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 15:17:20,448 EPOCH 2939
2024-02-02 15:17:27,059 Epoch 2939: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 15:17:27,060 EPOCH 2940
2024-02-02 15:17:33,708 Epoch 2940: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 15:17:33,708 EPOCH 2941
2024-02-02 15:17:40,603 Epoch 2941: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 15:17:40,603 EPOCH 2942
2024-02-02 15:17:43,173 [Epoch: 2942 Step: 00050000] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:      650 || Batch Translation Loss:   0.018471 => Txt Tokens per Sec:     1520 || Lr: 0.000050
2024-02-02 15:18:13,530 Validation result at epoch 2942, step    50000: duration: 30.3572s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00086	Translation Loss: 102544.16406	PPL: 28612.51758
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.28	(BLEU-1: 9.53,	BLEU-2: 2.24,	BLEU-3: 0.77,	BLEU-4: 0.28)
	CHRF 16.37	ROUGE 8.12
2024-02-02 15:18:13,531 Logging Recognition and Translation Outputs
2024-02-02 15:18:13,532 ========================================================================================================================
2024-02-02 15:18:13,532 Logging Sequence: 59_58.00
2024-02-02 15:18:13,532 	Gloss Reference :	A B+C+D+E
2024-02-02 15:18:13,532 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 15:18:13,532 	Gloss Alignment :	         
2024-02-02 15:18:13,532 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 15:18:13,533 	Text Reference  :	to fix the damage they did not have a   lot       of time   
2024-02-02 15:18:13,533 	Text Hypothesis :	** *** i   am     by   29  out of   the scheduled 56 matches
2024-02-02 15:18:13,534 	Text Alignment  :	D  D   S   S      S    S   S   S    S   S         S  S      
2024-02-02 15:18:13,534 ========================================================================================================================
2024-02-02 15:18:13,534 Logging Sequence: 165_2.00
2024-02-02 15:18:13,535 	Gloss Reference :	A B+C+D+E
2024-02-02 15:18:13,535 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 15:18:13,535 	Gloss Alignment :	         
2024-02-02 15:18:13,535 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 15:18:13,536 	Text Reference  :	many people believe in superstitions and think it  brings good luck and       bad luck    
2024-02-02 15:18:13,536 	Text Hypothesis :	**** ****** ******* ** ************* he  has   now handed over the  captaincy to  ravindra
2024-02-02 15:18:13,537 	Text Alignment  :	D    D      D       D  D             S   S     S   S      S    S    S         S   S       
2024-02-02 15:18:13,537 ========================================================================================================================
2024-02-02 15:18:13,537 Logging Sequence: 58_147.00
2024-02-02 15:18:13,537 	Gloss Reference :	A B+C+D+E
2024-02-02 15:18:13,537 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 15:18:13,537 	Gloss Alignment :	         
2024-02-02 15:18:13,537 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 15:18:13,538 	Text Reference  :	the women's cricket team grabbed gold by beating sri lanka in      the  finals what a  historic    win         
2024-02-02 15:18:13,539 	Text Hypothesis :	*** ******* ******* **** ******* **** ** ******* *** 6     spinner anil kumble has  an interesting superstition
2024-02-02 15:18:13,539 	Text Alignment  :	D   D       D       D    D       D    D  D       D   S     S       S    S      S    S  S           S           
2024-02-02 15:18:13,539 ========================================================================================================================
2024-02-02 15:18:13,539 Logging Sequence: 81_139.00
2024-02-02 15:18:13,539 	Gloss Reference :	A B+C+D+E
2024-02-02 15:18:13,539 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 15:18:13,539 	Gloss Alignment :	         
2024-02-02 15:18:13,540 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 15:18:13,542 	Text Reference  :	in 2017 the    case was filed first in  delhi high  court by   rhiti sports management on  behalf        of      dhoni 
2024-02-02 15:18:13,542 	Text Hypothesis :	** a    sports i    am  sure  you   all the   match we    will have  to     wait       and international cricket though
2024-02-02 15:18:13,542 	Text Alignment  :	D  S    S      S    S   S     S     S   S     S     S     S    S     S      S          S   S             S       S     
2024-02-02 15:18:13,542 ========================================================================================================================
2024-02-02 15:18:13,542 Logging Sequence: 125_72.00
2024-02-02 15:18:13,542 	Gloss Reference :	A B+C+D+E
2024-02-02 15:18:13,542 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 15:18:13,542 	Gloss Alignment :	         
2024-02-02 15:18:13,543 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 15:18:13,543 	Text Reference  :	some said the pakistani javelineer had milicious intentions of tampering with    the javelin      out      of  jealousy
2024-02-02 15:18:13,543 	Text Hypothesis :	**** **** *** ********* ********** *** ********* ********** ** and       decided to  indefinitely postpone ipl 2021    
2024-02-02 15:18:13,544 	Text Alignment  :	D    D    D   D         D          D   D         D          D  S         S       S   S            S        S   S       
2024-02-02 15:18:13,544 ========================================================================================================================
2024-02-02 15:18:17,685 Epoch 2942: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 15:18:17,686 EPOCH 2943
2024-02-02 15:18:24,531 Epoch 2943: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 15:18:24,532 EPOCH 2944
2024-02-02 15:18:31,275 Epoch 2944: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-02 15:18:31,276 EPOCH 2945
2024-02-02 15:18:38,091 Epoch 2945: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.85 
2024-02-02 15:18:38,091 EPOCH 2946
2024-02-02 15:18:44,516 Epoch 2946: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.98 
2024-02-02 15:18:44,517 EPOCH 2947
2024-02-02 15:18:51,273 Epoch 2947: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-02 15:18:51,274 EPOCH 2948
2024-02-02 15:18:51,412 [Epoch: 2948 Step: 00050100] Batch Recognition Loss:   0.000320 => Gls Tokens per Sec:     4638 || Batch Translation Loss:   0.034256 => Txt Tokens per Sec:    11891 || Lr: 0.000050
2024-02-02 15:18:58,036 Epoch 2948: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-02 15:18:58,037 EPOCH 2949
2024-02-02 15:19:04,824 Epoch 2949: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-02 15:19:04,824 EPOCH 2950
2024-02-02 15:19:11,623 Epoch 2950: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 15:19:11,623 EPOCH 2951
2024-02-02 15:19:18,031 Epoch 2951: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 15:19:18,032 EPOCH 2952
2024-02-02 15:19:24,640 Epoch 2952: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 15:19:24,640 EPOCH 2953
2024-02-02 15:19:31,253 [Epoch: 2953 Step: 00050200] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     1511 || Batch Translation Loss:   0.016883 => Txt Tokens per Sec:     4199 || Lr: 0.000050
2024-02-02 15:19:31,473 Epoch 2953: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 15:19:31,473 EPOCH 2954
2024-02-02 15:19:38,431 Epoch 2954: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 15:19:38,432 EPOCH 2955
2024-02-02 15:19:44,924 Epoch 2955: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 15:19:44,925 EPOCH 2956
2024-02-02 15:19:51,762 Epoch 2956: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 15:19:51,762 EPOCH 2957
2024-02-02 15:19:58,496 Epoch 2957: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 15:19:58,497 EPOCH 2958
2024-02-02 15:20:05,293 Epoch 2958: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 15:20:05,294 EPOCH 2959
2024-02-02 15:20:10,736 [Epoch: 2959 Step: 00050300] Batch Recognition Loss:   0.000109 => Gls Tokens per Sec:     1601 || Batch Translation Loss:   0.014234 => Txt Tokens per Sec:     4323 || Lr: 0.000050
2024-02-02 15:20:12,061 Epoch 2959: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 15:20:12,061 EPOCH 2960
2024-02-02 15:20:18,741 Epoch 2960: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 15:20:18,741 EPOCH 2961
2024-02-02 15:20:25,677 Epoch 2961: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 15:20:25,678 EPOCH 2962
2024-02-02 15:20:32,482 Epoch 2962: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:20:32,483 EPOCH 2963
2024-02-02 15:20:38,844 Epoch 2963: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 15:20:38,844 EPOCH 2964
2024-02-02 15:20:45,696 Epoch 2964: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:20:45,696 EPOCH 2965
2024-02-02 15:20:51,034 [Epoch: 2965 Step: 00050400] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:     1392 || Batch Translation Loss:   0.019019 => Txt Tokens per Sec:     3907 || Lr: 0.000050
2024-02-02 15:20:52,479 Epoch 2965: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:20:52,479 EPOCH 2966
2024-02-02 15:20:59,251 Epoch 2966: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 15:20:59,252 EPOCH 2967
2024-02-02 15:21:06,044 Epoch 2967: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 15:21:06,044 EPOCH 2968
2024-02-02 15:21:12,746 Epoch 2968: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 15:21:12,747 EPOCH 2969
2024-02-02 15:21:19,968 Epoch 2969: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 15:21:19,969 EPOCH 2970
2024-02-02 15:21:26,640 Epoch 2970: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 15:21:26,640 EPOCH 2971
2024-02-02 15:21:31,662 [Epoch: 2971 Step: 00050500] Batch Recognition Loss:   0.000148 => Gls Tokens per Sec:     1225 || Batch Translation Loss:   0.027962 => Txt Tokens per Sec:     3488 || Lr: 0.000050
2024-02-02 15:21:33,508 Epoch 2971: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 15:21:33,509 EPOCH 2972
2024-02-02 15:21:40,355 Epoch 2972: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 15:21:40,355 EPOCH 2973
2024-02-02 15:21:46,951 Epoch 2973: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-02 15:21:46,952 EPOCH 2974
2024-02-02 15:21:53,887 Epoch 2974: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 15:21:53,887 EPOCH 2975
2024-02-02 15:22:00,508 Epoch 2975: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 15:22:00,508 EPOCH 2976
2024-02-02 15:22:07,389 Epoch 2976: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 15:22:07,389 EPOCH 2977
2024-02-02 15:22:11,944 [Epoch: 2977 Step: 00050600] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:     1069 || Batch Translation Loss:   0.018882 => Txt Tokens per Sec:     3037 || Lr: 0.000050
2024-02-02 15:22:14,439 Epoch 2977: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 15:22:14,439 EPOCH 2978
2024-02-02 15:22:21,208 Epoch 2978: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 15:22:21,209 EPOCH 2979
2024-02-02 15:22:27,946 Epoch 2979: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 15:22:27,947 EPOCH 2980
2024-02-02 15:22:34,851 Epoch 2980: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 15:22:34,851 EPOCH 2981
2024-02-02 15:22:41,631 Epoch 2981: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 15:22:41,632 EPOCH 2982
2024-02-02 15:22:48,053 Epoch 2982: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 15:22:48,053 EPOCH 2983
2024-02-02 15:22:52,066 [Epoch: 2983 Step: 00050700] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:      895 || Batch Translation Loss:   0.029271 => Txt Tokens per Sec:     2827 || Lr: 0.000050
2024-02-02 15:22:54,768 Epoch 2983: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 15:22:54,769 EPOCH 2984
2024-02-02 15:23:01,545 Epoch 2984: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 15:23:01,545 EPOCH 2985
2024-02-02 15:23:08,684 Epoch 2985: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 15:23:08,685 EPOCH 2986
2024-02-02 15:23:15,785 Epoch 2986: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 15:23:15,785 EPOCH 2987
2024-02-02 15:23:22,561 Epoch 2987: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 15:23:22,561 EPOCH 2988
2024-02-02 15:23:28,782 Epoch 2988: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 15:23:28,783 EPOCH 2989
2024-02-02 15:23:29,493 [Epoch: 2989 Step: 00050800] Batch Recognition Loss:   0.000089 => Gls Tokens per Sec:     3614 || Batch Translation Loss:   0.005705 => Txt Tokens per Sec:     7926 || Lr: 0.000050
2024-02-02 15:23:35,576 Epoch 2989: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 15:23:35,577 EPOCH 2990
2024-02-02 15:23:42,399 Epoch 2990: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 15:23:42,399 EPOCH 2991
2024-02-02 15:23:49,352 Epoch 2991: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 15:23:49,352 EPOCH 2992
2024-02-02 15:23:56,155 Epoch 2992: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 15:23:56,156 EPOCH 2993
2024-02-02 15:24:02,986 Epoch 2993: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 15:24:02,987 EPOCH 2994
2024-02-02 15:24:09,377 Epoch 2994: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 15:24:09,378 EPOCH 2995
2024-02-02 15:24:09,940 [Epoch: 2995 Step: 00050900] Batch Recognition Loss:   0.000178 => Gls Tokens per Sec:     2278 || Batch Translation Loss:   0.018586 => Txt Tokens per Sec:     6346 || Lr: 0.000050
2024-02-02 15:24:16,081 Epoch 2995: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 15:24:16,082 EPOCH 2996
2024-02-02 15:24:23,031 Epoch 2996: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 15:24:23,032 EPOCH 2997
2024-02-02 15:24:29,807 Epoch 2997: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 15:24:29,808 EPOCH 2998
2024-02-02 15:24:36,629 Epoch 2998: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:24:36,629 EPOCH 2999
2024-02-02 15:24:42,981 Epoch 2999: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 15:24:42,982 EPOCH 3000
2024-02-02 15:24:49,386 [Epoch: 3000 Step: 00051000] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:     1660 || Batch Translation Loss:   0.012474 => Txt Tokens per Sec:     4608 || Lr: 0.000050
2024-02-02 15:24:49,387 Epoch 3000: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:24:49,387 EPOCH 3001
2024-02-02 15:24:56,247 Epoch 3001: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 15:24:56,248 EPOCH 3002
2024-02-02 15:25:03,069 Epoch 3002: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:25:03,070 EPOCH 3003
2024-02-02 15:25:10,021 Epoch 3003: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 15:25:10,021 EPOCH 3004
2024-02-02 15:25:16,772 Epoch 3004: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 15:25:16,772 EPOCH 3005
2024-02-02 15:25:23,395 Epoch 3005: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 15:25:23,396 EPOCH 3006
2024-02-02 15:25:29,857 [Epoch: 3006 Step: 00051100] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:     1447 || Batch Translation Loss:   0.050358 => Txt Tokens per Sec:     4078 || Lr: 0.000050
2024-02-02 15:25:30,186 Epoch 3006: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 15:25:30,187 EPOCH 3007
2024-02-02 15:25:37,011 Epoch 3007: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 15:25:37,011 EPOCH 3008
2024-02-02 15:25:43,286 Epoch 3008: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 15:25:43,287 EPOCH 3009
2024-02-02 15:25:49,728 Epoch 3009: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.74 
2024-02-02 15:25:49,728 EPOCH 3010
2024-02-02 15:25:56,405 Epoch 3010: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-02 15:25:56,406 EPOCH 3011
2024-02-02 15:26:02,976 Epoch 3011: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-02 15:26:02,976 EPOCH 3012
2024-02-02 15:26:08,677 [Epoch: 3012 Step: 00051200] Batch Recognition Loss:   0.000163 => Gls Tokens per Sec:     1416 || Batch Translation Loss:   0.024516 => Txt Tokens per Sec:     3954 || Lr: 0.000050
2024-02-02 15:26:09,837 Epoch 3012: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-02 15:26:09,837 EPOCH 3013
2024-02-02 15:26:16,633 Epoch 3013: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-02 15:26:16,633 EPOCH 3014
2024-02-02 15:26:23,209 Epoch 3014: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-02 15:26:23,210 EPOCH 3015
2024-02-02 15:26:29,834 Epoch 3015: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 15:26:29,835 EPOCH 3016
2024-02-02 15:26:36,626 Epoch 3016: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 15:26:36,627 EPOCH 3017
2024-02-02 15:26:43,494 Epoch 3017: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 15:26:43,495 EPOCH 3018
2024-02-02 15:26:48,609 [Epoch: 3018 Step: 00051300] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:     1328 || Batch Translation Loss:   0.013325 => Txt Tokens per Sec:     3587 || Lr: 0.000050
2024-02-02 15:26:50,583 Epoch 3018: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 15:26:50,584 EPOCH 3019
2024-02-02 15:26:57,405 Epoch 3019: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 15:26:57,406 EPOCH 3020
2024-02-02 15:27:04,000 Epoch 3020: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 15:27:04,001 EPOCH 3021
2024-02-02 15:27:10,363 Epoch 3021: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 15:27:10,364 EPOCH 3022
2024-02-02 15:27:17,258 Epoch 3022: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 15:27:17,258 EPOCH 3023
2024-02-02 15:27:23,972 Epoch 3023: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 15:27:23,973 EPOCH 3024
2024-02-02 15:27:26,241 [Epoch: 3024 Step: 00051400] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:     2542 || Batch Translation Loss:   0.015034 => Txt Tokens per Sec:     6978 || Lr: 0.000050
2024-02-02 15:27:30,785 Epoch 3024: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 15:27:30,785 EPOCH 3025
2024-02-02 15:27:37,740 Epoch 3025: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 15:27:37,741 EPOCH 3026
2024-02-02 15:27:44,348 Epoch 3026: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 15:27:44,348 EPOCH 3027
2024-02-02 15:27:51,089 Epoch 3027: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 15:27:51,090 EPOCH 3028
2024-02-02 15:27:57,583 Epoch 3028: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:27:57,584 EPOCH 3029
2024-02-02 15:28:04,485 Epoch 3029: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 15:28:04,486 EPOCH 3030
2024-02-02 15:28:06,500 [Epoch: 3030 Step: 00051500] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     2226 || Batch Translation Loss:   0.019376 => Txt Tokens per Sec:     6461 || Lr: 0.000050
2024-02-02 15:28:11,204 Epoch 3030: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 15:28:11,204 EPOCH 3031
2024-02-02 15:28:17,880 Epoch 3031: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 15:28:17,880 EPOCH 3032
2024-02-02 15:28:24,516 Epoch 3032: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:28:24,516 EPOCH 3033
2024-02-02 15:28:31,203 Epoch 3033: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:28:31,204 EPOCH 3034
2024-02-02 15:28:37,553 Epoch 3034: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:28:37,553 EPOCH 3035
2024-02-02 15:28:43,586 Epoch 3035: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 15:28:43,586 EPOCH 3036
2024-02-02 15:28:44,500 [Epoch: 3036 Step: 00051600] Batch Recognition Loss:   0.000103 => Gls Tokens per Sec:     3509 || Batch Translation Loss:   0.013444 => Txt Tokens per Sec:     8217 || Lr: 0.000050
2024-02-02 15:28:50,692 Epoch 3036: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 15:28:50,692 EPOCH 3037
2024-02-02 15:28:57,547 Epoch 3037: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 15:28:57,548 EPOCH 3038
2024-02-02 15:29:04,322 Epoch 3038: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 15:29:04,323 EPOCH 3039
2024-02-02 15:29:11,107 Epoch 3039: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-02 15:29:11,107 EPOCH 3040
2024-02-02 15:29:17,905 Epoch 3040: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.66 
2024-02-02 15:29:17,906 EPOCH 3041
2024-02-02 15:29:24,492 Epoch 3041: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.01 
2024-02-02 15:29:24,492 EPOCH 3042
2024-02-02 15:29:25,460 [Epoch: 3042 Step: 00051700] Batch Recognition Loss:   0.000866 => Gls Tokens per Sec:     1984 || Batch Translation Loss:   0.055152 => Txt Tokens per Sec:     5619 || Lr: 0.000050
2024-02-02 15:29:31,496 Epoch 3042: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.02 
2024-02-02 15:29:31,496 EPOCH 3043
2024-02-02 15:29:38,284 Epoch 3043: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-02 15:29:38,284 EPOCH 3044
2024-02-02 15:29:44,770 Epoch 3044: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-02 15:29:44,770 EPOCH 3045
2024-02-02 15:29:51,227 Epoch 3045: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-02 15:29:51,228 EPOCH 3046
2024-02-02 15:29:58,051 Epoch 3046: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 15:29:58,051 EPOCH 3047
2024-02-02 15:30:05,047 Epoch 3047: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 15:30:05,048 EPOCH 3048
2024-02-02 15:30:05,228 [Epoch: 3048 Step: 00051800] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     3596 || Batch Translation Loss:   0.015129 => Txt Tokens per Sec:     8635 || Lr: 0.000050
2024-02-02 15:30:11,802 Epoch 3048: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 15:30:11,802 EPOCH 3049
2024-02-02 15:30:18,548 Epoch 3049: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 15:30:18,548 EPOCH 3050
2024-02-02 15:30:25,470 Epoch 3050: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 15:30:25,470 EPOCH 3051
2024-02-02 15:30:32,368 Epoch 3051: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 15:30:32,368 EPOCH 3052
2024-02-02 15:30:39,106 Epoch 3052: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 15:30:39,107 EPOCH 3053
2024-02-02 15:30:45,482 [Epoch: 3053 Step: 00051900] Batch Recognition Loss:   0.000095 => Gls Tokens per Sec:     1567 || Batch Translation Loss:   0.009782 => Txt Tokens per Sec:     4407 || Lr: 0.000050
2024-02-02 15:30:45,628 Epoch 3053: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:30:45,628 EPOCH 3054
2024-02-02 15:30:52,450 Epoch 3054: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:30:52,450 EPOCH 3055
2024-02-02 15:30:59,099 Epoch 3055: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 15:30:59,099 EPOCH 3056
2024-02-02 15:31:05,905 Epoch 3056: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 15:31:05,905 EPOCH 3057
2024-02-02 15:31:12,528 Epoch 3057: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 15:31:12,529 EPOCH 3058
2024-02-02 15:31:19,241 Epoch 3058: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:31:19,242 EPOCH 3059
2024-02-02 15:31:24,788 [Epoch: 3059 Step: 00052000] Batch Recognition Loss:   0.000097 => Gls Tokens per Sec:     1571 || Batch Translation Loss:   0.009898 => Txt Tokens per Sec:     4310 || Lr: 0.000050
2024-02-02 15:31:55,259 Validation result at epoch 3059, step    52000: duration: 30.4711s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00080	Translation Loss: 102712.35938	PPL: 29098.18164
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.49	(BLEU-1: 10.59,	BLEU-2: 3.09,	BLEU-3: 1.10,	BLEU-4: 0.49)
	CHRF 16.80	ROUGE 9.03
2024-02-02 15:31:55,260 Logging Recognition and Translation Outputs
2024-02-02 15:31:55,261 ========================================================================================================================
2024-02-02 15:31:55,261 Logging Sequence: 87_229.00
2024-02-02 15:31:55,261 	Gloss Reference :	A B+C+D+E
2024-02-02 15:31:55,261 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 15:31:55,262 	Gloss Alignment :	         
2024-02-02 15:31:55,262 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 15:31:55,263 	Text Reference  :	** *** *** **** ***** it  was not against dhoni or    kohli
2024-02-02 15:31:55,263 	Text Hypothesis :	or her win some other did he  is  in      the   first time 
2024-02-02 15:31:55,263 	Text Alignment  :	I  I   I   I    I     S   S   S   S       S     S     S    
2024-02-02 15:31:55,263 ========================================================================================================================
2024-02-02 15:31:55,263 Logging Sequence: 134_153.00
2024-02-02 15:31:55,263 	Gloss Reference :	A B+C+D+E
2024-02-02 15:31:55,264 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 15:31:55,264 	Gloss Alignment :	         
2024-02-02 15:31:55,264 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 15:31:55,266 	Text Reference  :	pm modi in     his       interaction said  that deaf athletes must fight  for their goals and       never  give up  despite the     losses
2024-02-02 15:31:55,266 	Text Hypothesis :	** the  sports authority of          india sai  had  asked    the  reason for ***** our   champions trophy he   was widely  watched sports
2024-02-02 15:31:55,266 	Text Alignment  :	D  S    S      S         S           S     S    S    S        S    S          D     S     S         S      S    S   S       S       S     
2024-02-02 15:31:55,267 ========================================================================================================================
2024-02-02 15:31:55,267 Logging Sequence: 137_155.00
2024-02-02 15:31:55,267 	Gloss Reference :	A B+C+D+E
2024-02-02 15:31:55,267 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 15:31:55,267 	Gloss Alignment :	         
2024-02-02 15:31:55,267 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 15:31:55,268 	Text Reference  :	***** ****** an    extremely high   tax     named  as    sin     tax  will be  applied
2024-02-02 15:31:55,268 	Text Hypothesis :	after taking place with      indian premier league stage matches were all  the auction
2024-02-02 15:31:55,268 	Text Alignment  :	I     I      S     S         S      S       S      S     S       S    S    S   S      
2024-02-02 15:31:55,269 ========================================================================================================================
2024-02-02 15:31:55,269 Logging Sequence: 59_18.00
2024-02-02 15:31:55,269 	Gloss Reference :	A B+C+D+E
2024-02-02 15:31:55,269 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 15:31:55,269 	Gloss Alignment :	         
2024-02-02 15:31:55,269 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 15:31:55,271 	Text Reference  :	**** *** ********** 27-year-old jessica fox      from australia won    a   bronze a       gold medal in  canoeing
2024-02-02 15:31:55,271 	Text Hypothesis :	well the organisers of          the     olympics in   tokyo     handed out 60000  condoms to   all   the athletes
2024-02-02 15:31:55,271 	Text Alignment  :	I    I   I          S           S       S        S    S         S      S   S      S       S    S     S   S       
2024-02-02 15:31:55,271 ========================================================================================================================
2024-02-02 15:31:55,271 Logging Sequence: 173_103.00
2024-02-02 15:31:55,271 	Gloss Reference :	A B+C+D+E
2024-02-02 15:31:55,271 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 15:31:55,272 	Gloss Alignment :	         
2024-02-02 15:31:55,272 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 15:31:55,272 	Text Reference  :	***** *** ******** *** ***** **** these rumours are absolutely rubbish
2024-02-02 15:31:55,272 	Text Hypothesis :	after the marriage the other team was   held    in  huge       problem
2024-02-02 15:31:55,272 	Text Alignment  :	I     I   I        I   I     I    S     S       S   S          S      
2024-02-02 15:31:55,273 ========================================================================================================================
2024-02-02 15:31:56,368 Epoch 3059: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 15:31:56,368 EPOCH 3060
2024-02-02 15:32:03,278 Epoch 3060: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:32:03,279 EPOCH 3061
2024-02-02 15:32:09,776 Epoch 3061: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:32:09,777 EPOCH 3062
2024-02-02 15:32:16,425 Epoch 3062: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:32:16,426 EPOCH 3063
2024-02-02 15:32:22,910 Epoch 3063: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 15:32:22,910 EPOCH 3064
2024-02-02 15:32:29,811 Epoch 3064: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:32:29,811 EPOCH 3065
2024-02-02 15:32:35,306 [Epoch: 3065 Step: 00052100] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     1352 || Batch Translation Loss:   0.014975 => Txt Tokens per Sec:     3761 || Lr: 0.000050
2024-02-02 15:32:36,737 Epoch 3065: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 15:32:36,737 EPOCH 3066
2024-02-02 15:32:43,342 Epoch 3066: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 15:32:43,343 EPOCH 3067
2024-02-02 15:32:49,936 Epoch 3067: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:32:49,937 EPOCH 3068
2024-02-02 15:32:56,916 Epoch 3068: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:32:56,916 EPOCH 3069
2024-02-02 15:33:03,745 Epoch 3069: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:33:03,746 EPOCH 3070
2024-02-02 15:33:10,439 Epoch 3070: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:33:10,440 EPOCH 3071
2024-02-02 15:33:15,488 [Epoch: 3071 Step: 00052200] Batch Recognition Loss:   0.000101 => Gls Tokens per Sec:     1219 || Batch Translation Loss:   0.008686 => Txt Tokens per Sec:     3662 || Lr: 0.000050
2024-02-02 15:33:17,044 Epoch 3071: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:33:17,044 EPOCH 3072
2024-02-02 15:33:23,836 Epoch 3072: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:33:23,836 EPOCH 3073
2024-02-02 15:33:30,676 Epoch 3073: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 15:33:30,676 EPOCH 3074
2024-02-02 15:33:37,538 Epoch 3074: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:33:37,538 EPOCH 3075
2024-02-02 15:33:44,206 Epoch 3075: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 15:33:44,207 EPOCH 3076
2024-02-02 15:33:50,573 Epoch 3076: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 15:33:50,574 EPOCH 3077
2024-02-02 15:33:54,597 [Epoch: 3077 Step: 00052300] Batch Recognition Loss:   0.000111 => Gls Tokens per Sec:     1211 || Batch Translation Loss:   0.012412 => Txt Tokens per Sec:     3326 || Lr: 0.000050
2024-02-02 15:33:57,350 Epoch 3077: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 15:33:57,350 EPOCH 3078
2024-02-02 15:34:04,247 Epoch 3078: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:34:04,248 EPOCH 3079
2024-02-02 15:34:11,174 Epoch 3079: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:34:11,175 EPOCH 3080
2024-02-02 15:34:17,669 Epoch 3080: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 15:34:17,670 EPOCH 3081
2024-02-02 15:34:24,114 Epoch 3081: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 15:34:24,114 EPOCH 3082
2024-02-02 15:34:30,066 Epoch 3082: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 15:34:30,067 EPOCH 3083
2024-02-02 15:34:31,233 [Epoch: 3083 Step: 00052400] Batch Recognition Loss:   0.000111 => Gls Tokens per Sec:     3293 || Batch Translation Loss:   0.014849 => Txt Tokens per Sec:     9120 || Lr: 0.000050
2024-02-02 15:34:36,584 Epoch 3083: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 15:34:36,585 EPOCH 3084
2024-02-02 15:34:43,442 Epoch 3084: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 15:34:43,443 EPOCH 3085
2024-02-02 15:34:50,318 Epoch 3085: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 15:34:50,319 EPOCH 3086
2024-02-02 15:34:56,807 Epoch 3086: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:34:56,807 EPOCH 3087
2024-02-02 15:35:03,631 Epoch 3087: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 15:35:03,632 EPOCH 3088
2024-02-02 15:35:10,537 Epoch 3088: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:35:10,538 EPOCH 3089
2024-02-02 15:35:11,686 [Epoch: 3089 Step: 00052500] Batch Recognition Loss:   0.000107 => Gls Tokens per Sec:     2234 || Batch Translation Loss:   0.009594 => Txt Tokens per Sec:     6090 || Lr: 0.000050
2024-02-02 15:35:17,431 Epoch 3089: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:35:17,431 EPOCH 3090
2024-02-02 15:35:23,906 Epoch 3090: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:35:23,906 EPOCH 3091
2024-02-02 15:35:30,595 Epoch 3091: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:35:30,596 EPOCH 3092
2024-02-02 15:35:37,356 Epoch 3092: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:35:37,356 EPOCH 3093
2024-02-02 15:35:44,177 Epoch 3093: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:35:44,178 EPOCH 3094
2024-02-02 15:35:50,967 Epoch 3094: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 15:35:50,967 EPOCH 3095
2024-02-02 15:35:51,466 [Epoch: 3095 Step: 00052600] Batch Recognition Loss:   0.000087 => Gls Tokens per Sec:     2570 || Batch Translation Loss:   0.006336 => Txt Tokens per Sec:     6516 || Lr: 0.000050
2024-02-02 15:35:57,716 Epoch 3095: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-02 15:35:57,717 EPOCH 3096
2024-02-02 15:36:04,723 Epoch 3096: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-02 15:36:04,723 EPOCH 3097
2024-02-02 15:36:11,346 Epoch 3097: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.71 
2024-02-02 15:36:11,346 EPOCH 3098
2024-02-02 15:36:18,084 Epoch 3098: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.68 
2024-02-02 15:36:18,085 EPOCH 3099
2024-02-02 15:36:24,664 Epoch 3099: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 15:36:24,665 EPOCH 3100
2024-02-02 15:36:31,483 [Epoch: 3100 Step: 00052700] Batch Recognition Loss:   0.000234 => Gls Tokens per Sec:     1559 || Batch Translation Loss:   0.058611 => Txt Tokens per Sec:     4329 || Lr: 0.000050
2024-02-02 15:36:31,484 Epoch 3100: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 15:36:31,484 EPOCH 3101
2024-02-02 15:36:38,366 Epoch 3101: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 15:36:38,367 EPOCH 3102
2024-02-02 15:36:45,187 Epoch 3102: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 15:36:45,188 EPOCH 3103
2024-02-02 15:36:51,827 Epoch 3103: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 15:36:51,827 EPOCH 3104
2024-02-02 15:36:58,289 Epoch 3104: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 15:36:58,289 EPOCH 3105
2024-02-02 15:37:04,992 Epoch 3105: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 15:37:04,992 EPOCH 3106
2024-02-02 15:37:10,690 [Epoch: 3106 Step: 00052800] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:     1641 || Batch Translation Loss:   0.021175 => Txt Tokens per Sec:     4466 || Lr: 0.000050
2024-02-02 15:37:11,557 Epoch 3106: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 15:37:11,558 EPOCH 3107
2024-02-02 15:37:18,322 Epoch 3107: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 15:37:18,323 EPOCH 3108
2024-02-02 15:37:25,079 Epoch 3108: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 15:37:25,080 EPOCH 3109
2024-02-02 15:37:31,763 Epoch 3109: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 15:37:31,763 EPOCH 3110
2024-02-02 15:37:38,525 Epoch 3110: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 15:37:38,526 EPOCH 3111
2024-02-02 15:37:45,417 Epoch 3111: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 15:37:45,418 EPOCH 3112
2024-02-02 15:37:50,831 [Epoch: 3112 Step: 00052900] Batch Recognition Loss:   0.000130 => Gls Tokens per Sec:     1491 || Batch Translation Loss:   0.012707 => Txt Tokens per Sec:     4048 || Lr: 0.000050
2024-02-02 15:37:52,005 Epoch 3112: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 15:37:52,006 EPOCH 3113
2024-02-02 15:37:58,872 Epoch 3113: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 15:37:58,873 EPOCH 3114
2024-02-02 15:38:05,521 Epoch 3114: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 15:38:05,521 EPOCH 3115
2024-02-02 15:38:12,225 Epoch 3115: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 15:38:12,225 EPOCH 3116
2024-02-02 15:38:18,936 Epoch 3116: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 15:38:18,937 EPOCH 3117
2024-02-02 15:38:25,650 Epoch 3117: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:38:25,650 EPOCH 3118
2024-02-02 15:38:31,013 [Epoch: 3118 Step: 00053000] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:     1266 || Batch Translation Loss:   0.030930 => Txt Tokens per Sec:     3556 || Lr: 0.000050
2024-02-02 15:38:32,624 Epoch 3118: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 15:38:32,625 EPOCH 3119
2024-02-02 15:38:39,562 Epoch 3119: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 15:38:39,563 EPOCH 3120
2024-02-02 15:38:46,506 Epoch 3120: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 15:38:46,507 EPOCH 3121
2024-02-02 15:38:52,962 Epoch 3121: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 15:38:52,963 EPOCH 3122
2024-02-02 15:38:59,819 Epoch 3122: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 15:38:59,819 EPOCH 3123
2024-02-02 15:39:05,789 Epoch 3123: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 15:39:05,790 EPOCH 3124
2024-02-02 15:39:10,022 [Epoch: 3124 Step: 00053100] Batch Recognition Loss:   0.000111 => Gls Tokens per Sec:     1302 || Batch Translation Loss:   0.018438 => Txt Tokens per Sec:     3329 || Lr: 0.000050
2024-02-02 15:39:12,627 Epoch 3124: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 15:39:12,627 EPOCH 3125
2024-02-02 15:39:19,296 Epoch 3125: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 15:39:19,297 EPOCH 3126
2024-02-02 15:39:26,207 Epoch 3126: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 15:39:26,207 EPOCH 3127
2024-02-02 15:39:33,138 Epoch 3127: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 15:39:33,138 EPOCH 3128
2024-02-02 15:39:39,980 Epoch 3128: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 15:39:39,980 EPOCH 3129
2024-02-02 15:39:46,790 Epoch 3129: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 15:39:46,791 EPOCH 3130
2024-02-02 15:39:49,129 [Epoch: 3130 Step: 00053200] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:     1916 || Batch Translation Loss:   0.014119 => Txt Tokens per Sec:     5524 || Lr: 0.000050
2024-02-02 15:39:53,626 Epoch 3130: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 15:39:53,626 EPOCH 3131
2024-02-02 15:40:00,579 Epoch 3131: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 15:40:00,580 EPOCH 3132
2024-02-02 15:40:07,408 Epoch 3132: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:40:07,409 EPOCH 3133
2024-02-02 15:40:13,929 Epoch 3133: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 15:40:13,929 EPOCH 3134
2024-02-02 15:40:20,395 Epoch 3134: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:40:20,396 EPOCH 3135
2024-02-02 15:40:27,142 Epoch 3135: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 15:40:27,142 EPOCH 3136
2024-02-02 15:40:28,575 [Epoch: 3136 Step: 00053300] Batch Recognition Loss:   0.000096 => Gls Tokens per Sec:     2235 || Batch Translation Loss:   0.025832 => Txt Tokens per Sec:     6723 || Lr: 0.000050
2024-02-02 15:40:33,789 Epoch 3136: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 15:40:33,790 EPOCH 3137
2024-02-02 15:40:40,499 Epoch 3137: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-02 15:40:40,499 EPOCH 3138
2024-02-02 15:40:47,365 Epoch 3138: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 15:40:47,366 EPOCH 3139
2024-02-02 15:40:54,096 Epoch 3139: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 15:40:54,097 EPOCH 3140
2024-02-02 15:41:00,882 Epoch 3140: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-02 15:41:00,882 EPOCH 3141
2024-02-02 15:41:07,508 Epoch 3141: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 15:41:07,509 EPOCH 3142
2024-02-02 15:41:10,306 [Epoch: 3142 Step: 00053400] Batch Recognition Loss:   0.000245 => Gls Tokens per Sec:      597 || Batch Translation Loss:   0.025394 => Txt Tokens per Sec:     1637 || Lr: 0.000050
2024-02-02 15:41:14,341 Epoch 3142: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 15:41:14,341 EPOCH 3143
2024-02-02 15:41:21,179 Epoch 3143: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 15:41:21,179 EPOCH 3144
2024-02-02 15:41:28,082 Epoch 3144: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 15:41:28,083 EPOCH 3145
2024-02-02 15:41:35,014 Epoch 3145: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 15:41:35,015 EPOCH 3146
2024-02-02 15:41:41,482 Epoch 3146: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 15:41:41,483 EPOCH 3147
2024-02-02 15:41:48,447 Epoch 3147: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 15:41:48,448 EPOCH 3148
2024-02-02 15:41:48,571 [Epoch: 3148 Step: 00053500] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:     5246 || Batch Translation Loss:   0.013035 => Txt Tokens per Sec:    11942 || Lr: 0.000050
2024-02-02 15:41:55,197 Epoch 3148: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 15:41:55,198 EPOCH 3149
2024-02-02 15:42:01,820 Epoch 3149: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-02 15:42:01,821 EPOCH 3150
2024-02-02 15:42:08,268 Epoch 3150: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.76 
2024-02-02 15:42:08,268 EPOCH 3151
2024-02-02 15:42:14,304 Epoch 3151: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.88 
2024-02-02 15:42:14,304 EPOCH 3152
2024-02-02 15:42:21,384 Epoch 3152: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-02 15:42:21,385 EPOCH 3153
2024-02-02 15:42:28,130 [Epoch: 3153 Step: 00053600] Batch Recognition Loss:   0.000207 => Gls Tokens per Sec:     1481 || Batch Translation Loss:   0.044448 => Txt Tokens per Sec:     4130 || Lr: 0.000050
2024-02-02 15:42:28,303 Epoch 3153: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-02 15:42:28,303 EPOCH 3154
2024-02-02 15:42:34,818 Epoch 3154: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 15:42:34,819 EPOCH 3155
2024-02-02 15:42:41,598 Epoch 3155: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 15:42:41,598 EPOCH 3156
2024-02-02 15:42:48,199 Epoch 3156: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 15:42:48,199 EPOCH 3157
2024-02-02 15:42:55,082 Epoch 3157: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 15:42:55,083 EPOCH 3158
2024-02-02 15:43:01,754 Epoch 3158: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 15:43:01,755 EPOCH 3159
2024-02-02 15:43:07,772 [Epoch: 3159 Step: 00053700] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:     1448 || Batch Translation Loss:   0.015918 => Txt Tokens per Sec:     4137 || Lr: 0.000050
2024-02-02 15:43:08,307 Epoch 3159: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 15:43:08,307 EPOCH 3160
2024-02-02 15:43:15,065 Epoch 3160: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 15:43:15,066 EPOCH 3161
2024-02-02 15:43:21,841 Epoch 3161: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 15:43:21,842 EPOCH 3162
2024-02-02 15:43:28,583 Epoch 3162: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:43:28,584 EPOCH 3163
2024-02-02 15:43:35,316 Epoch 3163: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:43:35,316 EPOCH 3164
2024-02-02 15:43:42,200 Epoch 3164: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 15:43:42,201 EPOCH 3165
2024-02-02 15:43:47,911 [Epoch: 3165 Step: 00053800] Batch Recognition Loss:   0.000111 => Gls Tokens per Sec:     1302 || Batch Translation Loss:   0.009721 => Txt Tokens per Sec:     3674 || Lr: 0.000050
2024-02-02 15:43:49,193 Epoch 3165: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:43:49,193 EPOCH 3166
2024-02-02 15:43:55,865 Epoch 3166: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:43:55,865 EPOCH 3167
2024-02-02 15:44:02,701 Epoch 3167: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:44:02,701 EPOCH 3168
2024-02-02 15:44:09,421 Epoch 3168: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:44:09,421 EPOCH 3169
2024-02-02 15:44:16,065 Epoch 3169: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:44:16,065 EPOCH 3170
2024-02-02 15:44:22,946 Epoch 3170: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:44:22,947 EPOCH 3171
2024-02-02 15:44:27,644 [Epoch: 3171 Step: 00053900] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:     1310 || Batch Translation Loss:   0.014051 => Txt Tokens per Sec:     3433 || Lr: 0.000050
2024-02-02 15:44:29,918 Epoch 3171: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:44:29,918 EPOCH 3172
2024-02-02 15:44:36,701 Epoch 3172: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 15:44:36,702 EPOCH 3173
2024-02-02 15:44:43,516 Epoch 3173: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 15:44:43,516 EPOCH 3174
2024-02-02 15:44:50,094 Epoch 3174: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 15:44:50,095 EPOCH 3175
2024-02-02 15:44:56,862 Epoch 3175: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 15:44:56,862 EPOCH 3176
2024-02-02 15:45:03,653 Epoch 3176: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 15:45:03,654 EPOCH 3177
2024-02-02 15:45:05,792 [Epoch: 3177 Step: 00054000] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:     2396 || Batch Translation Loss:   0.010772 => Txt Tokens per Sec:     6831 || Lr: 0.000050
2024-02-02 15:45:36,067 Validation result at epoch 3177, step    54000: duration: 30.2753s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00103	Translation Loss: 103691.54688	PPL: 32093.79883
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.54	(BLEU-1: 9.27,	BLEU-2: 2.81,	BLEU-3: 1.13,	BLEU-4: 0.54)
	CHRF 15.94	ROUGE 8.09
2024-02-02 15:45:36,068 Logging Recognition and Translation Outputs
2024-02-02 15:45:36,068 ========================================================================================================================
2024-02-02 15:45:36,069 Logging Sequence: 130_139.00
2024-02-02 15:45:36,069 	Gloss Reference :	A B+C+D+E
2024-02-02 15:45:36,069 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 15:45:36,069 	Gloss Alignment :	         
2024-02-02 15:45:36,069 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 15:45:36,071 	Text Reference  :	he shared a picture of a little pouch he knit for his olympic gold medal with uk flag   on one   side    and **** japanese flag    on  the other  
2024-02-02 15:45:36,072 	Text Hypothesis :	** ****** * ******* ** * ****** ***** he **** *** *** ******* **** ***** **** ** played 18 tests matches and took 293      wickets due to  bowling
2024-02-02 15:45:36,072 	Text Alignment  :	D  D      D D       D  D D      D        D    D   D   D       D    D     D    D  S      S  S     S           I    S        S       S   S   S      
2024-02-02 15:45:36,072 ========================================================================================================================
2024-02-02 15:45:36,072 Logging Sequence: 148_155.00
2024-02-02 15:45:36,072 	Gloss Reference :	A B+C+D+E
2024-02-02 15:45:36,072 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 15:45:36,073 	Gloss Alignment :	         
2024-02-02 15:45:36,073 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 15:45:36,074 	Text Reference  :	india won the **** match with    263 balls remaining and without losing any wicket
2024-02-02 15:45:36,074 	Text Hypothesis :	india won the toss and   decided to  ball  first     3   wickets were   the bat   
2024-02-02 15:45:36,074 	Text Alignment  :	              I    S     S       S   S     S         S   S       S      S   S     
2024-02-02 15:45:36,074 ========================================================================================================================
2024-02-02 15:45:36,074 Logging Sequence: 126_99.00
2024-02-02 15:45:36,075 	Gloss Reference :	A B+C+D+E
2024-02-02 15:45:36,075 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 15:45:36,075 	Gloss Alignment :	         
2024-02-02 15:45:36,075 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 15:45:36,075 	Text Reference  :	he dedicated the   medal    to       sprinter milkha singh
2024-02-02 15:45:36,075 	Text Hypothesis :	** because   prime minister narendra modi     had    said 
2024-02-02 15:45:36,076 	Text Alignment  :	D  S         S     S        S        S        S      S    
2024-02-02 15:45:36,076 ========================================================================================================================
2024-02-02 15:45:36,076 Logging Sequence: 149_77.00
2024-02-02 15:45:36,076 	Gloss Reference :	A B+C+D+E
2024-02-02 15:45:36,076 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 15:45:36,076 	Gloss Alignment :	         
2024-02-02 15:45:36,076 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 15:45:36,078 	Text Reference  :	and arrested danushka for alleged sexual assault of a 29 year    old     woman     whose  name has **** not been  disclosed
2024-02-02 15:45:36,078 	Text Hypothesis :	*** ******** ******** *** ******* ****** ******* ** * to compete against rajasthan royals and  has left icc would lose     
2024-02-02 15:45:36,078 	Text Alignment  :	D   D        D        D   D       D      D       D  D S  S       S       S         S      S        I    S   S     S        
2024-02-02 15:45:36,078 ========================================================================================================================
2024-02-02 15:45:36,078 Logging Sequence: 168_15.00
2024-02-02 15:45:36,078 	Gloss Reference :	A B+C+D+E
2024-02-02 15:45:36,079 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 15:45:36,079 	Gloss Alignment :	         
2024-02-02 15:45:36,079 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 15:45:36,079 	Text Reference  :	when in public the couple are always approached for    photographys and     autographs
2024-02-02 15:45:36,079 	Text Hypothesis :	**** ** ****** *** ****** *** india  had        become a            strange match     
2024-02-02 15:45:36,080 	Text Alignment  :	D    D  D      D   D      D   S      S          S      S            S       S         
2024-02-02 15:45:36,080 ========================================================================================================================
2024-02-02 15:45:40,363 Epoch 3177: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 15:45:40,363 EPOCH 3178
2024-02-02 15:45:47,333 Epoch 3178: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 15:45:47,334 EPOCH 3179
2024-02-02 15:45:54,029 Epoch 3179: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 15:45:54,030 EPOCH 3180
2024-02-02 15:46:00,793 Epoch 3180: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 15:46:00,793 EPOCH 3181
2024-02-02 15:46:07,404 Epoch 3181: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 15:46:07,404 EPOCH 3182
2024-02-02 15:46:14,337 Epoch 3182: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 15:46:14,337 EPOCH 3183
2024-02-02 15:46:15,992 [Epoch: 3183 Step: 00054100] Batch Recognition Loss:   0.000169 => Gls Tokens per Sec:     2322 || Batch Translation Loss:   0.018926 => Txt Tokens per Sec:     6746 || Lr: 0.000050
2024-02-02 15:46:21,136 Epoch 3183: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 15:46:21,136 EPOCH 3184
2024-02-02 15:46:27,888 Epoch 3184: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 15:46:27,888 EPOCH 3185
2024-02-02 15:46:34,498 Epoch 3185: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 15:46:34,499 EPOCH 3186
2024-02-02 15:46:41,337 Epoch 3186: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-02 15:46:41,337 EPOCH 3187
2024-02-02 15:46:48,237 Epoch 3187: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-02 15:46:48,238 EPOCH 3188
2024-02-02 15:46:54,949 Epoch 3188: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-02 15:46:54,949 EPOCH 3189
2024-02-02 15:46:56,061 [Epoch: 3189 Step: 00054200] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     2305 || Batch Translation Loss:   0.101906 => Txt Tokens per Sec:     6512 || Lr: 0.000050
2024-02-02 15:47:01,694 Epoch 3189: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-02 15:47:01,694 EPOCH 3190
2024-02-02 15:47:08,302 Epoch 3190: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 15:47:08,303 EPOCH 3191
2024-02-02 15:47:15,217 Epoch 3191: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 15:47:15,218 EPOCH 3192
2024-02-02 15:47:21,936 Epoch 3192: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 15:47:21,937 EPOCH 3193
2024-02-02 15:47:28,852 Epoch 3193: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 15:47:28,853 EPOCH 3194
2024-02-02 15:47:35,077 Epoch 3194: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 15:47:35,078 EPOCH 3195
2024-02-02 15:47:35,665 [Epoch: 3195 Step: 00054300] Batch Recognition Loss:   0.000224 => Gls Tokens per Sec:     2182 || Batch Translation Loss:   0.015677 => Txt Tokens per Sec:     6076 || Lr: 0.000050
2024-02-02 15:47:41,916 Epoch 3195: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 15:47:41,916 EPOCH 3196
2024-02-02 15:47:48,767 Epoch 3196: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 15:47:48,768 EPOCH 3197
2024-02-02 15:47:55,490 Epoch 3197: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:47:55,491 EPOCH 3198
2024-02-02 15:48:02,309 Epoch 3198: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:48:02,310 EPOCH 3199
2024-02-02 15:48:08,911 Epoch 3199: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:48:08,912 EPOCH 3200
2024-02-02 15:48:15,826 [Epoch: 3200 Step: 00054400] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     1538 || Batch Translation Loss:   0.021456 => Txt Tokens per Sec:     4269 || Lr: 0.000050
2024-02-02 15:48:15,826 Epoch 3200: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:48:15,827 EPOCH 3201
2024-02-02 15:48:22,721 Epoch 3201: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 15:48:22,721 EPOCH 3202
2024-02-02 15:48:29,603 Epoch 3202: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 15:48:29,604 EPOCH 3203
2024-02-02 15:48:36,331 Epoch 3203: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-02 15:48:36,332 EPOCH 3204
2024-02-02 15:48:43,174 Epoch 3204: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 15:48:43,175 EPOCH 3205
2024-02-02 15:48:50,101 Epoch 3205: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 15:48:50,101 EPOCH 3206
2024-02-02 15:48:55,988 [Epoch: 3206 Step: 00054500] Batch Recognition Loss:   0.000079 => Gls Tokens per Sec:     1589 || Batch Translation Loss:   0.005370 => Txt Tokens per Sec:     4413 || Lr: 0.000050
2024-02-02 15:48:56,553 Epoch 3206: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 15:48:56,554 EPOCH 3207
2024-02-02 15:49:03,533 Epoch 3207: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 15:49:03,533 EPOCH 3208
2024-02-02 15:49:10,385 Epoch 3208: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 15:49:10,386 EPOCH 3209
2024-02-02 15:49:17,004 Epoch 3209: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 15:49:17,005 EPOCH 3210
2024-02-02 15:49:23,536 Epoch 3210: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 15:49:23,537 EPOCH 3211
2024-02-02 15:49:30,273 Epoch 3211: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 15:49:30,273 EPOCH 3212
2024-02-02 15:49:36,326 [Epoch: 3212 Step: 00054600] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:     1333 || Batch Translation Loss:   0.012557 => Txt Tokens per Sec:     3770 || Lr: 0.000050
2024-02-02 15:49:37,084 Epoch 3212: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 15:49:37,085 EPOCH 3213
2024-02-02 15:49:43,881 Epoch 3213: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:49:43,882 EPOCH 3214
2024-02-02 15:49:50,536 Epoch 3214: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 15:49:50,537 EPOCH 3215
2024-02-02 15:49:57,269 Epoch 3215: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 15:49:57,269 EPOCH 3216
2024-02-02 15:50:03,884 Epoch 3216: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:50:03,884 EPOCH 3217
2024-02-02 15:50:10,730 Epoch 3217: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:50:10,730 EPOCH 3218
2024-02-02 15:50:13,479 [Epoch: 3218 Step: 00054700] Batch Recognition Loss:   0.000129 => Gls Tokens per Sec:     2562 || Batch Translation Loss:   0.012298 => Txt Tokens per Sec:     7015 || Lr: 0.000050
2024-02-02 15:50:17,390 Epoch 3218: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:50:17,391 EPOCH 3219
2024-02-02 15:50:23,959 Epoch 3219: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 15:50:23,960 EPOCH 3220
2024-02-02 15:50:30,698 Epoch 3220: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 15:50:30,698 EPOCH 3221
2024-02-02 15:50:37,628 Epoch 3221: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:50:37,629 EPOCH 3222
2024-02-02 15:50:44,454 Epoch 3222: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 15:50:44,455 EPOCH 3223
2024-02-02 15:50:51,114 Epoch 3223: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-02 15:50:51,114 EPOCH 3224
2024-02-02 15:50:53,503 [Epoch: 3224 Step: 00054800] Batch Recognition Loss:   0.000108 => Gls Tokens per Sec:     2412 || Batch Translation Loss:   0.010171 => Txt Tokens per Sec:     6898 || Lr: 0.000050
2024-02-02 15:50:57,857 Epoch 3224: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-02 15:50:57,858 EPOCH 3225
2024-02-02 15:51:04,669 Epoch 3225: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-02 15:51:04,669 EPOCH 3226
2024-02-02 15:51:11,285 Epoch 3226: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 15:51:11,285 EPOCH 3227
2024-02-02 15:51:18,195 Epoch 3227: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 15:51:18,196 EPOCH 3228
2024-02-02 15:51:24,845 Epoch 3228: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.52 
2024-02-02 15:51:24,846 EPOCH 3229
2024-02-02 15:51:32,120 Epoch 3229: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.35 
2024-02-02 15:51:32,120 EPOCH 3230
2024-02-02 15:51:36,248 [Epoch: 3230 Step: 00054900] Batch Recognition Loss:   0.000651 => Gls Tokens per Sec:     1025 || Batch Translation Loss:   0.108817 => Txt Tokens per Sec:     2778 || Lr: 0.000050
2024-02-02 15:51:39,080 Epoch 3230: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.44 
2024-02-02 15:51:39,081 EPOCH 3231
2024-02-02 15:51:45,962 Epoch 3231: Total Training Recognition Loss 0.01  Total Training Translation Loss 5.58 
2024-02-02 15:51:45,962 EPOCH 3232
2024-02-02 15:51:52,721 Epoch 3232: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.38 
2024-02-02 15:51:52,721 EPOCH 3233
2024-02-02 15:51:59,126 Epoch 3233: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-02 15:51:59,127 EPOCH 3234
2024-02-02 15:52:05,957 Epoch 3234: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-02 15:52:05,957 EPOCH 3235
2024-02-02 15:52:12,443 Epoch 3235: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-02 15:52:12,443 EPOCH 3236
2024-02-02 15:52:13,729 [Epoch: 3236 Step: 00055000] Batch Recognition Loss:   0.000239 => Gls Tokens per Sec:     2490 || Batch Translation Loss:   0.025299 => Txt Tokens per Sec:     6846 || Lr: 0.000050
2024-02-02 15:52:19,105 Epoch 3236: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 15:52:19,106 EPOCH 3237
2024-02-02 15:52:25,842 Epoch 3237: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 15:52:25,843 EPOCH 3238
2024-02-02 15:52:32,801 Epoch 3238: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 15:52:32,802 EPOCH 3239
2024-02-02 15:52:39,544 Epoch 3239: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 15:52:39,545 EPOCH 3240
2024-02-02 15:52:46,442 Epoch 3240: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 15:52:46,442 EPOCH 3241
2024-02-02 15:52:53,173 Epoch 3241: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 15:52:53,174 EPOCH 3242
2024-02-02 15:52:53,697 [Epoch: 3242 Step: 00055100] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:     3685 || Batch Translation Loss:   0.013333 => Txt Tokens per Sec:     9282 || Lr: 0.000050
2024-02-02 15:52:59,728 Epoch 3242: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 15:52:59,728 EPOCH 3243
2024-02-02 15:53:06,222 Epoch 3243: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 15:53:06,223 EPOCH 3244
2024-02-02 15:53:13,154 Epoch 3244: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 15:53:13,155 EPOCH 3245
2024-02-02 15:53:19,874 Epoch 3245: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 15:53:19,875 EPOCH 3246
2024-02-02 15:53:26,685 Epoch 3246: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:53:26,686 EPOCH 3247
2024-02-02 15:53:33,293 Epoch 3247: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:53:33,294 EPOCH 3248
2024-02-02 15:53:33,401 [Epoch: 3248 Step: 00055200] Batch Recognition Loss:   0.000093 => Gls Tokens per Sec:     6038 || Batch Translation Loss:   0.007454 => Txt Tokens per Sec:     9283 || Lr: 0.000050
2024-02-02 15:53:39,983 Epoch 3248: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:53:39,984 EPOCH 3249
2024-02-02 15:53:46,793 Epoch 3249: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:53:46,794 EPOCH 3250
2024-02-02 15:53:52,743 Epoch 3250: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:53:52,743 EPOCH 3251
2024-02-02 15:53:59,522 Epoch 3251: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:53:59,522 EPOCH 3252
2024-02-02 15:54:06,080 Epoch 3252: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:54:06,081 EPOCH 3253
2024-02-02 15:54:12,474 [Epoch: 3253 Step: 00055300] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:     1563 || Batch Translation Loss:   0.012593 => Txt Tokens per Sec:     4416 || Lr: 0.000050
2024-02-02 15:54:12,604 Epoch 3253: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:54:12,605 EPOCH 3254
2024-02-02 15:54:19,013 Epoch 3254: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 15:54:19,013 EPOCH 3255
2024-02-02 15:54:25,838 Epoch 3255: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 15:54:25,838 EPOCH 3256
2024-02-02 15:54:32,633 Epoch 3256: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:54:32,633 EPOCH 3257
2024-02-02 15:54:39,618 Epoch 3257: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:54:39,619 EPOCH 3258
2024-02-02 15:54:46,519 Epoch 3258: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:54:46,520 EPOCH 3259
2024-02-02 15:54:49,998 [Epoch: 3259 Step: 00055400] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:     2577 || Batch Translation Loss:   0.026782 => Txt Tokens per Sec:     6948 || Lr: 0.000050
2024-02-02 15:54:53,155 Epoch 3259: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:54:53,155 EPOCH 3260
2024-02-02 15:54:59,886 Epoch 3260: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 15:54:59,886 EPOCH 3261
2024-02-02 15:55:05,851 Epoch 3261: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 15:55:05,852 EPOCH 3262
2024-02-02 15:55:12,692 Epoch 3262: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:55:12,693 EPOCH 3263
2024-02-02 15:55:19,601 Epoch 3263: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 15:55:19,601 EPOCH 3264
2024-02-02 15:55:26,470 Epoch 3264: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:55:26,471 EPOCH 3265
2024-02-02 15:55:31,635 [Epoch: 3265 Step: 00055500] Batch Recognition Loss:   0.000100 => Gls Tokens per Sec:     1439 || Batch Translation Loss:   0.009349 => Txt Tokens per Sec:     3801 || Lr: 0.000050
2024-02-02 15:55:33,274 Epoch 3265: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 15:55:33,274 EPOCH 3266
2024-02-02 15:55:40,065 Epoch 3266: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 15:55:40,065 EPOCH 3267
2024-02-02 15:55:46,710 Epoch 3267: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 15:55:46,711 EPOCH 3268
2024-02-02 15:55:53,661 Epoch 3268: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:55:53,662 EPOCH 3269
2024-02-02 15:56:00,532 Epoch 3269: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 15:56:00,533 EPOCH 3270
2024-02-02 15:56:07,480 Epoch 3270: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:56:07,481 EPOCH 3271
2024-02-02 15:56:12,142 [Epoch: 3271 Step: 00055600] Batch Recognition Loss:   0.000102 => Gls Tokens per Sec:     1320 || Batch Translation Loss:   0.008850 => Txt Tokens per Sec:     3497 || Lr: 0.000050
2024-02-02 15:56:14,417 Epoch 3271: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 15:56:14,417 EPOCH 3272
2024-02-02 15:56:21,097 Epoch 3272: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:56:21,098 EPOCH 3273
2024-02-02 15:56:27,880 Epoch 3273: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:56:27,881 EPOCH 3274
2024-02-02 15:56:34,703 Epoch 3274: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 15:56:34,704 EPOCH 3275
2024-02-02 15:56:41,246 Epoch 3275: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:56:41,247 EPOCH 3276
2024-02-02 15:56:47,966 Epoch 3276: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 15:56:47,967 EPOCH 3277
2024-02-02 15:56:50,212 [Epoch: 3277 Step: 00055700] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:     2282 || Batch Translation Loss:   0.016828 => Txt Tokens per Sec:     6201 || Lr: 0.000050
2024-02-02 15:56:54,553 Epoch 3277: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:56:54,554 EPOCH 3278
2024-02-02 15:57:01,465 Epoch 3278: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 15:57:01,465 EPOCH 3279
2024-02-02 15:57:08,262 Epoch 3279: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 15:57:08,262 EPOCH 3280
2024-02-02 15:57:14,876 Epoch 3280: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 15:57:14,876 EPOCH 3281
2024-02-02 15:57:21,487 Epoch 3281: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.67 
2024-02-02 15:57:21,487 EPOCH 3282
2024-02-02 15:57:28,324 Epoch 3282: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 15:57:28,325 EPOCH 3283
2024-02-02 15:57:29,231 [Epoch: 3283 Step: 00055800] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     4243 || Batch Translation Loss:   0.016067 => Txt Tokens per Sec:    10108 || Lr: 0.000050
2024-02-02 15:57:34,576 Epoch 3283: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-02 15:57:34,576 EPOCH 3284
2024-02-02 15:57:41,455 Epoch 3284: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 15:57:41,455 EPOCH 3285
2024-02-02 15:57:48,114 Epoch 3285: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 15:57:48,115 EPOCH 3286
2024-02-02 15:57:54,872 Epoch 3286: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 15:57:54,872 EPOCH 3287
2024-02-02 15:58:01,544 Epoch 3287: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 15:58:01,544 EPOCH 3288
2024-02-02 15:58:08,053 Epoch 3288: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 15:58:08,054 EPOCH 3289
2024-02-02 15:58:08,853 [Epoch: 3289 Step: 00055900] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:     3208 || Batch Translation Loss:   0.013378 => Txt Tokens per Sec:     8417 || Lr: 0.000050
2024-02-02 15:58:14,794 Epoch 3289: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 15:58:14,794 EPOCH 3290
2024-02-02 15:58:21,608 Epoch 3290: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 15:58:21,609 EPOCH 3291
2024-02-02 15:58:28,511 Epoch 3291: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 15:58:28,512 EPOCH 3292
2024-02-02 15:58:35,147 Epoch 3292: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-02 15:58:35,147 EPOCH 3293
2024-02-02 15:58:41,571 Epoch 3293: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-02 15:58:41,571 EPOCH 3294
2024-02-02 15:58:48,476 Epoch 3294: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-02 15:58:48,476 EPOCH 3295
2024-02-02 15:58:48,891 [Epoch: 3295 Step: 00056000] Batch Recognition Loss:   0.000219 => Gls Tokens per Sec:     3091 || Batch Translation Loss:   0.056737 => Txt Tokens per Sec:     8361 || Lr: 0.000050
2024-02-02 15:59:18,892 Validation result at epoch 3295, step    56000: duration: 30.0003s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00130	Translation Loss: 103407.50000	PPL: 31194.39844
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.30	(BLEU-1: 9.50,	BLEU-2: 2.47,	BLEU-3: 0.82,	BLEU-4: 0.30)
	CHRF 15.97	ROUGE 8.40
2024-02-02 15:59:18,893 Logging Recognition and Translation Outputs
2024-02-02 15:59:18,893 ========================================================================================================================
2024-02-02 15:59:18,893 Logging Sequence: 122_110.00
2024-02-02 15:59:18,893 	Gloss Reference :	A B+C+D+E
2024-02-02 15:59:18,893 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 15:59:18,894 	Gloss Alignment :	         
2024-02-02 15:59:18,894 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 15:59:18,895 	Text Reference  :	now that i achieved my  dream and secured a     silver medal
2024-02-02 15:59:18,895 	Text Hypothesis :	*** **** * ******** the bcci  is  very    proud of     this 
2024-02-02 15:59:18,895 	Text Alignment  :	D   D    D D        S   S     S   S       S     S      S    
2024-02-02 15:59:18,895 ========================================================================================================================
2024-02-02 15:59:18,895 Logging Sequence: 161_111.00
2024-02-02 15:59:18,896 	Gloss Reference :	A B+C+D+E
2024-02-02 15:59:18,896 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 15:59:18,896 	Gloss Alignment :	         
2024-02-02 15:59:18,896 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 15:59:18,898 	Text Reference  :	his last game as   captain was the      cape town     test  in south africa  in  jan 2022   
2024-02-02 15:59:18,898 	Text Hypothesis :	*** the  bcci then on      8th december 2021 replaced kohli as odi   captain and was playing
2024-02-02 15:59:18,898 	Text Alignment  :	D   S    S    S    S       S   S        S    S        S     S  S     S       S   S   S      
2024-02-02 15:59:18,898 ========================================================================================================================
2024-02-02 15:59:18,898 Logging Sequence: 136_79.00
2024-02-02 15:59:18,898 	Gloss Reference :	A B+C+D+E
2024-02-02 15:59:18,898 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 15:59:18,899 	Gloss Alignment :	         
2024-02-02 15:59:18,899 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 15:59:18,900 	Text Reference  :	with this win sindhu became the first indian woman  to       win  two individual olympic medals     
2024-02-02 15:59:18,900 	Text Hypothesis :	**** **** *** ****** ****** *** there are    strict measures were in  place      in      afghanistan
2024-02-02 15:59:18,900 	Text Alignment  :	D    D    D   D      D      D   S     S      S      S        S    S   S          S       S          
2024-02-02 15:59:18,900 ========================================================================================================================
2024-02-02 15:59:18,900 Logging Sequence: 166_335.00
2024-02-02 15:59:18,900 	Gloss Reference :	A B+C+D+E
2024-02-02 15:59:18,900 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 15:59:18,901 	Gloss Alignment :	         
2024-02-02 15:59:18,901 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 15:59:18,902 	Text Reference  :	the second world test championship is    scheduled    from june   2021       to 30     april 2023 
2024-02-02 15:59:18,902 	Text Hypothesis :	*** ****** ***** **** wearing      masks sanitisation and  social distancing to reduce the   cases
2024-02-02 15:59:18,902 	Text Alignment  :	D   D      D     D    S            S     S            S    S      S             S      S     S    
2024-02-02 15:59:18,902 ========================================================================================================================
2024-02-02 15:59:18,902 Logging Sequence: 95_152.00
2024-02-02 15:59:18,902 	Gloss Reference :	A B+C+D+E
2024-02-02 15:59:18,902 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 15:59:18,902 	Gloss Alignment :	         
2024-02-02 15:59:18,903 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 15:59:18,903 	Text Reference  :	**** ***** *** how    strange
2024-02-02 15:59:18,903 	Text Hypothesis :	what about the indian famous 
2024-02-02 15:59:18,903 	Text Alignment  :	I    I     I   S      S      
2024-02-02 15:59:18,903 ========================================================================================================================
2024-02-02 15:59:25,393 Epoch 3295: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-02 15:59:25,394 EPOCH 3296
2024-02-02 15:59:32,352 Epoch 3296: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 15:59:32,352 EPOCH 3297
2024-02-02 15:59:39,012 Epoch 3297: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 15:59:39,013 EPOCH 3298
2024-02-02 15:59:45,680 Epoch 3298: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 15:59:45,680 EPOCH 3299
2024-02-02 15:59:52,301 Epoch 3299: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 15:59:52,301 EPOCH 3300
2024-02-02 15:59:59,005 [Epoch: 3300 Step: 00056100] Batch Recognition Loss:   0.000192 => Gls Tokens per Sec:     1586 || Batch Translation Loss:   0.016631 => Txt Tokens per Sec:     4403 || Lr: 0.000050
2024-02-02 15:59:59,006 Epoch 3300: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 15:59:59,006 EPOCH 3301
2024-02-02 16:00:05,991 Epoch 3301: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 16:00:05,992 EPOCH 3302
2024-02-02 16:00:13,016 Epoch 3302: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 16:00:13,017 EPOCH 3303
2024-02-02 16:00:19,774 Epoch 3303: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 16:00:19,775 EPOCH 3304
2024-02-02 16:00:26,678 Epoch 3304: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 16:00:26,678 EPOCH 3305
2024-02-02 16:00:33,007 Epoch 3305: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 16:00:33,007 EPOCH 3306
2024-02-02 16:00:39,533 [Epoch: 3306 Step: 00056200] Batch Recognition Loss:   0.000163 => Gls Tokens per Sec:     1433 || Batch Translation Loss:   0.014372 => Txt Tokens per Sec:     4098 || Lr: 0.000050
2024-02-02 16:00:40,015 Epoch 3306: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 16:00:40,016 EPOCH 3307
2024-02-02 16:00:46,874 Epoch 3307: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 16:00:46,875 EPOCH 3308
2024-02-02 16:00:53,425 Epoch 3308: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 16:00:53,426 EPOCH 3309
2024-02-02 16:01:00,132 Epoch 3309: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 16:01:00,132 EPOCH 3310
2024-02-02 16:01:06,749 Epoch 3310: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 16:01:06,750 EPOCH 3311
2024-02-02 16:01:13,731 Epoch 3311: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 16:01:13,732 EPOCH 3312
2024-02-02 16:01:19,618 [Epoch: 3312 Step: 00056300] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     1371 || Batch Translation Loss:   0.038416 => Txt Tokens per Sec:     3862 || Lr: 0.000050
2024-02-02 16:01:20,563 Epoch 3312: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 16:01:20,563 EPOCH 3313
2024-02-02 16:01:27,483 Epoch 3313: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 16:01:27,484 EPOCH 3314
2024-02-02 16:01:34,131 Epoch 3314: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-02 16:01:34,132 EPOCH 3315
2024-02-02 16:01:40,758 Epoch 3315: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 16:01:40,758 EPOCH 3316
2024-02-02 16:01:47,673 Epoch 3316: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 16:01:47,673 EPOCH 3317
2024-02-02 16:01:54,388 Epoch 3317: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 16:01:54,389 EPOCH 3318
2024-02-02 16:01:59,577 [Epoch: 3318 Step: 00056400] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     1309 || Batch Translation Loss:   0.018320 => Txt Tokens per Sec:     3652 || Lr: 0.000050
2024-02-02 16:02:01,278 Epoch 3318: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 16:02:01,278 EPOCH 3319
2024-02-02 16:02:07,795 Epoch 3319: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 16:02:07,796 EPOCH 3320
2024-02-02 16:02:14,672 Epoch 3320: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 16:02:14,672 EPOCH 3321
2024-02-02 16:02:21,483 Epoch 3321: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 16:02:21,483 EPOCH 3322
2024-02-02 16:02:28,116 Epoch 3322: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 16:02:28,117 EPOCH 3323
2024-02-02 16:02:34,797 Epoch 3323: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 16:02:34,798 EPOCH 3324
2024-02-02 16:02:39,649 [Epoch: 3324 Step: 00056500] Batch Recognition Loss:   0.000099 => Gls Tokens per Sec:     1136 || Batch Translation Loss:   0.010730 => Txt Tokens per Sec:     3259 || Lr: 0.000050
2024-02-02 16:02:41,915 Epoch 3324: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 16:02:41,915 EPOCH 3325
2024-02-02 16:02:48,593 Epoch 3325: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 16:02:48,594 EPOCH 3326
2024-02-02 16:02:55,181 Epoch 3326: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 16:02:55,182 EPOCH 3327
2024-02-02 16:03:01,604 Epoch 3327: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 16:03:01,605 EPOCH 3328
2024-02-02 16:03:08,539 Epoch 3328: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 16:03:08,540 EPOCH 3329
2024-02-02 16:03:15,260 Epoch 3329: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 16:03:15,261 EPOCH 3330
2024-02-02 16:03:17,103 [Epoch: 3330 Step: 00056600] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:     2433 || Batch Translation Loss:   0.016166 => Txt Tokens per Sec:     6791 || Lr: 0.000050
2024-02-02 16:03:21,528 Epoch 3330: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 16:03:21,528 EPOCH 3331
2024-02-02 16:03:28,688 Epoch 3331: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 16:03:28,688 EPOCH 3332
2024-02-02 16:03:35,071 Epoch 3332: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 16:03:35,072 EPOCH 3333
2024-02-02 16:03:41,524 Epoch 3333: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 16:03:41,525 EPOCH 3334
2024-02-02 16:03:48,417 Epoch 3334: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 16:03:48,417 EPOCH 3335
2024-02-02 16:03:54,983 Epoch 3335: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 16:03:54,983 EPOCH 3336
2024-02-02 16:03:55,870 [Epoch: 3336 Step: 00056700] Batch Recognition Loss:   0.000096 => Gls Tokens per Sec:     3614 || Batch Translation Loss:   0.008294 => Txt Tokens per Sec:     8884 || Lr: 0.000050
2024-02-02 16:04:01,534 Epoch 3336: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 16:04:01,534 EPOCH 3337
2024-02-02 16:04:08,334 Epoch 3337: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 16:04:08,335 EPOCH 3338
2024-02-02 16:04:15,014 Epoch 3338: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 16:04:15,015 EPOCH 3339
2024-02-02 16:04:21,382 Epoch 3339: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 16:04:21,382 EPOCH 3340
2024-02-02 16:04:27,573 Epoch 3340: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 16:04:27,574 EPOCH 3341
2024-02-02 16:04:34,560 Epoch 3341: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 16:04:34,560 EPOCH 3342
2024-02-02 16:04:35,580 [Epoch: 3342 Step: 00056800] Batch Recognition Loss:   0.000090 => Gls Tokens per Sec:     1883 || Batch Translation Loss:   0.008035 => Txt Tokens per Sec:     5482 || Lr: 0.000050
2024-02-02 16:04:41,553 Epoch 3342: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-02 16:04:41,554 EPOCH 3343
2024-02-02 16:04:48,431 Epoch 3343: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 16:04:48,432 EPOCH 3344
2024-02-02 16:04:55,181 Epoch 3344: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 16:04:55,181 EPOCH 3345
2024-02-02 16:05:02,143 Epoch 3345: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 16:05:02,143 EPOCH 3346
2024-02-02 16:05:08,954 Epoch 3346: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-02 16:05:08,955 EPOCH 3347
2024-02-02 16:05:15,722 Epoch 3347: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-02 16:05:15,723 EPOCH 3348
2024-02-02 16:05:15,976 [Epoch: 3348 Step: 00056900] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     2540 || Batch Translation Loss:   0.022999 => Txt Tokens per Sec:     7710 || Lr: 0.000050
2024-02-02 16:05:22,703 Epoch 3348: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-02 16:05:22,704 EPOCH 3349
2024-02-02 16:05:29,397 Epoch 3349: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-02 16:05:29,398 EPOCH 3350
2024-02-02 16:05:36,098 Epoch 3350: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-02 16:05:36,098 EPOCH 3351
2024-02-02 16:05:43,071 Epoch 3351: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 16:05:43,072 EPOCH 3352
2024-02-02 16:05:50,016 Epoch 3352: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 16:05:50,017 EPOCH 3353
2024-02-02 16:05:56,095 [Epoch: 3353 Step: 00057000] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:     1644 || Batch Translation Loss:   0.017561 => Txt Tokens per Sec:     4485 || Lr: 0.000050
2024-02-02 16:05:56,666 Epoch 3353: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 16:05:56,667 EPOCH 3354
2024-02-02 16:06:03,586 Epoch 3354: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 16:06:03,586 EPOCH 3355
2024-02-02 16:06:10,379 Epoch 3355: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 16:06:10,379 EPOCH 3356
2024-02-02 16:06:17,462 Epoch 3356: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 16:06:17,462 EPOCH 3357
2024-02-02 16:06:25,173 Epoch 3357: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-02 16:06:25,173 EPOCH 3358
2024-02-02 16:06:31,499 Epoch 3358: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 16:06:31,499 EPOCH 3359
2024-02-02 16:06:37,396 [Epoch: 3359 Step: 00057100] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     1478 || Batch Translation Loss:   0.029013 => Txt Tokens per Sec:     4053 || Lr: 0.000050
2024-02-02 16:06:38,229 Epoch 3359: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 16:06:38,230 EPOCH 3360
2024-02-02 16:06:45,156 Epoch 3360: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 16:06:45,157 EPOCH 3361
2024-02-02 16:06:51,796 Epoch 3361: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-02 16:06:51,796 EPOCH 3362
2024-02-02 16:06:58,578 Epoch 3362: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 16:06:58,579 EPOCH 3363
2024-02-02 16:07:05,408 Epoch 3363: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-02 16:07:05,408 EPOCH 3364
2024-02-02 16:07:12,232 Epoch 3364: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 16:07:12,233 EPOCH 3365
2024-02-02 16:07:17,492 [Epoch: 3365 Step: 00057200] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:     1413 || Batch Translation Loss:   0.028581 => Txt Tokens per Sec:     3855 || Lr: 0.000050
2024-02-02 16:07:19,010 Epoch 3365: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 16:07:19,010 EPOCH 3366
2024-02-02 16:07:25,822 Epoch 3366: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-02 16:07:25,823 EPOCH 3367
2024-02-02 16:07:32,197 Epoch 3367: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-02 16:07:32,197 EPOCH 3368
2024-02-02 16:07:38,927 Epoch 3368: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 16:07:38,927 EPOCH 3369
2024-02-02 16:07:45,989 Epoch 3369: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-02 16:07:45,990 EPOCH 3370
2024-02-02 16:07:52,732 Epoch 3370: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-02 16:07:52,733 EPOCH 3371
2024-02-02 16:07:57,436 [Epoch: 3371 Step: 00057300] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     1308 || Batch Translation Loss:   0.017386 => Txt Tokens per Sec:     3588 || Lr: 0.000050
2024-02-02 16:07:59,372 Epoch 3371: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 16:07:59,373 EPOCH 3372
2024-02-02 16:08:05,908 Epoch 3372: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 16:08:05,908 EPOCH 3373
2024-02-02 16:08:12,717 Epoch 3373: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 16:08:12,718 EPOCH 3374
2024-02-02 16:08:19,603 Epoch 3374: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 16:08:19,604 EPOCH 3375
2024-02-02 16:08:26,178 Epoch 3375: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 16:08:26,178 EPOCH 3376
2024-02-02 16:08:33,235 Epoch 3376: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 16:08:33,236 EPOCH 3377
2024-02-02 16:08:38,047 [Epoch: 3377 Step: 00057400] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:     1012 || Batch Translation Loss:   0.041965 => Txt Tokens per Sec:     2982 || Lr: 0.000050
2024-02-02 16:08:40,136 Epoch 3377: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 16:08:40,136 EPOCH 3378
2024-02-02 16:08:46,943 Epoch 3378: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 16:08:46,944 EPOCH 3379
2024-02-02 16:08:53,330 Epoch 3379: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 16:08:53,331 EPOCH 3380
2024-02-02 16:09:00,131 Epoch 3380: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 16:09:00,132 EPOCH 3381
2024-02-02 16:09:06,907 Epoch 3381: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 16:09:06,907 EPOCH 3382
2024-02-02 16:09:13,719 Epoch 3382: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 16:09:13,719 EPOCH 3383
2024-02-02 16:09:15,112 [Epoch: 3383 Step: 00057500] Batch Recognition Loss:   0.000090 => Gls Tokens per Sec:     2761 || Batch Translation Loss:   0.007513 => Txt Tokens per Sec:     7615 || Lr: 0.000050
2024-02-02 16:09:20,338 Epoch 3383: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 16:09:20,339 EPOCH 3384
2024-02-02 16:09:26,990 Epoch 3384: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 16:09:26,990 EPOCH 3385
2024-02-02 16:09:33,559 Epoch 3385: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 16:09:33,560 EPOCH 3386
2024-02-02 16:09:40,485 Epoch 3386: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 16:09:40,485 EPOCH 3387
2024-02-02 16:09:47,238 Epoch 3387: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 16:09:47,238 EPOCH 3388
2024-02-02 16:09:53,867 Epoch 3388: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 16:09:53,868 EPOCH 3389
2024-02-02 16:09:54,974 [Epoch: 3389 Step: 00057600] Batch Recognition Loss:   0.000131 => Gls Tokens per Sec:     2319 || Batch Translation Loss:   0.013167 => Txt Tokens per Sec:     6303 || Lr: 0.000050
2024-02-02 16:10:00,619 Epoch 3389: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 16:10:00,620 EPOCH 3390
2024-02-02 16:10:07,452 Epoch 3390: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 16:10:07,452 EPOCH 3391
2024-02-02 16:10:14,193 Epoch 3391: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 16:10:14,194 EPOCH 3392
2024-02-02 16:10:20,820 Epoch 3392: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 16:10:20,820 EPOCH 3393
2024-02-02 16:10:27,276 Epoch 3393: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 16:10:27,276 EPOCH 3394
2024-02-02 16:10:33,943 Epoch 3394: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 16:10:33,944 EPOCH 3395
2024-02-02 16:10:34,491 [Epoch: 3395 Step: 00057700] Batch Recognition Loss:   0.000108 => Gls Tokens per Sec:     2340 || Batch Translation Loss:   0.011517 => Txt Tokens per Sec:     6525 || Lr: 0.000050
2024-02-02 16:10:40,776 Epoch 3395: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 16:10:40,776 EPOCH 3396
2024-02-02 16:10:47,358 Epoch 3396: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 16:10:47,359 EPOCH 3397
2024-02-02 16:10:54,337 Epoch 3397: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 16:10:54,338 EPOCH 3398
2024-02-02 16:11:01,257 Epoch 3398: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 16:11:01,258 EPOCH 3399
2024-02-02 16:11:07,839 Epoch 3399: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 16:11:07,840 EPOCH 3400
2024-02-02 16:11:14,860 [Epoch: 3400 Step: 00057800] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     1515 || Batch Translation Loss:   0.016288 => Txt Tokens per Sec:     4205 || Lr: 0.000050
2024-02-02 16:11:14,860 Epoch 3400: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 16:11:14,861 EPOCH 3401
2024-02-02 16:11:21,668 Epoch 3401: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 16:11:21,668 EPOCH 3402
2024-02-02 16:11:27,776 Epoch 3402: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 16:11:27,776 EPOCH 3403
2024-02-02 16:11:34,156 Epoch 3403: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 16:11:34,157 EPOCH 3404
2024-02-02 16:11:40,830 Epoch 3404: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 16:11:40,830 EPOCH 3405
2024-02-02 16:11:47,652 Epoch 3405: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 16:11:47,652 EPOCH 3406
2024-02-02 16:11:54,248 [Epoch: 3406 Step: 00057900] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     1418 || Batch Translation Loss:   0.020114 => Txt Tokens per Sec:     4128 || Lr: 0.000050
2024-02-02 16:11:54,480 Epoch 3406: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 16:11:54,480 EPOCH 3407
2024-02-02 16:12:01,288 Epoch 3407: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 16:12:01,288 EPOCH 3408
2024-02-02 16:12:08,126 Epoch 3408: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 16:12:08,126 EPOCH 3409
2024-02-02 16:12:14,395 Epoch 3409: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 16:12:14,396 EPOCH 3410
2024-02-02 16:12:20,983 Epoch 3410: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 16:12:20,983 EPOCH 3411
2024-02-02 16:12:27,957 Epoch 3411: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 16:12:27,958 EPOCH 3412
2024-02-02 16:12:33,221 [Epoch: 3412 Step: 00058000] Batch Recognition Loss:   0.000266 => Gls Tokens per Sec:     1534 || Batch Translation Loss:   0.032563 => Txt Tokens per Sec:     4271 || Lr: 0.000050
2024-02-02 16:13:03,330 Validation result at epoch 3412, step    58000: duration: 30.1073s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00141	Translation Loss: 105345.74219	PPL: 37871.49609
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.63	(BLEU-1: 9.90,	BLEU-2: 3.00,	BLEU-3: 1.23,	BLEU-4: 0.63)
	CHRF 16.62	ROUGE 8.54
2024-02-02 16:13:03,331 Logging Recognition and Translation Outputs
2024-02-02 16:13:03,331 ========================================================================================================================
2024-02-02 16:13:03,332 Logging Sequence: 180_138.00
2024-02-02 16:13:03,332 	Gloss Reference :	A B+C+D+E
2024-02-02 16:13:03,332 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 16:13:03,333 	Gloss Alignment :	         
2024-02-02 16:13:03,333 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 16:13:03,335 	Text Reference  :	ioa president p t usha constituted a    seven-member panel which included world   champions from various  sports to  inquire  into       the    allegations
2024-02-02 16:13:03,335 	Text Hypothesis :	*** ********* * * they also        said that         he    loves playing  cricket on        26th december 2021   for sexually assaulting female wrestlers  
2024-02-02 16:13:03,335 	Text Alignment  :	D   D         D D S    S           S    S            S     S     S        S       S         S    S        S      S   S        S          S      S          
2024-02-02 16:13:03,335 ========================================================================================================================
2024-02-02 16:13:03,335 Logging Sequence: 128_189.00
2024-02-02 16:13:03,336 	Gloss Reference :	A B+C+D+E
2024-02-02 16:13:03,336 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 16:13:03,336 	Gloss Alignment :	         
2024-02-02 16:13:03,336 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 16:13:03,337 	Text Reference  :	** *** ******* ** ****** meanwhile some funny incidents happened during the ***** match
2024-02-02 16:13:03,337 	Text Hypothesis :	on the streets of london let       me   tell  you       be       held   the first time 
2024-02-02 16:13:03,337 	Text Alignment  :	I  I   I       I  I      S         S    S     S         S        S          I     S    
2024-02-02 16:13:03,337 ========================================================================================================================
2024-02-02 16:13:03,337 Logging Sequence: 165_523.00
2024-02-02 16:13:03,338 	Gloss Reference :	A B+C+D+E
2024-02-02 16:13:03,338 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 16:13:03,338 	Gloss Alignment :	         
2024-02-02 16:13:03,338 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 16:13:03,339 	Text Reference  :	as he believed that his  team might lose if         he takes off his batting pads
2024-02-02 16:13:03,339 	Text Hypothesis :	** it was      a    good luck for   the  tournament he ***** *** *** may     2023
2024-02-02 16:13:03,339 	Text Alignment  :	D  S  S        S    S    S    S     S    S             D     D   D   S       S   
2024-02-02 16:13:03,340 ========================================================================================================================
2024-02-02 16:13:03,340 Logging Sequence: 145_168.00
2024-02-02 16:13:03,340 	Gloss Reference :	A B+C+D+E
2024-02-02 16:13:03,340 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 16:13:03,340 	Gloss Alignment :	         
2024-02-02 16:13:03,340 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 16:13:03,341 	Text Reference  :	**** ** **** the decision has   devastated sameeha and  her    parents
2024-02-02 16:13:03,341 	Text Hypothesis :	this is that the ******** first age        of      test series poland 
2024-02-02 16:13:03,341 	Text Alignment  :	I    I  I        D        S     S          S       S    S      S      
2024-02-02 16:13:03,341 ========================================================================================================================
2024-02-02 16:13:03,342 Logging Sequence: 92_123.00
2024-02-02 16:13:03,342 	Gloss Reference :	A B+C+D+E
2024-02-02 16:13:03,342 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 16:13:03,342 	Gloss Alignment :	         
2024-02-02 16:13:03,342 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 16:13:03,344 	Text Reference  :	a heated argument also took place  between members of   the family and  the two  men 
2024-02-02 16:13:03,344 	Text Hypothesis :	* ****** pakistan news that sushil kumar   was     from the ****** most hit very well
2024-02-02 16:13:03,344 	Text Alignment  :	D D      S        S    S    S      S       S       S        D      S    S   S    S   
2024-02-02 16:13:03,344 ========================================================================================================================
2024-02-02 16:13:04,494 Epoch 3412: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 16:13:04,494 EPOCH 3413
2024-02-02 16:13:11,278 Epoch 3413: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-02 16:13:11,278 EPOCH 3414
2024-02-02 16:13:17,964 Epoch 3414: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-02 16:13:17,965 EPOCH 3415
2024-02-02 16:13:24,819 Epoch 3415: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-02 16:13:24,820 EPOCH 3416
2024-02-02 16:13:31,079 Epoch 3416: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.71 
2024-02-02 16:13:31,079 EPOCH 3417
2024-02-02 16:13:37,497 Epoch 3417: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-02 16:13:37,497 EPOCH 3418
2024-02-02 16:13:42,401 [Epoch: 3418 Step: 00058100] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     1385 || Batch Translation Loss:   0.035727 => Txt Tokens per Sec:     3857 || Lr: 0.000050
2024-02-02 16:13:44,282 Epoch 3418: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.57 
2024-02-02 16:13:44,283 EPOCH 3419
2024-02-02 16:13:50,867 Epoch 3419: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-02 16:13:50,867 EPOCH 3420
2024-02-02 16:13:57,388 Epoch 3420: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 16:13:57,388 EPOCH 3421
2024-02-02 16:14:04,228 Epoch 3421: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 16:14:04,229 EPOCH 3422
2024-02-02 16:14:10,568 Epoch 3422: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 16:14:10,569 EPOCH 3423
2024-02-02 16:14:17,319 Epoch 3423: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 16:14:17,319 EPOCH 3424
2024-02-02 16:14:21,677 [Epoch: 3424 Step: 00058200] Batch Recognition Loss:   0.000112 => Gls Tokens per Sec:     1265 || Batch Translation Loss:   0.026386 => Txt Tokens per Sec:     3465 || Lr: 0.000050
2024-02-02 16:14:24,166 Epoch 3424: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 16:14:24,166 EPOCH 3425
2024-02-02 16:14:30,962 Epoch 3425: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 16:14:30,963 EPOCH 3426
2024-02-02 16:14:37,692 Epoch 3426: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 16:14:37,693 EPOCH 3427
2024-02-02 16:14:44,000 Epoch 3427: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 16:14:44,001 EPOCH 3428
2024-02-02 16:14:50,831 Epoch 3428: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 16:14:50,831 EPOCH 3429
2024-02-02 16:14:57,663 Epoch 3429: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 16:14:57,663 EPOCH 3430
2024-02-02 16:15:01,411 [Epoch: 3430 Step: 00058300] Batch Recognition Loss:   0.000089 => Gls Tokens per Sec:     1129 || Batch Translation Loss:   0.007651 => Txt Tokens per Sec:     3068 || Lr: 0.000050
2024-02-02 16:15:04,297 Epoch 3430: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 16:15:04,297 EPOCH 3431
2024-02-02 16:15:10,992 Epoch 3431: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 16:15:10,993 EPOCH 3432
2024-02-02 16:15:17,590 Epoch 3432: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 16:15:17,590 EPOCH 3433
2024-02-02 16:15:24,447 Epoch 3433: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 16:15:24,448 EPOCH 3434
2024-02-02 16:15:31,213 Epoch 3434: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 16:15:31,213 EPOCH 3435
2024-02-02 16:15:37,770 Epoch 3435: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 16:15:37,771 EPOCH 3436
2024-02-02 16:15:39,315 [Epoch: 3436 Step: 00058400] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     2076 || Batch Translation Loss:   0.022400 => Txt Tokens per Sec:     5820 || Lr: 0.000050
2024-02-02 16:15:44,792 Epoch 3436: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 16:15:44,793 EPOCH 3437
2024-02-02 16:15:51,489 Epoch 3437: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 16:15:51,489 EPOCH 3438
2024-02-02 16:15:58,215 Epoch 3438: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 16:15:58,215 EPOCH 3439
2024-02-02 16:16:04,986 Epoch 3439: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 16:16:04,986 EPOCH 3440
2024-02-02 16:16:11,555 Epoch 3440: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 16:16:11,556 EPOCH 3441
2024-02-02 16:16:18,106 Epoch 3441: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 16:16:18,106 EPOCH 3442
2024-02-02 16:16:18,876 [Epoch: 3442 Step: 00058500] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:     2497 || Batch Translation Loss:   0.023418 => Txt Tokens per Sec:     7464 || Lr: 0.000050
2024-02-02 16:16:24,660 Epoch 3442: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 16:16:24,660 EPOCH 3443
2024-02-02 16:16:31,649 Epoch 3443: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 16:16:31,649 EPOCH 3444
2024-02-02 16:16:38,581 Epoch 3444: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 16:16:38,581 EPOCH 3445
2024-02-02 16:16:45,226 Epoch 3445: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 16:16:45,226 EPOCH 3446
2024-02-02 16:16:51,974 Epoch 3446: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 16:16:51,975 EPOCH 3447
2024-02-02 16:16:58,793 Epoch 3447: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 16:16:58,793 EPOCH 3448
2024-02-02 16:16:59,230 [Epoch: 3448 Step: 00058600] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:     1470 || Batch Translation Loss:   0.032597 => Txt Tokens per Sec:     4612 || Lr: 0.000050
2024-02-02 16:17:05,778 Epoch 3448: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 16:17:05,779 EPOCH 3449
2024-02-02 16:17:12,032 Epoch 3449: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 16:17:12,033 EPOCH 3450
2024-02-02 16:17:18,726 Epoch 3450: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 16:17:18,726 EPOCH 3451
2024-02-02 16:17:25,505 Epoch 3451: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 16:17:25,506 EPOCH 3452
2024-02-02 16:17:32,323 Epoch 3452: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 16:17:32,324 EPOCH 3453
2024-02-02 16:17:38,875 [Epoch: 3453 Step: 00058700] Batch Recognition Loss:   0.000115 => Gls Tokens per Sec:     1525 || Batch Translation Loss:   0.013180 => Txt Tokens per Sec:     4228 || Lr: 0.000050
2024-02-02 16:17:39,090 Epoch 3453: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 16:17:39,090 EPOCH 3454
2024-02-02 16:17:45,723 Epoch 3454: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 16:17:45,723 EPOCH 3455
2024-02-02 16:17:52,577 Epoch 3455: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 16:17:52,577 EPOCH 3456
2024-02-02 16:17:59,337 Epoch 3456: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 16:17:59,338 EPOCH 3457
2024-02-02 16:18:05,858 Epoch 3457: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 16:18:05,858 EPOCH 3458
2024-02-02 16:18:12,566 Epoch 3458: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 16:18:12,566 EPOCH 3459
2024-02-02 16:18:18,469 [Epoch: 3459 Step: 00058800] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:     1476 || Batch Translation Loss:   0.033502 => Txt Tokens per Sec:     4148 || Lr: 0.000050
2024-02-02 16:18:19,295 Epoch 3459: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 16:18:19,296 EPOCH 3460
2024-02-02 16:18:26,006 Epoch 3460: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 16:18:26,007 EPOCH 3461
2024-02-02 16:18:32,062 Epoch 3461: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 16:18:32,063 EPOCH 3462
2024-02-02 16:18:38,400 Epoch 3462: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 16:18:38,401 EPOCH 3463
2024-02-02 16:18:45,284 Epoch 3463: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-02 16:18:45,285 EPOCH 3464
2024-02-02 16:18:52,088 Epoch 3464: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 16:18:52,089 EPOCH 3465
2024-02-02 16:18:55,154 [Epoch: 3465 Step: 00058900] Batch Recognition Loss:   0.000313 => Gls Tokens per Sec:     2506 || Batch Translation Loss:   0.026167 => Txt Tokens per Sec:     6671 || Lr: 0.000050
2024-02-02 16:18:58,665 Epoch 3465: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-02 16:18:58,665 EPOCH 3466
2024-02-02 16:19:05,499 Epoch 3466: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 16:19:05,499 EPOCH 3467
2024-02-02 16:19:12,263 Epoch 3467: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 16:19:12,263 EPOCH 3468
2024-02-02 16:19:19,087 Epoch 3468: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 16:19:19,087 EPOCH 3469
2024-02-02 16:19:25,386 Epoch 3469: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 16:19:25,387 EPOCH 3470
2024-02-02 16:19:32,151 Epoch 3470: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 16:19:32,152 EPOCH 3471
2024-02-02 16:19:34,357 [Epoch: 3471 Step: 00059000] Batch Recognition Loss:   0.000115 => Gls Tokens per Sec:     2902 || Batch Translation Loss:   0.013544 => Txt Tokens per Sec:     7553 || Lr: 0.000050
2024-02-02 16:19:39,000 Epoch 3471: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 16:19:39,000 EPOCH 3472
2024-02-02 16:19:45,721 Epoch 3472: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 16:19:45,722 EPOCH 3473
2024-02-02 16:19:52,470 Epoch 3473: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 16:19:52,471 EPOCH 3474
2024-02-02 16:19:58,935 Epoch 3474: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 16:19:58,935 EPOCH 3475
2024-02-02 16:20:05,909 Epoch 3475: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 16:20:05,909 EPOCH 3476
2024-02-02 16:20:12,550 Epoch 3476: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-02 16:20:12,551 EPOCH 3477
2024-02-02 16:20:14,851 [Epoch: 3477 Step: 00059100] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     2226 || Batch Translation Loss:   0.024184 => Txt Tokens per Sec:     6064 || Lr: 0.000050
2024-02-02 16:20:19,381 Epoch 3477: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 16:20:19,381 EPOCH 3478
2024-02-02 16:20:26,259 Epoch 3478: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 16:20:26,260 EPOCH 3479
2024-02-02 16:20:33,099 Epoch 3479: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 16:20:33,099 EPOCH 3480
2024-02-02 16:20:39,373 Epoch 3480: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 16:20:39,374 EPOCH 3481
2024-02-02 16:20:46,148 Epoch 3481: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 16:20:46,148 EPOCH 3482
2024-02-02 16:20:52,435 Epoch 3482: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 16:20:52,435 EPOCH 3483
2024-02-02 16:20:53,843 [Epoch: 3483 Step: 00059200] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:     2730 || Batch Translation Loss:   0.027570 => Txt Tokens per Sec:     7596 || Lr: 0.000050
2024-02-02 16:20:58,802 Epoch 3483: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 16:20:58,803 EPOCH 3484
2024-02-02 16:21:05,618 Epoch 3484: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 16:21:05,618 EPOCH 3485
2024-02-02 16:21:12,418 Epoch 3485: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 16:21:12,419 EPOCH 3486
2024-02-02 16:21:18,803 Epoch 3486: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 16:21:18,803 EPOCH 3487
2024-02-02 16:21:25,774 Epoch 3487: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 16:21:25,775 EPOCH 3488
2024-02-02 16:21:32,617 Epoch 3488: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 16:21:32,618 EPOCH 3489
2024-02-02 16:21:33,734 [Epoch: 3489 Step: 00059300] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:     2296 || Batch Translation Loss:   0.025773 => Txt Tokens per Sec:     6650 || Lr: 0.000050
2024-02-02 16:21:39,430 Epoch 3489: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 16:21:39,431 EPOCH 3490
2024-02-02 16:21:46,290 Epoch 3490: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 16:21:46,291 EPOCH 3491
2024-02-02 16:21:53,149 Epoch 3491: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 16:21:53,150 EPOCH 3492
2024-02-02 16:21:59,900 Epoch 3492: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 16:21:59,901 EPOCH 3493
2024-02-02 16:22:06,499 Epoch 3493: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 16:22:06,500 EPOCH 3494
2024-02-02 16:22:13,561 Epoch 3494: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 16:22:13,562 EPOCH 3495
2024-02-02 16:22:13,956 [Epoch: 3495 Step: 00059400] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:     3260 || Batch Translation Loss:   0.013973 => Txt Tokens per Sec:     8814 || Lr: 0.000050
2024-02-02 16:22:20,114 Epoch 3495: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 16:22:20,115 EPOCH 3496
2024-02-02 16:22:27,067 Epoch 3496: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 16:22:27,068 EPOCH 3497
2024-02-02 16:22:33,854 Epoch 3497: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 16:22:33,854 EPOCH 3498
2024-02-02 16:22:40,728 Epoch 3498: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 16:22:40,728 EPOCH 3499
2024-02-02 16:22:47,450 Epoch 3499: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 16:22:47,450 EPOCH 3500
2024-02-02 16:22:54,341 [Epoch: 3500 Step: 00059500] Batch Recognition Loss:   0.000116 => Gls Tokens per Sec:     1543 || Batch Translation Loss:   0.010358 => Txt Tokens per Sec:     4283 || Lr: 0.000050
2024-02-02 16:22:54,342 Epoch 3500: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 16:22:54,342 EPOCH 3501
2024-02-02 16:23:01,140 Epoch 3501: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 16:23:01,141 EPOCH 3502
2024-02-02 16:23:07,807 Epoch 3502: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 16:23:07,808 EPOCH 3503
2024-02-02 16:23:14,352 Epoch 3503: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 16:23:14,352 EPOCH 3504
2024-02-02 16:23:20,339 Epoch 3504: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 16:23:20,339 EPOCH 3505
2024-02-02 16:23:26,288 Epoch 3505: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 16:23:26,288 EPOCH 3506
2024-02-02 16:23:32,758 [Epoch: 3506 Step: 00059600] Batch Recognition Loss:   0.000178 => Gls Tokens per Sec:     1446 || Batch Translation Loss:   0.010468 => Txt Tokens per Sec:     4017 || Lr: 0.000050
2024-02-02 16:23:33,268 Epoch 3506: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 16:23:33,269 EPOCH 3507
2024-02-02 16:23:40,176 Epoch 3507: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 16:23:40,176 EPOCH 3508
2024-02-02 16:23:46,889 Epoch 3508: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 16:23:46,890 EPOCH 3509
2024-02-02 16:23:53,757 Epoch 3509: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 16:23:53,758 EPOCH 3510
2024-02-02 16:24:00,451 Epoch 3510: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 16:24:00,452 EPOCH 3511
2024-02-02 16:24:07,204 Epoch 3511: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 16:24:07,204 EPOCH 3512
2024-02-02 16:24:12,737 [Epoch: 3512 Step: 00059700] Batch Recognition Loss:   0.000083 => Gls Tokens per Sec:     1459 || Batch Translation Loss:   0.007145 => Txt Tokens per Sec:     4000 || Lr: 0.000050
2024-02-02 16:24:14,047 Epoch 3512: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 16:24:14,048 EPOCH 3513
2024-02-02 16:24:20,613 Epoch 3513: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 16:24:20,613 EPOCH 3514
2024-02-02 16:24:27,144 Epoch 3514: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 16:24:27,145 EPOCH 3515
2024-02-02 16:24:34,082 Epoch 3515: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.63 
2024-02-02 16:24:34,082 EPOCH 3516
2024-02-02 16:24:41,044 Epoch 3516: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.71 
2024-02-02 16:24:41,045 EPOCH 3517
2024-02-02 16:24:47,797 Epoch 3517: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 16:24:47,798 EPOCH 3518
2024-02-02 16:24:50,357 [Epoch: 3518 Step: 00059800] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     2752 || Batch Translation Loss:   0.024421 => Txt Tokens per Sec:     7463 || Lr: 0.000050
2024-02-02 16:24:54,362 Epoch 3518: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 16:24:54,363 EPOCH 3519
2024-02-02 16:25:00,865 Epoch 3519: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 16:25:00,865 EPOCH 3520
2024-02-02 16:25:07,647 Epoch 3520: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-02 16:25:07,647 EPOCH 3521
2024-02-02 16:25:14,505 Epoch 3521: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 16:25:14,506 EPOCH 3522
2024-02-02 16:25:21,235 Epoch 3522: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 16:25:21,236 EPOCH 3523
2024-02-02 16:25:28,004 Epoch 3523: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-02 16:25:28,005 EPOCH 3524
2024-02-02 16:25:30,308 [Epoch: 3524 Step: 00059900] Batch Recognition Loss:   0.000093 => Gls Tokens per Sec:     2503 || Batch Translation Loss:   0.202364 => Txt Tokens per Sec:     6601 || Lr: 0.000050
2024-02-02 16:25:34,670 Epoch 3524: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-02 16:25:34,670 EPOCH 3525
2024-02-02 16:25:41,425 Epoch 3525: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 16:25:41,425 EPOCH 3526
2024-02-02 16:25:48,166 Epoch 3526: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 16:25:48,167 EPOCH 3527
2024-02-02 16:25:54,772 Epoch 3527: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 16:25:54,772 EPOCH 3528
2024-02-02 16:26:01,457 Epoch 3528: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 16:26:01,457 EPOCH 3529
2024-02-02 16:26:08,334 Epoch 3529: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 16:26:08,334 EPOCH 3530
2024-02-02 16:26:10,058 [Epoch: 3530 Step: 00060000] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     2600 || Batch Translation Loss:   0.017199 => Txt Tokens per Sec:     6899 || Lr: 0.000050
2024-02-02 16:26:40,353 Validation result at epoch 3530, step    60000: duration: 30.2947s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00083	Translation Loss: 104635.66406	PPL: 35273.80078
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.46	(BLEU-1: 9.29,	BLEU-2: 2.66,	BLEU-3: 1.03,	BLEU-4: 0.46)
	CHRF 16.45	ROUGE 8.01
2024-02-02 16:26:40,355 Logging Recognition and Translation Outputs
2024-02-02 16:26:40,355 ========================================================================================================================
2024-02-02 16:26:40,355 Logging Sequence: 179_269.00
2024-02-02 16:26:40,356 	Gloss Reference :	A B+C+D+E
2024-02-02 16:26:40,356 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 16:26:40,356 	Gloss Alignment :	         
2024-02-02 16:26:40,356 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 16:26:40,357 	Text Reference  :	the ban would mean  she  can't compete in   any national or other domestic events 
2024-02-02 16:26:40,358 	Text Hypothesis :	*** *** ***** these kids think that    they are going    to other indian   flights
2024-02-02 16:26:40,358 	Text Alignment  :	D   D   D     S     S    S     S       S    S   S        S        S        S      
2024-02-02 16:26:40,358 ========================================================================================================================
2024-02-02 16:26:40,358 Logging Sequence: 94_253.00
2024-02-02 16:26:40,358 	Gloss Reference :	A B+C+D+E
2024-02-02 16:26:40,358 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 16:26:40,359 	Gloss Alignment :	         
2024-02-02 16:26:40,359 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 16:26:40,361 	Text Reference  :	however ***** *** some tickets will be kept aside for physical sale at    the   stadiums a   few days prior to the match 
2024-02-02 16:26:40,361 	Text Hypothesis :	however after 630 pm   there   will be **** ***** *** certain  fan  zones where beer     was 5   days ***** ** are mumbai
2024-02-02 16:26:40,361 	Text Alignment  :	        I     I   S    S               D    D     D   S        S    S     S     S        S   S        D     D  S   S     
2024-02-02 16:26:40,362 ========================================================================================================================
2024-02-02 16:26:40,362 Logging Sequence: 114_201.00
2024-02-02 16:26:40,362 	Gloss Reference :	A B+C+D+E
2024-02-02 16:26:40,362 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 16:26:40,362 	Gloss Alignment :	         
2024-02-02 16:26:40,363 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 16:26:40,364 	Text Reference  :	***** ****** ******* ******* ** this is  his first time winning the   copa  
2024-02-02 16:26:40,364 	Text Hypothesis :	still dahiya captain because he did  not saw sapna and  told    rohit sharma
2024-02-02 16:26:40,364 	Text Alignment  :	I     I      I       I       I  S    S   S   S     S    S       S     S     
2024-02-02 16:26:40,364 ========================================================================================================================
2024-02-02 16:26:40,364 Logging Sequence: 118_104.00
2024-02-02 16:26:40,364 	Gloss Reference :	A B+C+D+E
2024-02-02 16:26:40,365 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 16:26:40,365 	Gloss Alignment :	         
2024-02-02 16:26:40,365 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 16:26:40,366 	Text Reference  :	******* ** ********** *** *** **** * **** ** **** kylian mbapp strong performance in the match was greatly appreciated
2024-02-02 16:26:40,367 	Text Hypothesis :	however my expression was not like a goal of well and    kohli  for    147         in the ***** *** extra   time       
2024-02-02 16:26:40,367 	Text Alignment  :	I       I  I          I   I   I    I I    I  I    S      S      S      S                  D     D   S       S          
2024-02-02 16:26:40,367 ========================================================================================================================
2024-02-02 16:26:40,367 Logging Sequence: 144_74.00
2024-02-02 16:26:40,367 	Gloss Reference :	A B+C+D+E
2024-02-02 16:26:40,368 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 16:26:40,368 	Gloss Alignment :	         
2024-02-02 16:26:40,368 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 16:26:40,368 	Text Reference  :	** ** * *** ** ************ *** *** ** ** *** ******** * *** isn't that amazing
2024-02-02 16:26:40,368 	Text Hypothesis :	it is a lot of appreciation and was to it has garnered a lot of    a    lot    
2024-02-02 16:26:40,369 	Text Alignment  :	I  I  I I   I  I            I   I   I  I  I   I        I I   S     S    S      
2024-02-02 16:26:40,369 ========================================================================================================================
2024-02-02 16:26:45,497 Epoch 3530: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 16:26:45,497 EPOCH 3531
2024-02-02 16:26:52,308 Epoch 3531: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 16:26:52,309 EPOCH 3532
2024-02-02 16:26:58,867 Epoch 3532: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 16:26:58,868 EPOCH 3533
2024-02-02 16:27:05,801 Epoch 3533: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 16:27:05,801 EPOCH 3534
2024-02-02 16:27:12,491 Epoch 3534: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 16:27:12,492 EPOCH 3535
2024-02-02 16:27:19,025 Epoch 3535: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 16:27:19,026 EPOCH 3536
2024-02-02 16:27:22,395 [Epoch: 3536 Step: 00060100] Batch Recognition Loss:   0.000099 => Gls Tokens per Sec:      876 || Batch Translation Loss:   0.012415 => Txt Tokens per Sec:     2300 || Lr: 0.000050
2024-02-02 16:27:25,810 Epoch 3536: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 16:27:25,811 EPOCH 3537
2024-02-02 16:27:32,585 Epoch 3537: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 16:27:32,586 EPOCH 3538
2024-02-02 16:27:39,252 Epoch 3538: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 16:27:39,253 EPOCH 3539
2024-02-02 16:27:45,571 Epoch 3539: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 16:27:45,571 EPOCH 3540
2024-02-02 16:27:52,621 Epoch 3540: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 16:27:52,622 EPOCH 3541
2024-02-02 16:27:59,371 Epoch 3541: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 16:27:59,372 EPOCH 3542
2024-02-02 16:28:00,235 [Epoch: 3542 Step: 00060200] Batch Recognition Loss:   0.000086 => Gls Tokens per Sec:     2227 || Batch Translation Loss:   0.009794 => Txt Tokens per Sec:     6001 || Lr: 0.000050
2024-02-02 16:28:06,447 Epoch 3542: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 16:28:06,448 EPOCH 3543
2024-02-02 16:28:13,325 Epoch 3543: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 16:28:13,325 EPOCH 3544
2024-02-02 16:28:20,104 Epoch 3544: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 16:28:20,104 EPOCH 3545
2024-02-02 16:28:26,986 Epoch 3545: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 16:28:26,987 EPOCH 3546
2024-02-02 16:28:33,564 Epoch 3546: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 16:28:33,565 EPOCH 3547
2024-02-02 16:28:40,479 Epoch 3547: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 16:28:40,479 EPOCH 3548
2024-02-02 16:28:40,595 [Epoch: 3548 Step: 00060300] Batch Recognition Loss:   0.000082 => Gls Tokens per Sec:     5565 || Batch Translation Loss:   0.008325 => Txt Tokens per Sec:    10895 || Lr: 0.000050
2024-02-02 16:28:47,139 Epoch 3548: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 16:28:47,140 EPOCH 3549
2024-02-02 16:28:53,696 Epoch 3549: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 16:28:53,697 EPOCH 3550
2024-02-02 16:29:00,297 Epoch 3550: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 16:29:00,298 EPOCH 3551
2024-02-02 16:29:07,192 Epoch 3551: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 16:29:07,192 EPOCH 3552
2024-02-02 16:29:13,899 Epoch 3552: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 16:29:13,900 EPOCH 3553
2024-02-02 16:29:20,519 [Epoch: 3553 Step: 00060400] Batch Recognition Loss:   0.000086 => Gls Tokens per Sec:     1509 || Batch Translation Loss:   0.008484 => Txt Tokens per Sec:     4310 || Lr: 0.000050
2024-02-02 16:29:20,623 Epoch 3553: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 16:29:20,623 EPOCH 3554
2024-02-02 16:29:27,527 Epoch 3554: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 16:29:27,528 EPOCH 3555
2024-02-02 16:29:34,303 Epoch 3555: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 16:29:34,303 EPOCH 3556
2024-02-02 16:29:41,222 Epoch 3556: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 16:29:41,223 EPOCH 3557
2024-02-02 16:29:47,863 Epoch 3557: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 16:29:47,864 EPOCH 3558
2024-02-02 16:29:54,270 Epoch 3558: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 16:29:54,271 EPOCH 3559
2024-02-02 16:29:58,218 [Epoch: 3559 Step: 00060500] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:     2271 || Batch Translation Loss:   0.010943 => Txt Tokens per Sec:     6486 || Lr: 0.000050
2024-02-02 16:30:00,783 Epoch 3559: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 16:30:00,784 EPOCH 3560
2024-02-02 16:30:07,597 Epoch 3560: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-02 16:30:07,597 EPOCH 3561
2024-02-02 16:30:14,355 Epoch 3561: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 16:30:14,355 EPOCH 3562
2024-02-02 16:30:21,020 Epoch 3562: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 16:30:21,020 EPOCH 3563
2024-02-02 16:30:27,434 Epoch 3563: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 16:30:27,435 EPOCH 3564
2024-02-02 16:30:34,403 Epoch 3564: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.83 
2024-02-02 16:30:34,404 EPOCH 3565
2024-02-02 16:30:37,820 [Epoch: 3565 Step: 00060600] Batch Recognition Loss:   0.000109 => Gls Tokens per Sec:     2248 || Batch Translation Loss:   0.052733 => Txt Tokens per Sec:     6235 || Lr: 0.000050
2024-02-02 16:30:41,103 Epoch 3565: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.29 
2024-02-02 16:30:41,103 EPOCH 3566
2024-02-02 16:30:47,804 Epoch 3566: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.04 
2024-02-02 16:30:47,805 EPOCH 3567
2024-02-02 16:30:54,479 Epoch 3567: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-02 16:30:54,480 EPOCH 3568
2024-02-02 16:31:01,309 Epoch 3568: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-02 16:31:01,310 EPOCH 3569
2024-02-02 16:31:08,132 Epoch 3569: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-02 16:31:08,132 EPOCH 3570
2024-02-02 16:31:14,917 Epoch 3570: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 16:31:14,918 EPOCH 3571
2024-02-02 16:31:18,737 [Epoch: 3571 Step: 00060700] Batch Recognition Loss:   0.000125 => Gls Tokens per Sec:     1611 || Batch Translation Loss:   0.013327 => Txt Tokens per Sec:     4212 || Lr: 0.000050
2024-02-02 16:31:21,503 Epoch 3571: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 16:31:21,503 EPOCH 3572
2024-02-02 16:31:28,331 Epoch 3572: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 16:31:28,331 EPOCH 3573
2024-02-02 16:31:35,429 Epoch 3573: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 16:31:35,429 EPOCH 3574
2024-02-02 16:31:42,413 Epoch 3574: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 16:31:42,414 EPOCH 3575
2024-02-02 16:31:49,201 Epoch 3575: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 16:31:49,202 EPOCH 3576
2024-02-02 16:31:55,772 Epoch 3576: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 16:31:55,773 EPOCH 3577
2024-02-02 16:31:59,936 [Epoch: 3577 Step: 00060800] Batch Recognition Loss:   0.000189 => Gls Tokens per Sec:     1170 || Batch Translation Loss:   0.032752 => Txt Tokens per Sec:     3280 || Lr: 0.000050
2024-02-02 16:32:02,681 Epoch 3577: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-02 16:32:02,682 EPOCH 3578
2024-02-02 16:32:09,305 Epoch 3578: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 16:32:09,305 EPOCH 3579
2024-02-02 16:32:15,990 Epoch 3579: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 16:32:15,991 EPOCH 3580
2024-02-02 16:32:22,762 Epoch 3580: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 16:32:22,763 EPOCH 3581
2024-02-02 16:32:29,314 Epoch 3581: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 16:32:29,315 EPOCH 3582
2024-02-02 16:32:35,931 Epoch 3582: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 16:32:35,932 EPOCH 3583
2024-02-02 16:32:36,994 [Epoch: 3583 Step: 00060900] Batch Recognition Loss:   0.000102 => Gls Tokens per Sec:     3619 || Batch Translation Loss:   0.011647 => Txt Tokens per Sec:     8499 || Lr: 0.000050
2024-02-02 16:32:42,664 Epoch 3583: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 16:32:42,664 EPOCH 3584
2024-02-02 16:32:49,434 Epoch 3584: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 16:32:49,434 EPOCH 3585
2024-02-02 16:32:55,995 Epoch 3585: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 16:32:55,995 EPOCH 3586
2024-02-02 16:33:02,464 Epoch 3586: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 16:33:02,464 EPOCH 3587
2024-02-02 16:33:09,246 Epoch 3587: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 16:33:09,246 EPOCH 3588
2024-02-02 16:33:16,079 Epoch 3588: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 16:33:16,080 EPOCH 3589
2024-02-02 16:33:17,168 [Epoch: 3589 Step: 00061000] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     2355 || Batch Translation Loss:   0.018695 => Txt Tokens per Sec:     6826 || Lr: 0.000050
2024-02-02 16:33:22,329 Epoch 3589: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 16:33:22,329 EPOCH 3590
2024-02-02 16:33:29,342 Epoch 3590: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 16:33:29,343 EPOCH 3591
2024-02-02 16:33:35,935 Epoch 3591: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 16:33:35,936 EPOCH 3592
2024-02-02 16:33:42,824 Epoch 3592: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 16:33:42,824 EPOCH 3593
2024-02-02 16:33:49,538 Epoch 3593: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 16:33:49,539 EPOCH 3594
2024-02-02 16:33:56,184 Epoch 3594: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 16:33:56,184 EPOCH 3595
2024-02-02 16:33:57,211 [Epoch: 3595 Step: 00061100] Batch Recognition Loss:   0.000152 => Gls Tokens per Sec:     1248 || Batch Translation Loss:   0.014919 => Txt Tokens per Sec:     4370 || Lr: 0.000050
2024-02-02 16:34:02,905 Epoch 3595: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 16:34:02,905 EPOCH 3596
2024-02-02 16:34:09,669 Epoch 3596: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 16:34:09,670 EPOCH 3597
2024-02-02 16:34:16,442 Epoch 3597: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 16:34:16,443 EPOCH 3598
2024-02-02 16:34:23,084 Epoch 3598: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 16:34:23,084 EPOCH 3599
2024-02-02 16:34:30,052 Epoch 3599: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 16:34:30,053 EPOCH 3600
2024-02-02 16:34:36,873 [Epoch: 3600 Step: 00061200] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     1559 || Batch Translation Loss:   0.014622 => Txt Tokens per Sec:     4327 || Lr: 0.000050
2024-02-02 16:34:36,873 Epoch 3600: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 16:34:36,874 EPOCH 3601
2024-02-02 16:34:43,776 Epoch 3601: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 16:34:43,777 EPOCH 3602
2024-02-02 16:34:50,361 Epoch 3602: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 16:34:50,361 EPOCH 3603
2024-02-02 16:34:57,093 Epoch 3603: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 16:34:57,094 EPOCH 3604
2024-02-02 16:35:03,973 Epoch 3604: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 16:35:03,974 EPOCH 3605
2024-02-02 16:35:10,871 Epoch 3605: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 16:35:10,871 EPOCH 3606
2024-02-02 16:35:17,076 [Epoch: 3606 Step: 00061300] Batch Recognition Loss:   0.000111 => Gls Tokens per Sec:     1507 || Batch Translation Loss:   0.012888 => Txt Tokens per Sec:     4214 || Lr: 0.000050
2024-02-02 16:35:17,470 Epoch 3606: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 16:35:17,471 EPOCH 3607
2024-02-02 16:35:24,142 Epoch 3607: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-02 16:35:24,143 EPOCH 3608
2024-02-02 16:35:30,715 Epoch 3608: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 16:35:30,716 EPOCH 3609
2024-02-02 16:35:37,469 Epoch 3609: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-02 16:35:37,470 EPOCH 3610
2024-02-02 16:35:44,542 Epoch 3610: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-02 16:35:44,543 EPOCH 3611
2024-02-02 16:35:51,380 Epoch 3611: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-02 16:35:51,380 EPOCH 3612
2024-02-02 16:35:56,566 [Epoch: 3612 Step: 00061400] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     1556 || Batch Translation Loss:   0.015151 => Txt Tokens per Sec:     4245 || Lr: 0.000050
2024-02-02 16:35:58,318 Epoch 3612: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-02 16:35:58,318 EPOCH 3613
2024-02-02 16:36:05,136 Epoch 3613: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-02 16:36:05,137 EPOCH 3614
2024-02-02 16:36:11,552 Epoch 3614: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 16:36:11,553 EPOCH 3615
2024-02-02 16:36:18,474 Epoch 3615: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-02 16:36:18,474 EPOCH 3616
2024-02-02 16:36:25,231 Epoch 3616: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-02 16:36:25,232 EPOCH 3617
2024-02-02 16:36:32,008 Epoch 3617: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 16:36:32,008 EPOCH 3618
2024-02-02 16:36:36,943 [Epoch: 3618 Step: 00061500] Batch Recognition Loss:   0.000345 => Gls Tokens per Sec:     1376 || Batch Translation Loss:   0.023892 => Txt Tokens per Sec:     3844 || Lr: 0.000050
2024-02-02 16:36:38,499 Epoch 3618: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-02 16:36:38,499 EPOCH 3619
2024-02-02 16:36:45,471 Epoch 3619: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 16:36:45,472 EPOCH 3620
2024-02-02 16:36:52,021 Epoch 3620: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 16:36:52,023 EPOCH 3621
2024-02-02 16:36:59,043 Epoch 3621: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 16:36:59,044 EPOCH 3622
2024-02-02 16:37:05,779 Epoch 3622: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 16:37:05,779 EPOCH 3623
2024-02-02 16:37:12,302 Epoch 3623: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 16:37:12,302 EPOCH 3624
2024-02-02 16:37:17,396 [Epoch: 3624 Step: 00061600] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     1082 || Batch Translation Loss:   0.025195 => Txt Tokens per Sec:     3104 || Lr: 0.000050
2024-02-02 16:37:19,215 Epoch 3624: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 16:37:19,216 EPOCH 3625
2024-02-02 16:37:25,891 Epoch 3625: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 16:37:25,892 EPOCH 3626
2024-02-02 16:37:32,650 Epoch 3626: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 16:37:32,651 EPOCH 3627
2024-02-02 16:37:39,243 Epoch 3627: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 16:37:39,243 EPOCH 3628
2024-02-02 16:37:46,136 Epoch 3628: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 16:37:46,136 EPOCH 3629
2024-02-02 16:37:52,819 Epoch 3629: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 16:37:52,820 EPOCH 3630
2024-02-02 16:37:54,594 [Epoch: 3630 Step: 00061700] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     2525 || Batch Translation Loss:   0.019016 => Txt Tokens per Sec:     7020 || Lr: 0.000050
2024-02-02 16:37:59,349 Epoch 3630: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 16:37:59,349 EPOCH 3631
2024-02-02 16:38:06,167 Epoch 3631: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 16:38:06,168 EPOCH 3632
2024-02-02 16:38:12,915 Epoch 3632: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 16:38:12,915 EPOCH 3633
2024-02-02 16:38:19,807 Epoch 3633: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 16:38:19,808 EPOCH 3634
2024-02-02 16:38:26,700 Epoch 3634: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 16:38:26,701 EPOCH 3635
2024-02-02 16:38:33,446 Epoch 3635: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 16:38:33,446 EPOCH 3636
2024-02-02 16:38:37,024 [Epoch: 3636 Step: 00061800] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:      825 || Batch Translation Loss:   0.014762 => Txt Tokens per Sec:     2452 || Lr: 0.000050
2024-02-02 16:38:40,581 Epoch 3636: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.16 
2024-02-02 16:38:40,581 EPOCH 3637
2024-02-02 16:38:47,258 Epoch 3637: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.16 
2024-02-02 16:38:47,258 EPOCH 3638
2024-02-02 16:38:53,991 Epoch 3638: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.76 
2024-02-02 16:38:53,991 EPOCH 3639
2024-02-02 16:39:00,607 Epoch 3639: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-02 16:39:00,608 EPOCH 3640
2024-02-02 16:39:07,646 Epoch 3640: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-02 16:39:07,646 EPOCH 3641
2024-02-02 16:39:14,212 Epoch 3641: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 16:39:14,212 EPOCH 3642
2024-02-02 16:39:15,112 [Epoch: 3642 Step: 00061900] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:     2137 || Batch Translation Loss:   0.024164 => Txt Tokens per Sec:     6186 || Lr: 0.000050
2024-02-02 16:39:20,597 Epoch 3642: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 16:39:20,597 EPOCH 3643
2024-02-02 16:39:27,674 Epoch 3643: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 16:39:27,674 EPOCH 3644
2024-02-02 16:39:34,611 Epoch 3644: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 16:39:34,612 EPOCH 3645
2024-02-02 16:39:41,394 Epoch 3645: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 16:39:41,395 EPOCH 3646
2024-02-02 16:39:47,981 Epoch 3646: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 16:39:47,982 EPOCH 3647
2024-02-02 16:39:54,781 Epoch 3647: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 16:39:54,782 EPOCH 3648
2024-02-02 16:39:55,120 [Epoch: 3648 Step: 00062000] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:     1905 || Batch Translation Loss:   0.029633 => Txt Tokens per Sec:     6062 || Lr: 0.000050
2024-02-02 16:40:25,611 Validation result at epoch 3648, step    62000: duration: 30.4910s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00143	Translation Loss: 104309.01562	PPL: 34139.43750
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.72	(BLEU-1: 10.22,	BLEU-2: 3.05,	BLEU-3: 1.33,	BLEU-4: 0.72)
	CHRF 16.98	ROUGE 8.56
2024-02-02 16:40:25,612 Logging Recognition and Translation Outputs
2024-02-02 16:40:25,612 ========================================================================================================================
2024-02-02 16:40:25,613 Logging Sequence: 87_52.00
2024-02-02 16:40:25,613 	Gloss Reference :	A B+C+D+E
2024-02-02 16:40:25,613 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 16:40:25,613 	Gloss Alignment :	         
2024-02-02 16:40:25,614 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 16:40:25,615 	Text Reference  :	that  is        when        gambhir walked into bat   and rescued india with  his     brilliant 97 runs
2024-02-02 16:40:25,615 	Text Hypothesis :	other countries participate in      the    ipl  india had won     the   match between teams     of ipl 
2024-02-02 16:40:25,615 	Text Alignment  :	S     S         S           S       S      S    S     S   S       S     S     S       S         S  S   
2024-02-02 16:40:25,616 ========================================================================================================================
2024-02-02 16:40:25,616 Logging Sequence: 85_2.00
2024-02-02 16:40:25,616 	Gloss Reference :	A B+C+D+E
2024-02-02 16:40:25,616 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 16:40:25,616 	Gloss Alignment :	         
2024-02-02 16:40:25,616 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 16:40:25,617 	Text Reference  :	andrew symonds is one of the finest all  rounders in the history of      australian cricket
2024-02-02 16:40:25,618 	Text Hypothesis :	when   india   is *** ** *** ****** from present  at the stadium chennai super      kings  
2024-02-02 16:40:25,618 	Text Alignment  :	S      S          D   D  D   D      S    S        S      S       S       S          S      
2024-02-02 16:40:25,618 ========================================================================================================================
2024-02-02 16:40:25,618 Logging Sequence: 51_110.00
2024-02-02 16:40:25,618 	Gloss Reference :	A B+C+D+E
2024-02-02 16:40:25,618 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 16:40:25,618 	Gloss Alignment :	         
2024-02-02 16:40:25,619 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 16:40:25,619 	Text Reference  :	** ***** **** *** the   aussies  were very happy with their victory 
2024-02-02 16:40:25,619 	Text Hypothesis :	he don't know for their families did  not  to    be   from  pakistan
2024-02-02 16:40:25,620 	Text Alignment  :	I  I     I    I   S     S        S    S    S     S    S     S       
2024-02-02 16:40:25,620 ========================================================================================================================
2024-02-02 16:40:25,620 Logging Sequence: 72_59.00
2024-02-02 16:40:25,620 	Gloss Reference :	A B+C+D+E
2024-02-02 16:40:25,620 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 16:40:25,620 	Gloss Alignment :	         
2024-02-02 16:40:25,620 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 16:40:25,622 	Text Reference  :	***** *** ****** ****** ******* after that   sapna  and  shobit started arguing and  misbehaving with the cricketer
2024-02-02 16:40:25,622 	Text Hypothesis :	sapna are caught indian skipper rohit sharma wiping away tears  as      he      like you         can  see this     
2024-02-02 16:40:25,622 	Text Alignment  :	I     I   I      I      I       S     S      S      S    S      S       S       S    S           S    S   S        
2024-02-02 16:40:25,622 ========================================================================================================================
2024-02-02 16:40:25,622 Logging Sequence: 122_184.00
2024-02-02 16:40:25,623 	Gloss Reference :	A B+C+D+E
2024-02-02 16:40:25,623 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 16:40:25,623 	Gloss Alignment :	         
2024-02-02 16:40:25,623 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 16:40:25,624 	Text Reference  :	*** *** ***** are  playing exceptionally well  and keeping hopes   of  further olympic medals  alive    
2024-02-02 16:40:25,624 	Text Hypothesis :	you all aware that earlier the           games are a       ronaldo has nearly  300     million followers
2024-02-02 16:40:25,625 	Text Alignment  :	I   I   I     S    S       S             S     S   S       S       S   S       S       S       S        
2024-02-02 16:40:25,625 ========================================================================================================================
2024-02-02 16:40:32,140 Epoch 3648: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 16:40:32,140 EPOCH 3649
2024-02-02 16:40:38,806 Epoch 3649: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 16:40:38,807 EPOCH 3650
2024-02-02 16:40:45,368 Epoch 3650: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 16:40:45,368 EPOCH 3651
2024-02-02 16:40:52,430 Epoch 3651: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 16:40:52,430 EPOCH 3652
2024-02-02 16:40:59,275 Epoch 3652: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 16:40:59,276 EPOCH 3653
2024-02-02 16:41:05,266 [Epoch: 3653 Step: 00062100] Batch Recognition Loss:   0.000095 => Gls Tokens per Sec:     1668 || Batch Translation Loss:   0.006124 => Txt Tokens per Sec:     4550 || Lr: 0.000050
2024-02-02 16:41:05,756 Epoch 3653: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 16:41:05,756 EPOCH 3654
2024-02-02 16:41:12,299 Epoch 3654: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 16:41:12,299 EPOCH 3655
2024-02-02 16:41:19,171 Epoch 3655: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 16:41:19,171 EPOCH 3656
2024-02-02 16:41:25,872 Epoch 3656: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 16:41:25,872 EPOCH 3657
2024-02-02 16:41:32,631 Epoch 3657: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 16:41:32,631 EPOCH 3658
2024-02-02 16:41:39,194 Epoch 3658: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 16:41:39,194 EPOCH 3659
2024-02-02 16:41:45,199 [Epoch: 3659 Step: 00062200] Batch Recognition Loss:   0.000157 => Gls Tokens per Sec:     1451 || Batch Translation Loss:   0.012482 => Txt Tokens per Sec:     4022 || Lr: 0.000050
2024-02-02 16:41:46,001 Epoch 3659: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 16:41:46,001 EPOCH 3660
2024-02-02 16:41:52,743 Epoch 3660: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 16:41:52,744 EPOCH 3661
2024-02-02 16:41:59,494 Epoch 3661: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 16:41:59,495 EPOCH 3662
2024-02-02 16:42:06,105 Epoch 3662: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 16:42:06,105 EPOCH 3663
2024-02-02 16:42:12,958 Epoch 3663: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 16:42:12,958 EPOCH 3664
2024-02-02 16:42:19,499 Epoch 3664: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 16:42:19,499 EPOCH 3665
2024-02-02 16:42:24,674 [Epoch: 3665 Step: 00062300] Batch Recognition Loss:   0.000106 => Gls Tokens per Sec:     1436 || Batch Translation Loss:   0.013240 => Txt Tokens per Sec:     3854 || Lr: 0.000050
2024-02-02 16:42:26,206 Epoch 3665: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 16:42:26,206 EPOCH 3666
2024-02-02 16:42:32,877 Epoch 3666: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 16:42:32,878 EPOCH 3667
2024-02-02 16:42:39,669 Epoch 3667: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 16:42:39,670 EPOCH 3668
2024-02-02 16:42:46,506 Epoch 3668: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 16:42:46,506 EPOCH 3669
2024-02-02 16:42:53,325 Epoch 3669: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 16:42:53,325 EPOCH 3670
2024-02-02 16:42:59,991 Epoch 3670: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 16:42:59,991 EPOCH 3671
2024-02-02 16:43:04,330 [Epoch: 3671 Step: 00062400] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     1418 || Batch Translation Loss:   0.013748 => Txt Tokens per Sec:     3928 || Lr: 0.000050
2024-02-02 16:43:06,611 Epoch 3671: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 16:43:06,611 EPOCH 3672
2024-02-02 16:43:13,373 Epoch 3672: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 16:43:13,373 EPOCH 3673
2024-02-02 16:43:20,246 Epoch 3673: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 16:43:20,247 EPOCH 3674
2024-02-02 16:43:27,041 Epoch 3674: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 16:43:27,042 EPOCH 3675
2024-02-02 16:43:33,677 Epoch 3675: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 16:43:33,678 EPOCH 3676
2024-02-02 16:43:40,210 Epoch 3676: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 16:43:40,210 EPOCH 3677
2024-02-02 16:43:42,396 [Epoch: 3677 Step: 00062500] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     2344 || Batch Translation Loss:   0.017384 => Txt Tokens per Sec:     6759 || Lr: 0.000050
2024-02-02 16:43:46,909 Epoch 3677: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 16:43:46,909 EPOCH 3678
2024-02-02 16:43:53,608 Epoch 3678: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 16:43:53,608 EPOCH 3679
2024-02-02 16:44:00,141 Epoch 3679: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 16:44:00,142 EPOCH 3680
2024-02-02 16:44:07,119 Epoch 3680: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 16:44:07,120 EPOCH 3681
2024-02-02 16:44:13,890 Epoch 3681: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 16:44:13,891 EPOCH 3682
2024-02-02 16:44:20,760 Epoch 3682: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 16:44:20,761 EPOCH 3683
2024-02-02 16:44:22,624 [Epoch: 3683 Step: 00062600] Batch Recognition Loss:   0.000180 => Gls Tokens per Sec:     2062 || Batch Translation Loss:   0.016696 => Txt Tokens per Sec:     6214 || Lr: 0.000050
2024-02-02 16:44:27,545 Epoch 3683: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 16:44:27,546 EPOCH 3684
2024-02-02 16:44:34,496 Epoch 3684: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 16:44:34,497 EPOCH 3685
2024-02-02 16:44:41,112 Epoch 3685: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 16:44:41,112 EPOCH 3686
2024-02-02 16:44:47,987 Epoch 3686: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-02 16:44:47,987 EPOCH 3687
2024-02-02 16:44:54,762 Epoch 3687: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.42 
2024-02-02 16:44:54,763 EPOCH 3688
2024-02-02 16:45:01,490 Epoch 3688: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-02 16:45:01,491 EPOCH 3689
2024-02-02 16:45:02,688 [Epoch: 3689 Step: 00062700] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:     2140 || Batch Translation Loss:   0.020754 => Txt Tokens per Sec:     6077 || Lr: 0.000050
2024-02-02 16:45:07,846 Epoch 3689: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-02 16:45:07,847 EPOCH 3690
2024-02-02 16:45:14,535 Epoch 3690: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-02 16:45:14,535 EPOCH 3691
2024-02-02 16:45:21,578 Epoch 3691: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 16:45:21,579 EPOCH 3692
2024-02-02 16:45:28,441 Epoch 3692: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-02 16:45:28,441 EPOCH 3693
2024-02-02 16:45:35,062 Epoch 3693: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 16:45:35,063 EPOCH 3694
2024-02-02 16:45:41,623 Epoch 3694: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 16:45:41,623 EPOCH 3695
2024-02-02 16:45:42,407 [Epoch: 3695 Step: 00062800] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     1639 || Batch Translation Loss:   0.017885 => Txt Tokens per Sec:     4554 || Lr: 0.000050
2024-02-02 16:45:48,691 Epoch 3695: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 16:45:48,692 EPOCH 3696
2024-02-02 16:45:55,545 Epoch 3696: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 16:45:55,545 EPOCH 3697
2024-02-02 16:46:02,354 Epoch 3697: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 16:46:02,355 EPOCH 3698
2024-02-02 16:46:09,345 Epoch 3698: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 16:46:09,345 EPOCH 3699
2024-02-02 16:46:16,046 Epoch 3699: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-02 16:46:16,046 EPOCH 3700
2024-02-02 16:46:22,860 [Epoch: 3700 Step: 00062900] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     1560 || Batch Translation Loss:   0.014853 => Txt Tokens per Sec:     4331 || Lr: 0.000050
2024-02-02 16:46:22,861 Epoch 3700: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 16:46:22,861 EPOCH 3701
2024-02-02 16:46:29,496 Epoch 3701: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-02 16:46:29,496 EPOCH 3702
2024-02-02 16:46:36,028 Epoch 3702: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 16:46:36,028 EPOCH 3703
2024-02-02 16:46:42,994 Epoch 3703: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 16:46:42,995 EPOCH 3704
2024-02-02 16:46:49,851 Epoch 3704: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 16:46:49,852 EPOCH 3705
2024-02-02 16:46:56,536 Epoch 3705: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 16:46:56,537 EPOCH 3706
2024-02-02 16:47:00,878 [Epoch: 3706 Step: 00063000] Batch Recognition Loss:   0.000241 => Gls Tokens per Sec:     2212 || Batch Translation Loss:   0.051698 => Txt Tokens per Sec:     6249 || Lr: 0.000050
2024-02-02 16:47:03,364 Epoch 3706: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 16:47:03,364 EPOCH 3707
2024-02-02 16:47:10,062 Epoch 3707: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 16:47:10,063 EPOCH 3708
2024-02-02 16:47:16,672 Epoch 3708: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 16:47:16,672 EPOCH 3709
2024-02-02 16:47:23,807 Epoch 3709: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 16:47:23,808 EPOCH 3710
2024-02-02 16:47:30,672 Epoch 3710: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 16:47:30,673 EPOCH 3711
2024-02-02 16:47:37,201 Epoch 3711: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 16:47:37,202 EPOCH 3712
2024-02-02 16:47:42,352 [Epoch: 3712 Step: 00063100] Batch Recognition Loss:   0.000175 => Gls Tokens per Sec:     1567 || Batch Translation Loss:   0.017085 => Txt Tokens per Sec:     4189 || Lr: 0.000050
2024-02-02 16:47:43,814 Epoch 3712: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 16:47:43,815 EPOCH 3713
2024-02-02 16:47:50,635 Epoch 3713: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 16:47:50,636 EPOCH 3714
2024-02-02 16:47:57,526 Epoch 3714: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 16:47:57,527 EPOCH 3715
2024-02-02 16:48:04,265 Epoch 3715: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 16:48:04,266 EPOCH 3716
2024-02-02 16:48:10,826 Epoch 3716: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 16:48:10,826 EPOCH 3717
2024-02-02 16:48:17,450 Epoch 3717: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-02 16:48:17,450 EPOCH 3718
2024-02-02 16:48:20,555 [Epoch: 3718 Step: 00063200] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:     2269 || Batch Translation Loss:   0.009042 => Txt Tokens per Sec:     6308 || Lr: 0.000050
2024-02-02 16:48:24,227 Epoch 3718: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-02 16:48:24,227 EPOCH 3719
2024-02-02 16:48:30,883 Epoch 3719: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 16:48:30,884 EPOCH 3720
2024-02-02 16:48:37,578 Epoch 3720: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 16:48:37,579 EPOCH 3721
2024-02-02 16:48:44,106 Epoch 3721: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 16:48:44,107 EPOCH 3722
2024-02-02 16:48:50,982 Epoch 3722: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 16:48:50,983 EPOCH 3723
2024-02-02 16:48:57,570 Epoch 3723: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 16:48:57,571 EPOCH 3724
2024-02-02 16:49:02,270 [Epoch: 3724 Step: 00063300] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:     1173 || Batch Translation Loss:   0.023095 => Txt Tokens per Sec:     3321 || Lr: 0.000050
2024-02-02 16:49:04,384 Epoch 3724: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 16:49:04,385 EPOCH 3725
2024-02-02 16:49:11,124 Epoch 3725: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 16:49:11,124 EPOCH 3726
2024-02-02 16:49:17,916 Epoch 3726: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-02 16:49:17,917 EPOCH 3727
2024-02-02 16:49:23,929 Epoch 3727: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-02 16:49:23,929 EPOCH 3728
2024-02-02 16:49:30,453 Epoch 3728: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-02 16:49:30,454 EPOCH 3729
2024-02-02 16:49:37,235 Epoch 3729: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-02 16:49:37,235 EPOCH 3730
2024-02-02 16:49:41,019 [Epoch: 3730 Step: 00063400] Batch Recognition Loss:   0.000152 => Gls Tokens per Sec:     1118 || Batch Translation Loss:   0.029202 => Txt Tokens per Sec:     3136 || Lr: 0.000050
2024-02-02 16:49:43,985 Epoch 3730: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-02 16:49:43,985 EPOCH 3731
2024-02-02 16:49:50,692 Epoch 3731: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-02 16:49:50,693 EPOCH 3732
2024-02-02 16:49:57,434 Epoch 3732: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-02 16:49:57,435 EPOCH 3733
2024-02-02 16:50:04,276 Epoch 3733: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-02 16:50:04,276 EPOCH 3734
2024-02-02 16:50:11,036 Epoch 3734: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-02 16:50:11,036 EPOCH 3735
2024-02-02 16:50:17,923 Epoch 3735: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-02 16:50:17,923 EPOCH 3736
2024-02-02 16:50:18,849 [Epoch: 3736 Step: 00063500] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     3459 || Batch Translation Loss:   0.012652 => Txt Tokens per Sec:     8146 || Lr: 0.000050
2024-02-02 16:50:24,522 Epoch 3736: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 16:50:24,522 EPOCH 3737
2024-02-02 16:50:30,485 Epoch 3737: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 16:50:30,485 EPOCH 3738
2024-02-02 16:50:37,012 Epoch 3738: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 16:50:37,012 EPOCH 3739
2024-02-02 16:50:43,987 Epoch 3739: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 16:50:43,988 EPOCH 3740
2024-02-02 16:50:50,844 Epoch 3740: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-02 16:50:50,845 EPOCH 3741
2024-02-02 16:50:57,665 Epoch 3741: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-02 16:50:57,665 EPOCH 3742
2024-02-02 16:50:58,674 [Epoch: 3742 Step: 00063600] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     1903 || Batch Translation Loss:   0.014869 => Txt Tokens per Sec:     5520 || Lr: 0.000050
2024-02-02 16:51:04,198 Epoch 3742: Total Training Recognition Loss 0.01  Total Training Translation Loss 6.03 
2024-02-02 16:51:04,199 EPOCH 3743
2024-02-02 16:51:11,181 Epoch 3743: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.32 
2024-02-02 16:51:11,182 EPOCH 3744
2024-02-02 16:51:17,939 Epoch 3744: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-02 16:51:17,939 EPOCH 3745
2024-02-02 16:51:24,703 Epoch 3745: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-02 16:51:24,704 EPOCH 3746
2024-02-02 16:51:31,366 Epoch 3746: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-02 16:51:31,367 EPOCH 3747
2024-02-02 16:51:38,185 Epoch 3747: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-02 16:51:38,185 EPOCH 3748
2024-02-02 16:51:38,329 [Epoch: 3748 Step: 00063700] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:     4475 || Batch Translation Loss:   0.016388 => Txt Tokens per Sec:    10503 || Lr: 0.000050
2024-02-02 16:51:45,957 Epoch 3748: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 16:51:45,958 EPOCH 3749
2024-02-02 16:51:52,574 Epoch 3749: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-02 16:51:52,575 EPOCH 3750
2024-02-02 16:51:59,159 Epoch 3750: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-02 16:51:59,160 EPOCH 3751
2024-02-02 16:52:06,238 Epoch 3751: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 16:52:06,239 EPOCH 3752
2024-02-02 16:52:12,931 Epoch 3752: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-02 16:52:12,931 EPOCH 3753
2024-02-02 16:52:19,549 [Epoch: 3753 Step: 00063800] Batch Recognition Loss:   0.000239 => Gls Tokens per Sec:     1510 || Batch Translation Loss:   0.018658 => Txt Tokens per Sec:     4265 || Lr: 0.000050
2024-02-02 16:52:19,701 Epoch 3753: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-02 16:52:19,701 EPOCH 3754
2024-02-02 16:52:26,202 Epoch 3754: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-02 16:52:26,202 EPOCH 3755
2024-02-02 16:52:33,120 Epoch 3755: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-02 16:52:33,121 EPOCH 3756
2024-02-02 16:52:39,862 Epoch 3756: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 16:52:39,863 EPOCH 3757
2024-02-02 16:52:46,651 Epoch 3757: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-02 16:52:46,651 EPOCH 3758
2024-02-02 16:52:53,329 Epoch 3758: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 16:52:53,329 EPOCH 3759
2024-02-02 16:52:59,427 [Epoch: 3759 Step: 00063900] Batch Recognition Loss:   0.000163 => Gls Tokens per Sec:     1429 || Batch Translation Loss:   0.025011 => Txt Tokens per Sec:     4018 || Lr: 0.000050
2024-02-02 16:53:00,057 Epoch 3759: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-02 16:53:00,057 EPOCH 3760
2024-02-02 16:53:06,738 Epoch 3760: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 16:53:06,738 EPOCH 3761
2024-02-02 16:53:13,580 Epoch 3761: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-02 16:53:13,581 EPOCH 3762
2024-02-02 16:53:20,493 Epoch 3762: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-02 16:53:20,493 EPOCH 3763
2024-02-02 16:53:26,905 Epoch 3763: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 16:53:26,906 EPOCH 3764
2024-02-02 16:53:33,429 Epoch 3764: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-02 16:53:33,429 EPOCH 3765
2024-02-02 16:53:36,326 [Epoch: 3765 Step: 00064000] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:     2652 || Batch Translation Loss:   0.011039 => Txt Tokens per Sec:     7572 || Lr: 0.000050
2024-02-02 16:54:06,660 Validation result at epoch 3765, step    64000: duration: 30.3337s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.00107	Translation Loss: 104695.50781	PPL: 35485.69141
	Eval Metric: BLEU
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.43	(BLEU-1: 10.27,	BLEU-2: 2.70,	BLEU-3: 0.94,	BLEU-4: 0.43)
	CHRF 16.71	ROUGE 8.70
2024-02-02 16:54:06,661 Logging Recognition and Translation Outputs
2024-02-02 16:54:06,661 ========================================================================================================================
2024-02-02 16:54:06,662 Logging Sequence: 72_2.00
2024-02-02 16:54:06,662 	Gloss Reference :	A B+C+D+E
2024-02-02 16:54:06,662 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 16:54:06,662 	Gloss Alignment :	         
2024-02-02 16:54:06,662 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 16:54:06,664 	Text Reference  :	star india batter prithvi shaw and friend ashish yadav went    to   a    5    star  hotel  for dinner on 15          february early  morning
2024-02-02 16:54:06,664 	Text Hypothesis :	**** ***** ****** ******* **** *** ****** ****** the   caption said 'the name orion stands for ****** a  grandfather and      making sports 
2024-02-02 16:54:06,664 	Text Alignment  :	D    D     D      D       D    D   D      D      S     S       S    S    S    S     S          D      S  S           S        S      S      
2024-02-02 16:54:06,664 ========================================================================================================================
2024-02-02 16:54:06,665 Logging Sequence: 138_257.00
2024-02-02 16:54:06,665 	Gloss Reference :	A B+C+D+E
2024-02-02 16:54:06,665 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 16:54:06,665 	Gloss Alignment :	         
2024-02-02 16:54:06,665 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 16:54:06,666 	Text Reference  :	*** ******* **** ****** **** have vehemently condemned the racial attacks towards the      three players
2024-02-02 16:54:06,667 	Text Hypothesis :	the spinner anil kumble also sri  lanka      and       the last   day     that    mahendra singh dhoni  
2024-02-02 16:54:06,667 	Text Alignment  :	I   I       I    I      I    S    S          S             S      S       S       S        S     S      
2024-02-02 16:54:06,667 ========================================================================================================================
2024-02-02 16:54:06,667 Logging Sequence: 170_134.00
2024-02-02 16:54:06,667 	Gloss Reference :	A B+C+D+E
2024-02-02 16:54:06,667 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 16:54:06,667 	Gloss Alignment :	         
2024-02-02 16:54:06,667 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 16:54:06,669 	Text Reference  :	taylor felt that the owner at the back of the mind was upset that  he  had paid 8 crores but   taylor failed to    score a   run      
2024-02-02 16:54:06,669 	Text Hypothesis :	****** **** **** *** ***** ** *** **** ** *** **** *** ***** later who had **** * ****** asked him    a      young boy   for cricketer
2024-02-02 16:54:06,669 	Text Alignment  :	D      D    D    D   D     D  D   D    D  D   D    D   D     S     S       D    D D      S     S      S      S     S     S   S        
2024-02-02 16:54:06,670 ========================================================================================================================
2024-02-02 16:54:06,670 Logging Sequence: 70_39.00
2024-02-02 16:54:06,670 	Gloss Reference :	A B+C+D+E
2024-02-02 16:54:06,670 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 16:54:06,670 	Gloss Alignment :	         
2024-02-02 16:54:06,670 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 16:54:06,671 	Text Reference  :	well the tournament called euro    2020  when it    is   being  held         in 2021 
2024-02-02 16:54:06,671 	Text Hypothesis :	**** *** ********** ****** however games are  aware that strict restrictions in teams
2024-02-02 16:54:06,671 	Text Alignment  :	D    D   D          D      S       S     S    S     S    S      S               S    
2024-02-02 16:54:06,671 ========================================================================================================================
2024-02-02 16:54:06,671 Logging Sequence: 124_172.00
2024-02-02 16:54:06,672 	Gloss Reference :	A B+C+D+E
2024-02-02 16:54:06,672 	Gloss Hypothesis:	A B+C+D+E
2024-02-02 16:54:06,672 	Gloss Alignment :	         
2024-02-02 16:54:06,672 	--------------------------------------------------------------------------------------------------------------------
2024-02-02 16:54:06,673 	Text Reference  :	**** ** ******* ** the ipl matches will be held only in pune mumbai   at        25    capacity
2024-02-02 16:54:06,673 	Text Hypothesis :	this is because of the *** board   will be **** **** ** **** creating awareness about water   
2024-02-02 16:54:06,673 	Text Alignment  :	I    I  I       I      D   S               D    D    D  D    S        S         S     S       
2024-02-02 16:54:06,673 ========================================================================================================================
2024-02-02 16:54:06,677 Training ended since there were no improvements inthe last learning rate step: 0.000050
2024-02-02 16:54:06,680 Best validation result at step    12000:   1.06 eval_metric.
2024-02-02 16:54:31,940 ------------------------------------------------------------
2024-02-02 16:54:31,941 [DEV] partition [RECOGNITION] experiment [BW]: 1
2024-02-02 16:55:02,747 finished in 30.8062s 
2024-02-02 16:55:02,748 ************************************************************
2024-02-02 16:55:02,748 [DEV] partition [RECOGNITION] results:
	New Best CTC Decode Beam Size: 1
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
2024-02-02 16:55:02,748 ************************************************************
2024-02-02 16:55:02,748 ------------------------------------------------------------
2024-02-02 16:55:02,749 [DEV] partition [RECOGNITION] experiment [BW]: 2
2024-02-02 16:55:32,908 finished in 30.1600s 
2024-02-02 16:55:32,908 ------------------------------------------------------------
2024-02-02 16:55:32,909 [DEV] partition [RECOGNITION] experiment [BW]: 3
2024-02-02 16:56:03,066 finished in 30.1571s 
2024-02-02 16:56:03,067 ------------------------------------------------------------
2024-02-02 16:56:03,067 [DEV] partition [RECOGNITION] experiment [BW]: 4
2024-02-02 16:56:33,338 finished in 30.2708s 
2024-02-02 16:56:33,339 ------------------------------------------------------------
2024-02-02 16:56:33,339 [DEV] partition [RECOGNITION] experiment [BW]: 5
2024-02-02 16:57:03,657 finished in 30.3180s 
2024-02-02 16:57:03,658 ------------------------------------------------------------
2024-02-02 16:57:03,658 [DEV] partition [RECOGNITION] experiment [BW]: 6
2024-02-02 16:57:33,868 finished in 30.2101s 
2024-02-02 16:57:33,869 ------------------------------------------------------------
2024-02-02 16:57:33,869 [DEV] partition [RECOGNITION] experiment [BW]: 7
2024-02-02 16:58:04,277 finished in 30.4080s 
2024-02-02 16:58:04,278 ------------------------------------------------------------
2024-02-02 16:58:04,278 [DEV] partition [RECOGNITION] experiment [BW]: 8
2024-02-02 16:58:34,717 finished in 30.4393s 
2024-02-02 16:58:34,718 ------------------------------------------------------------
2024-02-02 16:58:34,718 [DEV] partition [RECOGNITION] experiment [BW]: 9
2024-02-02 16:59:05,230 finished in 30.5115s 
2024-02-02 16:59:05,231 ------------------------------------------------------------
2024-02-02 16:59:05,231 [DEV] partition [RECOGNITION] experiment [BW]: 10
2024-02-02 16:59:35,682 finished in 30.4511s 
2024-02-02 16:59:35,683 ============================================================
2024-02-02 17:00:05,847 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 1 and Alpha: -1
	BLEU-4 1.06	(BLEU-1: 11.73,	BLEU-2: 3.85,	BLEU-3: 1.85,	BLEU-4: 1.06)
	CHRF 17.62	ROUGE 10.14
2024-02-02 17:00:05,848 ------------------------------------------------------------
2024-02-02 17:53:44,317 ************************************************************
2024-02-02 17:53:44,318 [DEV] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 1
	Best Translation Beam Size: 1 and Alpha: -1
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 1.06	(BLEU-1: 11.73,	BLEU-2: 3.85,	BLEU-3: 1.85,	BLEU-4: 1.06)
	CHRF 17.62	ROUGE 10.14
2024-02-02 17:53:44,318 ************************************************************
2024-02-02 17:54:07,811 [TEST] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 1
	Best Translation Beam Size: 1 and Alpha: -1
	WER 0.00	(DEL: 0.00,	INS: 0.00,	SUB: 0.00)
	BLEU-4 0.63	(BLEU-1: 11.07,	BLEU-2: 3.54,	BLEU-3: 1.30,	BLEU-4: 0.63)
	CHRF 17.33	ROUGE 9.41
2024-02-02 17:54:07,812 ************************************************************
