2024-02-05 23:14:55,118 Hello! This is Joey-NMT.
2024-02-05 23:14:55,133 Total params: 25642504
2024-02-05 23:14:55,134 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'gloss_output_layer.bias', 'gloss_output_layer.weight', 'sgn_embed.ln.bias', 'sgn_embed.ln.weight', 'sgn_embed.norm.norm.bias', 'sgn_embed.norm.norm.weight', 'txt_embed.norm.norm.bias', 'txt_embed.norm.norm.weight']
2024-02-05 23:14:56,279 cfg.name                           : sign_experiment
2024-02-05 23:14:56,279 cfg.data.data_path                 : ./data/Sports_dataset/8/
2024-02-05 23:14:56,280 cfg.data.version                   : phoenix_2014_trans
2024-02-05 23:14:56,280 cfg.data.sgn                       : sign
2024-02-05 23:14:56,280 cfg.data.txt                       : text
2024-02-05 23:14:56,280 cfg.data.gls                       : gloss
2024-02-05 23:14:56,280 cfg.data.train                     : excel_data.train
2024-02-05 23:14:56,280 cfg.data.dev                       : excel_data.dev
2024-02-05 23:14:56,280 cfg.data.test                      : excel_data.test
2024-02-05 23:14:56,281 cfg.data.feature_size              : 2560
2024-02-05 23:14:56,281 cfg.data.level                     : word
2024-02-05 23:14:56,281 cfg.data.txt_lowercase             : True
2024-02-05 23:14:56,281 cfg.data.max_sent_length           : 500
2024-02-05 23:14:56,281 cfg.data.random_train_subset       : -1
2024-02-05 23:14:56,281 cfg.data.random_dev_subset         : -1
2024-02-05 23:14:56,281 cfg.testing.recognition_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-05 23:14:56,281 cfg.testing.translation_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-05 23:14:56,282 cfg.testing.translation_beam_alphas : [-1, 0, 1, 2, 3, 4, 5]
2024-02-05 23:14:56,282 cfg.training.reset_best_ckpt       : False
2024-02-05 23:14:56,282 cfg.training.reset_scheduler       : False
2024-02-05 23:14:56,282 cfg.training.reset_optimizer       : False
2024-02-05 23:14:56,282 cfg.training.random_seed           : 42
2024-02-05 23:14:56,282 cfg.training.model_dir             : ./sign_sample_model/fold8/32head/64batch
2024-02-05 23:14:56,282 cfg.training.recognition_loss_weight : 1.0
2024-02-05 23:14:56,283 cfg.training.translation_loss_weight : 1.0
2024-02-05 23:14:56,283 cfg.training.eval_metric           : bleu
2024-02-05 23:14:56,283 cfg.training.optimizer             : adam
2024-02-05 23:14:56,283 cfg.training.learning_rate         : 0.0001
2024-02-05 23:14:56,283 cfg.training.batch_size            : 64
2024-02-05 23:14:56,283 cfg.training.num_valid_log         : 5
2024-02-05 23:14:56,283 cfg.training.epochs                : 50000
2024-02-05 23:14:56,283 cfg.training.early_stopping_metric : eval_metric
2024-02-05 23:14:56,283 cfg.training.batch_type            : sentence
2024-02-05 23:14:56,284 cfg.training.translation_normalization : batch
2024-02-05 23:14:56,284 cfg.training.eval_recognition_beam_size : 1
2024-02-05 23:14:56,284 cfg.training.eval_translation_beam_size : 1
2024-02-05 23:14:56,284 cfg.training.eval_translation_beam_alpha : -1
2024-02-05 23:14:56,284 cfg.training.overwrite             : True
2024-02-05 23:14:56,284 cfg.training.shuffle               : True
2024-02-05 23:14:56,284 cfg.training.use_cuda              : True
2024-02-05 23:14:56,284 cfg.training.translation_max_output_length : 40
2024-02-05 23:14:56,285 cfg.training.keep_last_ckpts       : 1
2024-02-05 23:14:56,285 cfg.training.batch_multiplier      : 1
2024-02-05 23:14:56,285 cfg.training.logging_freq          : 100
2024-02-05 23:14:56,285 cfg.training.validation_freq       : 2000
2024-02-05 23:14:56,285 cfg.training.betas                 : [0.9, 0.998]
2024-02-05 23:14:56,285 cfg.training.scheduling            : plateau
2024-02-05 23:14:56,285 cfg.training.learning_rate_min     : 1e-08
2024-02-05 23:14:56,285 cfg.training.weight_decay          : 0.0001
2024-02-05 23:14:56,286 cfg.training.patience              : 12
2024-02-05 23:14:56,286 cfg.training.decrease_factor       : 0.5
2024-02-05 23:14:56,286 cfg.training.label_smoothing       : 0.0
2024-02-05 23:14:56,286 cfg.model.initializer              : xavier
2024-02-05 23:14:56,286 cfg.model.bias_initializer         : zeros
2024-02-05 23:14:56,286 cfg.model.init_gain                : 1.0
2024-02-05 23:14:56,286 cfg.model.embed_initializer        : xavier
2024-02-05 23:14:56,286 cfg.model.embed_init_gain          : 1.0
2024-02-05 23:14:56,287 cfg.model.tied_softmax             : True
2024-02-05 23:14:56,287 cfg.model.encoder.type             : transformer
2024-02-05 23:14:56,287 cfg.model.encoder.num_layers       : 3
2024-02-05 23:14:56,287 cfg.model.encoder.num_heads        : 32
2024-02-05 23:14:56,287 cfg.model.encoder.embeddings.embedding_dim : 512
2024-02-05 23:14:56,287 cfg.model.encoder.embeddings.scale : False
2024-02-05 23:14:56,287 cfg.model.encoder.embeddings.dropout : 0.1
2024-02-05 23:14:56,287 cfg.model.encoder.embeddings.norm_type : batch
2024-02-05 23:14:56,287 cfg.model.encoder.embeddings.activation_type : softsign
2024-02-05 23:14:56,288 cfg.model.encoder.hidden_size      : 512
2024-02-05 23:14:56,288 cfg.model.encoder.ff_size          : 2048
2024-02-05 23:14:56,288 cfg.model.encoder.dropout          : 0.1
2024-02-05 23:14:56,288 cfg.model.decoder.type             : transformer
2024-02-05 23:14:56,288 cfg.model.decoder.num_layers       : 3
2024-02-05 23:14:56,288 cfg.model.decoder.num_heads        : 32
2024-02-05 23:14:56,288 cfg.model.decoder.embeddings.embedding_dim : 512
2024-02-05 23:14:56,288 cfg.model.decoder.embeddings.scale : False
2024-02-05 23:14:56,289 cfg.model.decoder.embeddings.dropout : 0.1
2024-02-05 23:14:56,289 cfg.model.decoder.embeddings.norm_type : batch
2024-02-05 23:14:56,289 cfg.model.decoder.embeddings.activation_type : softsign
2024-02-05 23:14:56,289 cfg.model.decoder.hidden_size      : 512
2024-02-05 23:14:56,289 cfg.model.decoder.ff_size          : 2048
2024-02-05 23:14:56,289 cfg.model.decoder.dropout          : 0.1
2024-02-05 23:14:56,289 Data set sizes: 
	train 2124,
	valid 708,
	test 708
2024-02-05 23:14:56,289 First training example:
	[GLS] A B C D E
	[TXT] how did she become a champion
2024-02-05 23:14:56,290 First 10 words (gls): (0) <si> (1) <unk> (2) <pad> (3) A (4) B (5) C (6) D (7) E
2024-02-05 23:14:56,290 First 10 words (txt): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) and (6) to (7) in (8) a (9) of
2024-02-05 23:14:56,290 Number of unique glosses (types): 8
2024-02-05 23:14:56,290 Number of unique words (types): 4402
2024-02-05 23:14:56,290 SignModel(
	encoder=TransformerEncoder(num_layers=3, num_heads=32),
	decoder=TransformerDecoder(num_layers=3, num_heads=32),
	sgn_embed=SpatialEmbeddings(embedding_dim=512, input_size=2560),
	txt_embed=Embeddings(embedding_dim=512, vocab_size=4402))
2024-02-05 23:14:56,294 EPOCH 1
2024-02-05 23:15:01,566 Epoch   1: Total Training Recognition Loss 199.08  Total Training Translation Loss 3444.24 
2024-02-05 23:15:01,566 EPOCH 2
2024-02-05 23:15:06,348 Epoch   2: Total Training Recognition Loss 66.37  Total Training Translation Loss 3092.40 
2024-02-05 23:15:06,348 EPOCH 3
2024-02-05 23:15:10,649 [Epoch: 003 Step: 00000100] Batch Recognition Loss:   1.053872 => Gls Tokens per Sec:     2382 || Batch Translation Loss:  51.455860 => Txt Tokens per Sec:     6684 || Lr: 0.000100
2024-02-05 23:15:10,811 Epoch   3: Total Training Recognition Loss 41.30  Total Training Translation Loss 3007.27 
2024-02-05 23:15:10,811 EPOCH 4
2024-02-05 23:15:15,559 Epoch   4: Total Training Recognition Loss 30.55  Total Training Translation Loss 2979.99 
2024-02-05 23:15:15,560 EPOCH 5
2024-02-05 23:15:20,087 Epoch   5: Total Training Recognition Loss 26.58  Total Training Translation Loss 2932.03 
2024-02-05 23:15:20,087 EPOCH 6
2024-02-05 23:15:24,264 [Epoch: 006 Step: 00000200] Batch Recognition Loss:   0.824959 => Gls Tokens per Sec:     2236 || Batch Translation Loss:  88.531090 => Txt Tokens per Sec:     6273 || Lr: 0.000100
2024-02-05 23:15:24,700 Epoch   6: Total Training Recognition Loss 23.94  Total Training Translation Loss 2867.24 
2024-02-05 23:15:24,701 EPOCH 7
2024-02-05 23:15:29,270 Epoch   7: Total Training Recognition Loss 37.79  Total Training Translation Loss 2784.68 
2024-02-05 23:15:29,270 EPOCH 8
2024-02-05 23:15:33,893 Epoch   8: Total Training Recognition Loss 65.34  Total Training Translation Loss 2701.41 
2024-02-05 23:15:33,893 EPOCH 9
2024-02-05 23:15:37,826 [Epoch: 009 Step: 00000300] Batch Recognition Loss:   0.695411 => Gls Tokens per Sec:     2213 || Batch Translation Loss: 103.506454 => Txt Tokens per Sec:     6197 || Lr: 0.000100
2024-02-05 23:15:38,507 Epoch   9: Total Training Recognition Loss 31.36  Total Training Translation Loss 2629.18 
2024-02-05 23:15:38,507 EPOCH 10
2024-02-05 23:15:42,945 Epoch  10: Total Training Recognition Loss 22.09  Total Training Translation Loss 2549.24 
2024-02-05 23:15:42,946 EPOCH 11
2024-02-05 23:15:47,712 Epoch  11: Total Training Recognition Loss 18.71  Total Training Translation Loss 2464.22 
2024-02-05 23:15:47,712 EPOCH 12
2024-02-05 23:15:51,038 [Epoch: 012 Step: 00000400] Batch Recognition Loss:   0.442147 => Gls Tokens per Sec:     2425 || Batch Translation Loss:  40.034283 => Txt Tokens per Sec:     6877 || Lr: 0.000100
2024-02-05 23:15:51,983 Epoch  12: Total Training Recognition Loss 17.31  Total Training Translation Loss 2387.79 
2024-02-05 23:15:51,983 EPOCH 13
2024-02-05 23:15:57,061 Epoch  13: Total Training Recognition Loss 13.40  Total Training Translation Loss 2321.10 
2024-02-05 23:15:57,062 EPOCH 14
2024-02-05 23:16:01,460 Epoch  14: Total Training Recognition Loss 11.29  Total Training Translation Loss 2265.74 
2024-02-05 23:16:01,460 EPOCH 15
2024-02-05 23:16:04,779 [Epoch: 015 Step: 00000500] Batch Recognition Loss:   0.241104 => Gls Tokens per Sec:     2237 || Batch Translation Loss:  67.428558 => Txt Tokens per Sec:     6045 || Lr: 0.000100
2024-02-05 23:16:06,397 Epoch  15: Total Training Recognition Loss 10.23  Total Training Translation Loss 2190.96 
2024-02-05 23:16:06,397 EPOCH 16
2024-02-05 23:16:10,584 Epoch  16: Total Training Recognition Loss 9.19  Total Training Translation Loss 2133.00 
2024-02-05 23:16:10,584 EPOCH 17
2024-02-05 23:16:15,399 Epoch  17: Total Training Recognition Loss 8.25  Total Training Translation Loss 2071.43 
2024-02-05 23:16:15,400 EPOCH 18
2024-02-05 23:16:17,812 [Epoch: 018 Step: 00000600] Batch Recognition Loss:   0.266354 => Gls Tokens per Sec:     2920 || Batch Translation Loss:  24.121902 => Txt Tokens per Sec:     7682 || Lr: 0.000100
2024-02-05 23:16:20,026 Epoch  18: Total Training Recognition Loss 7.44  Total Training Translation Loss 2004.09 
2024-02-05 23:16:20,027 EPOCH 19
2024-02-05 23:16:24,961 Epoch  19: Total Training Recognition Loss 6.68  Total Training Translation Loss 1951.27 
2024-02-05 23:16:24,961 EPOCH 20
2024-02-05 23:16:29,477 Epoch  20: Total Training Recognition Loss 5.81  Total Training Translation Loss 1884.91 
2024-02-05 23:16:29,478 EPOCH 21
2024-02-05 23:16:32,450 [Epoch: 021 Step: 00000700] Batch Recognition Loss:   0.204490 => Gls Tokens per Sec:     2066 || Batch Translation Loss:  68.844452 => Txt Tokens per Sec:     5805 || Lr: 0.000100
2024-02-05 23:16:34,432 Epoch  21: Total Training Recognition Loss 5.60  Total Training Translation Loss 1865.68 
2024-02-05 23:16:34,432 EPOCH 22
2024-02-05 23:16:38,923 Epoch  22: Total Training Recognition Loss 5.20  Total Training Translation Loss 1791.92 
2024-02-05 23:16:38,923 EPOCH 23
2024-02-05 23:16:43,536 Epoch  23: Total Training Recognition Loss 4.89  Total Training Translation Loss 1742.06 
2024-02-05 23:16:43,537 EPOCH 24
2024-02-05 23:16:46,064 [Epoch: 024 Step: 00000800] Batch Recognition Loss:   0.098793 => Gls Tokens per Sec:     2178 || Batch Translation Loss:  50.491665 => Txt Tokens per Sec:     5936 || Lr: 0.000100
2024-02-05 23:16:48,187 Epoch  24: Total Training Recognition Loss 4.57  Total Training Translation Loss 1691.21 
2024-02-05 23:16:48,188 EPOCH 25
2024-02-05 23:16:53,202 Epoch  25: Total Training Recognition Loss 4.42  Total Training Translation Loss 1633.62 
2024-02-05 23:16:53,203 EPOCH 26
2024-02-05 23:16:57,873 Epoch  26: Total Training Recognition Loss 3.99  Total Training Translation Loss 1576.51 
2024-02-05 23:16:57,873 EPOCH 27
2024-02-05 23:16:59,761 [Epoch: 027 Step: 00000900] Batch Recognition Loss:   0.046470 => Gls Tokens per Sec:     2575 || Batch Translation Loss:  44.001110 => Txt Tokens per Sec:     7349 || Lr: 0.000100
2024-02-05 23:17:02,133 Epoch  27: Total Training Recognition Loss 3.59  Total Training Translation Loss 1534.78 
2024-02-05 23:17:02,133 EPOCH 28
2024-02-05 23:17:07,125 Epoch  28: Total Training Recognition Loss 3.54  Total Training Translation Loss 1505.17 
2024-02-05 23:17:07,126 EPOCH 29
2024-02-05 23:17:11,337 Epoch  29: Total Training Recognition Loss 3.51  Total Training Translation Loss 1446.78 
2024-02-05 23:17:11,337 EPOCH 30
2024-02-05 23:17:13,324 [Epoch: 030 Step: 00001000] Batch Recognition Loss:   0.203078 => Gls Tokens per Sec:     2257 || Batch Translation Loss:  16.697739 => Txt Tokens per Sec:     6005 || Lr: 0.000100
2024-02-05 23:17:16,288 Epoch  30: Total Training Recognition Loss 3.52  Total Training Translation Loss 1403.22 
2024-02-05 23:17:16,288 EPOCH 31
2024-02-05 23:17:20,580 Epoch  31: Total Training Recognition Loss 3.41  Total Training Translation Loss 1350.41 
2024-02-05 23:17:20,581 EPOCH 32
2024-02-05 23:17:25,430 Epoch  32: Total Training Recognition Loss 3.24  Total Training Translation Loss 1302.19 
2024-02-05 23:17:25,431 EPOCH 33
2024-02-05 23:17:27,272 [Epoch: 033 Step: 00001100] Batch Recognition Loss:   0.065149 => Gls Tokens per Sec:     2087 || Batch Translation Loss:  37.496822 => Txt Tokens per Sec:     5889 || Lr: 0.000100
2024-02-05 23:17:29,844 Epoch  33: Total Training Recognition Loss 3.15  Total Training Translation Loss 1285.21 
2024-02-05 23:17:29,845 EPOCH 34
2024-02-05 23:17:34,871 Epoch  34: Total Training Recognition Loss 3.34  Total Training Translation Loss 1258.03 
2024-02-05 23:17:34,872 EPOCH 35
2024-02-05 23:17:39,095 Epoch  35: Total Training Recognition Loss 3.54  Total Training Translation Loss 1191.24 
2024-02-05 23:17:39,095 EPOCH 36
2024-02-05 23:17:40,207 [Epoch: 036 Step: 00001200] Batch Recognition Loss:   0.139176 => Gls Tokens per Sec:     2883 || Batch Translation Loss:  44.087296 => Txt Tokens per Sec:     7809 || Lr: 0.000100
2024-02-05 23:17:43,897 Epoch  36: Total Training Recognition Loss 3.08  Total Training Translation Loss 1143.82 
2024-02-05 23:17:43,897 EPOCH 37
2024-02-05 23:17:48,379 Epoch  37: Total Training Recognition Loss 2.98  Total Training Translation Loss 1085.59 
2024-02-05 23:17:48,379 EPOCH 38
2024-02-05 23:17:53,057 Epoch  38: Total Training Recognition Loss 2.79  Total Training Translation Loss 1040.01 
2024-02-05 23:17:53,058 EPOCH 39
2024-02-05 23:17:54,123 [Epoch: 039 Step: 00001300] Batch Recognition Loss:   0.078342 => Gls Tokens per Sec:     2162 || Batch Translation Loss:  32.576775 => Txt Tokens per Sec:     6264 || Lr: 0.000100
2024-02-05 23:17:57,617 Epoch  39: Total Training Recognition Loss 2.73  Total Training Translation Loss 1000.75 
2024-02-05 23:17:57,617 EPOCH 40
2024-02-05 23:18:02,175 Epoch  40: Total Training Recognition Loss 2.65  Total Training Translation Loss 958.59 
2024-02-05 23:18:02,176 EPOCH 41
2024-02-05 23:18:07,126 Epoch  41: Total Training Recognition Loss 2.88  Total Training Translation Loss 928.24 
2024-02-05 23:18:07,126 EPOCH 42
2024-02-05 23:18:08,018 [Epoch: 042 Step: 00001400] Batch Recognition Loss:   0.090345 => Gls Tokens per Sec:     2156 || Batch Translation Loss:  19.919621 => Txt Tokens per Sec:     5788 || Lr: 0.000100
2024-02-05 23:18:12,261 Epoch  42: Total Training Recognition Loss 2.71  Total Training Translation Loss 880.15 
2024-02-05 23:18:12,261 EPOCH 43
2024-02-05 23:18:17,112 Epoch  43: Total Training Recognition Loss 2.70  Total Training Translation Loss 847.64 
2024-02-05 23:18:17,112 EPOCH 44
2024-02-05 23:18:22,026 Epoch  44: Total Training Recognition Loss 2.71  Total Training Translation Loss 829.63 
2024-02-05 23:18:22,027 EPOCH 45
2024-02-05 23:18:22,477 [Epoch: 045 Step: 00001500] Batch Recognition Loss:   0.032952 => Gls Tokens per Sec:     2857 || Batch Translation Loss:  19.728821 => Txt Tokens per Sec:     7594 || Lr: 0.000100
2024-02-05 23:18:26,706 Epoch  45: Total Training Recognition Loss 2.71  Total Training Translation Loss 786.73 
2024-02-05 23:18:26,707 EPOCH 46
2024-02-05 23:18:31,520 Epoch  46: Total Training Recognition Loss 2.66  Total Training Translation Loss 741.77 
2024-02-05 23:18:31,521 EPOCH 47
2024-02-05 23:18:36,293 Epoch  47: Total Training Recognition Loss 2.62  Total Training Translation Loss 717.74 
2024-02-05 23:18:36,293 EPOCH 48
2024-02-05 23:18:36,516 [Epoch: 048 Step: 00001600] Batch Recognition Loss:   0.032640 => Gls Tokens per Sec:     2883 || Batch Translation Loss:  22.582270 => Txt Tokens per Sec:     8293 || Lr: 0.000100
2024-02-05 23:18:41,098 Epoch  48: Total Training Recognition Loss 2.59  Total Training Translation Loss 682.24 
2024-02-05 23:18:41,098 EPOCH 49
2024-02-05 23:18:45,921 Epoch  49: Total Training Recognition Loss 2.57  Total Training Translation Loss 646.29 
2024-02-05 23:18:45,922 EPOCH 50
2024-02-05 23:18:50,685 [Epoch: 050 Step: 00001700] Batch Recognition Loss:   0.059141 => Gls Tokens per Sec:     2231 || Batch Translation Loss:  20.316795 => Txt Tokens per Sec:     6171 || Lr: 0.000100
2024-02-05 23:18:50,685 Epoch  50: Total Training Recognition Loss 2.50  Total Training Translation Loss 604.57 
2024-02-05 23:18:50,686 EPOCH 51
2024-02-05 23:18:55,510 Epoch  51: Total Training Recognition Loss 2.44  Total Training Translation Loss 574.13 
2024-02-05 23:18:55,510 EPOCH 52
2024-02-05 23:18:59,676 Epoch  52: Total Training Recognition Loss 2.22  Total Training Translation Loss 537.61 
2024-02-05 23:18:59,677 EPOCH 53
2024-02-05 23:19:04,489 [Epoch: 053 Step: 00001800] Batch Recognition Loss:   0.072295 => Gls Tokens per Sec:     2074 || Batch Translation Loss:  10.184749 => Txt Tokens per Sec:     5755 || Lr: 0.000100
2024-02-05 23:19:04,735 Epoch  53: Total Training Recognition Loss 2.41  Total Training Translation Loss 507.91 
2024-02-05 23:19:04,735 EPOCH 54
2024-02-05 23:19:09,255 Epoch  54: Total Training Recognition Loss 2.28  Total Training Translation Loss 475.32 
2024-02-05 23:19:09,255 EPOCH 55
2024-02-05 23:19:13,615 Epoch  55: Total Training Recognition Loss 2.23  Total Training Translation Loss 457.24 
2024-02-05 23:19:13,615 EPOCH 56
2024-02-05 23:19:17,745 [Epoch: 056 Step: 00001900] Batch Recognition Loss:   0.055720 => Gls Tokens per Sec:     2263 || Batch Translation Loss:  19.850578 => Txt Tokens per Sec:     6178 || Lr: 0.000100
2024-02-05 23:19:18,528 Epoch  56: Total Training Recognition Loss 2.27  Total Training Translation Loss 434.14 
2024-02-05 23:19:18,528 EPOCH 57
2024-02-05 23:19:22,741 Epoch  57: Total Training Recognition Loss 2.19  Total Training Translation Loss 418.23 
2024-02-05 23:19:22,742 EPOCH 58
2024-02-05 23:19:27,760 Epoch  58: Total Training Recognition Loss 2.28  Total Training Translation Loss 382.21 
2024-02-05 23:19:27,760 EPOCH 59
2024-02-05 23:19:31,348 [Epoch: 059 Step: 00002000] Batch Recognition Loss:   0.074057 => Gls Tokens per Sec:     2425 || Batch Translation Loss:  14.070908 => Txt Tokens per Sec:     6926 || Lr: 0.000100
2024-02-05 23:19:40,134 Hooray! New best validation result [eval_metric]!
2024-02-05 23:19:40,135 Saving new checkpoint.
2024-02-05 23:19:40,382 Validation result at epoch  59, step     2000: duration: 9.0337s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.67966	Translation Loss: 66935.32031	PPL: 800.71826
	Eval Metric: BLEU
	WER 7.56	(DEL: 0.00,	INS: 0.00,	SUB: 7.56)
	BLEU-4 0.67	(BLEU-1: 12.76,	BLEU-2: 4.33,	BLEU-3: 1.69,	BLEU-4: 0.67)
	CHRF 17.07	ROUGE 10.61
2024-02-05 23:19:40,383 Logging Recognition and Translation Outputs
2024-02-05 23:19:40,383 ========================================================================================================================
2024-02-05 23:19:40,383 Logging Sequence: 165_414.00
2024-02-05 23:19:40,383 	Gloss Reference :	A B+C+D+E
2024-02-05 23:19:40,383 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:19:40,384 	Gloss Alignment :	         
2024-02-05 23:19:40,384 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:19:40,386 	Text Reference  :	he  felt sachin was  lucky so   he  always    gave his ******* sweater to   give it   to   the  umpire 
2024-02-05 23:19:40,386 	Text Hypothesis :	ipl has  now    have a     huge fan following on   his batting but     some some some some some players
2024-02-05 23:19:40,386 	Text Alignment  :	S   S    S      S    S     S    S   S         S        I       S       S    S    S    S    S    S      
2024-02-05 23:19:40,386 ========================================================================================================================
2024-02-05 23:19:40,386 Logging Sequence: 169_268.00
2024-02-05 23:19:40,386 	Gloss Reference :	A B+C+D+E
2024-02-05 23:19:40,386 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:19:40,386 	Gloss Alignment :	         
2024-02-05 23:19:40,387 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:19:40,388 	Text Reference  :	shami supports arshdeep and  many     fans supported him as **** **** **** ***** *** ** ****** well 
2024-02-05 23:19:40,388 	Text Hypothesis :	***** ******** fans     were supposed to   see       him as they have been ruled out on social media
2024-02-05 23:19:40,388 	Text Alignment  :	D     D        S        S    S        S    S                I    I    I    I     I   I  I      S    
2024-02-05 23:19:40,388 ========================================================================================================================
2024-02-05 23:19:40,388 Logging Sequence: 172_15.00
2024-02-05 23:19:40,388 	Gloss Reference :	A B+C+D+E
2024-02-05 23:19:40,388 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:19:40,389 	Gloss Alignment :	         
2024-02-05 23:19:40,389 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:19:40,391 	Text Reference  :	now in the final match   on    28   may 2023    the ***** two teams  were up  against each other at   the  same venue 
2024-02-05 23:19:40,391 	Text Hypothesis :	*** ** *** ***** however after this was stopped the world cup trophy he   was covered with his   bare with his  jersey
2024-02-05 23:19:40,391 	Text Alignment  :	D   D  D   D     S       S     S    S   S           I     S   S      S    S   S       S    S     S    S    S    S     
2024-02-05 23:19:40,391 ========================================================================================================================
2024-02-05 23:19:40,391 Logging Sequence: 96_158.00
2024-02-05 23:19:40,392 	Gloss Reference :	A B+C+D+E    
2024-02-05 23:19:40,392 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-05 23:19:40,392 	Gloss Alignment :	  S          
2024-02-05 23:19:40,392 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:19:40,393 	Text Reference  :	after this   pandya fell     on his knees in *** **** * ***** ** *** **** **** **** disappointment
2024-02-05 23:19:40,393 	Text Hypothesis :	the   couple were   supposed to be  held  in the next 5 years of the next next next next          
2024-02-05 23:19:40,393 	Text Alignment  :	S     S      S      S        S  S   S        I   I    I I     I  I   I    I    I    S             
2024-02-05 23:19:40,393 ========================================================================================================================
2024-02-05 23:19:40,393 Logging Sequence: 152_73.00
2024-02-05 23:19:40,394 	Gloss Reference :	A B+C+D+E
2024-02-05 23:19:40,394 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:19:40,394 	Gloss Alignment :	         
2024-02-05 23:19:40,394 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:19:40,395 	Text Reference  :	******* *** ***** eventually he   too got out           by ******* ***** *** ******** **** shaheen afridi   
2024-02-05 23:19:40,395 	Text Hypothesis :	however the match was        held at  his sportsmanship by shaking hands and pakistan will be      available
2024-02-05 23:19:40,395 	Text Alignment  :	I       I   I     S          S    S   S   S                I       I     I   I        I    S       S        
2024-02-05 23:19:40,395 ========================================================================================================================
2024-02-05 23:19:41,088 Epoch  59: Total Training Recognition Loss 2.05  Total Training Translation Loss 351.06 
2024-02-05 23:19:41,089 EPOCH 60
2024-02-05 23:19:46,152 Epoch  60: Total Training Recognition Loss 2.04  Total Training Translation Loss 345.61 
2024-02-05 23:19:46,153 EPOCH 61
2024-02-05 23:19:50,543 Epoch  61: Total Training Recognition Loss 2.03  Total Training Translation Loss 320.42 
2024-02-05 23:19:50,544 EPOCH 62
2024-02-05 23:19:54,317 [Epoch: 062 Step: 00002100] Batch Recognition Loss:   0.037388 => Gls Tokens per Sec:     2137 || Batch Translation Loss:   9.629452 => Txt Tokens per Sec:     5969 || Lr: 0.000100
2024-02-05 23:19:55,302 Epoch  62: Total Training Recognition Loss 2.02  Total Training Translation Loss 312.82 
2024-02-05 23:19:55,302 EPOCH 63
2024-02-05 23:19:59,809 Epoch  63: Total Training Recognition Loss 1.94  Total Training Translation Loss 292.88 
2024-02-05 23:19:59,809 EPOCH 64
2024-02-05 23:20:04,662 Epoch  64: Total Training Recognition Loss 2.02  Total Training Translation Loss 274.66 
2024-02-05 23:20:04,662 EPOCH 65
2024-02-05 23:20:07,781 [Epoch: 065 Step: 00002200] Batch Recognition Loss:   0.029176 => Gls Tokens per Sec:     2380 || Batch Translation Loss:   5.633029 => Txt Tokens per Sec:     6196 || Lr: 0.000100
2024-02-05 23:20:09,606 Epoch  65: Total Training Recognition Loss 1.86  Total Training Translation Loss 248.06 
2024-02-05 23:20:09,606 EPOCH 66
2024-02-05 23:20:14,553 Epoch  66: Total Training Recognition Loss 1.67  Total Training Translation Loss 227.64 
2024-02-05 23:20:14,554 EPOCH 67
2024-02-05 23:20:19,251 Epoch  67: Total Training Recognition Loss 1.75  Total Training Translation Loss 209.98 
2024-02-05 23:20:19,252 EPOCH 68
2024-02-05 23:20:21,895 [Epoch: 068 Step: 00002300] Batch Recognition Loss:   0.070652 => Gls Tokens per Sec:     2566 || Batch Translation Loss:   2.952607 => Txt Tokens per Sec:     6897 || Lr: 0.000100
2024-02-05 23:20:24,038 Epoch  68: Total Training Recognition Loss 1.78  Total Training Translation Loss 197.22 
2024-02-05 23:20:24,039 EPOCH 69
2024-02-05 23:20:28,687 Epoch  69: Total Training Recognition Loss 1.65  Total Training Translation Loss 187.15 
2024-02-05 23:20:28,687 EPOCH 70
2024-02-05 23:20:33,624 Epoch  70: Total Training Recognition Loss 1.64  Total Training Translation Loss 174.44 
2024-02-05 23:20:33,625 EPOCH 71
2024-02-05 23:20:36,730 [Epoch: 071 Step: 00002400] Batch Recognition Loss:   0.046817 => Gls Tokens per Sec:     1978 || Batch Translation Loss:   5.362587 => Txt Tokens per Sec:     5742 || Lr: 0.000100
2024-02-05 23:20:38,335 Epoch  71: Total Training Recognition Loss 1.59  Total Training Translation Loss 168.98 
2024-02-05 23:20:38,336 EPOCH 72
2024-02-05 23:20:43,287 Epoch  72: Total Training Recognition Loss 1.45  Total Training Translation Loss 153.31 
2024-02-05 23:20:43,288 EPOCH 73
2024-02-05 23:20:48,089 Epoch  73: Total Training Recognition Loss 1.44  Total Training Translation Loss 144.45 
2024-02-05 23:20:48,089 EPOCH 74
2024-02-05 23:20:50,464 [Epoch: 074 Step: 00002500] Batch Recognition Loss:   0.027276 => Gls Tokens per Sec:     2317 || Batch Translation Loss:   5.390568 => Txt Tokens per Sec:     6359 || Lr: 0.000100
2024-02-05 23:20:53,085 Epoch  74: Total Training Recognition Loss 1.38  Total Training Translation Loss 135.70 
2024-02-05 23:20:53,086 EPOCH 75
2024-02-05 23:20:57,779 Epoch  75: Total Training Recognition Loss 1.44  Total Training Translation Loss 131.07 
2024-02-05 23:20:57,780 EPOCH 76
2024-02-05 23:21:02,714 Epoch  76: Total Training Recognition Loss 1.36  Total Training Translation Loss 121.60 
2024-02-05 23:21:02,715 EPOCH 77
2024-02-05 23:21:04,892 [Epoch: 077 Step: 00002600] Batch Recognition Loss:   0.022519 => Gls Tokens per Sec:     2353 || Batch Translation Loss:   3.675611 => Txt Tokens per Sec:     6771 || Lr: 0.000100
2024-02-05 23:21:07,461 Epoch  77: Total Training Recognition Loss 1.23  Total Training Translation Loss 114.25 
2024-02-05 23:21:07,461 EPOCH 78
2024-02-05 23:21:12,240 Epoch  78: Total Training Recognition Loss 1.27  Total Training Translation Loss 108.61 
2024-02-05 23:21:12,241 EPOCH 79
2024-02-05 23:21:17,100 Epoch  79: Total Training Recognition Loss 1.25  Total Training Translation Loss 101.37 
2024-02-05 23:21:17,101 EPOCH 80
2024-02-05 23:21:18,804 [Epoch: 080 Step: 00002700] Batch Recognition Loss:   0.092703 => Gls Tokens per Sec:     2631 || Batch Translation Loss:   0.993097 => Txt Tokens per Sec:     7261 || Lr: 0.000100
2024-02-05 23:21:21,861 Epoch  80: Total Training Recognition Loss 1.22  Total Training Translation Loss 94.02 
2024-02-05 23:21:21,861 EPOCH 81
2024-02-05 23:21:26,728 Epoch  81: Total Training Recognition Loss 1.29  Total Training Translation Loss 91.10 
2024-02-05 23:21:26,728 EPOCH 82
2024-02-05 23:21:31,477 Epoch  82: Total Training Recognition Loss 1.06  Total Training Translation Loss 86.87 
2024-02-05 23:21:31,477 EPOCH 83
2024-02-05 23:21:33,353 [Epoch: 083 Step: 00002800] Batch Recognition Loss:   0.015062 => Gls Tokens per Sec:     1910 || Batch Translation Loss:   2.440210 => Txt Tokens per Sec:     5662 || Lr: 0.000100
2024-02-05 23:21:36,330 Epoch  83: Total Training Recognition Loss 1.19  Total Training Translation Loss 84.00 
2024-02-05 23:21:36,330 EPOCH 84
2024-02-05 23:21:40,494 Epoch  84: Total Training Recognition Loss 1.13  Total Training Translation Loss 80.68 
2024-02-05 23:21:40,494 EPOCH 85
2024-02-05 23:21:45,329 Epoch  85: Total Training Recognition Loss 1.10  Total Training Translation Loss 75.89 
2024-02-05 23:21:45,330 EPOCH 86
2024-02-05 23:21:46,833 [Epoch: 086 Step: 00002900] Batch Recognition Loss:   0.033407 => Gls Tokens per Sec:     2129 || Batch Translation Loss:   2.591701 => Txt Tokens per Sec:     5853 || Lr: 0.000100
2024-02-05 23:21:49,765 Epoch  86: Total Training Recognition Loss 1.03  Total Training Translation Loss 71.91 
2024-02-05 23:21:49,765 EPOCH 87
2024-02-05 23:21:54,436 Epoch  87: Total Training Recognition Loss 1.03  Total Training Translation Loss 67.45 
2024-02-05 23:21:54,437 EPOCH 88
2024-02-05 23:21:58,955 Epoch  88: Total Training Recognition Loss 0.97  Total Training Translation Loss 63.69 
2024-02-05 23:21:58,955 EPOCH 89
2024-02-05 23:22:00,064 [Epoch: 089 Step: 00003000] Batch Recognition Loss:   0.030620 => Gls Tokens per Sec:     2310 || Batch Translation Loss:   2.054914 => Txt Tokens per Sec:     6662 || Lr: 0.000100
2024-02-05 23:22:03,511 Epoch  89: Total Training Recognition Loss 0.98  Total Training Translation Loss 60.84 
2024-02-05 23:22:03,511 EPOCH 90
2024-02-05 23:22:08,205 Epoch  90: Total Training Recognition Loss 0.91  Total Training Translation Loss 58.68 
2024-02-05 23:22:08,205 EPOCH 91
2024-02-05 23:22:12,746 Epoch  91: Total Training Recognition Loss 0.90  Total Training Translation Loss 56.43 
2024-02-05 23:22:12,746 EPOCH 92
2024-02-05 23:22:13,466 [Epoch: 092 Step: 00003100] Batch Recognition Loss:   0.013087 => Gls Tokens per Sec:     2670 || Batch Translation Loss:   1.722710 => Txt Tokens per Sec:     7122 || Lr: 0.000100
2024-02-05 23:22:17,448 Epoch  92: Total Training Recognition Loss 0.84  Total Training Translation Loss 53.56 
2024-02-05 23:22:17,448 EPOCH 93
2024-02-05 23:22:21,807 Epoch  93: Total Training Recognition Loss 0.88  Total Training Translation Loss 51.24 
2024-02-05 23:22:21,808 EPOCH 94
2024-02-05 23:22:26,745 Epoch  94: Total Training Recognition Loss 0.73  Total Training Translation Loss 48.51 
2024-02-05 23:22:26,745 EPOCH 95
2024-02-05 23:22:27,188 [Epoch: 095 Step: 00003200] Batch Recognition Loss:   0.026714 => Gls Tokens per Sec:     2900 || Batch Translation Loss:   0.920087 => Txt Tokens per Sec:     7760 || Lr: 0.000100
2024-02-05 23:22:30,927 Epoch  95: Total Training Recognition Loss 0.82  Total Training Translation Loss 47.02 
2024-02-05 23:22:30,928 EPOCH 96
2024-02-05 23:22:35,908 Epoch  96: Total Training Recognition Loss 0.73  Total Training Translation Loss 46.02 
2024-02-05 23:22:35,908 EPOCH 97
2024-02-05 23:22:40,087 Epoch  97: Total Training Recognition Loss 0.74  Total Training Translation Loss 44.67 
2024-02-05 23:22:40,087 EPOCH 98
2024-02-05 23:22:40,284 [Epoch: 098 Step: 00003300] Batch Recognition Loss:   0.023538 => Gls Tokens per Sec:     3265 || Batch Translation Loss:   1.078201 => Txt Tokens per Sec:     7954 || Lr: 0.000100
2024-02-05 23:22:44,985 Epoch  98: Total Training Recognition Loss 0.81  Total Training Translation Loss 43.67 
2024-02-05 23:22:44,985 EPOCH 99
2024-02-05 23:22:49,367 Epoch  99: Total Training Recognition Loss 0.72  Total Training Translation Loss 40.87 
2024-02-05 23:22:49,367 EPOCH 100
2024-02-05 23:22:54,209 [Epoch: 100 Step: 00003400] Batch Recognition Loss:   0.020821 => Gls Tokens per Sec:     2194 || Batch Translation Loss:   0.893569 => Txt Tokens per Sec:     6069 || Lr: 0.000100
2024-02-05 23:22:54,210 Epoch 100: Total Training Recognition Loss 0.67  Total Training Translation Loss 40.94 
2024-02-05 23:22:54,210 EPOCH 101
2024-02-05 23:22:58,628 Epoch 101: Total Training Recognition Loss 0.68  Total Training Translation Loss 39.23 
2024-02-05 23:22:58,628 EPOCH 102
2024-02-05 23:23:03,386 Epoch 102: Total Training Recognition Loss 0.67  Total Training Translation Loss 37.28 
2024-02-05 23:23:03,387 EPOCH 103
2024-02-05 23:23:07,648 [Epoch: 103 Step: 00003500] Batch Recognition Loss:   0.029991 => Gls Tokens per Sec:     2343 || Batch Translation Loss:   0.526594 => Txt Tokens per Sec:     6497 || Lr: 0.000100
2024-02-05 23:23:07,857 Epoch 103: Total Training Recognition Loss 0.60  Total Training Translation Loss 36.12 
2024-02-05 23:23:07,857 EPOCH 104
2024-02-05 23:23:12,373 Epoch 104: Total Training Recognition Loss 0.69  Total Training Translation Loss 35.16 
2024-02-05 23:23:12,374 EPOCH 105
2024-02-05 23:23:17,171 Epoch 105: Total Training Recognition Loss 0.64  Total Training Translation Loss 33.93 
2024-02-05 23:23:17,171 EPOCH 106
2024-02-05 23:23:21,227 [Epoch: 106 Step: 00003600] Batch Recognition Loss:   0.013351 => Gls Tokens per Sec:     2305 || Batch Translation Loss:   1.180814 => Txt Tokens per Sec:     6348 || Lr: 0.000100
2024-02-05 23:23:21,754 Epoch 106: Total Training Recognition Loss 0.59  Total Training Translation Loss 33.40 
2024-02-05 23:23:21,755 EPOCH 107
2024-02-05 23:23:26,329 Epoch 107: Total Training Recognition Loss 0.58  Total Training Translation Loss 32.44 
2024-02-05 23:23:26,329 EPOCH 108
2024-02-05 23:23:30,658 Epoch 108: Total Training Recognition Loss 0.55  Total Training Translation Loss 30.37 
2024-02-05 23:23:30,658 EPOCH 109
2024-02-05 23:23:35,028 [Epoch: 109 Step: 00003700] Batch Recognition Loss:   0.018822 => Gls Tokens per Sec:     1992 || Batch Translation Loss:   0.134283 => Txt Tokens per Sec:     5622 || Lr: 0.000100
2024-02-05 23:23:35,628 Epoch 109: Total Training Recognition Loss 0.57  Total Training Translation Loss 30.26 
2024-02-05 23:23:35,629 EPOCH 110
2024-02-05 23:23:39,881 Epoch 110: Total Training Recognition Loss 0.51  Total Training Translation Loss 30.24 
2024-02-05 23:23:39,882 EPOCH 111
2024-02-05 23:23:44,812 Epoch 111: Total Training Recognition Loss 0.58  Total Training Translation Loss 27.90 
2024-02-05 23:23:44,813 EPOCH 112
2024-02-05 23:23:48,115 [Epoch: 112 Step: 00003800] Batch Recognition Loss:   0.027075 => Gls Tokens per Sec:     2442 || Batch Translation Loss:   0.291939 => Txt Tokens per Sec:     6829 || Lr: 0.000100
2024-02-05 23:23:48,990 Epoch 112: Total Training Recognition Loss 0.53  Total Training Translation Loss 27.82 
2024-02-05 23:23:48,990 EPOCH 113
2024-02-05 23:23:53,952 Epoch 113: Total Training Recognition Loss 0.45  Total Training Translation Loss 27.52 
2024-02-05 23:23:53,952 EPOCH 114
2024-02-05 23:23:58,315 Epoch 114: Total Training Recognition Loss 0.58  Total Training Translation Loss 25.79 
2024-02-05 23:23:58,315 EPOCH 115
2024-02-05 23:24:01,874 [Epoch: 115 Step: 00003900] Batch Recognition Loss:   0.019903 => Gls Tokens per Sec:     2085 || Batch Translation Loss:   0.567159 => Txt Tokens per Sec:     5676 || Lr: 0.000100
2024-02-05 23:24:03,338 Epoch 115: Total Training Recognition Loss 0.57  Total Training Translation Loss 29.56 
2024-02-05 23:24:03,339 EPOCH 116
2024-02-05 23:24:07,992 Epoch 116: Total Training Recognition Loss 0.52  Total Training Translation Loss 28.49 
2024-02-05 23:24:07,992 EPOCH 117
2024-02-05 23:24:12,246 Epoch 117: Total Training Recognition Loss 0.67  Total Training Translation Loss 29.08 
2024-02-05 23:24:12,247 EPOCH 118
2024-02-05 23:24:15,378 [Epoch: 118 Step: 00004000] Batch Recognition Loss:   0.018193 => Gls Tokens per Sec:     2166 || Batch Translation Loss:   0.570639 => Txt Tokens per Sec:     5842 || Lr: 0.000100
2024-02-05 23:24:23,532 Validation result at epoch 118, step     4000: duration: 8.1526s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.69559	Translation Loss: 80010.64062	PPL: 2955.65112
	Eval Metric: BLEU
	WER 6.07	(DEL: 0.00,	INS: 0.00,	SUB: 6.07)
	BLEU-4 0.54	(BLEU-1: 11.20,	BLEU-2: 3.65,	BLEU-3: 1.24,	BLEU-4: 0.54)
	CHRF 16.21	ROUGE 10.19
2024-02-05 23:24:23,533 Logging Recognition and Translation Outputs
2024-02-05 23:24:23,533 ========================================================================================================================
2024-02-05 23:24:23,533 Logging Sequence: 112_165.00
2024-02-05 23:24:23,534 	Gloss Reference :	A B+C+D+E
2024-02-05 23:24:23,534 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:24:23,534 	Gloss Alignment :	         
2024-02-05 23:24:23,534 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:24:23,535 	Text Reference  :	the narendra modi stadium will be       the home for  the  ahmedabad-based franchise
2024-02-05 23:24:23,535 	Text Hypothesis :	*** this     was  india'  best olympics as  they will lose in              dubai    
2024-02-05 23:24:23,535 	Text Alignment  :	D   S        S    S       S    S        S   S    S    S    S               S        
2024-02-05 23:24:23,535 ========================================================================================================================
2024-02-05 23:24:23,536 Logging Sequence: 176_154.00
2024-02-05 23:24:23,536 	Gloss Reference :	A B+C+D+E
2024-02-05 23:24:23,536 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:24:23,536 	Gloss Alignment :	         
2024-02-05 23:24:23,536 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:24:23,537 	Text Reference  :	* ** dahiya    could   potentially bring home india's second gold medal
2024-02-05 23:24:23,537 	Text Hypothesis :	i am extremely sadened by          her   win  and     lost   the  match
2024-02-05 23:24:23,537 	Text Alignment  :	I I  S         S       S           S     S    S       S      S    S    
2024-02-05 23:24:23,537 ========================================================================================================================
2024-02-05 23:24:23,537 Logging Sequence: 94_2.00
2024-02-05 23:24:23,538 	Gloss Reference :	A B+C+D+E
2024-02-05 23:24:23,538 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:24:23,538 	Gloss Alignment :	         
2024-02-05 23:24:23,538 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:24:23,540 	Text Reference  :	***** *** *** the icc  odi   men'    world cup 2023     will  be     hosted     by           india on  5th   october   2023         
2024-02-05 23:24:23,540 	Text Hypothesis :	india did not win this match between india and pakistan south africa bangladesh participated in    the world athletics championships
2024-02-05 23:24:23,540 	Text Alignment  :	I     I   I   S   S    S     S       S     S   S        S     S      S          S            S     S   S     S         S            
2024-02-05 23:24:23,540 ========================================================================================================================
2024-02-05 23:24:23,540 Logging Sequence: 165_453.00
2024-02-05 23:24:23,541 	Gloss Reference :	A B+C+D+E
2024-02-05 23:24:23,541 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:24:23,541 	Gloss Alignment :	         
2024-02-05 23:24:23,541 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:24:23,542 	Text Reference  :	icc did not  agree    to sehwag' decision of  wearing a  numberless jersey
2024-02-05 23:24:23,542 	Text Hypothesis :	he  has been selected to ******* see      him luck    in the        match 
2024-02-05 23:24:23,542 	Text Alignment  :	S   S   S    S           D       S        S   S       S  S          S     
2024-02-05 23:24:23,542 ========================================================================================================================
2024-02-05 23:24:23,543 Logging Sequence: 139_46.00
2024-02-05 23:24:23,543 	Gloss Reference :	A B+C+D+E
2024-02-05 23:24:23,543 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:24:23,543 	Gloss Alignment :	         
2024-02-05 23:24:23,543 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:24:23,544 	Text Reference  :	everyone thought it would be a one sided match because morocco is an      amateur team and belgium ranks 2nd       in     the world
2024-02-05 23:24:23,544 	Text Hypothesis :	******** ******* ** ***** ** * *** ***** ***** ******* ******* ** another indian  team *** played  in    argentina during the match
2024-02-05 23:24:23,545 	Text Alignment  :	D        D       D  D     D  D D   D     D     D       D       D  S       S            D   S       S     S         S          S    
2024-02-05 23:24:23,545 ========================================================================================================================
2024-02-05 23:24:25,446 Epoch 118: Total Training Recognition Loss 0.64  Total Training Translation Loss 29.45 
2024-02-05 23:24:25,446 EPOCH 119
2024-02-05 23:24:30,027 Epoch 119: Total Training Recognition Loss 0.76  Total Training Translation Loss 31.49 
2024-02-05 23:24:30,027 EPOCH 120
2024-02-05 23:24:34,747 Epoch 120: Total Training Recognition Loss 0.78  Total Training Translation Loss 27.55 
2024-02-05 23:24:34,748 EPOCH 121
2024-02-05 23:24:37,372 [Epoch: 121 Step: 00004100] Batch Recognition Loss:   0.012983 => Gls Tokens per Sec:     2341 || Batch Translation Loss:   1.152667 => Txt Tokens per Sec:     6541 || Lr: 0.000100
2024-02-05 23:24:39,242 Epoch 121: Total Training Recognition Loss 1.13  Total Training Translation Loss 38.05 
2024-02-05 23:24:39,242 EPOCH 122
2024-02-05 23:24:43,970 Epoch 122: Total Training Recognition Loss 0.68  Total Training Translation Loss 32.71 
2024-02-05 23:24:43,971 EPOCH 123
2024-02-05 23:24:48,549 Epoch 123: Total Training Recognition Loss 0.54  Total Training Translation Loss 30.74 
2024-02-05 23:24:48,549 EPOCH 124
2024-02-05 23:24:50,557 [Epoch: 124 Step: 00004200] Batch Recognition Loss:   0.005573 => Gls Tokens per Sec:     2869 || Batch Translation Loss:   0.715268 => Txt Tokens per Sec:     7718 || Lr: 0.000100
2024-02-05 23:24:53,124 Epoch 124: Total Training Recognition Loss 0.55  Total Training Translation Loss 27.64 
2024-02-05 23:24:53,124 EPOCH 125
2024-02-05 23:24:57,746 Epoch 125: Total Training Recognition Loss 0.48  Total Training Translation Loss 27.32 
2024-02-05 23:24:57,746 EPOCH 126
2024-02-05 23:25:01,839 Epoch 126: Total Training Recognition Loss 0.45  Total Training Translation Loss 20.18 
2024-02-05 23:25:01,840 EPOCH 127
2024-02-05 23:25:04,369 [Epoch: 127 Step: 00004300] Batch Recognition Loss:   0.027023 => Gls Tokens per Sec:     2027 || Batch Translation Loss:   0.278682 => Txt Tokens per Sec:     5470 || Lr: 0.000100
2024-02-05 23:25:06,871 Epoch 127: Total Training Recognition Loss 0.44  Total Training Translation Loss 18.14 
2024-02-05 23:25:06,872 EPOCH 128
2024-02-05 23:25:11,116 Epoch 128: Total Training Recognition Loss 0.43  Total Training Translation Loss 19.09 
2024-02-05 23:25:11,116 EPOCH 129
2024-02-05 23:25:16,028 Epoch 129: Total Training Recognition Loss 0.40  Total Training Translation Loss 16.24 
2024-02-05 23:25:16,028 EPOCH 130
2024-02-05 23:25:18,057 [Epoch: 130 Step: 00004400] Batch Recognition Loss:   0.008584 => Gls Tokens per Sec:     2080 || Batch Translation Loss:   0.630196 => Txt Tokens per Sec:     5936 || Lr: 0.000100
2024-02-05 23:25:20,366 Epoch 130: Total Training Recognition Loss 0.39  Total Training Translation Loss 14.88 
2024-02-05 23:25:20,366 EPOCH 131
2024-02-05 23:25:25,407 Epoch 131: Total Training Recognition Loss 0.30  Total Training Translation Loss 13.85 
2024-02-05 23:25:25,408 EPOCH 132
2024-02-05 23:25:30,051 Epoch 132: Total Training Recognition Loss 0.33  Total Training Translation Loss 12.99 
2024-02-05 23:25:30,051 EPOCH 133
2024-02-05 23:25:31,605 [Epoch: 133 Step: 00004500] Batch Recognition Loss:   0.029517 => Gls Tokens per Sec:     2473 || Batch Translation Loss:   0.119193 => Txt Tokens per Sec:     6862 || Lr: 0.000100
2024-02-05 23:25:34,341 Epoch 133: Total Training Recognition Loss 0.34  Total Training Translation Loss 12.68 
2024-02-05 23:25:34,341 EPOCH 134
2024-02-05 23:25:39,264 Epoch 134: Total Training Recognition Loss 0.31  Total Training Translation Loss 13.67 
2024-02-05 23:25:39,264 EPOCH 135
2024-02-05 23:25:43,420 Epoch 135: Total Training Recognition Loss 0.30  Total Training Translation Loss 14.19 
2024-02-05 23:25:43,420 EPOCH 136
2024-02-05 23:25:44,915 [Epoch: 136 Step: 00004600] Batch Recognition Loss:   0.013401 => Gls Tokens per Sec:     2141 || Batch Translation Loss:   0.529784 => Txt Tokens per Sec:     5950 || Lr: 0.000100
2024-02-05 23:25:48,406 Epoch 136: Total Training Recognition Loss 0.27  Total Training Translation Loss 13.88 
2024-02-05 23:25:48,406 EPOCH 137
2024-02-05 23:25:52,669 Epoch 137: Total Training Recognition Loss 0.27  Total Training Translation Loss 13.79 
2024-02-05 23:25:52,670 EPOCH 138
2024-02-05 23:25:57,549 Epoch 138: Total Training Recognition Loss 0.27  Total Training Translation Loss 14.43 
2024-02-05 23:25:57,549 EPOCH 139
2024-02-05 23:25:58,899 [Epoch: 139 Step: 00004700] Batch Recognition Loss:   0.016766 => Gls Tokens per Sec:     1898 || Batch Translation Loss:   0.497621 => Txt Tokens per Sec:     5901 || Lr: 0.000100
2024-02-05 23:26:01,997 Epoch 139: Total Training Recognition Loss 0.33  Total Training Translation Loss 16.56 
2024-02-05 23:26:01,998 EPOCH 140
2024-02-05 23:26:06,949 Epoch 140: Total Training Recognition Loss 0.32  Total Training Translation Loss 15.33 
2024-02-05 23:26:06,950 EPOCH 141
2024-02-05 23:26:11,185 Epoch 141: Total Training Recognition Loss 0.35  Total Training Translation Loss 13.81 
2024-02-05 23:26:11,185 EPOCH 142
2024-02-05 23:26:11,800 [Epoch: 142 Step: 00004800] Batch Recognition Loss:   0.004892 => Gls Tokens per Sec:     3127 || Batch Translation Loss:   0.285721 => Txt Tokens per Sec:     7989 || Lr: 0.000100
2024-02-05 23:26:16,063 Epoch 142: Total Training Recognition Loss 0.33  Total Training Translation Loss 12.32 
2024-02-05 23:26:16,064 EPOCH 143
2024-02-05 23:26:20,530 Epoch 143: Total Training Recognition Loss 0.28  Total Training Translation Loss 12.10 
2024-02-05 23:26:20,530 EPOCH 144
2024-02-05 23:26:25,233 Epoch 144: Total Training Recognition Loss 0.29  Total Training Translation Loss 12.73 
2024-02-05 23:26:25,233 EPOCH 145
2024-02-05 23:26:25,778 [Epoch: 145 Step: 00004900] Batch Recognition Loss:   0.001553 => Gls Tokens per Sec:     1881 || Batch Translation Loss:   0.054739 => Txt Tokens per Sec:     5041 || Lr: 0.000100
2024-02-05 23:26:29,798 Epoch 145: Total Training Recognition Loss 0.29  Total Training Translation Loss 12.69 
2024-02-05 23:26:29,798 EPOCH 146
2024-02-05 23:26:34,374 Epoch 146: Total Training Recognition Loss 0.25  Total Training Translation Loss 12.69 
2024-02-05 23:26:34,375 EPOCH 147
2024-02-05 23:26:39,061 Epoch 147: Total Training Recognition Loss 0.33  Total Training Translation Loss 12.66 
2024-02-05 23:26:39,062 EPOCH 148
2024-02-05 23:26:39,237 [Epoch: 148 Step: 00005000] Batch Recognition Loss:   0.009567 => Gls Tokens per Sec:     3675 || Batch Translation Loss:   0.260944 => Txt Tokens per Sec:     9101 || Lr: 0.000100
2024-02-05 23:26:43,395 Epoch 148: Total Training Recognition Loss 0.52  Total Training Translation Loss 11.62 
2024-02-05 23:26:43,396 EPOCH 149
2024-02-05 23:26:48,262 Epoch 149: Total Training Recognition Loss 0.66  Total Training Translation Loss 12.07 
2024-02-05 23:26:48,263 EPOCH 150
2024-02-05 23:26:53,276 [Epoch: 150 Step: 00005100] Batch Recognition Loss:   0.004319 => Gls Tokens per Sec:     2119 || Batch Translation Loss:   0.417189 => Txt Tokens per Sec:     5862 || Lr: 0.000100
2024-02-05 23:26:53,277 Epoch 150: Total Training Recognition Loss 0.38  Total Training Translation Loss 12.26 
2024-02-05 23:26:53,277 EPOCH 151
2024-02-05 23:26:57,970 Epoch 151: Total Training Recognition Loss 0.33  Total Training Translation Loss 12.61 
2024-02-05 23:26:57,970 EPOCH 152
2024-02-05 23:27:02,962 Epoch 152: Total Training Recognition Loss 0.32  Total Training Translation Loss 12.51 
2024-02-05 23:27:02,962 EPOCH 153
2024-02-05 23:27:07,420 [Epoch: 153 Step: 00005200] Batch Recognition Loss:   0.012167 => Gls Tokens per Sec:     2239 || Batch Translation Loss:   0.274089 => Txt Tokens per Sec:     6176 || Lr: 0.000100
2024-02-05 23:27:07,700 Epoch 153: Total Training Recognition Loss 0.24  Total Training Translation Loss 11.96 
2024-02-05 23:27:07,701 EPOCH 154
2024-02-05 23:27:12,584 Epoch 154: Total Training Recognition Loss 0.28  Total Training Translation Loss 14.03 
2024-02-05 23:27:12,585 EPOCH 155
2024-02-05 23:27:17,324 Epoch 155: Total Training Recognition Loss 0.27  Total Training Translation Loss 14.61 
2024-02-05 23:27:17,324 EPOCH 156
2024-02-05 23:27:21,050 [Epoch: 156 Step: 00005300] Batch Recognition Loss:   0.004552 => Gls Tokens per Sec:     2577 || Batch Translation Loss:   0.614373 => Txt Tokens per Sec:     7220 || Lr: 0.000100
2024-02-05 23:27:21,453 Epoch 156: Total Training Recognition Loss 0.28  Total Training Translation Loss 14.05 
2024-02-05 23:27:21,453 EPOCH 157
2024-02-05 23:27:26,392 Epoch 157: Total Training Recognition Loss 0.27  Total Training Translation Loss 14.46 
2024-02-05 23:27:26,393 EPOCH 158
2024-02-05 23:27:31,359 Epoch 158: Total Training Recognition Loss 0.29  Total Training Translation Loss 15.54 
2024-02-05 23:27:31,360 EPOCH 159
2024-02-05 23:27:35,216 [Epoch: 159 Step: 00005400] Batch Recognition Loss:   0.008576 => Gls Tokens per Sec:     2257 || Batch Translation Loss:   0.206137 => Txt Tokens per Sec:     6166 || Lr: 0.000100
2024-02-05 23:27:36,405 Epoch 159: Total Training Recognition Loss 0.24  Total Training Translation Loss 12.06 
2024-02-05 23:27:36,406 EPOCH 160
2024-02-05 23:27:41,237 Epoch 160: Total Training Recognition Loss 0.27  Total Training Translation Loss 10.99 
2024-02-05 23:27:41,238 EPOCH 161
2024-02-05 23:27:46,216 Epoch 161: Total Training Recognition Loss 0.26  Total Training Translation Loss 10.24 
2024-02-05 23:27:46,216 EPOCH 162
2024-02-05 23:27:50,140 [Epoch: 162 Step: 00005500] Batch Recognition Loss:   0.010116 => Gls Tokens per Sec:     2054 || Batch Translation Loss:   0.150624 => Txt Tokens per Sec:     5789 || Lr: 0.000100
2024-02-05 23:27:51,044 Epoch 162: Total Training Recognition Loss 0.23  Total Training Translation Loss 10.41 
2024-02-05 23:27:51,044 EPOCH 163
2024-02-05 23:27:55,859 Epoch 163: Total Training Recognition Loss 0.29  Total Training Translation Loss 9.66 
2024-02-05 23:27:55,860 EPOCH 164
2024-02-05 23:28:00,750 Epoch 164: Total Training Recognition Loss 0.22  Total Training Translation Loss 8.71 
2024-02-05 23:28:00,750 EPOCH 165
2024-02-05 23:28:04,303 [Epoch: 165 Step: 00005600] Batch Recognition Loss:   0.004464 => Gls Tokens per Sec:     2162 || Batch Translation Loss:   0.310704 => Txt Tokens per Sec:     6215 || Lr: 0.000100
2024-02-05 23:28:05,438 Epoch 165: Total Training Recognition Loss 0.21  Total Training Translation Loss 9.28 
2024-02-05 23:28:05,438 EPOCH 166
2024-02-05 23:28:10,364 Epoch 166: Total Training Recognition Loss 0.23  Total Training Translation Loss 9.46 
2024-02-05 23:28:10,364 EPOCH 167
2024-02-05 23:28:14,988 Epoch 167: Total Training Recognition Loss 0.22  Total Training Translation Loss 10.55 
2024-02-05 23:28:14,989 EPOCH 168
2024-02-05 23:28:18,164 [Epoch: 168 Step: 00005700] Batch Recognition Loss:   0.001338 => Gls Tokens per Sec:     2137 || Batch Translation Loss:   0.266876 => Txt Tokens per Sec:     5735 || Lr: 0.000100
2024-02-05 23:28:19,893 Epoch 168: Total Training Recognition Loss 0.24  Total Training Translation Loss 10.29 
2024-02-05 23:28:19,893 EPOCH 169
2024-02-05 23:28:24,504 Epoch 169: Total Training Recognition Loss 0.23  Total Training Translation Loss 11.07 
2024-02-05 23:28:24,505 EPOCH 170
2024-02-05 23:28:29,275 Epoch 170: Total Training Recognition Loss 0.30  Total Training Translation Loss 15.46 
2024-02-05 23:28:29,276 EPOCH 171
2024-02-05 23:28:32,069 [Epoch: 171 Step: 00005800] Batch Recognition Loss:   0.002778 => Gls Tokens per Sec:     2292 || Batch Translation Loss:   0.467629 => Txt Tokens per Sec:     6248 || Lr: 0.000100
2024-02-05 23:28:34,137 Epoch 171: Total Training Recognition Loss 0.27  Total Training Translation Loss 15.62 
2024-02-05 23:28:34,138 EPOCH 172
2024-02-05 23:28:38,961 Epoch 172: Total Training Recognition Loss 0.26  Total Training Translation Loss 11.93 
2024-02-05 23:28:38,962 EPOCH 173
2024-02-05 23:28:43,945 Epoch 173: Total Training Recognition Loss 0.31  Total Training Translation Loss 10.17 
2024-02-05 23:28:43,945 EPOCH 174
2024-02-05 23:28:46,448 [Epoch: 174 Step: 00005900] Batch Recognition Loss:   0.017565 => Gls Tokens per Sec:     2302 || Batch Translation Loss:   0.227066 => Txt Tokens per Sec:     6415 || Lr: 0.000100
2024-02-05 23:28:48,997 Epoch 174: Total Training Recognition Loss 0.35  Total Training Translation Loss 9.24 
2024-02-05 23:28:48,997 EPOCH 175
2024-02-05 23:28:53,964 Epoch 175: Total Training Recognition Loss 0.26  Total Training Translation Loss 12.07 
2024-02-05 23:28:53,964 EPOCH 176
2024-02-05 23:28:58,761 Epoch 176: Total Training Recognition Loss 0.53  Total Training Translation Loss 19.92 
2024-02-05 23:28:58,762 EPOCH 177
2024-02-05 23:29:01,474 [Epoch: 177 Step: 00006000] Batch Recognition Loss:   0.030910 => Gls Tokens per Sec:     1889 || Batch Translation Loss:   0.478198 => Txt Tokens per Sec:     5540 || Lr: 0.000100
2024-02-05 23:29:10,272 Hooray! New best validation result [eval_metric]!
2024-02-05 23:29:10,273 Saving new checkpoint.
2024-02-05 23:29:10,552 Validation result at epoch 177, step     6000: duration: 9.0775s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.94545	Translation Loss: 84500.06250	PPL: 4627.99219
	Eval Metric: BLEU
	WER 5.86	(DEL: 0.00,	INS: 0.00,	SUB: 5.86)
	BLEU-4 0.80	(BLEU-1: 12.55,	BLEU-2: 4.03,	BLEU-3: 1.68,	BLEU-4: 0.80)
	CHRF 17.78	ROUGE 10.16
2024-02-05 23:29:10,553 Logging Recognition and Translation Outputs
2024-02-05 23:29:10,553 ========================================================================================================================
2024-02-05 23:29:10,553 Logging Sequence: 160_153.00
2024-02-05 23:29:10,553 	Gloss Reference :	A B+C+D+E
2024-02-05 23:29:10,553 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:29:10,553 	Gloss Alignment :	         
2024-02-05 23:29:10,554 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:29:10,555 	Text Reference  :	i have no hard feelings towards rohit sharma and he   will always have my full support   as he  is   my teammate
2024-02-05 23:29:10,555 	Text Hypothesis :	* **** ** **** ******** ******* ***** ****** now they will ****** **** ** be   wondering as you will be applied 
2024-02-05 23:29:10,555 	Text Alignment  :	D D    D  D    D        D       D     D      S   S         D      D    D  S    S            S   S    S  S       
2024-02-05 23:29:10,555 ========================================================================================================================
2024-02-05 23:29:10,556 Logging Sequence: 103_253.00
2024-02-05 23:29:10,556 	Gloss Reference :	A B+C+D+E
2024-02-05 23:29:10,556 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:29:10,556 	Gloss Alignment :	         
2024-02-05 23:29:10,556 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:29:10,557 	Text Reference  :	***** canada    is 3rd with 92   medals  
2024-02-05 23:29:10,557 	Text Hypothesis :	these cricketer is *** so   what happened
2024-02-05 23:29:10,557 	Text Alignment  :	I     S            D   S    S    S       
2024-02-05 23:29:10,557 ========================================================================================================================
2024-02-05 23:29:10,557 Logging Sequence: 155_25.00
2024-02-05 23:29:10,557 	Gloss Reference :	A B+C+D+E
2024-02-05 23:29:10,557 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:29:10,558 	Gloss Alignment :	         
2024-02-05 23:29:10,558 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:29:10,559 	Text Reference  :	****** *** ********* this is  because taliban overthrew the afghan government and took over the  country
2024-02-05 23:29:10,559 	Text Hypothesis :	people are desparate to   see the     taliban wanted    the ****** family     and it   is   very popular
2024-02-05 23:29:10,559 	Text Alignment  :	I      I   I         S    S   S               S             D      S              S    S    S    S      
2024-02-05 23:29:10,559 ========================================================================================================================
2024-02-05 23:29:10,560 Logging Sequence: 81_105.00
2024-02-05 23:29:10,560 	Gloss Reference :	A B+C+D+E
2024-02-05 23:29:10,560 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:29:10,560 	Gloss Alignment :	         
2024-02-05 23:29:10,560 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:29:10,561 	Text Reference  :	dhoni    was tagged in multiple such     posts    as  he   was the brand ambassador
2024-02-05 23:29:10,561 	Text Hypothesis :	everyone was ****** ** stunned  amrapali sapphire has been in  the ***** match     
2024-02-05 23:29:10,561 	Text Alignment  :	S            D      D  S        S        S        S   S    S       D     S         
2024-02-05 23:29:10,561 ========================================================================================================================
2024-02-05 23:29:10,561 Logging Sequence: 105_136.00
2024-02-05 23:29:10,562 	Gloss Reference :	A B+C+D+E
2024-02-05 23:29:10,562 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:29:10,562 	Gloss Alignment :	         
2024-02-05 23:29:10,562 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:29:10,563 	Text Reference  :	** **** *** beating him once    is  my       biggest dream
2024-02-05 23:29:10,563 	Text Hypothesis :	as they had to      be  careful the incident as      well 
2024-02-05 23:29:10,563 	Text Alignment  :	I  I    I   S       S   S       S   S        S       S    
2024-02-05 23:29:10,563 ========================================================================================================================
2024-02-05 23:29:12,691 Epoch 177: Total Training Recognition Loss 0.46  Total Training Translation Loss 12.48 
2024-02-05 23:29:12,691 EPOCH 178
2024-02-05 23:29:17,493 Epoch 178: Total Training Recognition Loss 0.35  Total Training Translation Loss 23.07 
2024-02-05 23:29:17,493 EPOCH 179
2024-02-05 23:29:22,393 Epoch 179: Total Training Recognition Loss 0.34  Total Training Translation Loss 19.06 
2024-02-05 23:29:22,394 EPOCH 180
2024-02-05 23:29:23,611 [Epoch: 180 Step: 00006100] Batch Recognition Loss:   0.022430 => Gls Tokens per Sec:     3684 || Batch Translation Loss:   0.354753 => Txt Tokens per Sec:     8905 || Lr: 0.000100
2024-02-05 23:29:26,723 Epoch 180: Total Training Recognition Loss 0.38  Total Training Translation Loss 17.21 
2024-02-05 23:29:26,724 EPOCH 181
2024-02-05 23:29:31,421 Epoch 181: Total Training Recognition Loss 0.41  Total Training Translation Loss 15.95 
2024-02-05 23:29:31,422 EPOCH 182
2024-02-05 23:29:35,959 Epoch 182: Total Training Recognition Loss 0.27  Total Training Translation Loss 10.22 
2024-02-05 23:29:35,959 EPOCH 183
2024-02-05 23:29:37,495 [Epoch: 183 Step: 00006200] Batch Recognition Loss:   0.002848 => Gls Tokens per Sec:     2502 || Batch Translation Loss:   0.287230 => Txt Tokens per Sec:     6910 || Lr: 0.000100
2024-02-05 23:29:40,510 Epoch 183: Total Training Recognition Loss 0.22  Total Training Translation Loss 7.74 
2024-02-05 23:29:40,511 EPOCH 184
2024-02-05 23:29:45,160 Epoch 184: Total Training Recognition Loss 0.23  Total Training Translation Loss 7.06 
2024-02-05 23:29:45,160 EPOCH 185
2024-02-05 23:29:50,008 Epoch 185: Total Training Recognition Loss 0.14  Total Training Translation Loss 6.01 
2024-02-05 23:29:50,008 EPOCH 186
2024-02-05 23:29:51,123 [Epoch: 186 Step: 00006300] Batch Recognition Loss:   0.001563 => Gls Tokens per Sec:     2873 || Batch Translation Loss:   0.220571 => Txt Tokens per Sec:     7568 || Lr: 0.000100
2024-02-05 23:29:54,483 Epoch 186: Total Training Recognition Loss 0.21  Total Training Translation Loss 4.83 
2024-02-05 23:29:54,483 EPOCH 187
2024-02-05 23:29:58,968 Epoch 187: Total Training Recognition Loss 0.18  Total Training Translation Loss 4.60 
2024-02-05 23:29:58,968 EPOCH 188
2024-02-05 23:30:03,687 Epoch 188: Total Training Recognition Loss 0.18  Total Training Translation Loss 4.55 
2024-02-05 23:30:03,687 EPOCH 189
2024-02-05 23:30:04,473 [Epoch: 189 Step: 00006400] Batch Recognition Loss:   0.003098 => Gls Tokens per Sec:     2930 || Batch Translation Loss:   0.130157 => Txt Tokens per Sec:     7615 || Lr: 0.000100
2024-02-05 23:30:07,954 Epoch 189: Total Training Recognition Loss 0.15  Total Training Translation Loss 4.23 
2024-02-05 23:30:07,955 EPOCH 190
2024-02-05 23:30:13,172 Epoch 190: Total Training Recognition Loss 0.13  Total Training Translation Loss 4.95 
2024-02-05 23:30:13,172 EPOCH 191
2024-02-05 23:30:17,506 Epoch 191: Total Training Recognition Loss 0.17  Total Training Translation Loss 4.86 
2024-02-05 23:30:17,507 EPOCH 192
2024-02-05 23:30:18,054 [Epoch: 192 Step: 00006500] Batch Recognition Loss:   0.006009 => Gls Tokens per Sec:     3516 || Batch Translation Loss:   0.127296 => Txt Tokens per Sec:     9035 || Lr: 0.000100
2024-02-05 23:30:22,079 Epoch 192: Total Training Recognition Loss 0.15  Total Training Translation Loss 5.40 
2024-02-05 23:30:22,079 EPOCH 193
2024-02-05 23:30:26,764 Epoch 193: Total Training Recognition Loss 0.18  Total Training Translation Loss 6.28 
2024-02-05 23:30:26,764 EPOCH 194
2024-02-05 23:30:31,285 Epoch 194: Total Training Recognition Loss 0.16  Total Training Translation Loss 8.23 
2024-02-05 23:30:31,285 EPOCH 195
2024-02-05 23:30:31,853 [Epoch: 195 Step: 00006600] Batch Recognition Loss:   0.001934 => Gls Tokens per Sec:     1801 || Batch Translation Loss:   0.103706 => Txt Tokens per Sec:     5254 || Lr: 0.000100
2024-02-05 23:30:36,083 Epoch 195: Total Training Recognition Loss 0.13  Total Training Translation Loss 6.97 
2024-02-05 23:30:36,083 EPOCH 196
2024-02-05 23:30:40,501 Epoch 196: Total Training Recognition Loss 0.17  Total Training Translation Loss 10.10 
2024-02-05 23:30:40,501 EPOCH 197
2024-02-05 23:30:45,337 Epoch 197: Total Training Recognition Loss 0.18  Total Training Translation Loss 10.21 
2024-02-05 23:30:45,337 EPOCH 198
2024-02-05 23:30:45,557 [Epoch: 198 Step: 00006700] Batch Recognition Loss:   0.002223 => Gls Tokens per Sec:     1735 || Batch Translation Loss:   0.080918 => Txt Tokens per Sec:     5160 || Lr: 0.000100
2024-02-05 23:30:50,234 Epoch 198: Total Training Recognition Loss 0.20  Total Training Translation Loss 7.88 
2024-02-05 23:30:50,234 EPOCH 199
2024-02-05 23:30:54,962 Epoch 199: Total Training Recognition Loss 0.16  Total Training Translation Loss 6.11 
2024-02-05 23:30:54,963 EPOCH 200
2024-02-05 23:30:59,806 [Epoch: 200 Step: 00006800] Batch Recognition Loss:   0.003289 => Gls Tokens per Sec:     2193 || Batch Translation Loss:   0.102797 => Txt Tokens per Sec:     6066 || Lr: 0.000100
2024-02-05 23:30:59,807 Epoch 200: Total Training Recognition Loss 0.16  Total Training Translation Loss 5.43 
2024-02-05 23:30:59,807 EPOCH 201
2024-02-05 23:31:04,630 Epoch 201: Total Training Recognition Loss 0.12  Total Training Translation Loss 5.24 
2024-02-05 23:31:04,630 EPOCH 202
2024-02-05 23:31:09,544 Epoch 202: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.17 
2024-02-05 23:31:09,544 EPOCH 203
2024-02-05 23:31:14,060 [Epoch: 203 Step: 00006900] Batch Recognition Loss:   0.003939 => Gls Tokens per Sec:     2210 || Batch Translation Loss:   0.158372 => Txt Tokens per Sec:     6223 || Lr: 0.000100
2024-02-05 23:31:14,202 Epoch 203: Total Training Recognition Loss 0.14  Total Training Translation Loss 4.49 
2024-02-05 23:31:14,202 EPOCH 204
2024-02-05 23:31:19,112 Epoch 204: Total Training Recognition Loss 0.13  Total Training Translation Loss 4.78 
2024-02-05 23:31:19,113 EPOCH 205
2024-02-05 23:31:23,858 Epoch 205: Total Training Recognition Loss 0.16  Total Training Translation Loss 4.47 
2024-02-05 23:31:23,858 EPOCH 206
2024-02-05 23:31:27,707 [Epoch: 206 Step: 00007000] Batch Recognition Loss:   0.002868 => Gls Tokens per Sec:     2427 || Batch Translation Loss:   0.196340 => Txt Tokens per Sec:     6603 || Lr: 0.000100
2024-02-05 23:31:28,724 Epoch 206: Total Training Recognition Loss 0.15  Total Training Translation Loss 5.02 
2024-02-05 23:31:28,725 EPOCH 207
2024-02-05 23:31:33,520 Epoch 207: Total Training Recognition Loss 0.18  Total Training Translation Loss 6.53 
2024-02-05 23:31:33,520 EPOCH 208
2024-02-05 23:31:38,280 Epoch 208: Total Training Recognition Loss 0.17  Total Training Translation Loss 6.47 
2024-02-05 23:31:38,280 EPOCH 209
2024-02-05 23:31:42,186 [Epoch: 209 Step: 00007100] Batch Recognition Loss:   0.002341 => Gls Tokens per Sec:     2228 || Batch Translation Loss:   0.273485 => Txt Tokens per Sec:     6080 || Lr: 0.000100
2024-02-05 23:31:43,133 Epoch 209: Total Training Recognition Loss 0.15  Total Training Translation Loss 7.43 
2024-02-05 23:31:43,133 EPOCH 210
2024-02-05 23:31:47,838 Epoch 210: Total Training Recognition Loss 0.15  Total Training Translation Loss 7.51 
2024-02-05 23:31:47,839 EPOCH 211
2024-02-05 23:31:52,745 Epoch 211: Total Training Recognition Loss 0.16  Total Training Translation Loss 6.61 
2024-02-05 23:31:52,746 EPOCH 212
2024-02-05 23:31:56,014 [Epoch: 212 Step: 00007200] Batch Recognition Loss:   0.000963 => Gls Tokens per Sec:     2467 || Batch Translation Loss:   0.335921 => Txt Tokens per Sec:     6952 || Lr: 0.000100
2024-02-05 23:31:56,867 Epoch 212: Total Training Recognition Loss 0.14  Total Training Translation Loss 6.77 
2024-02-05 23:31:56,867 EPOCH 213
2024-02-05 23:32:01,783 Epoch 213: Total Training Recognition Loss 0.17  Total Training Translation Loss 7.81 
2024-02-05 23:32:01,784 EPOCH 214
2024-02-05 23:32:06,194 Epoch 214: Total Training Recognition Loss 0.23  Total Training Translation Loss 10.95 
2024-02-05 23:32:06,195 EPOCH 215
2024-02-05 23:32:09,781 [Epoch: 215 Step: 00007300] Batch Recognition Loss:   0.004662 => Gls Tokens per Sec:     2070 || Batch Translation Loss:   0.274567 => Txt Tokens per Sec:     5885 || Lr: 0.000100
2024-02-05 23:32:11,051 Epoch 215: Total Training Recognition Loss 0.45  Total Training Translation Loss 13.42 
2024-02-05 23:32:11,051 EPOCH 216
2024-02-05 23:32:15,468 Epoch 216: Total Training Recognition Loss 0.28  Total Training Translation Loss 11.93 
2024-02-05 23:32:15,468 EPOCH 217
2024-02-05 23:32:20,113 Epoch 217: Total Training Recognition Loss 0.24  Total Training Translation Loss 11.19 
2024-02-05 23:32:20,114 EPOCH 218
2024-02-05 23:32:22,957 [Epoch: 218 Step: 00007400] Batch Recognition Loss:   0.005735 => Gls Tokens per Sec:     2385 || Batch Translation Loss:   0.592747 => Txt Tokens per Sec:     6526 || Lr: 0.000100
2024-02-05 23:32:24,700 Epoch 218: Total Training Recognition Loss 0.29  Total Training Translation Loss 13.64 
2024-02-05 23:32:24,700 EPOCH 219
2024-02-05 23:32:29,260 Epoch 219: Total Training Recognition Loss 0.89  Total Training Translation Loss 12.08 
2024-02-05 23:32:29,260 EPOCH 220
2024-02-05 23:32:33,976 Epoch 220: Total Training Recognition Loss 2.68  Total Training Translation Loss 10.09 
2024-02-05 23:32:33,976 EPOCH 221
2024-02-05 23:32:36,222 [Epoch: 221 Step: 00007500] Batch Recognition Loss:   0.105063 => Gls Tokens per Sec:     2851 || Batch Translation Loss:   0.153845 => Txt Tokens per Sec:     7799 || Lr: 0.000100
2024-02-05 23:32:38,510 Epoch 221: Total Training Recognition Loss 1.69  Total Training Translation Loss 8.56 
2024-02-05 23:32:38,511 EPOCH 222
2024-02-05 23:32:43,275 Epoch 222: Total Training Recognition Loss 0.72  Total Training Translation Loss 9.58 
2024-02-05 23:32:43,275 EPOCH 223
2024-02-05 23:32:47,618 Epoch 223: Total Training Recognition Loss 0.23  Total Training Translation Loss 9.43 
2024-02-05 23:32:47,618 EPOCH 224
2024-02-05 23:32:50,319 [Epoch: 224 Step: 00007600] Batch Recognition Loss:   0.006074 => Gls Tokens per Sec:     2037 || Batch Translation Loss:   0.681477 => Txt Tokens per Sec:     5611 || Lr: 0.000100
2024-02-05 23:32:52,532 Epoch 224: Total Training Recognition Loss 0.18  Total Training Translation Loss 10.26 
2024-02-05 23:32:52,532 EPOCH 225
2024-02-05 23:32:57,087 Epoch 225: Total Training Recognition Loss 0.19  Total Training Translation Loss 13.06 
2024-02-05 23:32:57,088 EPOCH 226
2024-02-05 23:33:01,802 Epoch 226: Total Training Recognition Loss 0.16  Total Training Translation Loss 11.27 
2024-02-05 23:33:01,803 EPOCH 227
2024-02-05 23:33:03,637 [Epoch: 227 Step: 00007700] Batch Recognition Loss:   0.000889 => Gls Tokens per Sec:     2792 || Batch Translation Loss:   0.198661 => Txt Tokens per Sec:     7882 || Lr: 0.000100
2024-02-05 23:33:05,964 Epoch 227: Total Training Recognition Loss 0.23  Total Training Translation Loss 10.78 
2024-02-05 23:33:05,965 EPOCH 228
2024-02-05 23:33:10,944 Epoch 228: Total Training Recognition Loss 0.20  Total Training Translation Loss 9.43 
2024-02-05 23:33:10,944 EPOCH 229
2024-02-05 23:33:15,205 Epoch 229: Total Training Recognition Loss 0.16  Total Training Translation Loss 5.19 
2024-02-05 23:33:15,206 EPOCH 230
2024-02-05 23:33:17,436 [Epoch: 230 Step: 00007800] Batch Recognition Loss:   0.019021 => Gls Tokens per Sec:     1893 || Batch Translation Loss:   0.107881 => Txt Tokens per Sec:     5349 || Lr: 0.000100
2024-02-05 23:33:20,115 Epoch 230: Total Training Recognition Loss 0.17  Total Training Translation Loss 3.40 
2024-02-05 23:33:20,115 EPOCH 231
2024-02-05 23:33:24,374 Epoch 231: Total Training Recognition Loss 0.14  Total Training Translation Loss 3.18 
2024-02-05 23:33:24,374 EPOCH 232
2024-02-05 23:33:29,242 Epoch 232: Total Training Recognition Loss 0.17  Total Training Translation Loss 3.38 
2024-02-05 23:33:29,243 EPOCH 233
2024-02-05 23:33:30,940 [Epoch: 233 Step: 00007900] Batch Recognition Loss:   0.002148 => Gls Tokens per Sec:     2265 || Batch Translation Loss:   0.073495 => Txt Tokens per Sec:     6656 || Lr: 0.000100
2024-02-05 23:33:33,617 Epoch 233: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.55 
2024-02-05 23:33:33,617 EPOCH 234
2024-02-05 23:33:38,328 Epoch 234: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.53 
2024-02-05 23:33:38,329 EPOCH 235
2024-02-05 23:33:42,860 Epoch 235: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.88 
2024-02-05 23:33:42,860 EPOCH 236
2024-02-05 23:33:44,226 [Epoch: 236 Step: 00008000] Batch Recognition Loss:   0.002640 => Gls Tokens per Sec:     2345 || Batch Translation Loss:   0.054774 => Txt Tokens per Sec:     6705 || Lr: 0.000100
2024-02-05 23:33:53,112 Validation result at epoch 236, step     8000: duration: 8.8859s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.66275	Translation Loss: 88913.33594	PPL: 7191.65234
	Eval Metric: BLEU
	WER 4.73	(DEL: 0.00,	INS: 0.00,	SUB: 4.73)
	BLEU-4 0.72	(BLEU-1: 11.98,	BLEU-2: 3.92,	BLEU-3: 1.58,	BLEU-4: 0.72)
	CHRF 17.78	ROUGE 10.07
2024-02-05 23:33:53,113 Logging Recognition and Translation Outputs
2024-02-05 23:33:53,113 ========================================================================================================================
2024-02-05 23:33:53,114 Logging Sequence: 180_236.00
2024-02-05 23:33:53,114 	Gloss Reference :	A B+C+D+E
2024-02-05 23:33:53,114 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:33:53,114 	Gloss Alignment :	         
2024-02-05 23:33:53,114 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:33:53,115 	Text Reference  :	however the  wrestlers returned to the protest site        at jantar mantar with  thier demands  
2024-02-05 23:33:53,115 	Text Hypothesis :	however they were      unable   to *** ******* participate in the    fir    which is    incorrect
2024-02-05 23:33:53,116 	Text Alignment  :	        S    S         S           D   D       S           S  S      S      S     S     S        
2024-02-05 23:33:53,116 ========================================================================================================================
2024-02-05 23:33:53,116 Logging Sequence: 111_154.00
2024-02-05 23:33:53,116 	Gloss Reference :	A B+C+D+E  
2024-02-05 23:33:53,116 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-05 23:33:53,116 	Gloss Alignment :	  S        
2024-02-05 23:33:53,117 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:33:53,118 	Text Reference  :	***** *** due   to       csk's  slow over   rate dhoni   was fined rs 12 lakh *** **** *********
2024-02-05 23:33:53,118 	Text Hypothesis :	after the first instance during a    season the  captain is  fined rs 12 lakh for slow over-rate
2024-02-05 23:33:53,118 	Text Alignment  :	I     I   S     S        S      S    S      S    S       S                    I   I    I        
2024-02-05 23:33:53,118 ========================================================================================================================
2024-02-05 23:33:53,119 Logging Sequence: 118_314.00
2024-02-05 23:33:53,119 	Gloss Reference :	A B+C+D+E
2024-02-05 23:33:53,119 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:33:53,119 	Gloss Alignment :	         
2024-02-05 23:33:53,119 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:33:53,120 	Text Reference  :	** wow  even the president had come to      watch  
2024-02-05 23:33:53,120 	Text Hypothesis :	so here are  the ********* *** **** various winners
2024-02-05 23:33:53,120 	Text Alignment  :	I  S    S        D         D   D    S       S      
2024-02-05 23:33:53,120 ========================================================================================================================
2024-02-05 23:33:53,120 Logging Sequence: 156_197.00
2024-02-05 23:33:53,120 	Gloss Reference :	A B+C+D+E
2024-02-05 23:33:53,120 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:33:53,121 	Gloss Alignment :	         
2024-02-05 23:33:53,121 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:33:53,122 	Text Reference  :	seattle orcas sor is   owned by   many investors including satya nadella microsoft ceo
2024-02-05 23:33:53,122 	Text Hypothesis :	she     did   not want to    book him  off       for       120   lakh    in        ipl
2024-02-05 23:33:53,122 	Text Alignment  :	S       S     S   S    S     S    S    S         S         S     S       S         S  
2024-02-05 23:33:53,122 ========================================================================================================================
2024-02-05 23:33:53,122 Logging Sequence: 183_159.00
2024-02-05 23:33:53,122 	Gloss Reference :	A B+C+D+E
2024-02-05 23:33:53,123 	Gloss Hypothesis:	A B+C+D  
2024-02-05 23:33:53,123 	Gloss Alignment :	  S      
2024-02-05 23:33:53,123 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:33:53,124 	Text Reference  :	however an exception to this is     virat   kohli   and his   wife anushka sharma who refuse to share    images of        their     daughter
2024-02-05 23:33:53,125 	Text Hypothesis :	******* ** ********* ** **** people praised cummins for being a    great   leader who ****** ** respects his    teammate' religious beliefs 
2024-02-05 23:33:53,125 	Text Alignment  :	D       D  D         D  D    S      S       S       S   S     S    S       S          D      D  S        S      S         S         S       
2024-02-05 23:33:53,125 ========================================================================================================================
2024-02-05 23:33:56,590 Epoch 236: Total Training Recognition Loss 0.11  Total Training Translation Loss 2.37 
2024-02-05 23:33:56,590 EPOCH 237
2024-02-05 23:34:01,381 Epoch 237: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.60 
2024-02-05 23:34:01,382 EPOCH 238
2024-02-05 23:34:05,663 Epoch 238: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.98 
2024-02-05 23:34:05,663 EPOCH 239
2024-02-05 23:34:07,117 [Epoch: 239 Step: 00008100] Batch Recognition Loss:   0.001145 => Gls Tokens per Sec:     1765 || Batch Translation Loss:   0.213419 => Txt Tokens per Sec:     4966 || Lr: 0.000100
2024-02-05 23:34:10,612 Epoch 239: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.35 
2024-02-05 23:34:10,612 EPOCH 240
2024-02-05 23:34:15,518 Epoch 240: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.67 
2024-02-05 23:34:15,518 EPOCH 241
2024-02-05 23:34:20,193 Epoch 241: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.38 
2024-02-05 23:34:20,193 EPOCH 242
2024-02-05 23:34:20,961 [Epoch: 242 Step: 00008200] Batch Recognition Loss:   0.002457 => Gls Tokens per Sec:     2503 || Batch Translation Loss:   0.091525 => Txt Tokens per Sec:     7138 || Lr: 0.000100
2024-02-05 23:34:25,018 Epoch 242: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.23 
2024-02-05 23:34:25,018 EPOCH 243
2024-02-05 23:34:29,862 Epoch 243: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.39 
2024-02-05 23:34:29,862 EPOCH 244
2024-02-05 23:34:34,665 Epoch 244: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.66 
2024-02-05 23:34:34,666 EPOCH 245
2024-02-05 23:34:35,569 [Epoch: 245 Step: 00008300] Batch Recognition Loss:   0.003119 => Gls Tokens per Sec:     1422 || Batch Translation Loss:   0.084591 => Txt Tokens per Sec:     4335 || Lr: 0.000100
2024-02-05 23:34:39,753 Epoch 245: Total Training Recognition Loss 0.09  Total Training Translation Loss 4.28 
2024-02-05 23:34:39,754 EPOCH 246
2024-02-05 23:34:44,540 Epoch 246: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.66 
2024-02-05 23:34:44,540 EPOCH 247
2024-02-05 23:34:49,209 Epoch 247: Total Training Recognition Loss 0.12  Total Training Translation Loss 4.72 
2024-02-05 23:34:49,210 EPOCH 248
2024-02-05 23:34:49,495 [Epoch: 248 Step: 00008400] Batch Recognition Loss:   0.005264 => Gls Tokens per Sec:     2254 || Batch Translation Loss:   0.248084 => Txt Tokens per Sec:     5444 || Lr: 0.000100
2024-02-05 23:34:54,154 Epoch 248: Total Training Recognition Loss 0.11  Total Training Translation Loss 6.95 
2024-02-05 23:34:54,155 EPOCH 249
2024-02-05 23:34:58,785 Epoch 249: Total Training Recognition Loss 0.10  Total Training Translation Loss 7.72 
2024-02-05 23:34:58,786 EPOCH 250
2024-02-05 23:35:03,646 [Epoch: 250 Step: 00008500] Batch Recognition Loss:   0.014084 => Gls Tokens per Sec:     2186 || Batch Translation Loss:   0.238172 => Txt Tokens per Sec:     6047 || Lr: 0.000100
2024-02-05 23:35:03,646 Epoch 250: Total Training Recognition Loss 0.13  Total Training Translation Loss 6.21 
2024-02-05 23:35:03,647 EPOCH 251
2024-02-05 23:35:08,592 Epoch 251: Total Training Recognition Loss 0.10  Total Training Translation Loss 6.73 
2024-02-05 23:35:08,592 EPOCH 252
2024-02-05 23:35:13,302 Epoch 252: Total Training Recognition Loss 0.09  Total Training Translation Loss 9.13 
2024-02-05 23:35:13,302 EPOCH 253
2024-02-05 23:35:17,890 [Epoch: 253 Step: 00008600] Batch Recognition Loss:   0.006896 => Gls Tokens per Sec:     2232 || Batch Translation Loss:   0.236075 => Txt Tokens per Sec:     6155 || Lr: 0.000100
2024-02-05 23:35:18,160 Epoch 253: Total Training Recognition Loss 0.22  Total Training Translation Loss 14.65 
2024-02-05 23:35:18,161 EPOCH 254
2024-02-05 23:35:22,890 Epoch 254: Total Training Recognition Loss 0.16  Total Training Translation Loss 9.76 
2024-02-05 23:35:22,890 EPOCH 255
2024-02-05 23:35:27,665 Epoch 255: Total Training Recognition Loss 0.16  Total Training Translation Loss 7.13 
2024-02-05 23:35:27,666 EPOCH 256
2024-02-05 23:35:32,077 [Epoch: 256 Step: 00008700] Batch Recognition Loss:   0.001176 => Gls Tokens per Sec:     2118 || Batch Translation Loss:   0.162933 => Txt Tokens per Sec:     5916 || Lr: 0.000100
2024-02-05 23:35:32,505 Epoch 256: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.42 
2024-02-05 23:35:32,505 EPOCH 257
2024-02-05 23:35:37,290 Epoch 257: Total Training Recognition Loss 0.15  Total Training Translation Loss 4.69 
2024-02-05 23:35:37,291 EPOCH 258
2024-02-05 23:35:42,157 Epoch 258: Total Training Recognition Loss 0.12  Total Training Translation Loss 3.53 
2024-02-05 23:35:42,158 EPOCH 259
2024-02-05 23:35:45,915 [Epoch: 259 Step: 00008800] Batch Recognition Loss:   0.001187 => Gls Tokens per Sec:     2317 || Batch Translation Loss:   0.061971 => Txt Tokens per Sec:     6258 || Lr: 0.000100
2024-02-05 23:35:46,829 Epoch 259: Total Training Recognition Loss 0.12  Total Training Translation Loss 3.62 
2024-02-05 23:35:46,830 EPOCH 260
2024-02-05 23:35:51,708 Epoch 260: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.10 
2024-02-05 23:35:51,708 EPOCH 261
2024-02-05 23:35:56,331 Epoch 261: Total Training Recognition Loss 0.12  Total Training Translation Loss 3.06 
2024-02-05 23:35:56,331 EPOCH 262
2024-02-05 23:36:00,232 [Epoch: 262 Step: 00008900] Batch Recognition Loss:   0.006692 => Gls Tokens per Sec:     2066 || Batch Translation Loss:   0.163340 => Txt Tokens per Sec:     5799 || Lr: 0.000100
2024-02-05 23:36:01,279 Epoch 262: Total Training Recognition Loss 0.12  Total Training Translation Loss 3.12 
2024-02-05 23:36:01,279 EPOCH 263
2024-02-05 23:36:05,904 Epoch 263: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.89 
2024-02-05 23:36:05,905 EPOCH 264
2024-02-05 23:36:10,827 Epoch 264: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.32 
2024-02-05 23:36:10,828 EPOCH 265
2024-02-05 23:36:13,868 [Epoch: 265 Step: 00009000] Batch Recognition Loss:   0.004913 => Gls Tokens per Sec:     2442 || Batch Translation Loss:   0.085572 => Txt Tokens per Sec:     6722 || Lr: 0.000100
2024-02-05 23:36:15,027 Epoch 265: Total Training Recognition Loss 0.09  Total Training Translation Loss 4.10 
2024-02-05 23:36:15,027 EPOCH 266
2024-02-05 23:36:19,790 Epoch 266: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.56 
2024-02-05 23:36:19,791 EPOCH 267
2024-02-05 23:36:24,248 Epoch 267: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.02 
2024-02-05 23:36:24,248 EPOCH 268
2024-02-05 23:36:26,868 [Epoch: 268 Step: 00009100] Batch Recognition Loss:   0.000733 => Gls Tokens per Sec:     2687 || Batch Translation Loss:   0.082514 => Txt Tokens per Sec:     7343 || Lr: 0.000100
2024-02-05 23:36:28,884 Epoch 268: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.93 
2024-02-05 23:36:28,884 EPOCH 269
2024-02-05 23:36:33,491 Epoch 269: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.83 
2024-02-05 23:36:33,491 EPOCH 270
2024-02-05 23:36:38,033 Epoch 270: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.05 
2024-02-05 23:36:38,033 EPOCH 271
2024-02-05 23:36:40,432 [Epoch: 271 Step: 00009200] Batch Recognition Loss:   0.003082 => Gls Tokens per Sec:     2669 || Batch Translation Loss:   0.045610 => Txt Tokens per Sec:     7065 || Lr: 0.000100
2024-02-05 23:36:42,717 Epoch 271: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.96 
2024-02-05 23:36:42,718 EPOCH 272
2024-02-05 23:36:47,221 Epoch 272: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.76 
2024-02-05 23:36:47,222 EPOCH 273
2024-02-05 23:36:52,009 Epoch 273: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.31 
2024-02-05 23:36:52,009 EPOCH 274
2024-02-05 23:36:54,003 [Epoch: 274 Step: 00009300] Batch Recognition Loss:   0.005113 => Gls Tokens per Sec:     2760 || Batch Translation Loss:   0.100868 => Txt Tokens per Sec:     7573 || Lr: 0.000100
2024-02-05 23:36:56,197 Epoch 274: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.18 
2024-02-05 23:36:56,198 EPOCH 275
2024-02-05 23:37:01,179 Epoch 275: Total Training Recognition Loss 0.09  Total Training Translation Loss 4.33 
2024-02-05 23:37:01,180 EPOCH 276
2024-02-05 23:37:05,355 Epoch 276: Total Training Recognition Loss 0.10  Total Training Translation Loss 5.83 
2024-02-05 23:37:05,355 EPOCH 277
2024-02-05 23:37:07,865 [Epoch: 277 Step: 00009400] Batch Recognition Loss:   0.004983 => Gls Tokens per Sec:     1937 || Batch Translation Loss:   0.155210 => Txt Tokens per Sec:     5339 || Lr: 0.000100
2024-02-05 23:37:10,353 Epoch 277: Total Training Recognition Loss 0.09  Total Training Translation Loss 5.54 
2024-02-05 23:37:10,353 EPOCH 278
2024-02-05 23:37:14,687 Epoch 278: Total Training Recognition Loss 0.12  Total Training Translation Loss 5.96 
2024-02-05 23:37:14,687 EPOCH 279
2024-02-05 23:37:19,615 Epoch 279: Total Training Recognition Loss 0.12  Total Training Translation Loss 5.22 
2024-02-05 23:37:19,616 EPOCH 280
2024-02-05 23:37:21,456 [Epoch: 280 Step: 00009500] Batch Recognition Loss:   0.003890 => Gls Tokens per Sec:     2436 || Batch Translation Loss:   0.133290 => Txt Tokens per Sec:     6685 || Lr: 0.000100
2024-02-05 23:37:23,986 Epoch 280: Total Training Recognition Loss 0.09  Total Training Translation Loss 5.93 
2024-02-05 23:37:23,986 EPOCH 281
2024-02-05 23:37:28,802 Epoch 281: Total Training Recognition Loss 0.10  Total Training Translation Loss 6.86 
2024-02-05 23:37:28,802 EPOCH 282
2024-02-05 23:37:33,235 Epoch 282: Total Training Recognition Loss 0.11  Total Training Translation Loss 5.95 
2024-02-05 23:37:33,235 EPOCH 283
2024-02-05 23:37:34,750 [Epoch: 283 Step: 00009600] Batch Recognition Loss:   0.001493 => Gls Tokens per Sec:     2364 || Batch Translation Loss:   0.148634 => Txt Tokens per Sec:     6484 || Lr: 0.000100
2024-02-05 23:37:38,204 Epoch 283: Total Training Recognition Loss 0.09  Total Training Translation Loss 4.19 
2024-02-05 23:37:38,205 EPOCH 284
2024-02-05 23:37:43,075 Epoch 284: Total Training Recognition Loss 0.11  Total Training Translation Loss 4.64 
2024-02-05 23:37:43,075 EPOCH 285
2024-02-05 23:37:48,021 Epoch 285: Total Training Recognition Loss 0.10  Total Training Translation Loss 5.69 
2024-02-05 23:37:48,021 EPOCH 286
2024-02-05 23:37:49,306 [Epoch: 286 Step: 00009700] Batch Recognition Loss:   0.001131 => Gls Tokens per Sec:     2492 || Batch Translation Loss:   0.212662 => Txt Tokens per Sec:     7209 || Lr: 0.000100
2024-02-05 23:37:52,752 Epoch 286: Total Training Recognition Loss 0.09  Total Training Translation Loss 6.02 
2024-02-05 23:37:52,752 EPOCH 287
2024-02-05 23:37:57,538 Epoch 287: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.59 
2024-02-05 23:37:57,539 EPOCH 288
2024-02-05 23:38:02,113 Epoch 288: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.89 
2024-02-05 23:38:02,114 EPOCH 289
2024-02-05 23:38:03,605 [Epoch: 289 Step: 00009800] Batch Recognition Loss:   0.003550 => Gls Tokens per Sec:     1544 || Batch Translation Loss:   0.177015 => Txt Tokens per Sec:     4330 || Lr: 0.000100
2024-02-05 23:38:07,193 Epoch 289: Total Training Recognition Loss 0.12  Total Training Translation Loss 6.46 
2024-02-05 23:38:07,194 EPOCH 290
2024-02-05 23:38:11,695 Epoch 290: Total Training Recognition Loss 0.11  Total Training Translation Loss 15.96 
2024-02-05 23:38:11,695 EPOCH 291
2024-02-05 23:38:15,817 Epoch 291: Total Training Recognition Loss 0.11  Total Training Translation Loss 8.27 
2024-02-05 23:38:15,817 EPOCH 292
2024-02-05 23:38:16,506 [Epoch: 292 Step: 00009900] Batch Recognition Loss:   0.001866 => Gls Tokens per Sec:     2793 || Batch Translation Loss:   0.166385 => Txt Tokens per Sec:     7772 || Lr: 0.000100
2024-02-05 23:38:20,330 Epoch 292: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.46 
2024-02-05 23:38:20,331 EPOCH 293
2024-02-05 23:38:25,350 Epoch 293: Total Training Recognition Loss 0.16  Total Training Translation Loss 4.77 
2024-02-05 23:38:25,350 EPOCH 294
2024-02-05 23:38:30,109 Epoch 294: Total Training Recognition Loss 0.13  Total Training Translation Loss 4.51 
2024-02-05 23:38:30,110 EPOCH 295
2024-02-05 23:38:30,714 [Epoch: 295 Step: 00010000] Batch Recognition Loss:   0.002626 => Gls Tokens per Sec:     2123 || Batch Translation Loss:   0.099777 => Txt Tokens per Sec:     6061 || Lr: 0.000100
2024-02-05 23:38:39,335 Hooray! New best validation result [eval_metric]!
2024-02-05 23:38:39,336 Saving new checkpoint.
2024-02-05 23:38:39,637 Validation result at epoch 295, step    10000: duration: 8.9229s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.87127	Translation Loss: 92395.35938	PPL: 10182.86035
	Eval Metric: BLEU
	WER 5.44	(DEL: 0.00,	INS: 0.00,	SUB: 5.44)
	BLEU-4 0.83	(BLEU-1: 12.53,	BLEU-2: 4.22,	BLEU-3: 1.68,	BLEU-4: 0.83)
	CHRF 17.56	ROUGE 10.45
2024-02-05 23:38:39,639 Logging Recognition and Translation Outputs
2024-02-05 23:38:39,639 ========================================================================================================================
2024-02-05 23:38:39,639 Logging Sequence: 123_147.00
2024-02-05 23:38:39,639 	Gloss Reference :	A B+C+D+E
2024-02-05 23:38:39,639 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:38:39,640 	Gloss Alignment :	         
2024-02-05 23:38:39,640 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:38:39,640 	Text Reference  :	the former captain also owns the pontiac firebird trans am car   worth rs 68                   lakh  
2024-02-05 23:38:39,641 	Text Hypothesis :	*** ****** ******* **** **** *** ******* ******** ***** ** dhoni is    a  once-in-a-generation player
2024-02-05 23:38:39,641 	Text Alignment  :	D   D      D       D    D    D   D       D        D     D  S     S     S  S                    S     
2024-02-05 23:38:39,641 ========================================================================================================================
2024-02-05 23:38:39,641 Logging Sequence: 58_196.00
2024-02-05 23:38:39,641 	Gloss Reference :	A B+C+D+E
2024-02-05 23:38:39,641 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:38:39,642 	Gloss Alignment :	         
2024-02-05 23:38:39,642 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:38:39,643 	Text Reference  :	the talents and skills of our athletes *** ********* ***** **** knows no  bounds    
2024-02-05 23:38:39,643 	Text Hypothesis :	*** while   28  years  of *** athletes are currently being held in    the tournament
2024-02-05 23:38:39,643 	Text Alignment  :	D   S       S   S         D            I   I         I     I    S     S   S         
2024-02-05 23:38:39,643 ========================================================================================================================
2024-02-05 23:38:39,643 Logging Sequence: 168_184.00
2024-02-05 23:38:39,644 	Gloss Reference :	A B+C+D+E
2024-02-05 23:38:39,644 	Gloss Hypothesis:	A B+C+D  
2024-02-05 23:38:39,644 	Gloss Alignment :	  S      
2024-02-05 23:38:39,644 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:38:39,646 	Text Reference  :	people say that we may get a true glimpse of    vamika in *** february   2022  when she     turns 1     year   old    
2024-02-05 23:38:39,646 	Text Hypothesis :	****** *** **** ** *** *** * this rule    would be     in the tournament after the  opening game  where mumbai indians
2024-02-05 23:38:39,646 	Text Alignment  :	D      D   D    D  D   D   D S    S       S     S         I   S          S     S    S       S     S     S      S      
2024-02-05 23:38:39,646 ========================================================================================================================
2024-02-05 23:38:39,646 Logging Sequence: 87_123.00
2024-02-05 23:38:39,646 	Gloss Reference :	A B+C+D+E
2024-02-05 23:38:39,647 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:38:39,647 	Gloss Alignment :	         
2024-02-05 23:38:39,647 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:38:39,648 	Text Reference  :	he said that  he hoped kl    rahul would  be      fit       for  the ******* ******* upcoming world cup     
2024-02-05 23:38:39,648 	Text Hypothesis :	** the  final of the   world cup   trophy however yesterday with the penalty between india    and   pakistan
2024-02-05 23:38:39,648 	Text Alignment  :	D  S    S     S  S     S     S     S      S       S         S        I       I       S        S     S       
2024-02-05 23:38:39,649 ========================================================================================================================
2024-02-05 23:38:39,649 Logging Sequence: 144_154.00
2024-02-05 23:38:39,649 	Gloss Reference :	A B+C+D+E
2024-02-05 23:38:39,649 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:38:39,649 	Gloss Alignment :	         
2024-02-05 23:38:39,649 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:38:39,651 	Text Reference  :	she also participated in the **** ********** rural olympic games organised in       rajasthan a     few months
2024-02-05 23:38:39,651 	Text Hypothesis :	*** the  final        of the u-19 tournament was   held    on    5th       february between   india and japan 
2024-02-05 23:38:39,651 	Text Alignment  :	D   S    S            S      I    I          S     S       S     S         S        S         S     S   S     
2024-02-05 23:38:39,651 ========================================================================================================================
2024-02-05 23:38:44,124 Epoch 295: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.33 
2024-02-05 23:38:44,125 EPOCH 296
2024-02-05 23:38:48,742 Epoch 296: Total Training Recognition Loss 0.10  Total Training Translation Loss 4.01 
2024-02-05 23:38:48,742 EPOCH 297
2024-02-05 23:38:53,759 Epoch 297: Total Training Recognition Loss 0.09  Total Training Translation Loss 4.98 
2024-02-05 23:38:53,760 EPOCH 298
2024-02-05 23:38:53,999 [Epoch: 298 Step: 00010100] Batch Recognition Loss:   0.000956 => Gls Tokens per Sec:     2700 || Batch Translation Loss:   0.099137 => Txt Tokens per Sec:     7464 || Lr: 0.000100
2024-02-05 23:38:58,392 Epoch 298: Total Training Recognition Loss 0.12  Total Training Translation Loss 4.80 
2024-02-05 23:38:58,392 EPOCH 299
2024-02-05 23:39:02,649 Epoch 299: Total Training Recognition Loss 0.11  Total Training Translation Loss 9.94 
2024-02-05 23:39:02,650 EPOCH 300
2024-02-05 23:39:07,563 [Epoch: 300 Step: 00010200] Batch Recognition Loss:   0.001830 => Gls Tokens per Sec:     2162 || Batch Translation Loss:   0.154651 => Txt Tokens per Sec:     5980 || Lr: 0.000100
2024-02-05 23:39:07,564 Epoch 300: Total Training Recognition Loss 0.13  Total Training Translation Loss 7.47 
2024-02-05 23:39:07,564 EPOCH 301
2024-02-05 23:39:11,766 Epoch 301: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.53 
2024-02-05 23:39:11,766 EPOCH 302
2024-02-05 23:39:16,798 Epoch 302: Total Training Recognition Loss 0.16  Total Training Translation Loss 5.35 
2024-02-05 23:39:16,798 EPOCH 303
2024-02-05 23:39:20,824 [Epoch: 303 Step: 00010300] Batch Recognition Loss:   0.002104 => Gls Tokens per Sec:     2480 || Batch Translation Loss:   0.065618 => Txt Tokens per Sec:     6872 || Lr: 0.000100
2024-02-05 23:39:21,048 Epoch 303: Total Training Recognition Loss 0.11  Total Training Translation Loss 2.97 
2024-02-05 23:39:21,048 EPOCH 304
2024-02-05 23:39:25,926 Epoch 304: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.07 
2024-02-05 23:39:25,926 EPOCH 305
2024-02-05 23:39:30,328 Epoch 305: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.14 
2024-02-05 23:39:30,328 EPOCH 306
2024-02-05 23:39:34,451 [Epoch: 306 Step: 00010400] Batch Recognition Loss:   0.000405 => Gls Tokens per Sec:     2266 || Batch Translation Loss:   0.015633 => Txt Tokens per Sec:     6366 || Lr: 0.000100
2024-02-05 23:39:34,911 Epoch 306: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.73 
2024-02-05 23:39:34,911 EPOCH 307
2024-02-05 23:39:39,496 Epoch 307: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.94 
2024-02-05 23:39:39,496 EPOCH 308
2024-02-05 23:39:44,010 Epoch 308: Total Training Recognition Loss 0.11  Total Training Translation Loss 1.97 
2024-02-05 23:39:44,011 EPOCH 309
2024-02-05 23:39:47,761 [Epoch: 309 Step: 00010500] Batch Recognition Loss:   0.001342 => Gls Tokens per Sec:     2321 || Batch Translation Loss:   0.053714 => Txt Tokens per Sec:     6411 || Lr: 0.000100
2024-02-05 23:39:48,762 Epoch 309: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.59 
2024-02-05 23:39:48,763 EPOCH 310
2024-02-05 23:39:53,044 Epoch 310: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.21 
2024-02-05 23:39:53,044 EPOCH 311
2024-02-05 23:39:57,920 Epoch 311: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.61 
2024-02-05 23:39:57,921 EPOCH 312
2024-02-05 23:40:01,087 [Epoch: 312 Step: 00010600] Batch Recognition Loss:   0.004550 => Gls Tokens per Sec:     2546 || Batch Translation Loss:   0.178906 => Txt Tokens per Sec:     6941 || Lr: 0.000100
2024-02-05 23:40:02,182 Epoch 312: Total Training Recognition Loss 0.14  Total Training Translation Loss 7.06 
2024-02-05 23:40:02,183 EPOCH 313
2024-02-05 23:40:07,101 Epoch 313: Total Training Recognition Loss 0.10  Total Training Translation Loss 7.26 
2024-02-05 23:40:07,101 EPOCH 314
2024-02-05 23:40:11,497 Epoch 314: Total Training Recognition Loss 0.10  Total Training Translation Loss 9.58 
2024-02-05 23:40:11,498 EPOCH 315
2024-02-05 23:40:15,124 [Epoch: 315 Step: 00010700] Batch Recognition Loss:   0.001267 => Gls Tokens per Sec:     2119 || Batch Translation Loss:   0.167631 => Txt Tokens per Sec:     5775 || Lr: 0.000100
2024-02-05 23:40:16,428 Epoch 315: Total Training Recognition Loss 0.08  Total Training Translation Loss 5.42 
2024-02-05 23:40:16,429 EPOCH 316
2024-02-05 23:40:20,569 Epoch 316: Total Training Recognition Loss 0.09  Total Training Translation Loss 5.09 
2024-02-05 23:40:20,570 EPOCH 317
2024-02-05 23:40:25,536 Epoch 317: Total Training Recognition Loss 0.11  Total Training Translation Loss 4.24 
2024-02-05 23:40:25,537 EPOCH 318
2024-02-05 23:40:28,236 [Epoch: 318 Step: 00010800] Batch Recognition Loss:   0.001403 => Gls Tokens per Sec:     2608 || Batch Translation Loss:   0.181332 => Txt Tokens per Sec:     7355 || Lr: 0.000100
2024-02-05 23:40:29,757 Epoch 318: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.62 
2024-02-05 23:40:29,758 EPOCH 319
2024-02-05 23:40:34,724 Epoch 319: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.71 
2024-02-05 23:40:34,724 EPOCH 320
2024-02-05 23:40:39,174 Epoch 320: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.89 
2024-02-05 23:40:39,175 EPOCH 321
2024-02-05 23:40:41,606 [Epoch: 321 Step: 00010900] Batch Recognition Loss:   0.002682 => Gls Tokens per Sec:     2527 || Batch Translation Loss:   0.061749 => Txt Tokens per Sec:     6978 || Lr: 0.000100
2024-02-05 23:40:43,784 Epoch 321: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.21 
2024-02-05 23:40:43,785 EPOCH 322
2024-02-05 23:40:48,372 Epoch 322: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.97 
2024-02-05 23:40:48,372 EPOCH 323
2024-02-05 23:40:52,950 Epoch 323: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.75 
2024-02-05 23:40:52,951 EPOCH 324
2024-02-05 23:40:55,156 [Epoch: 324 Step: 00011000] Batch Recognition Loss:   0.000493 => Gls Tokens per Sec:     2613 || Batch Translation Loss:   0.040743 => Txt Tokens per Sec:     6950 || Lr: 0.000100
2024-02-05 23:40:57,694 Epoch 324: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.78 
2024-02-05 23:40:57,694 EPOCH 325
2024-02-05 23:41:02,136 Epoch 325: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.00 
2024-02-05 23:41:02,137 EPOCH 326
2024-02-05 23:41:06,968 Epoch 326: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.36 
2024-02-05 23:41:06,968 EPOCH 327
2024-02-05 23:41:08,838 [Epoch: 327 Step: 00011100] Batch Recognition Loss:   0.000920 => Gls Tokens per Sec:     2599 || Batch Translation Loss:   0.038316 => Txt Tokens per Sec:     6982 || Lr: 0.000100
2024-02-05 23:41:11,164 Epoch 327: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.35 
2024-02-05 23:41:11,164 EPOCH 328
2024-02-05 23:41:16,067 Epoch 328: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.72 
2024-02-05 23:41:16,068 EPOCH 329
2024-02-05 23:41:20,646 Epoch 329: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.77 
2024-02-05 23:41:20,647 EPOCH 330
2024-02-05 23:41:22,398 [Epoch: 330 Step: 00011200] Batch Recognition Loss:   0.000437 => Gls Tokens per Sec:     2411 || Batch Translation Loss:   0.064678 => Txt Tokens per Sec:     6418 || Lr: 0.000100
2024-02-05 23:41:25,350 Epoch 330: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.10 
2024-02-05 23:41:25,350 EPOCH 331
2024-02-05 23:41:29,491 Epoch 331: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.00 
2024-02-05 23:41:29,492 EPOCH 332
2024-02-05 23:41:34,438 Epoch 332: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.29 
2024-02-05 23:41:34,439 EPOCH 333
2024-02-05 23:41:35,742 [Epoch: 333 Step: 00011300] Batch Recognition Loss:   0.003007 => Gls Tokens per Sec:     2749 || Batch Translation Loss:   0.182867 => Txt Tokens per Sec:     7366 || Lr: 0.000100
2024-02-05 23:41:38,629 Epoch 333: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.99 
2024-02-05 23:41:38,630 EPOCH 334
2024-02-05 23:41:43,589 Epoch 334: Total Training Recognition Loss 0.08  Total Training Translation Loss 6.71 
2024-02-05 23:41:43,589 EPOCH 335
2024-02-05 23:41:48,298 Epoch 335: Total Training Recognition Loss 0.08  Total Training Translation Loss 9.08 
2024-02-05 23:41:48,299 EPOCH 336
2024-02-05 23:41:49,724 [Epoch: 336 Step: 00011400] Batch Recognition Loss:   0.005760 => Gls Tokens per Sec:     2065 || Batch Translation Loss:   2.659433 => Txt Tokens per Sec:     5829 || Lr: 0.000100
2024-02-05 23:41:53,229 Epoch 336: Total Training Recognition Loss 0.12  Total Training Translation Loss 15.61 
2024-02-05 23:41:53,229 EPOCH 337
2024-02-05 23:41:57,401 Epoch 337: Total Training Recognition Loss 0.14  Total Training Translation Loss 10.58 
2024-02-05 23:41:57,401 EPOCH 338
2024-02-05 23:42:01,729 Epoch 338: Total Training Recognition Loss 0.14  Total Training Translation Loss 13.36 
2024-02-05 23:42:01,730 EPOCH 339
2024-02-05 23:42:02,697 [Epoch: 339 Step: 00011500] Batch Recognition Loss:   0.001579 => Gls Tokens per Sec:     2652 || Batch Translation Loss:   0.367171 => Txt Tokens per Sec:     7011 || Lr: 0.000100
2024-02-05 23:42:06,599 Epoch 339: Total Training Recognition Loss 0.26  Total Training Translation Loss 14.35 
2024-02-05 23:42:06,600 EPOCH 340
2024-02-05 23:42:11,194 Epoch 340: Total Training Recognition Loss 0.19  Total Training Translation Loss 10.24 
2024-02-05 23:42:11,194 EPOCH 341
2024-02-05 23:42:16,228 Epoch 341: Total Training Recognition Loss 0.18  Total Training Translation Loss 10.68 
2024-02-05 23:42:16,229 EPOCH 342
2024-02-05 23:42:16,942 [Epoch: 342 Step: 00011600] Batch Recognition Loss:   0.001567 => Gls Tokens per Sec:     2697 || Batch Translation Loss:   0.105730 => Txt Tokens per Sec:     7063 || Lr: 0.000100
2024-02-05 23:42:20,887 Epoch 342: Total Training Recognition Loss 0.17  Total Training Translation Loss 4.46 
2024-02-05 23:42:20,888 EPOCH 343
2024-02-05 23:42:25,023 Epoch 343: Total Training Recognition Loss 0.48  Total Training Translation Loss 2.67 
2024-02-05 23:42:25,024 EPOCH 344
2024-02-05 23:42:29,950 Epoch 344: Total Training Recognition Loss 2.55  Total Training Translation Loss 3.50 
2024-02-05 23:42:29,951 EPOCH 345
2024-02-05 23:42:30,357 [Epoch: 345 Step: 00011700] Batch Recognition Loss:   0.008662 => Gls Tokens per Sec:     3160 || Batch Translation Loss:   0.131739 => Txt Tokens per Sec:     7553 || Lr: 0.000100
2024-02-05 23:42:34,211 Epoch 345: Total Training Recognition Loss 2.54  Total Training Translation Loss 2.85 
2024-02-05 23:42:34,211 EPOCH 346
2024-02-05 23:42:39,106 Epoch 346: Total Training Recognition Loss 1.69  Total Training Translation Loss 2.82 
2024-02-05 23:42:39,107 EPOCH 347
2024-02-05 23:42:43,453 Epoch 347: Total Training Recognition Loss 4.05  Total Training Translation Loss 2.68 
2024-02-05 23:42:43,453 EPOCH 348
2024-02-05 23:42:43,805 [Epoch: 348 Step: 00011800] Batch Recognition Loss:   0.075246 => Gls Tokens per Sec:     1827 || Batch Translation Loss:   0.105343 => Txt Tokens per Sec:     5847 || Lr: 0.000100
2024-02-05 23:42:48,240 Epoch 348: Total Training Recognition Loss 0.46  Total Training Translation Loss 2.17 
2024-02-05 23:42:48,241 EPOCH 349
2024-02-05 23:42:52,699 Epoch 349: Total Training Recognition Loss 0.23  Total Training Translation Loss 1.69 
2024-02-05 23:42:52,700 EPOCH 350
2024-02-05 23:42:57,482 [Epoch: 350 Step: 00011900] Batch Recognition Loss:   0.000687 => Gls Tokens per Sec:     2222 || Batch Translation Loss:   0.074886 => Txt Tokens per Sec:     6147 || Lr: 0.000100
2024-02-05 23:42:57,482 Epoch 350: Total Training Recognition Loss 0.15  Total Training Translation Loss 2.17 
2024-02-05 23:42:57,483 EPOCH 351
2024-02-05 23:43:02,343 Epoch 351: Total Training Recognition Loss 0.12  Total Training Translation Loss 2.29 
2024-02-05 23:43:02,343 EPOCH 352
2024-02-05 23:43:06,448 Epoch 352: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.07 
2024-02-05 23:43:06,448 EPOCH 353
2024-02-05 23:43:10,963 [Epoch: 353 Step: 00012000] Batch Recognition Loss:   0.000418 => Gls Tokens per Sec:     2212 || Batch Translation Loss:   0.042480 => Txt Tokens per Sec:     6056 || Lr: 0.000100
2024-02-05 23:43:19,641 Validation result at epoch 353, step    12000: duration: 8.6786s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.74011	Translation Loss: 92937.74219	PPL: 10749.71680
	Eval Metric: BLEU
	WER 4.03	(DEL: 0.00,	INS: 0.00,	SUB: 4.03)
	BLEU-4 0.63	(BLEU-1: 11.20,	BLEU-2: 3.54,	BLEU-3: 1.40,	BLEU-4: 0.63)
	CHRF 17.14	ROUGE 9.57
2024-02-05 23:43:19,643 Logging Recognition and Translation Outputs
2024-02-05 23:43:19,643 ========================================================================================================================
2024-02-05 23:43:19,643 Logging Sequence: 168_56.00
2024-02-05 23:43:19,643 	Gloss Reference :	A B+C+D+E
2024-02-05 23:43:19,643 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:43:19,644 	Gloss Alignment :	         
2024-02-05 23:43:19,644 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:43:19,645 	Text Reference  :	** *** ****** fans    have  been waiting to see vamika for  a     long  time   
2024-02-05 23:43:19,645 	Text Hypothesis :	in the second innings india has  bowled  in the first  over while these attacks
2024-02-05 23:43:19,645 	Text Alignment  :	I  I   I      S       S     S    S       S  S   S      S    S     S     S      
2024-02-05 23:43:19,645 ========================================================================================================================
2024-02-05 23:43:19,645 Logging Sequence: 161_74.00
2024-02-05 23:43:19,645 	Gloss Reference :	A B+C+D+E
2024-02-05 23:43:19,646 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:43:19,646 	Gloss Alignment :	         
2024-02-05 23:43:19,646 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:43:19,647 	Text Reference  :	*** **** **** i     am     proud    of the ****** **** indian team's achievements
2024-02-05 23:43:19,647 	Text Hypothesis :	the bcci then rohit sharma escorted by the rising pune for    the    team        
2024-02-05 23:43:19,647 	Text Alignment  :	I   I    I    S     S      S        S      I      I    S      S      S           
2024-02-05 23:43:19,647 ========================================================================================================================
2024-02-05 23:43:19,647 Logging Sequence: 111_83.00
2024-02-05 23:43:19,647 	Gloss Reference :	A B+C+D+E
2024-02-05 23:43:19,647 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:43:19,648 	Gloss Alignment :	         
2024-02-05 23:43:19,648 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:43:19,649 	Text Reference  :	** and  the other 10 team members are     fined 25 of     the      match fee or    rs 6  lakh
2024-02-05 23:43:19,649 	Text Hypothesis :	in june the ***** ** **** bcci    doctors set   to resume training but   was fined rs 30 lakh
2024-02-05 23:43:19,650 	Text Alignment  :	I  S        D     D  D    S       S       S     S  S      S        S     S   S        S      
2024-02-05 23:43:19,650 ========================================================================================================================
2024-02-05 23:43:19,650 Logging Sequence: 61_218.00
2024-02-05 23:43:19,650 	Gloss Reference :	A B+C+D+E
2024-02-05 23:43:19,650 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:43:19,650 	Gloss Alignment :	         
2024-02-05 23:43:19,651 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:43:19,651 	Text Reference  :	in 2020 a woman had said    at the  press   conference
2024-02-05 23:43:19,651 	Text Hypothesis :	in **** a ***** *** caption on 14th october 2023      
2024-02-05 23:43:19,651 	Text Alignment  :	   D      D     D   S       S  S    S       S         
2024-02-05 23:43:19,652 ========================================================================================================================
2024-02-05 23:43:19,652 Logging Sequence: 94_123.00
2024-02-05 23:43:19,652 	Gloss Reference :	A B+C+D+E
2024-02-05 23:43:19,652 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:43:19,652 	Gloss Alignment :	         
2024-02-05 23:43:19,652 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:43:19,655 	Text Reference  :	**** * the venue   narendra modi stadium for the     india-pakistan match has     been kept the same    people  can book  flights etc
2024-02-05 23:43:19,655 	Text Hypothesis :	only 1 day earlier ie       14th october was decided by             14th  october and  does not majorly disrupt the plans of      ipl
2024-02-05 23:43:19,655 	Text Alignment  :	I    I S   S       S        S    S       S   S       S              S     S       S    S    S   S       S       S   S     S       S  
2024-02-05 23:43:19,655 ========================================================================================================================
2024-02-05 23:43:20,063 Epoch 353: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.90 
2024-02-05 23:43:20,063 EPOCH 354
2024-02-05 23:43:24,814 Epoch 354: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.50 
2024-02-05 23:43:24,814 EPOCH 355
2024-02-05 23:43:29,350 Epoch 355: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.39 
2024-02-05 23:43:29,350 EPOCH 356
2024-02-05 23:43:33,552 [Epoch: 356 Step: 00012100] Batch Recognition Loss:   0.000704 => Gls Tokens per Sec:     2223 || Batch Translation Loss:   0.027361 => Txt Tokens per Sec:     6136 || Lr: 0.000100
2024-02-05 23:43:33,982 Epoch 356: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.07 
2024-02-05 23:43:33,983 EPOCH 357
2024-02-05 23:43:38,508 Epoch 357: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.17 
2024-02-05 23:43:38,509 EPOCH 358
2024-02-05 23:43:43,314 Epoch 358: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.07 
2024-02-05 23:43:43,315 EPOCH 359
2024-02-05 23:43:46,740 [Epoch: 359 Step: 00012200] Batch Recognition Loss:   0.000381 => Gls Tokens per Sec:     2541 || Batch Translation Loss:   0.014709 => Txt Tokens per Sec:     7019 || Lr: 0.000100
2024-02-05 23:43:47,658 Epoch 359: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.39 
2024-02-05 23:43:47,658 EPOCH 360
2024-02-05 23:43:52,510 Epoch 360: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.75 
2024-02-05 23:43:52,511 EPOCH 361
2024-02-05 23:43:57,475 Epoch 361: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.60 
2024-02-05 23:43:57,476 EPOCH 362
2024-02-05 23:44:01,543 [Epoch: 362 Step: 00012300] Batch Recognition Loss:   0.001698 => Gls Tokens per Sec:     1982 || Batch Translation Loss:   0.071428 => Txt Tokens per Sec:     5576 || Lr: 0.000100
2024-02-05 23:44:02,614 Epoch 362: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.65 
2024-02-05 23:44:02,614 EPOCH 363
2024-02-05 23:44:06,951 Epoch 363: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.46 
2024-02-05 23:44:06,951 EPOCH 364
2024-02-05 23:44:11,695 Epoch 364: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.52 
2024-02-05 23:44:11,695 EPOCH 365
2024-02-05 23:44:14,968 [Epoch: 365 Step: 00012400] Batch Recognition Loss:   0.001079 => Gls Tokens per Sec:     2268 || Batch Translation Loss:   0.036203 => Txt Tokens per Sec:     6353 || Lr: 0.000100
2024-02-05 23:44:16,112 Epoch 365: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.38 
2024-02-05 23:44:16,112 EPOCH 366
2024-02-05 23:44:20,206 Epoch 366: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.11 
2024-02-05 23:44:20,207 EPOCH 367
2024-02-05 23:44:24,361 Epoch 367: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.39 
2024-02-05 23:44:24,361 EPOCH 368
2024-02-05 23:44:27,282 [Epoch: 368 Step: 00012500] Batch Recognition Loss:   0.000272 => Gls Tokens per Sec:     2321 || Batch Translation Loss:   0.112093 => Txt Tokens per Sec:     6634 || Lr: 0.000100
2024-02-05 23:44:28,498 Epoch 368: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.48 
2024-02-05 23:44:28,499 EPOCH 369
2024-02-05 23:44:33,074 Epoch 369: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.90 
2024-02-05 23:44:33,075 EPOCH 370
2024-02-05 23:44:37,879 Epoch 370: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.96 
2024-02-05 23:44:37,880 EPOCH 371
2024-02-05 23:44:40,727 [Epoch: 371 Step: 00012600] Batch Recognition Loss:   0.001607 => Gls Tokens per Sec:     2157 || Batch Translation Loss:   0.166497 => Txt Tokens per Sec:     5890 || Lr: 0.000100
2024-02-05 23:44:42,705 Epoch 371: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.53 
2024-02-05 23:44:42,706 EPOCH 372
2024-02-05 23:44:47,430 Epoch 372: Total Training Recognition Loss 0.05  Total Training Translation Loss 6.45 
2024-02-05 23:44:47,430 EPOCH 373
2024-02-05 23:44:52,283 Epoch 373: Total Training Recognition Loss 0.09  Total Training Translation Loss 6.90 
2024-02-05 23:44:52,283 EPOCH 374
2024-02-05 23:44:54,982 [Epoch: 374 Step: 00012700] Batch Recognition Loss:   0.000569 => Gls Tokens per Sec:     2038 || Batch Translation Loss:   0.213272 => Txt Tokens per Sec:     5695 || Lr: 0.000100
2024-02-05 23:44:57,061 Epoch 374: Total Training Recognition Loss 0.07  Total Training Translation Loss 5.92 
2024-02-05 23:44:57,062 EPOCH 375
2024-02-05 23:45:01,801 Epoch 375: Total Training Recognition Loss 0.07  Total Training Translation Loss 5.35 
2024-02-05 23:45:01,802 EPOCH 376
2024-02-05 23:45:06,696 Epoch 376: Total Training Recognition Loss 0.07  Total Training Translation Loss 7.08 
2024-02-05 23:45:06,696 EPOCH 377
2024-02-05 23:45:08,622 [Epoch: 377 Step: 00012800] Batch Recognition Loss:   0.001222 => Gls Tokens per Sec:     2661 || Batch Translation Loss:   0.120733 => Txt Tokens per Sec:     7508 || Lr: 0.000100
2024-02-05 23:45:11,136 Epoch 377: Total Training Recognition Loss 0.08  Total Training Translation Loss 6.61 
2024-02-05 23:45:11,136 EPOCH 378
2024-02-05 23:45:15,976 Epoch 378: Total Training Recognition Loss 0.08  Total Training Translation Loss 7.32 
2024-02-05 23:45:15,976 EPOCH 379
2024-02-05 23:45:20,209 Epoch 379: Total Training Recognition Loss 0.09  Total Training Translation Loss 5.70 
2024-02-05 23:45:20,210 EPOCH 380
2024-02-05 23:45:21,825 [Epoch: 380 Step: 00012900] Batch Recognition Loss:   0.000707 => Gls Tokens per Sec:     2614 || Batch Translation Loss:   0.150153 => Txt Tokens per Sec:     6797 || Lr: 0.000100
2024-02-05 23:45:25,121 Epoch 380: Total Training Recognition Loss 0.09  Total Training Translation Loss 6.66 
2024-02-05 23:45:25,121 EPOCH 381
2024-02-05 23:45:29,330 Epoch 381: Total Training Recognition Loss 0.12  Total Training Translation Loss 5.28 
2024-02-05 23:45:29,330 EPOCH 382
2024-02-05 23:45:34,299 Epoch 382: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.43 
2024-02-05 23:45:34,300 EPOCH 383
2024-02-05 23:45:35,791 [Epoch: 383 Step: 00013000] Batch Recognition Loss:   0.003242 => Gls Tokens per Sec:     2579 || Batch Translation Loss:   0.098975 => Txt Tokens per Sec:     6962 || Lr: 0.000100
2024-02-05 23:45:38,525 Epoch 383: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.42 
2024-02-05 23:45:38,525 EPOCH 384
2024-02-05 23:45:43,367 Epoch 384: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.95 
2024-02-05 23:45:43,367 EPOCH 385
2024-02-05 23:45:47,817 Epoch 385: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.09 
2024-02-05 23:45:47,817 EPOCH 386
2024-02-05 23:45:48,762 [Epoch: 386 Step: 00013100] Batch Recognition Loss:   0.000346 => Gls Tokens per Sec:     3388 || Batch Translation Loss:   0.030550 => Txt Tokens per Sec:     8642 || Lr: 0.000100
2024-02-05 23:45:52,382 Epoch 386: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.89 
2024-02-05 23:45:52,382 EPOCH 387
2024-02-05 23:45:57,076 Epoch 387: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.21 
2024-02-05 23:45:57,076 EPOCH 388
2024-02-05 23:46:01,541 Epoch 388: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.22 
2024-02-05 23:46:01,541 EPOCH 389
2024-02-05 23:46:02,465 [Epoch: 389 Step: 00013200] Batch Recognition Loss:   0.001168 => Gls Tokens per Sec:     2492 || Batch Translation Loss:   0.052820 => Txt Tokens per Sec:     6096 || Lr: 0.000100
2024-02-05 23:46:06,307 Epoch 389: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.40 
2024-02-05 23:46:06,308 EPOCH 390
2024-02-05 23:46:10,617 Epoch 390: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.03 
2024-02-05 23:46:10,618 EPOCH 391
2024-02-05 23:46:15,539 Epoch 391: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.31 
2024-02-05 23:46:15,540 EPOCH 392
2024-02-05 23:46:16,205 [Epoch: 392 Step: 00013300] Batch Recognition Loss:   0.000994 => Gls Tokens per Sec:     2892 || Batch Translation Loss:   0.041712 => Txt Tokens per Sec:     8227 || Lr: 0.000100
2024-02-05 23:46:19,682 Epoch 392: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.42 
2024-02-05 23:46:19,683 EPOCH 393
2024-02-05 23:46:24,636 Epoch 393: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.75 
2024-02-05 23:46:24,637 EPOCH 394
2024-02-05 23:46:28,873 Epoch 394: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.06 
2024-02-05 23:46:28,874 EPOCH 395
2024-02-05 23:46:29,421 [Epoch: 395 Step: 00013400] Batch Recognition Loss:   0.001750 => Gls Tokens per Sec:     2340 || Batch Translation Loss:   0.046369 => Txt Tokens per Sec:     6474 || Lr: 0.000100
2024-02-05 23:46:33,818 Epoch 395: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.61 
2024-02-05 23:46:33,819 EPOCH 396
2024-02-05 23:46:38,108 Epoch 396: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.66 
2024-02-05 23:46:38,108 EPOCH 397
2024-02-05 23:46:42,976 Epoch 397: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.58 
2024-02-05 23:46:42,977 EPOCH 398
2024-02-05 23:46:43,312 [Epoch: 398 Step: 00013500] Batch Recognition Loss:   0.000635 => Gls Tokens per Sec:     1916 || Batch Translation Loss:   0.041649 => Txt Tokens per Sec:     5069 || Lr: 0.000100
2024-02-05 23:46:47,306 Epoch 398: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.63 
2024-02-05 23:46:47,306 EPOCH 399
2024-02-05 23:46:52,050 Epoch 399: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.35 
2024-02-05 23:46:52,050 EPOCH 400
2024-02-05 23:46:56,526 [Epoch: 400 Step: 00013600] Batch Recognition Loss:   0.000780 => Gls Tokens per Sec:     2373 || Batch Translation Loss:   0.057883 => Txt Tokens per Sec:     6565 || Lr: 0.000100
2024-02-05 23:46:56,527 Epoch 400: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.49 
2024-02-05 23:46:56,527 EPOCH 401
2024-02-05 23:47:01,220 Epoch 401: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.31 
2024-02-05 23:47:01,221 EPOCH 402
2024-02-05 23:47:05,821 Epoch 402: Total Training Recognition Loss 0.06  Total Training Translation Loss 5.71 
2024-02-05 23:47:05,822 EPOCH 403
2024-02-05 23:47:10,148 [Epoch: 403 Step: 00013700] Batch Recognition Loss:   0.003117 => Gls Tokens per Sec:     2307 || Batch Translation Loss:   0.186280 => Txt Tokens per Sec:     6368 || Lr: 0.000100
2024-02-05 23:47:10,384 Epoch 403: Total Training Recognition Loss 0.08  Total Training Translation Loss 9.48 
2024-02-05 23:47:10,384 EPOCH 404
2024-02-05 23:47:15,162 Epoch 404: Total Training Recognition Loss 0.09  Total Training Translation Loss 8.07 
2024-02-05 23:47:15,162 EPOCH 405
2024-02-05 23:47:19,549 Epoch 405: Total Training Recognition Loss 0.08  Total Training Translation Loss 8.88 
2024-02-05 23:47:19,550 EPOCH 406
2024-02-05 23:47:23,775 [Epoch: 406 Step: 00013800] Batch Recognition Loss:   0.001165 => Gls Tokens per Sec:     2212 || Batch Translation Loss:   0.140223 => Txt Tokens per Sec:     6074 || Lr: 0.000100
2024-02-05 23:47:24,388 Epoch 406: Total Training Recognition Loss 0.09  Total Training Translation Loss 6.67 
2024-02-05 23:47:24,388 EPOCH 407
2024-02-05 23:47:28,602 Epoch 407: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.02 
2024-02-05 23:47:28,603 EPOCH 408
2024-02-05 23:47:33,525 Epoch 408: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.69 
2024-02-05 23:47:33,526 EPOCH 409
2024-02-05 23:47:37,069 [Epoch: 409 Step: 00013900] Batch Recognition Loss:   0.000308 => Gls Tokens per Sec:     2457 || Batch Translation Loss:   0.063813 => Txt Tokens per Sec:     6959 || Lr: 0.000100
2024-02-05 23:47:37,695 Epoch 409: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.44 
2024-02-05 23:47:37,695 EPOCH 410
2024-02-05 23:47:42,600 Epoch 410: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.28 
2024-02-05 23:47:42,601 EPOCH 411
2024-02-05 23:47:46,912 Epoch 411: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.95 
2024-02-05 23:47:46,912 EPOCH 412
2024-02-05 23:47:50,675 [Epoch: 412 Step: 00014000] Batch Recognition Loss:   0.000428 => Gls Tokens per Sec:     2212 || Batch Translation Loss:   0.057215 => Txt Tokens per Sec:     6075 || Lr: 0.000100
2024-02-05 23:47:59,349 Validation result at epoch 412, step    14000: duration: 8.6738s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.64353	Translation Loss: 94406.76562	PPL: 12448.57324
	Eval Metric: BLEU
	WER 4.03	(DEL: 0.00,	INS: 0.00,	SUB: 4.03)
	BLEU-4 0.51	(BLEU-1: 11.65,	BLEU-2: 3.74,	BLEU-3: 1.33,	BLEU-4: 0.51)
	CHRF 16.96	ROUGE 9.90
2024-02-05 23:47:59,351 Logging Recognition and Translation Outputs
2024-02-05 23:47:59,351 ========================================================================================================================
2024-02-05 23:47:59,351 Logging Sequence: 177_50.00
2024-02-05 23:47:59,351 	Gloss Reference :	A B+C+D+E
2024-02-05 23:47:59,351 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:47:59,351 	Gloss Alignment :	         
2024-02-05 23:47:59,352 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:47:59,353 	Text Reference  :	***** a     similar reward of    rs    50000  was announced    for    information against his associate ajay   kumar
2024-02-05 23:47:59,353 	Text Hypothesis :	after rana' death   a      delhi court issued a   non-bailable arrest warrant     against *** ********* sushil kumar
2024-02-05 23:47:59,353 	Text Alignment  :	I     S     S       S      S     S     S      S   S            S      S                   D   D         S           
2024-02-05 23:47:59,353 ========================================================================================================================
2024-02-05 23:47:59,354 Logging Sequence: 136_175.00
2024-02-05 23:47:59,354 	Gloss Reference :	A B+C+D+E      
2024-02-05 23:47:59,354 	Gloss Hypothesis:	A B+C+D+E+D+E+D
2024-02-05 23:47:59,354 	Gloss Alignment :	  S            
2024-02-05 23:47:59,354 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:47:59,355 	Text Reference  :	*** after     49          years     india' hockey team beat britain and qualified for the semi-finals
2024-02-05 23:47:59,356 	Text Hypothesis :	the broadcast advertisers ticketing etc    would  be   held in      the next      to  the world      
2024-02-05 23:47:59,356 	Text Alignment  :	I   S         S           S         S      S      S    S    S       S   S         S       S          
2024-02-05 23:47:59,356 ========================================================================================================================
2024-02-05 23:47:59,356 Logging Sequence: 126_159.00
2024-02-05 23:47:59,356 	Gloss Reference :	A B+C+D+E
2024-02-05 23:47:59,356 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:47:59,356 	Gloss Alignment :	         
2024-02-05 23:47:59,357 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:47:59,357 	Text Reference  :	despite multiple challenges and  injuries you did   not give    up   
2024-02-05 23:47:59,357 	Text Hypothesis :	******* ******** he         then took     a   medal in  javelin throw
2024-02-05 23:47:59,358 	Text Alignment  :	D       D        S          S    S        S   S     S   S       S    
2024-02-05 23:47:59,358 ========================================================================================================================
2024-02-05 23:47:59,358 Logging Sequence: 70_88.00
2024-02-05 23:47:59,358 	Gloss Reference :	A B+C+D+E
2024-02-05 23:47:59,358 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:47:59,358 	Gloss Alignment :	         
2024-02-05 23:47:59,358 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:47:59,359 	Text Reference  :	***** two  coca-cola bottles were placed   on   the table next  to the ******* mic     
2024-02-05 23:47:59,360 	Text Hypothesis :	after this there     was     a    backdrop with the ***** logos of the various sponsors
2024-02-05 23:47:59,360 	Text Alignment  :	I     S    S         S       S    S        S        D     S     S      I       S       
2024-02-05 23:47:59,360 ========================================================================================================================
2024-02-05 23:47:59,360 Logging Sequence: 54_201.00
2024-02-05 23:47:59,360 	Gloss Reference :	A B+C+D+E
2024-02-05 23:47:59,360 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:47:59,360 	Gloss Alignment :	         
2024-02-05 23:47:59,361 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:47:59,362 	Text Reference  :	there is a huge demand mostly from non-resident indians nris who are excited to see the match and they have  booked the hotel rooms
2024-02-05 23:47:59,362 	Text Hypothesis :	***** ** * **** ****** ****** **** ************ ******* **** *** *** ******* ** *** the bcci  can not  image of     the 2     years
2024-02-05 23:47:59,362 	Text Alignment  :	D     D  D D    D      D      D    D            D       D    D   D   D       D  D       S     S   S    S     S          S     S    
2024-02-05 23:47:59,362 ========================================================================================================================
2024-02-05 23:48:00,462 Epoch 412: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.15 
2024-02-05 23:48:00,462 EPOCH 413
2024-02-05 23:48:05,170 Epoch 413: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.13 
2024-02-05 23:48:05,170 EPOCH 414
2024-02-05 23:48:09,747 Epoch 414: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.07 
2024-02-05 23:48:09,748 EPOCH 415
2024-02-05 23:48:13,183 [Epoch: 415 Step: 00014100] Batch Recognition Loss:   0.003198 => Gls Tokens per Sec:     2161 || Batch Translation Loss:   0.044357 => Txt Tokens per Sec:     6093 || Lr: 0.000100
2024-02-05 23:48:14,436 Epoch 415: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.65 
2024-02-05 23:48:14,436 EPOCH 416
2024-02-05 23:48:18,962 Epoch 416: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.71 
2024-02-05 23:48:18,963 EPOCH 417
2024-02-05 23:48:23,700 Epoch 417: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.73 
2024-02-05 23:48:23,700 EPOCH 418
2024-02-05 23:48:26,574 [Epoch: 418 Step: 00014200] Batch Recognition Loss:   0.000242 => Gls Tokens per Sec:     2360 || Batch Translation Loss:   0.028949 => Txt Tokens per Sec:     6734 || Lr: 0.000100
2024-02-05 23:48:28,060 Epoch 418: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.45 
2024-02-05 23:48:28,060 EPOCH 419
2024-02-05 23:48:32,980 Epoch 419: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.53 
2024-02-05 23:48:32,980 EPOCH 420
2024-02-05 23:48:37,101 Epoch 420: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.57 
2024-02-05 23:48:37,101 EPOCH 421
2024-02-05 23:48:39,705 [Epoch: 421 Step: 00014300] Batch Recognition Loss:   0.000321 => Gls Tokens per Sec:     2460 || Batch Translation Loss:   0.093729 => Txt Tokens per Sec:     6549 || Lr: 0.000100
2024-02-05 23:48:42,113 Epoch 421: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.41 
2024-02-05 23:48:42,114 EPOCH 422
2024-02-05 23:48:46,365 Epoch 422: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.52 
2024-02-05 23:48:46,365 EPOCH 423
2024-02-05 23:48:51,417 Epoch 423: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.90 
2024-02-05 23:48:51,418 EPOCH 424
2024-02-05 23:48:53,587 [Epoch: 424 Step: 00014400] Batch Recognition Loss:   0.000348 => Gls Tokens per Sec:     2657 || Batch Translation Loss:   0.058072 => Txt Tokens per Sec:     6968 || Lr: 0.000100
2024-02-05 23:48:56,332 Epoch 424: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.67 
2024-02-05 23:48:56,332 EPOCH 425
2024-02-05 23:49:01,279 Epoch 425: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.81 
2024-02-05 23:49:01,279 EPOCH 426
2024-02-05 23:49:05,554 Epoch 426: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.92 
2024-02-05 23:49:05,554 EPOCH 427
2024-02-05 23:49:07,741 [Epoch: 427 Step: 00014500] Batch Recognition Loss:   0.000483 => Gls Tokens per Sec:     2223 || Batch Translation Loss:   0.073919 => Txt Tokens per Sec:     6345 || Lr: 0.000100
2024-02-05 23:49:10,207 Epoch 427: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.21 
2024-02-05 23:49:10,207 EPOCH 428
2024-02-05 23:49:14,834 Epoch 428: Total Training Recognition Loss 0.07  Total Training Translation Loss 6.10 
2024-02-05 23:49:14,834 EPOCH 429
2024-02-05 23:49:19,329 Epoch 429: Total Training Recognition Loss 0.08  Total Training Translation Loss 5.54 
2024-02-05 23:49:19,330 EPOCH 430
2024-02-05 23:49:21,289 [Epoch: 430 Step: 00014600] Batch Recognition Loss:   0.006094 => Gls Tokens per Sec:     2154 || Batch Translation Loss:   0.156926 => Txt Tokens per Sec:     6043 || Lr: 0.000100
2024-02-05 23:49:24,200 Epoch 430: Total Training Recognition Loss 0.07  Total Training Translation Loss 6.20 
2024-02-05 23:49:24,200 EPOCH 431
2024-02-05 23:49:28,471 Epoch 431: Total Training Recognition Loss 0.07  Total Training Translation Loss 5.19 
2024-02-05 23:49:28,472 EPOCH 432
2024-02-05 23:49:33,422 Epoch 432: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.63 
2024-02-05 23:49:33,422 EPOCH 433
2024-02-05 23:49:34,930 [Epoch: 433 Step: 00014700] Batch Recognition Loss:   0.000798 => Gls Tokens per Sec:     2377 || Batch Translation Loss:   0.065939 => Txt Tokens per Sec:     6595 || Lr: 0.000100
2024-02-05 23:49:37,900 Epoch 433: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.98 
2024-02-05 23:49:37,901 EPOCH 434
2024-02-05 23:49:42,758 Epoch 434: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.96 
2024-02-05 23:49:42,758 EPOCH 435
2024-02-05 23:49:47,774 Epoch 435: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.03 
2024-02-05 23:49:47,775 EPOCH 436
2024-02-05 23:49:49,117 [Epoch: 436 Step: 00014800] Batch Recognition Loss:   0.006145 => Gls Tokens per Sec:     2388 || Batch Translation Loss:   0.052646 => Txt Tokens per Sec:     6356 || Lr: 0.000100
2024-02-05 23:49:52,444 Epoch 436: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.94 
2024-02-05 23:49:52,444 EPOCH 437
2024-02-05 23:49:57,288 Epoch 437: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.46 
2024-02-05 23:49:57,288 EPOCH 438
2024-02-05 23:50:02,075 Epoch 438: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.03 
2024-02-05 23:50:02,075 EPOCH 439
2024-02-05 23:50:03,256 [Epoch: 439 Step: 00014900] Batch Recognition Loss:   0.000497 => Gls Tokens per Sec:     2171 || Batch Translation Loss:   0.036264 => Txt Tokens per Sec:     6524 || Lr: 0.000100
2024-02-05 23:50:06,164 Epoch 439: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.66 
2024-02-05 23:50:06,164 EPOCH 440
2024-02-05 23:50:11,042 Epoch 440: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.73 
2024-02-05 23:50:11,043 EPOCH 441
2024-02-05 23:50:15,750 Epoch 441: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.29 
2024-02-05 23:50:15,751 EPOCH 442
2024-02-05 23:50:16,374 [Epoch: 442 Step: 00015000] Batch Recognition Loss:   0.000322 => Gls Tokens per Sec:     3087 || Batch Translation Loss:   0.035107 => Txt Tokens per Sec:     7516 || Lr: 0.000100
2024-02-05 23:50:20,608 Epoch 442: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.39 
2024-02-05 23:50:20,608 EPOCH 443
2024-02-05 23:50:24,863 Epoch 443: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.05 
2024-02-05 23:50:24,864 EPOCH 444
2024-02-05 23:50:29,612 Epoch 444: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.84 
2024-02-05 23:50:29,613 EPOCH 445
2024-02-05 23:50:30,082 [Epoch: 445 Step: 00015100] Batch Recognition Loss:   0.001473 => Gls Tokens per Sec:     2741 || Batch Translation Loss:   0.178788 => Txt Tokens per Sec:     7340 || Lr: 0.000100
2024-02-05 23:50:34,378 Epoch 445: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.56 
2024-02-05 23:50:34,379 EPOCH 446
2024-02-05 23:50:39,232 Epoch 446: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.58 
2024-02-05 23:50:39,232 EPOCH 447
2024-02-05 23:50:43,333 Epoch 447: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.65 
2024-02-05 23:50:43,333 EPOCH 448
2024-02-05 23:50:43,646 [Epoch: 448 Step: 00015200] Batch Recognition Loss:   0.020992 => Gls Tokens per Sec:     2051 || Batch Translation Loss:   0.144248 => Txt Tokens per Sec:     6439 || Lr: 0.000100
2024-02-05 23:50:47,897 Epoch 448: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.39 
2024-02-05 23:50:47,897 EPOCH 449
2024-02-05 23:50:52,559 Epoch 449: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.51 
2024-02-05 23:50:52,559 EPOCH 450
2024-02-05 23:50:57,089 [Epoch: 450 Step: 00015300] Batch Recognition Loss:   0.001311 => Gls Tokens per Sec:     2345 || Batch Translation Loss:   0.018297 => Txt Tokens per Sec:     6487 || Lr: 0.000100
2024-02-05 23:50:57,090 Epoch 450: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.07 
2024-02-05 23:50:57,090 EPOCH 451
2024-02-05 23:51:02,145 Epoch 451: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.27 
2024-02-05 23:51:02,146 EPOCH 452
2024-02-05 23:51:06,850 Epoch 452: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.80 
2024-02-05 23:51:06,850 EPOCH 453
2024-02-05 23:51:11,359 [Epoch: 453 Step: 00015400] Batch Recognition Loss:   0.001017 => Gls Tokens per Sec:     2272 || Batch Translation Loss:   0.049843 => Txt Tokens per Sec:     6256 || Lr: 0.000100
2024-02-05 23:51:11,657 Epoch 453: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.16 
2024-02-05 23:51:11,657 EPOCH 454
2024-02-05 23:51:15,923 Epoch 454: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.86 
2024-02-05 23:51:15,924 EPOCH 455
2024-02-05 23:51:20,833 Epoch 455: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.97 
2024-02-05 23:51:20,834 EPOCH 456
2024-02-05 23:51:24,529 [Epoch: 456 Step: 00015500] Batch Recognition Loss:   0.000597 => Gls Tokens per Sec:     2528 || Batch Translation Loss:   0.007284 => Txt Tokens per Sec:     7038 || Lr: 0.000100
2024-02-05 23:51:25,070 Epoch 456: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.91 
2024-02-05 23:51:25,071 EPOCH 457
2024-02-05 23:51:30,234 Epoch 457: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.08 
2024-02-05 23:51:30,235 EPOCH 458
2024-02-05 23:51:34,622 Epoch 458: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.25 
2024-02-05 23:51:34,622 EPOCH 459
2024-02-05 23:51:38,335 [Epoch: 459 Step: 00015600] Batch Recognition Loss:   0.004504 => Gls Tokens per Sec:     2344 || Batch Translation Loss:   0.098041 => Txt Tokens per Sec:     6532 || Lr: 0.000100
2024-02-05 23:51:39,162 Epoch 459: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.66 
2024-02-05 23:51:39,162 EPOCH 460
2024-02-05 23:51:43,867 Epoch 460: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.35 
2024-02-05 23:51:43,868 EPOCH 461
2024-02-05 23:51:47,995 Epoch 461: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.65 
2024-02-05 23:51:47,996 EPOCH 462
2024-02-05 23:51:51,999 [Epoch: 462 Step: 00015700] Batch Recognition Loss:   0.000853 => Gls Tokens per Sec:     2014 || Batch Translation Loss:   0.076708 => Txt Tokens per Sec:     5698 || Lr: 0.000100
2024-02-05 23:51:52,988 Epoch 462: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.99 
2024-02-05 23:51:52,989 EPOCH 463
2024-02-05 23:51:57,276 Epoch 463: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.14 
2024-02-05 23:51:57,276 EPOCH 464
2024-02-05 23:52:02,189 Epoch 464: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.59 
2024-02-05 23:52:02,190 EPOCH 465
2024-02-05 23:52:05,444 [Epoch: 465 Step: 00015800] Batch Recognition Loss:   0.001263 => Gls Tokens per Sec:     2282 || Batch Translation Loss:   0.088764 => Txt Tokens per Sec:     6497 || Lr: 0.000100
2024-02-05 23:52:06,490 Epoch 465: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.20 
2024-02-05 23:52:06,490 EPOCH 466
2024-02-05 23:52:11,248 Epoch 466: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.82 
2024-02-05 23:52:11,249 EPOCH 467
2024-02-05 23:52:15,776 Epoch 467: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.16 
2024-02-05 23:52:15,776 EPOCH 468
2024-02-05 23:52:18,985 [Epoch: 468 Step: 00015900] Batch Recognition Loss:   0.000358 => Gls Tokens per Sec:     2114 || Batch Translation Loss:   0.076271 => Txt Tokens per Sec:     6075 || Lr: 0.000100
2024-02-05 23:52:20,504 Epoch 468: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.26 
2024-02-05 23:52:20,504 EPOCH 469
2024-02-05 23:52:25,022 Epoch 469: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.43 
2024-02-05 23:52:25,022 EPOCH 470
2024-02-05 23:52:29,185 Epoch 470: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.86 
2024-02-05 23:52:29,186 EPOCH 471
2024-02-05 23:52:32,009 [Epoch: 471 Step: 00016000] Batch Recognition Loss:   0.000396 => Gls Tokens per Sec:     2176 || Batch Translation Loss:   0.009169 => Txt Tokens per Sec:     6423 || Lr: 0.000100
2024-02-05 23:52:40,695 Validation result at epoch 471, step    16000: duration: 8.6850s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.63675	Translation Loss: 94130.39062	PPL: 12109.62988
	Eval Metric: BLEU
	WER 4.38	(DEL: 0.00,	INS: 0.00,	SUB: 4.38)
	BLEU-4 0.62	(BLEU-1: 11.08,	BLEU-2: 3.22,	BLEU-3: 1.20,	BLEU-4: 0.62)
	CHRF 16.52	ROUGE 9.37
2024-02-05 23:52:40,697 Logging Recognition and Translation Outputs
2024-02-05 23:52:40,697 ========================================================================================================================
2024-02-05 23:52:40,697 Logging Sequence: 163_116.00
2024-02-05 23:52:40,697 	Gloss Reference :	A B+C+D+E
2024-02-05 23:52:40,697 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:52:40,697 	Gloss Alignment :	         
2024-02-05 23:52:40,698 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:52:40,698 	Text Reference  :	people said that ***** she   looked similar to ** ******** ** virat   
2024-02-05 23:52:40,698 	Text Hypothesis :	they   say  that virat kohli and    respect to be pictures of pictures
2024-02-05 23:52:40,699 	Text Alignment  :	S      S         I     S     S      S          I  I        I  S       
2024-02-05 23:52:40,699 ========================================================================================================================
2024-02-05 23:52:40,699 Logging Sequence: 53_161.00
2024-02-05 23:52:40,699 	Gloss Reference :	A B+C+D+E
2024-02-05 23:52:40,699 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:52:40,699 	Gloss Alignment :	         
2024-02-05 23:52:40,699 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:52:40,701 	Text Reference  :	rashid has also been urging people to  donate  to      his      rashid  khan foundation  and   afghanistan cricket association
2024-02-05 23:52:40,701 	Text Hypothesis :	****** *** **** **** ****** ****** the taliban swiftly regained control of   afghanistan after us          troops  withdrawal 
2024-02-05 23:52:40,701 	Text Alignment  :	D      D   D    D    D      D      S   S       S       S        S       S    S           S     S           S       S          
2024-02-05 23:52:40,701 ========================================================================================================================
2024-02-05 23:52:40,701 Logging Sequence: 67_73.00
2024-02-05 23:52:40,701 	Gloss Reference :	A B+C+D+E
2024-02-05 23:52:40,701 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:52:40,702 	Gloss Alignment :	         
2024-02-05 23:52:40,702 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:52:40,702 	Text Reference  :	*** ***** **** **** ******** ** in        his tweet he  also said  
2024-02-05 23:52:40,702 	Text Hypothesis :	the first time that everyone is currently all out   for the  series
2024-02-05 23:52:40,703 	Text Alignment  :	I   I     I    I    I        I  S         S   S     S   S    S     
2024-02-05 23:52:40,703 ========================================================================================================================
2024-02-05 23:52:40,703 Logging Sequence: 137_44.00
2024-02-05 23:52:40,703 	Gloss Reference :	A B+C+D+E
2024-02-05 23:52:40,703 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:52:40,703 	Gloss Alignment :	         
2024-02-05 23:52:40,703 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:52:40,705 	Text Reference  :	let me tell you the rules that qatar  has announced for the fans travelling for the world cup  
2024-02-05 23:52:40,705 	Text Hypothesis :	*** ** **** *** the ***** **** series was played    at  the **** ********** age of  18    years
2024-02-05 23:52:40,705 	Text Alignment  :	D   D  D    D       D     D    S      S   S         S       D    D          S   S   S     S    
2024-02-05 23:52:40,705 ========================================================================================================================
2024-02-05 23:52:40,705 Logging Sequence: 99_158.00
2024-02-05 23:52:40,705 	Gloss Reference :	A B+C+D+E
2024-02-05 23:52:40,705 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:52:40,705 	Gloss Alignment :	         
2024-02-05 23:52:40,706 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:52:40,706 	Text Reference  :	*** the  incident occured  in  dubai and *********** ** it  was extremely shameful
2024-02-05 23:52:40,707 	Text Hypothesis :	his fans love     watching all posts and participate in the t20 world     cup     
2024-02-05 23:52:40,707 	Text Alignment  :	I   S    S        S        S   S         I           I  S   S   S         S       
2024-02-05 23:52:40,707 ========================================================================================================================
2024-02-05 23:52:42,246 Epoch 471: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.52 
2024-02-05 23:52:42,246 EPOCH 472
2024-02-05 23:52:47,294 Epoch 472: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.16 
2024-02-05 23:52:47,295 EPOCH 473
2024-02-05 23:52:51,695 Epoch 473: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.52 
2024-02-05 23:52:51,695 EPOCH 474
2024-02-05 23:52:54,113 [Epoch: 474 Step: 00016100] Batch Recognition Loss:   0.000328 => Gls Tokens per Sec:     2276 || Batch Translation Loss:   0.089649 => Txt Tokens per Sec:     6372 || Lr: 0.000100
2024-02-05 23:52:56,391 Epoch 474: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.20 
2024-02-05 23:52:56,392 EPOCH 475
2024-02-05 23:53:00,932 Epoch 475: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.42 
2024-02-05 23:53:00,932 EPOCH 476
2024-02-05 23:53:05,502 Epoch 476: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.19 
2024-02-05 23:53:05,502 EPOCH 477
2024-02-05 23:53:07,690 [Epoch: 477 Step: 00016200] Batch Recognition Loss:   0.002160 => Gls Tokens per Sec:     2222 || Batch Translation Loss:   0.046466 => Txt Tokens per Sec:     6339 || Lr: 0.000100
2024-02-05 23:53:10,209 Epoch 477: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.51 
2024-02-05 23:53:10,209 EPOCH 478
2024-02-05 23:53:14,431 Epoch 478: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.06 
2024-02-05 23:53:14,431 EPOCH 479
2024-02-05 23:53:19,368 Epoch 479: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.56 
2024-02-05 23:53:19,368 EPOCH 480
2024-02-05 23:53:20,956 [Epoch: 480 Step: 00016300] Batch Recognition Loss:   0.001508 => Gls Tokens per Sec:     2660 || Batch Translation Loss:   0.053013 => Txt Tokens per Sec:     7181 || Lr: 0.000100
2024-02-05 23:53:23,564 Epoch 480: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.20 
2024-02-05 23:53:23,564 EPOCH 481
2024-02-05 23:53:28,584 Epoch 481: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.60 
2024-02-05 23:53:28,584 EPOCH 482
2024-02-05 23:53:32,892 Epoch 482: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.32 
2024-02-05 23:53:32,893 EPOCH 483
2024-02-05 23:53:34,603 [Epoch: 483 Step: 00016400] Batch Recognition Loss:   0.003100 => Gls Tokens per Sec:     2095 || Batch Translation Loss:   0.021949 => Txt Tokens per Sec:     5545 || Lr: 0.000100
2024-02-05 23:53:37,803 Epoch 483: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.99 
2024-02-05 23:53:37,804 EPOCH 484
2024-02-05 23:53:42,157 Epoch 484: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.54 
2024-02-05 23:53:42,157 EPOCH 485
2024-02-05 23:53:46,932 Epoch 485: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.85 
2024-02-05 23:53:46,932 EPOCH 486
2024-02-05 23:53:48,361 [Epoch: 486 Step: 00016500] Batch Recognition Loss:   0.001860 => Gls Tokens per Sec:     2240 || Batch Translation Loss:   0.114046 => Txt Tokens per Sec:     6369 || Lr: 0.000100
2024-02-05 23:53:51,514 Epoch 486: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.23 
2024-02-05 23:53:51,514 EPOCH 487
2024-02-05 23:53:56,188 Epoch 487: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.40 
2024-02-05 23:53:56,188 EPOCH 488
2024-02-05 23:54:00,772 Epoch 488: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.70 
2024-02-05 23:54:00,772 EPOCH 489
2024-02-05 23:54:01,617 [Epoch: 489 Step: 00016600] Batch Recognition Loss:   0.001851 => Gls Tokens per Sec:     3033 || Batch Translation Loss:   0.313223 => Txt Tokens per Sec:     7995 || Lr: 0.000100
2024-02-05 23:54:05,332 Epoch 489: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.89 
2024-02-05 23:54:05,332 EPOCH 490
2024-02-05 23:54:10,011 Epoch 490: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.82 
2024-02-05 23:54:10,012 EPOCH 491
2024-02-05 23:54:14,178 Epoch 491: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.67 
2024-02-05 23:54:14,179 EPOCH 492
2024-02-05 23:54:14,771 [Epoch: 492 Step: 00016700] Batch Recognition Loss:   0.001775 => Gls Tokens per Sec:     3249 || Batch Translation Loss:   0.031862 => Txt Tokens per Sec:     7553 || Lr: 0.000100
2024-02-05 23:54:19,124 Epoch 492: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.03 
2024-02-05 23:54:19,124 EPOCH 493
2024-02-05 23:54:23,296 Epoch 493: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.87 
2024-02-05 23:54:23,296 EPOCH 494
2024-02-05 23:54:28,353 Epoch 494: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.50 
2024-02-05 23:54:28,353 EPOCH 495
2024-02-05 23:54:28,899 [Epoch: 495 Step: 00016800] Batch Recognition Loss:   0.000737 => Gls Tokens per Sec:     2349 || Batch Translation Loss:   0.017718 => Txt Tokens per Sec:     6356 || Lr: 0.000100
2024-02-05 23:54:33,171 Epoch 495: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.46 
2024-02-05 23:54:33,171 EPOCH 496
2024-02-05 23:54:38,093 Epoch 496: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.43 
2024-02-05 23:54:38,094 EPOCH 497
2024-02-05 23:54:42,864 Epoch 497: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.33 
2024-02-05 23:54:42,864 EPOCH 498
2024-02-05 23:54:43,116 [Epoch: 498 Step: 00016900] Batch Recognition Loss:   0.000875 => Gls Tokens per Sec:     2550 || Batch Translation Loss:   0.031514 => Txt Tokens per Sec:     7402 || Lr: 0.000100
2024-02-05 23:54:47,007 Epoch 498: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.99 
2024-02-05 23:54:47,007 EPOCH 499
2024-02-05 23:54:51,731 Epoch 499: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.10 
2024-02-05 23:54:51,732 EPOCH 500
2024-02-05 23:54:56,238 [Epoch: 500 Step: 00017000] Batch Recognition Loss:   0.000399 => Gls Tokens per Sec:     2358 || Batch Translation Loss:   0.020558 => Txt Tokens per Sec:     6523 || Lr: 0.000100
2024-02-05 23:54:56,238 Epoch 500: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.22 
2024-02-05 23:54:56,239 EPOCH 501
2024-02-05 23:55:00,964 Epoch 501: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.15 
2024-02-05 23:55:00,964 EPOCH 502
2024-02-05 23:55:05,557 Epoch 502: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.56 
2024-02-05 23:55:05,558 EPOCH 503
2024-02-05 23:55:09,844 [Epoch: 503 Step: 00017100] Batch Recognition Loss:   0.000320 => Gls Tokens per Sec:     2329 || Batch Translation Loss:   0.066166 => Txt Tokens per Sec:     6423 || Lr: 0.000100
2024-02-05 23:55:10,087 Epoch 503: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.99 
2024-02-05 23:55:10,087 EPOCH 504
2024-02-05 23:55:14,752 Epoch 504: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.67 
2024-02-05 23:55:14,753 EPOCH 505
2024-02-05 23:55:19,794 Epoch 505: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.66 
2024-02-05 23:55:19,794 EPOCH 506
2024-02-05 23:55:23,806 [Epoch: 506 Step: 00017200] Batch Recognition Loss:   0.000908 => Gls Tokens per Sec:     2330 || Batch Translation Loss:   0.093972 => Txt Tokens per Sec:     6362 || Lr: 0.000100
2024-02-05 23:55:24,414 Epoch 506: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.07 
2024-02-05 23:55:24,414 EPOCH 507
2024-02-05 23:55:28,692 Epoch 507: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.39 
2024-02-05 23:55:28,693 EPOCH 508
2024-02-05 23:55:33,584 Epoch 508: Total Training Recognition Loss 0.10  Total Training Translation Loss 4.79 
2024-02-05 23:55:33,584 EPOCH 509
2024-02-05 23:55:36,995 [Epoch: 509 Step: 00017300] Batch Recognition Loss:   0.001682 => Gls Tokens per Sec:     2552 || Batch Translation Loss:   0.066665 => Txt Tokens per Sec:     6970 || Lr: 0.000100
2024-02-05 23:55:37,764 Epoch 509: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.73 
2024-02-05 23:55:37,764 EPOCH 510
2024-02-05 23:55:42,778 Epoch 510: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.92 
2024-02-05 23:55:42,779 EPOCH 511
2024-02-05 23:55:47,034 Epoch 511: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.77 
2024-02-05 23:55:47,034 EPOCH 512
2024-02-05 23:55:50,953 [Epoch: 512 Step: 00017400] Batch Recognition Loss:   0.000366 => Gls Tokens per Sec:     2124 || Batch Translation Loss:   0.023923 => Txt Tokens per Sec:     5824 || Lr: 0.000100
2024-02-05 23:55:51,964 Epoch 512: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.83 
2024-02-05 23:55:51,964 EPOCH 513
2024-02-05 23:55:56,234 Epoch 513: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.49 
2024-02-05 23:55:56,234 EPOCH 514
2024-02-05 23:56:00,958 Epoch 514: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.89 
2024-02-05 23:56:00,959 EPOCH 515
2024-02-05 23:56:04,117 [Epoch: 515 Step: 00017500] Batch Recognition Loss:   0.000453 => Gls Tokens per Sec:     2351 || Batch Translation Loss:   0.055493 => Txt Tokens per Sec:     6452 || Lr: 0.000100
2024-02-05 23:56:05,587 Epoch 515: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.18 
2024-02-05 23:56:05,587 EPOCH 516
2024-02-05 23:56:10,539 Epoch 516: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.29 
2024-02-05 23:56:10,539 EPOCH 517
2024-02-05 23:56:15,036 Epoch 517: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.38 
2024-02-05 23:56:15,037 EPOCH 518
2024-02-05 23:56:18,135 [Epoch: 518 Step: 00017600] Batch Recognition Loss:   0.000368 => Gls Tokens per Sec:     2273 || Batch Translation Loss:   0.191296 => Txt Tokens per Sec:     6115 || Lr: 0.000100
2024-02-05 23:56:19,929 Epoch 518: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.69 
2024-02-05 23:56:19,929 EPOCH 519
2024-02-05 23:56:24,017 Epoch 519: Total Training Recognition Loss 0.08  Total Training Translation Loss 7.51 
2024-02-05 23:56:24,017 EPOCH 520
2024-02-05 23:56:28,820 Epoch 520: Total Training Recognition Loss 0.11  Total Training Translation Loss 7.38 
2024-02-05 23:56:28,821 EPOCH 521
2024-02-05 23:56:31,351 [Epoch: 521 Step: 00017700] Batch Recognition Loss:   0.002178 => Gls Tokens per Sec:     2428 || Batch Translation Loss:   0.016399 => Txt Tokens per Sec:     6618 || Lr: 0.000100
2024-02-05 23:56:33,280 Epoch 521: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.72 
2024-02-05 23:56:33,281 EPOCH 522
2024-02-05 23:56:37,878 Epoch 522: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.22 
2024-02-05 23:56:37,878 EPOCH 523
2024-02-05 23:56:42,481 Epoch 523: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.28 
2024-02-05 23:56:42,481 EPOCH 524
2024-02-05 23:56:44,921 [Epoch: 524 Step: 00017800] Batch Recognition Loss:   0.002260 => Gls Tokens per Sec:     2255 || Batch Translation Loss:   0.158253 => Txt Tokens per Sec:     6499 || Lr: 0.000100
2024-02-05 23:56:47,019 Epoch 524: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.85 
2024-02-05 23:56:47,019 EPOCH 525
2024-02-05 23:56:51,738 Epoch 525: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.17 
2024-02-05 23:56:51,739 EPOCH 526
2024-02-05 23:56:56,481 Epoch 526: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.75 
2024-02-05 23:56:56,481 EPOCH 527
2024-02-05 23:56:58,733 [Epoch: 527 Step: 00017900] Batch Recognition Loss:   0.000886 => Gls Tokens per Sec:     2160 || Batch Translation Loss:   0.031038 => Txt Tokens per Sec:     6067 || Lr: 0.000100
2024-02-05 23:57:00,962 Epoch 527: Total Training Recognition Loss 0.12  Total Training Translation Loss 1.68 
2024-02-05 23:57:00,962 EPOCH 528
2024-02-05 23:57:05,368 Epoch 528: Total Training Recognition Loss 1.53  Total Training Translation Loss 2.18 
2024-02-05 23:57:05,368 EPOCH 529
2024-02-05 23:57:10,210 Epoch 529: Total Training Recognition Loss 5.08  Total Training Translation Loss 3.06 
2024-02-05 23:57:10,210 EPOCH 530
2024-02-05 23:57:11,835 [Epoch: 530 Step: 00018000] Batch Recognition Loss:   0.032118 => Gls Tokens per Sec:     2599 || Batch Translation Loss:   0.030549 => Txt Tokens per Sec:     7197 || Lr: 0.000100
2024-02-05 23:57:20,473 Hooray! New best validation result [eval_metric]!
2024-02-05 23:57:20,482 Saving new checkpoint.
2024-02-05 23:57:20,750 Validation result at epoch 530, step    18000: duration: 8.9143s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 2.61907	Translation Loss: 94000.37500	PPL: 11953.40039
	Eval Metric: BLEU
	WER 6.57	(DEL: 0.00,	INS: 0.00,	SUB: 6.57)
	BLEU-4 0.92	(BLEU-1: 11.47,	BLEU-2: 3.78,	BLEU-3: 1.66,	BLEU-4: 0.92)
	CHRF 17.31	ROUGE 9.69
2024-02-05 23:57:20,751 Logging Recognition and Translation Outputs
2024-02-05 23:57:20,751 ========================================================================================================================
2024-02-05 23:57:20,752 Logging Sequence: 179_309.00
2024-02-05 23:57:20,752 	Gloss Reference :	A B+C+D+E
2024-02-05 23:57:20,752 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:57:20,752 	Gloss Alignment :	         
2024-02-05 23:57:20,752 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:57:20,753 	Text Reference  :	before the ioa could send the notice wfi    has asked phogat to  explain her  indiscipline
2024-02-05 23:57:20,753 	Text Hypothesis :	****** *** we  could **** *** not    travel to  delhi as     our home    town haryana     
2024-02-05 23:57:20,754 	Text Alignment  :	D      D   S         D    D   S      S      S   S     S      S   S       S    S           
2024-02-05 23:57:20,754 ========================================================================================================================
2024-02-05 23:57:20,754 Logging Sequence: 156_35.00
2024-02-05 23:57:20,754 	Gloss Reference :	A B+C+D+E
2024-02-05 23:57:20,754 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:57:20,754 	Gloss Alignment :	         
2024-02-05 23:57:20,754 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:57:20,756 	Text Reference  :	the first season of  mlc  began on 13th july 2023 and ended    on   30th july 2023 with six teams
2024-02-05 23:57:20,756 	Text Hypothesis :	*** ***** ****** ipl will carry on **** **** **** 15  december 2021 in   loss csk  has  10  teams
2024-02-05 23:57:20,756 	Text Alignment  :	D   D     D      S   S    S        D    D    D    S   S        S    S    S    S    S    S        
2024-02-05 23:57:20,756 ========================================================================================================================
2024-02-05 23:57:20,756 Logging Sequence: 129_45.00
2024-02-05 23:57:20,757 	Gloss Reference :	A B+C+D+E
2024-02-05 23:57:20,757 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:57:20,757 	Gloss Alignment :	         
2024-02-05 23:57:20,757 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:57:20,759 	Text Reference  :	suga then *** announced that     from 5    july onwards japan will be        in  a  state of    emergency
2024-02-05 23:57:20,759 	Text Hypothesis :	**** then the 2020      olympics were held in   2020    but   were postponed due to the   covid pandemic 
2024-02-05 23:57:20,759 	Text Alignment  :	D         I   S         S        S    S    S    S       S     S    S         S   S  S     S     S        
2024-02-05 23:57:20,759 ========================================================================================================================
2024-02-05 23:57:20,759 Logging Sequence: 56_17.00
2024-02-05 23:57:20,759 	Gloss Reference :	A B+C+D+E
2024-02-05 23:57:20,759 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:57:20,760 	Gloss Alignment :	         
2024-02-05 23:57:20,760 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:57:20,760 	Text Reference  :	it  was   held at   mumbai's wankhede stadium
2024-02-05 23:57:20,760 	Text Hypothesis :	the world cup  will last     for      india  
2024-02-05 23:57:20,760 	Text Alignment  :	S   S     S    S    S        S        S      
2024-02-05 23:57:20,760 ========================================================================================================================
2024-02-05 23:57:20,761 Logging Sequence: 152_73.00
2024-02-05 23:57:20,761 	Gloss Reference :	A B+C+D+E
2024-02-05 23:57:20,761 	Gloss Hypothesis:	A B+C+D+E
2024-02-05 23:57:20,761 	Gloss Alignment :	         
2024-02-05 23:57:20,761 	--------------------------------------------------------------------------------------------------------------------
2024-02-05 23:57:20,762 	Text Reference  :	***** *** ** ************ ******* * ******** eventually he     too      got out       by  shaheen afridi
2024-02-05 23:57:20,762 	Text Hypothesis :	after her no weightlifter secured a suburban of         sydney olympics in  ahmedabad are sold    out   
2024-02-05 23:57:20,762 	Text Alignment  :	I     I   I  I            I       I I        S          S      S        S   S         S   S       S     
2024-02-05 23:57:20,762 ========================================================================================================================
2024-02-05 23:57:23,680 Epoch 530: Total Training Recognition Loss 1.41  Total Training Translation Loss 2.81 
2024-02-05 23:57:23,681 EPOCH 531
2024-02-05 23:57:28,672 Epoch 531: Total Training Recognition Loss 0.64  Total Training Translation Loss 2.03 
2024-02-05 23:57:28,673 EPOCH 532
2024-02-05 23:57:32,892 Epoch 532: Total Training Recognition Loss 0.18  Total Training Translation Loss 1.94 
2024-02-05 23:57:32,892 EPOCH 533
2024-02-05 23:57:34,491 [Epoch: 533 Step: 00018100] Batch Recognition Loss:   0.003825 => Gls Tokens per Sec:     2403 || Batch Translation Loss:   0.407078 => Txt Tokens per Sec:     6510 || Lr: 0.000100
2024-02-05 23:57:37,792 Epoch 533: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.39 
2024-02-05 23:57:37,793 EPOCH 534
2024-02-05 23:57:42,179 Epoch 534: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.53 
2024-02-05 23:57:42,179 EPOCH 535
2024-02-05 23:57:46,961 Epoch 535: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.08 
2024-02-05 23:57:46,961 EPOCH 536
2024-02-05 23:57:48,270 [Epoch: 536 Step: 00018200] Batch Recognition Loss:   0.003109 => Gls Tokens per Sec:     2249 || Batch Translation Loss:   0.058081 => Txt Tokens per Sec:     6548 || Lr: 0.000100
2024-02-05 23:57:51,375 Epoch 536: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.51 
2024-02-05 23:57:51,376 EPOCH 537
2024-02-05 23:57:55,994 Epoch 537: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.62 
2024-02-05 23:57:55,995 EPOCH 538
2024-02-05 23:58:00,570 Epoch 538: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.46 
2024-02-05 23:58:00,570 EPOCH 539
2024-02-05 23:58:01,683 [Epoch: 539 Step: 00018300] Batch Recognition Loss:   0.000379 => Gls Tokens per Sec:     2302 || Batch Translation Loss:   0.050742 => Txt Tokens per Sec:     6821 || Lr: 0.000100
2024-02-05 23:58:05,178 Epoch 539: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.63 
2024-02-05 23:58:05,179 EPOCH 540
2024-02-05 23:58:09,944 Epoch 540: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.14 
2024-02-05 23:58:09,944 EPOCH 541
2024-02-05 23:58:14,279 Epoch 541: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.61 
2024-02-05 23:58:14,280 EPOCH 542
2024-02-05 23:58:15,026 [Epoch: 542 Step: 00018400] Batch Recognition Loss:   0.000753 => Gls Tokens per Sec:     2578 || Batch Translation Loss:   0.013447 => Txt Tokens per Sec:     6684 || Lr: 0.000100
2024-02-05 23:58:19,137 Epoch 542: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.51 
2024-02-05 23:58:19,137 EPOCH 543
2024-02-05 23:58:23,363 Epoch 543: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.60 
2024-02-05 23:58:23,364 EPOCH 544
2024-02-05 23:58:28,325 Epoch 544: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.13 
2024-02-05 23:58:28,326 EPOCH 545
2024-02-05 23:58:28,690 [Epoch: 545 Step: 00018500] Batch Recognition Loss:   0.000425 => Gls Tokens per Sec:     3516 || Batch Translation Loss:   0.039308 => Txt Tokens per Sec:     8637 || Lr: 0.000100
2024-02-05 23:58:32,535 Epoch 545: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.15 
2024-02-05 23:58:32,535 EPOCH 546
2024-02-05 23:58:37,488 Epoch 546: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.05 
2024-02-05 23:58:37,489 EPOCH 547
2024-02-05 23:58:41,776 Epoch 547: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.17 
2024-02-05 23:58:41,776 EPOCH 548
2024-02-05 23:58:41,955 [Epoch: 548 Step: 00018600] Batch Recognition Loss:   0.000248 => Gls Tokens per Sec:     3589 || Batch Translation Loss:   0.019213 => Txt Tokens per Sec:     8580 || Lr: 0.000100
2024-02-05 23:58:46,469 Epoch 548: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.28 
2024-02-05 23:58:46,469 EPOCH 549
2024-02-05 23:58:51,044 Epoch 549: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.24 
2024-02-05 23:58:51,044 EPOCH 550
2024-02-05 23:58:55,665 [Epoch: 550 Step: 00018700] Batch Recognition Loss:   0.002671 => Gls Tokens per Sec:     2299 || Batch Translation Loss:   0.023541 => Txt Tokens per Sec:     6359 || Lr: 0.000100
2024-02-05 23:58:55,666 Epoch 550: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.91 
2024-02-05 23:58:55,666 EPOCH 551
2024-02-05 23:59:00,286 Epoch 551: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.87 
2024-02-05 23:59:00,286 EPOCH 552
2024-02-05 23:59:04,839 Epoch 552: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.38 
2024-02-05 23:59:04,839 EPOCH 553
2024-02-05 23:59:09,262 [Epoch: 553 Step: 00018800] Batch Recognition Loss:   0.005481 => Gls Tokens per Sec:     2257 || Batch Translation Loss:   0.036802 => Txt Tokens per Sec:     6212 || Lr: 0.000100
2024-02-05 23:59:09,538 Epoch 553: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.26 
2024-02-05 23:59:09,538 EPOCH 554
2024-02-05 23:59:14,024 Epoch 554: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.79 
2024-02-05 23:59:14,025 EPOCH 555
2024-02-05 23:59:18,789 Epoch 555: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.86 
2024-02-05 23:59:18,789 EPOCH 556
2024-02-05 23:59:22,423 [Epoch: 556 Step: 00018900] Batch Recognition Loss:   0.000463 => Gls Tokens per Sec:     2571 || Batch Translation Loss:   0.058478 => Txt Tokens per Sec:     7114 || Lr: 0.000100
2024-02-05 23:59:23,028 Epoch 556: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.95 
2024-02-05 23:59:23,028 EPOCH 557
2024-02-05 23:59:27,980 Epoch 557: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.85 
2024-02-05 23:59:27,981 EPOCH 558
2024-02-05 23:59:32,154 Epoch 558: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.94 
2024-02-05 23:59:32,155 EPOCH 559
2024-02-05 23:59:36,300 [Epoch: 559 Step: 00019000] Batch Recognition Loss:   0.001908 => Gls Tokens per Sec:     2163 || Batch Translation Loss:   0.091363 => Txt Tokens per Sec:     5975 || Lr: 0.000100
2024-02-05 23:59:37,139 Epoch 559: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.95 
2024-02-05 23:59:37,139 EPOCH 560
2024-02-05 23:59:41,459 Epoch 560: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.25 
2024-02-05 23:59:41,460 EPOCH 561
2024-02-05 23:59:46,310 Epoch 561: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.98 
2024-02-05 23:59:46,311 EPOCH 562
2024-02-05 23:59:49,855 [Epoch: 562 Step: 00019100] Batch Recognition Loss:   0.000763 => Gls Tokens per Sec:     2275 || Batch Translation Loss:   0.180959 => Txt Tokens per Sec:     6447 || Lr: 0.000100
2024-02-05 23:59:50,707 Epoch 562: Total Training Recognition Loss 0.10  Total Training Translation Loss 6.00 
2024-02-05 23:59:50,707 EPOCH 563
2024-02-05 23:59:55,641 Epoch 563: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.52 
2024-02-05 23:59:55,642 EPOCH 564
2024-02-06 00:00:00,044 Epoch 564: Total Training Recognition Loss 0.06  Total Training Translation Loss 7.13 
2024-02-06 00:00:00,044 EPOCH 565
2024-02-06 00:00:03,498 [Epoch: 565 Step: 00019200] Batch Recognition Loss:   0.002893 => Gls Tokens per Sec:     2224 || Batch Translation Loss:   0.054160 => Txt Tokens per Sec:     6239 || Lr: 0.000100
2024-02-06 00:00:04,789 Epoch 565: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.84 
2024-02-06 00:00:04,789 EPOCH 566
2024-02-06 00:00:09,279 Epoch 566: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.31 
2024-02-06 00:00:09,279 EPOCH 567
2024-02-06 00:00:13,908 Epoch 567: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.39 
2024-02-06 00:00:13,909 EPOCH 568
2024-02-06 00:00:17,202 [Epoch: 568 Step: 00019300] Batch Recognition Loss:   0.000733 => Gls Tokens per Sec:     2060 || Batch Translation Loss:   0.080131 => Txt Tokens per Sec:     5848 || Lr: 0.000100
2024-02-06 00:00:18,594 Epoch 568: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.19 
2024-02-06 00:00:18,594 EPOCH 569
2024-02-06 00:00:23,156 Epoch 569: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.74 
2024-02-06 00:00:23,157 EPOCH 570
2024-02-06 00:00:27,871 Epoch 570: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.25 
2024-02-06 00:00:27,871 EPOCH 571
2024-02-06 00:00:30,427 [Epoch: 571 Step: 00019400] Batch Recognition Loss:   0.001065 => Gls Tokens per Sec:     2404 || Batch Translation Loss:   0.229031 => Txt Tokens per Sec:     6706 || Lr: 0.000100
2024-02-06 00:00:32,310 Epoch 571: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.52 
2024-02-06 00:00:32,311 EPOCH 572
2024-02-06 00:00:37,163 Epoch 572: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.39 
2024-02-06 00:00:37,163 EPOCH 573
2024-02-06 00:00:41,308 Epoch 573: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.21 
2024-02-06 00:00:41,308 EPOCH 574
2024-02-06 00:00:44,066 [Epoch: 574 Step: 00019500] Batch Recognition Loss:   0.000330 => Gls Tokens per Sec:     1995 || Batch Translation Loss:   0.030647 => Txt Tokens per Sec:     5573 || Lr: 0.000100
2024-02-06 00:00:46,285 Epoch 574: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.21 
2024-02-06 00:00:46,285 EPOCH 575
2024-02-06 00:00:50,442 Epoch 575: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.18 
2024-02-06 00:00:50,442 EPOCH 576
2024-02-06 00:00:55,394 Epoch 576: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.25 
2024-02-06 00:00:55,395 EPOCH 577
2024-02-06 00:00:57,731 [Epoch: 577 Step: 00019600] Batch Recognition Loss:   0.000467 => Gls Tokens per Sec:     2083 || Batch Translation Loss:   0.037125 => Txt Tokens per Sec:     5989 || Lr: 0.000100
2024-02-06 00:00:59,725 Epoch 577: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.98 
2024-02-06 00:00:59,726 EPOCH 578
2024-02-06 00:01:04,578 Epoch 578: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.68 
2024-02-06 00:01:04,579 EPOCH 579
2024-02-06 00:01:08,999 Epoch 579: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.49 
2024-02-06 00:01:08,999 EPOCH 580
2024-02-06 00:01:10,710 [Epoch: 580 Step: 00019700] Batch Recognition Loss:   0.000847 => Gls Tokens per Sec:     2468 || Batch Translation Loss:   0.046883 => Txt Tokens per Sec:     6929 || Lr: 0.000100
2024-02-06 00:01:13,736 Epoch 580: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.67 
2024-02-06 00:01:13,737 EPOCH 581
2024-02-06 00:01:18,268 Epoch 581: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.17 
2024-02-06 00:01:18,268 EPOCH 582
2024-02-06 00:01:22,911 Epoch 582: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.18 
2024-02-06 00:01:22,912 EPOCH 583
2024-02-06 00:01:24,690 [Epoch: 583 Step: 00019800] Batch Recognition Loss:   0.001640 => Gls Tokens per Sec:     2161 || Batch Translation Loss:   0.049215 => Txt Tokens per Sec:     6006 || Lr: 0.000100
2024-02-06 00:01:27,576 Epoch 583: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.02 
2024-02-06 00:01:27,576 EPOCH 584
2024-02-06 00:01:31,993 Epoch 584: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.74 
2024-02-06 00:01:31,994 EPOCH 585
2024-02-06 00:01:36,733 Epoch 585: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.29 
2024-02-06 00:01:36,733 EPOCH 586
2024-02-06 00:01:38,199 [Epoch: 586 Step: 00019900] Batch Recognition Loss:   0.000329 => Gls Tokens per Sec:     2007 || Batch Translation Loss:   0.098743 => Txt Tokens per Sec:     5960 || Lr: 0.000100
2024-02-06 00:01:41,020 Epoch 586: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.59 
2024-02-06 00:01:41,021 EPOCH 587
2024-02-06 00:01:45,908 Epoch 587: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.70 
2024-02-06 00:01:45,909 EPOCH 588
2024-02-06 00:01:50,080 Epoch 588: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.30 
2024-02-06 00:01:50,080 EPOCH 589
2024-02-06 00:01:50,836 [Epoch: 589 Step: 00020000] Batch Recognition Loss:   0.007380 => Gls Tokens per Sec:     3385 || Batch Translation Loss:   0.038654 => Txt Tokens per Sec:     8455 || Lr: 0.000100
2024-02-06 00:01:59,484 Validation result at epoch 589, step    20000: duration: 8.6480s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.42912	Translation Loss: 92852.85938	PPL: 10658.96484
	Eval Metric: BLEU
	WER 4.03	(DEL: 0.00,	INS: 0.00,	SUB: 4.03)
	BLEU-4 0.46	(BLEU-1: 9.81,	BLEU-2: 2.73,	BLEU-3: 0.91,	BLEU-4: 0.46)
	CHRF 16.62	ROUGE 8.44
2024-02-06 00:01:59,485 Logging Recognition and Translation Outputs
2024-02-06 00:01:59,485 ========================================================================================================================
2024-02-06 00:01:59,485 Logging Sequence: 120_7.00
2024-02-06 00:01:59,485 	Gloss Reference :	A B+C+D+E
2024-02-06 00:01:59,485 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:01:59,485 	Gloss Alignment :	         
2024-02-06 00:01:59,486 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:01:59,486 	Text Reference  :	he had tested positive for covid-19 on    may 19 
2024-02-06 00:01:59,486 	Text Hypothesis :	** on  5th    may      the police   filed an  fir
2024-02-06 00:01:59,486 	Text Alignment  :	D  S   S      S        S   S        S     S   S  
2024-02-06 00:01:59,486 ========================================================================================================================
2024-02-06 00:01:59,487 Logging Sequence: 148_186.00
2024-02-06 00:01:59,487 	Gloss Reference :	A B+C+D+E
2024-02-06 00:01:59,487 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:01:59,487 	Gloss Alignment :	         
2024-02-06 00:01:59,487 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:01:59,489 	Text Reference  :	siraj also took four    wickets in     1  over  thus becoming  the   record-holder for  most wickets in   an     over in  odis 
2024-02-06 00:01:59,489 	Text Hypothesis :	***** **** **** kolkata knight  riders is owned by   bollywood actor shah          rukh khan actress juhi chawla and  sri lanka
2024-02-06 00:01:59,489 	Text Alignment  :	D     D    D    S       S       S      S  S     S    S         S     S             S    S    S       S    S      S    S   S    
2024-02-06 00:01:59,489 ========================================================================================================================
2024-02-06 00:01:59,490 Logging Sequence: 67_73.00
2024-02-06 00:01:59,490 	Gloss Reference :	A B+C+D+E
2024-02-06 00:01:59,490 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:01:59,490 	Gloss Alignment :	         
2024-02-06 00:01:59,490 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:01:59,491 	Text Reference  :	** in   his tweet he also said 
2024-02-06 00:01:59,491 	Text Hypothesis :	pm modi and india is very sorry
2024-02-06 00:01:59,491 	Text Alignment  :	I  S    S   S     S  S    S    
2024-02-06 00:01:59,491 ========================================================================================================================
2024-02-06 00:01:59,491 Logging Sequence: 164_526.00
2024-02-06 00:01:59,491 	Gloss Reference :	A B+C+D+E
2024-02-06 00:01:59,491 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:01:59,492 	Gloss Alignment :	         
2024-02-06 00:01:59,492 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:01:59,493 	Text Reference  :	*** *** ***** ********* **** you   are aware that  viacom18 bought  the   broadcast rights of  ipl  
2024-02-06 00:01:59,493 	Text Hypothesis :	the two prime ministers were taken and each  other indian   skipper rohit sharma    lost   the match
2024-02-06 00:01:59,493 	Text Alignment  :	I   I   I     I         I    S     S   S     S     S        S       S     S         S      S   S    
2024-02-06 00:01:59,493 ========================================================================================================================
2024-02-06 00:01:59,493 Logging Sequence: 108_28.00
2024-02-06 00:01:59,494 	Gloss Reference :	A B+C+D+E
2024-02-06 00:01:59,494 	Gloss Hypothesis:	A B+C+D  
2024-02-06 00:01:59,494 	Gloss Alignment :	  S      
2024-02-06 00:01:59,494 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:01:59,495 	Text Reference  :	the 10 teams bought 204    players including 67      foreign players after spending a   total    of  rs   55170 crore
2024-02-06 00:01:59,496 	Text Hypothesis :	*** ** ***** ****** mumbai indians were      batting at      the     ipl   seasons  and pakistan was left pad   first
2024-02-06 00:01:59,496 	Text Alignment  :	D   D  D     D      S      S       S         S       S       S       S     S        S   S        S   S    S     S    
2024-02-06 00:01:59,496 ========================================================================================================================
2024-02-06 00:02:03,628 Epoch 589: Total Training Recognition Loss 0.06  Total Training Translation Loss 5.08 
2024-02-06 00:02:03,628 EPOCH 590
2024-02-06 00:02:08,350 Epoch 590: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.86 
2024-02-06 00:02:08,351 EPOCH 591
2024-02-06 00:02:12,741 Epoch 591: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.14 
2024-02-06 00:02:12,742 EPOCH 592
2024-02-06 00:02:13,778 [Epoch: 592 Step: 00020100] Batch Recognition Loss:   0.000487 => Gls Tokens per Sec:     1855 || Batch Translation Loss:   0.221555 => Txt Tokens per Sec:     5664 || Lr: 0.000100
2024-02-06 00:02:17,634 Epoch 592: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.79 
2024-02-06 00:02:17,634 EPOCH 593
2024-02-06 00:02:22,050 Epoch 593: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.98 
2024-02-06 00:02:22,050 EPOCH 594
2024-02-06 00:02:26,912 Epoch 594: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.83 
2024-02-06 00:02:26,912 EPOCH 595
2024-02-06 00:02:27,310 [Epoch: 595 Step: 00020200] Batch Recognition Loss:   0.000386 => Gls Tokens per Sec:     3224 || Batch Translation Loss:   0.033927 => Txt Tokens per Sec:     8189 || Lr: 0.000100
2024-02-06 00:02:31,073 Epoch 595: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.48 
2024-02-06 00:02:31,074 EPOCH 596
2024-02-06 00:02:36,067 Epoch 596: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.07 
2024-02-06 00:02:36,067 EPOCH 597
2024-02-06 00:02:40,280 Epoch 597: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.17 
2024-02-06 00:02:40,280 EPOCH 598
2024-02-06 00:02:40,530 [Epoch: 598 Step: 00020300] Batch Recognition Loss:   0.000364 => Gls Tokens per Sec:     2570 || Batch Translation Loss:   0.028251 => Txt Tokens per Sec:     7851 || Lr: 0.000100
2024-02-06 00:02:45,124 Epoch 598: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.94 
2024-02-06 00:02:45,124 EPOCH 599
2024-02-06 00:02:49,524 Epoch 599: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.89 
2024-02-06 00:02:49,525 EPOCH 600
2024-02-06 00:02:54,265 [Epoch: 600 Step: 00020400] Batch Recognition Loss:   0.000377 => Gls Tokens per Sec:     2242 || Batch Translation Loss:   0.003760 => Txt Tokens per Sec:     6202 || Lr: 0.000100
2024-02-06 00:02:54,265 Epoch 600: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.19 
2024-02-06 00:02:54,266 EPOCH 601
2024-02-06 00:02:58,811 Epoch 601: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.32 
2024-02-06 00:02:58,812 EPOCH 602
2024-02-06 00:03:03,401 Epoch 602: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.96 
2024-02-06 00:03:03,401 EPOCH 603
2024-02-06 00:03:07,829 [Epoch: 603 Step: 00020500] Batch Recognition Loss:   0.000372 => Gls Tokens per Sec:     2254 || Batch Translation Loss:   0.027771 => Txt Tokens per Sec:     6223 || Lr: 0.000100
2024-02-06 00:03:08,048 Epoch 603: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.95 
2024-02-06 00:03:08,048 EPOCH 604
2024-02-06 00:03:12,379 Epoch 604: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.68 
2024-02-06 00:03:12,380 EPOCH 605
2024-02-06 00:03:17,254 Epoch 605: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.59 
2024-02-06 00:03:17,254 EPOCH 606
2024-02-06 00:03:20,947 [Epoch: 606 Step: 00020600] Batch Recognition Loss:   0.000239 => Gls Tokens per Sec:     2530 || Batch Translation Loss:   0.027261 => Txt Tokens per Sec:     6955 || Lr: 0.000100
2024-02-06 00:03:21,564 Epoch 606: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.25 
2024-02-06 00:03:21,564 EPOCH 607
2024-02-06 00:03:26,551 Epoch 607: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.50 
2024-02-06 00:03:26,552 EPOCH 608
2024-02-06 00:03:30,815 Epoch 608: Total Training Recognition Loss 0.09  Total Training Translation Loss 4.09 
2024-02-06 00:03:30,816 EPOCH 609
2024-02-06 00:03:35,276 [Epoch: 609 Step: 00020700] Batch Recognition Loss:   0.000695 => Gls Tokens per Sec:     1951 || Batch Translation Loss:   0.119186 => Txt Tokens per Sec:     5656 || Lr: 0.000100
2024-02-06 00:03:35,804 Epoch 609: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.10 
2024-02-06 00:03:35,805 EPOCH 610
2024-02-06 00:03:40,021 Epoch 610: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.82 
2024-02-06 00:03:40,022 EPOCH 611
2024-02-06 00:03:44,978 Epoch 611: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.89 
2024-02-06 00:03:44,978 EPOCH 612
2024-02-06 00:03:48,270 [Epoch: 612 Step: 00020800] Batch Recognition Loss:   0.000837 => Gls Tokens per Sec:     2528 || Batch Translation Loss:   0.045228 => Txt Tokens per Sec:     6975 || Lr: 0.000100
2024-02-06 00:03:49,315 Epoch 612: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.59 
2024-02-06 00:03:49,315 EPOCH 613
2024-02-06 00:03:54,097 Epoch 613: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.45 
2024-02-06 00:03:54,098 EPOCH 614
2024-02-06 00:03:58,535 Epoch 614: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.69 
2024-02-06 00:03:58,535 EPOCH 615
2024-02-06 00:04:02,049 [Epoch: 615 Step: 00020900] Batch Recognition Loss:   0.000314 => Gls Tokens per Sec:     2113 || Batch Translation Loss:   0.042067 => Txt Tokens per Sec:     5940 || Lr: 0.000100
2024-02-06 00:04:03,233 Epoch 615: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.84 
2024-02-06 00:04:03,233 EPOCH 616
2024-02-06 00:04:07,787 Epoch 616: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.14 
2024-02-06 00:04:07,788 EPOCH 617
2024-02-06 00:04:12,440 Epoch 617: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.81 
2024-02-06 00:04:12,440 EPOCH 618
2024-02-06 00:04:15,375 [Epoch: 618 Step: 00021000] Batch Recognition Loss:   0.000327 => Gls Tokens per Sec:     2311 || Batch Translation Loss:   0.031977 => Txt Tokens per Sec:     6310 || Lr: 0.000100
2024-02-06 00:04:17,044 Epoch 618: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.79 
2024-02-06 00:04:17,044 EPOCH 619
2024-02-06 00:04:21,624 Epoch 619: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.19 
2024-02-06 00:04:21,625 EPOCH 620
2024-02-06 00:04:26,380 Epoch 620: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.25 
2024-02-06 00:04:26,381 EPOCH 621
2024-02-06 00:04:28,839 [Epoch: 621 Step: 00021100] Batch Recognition Loss:   0.000479 => Gls Tokens per Sec:     2498 || Batch Translation Loss:   0.024038 => Txt Tokens per Sec:     6847 || Lr: 0.000100
2024-02-06 00:04:30,772 Epoch 621: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.15 
2024-02-06 00:04:30,773 EPOCH 622
2024-02-06 00:04:35,634 Epoch 622: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.93 
2024-02-06 00:04:35,634 EPOCH 623
2024-02-06 00:04:39,863 Epoch 623: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.49 
2024-02-06 00:04:39,864 EPOCH 624
2024-02-06 00:04:42,917 [Epoch: 624 Step: 00021200] Batch Recognition Loss:   0.000320 => Gls Tokens per Sec:     1801 || Batch Translation Loss:   0.069247 => Txt Tokens per Sec:     5441 || Lr: 0.000100
2024-02-06 00:04:44,759 Epoch 624: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.56 
2024-02-06 00:04:44,759 EPOCH 625
2024-02-06 00:04:48,938 Epoch 625: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.32 
2024-02-06 00:04:48,938 EPOCH 626
2024-02-06 00:04:53,866 Epoch 626: Total Training Recognition Loss 0.05  Total Training Translation Loss 6.23 
2024-02-06 00:04:53,867 EPOCH 627
2024-02-06 00:04:56,099 [Epoch: 627 Step: 00021300] Batch Recognition Loss:   0.001382 => Gls Tokens per Sec:     2179 || Batch Translation Loss:   0.273903 => Txt Tokens per Sec:     6225 || Lr: 0.000100
2024-02-06 00:04:58,141 Epoch 627: Total Training Recognition Loss 0.06  Total Training Translation Loss 5.63 
2024-02-06 00:04:58,141 EPOCH 628
2024-02-06 00:05:03,024 Epoch 628: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.57 
2024-02-06 00:05:03,025 EPOCH 629
2024-02-06 00:05:07,422 Epoch 629: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.96 
2024-02-06 00:05:07,423 EPOCH 630
2024-02-06 00:05:09,509 [Epoch: 630 Step: 00021400] Batch Recognition Loss:   0.000374 => Gls Tokens per Sec:     2150 || Batch Translation Loss:   0.127753 => Txt Tokens per Sec:     6288 || Lr: 0.000100
2024-02-06 00:05:12,126 Epoch 630: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.45 
2024-02-06 00:05:12,127 EPOCH 631
2024-02-06 00:05:16,686 Epoch 631: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.25 
2024-02-06 00:05:16,686 EPOCH 632
2024-02-06 00:05:21,282 Epoch 632: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.49 
2024-02-06 00:05:21,283 EPOCH 633
2024-02-06 00:05:22,911 [Epoch: 633 Step: 00021500] Batch Recognition Loss:   0.000327 => Gls Tokens per Sec:     2361 || Batch Translation Loss:   0.026748 => Txt Tokens per Sec:     6622 || Lr: 0.000100
2024-02-06 00:05:25,960 Epoch 633: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.82 
2024-02-06 00:05:25,961 EPOCH 634
2024-02-06 00:05:30,520 Epoch 634: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.41 
2024-02-06 00:05:30,521 EPOCH 635
2024-02-06 00:05:35,255 Epoch 635: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.10 
2024-02-06 00:05:35,255 EPOCH 636
2024-02-06 00:05:36,282 [Epoch: 636 Step: 00021600] Batch Recognition Loss:   0.000973 => Gls Tokens per Sec:     3121 || Batch Translation Loss:   0.269765 => Txt Tokens per Sec:     8006 || Lr: 0.000100
2024-02-06 00:05:39,741 Epoch 636: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.52 
2024-02-06 00:05:39,741 EPOCH 637
2024-02-06 00:05:44,498 Epoch 637: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.70 
2024-02-06 00:05:44,498 EPOCH 638
2024-02-06 00:05:48,758 Epoch 638: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.46 
2024-02-06 00:05:48,759 EPOCH 639
2024-02-06 00:05:49,648 [Epoch: 639 Step: 00021700] Batch Recognition Loss:   0.001048 => Gls Tokens per Sec:     2882 || Batch Translation Loss:   0.033179 => Txt Tokens per Sec:     7254 || Lr: 0.000100
2024-02-06 00:05:53,675 Epoch 639: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.78 
2024-02-06 00:05:53,675 EPOCH 640
2024-02-06 00:05:57,832 Epoch 640: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.06 
2024-02-06 00:05:57,833 EPOCH 641
2024-02-06 00:06:02,839 Epoch 641: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.97 
2024-02-06 00:06:02,840 EPOCH 642
2024-02-06 00:06:03,719 [Epoch: 642 Step: 00021800] Batch Recognition Loss:   0.001050 => Gls Tokens per Sec:     1891 || Batch Translation Loss:   0.068977 => Txt Tokens per Sec:     5795 || Lr: 0.000100
2024-02-06 00:06:07,089 Epoch 642: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.59 
2024-02-06 00:06:07,089 EPOCH 643
2024-02-06 00:06:12,006 Epoch 643: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.06 
2024-02-06 00:06:12,007 EPOCH 644
2024-02-06 00:06:16,361 Epoch 644: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.73 
2024-02-06 00:06:16,361 EPOCH 645
2024-02-06 00:06:16,689 [Epoch: 645 Step: 00021900] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:     3116 || Batch Translation Loss:   0.014075 => Txt Tokens per Sec:     7222 || Lr: 0.000100
2024-02-06 00:06:21,177 Epoch 645: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.66 
2024-02-06 00:06:21,177 EPOCH 646
2024-02-06 00:06:25,583 Epoch 646: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.62 
2024-02-06 00:06:25,583 EPOCH 647
2024-02-06 00:06:30,261 Epoch 647: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.01 
2024-02-06 00:06:30,262 EPOCH 648
2024-02-06 00:06:30,527 [Epoch: 648 Step: 00022000] Batch Recognition Loss:   0.000777 => Gls Tokens per Sec:     2424 || Batch Translation Loss:   0.058863 => Txt Tokens per Sec:     7133 || Lr: 0.000100
2024-02-06 00:06:39,253 Validation result at epoch 648, step    22000: duration: 8.7265s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.57841	Translation Loss: 93927.10156	PPL: 11866.23926
	Eval Metric: BLEU
	WER 3.60	(DEL: 0.00,	INS: 0.00,	SUB: 3.60)
	BLEU-4 0.53	(BLEU-1: 10.96,	BLEU-2: 3.49,	BLEU-3: 1.23,	BLEU-4: 0.53)
	CHRF 17.16	ROUGE 9.35
2024-02-06 00:06:39,254 Logging Recognition and Translation Outputs
2024-02-06 00:06:39,254 ========================================================================================================================
2024-02-06 00:06:39,255 Logging Sequence: 179_2.00
2024-02-06 00:06:39,255 	Gloss Reference :	A B+C+D+E
2024-02-06 00:06:39,255 	Gloss Hypothesis:	A B+C+D  
2024-02-06 00:06:39,255 	Gloss Alignment :	  S      
2024-02-06 00:06:39,255 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:06:39,256 	Text Reference  :	* ** vinesh phogat is a         well    known wrestler
2024-02-06 00:06:39,256 	Text Hypothesis :	i am very   happy  to encourage harmony among nations 
2024-02-06 00:06:39,256 	Text Alignment  :	I I  S      S      S  S         S       S     S       
2024-02-06 00:06:39,256 ========================================================================================================================
2024-02-06 00:06:39,257 Logging Sequence: 55_124.00
2024-02-06 00:06:39,257 	Gloss Reference :	A B+C+D+E
2024-02-06 00:06:39,257 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:06:39,257 	Gloss Alignment :	         
2024-02-06 00:06:39,258 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:06:39,258 	Text Reference  :	*** ***** *** next  to him   with the    patel jersey was   ajaz patel  
2024-02-06 00:06:39,259 	Text Hypothesis :	the event was about to start any  minute when  i      asked the  parents
2024-02-06 00:06:39,259 	Text Alignment  :	I   I     I   S        S     S    S      S     S      S     S    S      
2024-02-06 00:06:39,259 ========================================================================================================================
2024-02-06 00:06:39,259 Logging Sequence: 148_105.00
2024-02-06 00:06:39,259 	Gloss Reference :	A B+C+D+E
2024-02-06 00:06:39,259 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:06:39,259 	Gloss Alignment :	         
2024-02-06 00:06:39,260 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:06:39,261 	Text Reference  :	later with amazing bowling by  hardik pandya and kuldeep yadav   sri lanka were all out in    just 50 runs ** ***
2024-02-06 00:06:39,261 	Text Hypothesis :	***** **** ******* ******* the second match  is  in      colombo sri lanka **** for a   total of   50 runs to bat
2024-02-06 00:06:39,262 	Text Alignment  :	D     D    D       D       S   S      S      S   S       S                 D    S   S   S     S            I  I  
2024-02-06 00:06:39,262 ========================================================================================================================
2024-02-06 00:06:39,262 Logging Sequence: 125_165.00
2024-02-06 00:06:39,262 	Gloss Reference :	A B+C+D+E
2024-02-06 00:06:39,262 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:06:39,262 	Gloss Alignment :	         
2024-02-06 00:06:39,263 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:06:39,263 	Text Reference  :	please do not target nadeem we speak to   each other and share a      good    bond  
2024-02-06 00:06:39,264 	Text Hypothesis :	****** ** *** ****** ****** ** so    here are  a     lot of    indian premier league
2024-02-06 00:06:39,264 	Text Alignment  :	D      D  D   D      D      D  S     S    S    S     S   S     S      S       S     
2024-02-06 00:06:39,264 ========================================================================================================================
2024-02-06 00:06:39,264 Logging Sequence: 77_52.00
2024-02-06 00:06:39,264 	Gloss Reference :	A B+C+D+E
2024-02-06 00:06:39,264 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:06:39,265 	Gloss Alignment :	         
2024-02-06 00:06:39,265 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:06:39,266 	Text Reference  :	kane williamson held down the fort for hyderabad by    scoring 66 runs and ended    the       match in            a        tie     
2024-02-06 00:06:39,266 	Text Hypothesis :	**** ********** **** **** *** **** *** ********* there is      a  many of  athletes passports of    uttarakhand's haridwar district
2024-02-06 00:06:39,266 	Text Alignment  :	D    D          D    D    D   D    D   D         S     S       S  S    S   S        S         S     S             S        S       
2024-02-06 00:06:39,266 ========================================================================================================================
2024-02-06 00:06:43,900 Epoch 648: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.52 
2024-02-06 00:06:43,900 EPOCH 649
2024-02-06 00:06:48,325 Epoch 649: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.68 
2024-02-06 00:06:48,325 EPOCH 650
2024-02-06 00:06:53,219 [Epoch: 650 Step: 00022100] Batch Recognition Loss:   0.000484 => Gls Tokens per Sec:     2171 || Batch Translation Loss:   0.044940 => Txt Tokens per Sec:     6006 || Lr: 0.000100
2024-02-06 00:06:53,220 Epoch 650: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.33 
2024-02-06 00:06:53,220 EPOCH 651
2024-02-06 00:06:57,429 Epoch 651: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.82 
2024-02-06 00:06:57,430 EPOCH 652
2024-02-06 00:07:02,393 Epoch 652: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.39 
2024-02-06 00:07:02,393 EPOCH 653
2024-02-06 00:07:06,315 [Epoch: 653 Step: 00022200] Batch Recognition Loss:   0.000560 => Gls Tokens per Sec:     2546 || Batch Translation Loss:   0.049528 => Txt Tokens per Sec:     6996 || Lr: 0.000100
2024-02-06 00:07:06,583 Epoch 653: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.70 
2024-02-06 00:07:06,583 EPOCH 654
2024-02-06 00:07:11,574 Epoch 654: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.01 
2024-02-06 00:07:11,574 EPOCH 655
2024-02-06 00:07:15,828 Epoch 655: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.09 
2024-02-06 00:07:15,828 EPOCH 656
2024-02-06 00:07:20,217 [Epoch: 656 Step: 00022300] Batch Recognition Loss:   0.000284 => Gls Tokens per Sec:     2129 || Batch Translation Loss:   0.050372 => Txt Tokens per Sec:     5882 || Lr: 0.000100
2024-02-06 00:07:20,708 Epoch 656: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.29 
2024-02-06 00:07:20,708 EPOCH 657
2024-02-06 00:07:25,056 Epoch 657: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.04 
2024-02-06 00:07:25,057 EPOCH 658
2024-02-06 00:07:29,820 Epoch 658: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.47 
2024-02-06 00:07:29,821 EPOCH 659
2024-02-06 00:07:33,730 [Epoch: 659 Step: 00022400] Batch Recognition Loss:   0.000412 => Gls Tokens per Sec:     2227 || Batch Translation Loss:   0.048682 => Txt Tokens per Sec:     6195 || Lr: 0.000100
2024-02-06 00:07:34,391 Epoch 659: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.98 
2024-02-06 00:07:34,391 EPOCH 660
2024-02-06 00:07:39,114 Epoch 660: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.36 
2024-02-06 00:07:39,115 EPOCH 661
2024-02-06 00:07:43,711 Epoch 661: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.02 
2024-02-06 00:07:43,711 EPOCH 662
2024-02-06 00:07:47,257 [Epoch: 662 Step: 00022500] Batch Recognition Loss:   0.000363 => Gls Tokens per Sec:     2347 || Batch Translation Loss:   0.051844 => Txt Tokens per Sec:     6624 || Lr: 0.000100
2024-02-06 00:07:48,200 Epoch 662: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.55 
2024-02-06 00:07:48,200 EPOCH 663
2024-02-06 00:07:52,900 Epoch 663: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.39 
2024-02-06 00:07:52,900 EPOCH 664
2024-02-06 00:07:57,187 Epoch 664: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.52 
2024-02-06 00:07:57,188 EPOCH 665
2024-02-06 00:08:00,834 [Epoch: 665 Step: 00022600] Batch Recognition Loss:   0.005426 => Gls Tokens per Sec:     2036 || Batch Translation Loss:   0.348423 => Txt Tokens per Sec:     5747 || Lr: 0.000100
2024-02-06 00:08:02,082 Epoch 665: Total Training Recognition Loss 0.20  Total Training Translation Loss 7.07 
2024-02-06 00:08:02,082 EPOCH 666
2024-02-06 00:08:06,194 Epoch 666: Total Training Recognition Loss 7.01  Total Training Translation Loss 7.75 
2024-02-06 00:08:06,195 EPOCH 667
2024-02-06 00:08:11,179 Epoch 667: Total Training Recognition Loss 5.13  Total Training Translation Loss 8.81 
2024-02-06 00:08:11,180 EPOCH 668
2024-02-06 00:08:13,786 [Epoch: 668 Step: 00022700] Batch Recognition Loss:   0.009791 => Gls Tokens per Sec:     2604 || Batch Translation Loss:   0.104619 => Txt Tokens per Sec:     7035 || Lr: 0.000100
2024-02-06 00:08:15,453 Epoch 668: Total Training Recognition Loss 1.92  Total Training Translation Loss 7.96 
2024-02-06 00:08:15,453 EPOCH 669
2024-02-06 00:08:20,399 Epoch 669: Total Training Recognition Loss 0.77  Total Training Translation Loss 6.89 
2024-02-06 00:08:20,400 EPOCH 670
2024-02-06 00:08:24,774 Epoch 670: Total Training Recognition Loss 0.28  Total Training Translation Loss 4.80 
2024-02-06 00:08:24,774 EPOCH 671
2024-02-06 00:08:27,663 [Epoch: 671 Step: 00022800] Batch Recognition Loss:   0.001127 => Gls Tokens per Sec:     2216 || Batch Translation Loss:   0.042560 => Txt Tokens per Sec:     6017 || Lr: 0.000100
2024-02-06 00:08:29,580 Epoch 671: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.29 
2024-02-06 00:08:29,580 EPOCH 672
2024-02-06 00:08:33,950 Epoch 672: Total Training Recognition Loss 0.12  Total Training Translation Loss 2.31 
2024-02-06 00:08:33,950 EPOCH 673
2024-02-06 00:08:38,698 Epoch 673: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.76 
2024-02-06 00:08:38,698 EPOCH 674
2024-02-06 00:08:41,454 [Epoch: 674 Step: 00022900] Batch Recognition Loss:   0.003365 => Gls Tokens per Sec:     2090 || Batch Translation Loss:   0.050538 => Txt Tokens per Sec:     6087 || Lr: 0.000100
2024-02-06 00:08:43,265 Epoch 674: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.42 
2024-02-06 00:08:43,265 EPOCH 675
2024-02-06 00:08:47,855 Epoch 675: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.42 
2024-02-06 00:08:47,856 EPOCH 676
2024-02-06 00:08:52,534 Epoch 676: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.20 
2024-02-06 00:08:52,534 EPOCH 677
2024-02-06 00:08:54,579 [Epoch: 677 Step: 00023000] Batch Recognition Loss:   0.000725 => Gls Tokens per Sec:     2378 || Batch Translation Loss:   0.184814 => Txt Tokens per Sec:     6844 || Lr: 0.000100
2024-02-06 00:08:57,062 Epoch 677: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.29 
2024-02-06 00:08:57,063 EPOCH 678
2024-02-06 00:09:01,834 Epoch 678: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.02 
2024-02-06 00:09:01,834 EPOCH 679
2024-02-06 00:09:06,119 Epoch 679: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.64 
2024-02-06 00:09:06,120 EPOCH 680
2024-02-06 00:09:08,147 [Epoch: 680 Step: 00023100] Batch Recognition Loss:   0.001253 => Gls Tokens per Sec:     2211 || Batch Translation Loss:   0.012129 => Txt Tokens per Sec:     6121 || Lr: 0.000100
2024-02-06 00:09:11,002 Epoch 680: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.48 
2024-02-06 00:09:11,002 EPOCH 681
2024-02-06 00:09:15,139 Epoch 681: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.74 
2024-02-06 00:09:15,139 EPOCH 682
2024-02-06 00:09:20,146 Epoch 682: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.52 
2024-02-06 00:09:20,146 EPOCH 683
2024-02-06 00:09:21,587 [Epoch: 683 Step: 00023200] Batch Recognition Loss:   0.000974 => Gls Tokens per Sec:     2486 || Batch Translation Loss:   0.226603 => Txt Tokens per Sec:     6753 || Lr: 0.000100
2024-02-06 00:09:24,385 Epoch 683: Total Training Recognition Loss 0.07  Total Training Translation Loss 5.66 
2024-02-06 00:09:24,385 EPOCH 684
2024-02-06 00:09:29,232 Epoch 684: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.64 
2024-02-06 00:09:29,233 EPOCH 685
2024-02-06 00:09:33,581 Epoch 685: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.51 
2024-02-06 00:09:33,581 EPOCH 686
2024-02-06 00:09:34,808 [Epoch: 686 Step: 00023300] Batch Recognition Loss:   0.000867 => Gls Tokens per Sec:     2611 || Batch Translation Loss:   0.018925 => Txt Tokens per Sec:     7370 || Lr: 0.000100
2024-02-06 00:09:38,276 Epoch 686: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.92 
2024-02-06 00:09:38,276 EPOCH 687
2024-02-06 00:09:43,086 Epoch 687: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.82 
2024-02-06 00:09:43,087 EPOCH 688
2024-02-06 00:09:48,028 Epoch 688: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.86 
2024-02-06 00:09:48,029 EPOCH 689
2024-02-06 00:09:48,793 [Epoch: 689 Step: 00023400] Batch Recognition Loss:   0.002911 => Gls Tokens per Sec:     3355 || Batch Translation Loss:   0.008357 => Txt Tokens per Sec:     8505 || Lr: 0.000100
2024-02-06 00:09:52,297 Epoch 689: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.83 
2024-02-06 00:09:52,297 EPOCH 690
2024-02-06 00:09:57,004 Epoch 690: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.81 
2024-02-06 00:09:57,005 EPOCH 691
2024-02-06 00:10:01,632 Epoch 691: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.78 
2024-02-06 00:10:01,633 EPOCH 692
2024-02-06 00:10:02,261 [Epoch: 692 Step: 00023500] Batch Recognition Loss:   0.000312 => Gls Tokens per Sec:     3067 || Batch Translation Loss:   0.024509 => Txt Tokens per Sec:     8375 || Lr: 0.000100
2024-02-06 00:10:06,227 Epoch 692: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.63 
2024-02-06 00:10:06,228 EPOCH 693
2024-02-06 00:10:10,928 Epoch 693: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.67 
2024-02-06 00:10:10,929 EPOCH 694
2024-02-06 00:10:15,326 Epoch 694: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.68 
2024-02-06 00:10:15,327 EPOCH 695
2024-02-06 00:10:15,716 [Epoch: 695 Step: 00023600] Batch Recognition Loss:   0.000587 => Gls Tokens per Sec:     3299 || Batch Translation Loss:   0.017065 => Txt Tokens per Sec:     7881 || Lr: 0.000100
2024-02-06 00:10:20,107 Epoch 695: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.85 
2024-02-06 00:10:20,107 EPOCH 696
2024-02-06 00:10:24,393 Epoch 696: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.75 
2024-02-06 00:10:24,393 EPOCH 697
2024-02-06 00:10:29,366 Epoch 697: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.56 
2024-02-06 00:10:29,367 EPOCH 698
2024-02-06 00:10:29,858 [Epoch: 698 Step: 00023700] Batch Recognition Loss:   0.000843 => Gls Tokens per Sec:     1303 || Batch Translation Loss:   0.021512 => Txt Tokens per Sec:     4206 || Lr: 0.000100
2024-02-06 00:10:33,533 Epoch 698: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.72 
2024-02-06 00:10:33,533 EPOCH 699
2024-02-06 00:10:38,530 Epoch 699: Total Training Recognition Loss 0.06  Total Training Translation Loss 5.34 
2024-02-06 00:10:38,530 EPOCH 700
2024-02-06 00:10:42,748 [Epoch: 700 Step: 00023800] Batch Recognition Loss:   0.000669 => Gls Tokens per Sec:     2519 || Batch Translation Loss:   0.107435 => Txt Tokens per Sec:     6970 || Lr: 0.000100
2024-02-06 00:10:42,748 Epoch 700: Total Training Recognition Loss 0.07  Total Training Translation Loss 6.94 
2024-02-06 00:10:42,748 EPOCH 701
2024-02-06 00:10:47,639 Epoch 701: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.70 
2024-02-06 00:10:47,639 EPOCH 702
2024-02-06 00:10:52,057 Epoch 702: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.41 
2024-02-06 00:10:52,058 EPOCH 703
2024-02-06 00:10:56,518 [Epoch: 703 Step: 00023900] Batch Recognition Loss:   0.000818 => Gls Tokens per Sec:     2238 || Batch Translation Loss:   0.064679 => Txt Tokens per Sec:     6168 || Lr: 0.000100
2024-02-06 00:10:56,818 Epoch 703: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.25 
2024-02-06 00:10:56,818 EPOCH 704
2024-02-06 00:11:01,297 Epoch 704: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.70 
2024-02-06 00:11:01,297 EPOCH 705
2024-02-06 00:11:05,939 Epoch 705: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.85 
2024-02-06 00:11:05,940 EPOCH 706
2024-02-06 00:11:09,984 [Epoch: 706 Step: 00024000] Batch Recognition Loss:   0.004193 => Gls Tokens per Sec:     2311 || Batch Translation Loss:   0.027780 => Txt Tokens per Sec:     6346 || Lr: 0.000100
2024-02-06 00:11:18,883 Validation result at epoch 706, step    24000: duration: 8.8997s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.30562	Translation Loss: 93767.31250	PPL: 11678.36035
	Eval Metric: BLEU
	WER 3.60	(DEL: 0.00,	INS: 0.00,	SUB: 3.60)
	BLEU-4 0.67	(BLEU-1: 11.25,	BLEU-2: 3.76,	BLEU-3: 1.43,	BLEU-4: 0.67)
	CHRF 17.33	ROUGE 9.50
2024-02-06 00:11:18,884 Logging Recognition and Translation Outputs
2024-02-06 00:11:18,884 ========================================================================================================================
2024-02-06 00:11:18,885 Logging Sequence: 171_2.00
2024-02-06 00:11:18,885 	Gloss Reference :	A B+C+D+E      
2024-02-06 00:11:18,885 	Gloss Hypothesis:	A B+C+D+E+D+E+D
2024-02-06 00:11:18,885 	Gloss Alignment :	  S            
2024-02-06 00:11:18,885 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:11:18,887 	Text Reference  :	as you might all   know that the     ipl   is  about    to end  the finals are on    28th may   
2024-02-06 00:11:18,887 	Text Hypothesis :	** *** in    t20is and  ipl  bowling sides are expected to bowl the ****** 20  overs in   mumbai
2024-02-06 00:11:18,887 	Text Alignment  :	D  D   S     S     S    S    S       S     S   S           S        D      S   S     S    S     
2024-02-06 00:11:18,887 ========================================================================================================================
2024-02-06 00:11:18,887 Logging Sequence: 119_33.00
2024-02-06 00:11:18,888 	Gloss Reference :	A B+C+D+E
2024-02-06 00:11:18,888 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:11:18,888 	Gloss Alignment :	         
2024-02-06 00:11:18,888 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:11:18,889 	Text Reference  :	he wanted   to *** *** gift 35    people wow wonderful
2024-02-06 00:11:18,889 	Text Hypothesis :	** shocking to see the gift staff to     the argentina
2024-02-06 00:11:18,889 	Text Alignment  :	D  S           I   I        S     S      S   S        
2024-02-06 00:11:18,889 ========================================================================================================================
2024-02-06 00:11:18,889 Logging Sequence: 158_131.00
2024-02-06 00:11:18,889 	Gloss Reference :	A B+C+D+E
2024-02-06 00:11:18,890 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:11:18,890 	Gloss Alignment :	         
2024-02-06 00:11:18,890 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:11:18,890 	Text Reference  :	on 10th april 2023 there was a match between rcb and    lsg  in     bengaluru
2024-02-06 00:11:18,891 	Text Hypothesis :	** **** ***** **** ***** *** * ***** former  rcb player anil kumble said     
2024-02-06 00:11:18,891 	Text Alignment  :	D  D    D     D    D     D   D D     S           S      S    S      S        
2024-02-06 00:11:18,891 ========================================================================================================================
2024-02-06 00:11:18,891 Logging Sequence: 164_412.00
2024-02-06 00:11:18,891 	Gloss Reference :	A B+C+D+E
2024-02-06 00:11:18,891 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:11:18,891 	Gloss Alignment :	         
2024-02-06 00:11:18,892 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:11:18,893 	Text Reference  :	if you divide these two figures you  will **** be   shocked to    know that each ball's worth is rs   50     lakhs 
2024-02-06 00:11:18,893 	Text Hypothesis :	** *** ****** ***** *** the     bcci will also earn rs      10-15 lakh from each ****** ***** ** over having tested
2024-02-06 00:11:18,893 	Text Alignment  :	D  D   D      D     D   S       S         I    S    S       S     S    S         D      D     D  S    S      S     
2024-02-06 00:11:18,893 ========================================================================================================================
2024-02-06 00:11:18,894 Logging Sequence: 159_112.00
2024-02-06 00:11:18,894 	Gloss Reference :	A B+C+D+E
2024-02-06 00:11:18,894 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:11:18,894 	Gloss Alignment :	         
2024-02-06 00:11:18,894 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:11:18,896 	Text Reference  :	kohli had revealed that before the tournament he    did not touch his bat    for     a       month yes   1    month       
2024-02-06 00:11:18,896 	Text Hypothesis :	***** *** ******** **** ****** *** whoa       there is  a   break for making cricket council acc   whose name participated
2024-02-06 00:11:18,896 	Text Alignment  :	D     D   D        D    D      D   S          S     S   S   S     S   S      S       S       S     S     S    S           
2024-02-06 00:11:18,896 ========================================================================================================================
2024-02-06 00:11:19,522 Epoch 706: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.49 
2024-02-06 00:11:19,522 EPOCH 707
2024-02-06 00:11:24,494 Epoch 707: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.40 
2024-02-06 00:11:24,494 EPOCH 708
2024-02-06 00:11:29,294 Epoch 708: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.14 
2024-02-06 00:11:29,295 EPOCH 709
2024-02-06 00:11:32,756 [Epoch: 709 Step: 00024100] Batch Recognition Loss:   0.000300 => Gls Tokens per Sec:     2514 || Batch Translation Loss:   0.035125 => Txt Tokens per Sec:     6977 || Lr: 0.000100
2024-02-06 00:11:33,554 Epoch 709: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.97 
2024-02-06 00:11:33,554 EPOCH 710
2024-02-06 00:11:38,486 Epoch 710: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.22 
2024-02-06 00:11:38,487 EPOCH 711
2024-02-06 00:11:42,656 Epoch 711: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.25 
2024-02-06 00:11:42,656 EPOCH 712
2024-02-06 00:11:46,431 [Epoch: 712 Step: 00024200] Batch Recognition Loss:   0.000612 => Gls Tokens per Sec:     2135 || Batch Translation Loss:   0.039600 => Txt Tokens per Sec:     5919 || Lr: 0.000100
2024-02-06 00:11:47,675 Epoch 712: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.28 
2024-02-06 00:11:47,675 EPOCH 713
2024-02-06 00:11:51,972 Epoch 713: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.34 
2024-02-06 00:11:51,972 EPOCH 714
2024-02-06 00:11:56,858 Epoch 714: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.98 
2024-02-06 00:11:56,858 EPOCH 715
2024-02-06 00:11:59,733 [Epoch: 715 Step: 00024300] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:     2581 || Batch Translation Loss:   0.021115 => Txt Tokens per Sec:     7145 || Lr: 0.000100
2024-02-06 00:12:01,216 Epoch 715: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.97 
2024-02-06 00:12:01,217 EPOCH 716
2024-02-06 00:12:06,105 Epoch 716: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.93 
2024-02-06 00:12:06,106 EPOCH 717
2024-02-06 00:12:10,589 Epoch 717: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.79 
2024-02-06 00:12:10,589 EPOCH 718
2024-02-06 00:12:13,419 [Epoch: 718 Step: 00024400] Batch Recognition Loss:   0.000317 => Gls Tokens per Sec:     2489 || Batch Translation Loss:   0.027754 => Txt Tokens per Sec:     6771 || Lr: 0.000100
2024-02-06 00:12:15,282 Epoch 718: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.09 
2024-02-06 00:12:15,282 EPOCH 719
2024-02-06 00:12:19,807 Epoch 719: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.09 
2024-02-06 00:12:19,808 EPOCH 720
2024-02-06 00:12:24,308 Epoch 720: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.36 
2024-02-06 00:12:24,309 EPOCH 721
2024-02-06 00:12:27,422 [Epoch: 721 Step: 00024500] Batch Recognition Loss:   0.003512 => Gls Tokens per Sec:     2057 || Batch Translation Loss:   0.044456 => Txt Tokens per Sec:     5768 || Lr: 0.000100
2024-02-06 00:12:29,058 Epoch 721: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.24 
2024-02-06 00:12:29,059 EPOCH 722
2024-02-06 00:12:33,173 Epoch 722: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.36 
2024-02-06 00:12:33,173 EPOCH 723
2024-02-06 00:12:37,768 Epoch 723: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.10 
2024-02-06 00:12:37,768 EPOCH 724
2024-02-06 00:12:40,338 [Epoch: 724 Step: 00024600] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     2142 || Batch Translation Loss:   0.029246 => Txt Tokens per Sec:     5939 || Lr: 0.000100
2024-02-06 00:12:42,376 Epoch 724: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.83 
2024-02-06 00:12:42,377 EPOCH 725
2024-02-06 00:12:46,490 Epoch 725: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.07 
2024-02-06 00:12:46,491 EPOCH 726
2024-02-06 00:12:51,684 Epoch 726: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.14 
2024-02-06 00:12:51,685 EPOCH 727
2024-02-06 00:12:53,548 [Epoch: 727 Step: 00024700] Batch Recognition Loss:   0.000368 => Gls Tokens per Sec:     2610 || Batch Translation Loss:   0.029190 => Txt Tokens per Sec:     6897 || Lr: 0.000100
2024-02-06 00:12:56,158 Epoch 727: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.06 
2024-02-06 00:12:56,158 EPOCH 728
2024-02-06 00:13:00,584 Epoch 728: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.05 
2024-02-06 00:13:00,585 EPOCH 729
2024-02-06 00:13:05,426 Epoch 729: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.87 
2024-02-06 00:13:05,427 EPOCH 730
2024-02-06 00:13:07,211 [Epoch: 730 Step: 00024800] Batch Recognition Loss:   0.000490 => Gls Tokens per Sec:     2367 || Batch Translation Loss:   0.046217 => Txt Tokens per Sec:     6581 || Lr: 0.000100
2024-02-06 00:13:09,722 Epoch 730: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.57 
2024-02-06 00:13:09,722 EPOCH 731
2024-02-06 00:13:14,590 Epoch 731: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.06 
2024-02-06 00:13:14,591 EPOCH 732
2024-02-06 00:13:18,740 Epoch 732: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.75 
2024-02-06 00:13:18,741 EPOCH 733
2024-02-06 00:13:20,449 [Epoch: 733 Step: 00024900] Batch Recognition Loss:   0.001150 => Gls Tokens per Sec:     2250 || Batch Translation Loss:   0.074064 => Txt Tokens per Sec:     6159 || Lr: 0.000100
2024-02-06 00:13:23,738 Epoch 733: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.32 
2024-02-06 00:13:23,738 EPOCH 734
2024-02-06 00:13:28,003 Epoch 734: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.62 
2024-02-06 00:13:28,003 EPOCH 735
2024-02-06 00:13:32,952 Epoch 735: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.76 
2024-02-06 00:13:32,952 EPOCH 736
2024-02-06 00:13:34,118 [Epoch: 736 Step: 00025000] Batch Recognition Loss:   0.001027 => Gls Tokens per Sec:     2746 || Batch Translation Loss:   0.025762 => Txt Tokens per Sec:     7621 || Lr: 0.000100
2024-02-06 00:13:37,600 Epoch 736: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.38 
2024-02-06 00:13:37,601 EPOCH 737
2024-02-06 00:13:42,528 Epoch 737: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.50 
2024-02-06 00:13:42,529 EPOCH 738
2024-02-06 00:13:46,910 Epoch 738: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.15 
2024-02-06 00:13:46,910 EPOCH 739
2024-02-06 00:13:47,675 [Epoch: 739 Step: 00025100] Batch Recognition Loss:   0.000270 => Gls Tokens per Sec:     3351 || Batch Translation Loss:   0.034916 => Txt Tokens per Sec:     8403 || Lr: 0.000100
2024-02-06 00:13:51,067 Epoch 739: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.82 
2024-02-06 00:13:51,067 EPOCH 740
2024-02-06 00:13:55,494 Epoch 740: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.65 
2024-02-06 00:13:55,494 EPOCH 741
2024-02-06 00:14:00,275 Epoch 741: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.92 
2024-02-06 00:14:00,275 EPOCH 742
2024-02-06 00:14:00,954 [Epoch: 742 Step: 00025200] Batch Recognition Loss:   0.003606 => Gls Tokens per Sec:     2832 || Batch Translation Loss:   0.042451 => Txt Tokens per Sec:     7335 || Lr: 0.000100
2024-02-06 00:14:04,537 Epoch 742: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.40 
2024-02-06 00:14:04,537 EPOCH 743
2024-02-06 00:14:09,459 Epoch 743: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.99 
2024-02-06 00:14:09,460 EPOCH 744
2024-02-06 00:14:13,671 Epoch 744: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.27 
2024-02-06 00:14:13,671 EPOCH 745
2024-02-06 00:14:14,205 [Epoch: 745 Step: 00025300] Batch Recognition Loss:   0.001581 => Gls Tokens per Sec:     2401 || Batch Translation Loss:   0.051513 => Txt Tokens per Sec:     6705 || Lr: 0.000100
2024-02-06 00:14:18,647 Epoch 745: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.60 
2024-02-06 00:14:18,647 EPOCH 746
2024-02-06 00:14:22,927 Epoch 746: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.64 
2024-02-06 00:14:22,927 EPOCH 747
2024-02-06 00:14:27,810 Epoch 747: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.11 
2024-02-06 00:14:27,810 EPOCH 748
2024-02-06 00:14:27,935 [Epoch: 748 Step: 00025400] Batch Recognition Loss:   0.011696 => Gls Tokens per Sec:     5161 || Batch Translation Loss:   0.054885 => Txt Tokens per Sec:    10032 || Lr: 0.000100
2024-02-06 00:14:32,179 Epoch 748: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.65 
2024-02-06 00:14:32,179 EPOCH 749
2024-02-06 00:14:36,813 Epoch 749: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.14 
2024-02-06 00:14:36,813 EPOCH 750
2024-02-06 00:14:41,473 [Epoch: 750 Step: 00025500] Batch Recognition Loss:   0.000566 => Gls Tokens per Sec:     2280 || Batch Translation Loss:   0.035814 => Txt Tokens per Sec:     6308 || Lr: 0.000100
2024-02-06 00:14:41,473 Epoch 750: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.94 
2024-02-06 00:14:41,473 EPOCH 751
2024-02-06 00:14:45,979 Epoch 751: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.45 
2024-02-06 00:14:45,979 EPOCH 752
2024-02-06 00:14:50,727 Epoch 752: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.83 
2024-02-06 00:14:50,728 EPOCH 753
2024-02-06 00:14:54,883 [Epoch: 753 Step: 00025600] Batch Recognition Loss:   0.000666 => Gls Tokens per Sec:     2402 || Batch Translation Loss:   0.025777 => Txt Tokens per Sec:     6635 || Lr: 0.000100
2024-02-06 00:14:55,144 Epoch 753: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.11 
2024-02-06 00:14:55,145 EPOCH 754
2024-02-06 00:15:00,061 Epoch 754: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.78 
2024-02-06 00:15:00,062 EPOCH 755
2024-02-06 00:15:04,199 Epoch 755: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.82 
2024-02-06 00:15:04,200 EPOCH 756
2024-02-06 00:15:07,858 [Epoch: 756 Step: 00025700] Batch Recognition Loss:   0.000415 => Gls Tokens per Sec:     2553 || Batch Translation Loss:   0.006418 => Txt Tokens per Sec:     7042 || Lr: 0.000100
2024-02-06 00:15:08,379 Epoch 756: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.15 
2024-02-06 00:15:08,379 EPOCH 757
2024-02-06 00:15:12,465 Epoch 757: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.36 
2024-02-06 00:15:12,466 EPOCH 758
2024-02-06 00:15:17,504 Epoch 758: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.43 
2024-02-06 00:15:17,505 EPOCH 759
2024-02-06 00:15:21,209 [Epoch: 759 Step: 00025800] Batch Recognition Loss:   0.000413 => Gls Tokens per Sec:     2419 || Batch Translation Loss:   0.033286 => Txt Tokens per Sec:     6664 || Lr: 0.000100
2024-02-06 00:15:21,837 Epoch 759: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.96 
2024-02-06 00:15:21,838 EPOCH 760
2024-02-06 00:15:26,596 Epoch 760: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.58 
2024-02-06 00:15:26,597 EPOCH 761
2024-02-06 00:15:31,081 Epoch 761: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.23 
2024-02-06 00:15:31,081 EPOCH 762
2024-02-06 00:15:34,475 [Epoch: 762 Step: 00025900] Batch Recognition Loss:   0.000384 => Gls Tokens per Sec:     2375 || Batch Translation Loss:   0.058532 => Txt Tokens per Sec:     6674 || Lr: 0.000100
2024-02-06 00:15:35,708 Epoch 762: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.86 
2024-02-06 00:15:35,709 EPOCH 763
2024-02-06 00:15:40,310 Epoch 763: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.35 
2024-02-06 00:15:40,310 EPOCH 764
2024-02-06 00:15:44,837 Epoch 764: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.53 
2024-02-06 00:15:44,838 EPOCH 765
2024-02-06 00:15:47,904 [Epoch: 765 Step: 00026000] Batch Recognition Loss:   0.000635 => Gls Tokens per Sec:     2421 || Batch Translation Loss:   0.061987 => Txt Tokens per Sec:     6485 || Lr: 0.000100
2024-02-06 00:15:56,578 Validation result at epoch 765, step    26000: duration: 8.6726s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.47309	Translation Loss: 93612.33594	PPL: 11498.98145
	Eval Metric: BLEU
	WER 3.67	(DEL: 0.00,	INS: 0.00,	SUB: 3.67)
	BLEU-4 0.80	(BLEU-1: 11.59,	BLEU-2: 3.82,	BLEU-3: 1.60,	BLEU-4: 0.80)
	CHRF 17.32	ROUGE 9.82
2024-02-06 00:15:56,579 Logging Recognition and Translation Outputs
2024-02-06 00:15:56,579 ========================================================================================================================
2024-02-06 00:15:56,580 Logging Sequence: 166_243.00
2024-02-06 00:15:56,580 	Gloss Reference :	A B+C+D+E
2024-02-06 00:15:56,580 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:15:56,580 	Gloss Alignment :	         
2024-02-06 00:15:56,580 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:15:56,581 	Text Reference  :	*** ********* *********** ********* *** ***** ** icc     worked with members boards like bcci pcb   cricket australia etc 
2024-02-06 00:15:56,582 	Text Hypothesis :	the broadcast advertisers ticketing etc would be decided by     the  board   of     the  2    teams playing the       test
2024-02-06 00:15:56,582 	Text Alignment  :	I   I         I           I         I   I     I  S       S      S    S       S      S    S    S     S       S         S   
2024-02-06 00:15:56,582 ========================================================================================================================
2024-02-06 00:15:56,582 Logging Sequence: 59_152.00
2024-02-06 00:15:56,582 	Gloss Reference :	A B+C+D+E
2024-02-06 00:15:56,582 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:15:56,583 	Gloss Alignment :	         
2024-02-06 00:15:56,583 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:15:56,583 	Text Reference  :	the organisers encouraged athletes to use    the         condoms in their home countries
2024-02-06 00:15:56,583 	Text Hypothesis :	the ********** ********** ******** ** second tie-breaker ended   in ***** a    draw     
2024-02-06 00:15:56,584 	Text Alignment  :	    D          D          D        D  S      S           S          D     S    S        
2024-02-06 00:15:56,584 ========================================================================================================================
2024-02-06 00:15:56,584 Logging Sequence: 145_52.00
2024-02-06 00:15:56,584 	Gloss Reference :	A B+C+D+E
2024-02-06 00:15:56,584 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:15:56,584 	Gloss Alignment :	         
2024-02-06 00:15:56,585 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:15:56,585 	Text Reference  :	her name was dropped despite having qualified as    she was the only      female  athlete
2024-02-06 00:15:56,585 	Text Hypothesis :	*** **** *** ******* ******* ****** after     which she *** *** announced women's team   
2024-02-06 00:15:56,586 	Text Alignment  :	D   D    D   D       D       D      S         S         D   D   S         S       S      
2024-02-06 00:15:56,586 ========================================================================================================================
2024-02-06 00:15:56,586 Logging Sequence: 172_163.00
2024-02-06 00:15:56,586 	Gloss Reference :	A B+C+D+E
2024-02-06 00:15:56,586 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:15:56,586 	Gloss Alignment :	         
2024-02-06 00:15:56,586 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:15:56,588 	Text Reference  :	*** ** if      the ***** match starts anywhere between 730  pm    to 935 pm a        full 20-over match can    be  played
2024-02-06 00:15:56,589 	Text Hypothesis :	the oc started the probe and   were   given    2       more weeks to *** ** complete the  probe   and   submit the report
2024-02-06 00:15:56,589 	Text Alignment  :	I   I  S           I     S     S      S        S       S    S        D   D  S        S    S       S     S      S   S     
2024-02-06 00:15:56,589 ========================================================================================================================
2024-02-06 00:15:56,589 Logging Sequence: 150_20.00
2024-02-06 00:15:56,589 	Gloss Reference :	A B+C+D+E
2024-02-06 00:15:56,589 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:15:56,589 	Gloss Alignment :	         
2024-02-06 00:15:56,589 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:15:56,590 	Text Reference  :	*** **** ** after a        tough   match india  won the saff championship 2023 title
2024-02-06 00:15:56,591 	Text Hypothesis :	now said 'i will  continue playing in    mumbai if  we  will play         sri  lanka
2024-02-06 00:15:56,591 	Text Alignment  :	I   I    I  S     S        S       S     S      S   S   S    S            S    S    
2024-02-06 00:15:56,591 ========================================================================================================================
2024-02-06 00:15:58,377 Epoch 765: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.24 
2024-02-06 00:15:58,377 EPOCH 766
2024-02-06 00:16:02,921 Epoch 766: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.22 
2024-02-06 00:16:02,921 EPOCH 767
2024-02-06 00:16:07,901 Epoch 767: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.60 
2024-02-06 00:16:07,902 EPOCH 768
2024-02-06 00:16:10,504 [Epoch: 768 Step: 00026100] Batch Recognition Loss:   0.001088 => Gls Tokens per Sec:     2608 || Batch Translation Loss:   0.020174 => Txt Tokens per Sec:     7143 || Lr: 0.000100
2024-02-06 00:16:12,148 Epoch 768: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.34 
2024-02-06 00:16:12,148 EPOCH 769
2024-02-06 00:16:17,086 Epoch 769: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.11 
2024-02-06 00:16:17,086 EPOCH 770
2024-02-06 00:16:21,456 Epoch 770: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.99 
2024-02-06 00:16:21,456 EPOCH 771
2024-02-06 00:16:24,519 [Epoch: 771 Step: 00026200] Batch Recognition Loss:   0.002854 => Gls Tokens per Sec:     2005 || Batch Translation Loss:   0.030831 => Txt Tokens per Sec:     5705 || Lr: 0.000100
2024-02-06 00:16:26,357 Epoch 771: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.91 
2024-02-06 00:16:26,357 EPOCH 772
2024-02-06 00:16:30,839 Epoch 772: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.98 
2024-02-06 00:16:30,839 EPOCH 773
2024-02-06 00:16:35,536 Epoch 773: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.08 
2024-02-06 00:16:35,537 EPOCH 774
2024-02-06 00:16:38,351 [Epoch: 774 Step: 00026300] Batch Recognition Loss:   0.002268 => Gls Tokens per Sec:     2048 || Batch Translation Loss:   0.050717 => Txt Tokens per Sec:     6017 || Lr: 0.000100
2024-02-06 00:16:40,262 Epoch 774: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.49 
2024-02-06 00:16:40,262 EPOCH 775
2024-02-06 00:16:45,228 Epoch 775: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.34 
2024-02-06 00:16:45,228 EPOCH 776
2024-02-06 00:16:49,672 Epoch 776: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.15 
2024-02-06 00:16:49,672 EPOCH 777
2024-02-06 00:16:51,252 [Epoch: 777 Step: 00026400] Batch Recognition Loss:   0.000240 => Gls Tokens per Sec:     3077 || Batch Translation Loss:   0.019404 => Txt Tokens per Sec:     7923 || Lr: 0.000100
2024-02-06 00:16:54,155 Epoch 777: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.07 
2024-02-06 00:16:54,156 EPOCH 778
2024-02-06 00:16:58,903 Epoch 778: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.53 
2024-02-06 00:16:58,903 EPOCH 779
2024-02-06 00:17:03,036 Epoch 779: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.80 
2024-02-06 00:17:03,037 EPOCH 780
2024-02-06 00:17:04,995 [Epoch: 780 Step: 00026500] Batch Recognition Loss:   0.000220 => Gls Tokens per Sec:     2156 || Batch Translation Loss:   0.004392 => Txt Tokens per Sec:     6192 || Lr: 0.000100
2024-02-06 00:17:07,564 Epoch 780: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.50 
2024-02-06 00:17:07,564 EPOCH 781
2024-02-06 00:17:12,509 Epoch 781: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.62 
2024-02-06 00:17:12,509 EPOCH 782
2024-02-06 00:17:16,707 Epoch 782: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.03 
2024-02-06 00:17:16,707 EPOCH 783
2024-02-06 00:17:18,099 [Epoch: 783 Step: 00026600] Batch Recognition Loss:   0.005606 => Gls Tokens per Sec:     2759 || Batch Translation Loss:   0.233779 => Txt Tokens per Sec:     7022 || Lr: 0.000100
2024-02-06 00:17:21,489 Epoch 783: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.14 
2024-02-06 00:17:21,489 EPOCH 784
2024-02-06 00:17:26,009 Epoch 784: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.76 
2024-02-06 00:17:26,009 EPOCH 785
2024-02-06 00:17:30,806 Epoch 785: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.31 
2024-02-06 00:17:30,807 EPOCH 786
2024-02-06 00:17:32,479 [Epoch: 786 Step: 00026700] Batch Recognition Loss:   0.000323 => Gls Tokens per Sec:     1916 || Batch Translation Loss:   0.117107 => Txt Tokens per Sec:     5514 || Lr: 0.000100
2024-02-06 00:17:35,252 Epoch 786: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.50 
2024-02-06 00:17:35,252 EPOCH 787
2024-02-06 00:17:39,887 Epoch 787: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.53 
2024-02-06 00:17:39,888 EPOCH 788
2024-02-06 00:17:44,507 Epoch 788: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.40 
2024-02-06 00:17:44,507 EPOCH 789
2024-02-06 00:17:45,665 [Epoch: 789 Step: 00026800] Batch Recognition Loss:   0.001338 => Gls Tokens per Sec:     2215 || Batch Translation Loss:   0.019811 => Txt Tokens per Sec:     6381 || Lr: 0.000100
2024-02-06 00:17:49,008 Epoch 789: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.66 
2024-02-06 00:17:49,008 EPOCH 790
2024-02-06 00:17:53,755 Epoch 790: Total Training Recognition Loss 0.11  Total Training Translation Loss 5.62 
2024-02-06 00:17:53,755 EPOCH 791
2024-02-06 00:17:58,448 Epoch 791: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.64 
2024-02-06 00:17:58,449 EPOCH 792
2024-02-06 00:17:59,296 [Epoch: 792 Step: 00026900] Batch Recognition Loss:   0.000352 => Gls Tokens per Sec:     2270 || Batch Translation Loss:   0.054160 => Txt Tokens per Sec:     6606 || Lr: 0.000100
2024-02-06 00:18:03,002 Epoch 792: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.00 
2024-02-06 00:18:03,002 EPOCH 793
2024-02-06 00:18:07,368 Epoch 793: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.14 
2024-02-06 00:18:07,369 EPOCH 794
2024-02-06 00:18:12,268 Epoch 794: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.78 
2024-02-06 00:18:12,269 EPOCH 795
2024-02-06 00:18:12,758 [Epoch: 795 Step: 00027000] Batch Recognition Loss:   0.000412 => Gls Tokens per Sec:     2086 || Batch Translation Loss:   0.040987 => Txt Tokens per Sec:     6393 || Lr: 0.000100
2024-02-06 00:18:16,493 Epoch 795: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.94 
2024-02-06 00:18:16,493 EPOCH 796
2024-02-06 00:18:21,538 Epoch 796: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.80 
2024-02-06 00:18:21,538 EPOCH 797
2024-02-06 00:18:25,783 Epoch 797: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.68 
2024-02-06 00:18:25,783 EPOCH 798
2024-02-06 00:18:25,949 [Epoch: 798 Step: 00027100] Batch Recognition Loss:   0.000399 => Gls Tokens per Sec:     3884 || Batch Translation Loss:   0.016560 => Txt Tokens per Sec:    10390 || Lr: 0.000100
2024-02-06 00:18:30,709 Epoch 798: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.87 
2024-02-06 00:18:30,710 EPOCH 799
2024-02-06 00:18:34,985 Epoch 799: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.32 
2024-02-06 00:18:34,985 EPOCH 800
2024-02-06 00:18:39,787 [Epoch: 800 Step: 00027200] Batch Recognition Loss:   0.000300 => Gls Tokens per Sec:     2212 || Batch Translation Loss:   0.029758 => Txt Tokens per Sec:     6120 || Lr: 0.000100
2024-02-06 00:18:39,788 Epoch 800: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.15 
2024-02-06 00:18:39,788 EPOCH 801
2024-02-06 00:18:44,218 Epoch 801: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.73 
2024-02-06 00:18:44,219 EPOCH 802
2024-02-06 00:18:48,809 Epoch 802: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.68 
2024-02-06 00:18:48,809 EPOCH 803
2024-02-06 00:18:53,231 [Epoch: 803 Step: 00027300] Batch Recognition Loss:   0.001646 => Gls Tokens per Sec:     2258 || Batch Translation Loss:   0.009873 => Txt Tokens per Sec:     6208 || Lr: 0.000100
2024-02-06 00:18:53,498 Epoch 803: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.54 
2024-02-06 00:18:53,499 EPOCH 804
2024-02-06 00:18:57,935 Epoch 804: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-06 00:18:57,936 EPOCH 805
2024-02-06 00:19:02,734 Epoch 805: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.06 
2024-02-06 00:19:02,734 EPOCH 806
2024-02-06 00:19:07,204 [Epoch: 806 Step: 00027400] Batch Recognition Loss:   0.000386 => Gls Tokens per Sec:     2090 || Batch Translation Loss:   0.041930 => Txt Tokens per Sec:     5864 || Lr: 0.000100
2024-02-06 00:19:07,666 Epoch 806: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.23 
2024-02-06 00:19:07,666 EPOCH 807
2024-02-06 00:19:12,371 Epoch 807: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.11 
2024-02-06 00:19:12,372 EPOCH 808
2024-02-06 00:19:16,574 Epoch 808: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.63 
2024-02-06 00:19:16,574 EPOCH 809
2024-02-06 00:19:20,819 [Epoch: 809 Step: 00027500] Batch Recognition Loss:   0.000755 => Gls Tokens per Sec:     2112 || Batch Translation Loss:   0.062923 => Txt Tokens per Sec:     5890 || Lr: 0.000100
2024-02-06 00:19:21,584 Epoch 809: Total Training Recognition Loss 0.06  Total Training Translation Loss 5.30 
2024-02-06 00:19:21,584 EPOCH 810
2024-02-06 00:19:25,792 Epoch 810: Total Training Recognition Loss 0.07  Total Training Translation Loss 7.48 
2024-02-06 00:19:25,793 EPOCH 811
2024-02-06 00:19:30,725 Epoch 811: Total Training Recognition Loss 0.15  Total Training Translation Loss 7.45 
2024-02-06 00:19:30,726 EPOCH 812
2024-02-06 00:19:34,290 [Epoch: 812 Step: 00027600] Batch Recognition Loss:   0.000729 => Gls Tokens per Sec:     2262 || Batch Translation Loss:   0.128679 => Txt Tokens per Sec:     6431 || Lr: 0.000100
2024-02-06 00:19:35,109 Epoch 812: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.96 
2024-02-06 00:19:35,109 EPOCH 813
2024-02-06 00:19:40,007 Epoch 813: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.30 
2024-02-06 00:19:40,008 EPOCH 814
2024-02-06 00:19:44,694 Epoch 814: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.89 
2024-02-06 00:19:44,695 EPOCH 815
2024-02-06 00:19:47,888 [Epoch: 815 Step: 00027700] Batch Recognition Loss:   0.000349 => Gls Tokens per Sec:     2325 || Batch Translation Loss:   0.033887 => Txt Tokens per Sec:     6217 || Lr: 0.000100
2024-02-06 00:19:49,644 Epoch 815: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.74 
2024-02-06 00:19:49,644 EPOCH 816
2024-02-06 00:19:53,966 Epoch 816: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.64 
2024-02-06 00:19:53,966 EPOCH 817
2024-02-06 00:19:58,652 Epoch 817: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.47 
2024-02-06 00:19:58,653 EPOCH 818
2024-02-06 00:20:01,702 [Epoch: 818 Step: 00027800] Batch Recognition Loss:   0.000328 => Gls Tokens per Sec:     2225 || Batch Translation Loss:   0.014072 => Txt Tokens per Sec:     6229 || Lr: 0.000100
2024-02-06 00:20:03,267 Epoch 818: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.33 
2024-02-06 00:20:03,267 EPOCH 819
2024-02-06 00:20:07,798 Epoch 819: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.44 
2024-02-06 00:20:07,798 EPOCH 820
2024-02-06 00:20:12,434 Epoch 820: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.66 
2024-02-06 00:20:12,435 EPOCH 821
2024-02-06 00:20:14,556 [Epoch: 821 Step: 00027900] Batch Recognition Loss:   0.000316 => Gls Tokens per Sec:     2896 || Batch Translation Loss:   0.034075 => Txt Tokens per Sec:     7638 || Lr: 0.000100
2024-02-06 00:20:16,868 Epoch 821: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.54 
2024-02-06 00:20:16,869 EPOCH 822
2024-02-06 00:20:21,709 Epoch 822: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.85 
2024-02-06 00:20:21,709 EPOCH 823
2024-02-06 00:20:26,617 Epoch 823: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.82 
2024-02-06 00:20:26,618 EPOCH 824
2024-02-06 00:20:28,763 [Epoch: 824 Step: 00028000] Batch Recognition Loss:   0.000298 => Gls Tokens per Sec:     2686 || Batch Translation Loss:   0.108413 => Txt Tokens per Sec:     7085 || Lr: 0.000100
2024-02-06 00:20:37,335 Validation result at epoch 824, step    28000: duration: 8.5717s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.66081	Translation Loss: 94164.29688	PPL: 12150.70801
	Eval Metric: BLEU
	WER 3.88	(DEL: 0.00,	INS: 0.00,	SUB: 3.88)
	BLEU-4 0.70	(BLEU-1: 11.56,	BLEU-2: 3.78,	BLEU-3: 1.51,	BLEU-4: 0.70)
	CHRF 17.56	ROUGE 9.91
2024-02-06 00:20:37,336 Logging Recognition and Translation Outputs
2024-02-06 00:20:37,337 ========================================================================================================================
2024-02-06 00:20:37,337 Logging Sequence: 156_288.00
2024-02-06 00:20:37,337 	Gloss Reference :	A B+C+D+E      
2024-02-06 00:20:37,337 	Gloss Hypothesis:	A B+C+D+E+D+E+D
2024-02-06 00:20:37,338 	Gloss Alignment :	  S            
2024-02-06 00:20:37,338 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:20:37,339 	Text Reference  :	****** pooran led  the         team to victory miny became winners of  the 1st season
2024-02-06 00:20:37,339 	Text Hypothesis :	people were   seen celebrating this to ******* **** ****** ******* see him as  well  
2024-02-06 00:20:37,339 	Text Alignment  :	I      S      S    S           S       D       D    D      D       S   S   S   S     
2024-02-06 00:20:37,339 ========================================================================================================================
2024-02-06 00:20:37,339 Logging Sequence: 98_135.00
2024-02-06 00:20:37,339 	Gloss Reference :	A B+C+D+E
2024-02-06 00:20:37,339 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:20:37,340 	Gloss Alignment :	         
2024-02-06 00:20:37,340 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:20:37,340 	Text Reference  :	however due to the rise   in   coronavirus cases the        tournament was shifted
2024-02-06 00:20:37,341 	Text Hypothesis :	******* *** ** the indian team was         very  particular about      his parents
2024-02-06 00:20:37,341 	Text Alignment  :	D       D   D      S      S    S           S     S          S          S   S      
2024-02-06 00:20:37,341 ========================================================================================================================
2024-02-06 00:20:37,341 Logging Sequence: 161_47.00
2024-02-06 00:20:37,341 	Gloss Reference :	A B+C+D+E
2024-02-06 00:20:37,341 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:20:37,341 	Gloss Alignment :	         
2024-02-06 00:20:37,342 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:20:37,342 	Text Reference  :	** **** he ********* requested confidentiality as he was planning to make an official announcement
2024-02-06 00:20:37,343 	Text Hypothesis :	we hope he continues to        rain            as he was ******** ** **** ** not      comfortable 
2024-02-06 00:20:37,343 	Text Alignment  :	I  I       I         S         S                         D        D  D    D  S        S           
2024-02-06 00:20:37,343 ========================================================================================================================
2024-02-06 00:20:37,343 Logging Sequence: 131_159.00
2024-02-06 00:20:37,343 	Gloss Reference :	A B+C+D+E
2024-02-06 00:20:37,343 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:20:37,343 	Gloss Alignment :	         
2024-02-06 00:20:37,344 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:20:37,345 	Text Reference  :	chanu also met  biren singh following the meeting singh described chanu as            our nation' pride  
2024-02-06 00:20:37,345 	Text Hypothesis :	***** the  bcci will  also  earn      rs  10-15   lakh  from      each  advertisement of  30      seconds
2024-02-06 00:20:37,345 	Text Alignment  :	D     S    S    S     S     S         S   S       S     S         S     S             S   S       S      
2024-02-06 00:20:37,345 ========================================================================================================================
2024-02-06 00:20:37,345 Logging Sequence: 137_167.00
2024-02-06 00:20:37,346 	Gloss Reference :	A B+C+D+E
2024-02-06 00:20:37,346 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:20:37,346 	Gloss Alignment :	         
2024-02-06 00:20:37,346 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:20:37,348 	Text Reference  :	however after 630 pm  there  will     be       certain fan zones    where beer    will be available and nowhere else 
2024-02-06 00:20:37,348 	Text Hypothesis :	******* ***** *** the polish national football team    was escorted by    fighter jets on their     way to      qatar
2024-02-06 00:20:37,348 	Text Alignment  :	D       D     D   S   S      S        S        S       S   S        S     S       S    S  S         S   S       S    
2024-02-06 00:20:37,348 ========================================================================================================================
2024-02-06 00:20:40,076 Epoch 824: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.82 
2024-02-06 00:20:40,076 EPOCH 825
2024-02-06 00:20:44,462 Epoch 825: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.68 
2024-02-06 00:20:44,463 EPOCH 826
2024-02-06 00:20:49,262 Epoch 826: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.68 
2024-02-06 00:20:49,262 EPOCH 827
2024-02-06 00:20:51,609 [Epoch: 827 Step: 00028100] Batch Recognition Loss:   0.003838 => Gls Tokens per Sec:     2072 || Batch Translation Loss:   0.018217 => Txt Tokens per Sec:     5971 || Lr: 0.000100
2024-02-06 00:20:53,719 Epoch 827: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.81 
2024-02-06 00:20:53,719 EPOCH 828
2024-02-06 00:20:58,577 Epoch 828: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.12 
2024-02-06 00:20:58,577 EPOCH 829
2024-02-06 00:21:02,936 Epoch 829: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.65 
2024-02-06 00:21:02,936 EPOCH 830
2024-02-06 00:21:04,739 [Epoch: 830 Step: 00028200] Batch Recognition Loss:   0.000243 => Gls Tokens per Sec:     2487 || Batch Translation Loss:   0.040835 => Txt Tokens per Sec:     7043 || Lr: 0.000100
2024-02-06 00:21:07,783 Epoch 830: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.31 
2024-02-06 00:21:07,784 EPOCH 831
2024-02-06 00:21:12,510 Epoch 831: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.03 
2024-02-06 00:21:12,510 EPOCH 832
2024-02-06 00:21:17,222 Epoch 832: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.76 
2024-02-06 00:21:17,223 EPOCH 833
2024-02-06 00:21:18,998 [Epoch: 833 Step: 00028300] Batch Recognition Loss:   0.000328 => Gls Tokens per Sec:     2164 || Batch Translation Loss:   0.056987 => Txt Tokens per Sec:     6093 || Lr: 0.000100
2024-02-06 00:21:21,867 Epoch 833: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.08 
2024-02-06 00:21:21,867 EPOCH 834
2024-02-06 00:21:26,313 Epoch 834: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.21 
2024-02-06 00:21:26,314 EPOCH 835
2024-02-06 00:21:31,100 Epoch 835: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.22 
2024-02-06 00:21:31,101 EPOCH 836
2024-02-06 00:21:32,056 [Epoch: 836 Step: 00028400] Batch Recognition Loss:   0.002521 => Gls Tokens per Sec:     3354 || Batch Translation Loss:   0.113848 => Txt Tokens per Sec:     8420 || Lr: 0.000100
2024-02-06 00:21:35,451 Epoch 836: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.37 
2024-02-06 00:21:35,451 EPOCH 837
2024-02-06 00:21:40,404 Epoch 837: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.38 
2024-02-06 00:21:40,404 EPOCH 838
2024-02-06 00:21:44,645 Epoch 838: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.43 
2024-02-06 00:21:44,646 EPOCH 839
2024-02-06 00:21:46,118 [Epoch: 839 Step: 00028500] Batch Recognition Loss:   0.004728 => Gls Tokens per Sec:     1564 || Batch Translation Loss:   0.180697 => Txt Tokens per Sec:     4759 || Lr: 0.000100
2024-02-06 00:21:49,639 Epoch 839: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.47 
2024-02-06 00:21:49,639 EPOCH 840
2024-02-06 00:21:53,833 Epoch 840: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.16 
2024-02-06 00:21:53,833 EPOCH 841
2024-02-06 00:21:58,751 Epoch 841: Total Training Recognition Loss 0.07  Total Training Translation Loss 5.49 
2024-02-06 00:21:58,752 EPOCH 842
2024-02-06 00:21:59,671 [Epoch: 842 Step: 00028600] Batch Recognition Loss:   0.000744 => Gls Tokens per Sec:     2092 || Batch Translation Loss:   0.203822 => Txt Tokens per Sec:     5705 || Lr: 0.000100
2024-02-06 00:22:03,044 Epoch 842: Total Training Recognition Loss 0.14  Total Training Translation Loss 2.52 
2024-02-06 00:22:03,045 EPOCH 843
2024-02-06 00:22:07,792 Epoch 843: Total Training Recognition Loss 0.99  Total Training Translation Loss 1.61 
2024-02-06 00:22:07,792 EPOCH 844
2024-02-06 00:22:12,468 Epoch 844: Total Training Recognition Loss 1.61  Total Training Translation Loss 1.31 
2024-02-06 00:22:12,469 EPOCH 845
2024-02-06 00:22:12,931 [Epoch: 845 Step: 00028700] Batch Recognition Loss:   0.002134 => Gls Tokens per Sec:     2777 || Batch Translation Loss:   0.029057 => Txt Tokens per Sec:     6870 || Lr: 0.000100
2024-02-06 00:22:17,403 Epoch 845: Total Training Recognition Loss 1.93  Total Training Translation Loss 1.36 
2024-02-06 00:22:17,404 EPOCH 846
2024-02-06 00:22:21,829 Epoch 846: Total Training Recognition Loss 0.52  Total Training Translation Loss 1.26 
2024-02-06 00:22:21,829 EPOCH 847
2024-02-06 00:22:26,464 Epoch 847: Total Training Recognition Loss 0.56  Total Training Translation Loss 0.95 
2024-02-06 00:22:26,465 EPOCH 848
2024-02-06 00:22:26,711 [Epoch: 848 Step: 00028800] Batch Recognition Loss:   0.011148 => Gls Tokens per Sec:     2612 || Batch Translation Loss:   0.026701 => Txt Tokens per Sec:     6763 || Lr: 0.000100
2024-02-06 00:22:31,047 Epoch 848: Total Training Recognition Loss 0.38  Total Training Translation Loss 0.90 
2024-02-06 00:22:31,048 EPOCH 849
2024-02-06 00:22:35,608 Epoch 849: Total Training Recognition Loss 0.11  Total Training Translation Loss 0.90 
2024-02-06 00:22:35,609 EPOCH 850
2024-02-06 00:22:40,366 [Epoch: 850 Step: 00028900] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:     2233 || Batch Translation Loss:   0.021996 => Txt Tokens per Sec:     6179 || Lr: 0.000100
2024-02-06 00:22:40,366 Epoch 850: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.74 
2024-02-06 00:22:40,366 EPOCH 851
2024-02-06 00:22:44,706 Epoch 851: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.74 
2024-02-06 00:22:44,707 EPOCH 852
2024-02-06 00:22:49,654 Epoch 852: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.84 
2024-02-06 00:22:49,655 EPOCH 853
2024-02-06 00:22:53,873 [Epoch: 853 Step: 00029000] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     2367 || Batch Translation Loss:   0.022609 => Txt Tokens per Sec:     6563 || Lr: 0.000100
2024-02-06 00:22:54,188 Epoch 853: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.86 
2024-02-06 00:22:54,188 EPOCH 854
2024-02-06 00:22:58,870 Epoch 854: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.79 
2024-02-06 00:22:58,871 EPOCH 855
2024-02-06 00:23:03,055 Epoch 855: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-06 00:23:03,055 EPOCH 856
2024-02-06 00:23:07,559 [Epoch: 856 Step: 00029100] Batch Recognition Loss:   0.000166 => Gls Tokens per Sec:     2133 || Batch Translation Loss:   0.014804 => Txt Tokens per Sec:     5874 || Lr: 0.000100
2024-02-06 00:23:08,113 Epoch 856: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.77 
2024-02-06 00:23:08,113 EPOCH 857
2024-02-06 00:23:12,346 Epoch 857: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.86 
2024-02-06 00:23:12,346 EPOCH 858
2024-02-06 00:23:17,344 Epoch 858: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.02 
2024-02-06 00:23:17,344 EPOCH 859
2024-02-06 00:23:20,977 [Epoch: 859 Step: 00029200] Batch Recognition Loss:   0.000197 => Gls Tokens per Sec:     2467 || Batch Translation Loss:   0.021461 => Txt Tokens per Sec:     6803 || Lr: 0.000100
2024-02-06 00:23:21,635 Epoch 859: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.54 
2024-02-06 00:23:21,635 EPOCH 860
2024-02-06 00:23:26,480 Epoch 860: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.99 
2024-02-06 00:23:26,481 EPOCH 861
2024-02-06 00:23:30,904 Epoch 861: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.09 
2024-02-06 00:23:30,905 EPOCH 862
2024-02-06 00:23:34,286 [Epoch: 862 Step: 00029300] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:     2385 || Batch Translation Loss:   0.023518 => Txt Tokens per Sec:     6544 || Lr: 0.000100
2024-02-06 00:23:35,594 Epoch 862: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.92 
2024-02-06 00:23:35,594 EPOCH 863
2024-02-06 00:23:40,162 Epoch 863: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.04 
2024-02-06 00:23:40,162 EPOCH 864
2024-02-06 00:23:44,824 Epoch 864: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.85 
2024-02-06 00:23:44,824 EPOCH 865
2024-02-06 00:23:48,357 [Epoch: 865 Step: 00029400] Batch Recognition Loss:   0.000299 => Gls Tokens per Sec:     2174 || Batch Translation Loss:   0.160841 => Txt Tokens per Sec:     6077 || Lr: 0.000100
2024-02-06 00:23:49,450 Epoch 865: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.80 
2024-02-06 00:23:49,450 EPOCH 866
2024-02-06 00:23:53,946 Epoch 866: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.78 
2024-02-06 00:23:53,946 EPOCH 867
2024-02-06 00:23:58,737 Epoch 867: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.59 
2024-02-06 00:23:58,738 EPOCH 868
2024-02-06 00:24:01,413 [Epoch: 868 Step: 00029500] Batch Recognition Loss:   0.000583 => Gls Tokens per Sec:     2631 || Batch Translation Loss:   0.096668 => Txt Tokens per Sec:     7314 || Lr: 0.000100
2024-02-06 00:24:03,003 Epoch 868: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.39 
2024-02-06 00:24:03,003 EPOCH 869
2024-02-06 00:24:07,933 Epoch 869: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.43 
2024-02-06 00:24:07,933 EPOCH 870
2024-02-06 00:24:12,072 Epoch 870: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.12 
2024-02-06 00:24:12,072 EPOCH 871
2024-02-06 00:24:15,183 [Epoch: 871 Step: 00029600] Batch Recognition Loss:   0.000464 => Gls Tokens per Sec:     1974 || Batch Translation Loss:   0.081952 => Txt Tokens per Sec:     5596 || Lr: 0.000100
2024-02-06 00:24:17,077 Epoch 871: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.44 
2024-02-06 00:24:17,077 EPOCH 872
2024-02-06 00:24:21,380 Epoch 872: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.20 
2024-02-06 00:24:21,380 EPOCH 873
2024-02-06 00:24:26,258 Epoch 873: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.81 
2024-02-06 00:24:26,259 EPOCH 874
2024-02-06 00:24:28,331 [Epoch: 874 Step: 00029700] Batch Recognition Loss:   0.000498 => Gls Tokens per Sec:     2783 || Batch Translation Loss:   0.067340 => Txt Tokens per Sec:     7322 || Lr: 0.000100
2024-02-06 00:24:30,597 Epoch 874: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.72 
2024-02-06 00:24:30,598 EPOCH 875
2024-02-06 00:24:35,265 Epoch 875: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.56 
2024-02-06 00:24:35,265 EPOCH 876
2024-02-06 00:24:39,884 Epoch 876: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.85 
2024-02-06 00:24:39,884 EPOCH 877
2024-02-06 00:24:41,886 [Epoch: 877 Step: 00029800] Batch Recognition Loss:   0.000834 => Gls Tokens per Sec:     2430 || Batch Translation Loss:   0.010406 => Txt Tokens per Sec:     6660 || Lr: 0.000100
2024-02-06 00:24:44,394 Epoch 877: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.44 
2024-02-06 00:24:44,394 EPOCH 878
2024-02-06 00:24:49,195 Epoch 878: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.55 
2024-02-06 00:24:49,196 EPOCH 879
2024-02-06 00:24:53,487 Epoch 879: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.55 
2024-02-06 00:24:53,488 EPOCH 880
2024-02-06 00:24:55,875 [Epoch: 880 Step: 00029900] Batch Recognition Loss:   0.000285 => Gls Tokens per Sec:     1878 || Batch Translation Loss:   0.046949 => Txt Tokens per Sec:     5695 || Lr: 0.000100
2024-02-06 00:24:58,401 Epoch 880: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.17 
2024-02-06 00:24:58,402 EPOCH 881
2024-02-06 00:25:02,663 Epoch 881: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.68 
2024-02-06 00:25:02,663 EPOCH 882
2024-02-06 00:25:07,571 Epoch 882: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.78 
2024-02-06 00:25:07,571 EPOCH 883
2024-02-06 00:25:08,923 [Epoch: 883 Step: 00030000] Batch Recognition Loss:   0.000343 => Gls Tokens per Sec:     2842 || Batch Translation Loss:   0.055944 => Txt Tokens per Sec:     7496 || Lr: 0.000100
2024-02-06 00:25:17,847 Validation result at epoch 883, step    30000: duration: 8.9240s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.44943	Translation Loss: 93273.26562	PPL: 11116.06641
	Eval Metric: BLEU
	WER 3.46	(DEL: 0.00,	INS: 0.00,	SUB: 3.46)
	BLEU-4 0.56	(BLEU-1: 11.03,	BLEU-2: 3.42,	BLEU-3: 1.21,	BLEU-4: 0.56)
	CHRF 17.13	ROUGE 9.09
2024-02-06 00:25:17,848 Logging Recognition and Translation Outputs
2024-02-06 00:25:17,849 ========================================================================================================================
2024-02-06 00:25:17,849 Logging Sequence: 146_102.00
2024-02-06 00:25:17,849 	Gloss Reference :	A B+C+D+E
2024-02-06 00:25:17,849 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:25:17,849 	Gloss Alignment :	         
2024-02-06 00:25:17,849 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:25:17,851 	Text Reference  :	***** ****** **** famous indian      champion players like kidambi srikanth and  ashwini ponappa have  tested positive for       coronavirus
2024-02-06 00:25:17,851 	Text Hypothesis :	rohit sharma will assume captainship while    kohli   will be      free     year old     it      today on     his      religious beliefs    
2024-02-06 00:25:17,851 	Text Alignment  :	I     I      I    S      S           S        S       S    S       S        S    S       S       S     S      S        S         S          
2024-02-06 00:25:17,851 ========================================================================================================================
2024-02-06 00:25:17,851 Logging Sequence: 53_178.00
2024-02-06 00:25:17,852 	Gloss Reference :	A B+C+D+E
2024-02-06 00:25:17,852 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:25:17,852 	Gloss Alignment :	         
2024-02-06 00:25:17,852 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:25:17,853 	Text Reference  :	* ** **** the   money would help all those affected by  the humanitarian crisis in *** **** afghanistan
2024-02-06 00:25:17,853 	Text Hypothesis :	i am also asked to    tell  you  all ***** rights   for the various      groups in the next season     
2024-02-06 00:25:17,854 	Text Alignment  :	I I  I    S     S     S     S        D     S        S       S            S         I   I    S          
2024-02-06 00:25:17,854 ========================================================================================================================
2024-02-06 00:25:17,854 Logging Sequence: 129_200.00
2024-02-06 00:25:17,854 	Gloss Reference :	A B+C+D+E  
2024-02-06 00:25:17,854 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-06 00:25:17,854 	Gloss Alignment :	  S        
2024-02-06 00:25:17,855 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:25:17,856 	Text Reference  :	the ioc would lose about 4    billion if   the **** **** olympics were  to be      cancelled
2024-02-06 00:25:17,856 	Text Hypothesis :	*** he  never used a     look to      know the 2000 2008 olympic  games in javelin throw    
2024-02-06 00:25:17,856 	Text Alignment  :	D   S   S     S    S     S    S       S        I    I    S        S     S  S       S        
2024-02-06 00:25:17,856 ========================================================================================================================
2024-02-06 00:25:17,856 Logging Sequence: 77_2.00
2024-02-06 00:25:17,857 	Gloss Reference :	A B+C+D+E  
2024-02-06 00:25:17,857 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-06 00:25:17,857 	Gloss Alignment :	  S        
2024-02-06 00:25:17,857 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:25:17,858 	Text Reference  :	on 25th april the ipl match between sunrisers hyderabad and delhi       capitals ended    in a   tie
2024-02-06 00:25:17,858 	Text Hypothesis :	on **** ***** 4th may 2022  bowler  bowled    in        the semi-finals against  pakistan of his run
2024-02-06 00:25:17,859 	Text Alignment  :	   D    D     S   S   S     S       S         S         S   S           S        S        S  S   S  
2024-02-06 00:25:17,859 ========================================================================================================================
2024-02-06 00:25:17,859 Logging Sequence: 119_170.00
2024-02-06 00:25:17,859 	Gloss Reference :	A B+C+D+E
2024-02-06 00:25:17,859 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:25:17,859 	Gloss Alignment :	         
2024-02-06 00:25:17,859 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:25:17,860 	Text Reference  :	they said  it  was  a     proud moment messi is a big hearted man
2024-02-06 00:25:17,860 	Text Hypothesis :	the  girls had made india proud ****** ***** ** * *** ******* ***
2024-02-06 00:25:17,860 	Text Alignment  :	S    S     S   S    S           D      D     D  D D   D       D  
2024-02-06 00:25:17,860 ========================================================================================================================
2024-02-06 00:25:21,177 Epoch 883: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.88 
2024-02-06 00:25:21,177 EPOCH 884
2024-02-06 00:25:26,237 Epoch 884: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.24 
2024-02-06 00:25:26,238 EPOCH 885
2024-02-06 00:25:30,526 Epoch 885: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.10 
2024-02-06 00:25:30,526 EPOCH 886
2024-02-06 00:25:31,813 [Epoch: 886 Step: 00030100] Batch Recognition Loss:   0.000505 => Gls Tokens per Sec:     2286 || Batch Translation Loss:   0.038769 => Txt Tokens per Sec:     6507 || Lr: 0.000100
2024-02-06 00:25:35,347 Epoch 886: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.90 
2024-02-06 00:25:35,348 EPOCH 887
2024-02-06 00:25:39,710 Epoch 887: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.63 
2024-02-06 00:25:39,710 EPOCH 888
2024-02-06 00:25:44,399 Epoch 888: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.03 
2024-02-06 00:25:44,400 EPOCH 889
2024-02-06 00:25:45,189 [Epoch: 889 Step: 00030200] Batch Recognition Loss:   0.000279 => Gls Tokens per Sec:     3247 || Batch Translation Loss:   0.022322 => Txt Tokens per Sec:     8164 || Lr: 0.000100
2024-02-06 00:25:48,982 Epoch 889: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.78 
2024-02-06 00:25:48,983 EPOCH 890
2024-02-06 00:25:53,170 Epoch 890: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.98 
2024-02-06 00:25:53,171 EPOCH 891
2024-02-06 00:25:58,172 Epoch 891: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.38 
2024-02-06 00:25:58,172 EPOCH 892
2024-02-06 00:25:58,704 [Epoch: 892 Step: 00030300] Batch Recognition Loss:   0.000496 => Gls Tokens per Sec:     3620 || Batch Translation Loss:   0.030199 => Txt Tokens per Sec:     8657 || Lr: 0.000100
2024-02-06 00:26:02,353 Epoch 892: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.11 
2024-02-06 00:26:02,353 EPOCH 893
2024-02-06 00:26:07,293 Epoch 893: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.00 
2024-02-06 00:26:07,293 EPOCH 894
2024-02-06 00:26:11,603 Epoch 894: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.07 
2024-02-06 00:26:11,603 EPOCH 895
2024-02-06 00:26:12,074 [Epoch: 895 Step: 00030400] Batch Recognition Loss:   0.000418 => Gls Tokens per Sec:     2173 || Batch Translation Loss:   0.027245 => Txt Tokens per Sec:     6023 || Lr: 0.000100
2024-02-06 00:26:16,407 Epoch 895: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.24 
2024-02-06 00:26:16,407 EPOCH 896
2024-02-06 00:26:20,776 Epoch 896: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.07 
2024-02-06 00:26:20,776 EPOCH 897
2024-02-06 00:26:25,487 Epoch 897: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.72 
2024-02-06 00:26:25,487 EPOCH 898
2024-02-06 00:26:25,889 [Epoch: 898 Step: 00030500] Batch Recognition Loss:   0.000499 => Gls Tokens per Sec:     1601 || Batch Translation Loss:   0.037552 => Txt Tokens per Sec:     5344 || Lr: 0.000100
2024-02-06 00:26:30,073 Epoch 898: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.02 
2024-02-06 00:26:30,073 EPOCH 899
2024-02-06 00:26:34,643 Epoch 899: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.19 
2024-02-06 00:26:34,643 EPOCH 900
2024-02-06 00:26:39,305 [Epoch: 900 Step: 00030600] Batch Recognition Loss:   0.000707 => Gls Tokens per Sec:     2278 || Batch Translation Loss:   0.013590 => Txt Tokens per Sec:     6304 || Lr: 0.000100
2024-02-06 00:26:39,306 Epoch 900: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.18 
2024-02-06 00:26:39,306 EPOCH 901
2024-02-06 00:26:43,697 Epoch 901: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.84 
2024-02-06 00:26:43,698 EPOCH 902
2024-02-06 00:26:48,487 Epoch 902: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.67 
2024-02-06 00:26:48,487 EPOCH 903
2024-02-06 00:26:52,517 [Epoch: 903 Step: 00030700] Batch Recognition Loss:   0.000585 => Gls Tokens per Sec:     2477 || Batch Translation Loss:   0.049385 => Txt Tokens per Sec:     6937 || Lr: 0.000100
2024-02-06 00:26:52,757 Epoch 903: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.95 
2024-02-06 00:26:52,757 EPOCH 904
2024-02-06 00:26:57,659 Epoch 904: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.36 
2024-02-06 00:26:57,659 EPOCH 905
2024-02-06 00:27:01,825 Epoch 905: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.09 
2024-02-06 00:27:01,825 EPOCH 906
2024-02-06 00:27:06,214 [Epoch: 906 Step: 00030800] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     2129 || Batch Translation Loss:   0.023256 => Txt Tokens per Sec:     5930 || Lr: 0.000100
2024-02-06 00:27:06,787 Epoch 906: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.19 
2024-02-06 00:27:06,787 EPOCH 907
2024-02-06 00:27:11,153 Epoch 907: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.99 
2024-02-06 00:27:11,153 EPOCH 908
2024-02-06 00:27:15,285 Epoch 908: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.59 
2024-02-06 00:27:15,285 EPOCH 909
2024-02-06 00:27:19,151 [Epoch: 909 Step: 00030900] Batch Recognition Loss:   0.000419 => Gls Tokens per Sec:     2319 || Batch Translation Loss:   0.055316 => Txt Tokens per Sec:     6468 || Lr: 0.000100
2024-02-06 00:27:19,857 Epoch 909: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.17 
2024-02-06 00:27:19,857 EPOCH 910
2024-02-06 00:27:24,578 Epoch 910: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.94 
2024-02-06 00:27:24,578 EPOCH 911
2024-02-06 00:27:28,644 Epoch 911: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.42 
2024-02-06 00:27:28,644 EPOCH 912
2024-02-06 00:27:32,436 [Epoch: 912 Step: 00031000] Batch Recognition Loss:   0.000334 => Gls Tokens per Sec:     2194 || Batch Translation Loss:   0.013671 => Txt Tokens per Sec:     6173 || Lr: 0.000100
2024-02-06 00:27:33,497 Epoch 912: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.32 
2024-02-06 00:27:33,497 EPOCH 913
2024-02-06 00:27:37,955 Epoch 913: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.92 
2024-02-06 00:27:37,955 EPOCH 914
2024-02-06 00:27:42,668 Epoch 914: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.19 
2024-02-06 00:27:42,669 EPOCH 915
2024-02-06 00:27:45,923 [Epoch: 915 Step: 00031100] Batch Recognition Loss:   0.000367 => Gls Tokens per Sec:     2281 || Batch Translation Loss:   0.032703 => Txt Tokens per Sec:     6204 || Lr: 0.000100
2024-02-06 00:27:47,194 Epoch 915: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.18 
2024-02-06 00:27:47,194 EPOCH 916
2024-02-06 00:27:51,794 Epoch 916: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.43 
2024-02-06 00:27:51,795 EPOCH 917
2024-02-06 00:27:56,464 Epoch 917: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.09 
2024-02-06 00:27:56,464 EPOCH 918
2024-02-06 00:27:59,345 [Epoch: 918 Step: 00031200] Batch Recognition Loss:   0.000365 => Gls Tokens per Sec:     2445 || Batch Translation Loss:   0.031280 => Txt Tokens per Sec:     6999 || Lr: 0.000100
2024-02-06 00:28:01,007 Epoch 918: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.72 
2024-02-06 00:28:01,008 EPOCH 919
2024-02-06 00:28:05,753 Epoch 919: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.31 
2024-02-06 00:28:05,753 EPOCH 920
2024-02-06 00:28:10,040 Epoch 920: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.52 
2024-02-06 00:28:10,040 EPOCH 921
2024-02-06 00:28:12,926 [Epoch: 921 Step: 00031300] Batch Recognition Loss:   0.000427 => Gls Tokens per Sec:     2219 || Batch Translation Loss:   0.260952 => Txt Tokens per Sec:     6320 || Lr: 0.000100
2024-02-06 00:28:15,022 Epoch 921: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.91 
2024-02-06 00:28:15,023 EPOCH 922
2024-02-06 00:28:20,060 Epoch 922: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.08 
2024-02-06 00:28:20,061 EPOCH 923
2024-02-06 00:28:24,773 Epoch 923: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.90 
2024-02-06 00:28:24,774 EPOCH 924
2024-02-06 00:28:27,339 [Epoch: 924 Step: 00031400] Batch Recognition Loss:   0.001090 => Gls Tokens per Sec:     2246 || Batch Translation Loss:   0.078240 => Txt Tokens per Sec:     6299 || Lr: 0.000100
2024-02-06 00:28:29,615 Epoch 924: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.76 
2024-02-06 00:28:29,615 EPOCH 925
2024-02-06 00:28:34,262 Epoch 925: Total Training Recognition Loss 0.12  Total Training Translation Loss 17.86 
2024-02-06 00:28:34,262 EPOCH 926
2024-02-06 00:28:39,132 Epoch 926: Total Training Recognition Loss 0.10  Total Training Translation Loss 4.57 
2024-02-06 00:28:39,132 EPOCH 927
2024-02-06 00:28:41,446 [Epoch: 927 Step: 00031500] Batch Recognition Loss:   0.000433 => Gls Tokens per Sec:     2102 || Batch Translation Loss:   0.036864 => Txt Tokens per Sec:     5702 || Lr: 0.000100
2024-02-06 00:28:43,868 Epoch 927: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.70 
2024-02-06 00:28:43,868 EPOCH 928
2024-02-06 00:28:48,590 Epoch 928: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.34 
2024-02-06 00:28:48,590 EPOCH 929
2024-02-06 00:28:53,451 Epoch 929: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.95 
2024-02-06 00:28:53,451 EPOCH 930
2024-02-06 00:28:55,488 [Epoch: 930 Step: 00031600] Batch Recognition Loss:   0.002251 => Gls Tokens per Sec:     2074 || Batch Translation Loss:   0.016349 => Txt Tokens per Sec:     5359 || Lr: 0.000100
2024-02-06 00:28:58,585 Epoch 930: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.71 
2024-02-06 00:28:58,586 EPOCH 931
2024-02-06 00:29:03,548 Epoch 931: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-06 00:29:03,549 EPOCH 932
2024-02-06 00:29:08,482 Epoch 932: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.65 
2024-02-06 00:29:08,483 EPOCH 933
2024-02-06 00:29:10,003 [Epoch: 933 Step: 00031700] Batch Recognition Loss:   0.000283 => Gls Tokens per Sec:     2528 || Batch Translation Loss:   0.017583 => Txt Tokens per Sec:     6563 || Lr: 0.000100
2024-02-06 00:29:13,311 Epoch 933: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.79 
2024-02-06 00:29:13,312 EPOCH 934
2024-02-06 00:29:18,171 Epoch 934: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.97 
2024-02-06 00:29:18,172 EPOCH 935
2024-02-06 00:29:23,055 Epoch 935: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.93 
2024-02-06 00:29:23,055 EPOCH 936
2024-02-06 00:29:24,297 [Epoch: 936 Step: 00031800] Batch Recognition Loss:   0.000242 => Gls Tokens per Sec:     2579 || Batch Translation Loss:   0.200806 => Txt Tokens per Sec:     6739 || Lr: 0.000100
2024-02-06 00:29:27,635 Epoch 936: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-06 00:29:27,635 EPOCH 937
2024-02-06 00:29:32,497 Epoch 937: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-06 00:29:32,497 EPOCH 938
2024-02-06 00:29:37,252 Epoch 938: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.18 
2024-02-06 00:29:37,252 EPOCH 939
2024-02-06 00:29:38,362 [Epoch: 939 Step: 00031900] Batch Recognition Loss:   0.000221 => Gls Tokens per Sec:     2308 || Batch Translation Loss:   0.014165 => Txt Tokens per Sec:     6024 || Lr: 0.000100
2024-02-06 00:29:42,244 Epoch 939: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.71 
2024-02-06 00:29:42,244 EPOCH 940
2024-02-06 00:29:47,271 Epoch 940: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.64 
2024-02-06 00:29:47,272 EPOCH 941
2024-02-06 00:29:51,869 Epoch 941: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.57 
2024-02-06 00:29:51,869 EPOCH 942
2024-02-06 00:29:52,601 [Epoch: 942 Step: 00032000] Batch Recognition Loss:   0.000460 => Gls Tokens per Sec:     2626 || Batch Translation Loss:   0.022604 => Txt Tokens per Sec:     6864 || Lr: 0.000100
2024-02-06 00:30:01,604 Validation result at epoch 942, step    32000: duration: 9.0020s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.42855	Translation Loss: 92643.82812	PPL: 10438.73535
	Eval Metric: BLEU
	WER 3.67	(DEL: 0.00,	INS: 0.00,	SUB: 3.67)
	BLEU-4 0.62	(BLEU-1: 11.02,	BLEU-2: 3.36,	BLEU-3: 1.29,	BLEU-4: 0.62)
	CHRF 17.15	ROUGE 9.24
2024-02-06 00:30:01,605 Logging Recognition and Translation Outputs
2024-02-06 00:30:01,605 ========================================================================================================================
2024-02-06 00:30:01,605 Logging Sequence: 162_133.00
2024-02-06 00:30:01,605 	Gloss Reference :	A B+C+D+E
2024-02-06 00:30:01,605 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:30:01,606 	Gloss Alignment :	         
2024-02-06 00:30:01,606 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:30:01,606 	Text Reference  :	they also sent rape threats to his 9-month old daughter
2024-02-06 00:30:01,607 	Text Hypothesis :	**** now  we   will have    to *** wait    for updates 
2024-02-06 00:30:01,607 	Text Alignment  :	D    S    S    S    S          D   S       S   S       
2024-02-06 00:30:01,607 ========================================================================================================================
2024-02-06 00:30:01,607 Logging Sequence: 134_236.00
2024-02-06 00:30:01,607 	Gloss Reference :	A B+C+D+E
2024-02-06 00:30:01,607 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:30:01,607 	Gloss Alignment :	         
2024-02-06 00:30:01,607 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:30:01,609 	Text Reference  :	**** *** * ****** after the interaction **** ******* modi    tweeted the images and         captioned it saying
2024-02-06 00:30:01,609 	Text Hypothesis :	2022 was a report by    the interaction with india's captain and     the ****** deaflympics debut     in 1965  
2024-02-06 00:30:01,609 	Text Alignment  :	I    I   I I      S                     I    I       S       S           D      S           S         S  S     
2024-02-06 00:30:01,609 ========================================================================================================================
2024-02-06 00:30:01,609 Logging Sequence: 145_52.00
2024-02-06 00:30:01,609 	Gloss Reference :	A B+C+D+E
2024-02-06 00:30:01,609 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:30:01,610 	Gloss Alignment :	         
2024-02-06 00:30:01,610 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:30:01,611 	Text Reference  :	her name was dropped despite having qualified as      she  was the    only   female athlete
2024-02-06 00:30:01,611 	Text Hypothesis :	*** **** *** ******* ******* ****** the       matches will be  played across 4      victory
2024-02-06 00:30:01,611 	Text Alignment  :	D   D    D   D       D       D      S         S       S    S   S      S      S      S      
2024-02-06 00:30:01,611 ========================================================================================================================
2024-02-06 00:30:01,611 Logging Sequence: 175_40.00
2024-02-06 00:30:01,611 	Gloss Reference :	A B+C+D+E
2024-02-06 00:30:01,611 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:30:01,611 	Gloss Alignment :	         
2024-02-06 00:30:01,612 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:30:01,612 	Text Reference  :	soumyadeep and shreya bagged three     medals each  including a        silver medal     each   
2024-02-06 00:30:01,612 	Text Hypothesis :	********** *** on     10th   september 2023   match between   pakistan and    different stories
2024-02-06 00:30:01,613 	Text Alignment  :	D          D   S      S      S         S      S     S         S        S      S         S      
2024-02-06 00:30:01,613 ========================================================================================================================
2024-02-06 00:30:01,613 Logging Sequence: 156_51.00
2024-02-06 00:30:01,613 	Gloss Reference :	A B+C+D+E
2024-02-06 00:30:01,613 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:30:01,613 	Gloss Alignment :	         
2024-02-06 00:30:01,613 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:30:01,614 	Text Reference  :	the selection of    the players was similar  to   that of ipl
2024-02-06 00:30:01,614 	Text Hypothesis :	*** what      about the world   cup stadiums have fun  in ipl
2024-02-06 00:30:01,614 	Text Alignment  :	D   S         S         S       S   S        S    S    S     
2024-02-06 00:30:01,615 ========================================================================================================================
2024-02-06 00:30:05,721 Epoch 942: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.18 
2024-02-06 00:30:05,721 EPOCH 943
2024-02-06 00:30:10,479 Epoch 943: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.24 
2024-02-06 00:30:10,479 EPOCH 944
2024-02-06 00:30:15,086 Epoch 944: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.88 
2024-02-06 00:30:15,086 EPOCH 945
2024-02-06 00:30:15,583 [Epoch: 945 Step: 00032100] Batch Recognition Loss:   0.000494 => Gls Tokens per Sec:     2581 || Batch Translation Loss:   0.022781 => Txt Tokens per Sec:     7712 || Lr: 0.000100
2024-02-06 00:30:19,668 Epoch 945: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.77 
2024-02-06 00:30:19,669 EPOCH 946
2024-02-06 00:30:24,339 Epoch 946: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.84 
2024-02-06 00:30:24,340 EPOCH 947
2024-02-06 00:30:28,872 Epoch 947: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-06 00:30:28,872 EPOCH 948
2024-02-06 00:30:29,175 [Epoch: 948 Step: 00032200] Batch Recognition Loss:   0.000312 => Gls Tokens per Sec:     2118 || Batch Translation Loss:   0.012095 => Txt Tokens per Sec:     6212 || Lr: 0.000100
2024-02-06 00:30:33,603 Epoch 948: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.39 
2024-02-06 00:30:33,603 EPOCH 949
2024-02-06 00:30:37,851 Epoch 949: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.86 
2024-02-06 00:30:37,852 EPOCH 950
2024-02-06 00:30:42,800 [Epoch: 950 Step: 00032300] Batch Recognition Loss:   0.000335 => Gls Tokens per Sec:     2147 || Batch Translation Loss:   0.039418 => Txt Tokens per Sec:     5941 || Lr: 0.000100
2024-02-06 00:30:42,800 Epoch 950: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.15 
2024-02-06 00:30:42,800 EPOCH 951
2024-02-06 00:30:46,938 Epoch 951: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.25 
2024-02-06 00:30:46,938 EPOCH 952
2024-02-06 00:30:51,860 Epoch 952: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.03 
2024-02-06 00:30:51,861 EPOCH 953
2024-02-06 00:30:55,921 [Epoch: 953 Step: 00032400] Batch Recognition Loss:   0.009519 => Gls Tokens per Sec:     2459 || Batch Translation Loss:   0.027157 => Txt Tokens per Sec:     6809 || Lr: 0.000100
2024-02-06 00:30:56,136 Epoch 953: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.83 
2024-02-06 00:30:56,136 EPOCH 954
2024-02-06 00:31:00,931 Epoch 954: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.97 
2024-02-06 00:31:00,931 EPOCH 955
2024-02-06 00:31:05,381 Epoch 955: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.23 
2024-02-06 00:31:05,381 EPOCH 956
2024-02-06 00:31:09,580 [Epoch: 956 Step: 00032500] Batch Recognition Loss:   0.001294 => Gls Tokens per Sec:     2225 || Batch Translation Loss:   0.062034 => Txt Tokens per Sec:     6136 || Lr: 0.000100
2024-02-06 00:31:10,143 Epoch 956: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.02 
2024-02-06 00:31:10,143 EPOCH 957
2024-02-06 00:31:14,804 Epoch 957: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.37 
2024-02-06 00:31:14,804 EPOCH 958
2024-02-06 00:31:19,752 Epoch 958: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.54 
2024-02-06 00:31:19,753 EPOCH 959
2024-02-06 00:31:23,306 [Epoch: 959 Step: 00032600] Batch Recognition Loss:   0.000239 => Gls Tokens per Sec:     2450 || Batch Translation Loss:   0.179553 => Txt Tokens per Sec:     6661 || Lr: 0.000100
2024-02-06 00:31:24,305 Epoch 959: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.11 
2024-02-06 00:31:24,306 EPOCH 960
2024-02-06 00:31:29,306 Epoch 960: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.90 
2024-02-06 00:31:29,307 EPOCH 961
2024-02-06 00:31:33,864 Epoch 961: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.47 
2024-02-06 00:31:33,864 EPOCH 962
2024-02-06 00:31:37,636 [Epoch: 962 Step: 00032700] Batch Recognition Loss:   0.003520 => Gls Tokens per Sec:     2138 || Batch Translation Loss:   0.034268 => Txt Tokens per Sec:     5820 || Lr: 0.000100
2024-02-06 00:31:38,892 Epoch 962: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.79 
2024-02-06 00:31:38,893 EPOCH 963
2024-02-06 00:31:43,386 Epoch 963: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.09 
2024-02-06 00:31:43,386 EPOCH 964
2024-02-06 00:31:48,445 Epoch 964: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.12 
2024-02-06 00:31:48,445 EPOCH 965
2024-02-06 00:31:51,707 [Epoch: 965 Step: 00032800] Batch Recognition Loss:   0.000444 => Gls Tokens per Sec:     2355 || Batch Translation Loss:   0.054808 => Txt Tokens per Sec:     6454 || Lr: 0.000100
2024-02-06 00:31:53,075 Epoch 965: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.05 
2024-02-06 00:31:53,076 EPOCH 966
2024-02-06 00:31:57,375 Epoch 966: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.12 
2024-02-06 00:31:57,376 EPOCH 967
2024-02-06 00:32:02,236 Epoch 967: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.31 
2024-02-06 00:32:02,236 EPOCH 968
2024-02-06 00:32:04,953 [Epoch: 968 Step: 00032900] Batch Recognition Loss:   0.000375 => Gls Tokens per Sec:     2496 || Batch Translation Loss:   0.027422 => Txt Tokens per Sec:     6809 || Lr: 0.000100
2024-02-06 00:32:06,341 Epoch 968: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.44 
2024-02-06 00:32:06,341 EPOCH 969
2024-02-06 00:32:11,352 Epoch 969: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.93 
2024-02-06 00:32:11,353 EPOCH 970
2024-02-06 00:32:15,852 Epoch 970: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.95 
2024-02-06 00:32:15,853 EPOCH 971
2024-02-06 00:32:19,040 [Epoch: 971 Step: 00033000] Batch Recognition Loss:   0.000626 => Gls Tokens per Sec:     2009 || Batch Translation Loss:   0.019467 => Txt Tokens per Sec:     5797 || Lr: 0.000100
2024-02-06 00:32:20,683 Epoch 971: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.15 
2024-02-06 00:32:20,683 EPOCH 972
2024-02-06 00:32:24,763 Epoch 972: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.97 
2024-02-06 00:32:24,763 EPOCH 973
2024-02-06 00:32:29,576 Epoch 973: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.15 
2024-02-06 00:32:29,576 EPOCH 974
2024-02-06 00:32:31,840 [Epoch: 974 Step: 00033100] Batch Recognition Loss:   0.000248 => Gls Tokens per Sec:     2431 || Batch Translation Loss:   0.024691 => Txt Tokens per Sec:     6895 || Lr: 0.000100
2024-02-06 00:32:33,963 Epoch 974: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.35 
2024-02-06 00:32:33,963 EPOCH 975
2024-02-06 00:32:38,958 Epoch 975: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.92 
2024-02-06 00:32:38,958 EPOCH 976
2024-02-06 00:32:43,626 Epoch 976: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.40 
2024-02-06 00:32:43,627 EPOCH 977
2024-02-06 00:32:45,513 [Epoch: 977 Step: 00033200] Batch Recognition Loss:   0.000583 => Gls Tokens per Sec:     2715 || Batch Translation Loss:   0.038028 => Txt Tokens per Sec:     7262 || Lr: 0.000100
2024-02-06 00:32:47,754 Epoch 977: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.56 
2024-02-06 00:32:47,755 EPOCH 978
2024-02-06 00:32:52,703 Epoch 978: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.47 
2024-02-06 00:32:52,704 EPOCH 979
2024-02-06 00:32:56,959 Epoch 979: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.58 
2024-02-06 00:32:56,959 EPOCH 980
2024-02-06 00:32:58,636 [Epoch: 980 Step: 00033300] Batch Recognition Loss:   0.000152 => Gls Tokens per Sec:     2518 || Batch Translation Loss:   0.005454 => Txt Tokens per Sec:     6768 || Lr: 0.000100
2024-02-06 00:33:01,794 Epoch 980: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.92 
2024-02-06 00:33:01,794 EPOCH 981
2024-02-06 00:33:06,190 Epoch 981: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.61 
2024-02-06 00:33:06,191 EPOCH 982
2024-02-06 00:33:10,932 Epoch 982: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.50 
2024-02-06 00:33:10,933 EPOCH 983
2024-02-06 00:33:12,459 [Epoch: 983 Step: 00033400] Batch Recognition Loss:   0.000348 => Gls Tokens per Sec:     2520 || Batch Translation Loss:   0.030609 => Txt Tokens per Sec:     6885 || Lr: 0.000100
2024-02-06 00:33:15,465 Epoch 983: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.03 
2024-02-06 00:33:15,465 EPOCH 984
2024-02-06 00:33:20,039 Epoch 984: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.41 
2024-02-06 00:33:20,039 EPOCH 985
2024-02-06 00:33:24,701 Epoch 985: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.32 
2024-02-06 00:33:24,702 EPOCH 986
2024-02-06 00:33:25,649 [Epoch: 986 Step: 00033500] Batch Recognition Loss:   0.000336 => Gls Tokens per Sec:     3105 || Batch Translation Loss:   0.210835 => Txt Tokens per Sec:     8087 || Lr: 0.000100
2024-02-06 00:33:29,183 Epoch 986: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.23 
2024-02-06 00:33:29,184 EPOCH 987
2024-02-06 00:33:33,947 Epoch 987: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.29 
2024-02-06 00:33:33,947 EPOCH 988
2024-02-06 00:33:38,209 Epoch 988: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.85 
2024-02-06 00:33:38,210 EPOCH 989
2024-02-06 00:33:39,487 [Epoch: 989 Step: 00033600] Batch Recognition Loss:   0.000919 => Gls Tokens per Sec:     2006 || Batch Translation Loss:   0.054283 => Txt Tokens per Sec:     5120 || Lr: 0.000100
2024-02-06 00:33:43,186 Epoch 989: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.30 
2024-02-06 00:33:43,186 EPOCH 990
2024-02-06 00:33:47,334 Epoch 990: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.55 
2024-02-06 00:33:47,334 EPOCH 991
2024-02-06 00:33:52,242 Epoch 991: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.58 
2024-02-06 00:33:52,242 EPOCH 992
2024-02-06 00:33:53,186 [Epoch: 992 Step: 00033700] Batch Recognition Loss:   0.001395 => Gls Tokens per Sec:     2036 || Batch Translation Loss:   0.089735 => Txt Tokens per Sec:     6064 || Lr: 0.000100
2024-02-06 00:33:56,588 Epoch 992: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.64 
2024-02-06 00:33:56,588 EPOCH 993
2024-02-06 00:34:01,402 Epoch 993: Total Training Recognition Loss 0.18  Total Training Translation Loss 2.27 
2024-02-06 00:34:01,403 EPOCH 994
2024-02-06 00:34:05,846 Epoch 994: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.45 
2024-02-06 00:34:05,846 EPOCH 995
2024-02-06 00:34:06,279 [Epoch: 995 Step: 00033800] Batch Recognition Loss:   0.000342 => Gls Tokens per Sec:     2963 || Batch Translation Loss:   0.015166 => Txt Tokens per Sec:     8273 || Lr: 0.000100
2024-02-06 00:34:10,614 Epoch 995: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.17 
2024-02-06 00:34:10,615 EPOCH 996
2024-02-06 00:34:15,228 Epoch 996: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.93 
2024-02-06 00:34:15,229 EPOCH 997
2024-02-06 00:34:20,225 Epoch 997: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.89 
2024-02-06 00:34:20,226 EPOCH 998
2024-02-06 00:34:20,511 [Epoch: 998 Step: 00033900] Batch Recognition Loss:   0.000665 => Gls Tokens per Sec:     2267 || Batch Translation Loss:   0.053712 => Txt Tokens per Sec:     7024 || Lr: 0.000100
2024-02-06 00:34:25,077 Epoch 998: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.69 
2024-02-06 00:34:25,077 EPOCH 999
2024-02-06 00:34:29,744 Epoch 999: Total Training Recognition Loss 0.05  Total Training Translation Loss 8.89 
2024-02-06 00:34:29,744 EPOCH 1000
2024-02-06 00:34:34,271 [Epoch: 1000 Step: 00034000] Batch Recognition Loss:   0.000927 => Gls Tokens per Sec:     2346 || Batch Translation Loss:   0.073218 => Txt Tokens per Sec:     6492 || Lr: 0.000100
2024-02-06 00:34:43,184 Validation result at epoch 1000, step    34000: duration: 8.9116s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.33577	Translation Loss: 90786.55469	PPL: 8671.29785
	Eval Metric: BLEU
	WER 3.60	(DEL: 0.00,	INS: 0.00,	SUB: 3.60)
	BLEU-4 0.31	(BLEU-1: 10.00,	BLEU-2: 2.95,	BLEU-3: 0.87,	BLEU-4: 0.31)
	CHRF 17.00	ROUGE 8.23
2024-02-06 00:34:43,186 Logging Recognition and Translation Outputs
2024-02-06 00:34:43,186 ========================================================================================================================
2024-02-06 00:34:43,186 Logging Sequence: 171_158.00
2024-02-06 00:34:43,187 	Gloss Reference :	A B+C+D+E
2024-02-06 00:34:43,187 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:34:43,187 	Gloss Alignment :	         
2024-02-06 00:34:43,187 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:34:43,189 	Text Reference  :	with speculations of dhoni being banned        are     spreading many    say that it       is unlikely to happen   
2024-02-06 00:34:43,189 	Text Hypothesis :	**** this         is why   the   international cricket team      members of  the  comments be allowed  to different
2024-02-06 00:34:43,189 	Text Alignment  :	D    S            S  S     S     S             S       S         S       S   S    S        S  S           S        
2024-02-06 00:34:43,189 ========================================================================================================================
2024-02-06 00:34:43,189 Logging Sequence: 108_235.00
2024-02-06 00:34:43,189 	Gloss Reference :	A B+C+D+E
2024-02-06 00:34:43,190 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:34:43,190 	Gloss Alignment :	         
2024-02-06 00:34:43,190 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:34:43,191 	Text Reference  :	he was taken to the hospital and   it  was     reported that he is  not  in    any danger
2024-02-06 00:34:43,191 	Text Hypothesis :	** *** ***** ** *** ******** those who violate saying   that ** the rule would be  jailed
2024-02-06 00:34:43,191 	Text Alignment  :	D  D   D     D  D   D        S     S   S       S             D  S   S    S     S   S     
2024-02-06 00:34:43,191 ========================================================================================================================
2024-02-06 00:34:43,192 Logging Sequence: 153_206.00
2024-02-06 00:34:43,192 	Gloss Reference :	A B+C+D+E
2024-02-06 00:34:43,192 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:34:43,192 	Gloss Alignment :	         
2024-02-06 00:34:43,192 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:34:43,193 	Text Reference  :	*** ****** **** ******** ** ** **** ** *** now  on  13th november everyone is    hoping  pakistan rewrites history
2024-02-06 00:34:43,193 	Text Hypothesis :	the couple were supposed to be held in the 2020 but they lost     the      match between the      same     time   
2024-02-06 00:34:43,194 	Text Alignment  :	I   I      I    I        I  I  I    I  I   S    S   S    S        S        S     S       S        S        S      
2024-02-06 00:34:43,194 ========================================================================================================================
2024-02-06 00:34:43,194 Logging Sequence: 87_202.00
2024-02-06 00:34:43,194 	Gloss Reference :	A B+C+D+E
2024-02-06 00:34:43,194 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:34:43,194 	Gloss Alignment :	         
2024-02-06 00:34:43,195 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:34:43,195 	Text Reference  :	i ** love our players and     i  love  my country
2024-02-06 00:34:43,195 	Text Hypothesis :	i am sure you all     because of dhoni or kohli  
2024-02-06 00:34:43,195 	Text Alignment  :	  I  S    S   S       S       S  S     S  S      
2024-02-06 00:34:43,195 ========================================================================================================================
2024-02-06 00:34:43,196 Logging Sequence: 84_2.00
2024-02-06 00:34:43,196 	Gloss Reference :	A B+C+D+E
2024-02-06 00:34:43,196 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:34:43,196 	Gloss Alignment :	         
2024-02-06 00:34:43,196 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:34:43,198 	Text Reference  :	*** ** the  2022 fifa  football world cup ****** is   going on in qatar from 20th november 2022 to     18th  december 2022 
2024-02-06 00:34:43,198 	Text Hypothesis :	let me tell you  about the      world cup trophy then goes  on ** ***** **** **** ******** **** social media that     india
2024-02-06 00:34:43,198 	Text Alignment  :	I   I  S    S    S     S                  I      S    S        D  D     D    D    D        D    S      S     S        S    
2024-02-06 00:34:43,198 ========================================================================================================================
2024-02-06 00:34:43,202 Epoch 1000: Total Training Recognition Loss 0.07  Total Training Translation Loss 6.30 
2024-02-06 00:34:43,202 EPOCH 1001
2024-02-06 00:34:48,143 Epoch 1001: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.76 
2024-02-06 00:34:48,143 EPOCH 1002
2024-02-06 00:34:52,227 Epoch 1002: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.04 
2024-02-06 00:34:52,227 EPOCH 1003
2024-02-06 00:34:56,915 [Epoch: 1003 Step: 00034100] Batch Recognition Loss:   0.001306 => Gls Tokens per Sec:     2129 || Batch Translation Loss:   0.121296 => Txt Tokens per Sec:     5871 || Lr: 0.000100
2024-02-06 00:34:57,166 Epoch 1003: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.13 
2024-02-06 00:34:57,166 EPOCH 1004
2024-02-06 00:35:01,406 Epoch 1004: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.11 
2024-02-06 00:35:01,406 EPOCH 1005
2024-02-06 00:35:06,261 Epoch 1005: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.61 
2024-02-06 00:35:06,261 EPOCH 1006
2024-02-06 00:35:09,842 [Epoch: 1006 Step: 00034200] Batch Recognition Loss:   0.001303 => Gls Tokens per Sec:     2609 || Batch Translation Loss:   0.036747 => Txt Tokens per Sec:     7114 || Lr: 0.000100
2024-02-06 00:35:10,607 Epoch 1006: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.15 
2024-02-06 00:35:10,607 EPOCH 1007
2024-02-06 00:35:15,318 Epoch 1007: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.13 
2024-02-06 00:35:15,319 EPOCH 1008
2024-02-06 00:35:19,920 Epoch 1008: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.14 
2024-02-06 00:35:19,920 EPOCH 1009
2024-02-06 00:35:23,748 [Epoch: 1009 Step: 00034300] Batch Recognition Loss:   0.001475 => Gls Tokens per Sec:     2342 || Batch Translation Loss:   0.013075 => Txt Tokens per Sec:     6576 || Lr: 0.000100
2024-02-06 00:35:24,496 Epoch 1009: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-06 00:35:24,496 EPOCH 1010
2024-02-06 00:35:29,189 Epoch 1010: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.60 
2024-02-06 00:35:29,190 EPOCH 1011
2024-02-06 00:35:34,192 Epoch 1011: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-06 00:35:34,193 EPOCH 1012
2024-02-06 00:35:37,713 [Epoch: 1012 Step: 00034400] Batch Recognition Loss:   0.000388 => Gls Tokens per Sec:     2365 || Batch Translation Loss:   0.019528 => Txt Tokens per Sec:     6440 || Lr: 0.000100
2024-02-06 00:35:38,742 Epoch 1012: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.52 
2024-02-06 00:35:38,742 EPOCH 1013
2024-02-06 00:35:43,117 Epoch 1013: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.49 
2024-02-06 00:35:43,118 EPOCH 1014
2024-02-06 00:35:48,035 Epoch 1014: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.44 
2024-02-06 00:35:48,035 EPOCH 1015
2024-02-06 00:35:50,959 [Epoch: 1015 Step: 00034500] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     2628 || Batch Translation Loss:   0.019393 => Txt Tokens per Sec:     7207 || Lr: 0.000100
2024-02-06 00:35:52,138 Epoch 1015: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.49 
2024-02-06 00:35:52,138 EPOCH 1016
2024-02-06 00:35:56,612 Epoch 1016: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.52 
2024-02-06 00:35:56,613 EPOCH 1017
2024-02-06 00:36:01,416 Epoch 1017: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.55 
2024-02-06 00:36:01,417 EPOCH 1018
2024-02-06 00:36:03,990 [Epoch: 1018 Step: 00034600] Batch Recognition Loss:   0.000535 => Gls Tokens per Sec:     2635 || Batch Translation Loss:   0.015270 => Txt Tokens per Sec:     7007 || Lr: 0.000100
2024-02-06 00:36:05,746 Epoch 1018: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.47 
2024-02-06 00:36:05,747 EPOCH 1019
2024-02-06 00:36:10,649 Epoch 1019: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.48 
2024-02-06 00:36:10,649 EPOCH 1020
2024-02-06 00:36:15,158 Epoch 1020: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-06 00:36:15,159 EPOCH 1021
2024-02-06 00:36:17,993 [Epoch: 1021 Step: 00034700] Batch Recognition Loss:   0.012543 => Gls Tokens per Sec:     2259 || Batch Translation Loss:   0.017677 => Txt Tokens per Sec:     6093 || Lr: 0.000100
2024-02-06 00:36:20,020 Epoch 1021: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.45 
2024-02-06 00:36:20,020 EPOCH 1022
2024-02-06 00:36:24,121 Epoch 1022: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 00:36:24,122 EPOCH 1023
2024-02-06 00:36:29,009 Epoch 1023: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 00:36:29,010 EPOCH 1024
2024-02-06 00:36:31,824 [Epoch: 1024 Step: 00034800] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     2047 || Batch Translation Loss:   0.010136 => Txt Tokens per Sec:     6003 || Lr: 0.000100
2024-02-06 00:36:33,401 Epoch 1024: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-06 00:36:33,401 EPOCH 1025
2024-02-06 00:36:37,941 Epoch 1025: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.87 
2024-02-06 00:36:37,942 EPOCH 1026
2024-02-06 00:36:42,676 Epoch 1026: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.20 
2024-02-06 00:36:42,676 EPOCH 1027
2024-02-06 00:36:44,343 [Epoch: 1027 Step: 00034900] Batch Recognition Loss:   0.000759 => Gls Tokens per Sec:     3074 || Batch Translation Loss:   0.027349 => Txt Tokens per Sec:     8071 || Lr: 0.000100
2024-02-06 00:36:47,106 Epoch 1027: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.70 
2024-02-06 00:36:47,107 EPOCH 1028
2024-02-06 00:36:51,925 Epoch 1028: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.49 
2024-02-06 00:36:51,925 EPOCH 1029
2024-02-06 00:36:56,132 Epoch 1029: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.38 
2024-02-06 00:36:56,133 EPOCH 1030
2024-02-06 00:36:58,469 [Epoch: 1030 Step: 00035000] Batch Recognition Loss:   0.000963 => Gls Tokens per Sec:     1918 || Batch Translation Loss:   0.028873 => Txt Tokens per Sec:     5573 || Lr: 0.000100
2024-02-06 00:37:01,126 Epoch 1030: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.08 
2024-02-06 00:37:01,126 EPOCH 1031
2024-02-06 00:37:05,327 Epoch 1031: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.07 
2024-02-06 00:37:05,327 EPOCH 1032
2024-02-06 00:37:10,249 Epoch 1032: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.26 
2024-02-06 00:37:10,250 EPOCH 1033
2024-02-06 00:37:11,765 [Epoch: 1033 Step: 00035100] Batch Recognition Loss:   0.000281 => Gls Tokens per Sec:     2366 || Batch Translation Loss:   0.036483 => Txt Tokens per Sec:     6795 || Lr: 0.000100
2024-02-06 00:37:14,500 Epoch 1033: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.01 
2024-02-06 00:37:14,500 EPOCH 1034
2024-02-06 00:37:19,364 Epoch 1034: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.59 
2024-02-06 00:37:19,364 EPOCH 1035
2024-02-06 00:37:23,771 Epoch 1035: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.68 
2024-02-06 00:37:23,772 EPOCH 1036
2024-02-06 00:37:24,856 [Epoch: 1036 Step: 00035200] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:     2954 || Batch Translation Loss:   0.024638 => Txt Tokens per Sec:     7914 || Lr: 0.000100
2024-02-06 00:37:28,489 Epoch 1036: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.57 
2024-02-06 00:37:28,490 EPOCH 1037
2024-02-06 00:37:33,064 Epoch 1037: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.40 
2024-02-06 00:37:33,064 EPOCH 1038
2024-02-06 00:37:37,603 Epoch 1038: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.61 
2024-02-06 00:37:37,604 EPOCH 1039
2024-02-06 00:37:38,556 [Epoch: 1039 Step: 00035300] Batch Recognition Loss:   0.000870 => Gls Tokens per Sec:     2692 || Batch Translation Loss:   0.056469 => Txt Tokens per Sec:     7498 || Lr: 0.000100
2024-02-06 00:37:42,269 Epoch 1039: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.96 
2024-02-06 00:37:42,269 EPOCH 1040
2024-02-06 00:37:46,776 Epoch 1040: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.20 
2024-02-06 00:37:46,777 EPOCH 1041
2024-02-06 00:37:51,540 Epoch 1041: Total Training Recognition Loss 0.08  Total Training Translation Loss 5.39 
2024-02-06 00:37:51,540 EPOCH 1042
2024-02-06 00:37:52,247 [Epoch: 1042 Step: 00035400] Batch Recognition Loss:   0.005054 => Gls Tokens per Sec:     2720 || Batch Translation Loss:   0.145093 => Txt Tokens per Sec:     7761 || Lr: 0.000100
2024-02-06 00:37:55,734 Epoch 1042: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.65 
2024-02-06 00:37:55,735 EPOCH 1043
2024-02-06 00:38:00,687 Epoch 1043: Total Training Recognition Loss 0.10  Total Training Translation Loss 8.54 
2024-02-06 00:38:00,687 EPOCH 1044
2024-02-06 00:38:04,834 Epoch 1044: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.55 
2024-02-06 00:38:04,835 EPOCH 1045
2024-02-06 00:38:05,294 [Epoch: 1045 Step: 00035500] Batch Recognition Loss:   0.000788 => Gls Tokens per Sec:     2798 || Batch Translation Loss:   0.032298 => Txt Tokens per Sec:     7840 || Lr: 0.000100
2024-02-06 00:38:09,806 Epoch 1045: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.29 
2024-02-06 00:38:09,807 EPOCH 1046
2024-02-06 00:38:14,165 Epoch 1046: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.85 
2024-02-06 00:38:14,165 EPOCH 1047
2024-02-06 00:38:19,030 Epoch 1047: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.78 
2024-02-06 00:38:19,030 EPOCH 1048
2024-02-06 00:38:19,595 [Epoch: 1048 Step: 00035600] Batch Recognition Loss:   0.001868 => Gls Tokens per Sec:     1135 || Batch Translation Loss:   0.021286 => Txt Tokens per Sec:     3940 || Lr: 0.000100
2024-02-06 00:38:23,434 Epoch 1048: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.73 
2024-02-06 00:38:23,434 EPOCH 1049
2024-02-06 00:38:28,263 Epoch 1049: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.79 
2024-02-06 00:38:28,264 EPOCH 1050
2024-02-06 00:38:32,773 [Epoch: 1050 Step: 00035700] Batch Recognition Loss:   0.000554 => Gls Tokens per Sec:     2356 || Batch Translation Loss:   0.018198 => Txt Tokens per Sec:     6518 || Lr: 0.000100
2024-02-06 00:38:32,773 Epoch 1050: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.80 
2024-02-06 00:38:32,774 EPOCH 1051
2024-02-06 00:38:37,416 Epoch 1051: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.94 
2024-02-06 00:38:37,416 EPOCH 1052
2024-02-06 00:38:42,049 Epoch 1052: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.89 
2024-02-06 00:38:42,049 EPOCH 1053
2024-02-06 00:38:46,160 [Epoch: 1053 Step: 00035800] Batch Recognition Loss:   0.003487 => Gls Tokens per Sec:     2428 || Batch Translation Loss:   0.018841 => Txt Tokens per Sec:     6663 || Lr: 0.000100
2024-02-06 00:38:46,573 Epoch 1053: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.68 
2024-02-06 00:38:46,574 EPOCH 1054
2024-02-06 00:38:51,336 Epoch 1054: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.71 
2024-02-06 00:38:51,336 EPOCH 1055
2024-02-06 00:38:55,665 Epoch 1055: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.87 
2024-02-06 00:38:55,666 EPOCH 1056
2024-02-06 00:38:59,581 [Epoch: 1056 Step: 00035900] Batch Recognition Loss:   0.000310 => Gls Tokens per Sec:     2386 || Batch Translation Loss:   0.031904 => Txt Tokens per Sec:     6438 || Lr: 0.000100
2024-02-06 00:39:00,583 Epoch 1056: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.32 
2024-02-06 00:39:00,584 EPOCH 1057
2024-02-06 00:39:04,774 Epoch 1057: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.71 
2024-02-06 00:39:04,775 EPOCH 1058
2024-02-06 00:39:09,757 Epoch 1058: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.08 
2024-02-06 00:39:09,758 EPOCH 1059
2024-02-06 00:39:13,105 [Epoch: 1059 Step: 00036000] Batch Recognition Loss:   0.000946 => Gls Tokens per Sec:     2677 || Batch Translation Loss:   0.057825 => Txt Tokens per Sec:     7242 || Lr: 0.000100
2024-02-06 00:39:21,939 Validation result at epoch 1059, step    36000: duration: 8.8347s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.40635	Translation Loss: 94208.26562	PPL: 12204.18750
	Eval Metric: BLEU
	WER 3.53	(DEL: 0.00,	INS: 0.00,	SUB: 3.53)
	BLEU-4 0.72	(BLEU-1: 11.03,	BLEU-2: 3.42,	BLEU-3: 1.39,	BLEU-4: 0.72)
	CHRF 17.19	ROUGE 9.31
2024-02-06 00:39:21,941 Logging Recognition and Translation Outputs
2024-02-06 00:39:21,941 ========================================================================================================================
2024-02-06 00:39:21,941 Logging Sequence: 153_36.00
2024-02-06 00:39:21,941 	Gloss Reference :	A B+C+D+E
2024-02-06 00:39:21,941 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:39:21,941 	Gloss Alignment :	         
2024-02-06 00:39:21,941 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:39:21,942 	Text Reference  :	india made a  good score   of  1686 in **** 20    overs
2024-02-06 00:39:21,943 	Text Hypothesis :	***** **** at the  stadium was held in 1992 world cup  
2024-02-06 00:39:21,943 	Text Alignment  :	D     D    S  S    S       S   S       I    S     S    
2024-02-06 00:39:21,943 ========================================================================================================================
2024-02-06 00:39:21,943 Logging Sequence: 163_30.00
2024-02-06 00:39:21,943 	Gloss Reference :	A B+C+D+E
2024-02-06 00:39:21,943 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:39:21,943 	Gloss Alignment :	         
2024-02-06 00:39:21,944 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:39:21,944 	Text Reference  :	they ***** *** ******* **** ****** never permitted anyone to  reveal her    face    
2024-02-06 00:39:21,945 	Text Hypothesis :	they could not believe that sushil kumar who       is     the most   famous wrestler
2024-02-06 00:39:21,945 	Text Alignment  :	     I     I   I       I    I      S     S         S      S   S      S      S       
2024-02-06 00:39:21,945 ========================================================================================================================
2024-02-06 00:39:21,945 Logging Sequence: 167_60.00
2024-02-06 00:39:21,945 	Gloss Reference :	A B+C+D+E
2024-02-06 00:39:21,946 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:39:21,946 	Gloss Alignment :	         
2024-02-06 00:39:21,946 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:39:21,947 	Text Reference  :	camel flu spreads rapidly when one  comes in      close contact with      the infected
2024-02-06 00:39:21,947 	Text Hypothesis :	***** *** the     stadium was  held on    twitter and   prevent infection hiv etc     
2024-02-06 00:39:21,947 	Text Alignment  :	D     D   S       S       S    S    S     S       S     S       S         S   S       
2024-02-06 00:39:21,947 ========================================================================================================================
2024-02-06 00:39:21,948 Logging Sequence: 84_35.00
2024-02-06 00:39:21,948 	Gloss Reference :	A B+C+D+E
2024-02-06 00:39:21,948 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:39:21,948 	Gloss Alignment :	         
2024-02-06 00:39:21,948 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:39:21,949 	Text Reference  :	*** **** *** here is     the reason why  they covered their mouth  
2024-02-06 00:39:21,949 	Text Hypothesis :	and they had to   second we  will   have to   wait    for   updates
2024-02-06 00:39:21,949 	Text Alignment  :	I   I    I   S    S      S   S      S    S    S       S     S      
2024-02-06 00:39:21,949 ========================================================================================================================
2024-02-06 00:39:21,950 Logging Sequence: 96_2.00
2024-02-06 00:39:21,950 	Gloss Reference :	A B+C+D+E
2024-02-06 00:39:21,950 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:39:21,950 	Gloss Alignment :	         
2024-02-06 00:39:21,950 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:39:21,952 	Text Reference  :	the world is preparing for the      t20     world cup scheduled to start from 16th  october this year
2024-02-06 00:39:21,952 	Text Hypothesis :	the ***** ** ********* icc under-19 cricket world cup ********* ** ***** was  first played  in   1988
2024-02-06 00:39:21,952 	Text Alignment  :	    D     D  D         S   S        S                 D         D  D     S    S     S       S    S   
2024-02-06 00:39:21,952 ========================================================================================================================
2024-02-06 00:39:23,015 Epoch 1059: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.55 
2024-02-06 00:39:23,015 EPOCH 1060
2024-02-06 00:39:28,303 Epoch 1060: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.38 
2024-02-06 00:39:28,304 EPOCH 1061
2024-02-06 00:39:32,853 Epoch 1061: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.73 
2024-02-06 00:39:32,853 EPOCH 1062
2024-02-06 00:39:36,395 [Epoch: 1062 Step: 00036100] Batch Recognition Loss:   0.000982 => Gls Tokens per Sec:     2276 || Batch Translation Loss:   0.006786 => Txt Tokens per Sec:     6300 || Lr: 0.000100
2024-02-06 00:39:37,595 Epoch 1062: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.24 
2024-02-06 00:39:37,595 EPOCH 1063
2024-02-06 00:39:42,148 Epoch 1063: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.48 
2024-02-06 00:39:42,149 EPOCH 1064
2024-02-06 00:39:46,633 Epoch 1064: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.77 
2024-02-06 00:39:46,633 EPOCH 1065
2024-02-06 00:39:50,280 [Epoch: 1065 Step: 00036200] Batch Recognition Loss:   0.000374 => Gls Tokens per Sec:     2036 || Batch Translation Loss:   0.161353 => Txt Tokens per Sec:     5734 || Lr: 0.000100
2024-02-06 00:39:51,405 Epoch 1065: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.80 
2024-02-06 00:39:51,406 EPOCH 1066
2024-02-06 00:39:55,483 Epoch 1066: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.18 
2024-02-06 00:39:55,483 EPOCH 1067
2024-02-06 00:40:00,526 Epoch 1067: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.15 
2024-02-06 00:40:00,526 EPOCH 1068
2024-02-06 00:40:03,293 [Epoch: 1068 Step: 00036300] Batch Recognition Loss:   0.000558 => Gls Tokens per Sec:     2546 || Batch Translation Loss:   0.105064 => Txt Tokens per Sec:     7123 || Lr: 0.000100
2024-02-06 00:40:04,829 Epoch 1068: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.72 
2024-02-06 00:40:04,830 EPOCH 1069
2024-02-06 00:40:09,856 Epoch 1069: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.26 
2024-02-06 00:40:09,856 EPOCH 1070
2024-02-06 00:40:14,213 Epoch 1070: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.40 
2024-02-06 00:40:14,214 EPOCH 1071
2024-02-06 00:40:17,261 [Epoch: 1071 Step: 00036400] Batch Recognition Loss:   0.000445 => Gls Tokens per Sec:     2016 || Batch Translation Loss:   0.036436 => Txt Tokens per Sec:     5688 || Lr: 0.000100
2024-02-06 00:40:19,052 Epoch 1071: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.58 
2024-02-06 00:40:19,052 EPOCH 1072
2024-02-06 00:40:23,477 Epoch 1072: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.15 
2024-02-06 00:40:23,478 EPOCH 1073
2024-02-06 00:40:28,163 Epoch 1073: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.78 
2024-02-06 00:40:28,164 EPOCH 1074
2024-02-06 00:40:30,376 [Epoch: 1074 Step: 00036500] Batch Recognition Loss:   0.001697 => Gls Tokens per Sec:     2606 || Batch Translation Loss:   0.021468 => Txt Tokens per Sec:     6976 || Lr: 0.000100
2024-02-06 00:40:32,769 Epoch 1074: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.84 
2024-02-06 00:40:32,770 EPOCH 1075
2024-02-06 00:40:37,061 Epoch 1075: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.80 
2024-02-06 00:40:37,062 EPOCH 1076
2024-02-06 00:40:41,978 Epoch 1076: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-06 00:40:41,979 EPOCH 1077
2024-02-06 00:40:43,813 [Epoch: 1077 Step: 00036600] Batch Recognition Loss:   0.000175 => Gls Tokens per Sec:     2795 || Batch Translation Loss:   0.029475 => Txt Tokens per Sec:     7791 || Lr: 0.000100
2024-02-06 00:40:46,074 Epoch 1077: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-06 00:40:46,075 EPOCH 1078
2024-02-06 00:40:51,078 Epoch 1078: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-06 00:40:51,078 EPOCH 1079
2024-02-06 00:40:55,360 Epoch 1079: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 00:40:55,361 EPOCH 1080
2024-02-06 00:40:56,950 [Epoch: 1080 Step: 00036700] Batch Recognition Loss:   0.001716 => Gls Tokens per Sec:     2821 || Batch Translation Loss:   0.005624 => Txt Tokens per Sec:     7456 || Lr: 0.000100
2024-02-06 00:41:00,262 Epoch 1080: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-06 00:41:00,263 EPOCH 1081
2024-02-06 00:41:04,599 Epoch 1081: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.13 
2024-02-06 00:41:04,599 EPOCH 1082
2024-02-06 00:41:09,424 Epoch 1082: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.41 
2024-02-06 00:41:09,425 EPOCH 1083
2024-02-06 00:41:11,066 [Epoch: 1083 Step: 00036800] Batch Recognition Loss:   0.000649 => Gls Tokens per Sec:     2183 || Batch Translation Loss:   0.027352 => Txt Tokens per Sec:     6300 || Lr: 0.000100
2024-02-06 00:41:13,895 Epoch 1083: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.96 
2024-02-06 00:41:13,896 EPOCH 1084
2024-02-06 00:41:18,587 Epoch 1084: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.07 
2024-02-06 00:41:18,588 EPOCH 1085
2024-02-06 00:41:23,241 Epoch 1085: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.92 
2024-02-06 00:41:23,241 EPOCH 1086
2024-02-06 00:41:24,304 [Epoch: 1086 Step: 00036900] Batch Recognition Loss:   0.000364 => Gls Tokens per Sec:     3012 || Batch Translation Loss:   0.024508 => Txt Tokens per Sec:     7753 || Lr: 0.000100
2024-02-06 00:41:27,834 Epoch 1086: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.36 
2024-02-06 00:41:27,834 EPOCH 1087
2024-02-06 00:41:32,567 Epoch 1087: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.21 
2024-02-06 00:41:32,567 EPOCH 1088
2024-02-06 00:41:37,027 Epoch 1088: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.06 
2024-02-06 00:41:37,028 EPOCH 1089
2024-02-06 00:41:38,123 [Epoch: 1089 Step: 00037000] Batch Recognition Loss:   0.000319 => Gls Tokens per Sec:     2103 || Batch Translation Loss:   0.005906 => Txt Tokens per Sec:     5987 || Lr: 0.000100
2024-02-06 00:41:41,841 Epoch 1089: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.71 
2024-02-06 00:41:41,841 EPOCH 1090
2024-02-06 00:41:46,020 Epoch 1090: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.93 
2024-02-06 00:41:46,021 EPOCH 1091
2024-02-06 00:41:50,987 Epoch 1091: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.11 
2024-02-06 00:41:50,987 EPOCH 1092
2024-02-06 00:41:51,795 [Epoch: 1092 Step: 00037100] Batch Recognition Loss:   0.000344 => Gls Tokens per Sec:     2379 || Batch Translation Loss:   0.076983 => Txt Tokens per Sec:     6867 || Lr: 0.000100
2024-02-06 00:41:55,149 Epoch 1092: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.55 
2024-02-06 00:41:55,150 EPOCH 1093
2024-02-06 00:42:00,102 Epoch 1093: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.41 
2024-02-06 00:42:00,102 EPOCH 1094
2024-02-06 00:42:04,462 Epoch 1094: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.24 
2024-02-06 00:42:04,462 EPOCH 1095
2024-02-06 00:42:05,018 [Epoch: 1095 Step: 00037200] Batch Recognition Loss:   0.002380 => Gls Tokens per Sec:     2303 || Batch Translation Loss:   0.058528 => Txt Tokens per Sec:     6845 || Lr: 0.000100
2024-02-06 00:42:09,295 Epoch 1095: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.50 
2024-02-06 00:42:09,296 EPOCH 1096
2024-02-06 00:42:13,747 Epoch 1096: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.72 
2024-02-06 00:42:13,747 EPOCH 1097
2024-02-06 00:42:18,586 Epoch 1097: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.68 
2024-02-06 00:42:18,587 EPOCH 1098
2024-02-06 00:42:18,848 [Epoch: 1098 Step: 00037300] Batch Recognition Loss:   0.003260 => Gls Tokens per Sec:     2452 || Batch Translation Loss:   0.021843 => Txt Tokens per Sec:     7080 || Lr: 0.000100
2024-02-06 00:42:23,016 Epoch 1098: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.37 
2024-02-06 00:42:23,016 EPOCH 1099
2024-02-06 00:42:27,610 Epoch 1099: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.17 
2024-02-06 00:42:27,611 EPOCH 1100
2024-02-06 00:42:32,262 [Epoch: 1100 Step: 00037400] Batch Recognition Loss:   0.004731 => Gls Tokens per Sec:     2284 || Batch Translation Loss:   0.012975 => Txt Tokens per Sec:     6319 || Lr: 0.000100
2024-02-06 00:42:32,263 Epoch 1100: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.71 
2024-02-06 00:42:32,263 EPOCH 1101
2024-02-06 00:42:36,402 Epoch 1101: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.69 
2024-02-06 00:42:36,403 EPOCH 1102
2024-02-06 00:42:41,374 Epoch 1102: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.69 
2024-02-06 00:42:41,375 EPOCH 1103
2024-02-06 00:42:45,348 [Epoch: 1103 Step: 00037500] Batch Recognition Loss:   0.001375 => Gls Tokens per Sec:     2514 || Batch Translation Loss:   0.021618 => Txt Tokens per Sec:     6932 || Lr: 0.000100
2024-02-06 00:42:45,583 Epoch 1103: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-06 00:42:45,583 EPOCH 1104
2024-02-06 00:42:50,482 Epoch 1104: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.82 
2024-02-06 00:42:50,483 EPOCH 1105
2024-02-06 00:42:54,873 Epoch 1105: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.82 
2024-02-06 00:42:54,873 EPOCH 1106
2024-02-06 00:42:59,077 [Epoch: 1106 Step: 00037600] Batch Recognition Loss:   0.000446 => Gls Tokens per Sec:     2223 || Batch Translation Loss:   0.019604 => Txt Tokens per Sec:     6124 || Lr: 0.000100
2024-02-06 00:42:59,651 Epoch 1106: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.65 
2024-02-06 00:42:59,651 EPOCH 1107
2024-02-06 00:43:04,057 Epoch 1107: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.98 
2024-02-06 00:43:04,058 EPOCH 1108
2024-02-06 00:43:08,821 Epoch 1108: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.79 
2024-02-06 00:43:08,821 EPOCH 1109
2024-02-06 00:43:12,654 [Epoch: 1109 Step: 00037700] Batch Recognition Loss:   0.001050 => Gls Tokens per Sec:     2339 || Batch Translation Loss:   0.032610 => Txt Tokens per Sec:     6495 || Lr: 0.000100
2024-02-06 00:43:13,367 Epoch 1109: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-06 00:43:13,367 EPOCH 1110
2024-02-06 00:43:17,917 Epoch 1110: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.66 
2024-02-06 00:43:17,918 EPOCH 1111
2024-02-06 00:43:22,605 Epoch 1111: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.74 
2024-02-06 00:43:22,606 EPOCH 1112
2024-02-06 00:43:26,768 [Epoch: 1112 Step: 00037800] Batch Recognition Loss:   0.000256 => Gls Tokens per Sec:     1938 || Batch Translation Loss:   0.011829 => Txt Tokens per Sec:     5528 || Lr: 0.000100
2024-02-06 00:43:27,639 Epoch 1112: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.03 
2024-02-06 00:43:27,639 EPOCH 1113
2024-02-06 00:43:32,385 Epoch 1113: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.94 
2024-02-06 00:43:32,386 EPOCH 1114
2024-02-06 00:43:37,193 Epoch 1114: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.90 
2024-02-06 00:43:37,193 EPOCH 1115
2024-02-06 00:43:39,892 [Epoch: 1115 Step: 00037900] Batch Recognition Loss:   0.000240 => Gls Tokens per Sec:     2848 || Batch Translation Loss:   0.015194 => Txt Tokens per Sec:     7874 || Lr: 0.000100
2024-02-06 00:43:41,638 Epoch 1115: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.73 
2024-02-06 00:43:41,638 EPOCH 1116
2024-02-06 00:43:46,429 Epoch 1116: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.04 
2024-02-06 00:43:46,429 EPOCH 1117
2024-02-06 00:43:51,038 Epoch 1117: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.05 
2024-02-06 00:43:51,038 EPOCH 1118
2024-02-06 00:43:54,193 [Epoch: 1118 Step: 00038000] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:     2233 || Batch Translation Loss:   0.028460 => Txt Tokens per Sec:     6136 || Lr: 0.000100
2024-02-06 00:44:03,453 Validation result at epoch 1118, step    38000: duration: 9.2601s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.56175	Translation Loss: 94598.20312	PPL: 12688.88281
	Eval Metric: BLEU
	WER 3.04	(DEL: 0.00,	INS: 0.00,	SUB: 3.04)
	BLEU-4 0.32	(BLEU-1: 9.21,	BLEU-2: 2.56,	BLEU-3: 0.91,	BLEU-4: 0.32)
	CHRF 16.31	ROUGE 8.09
2024-02-06 00:44:03,455 Logging Recognition and Translation Outputs
2024-02-06 00:44:03,455 ========================================================================================================================
2024-02-06 00:44:03,455 Logging Sequence: 59_152.00
2024-02-06 00:44:03,455 	Gloss Reference :	A B+C+D+E
2024-02-06 00:44:03,455 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:44:03,455 	Gloss Alignment :	         
2024-02-06 00:44:03,455 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:44:03,457 	Text Reference  :	the organisers encouraged athletes to use the condoms in *** their home countries
2024-02-06 00:44:03,457 	Text Hypothesis :	*** they       are        supposed to *** be  held    in uae as    the  players  
2024-02-06 00:44:03,457 	Text Alignment  :	D   S          S          S           D   S   S          I   S     S    S        
2024-02-06 00:44:03,457 ========================================================================================================================
2024-02-06 00:44:03,457 Logging Sequence: 155_78.00
2024-02-06 00:44:03,457 	Gloss Reference :	A B+C+D+E
2024-02-06 00:44:03,457 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:44:03,458 	Gloss Alignment :	         
2024-02-06 00:44:03,458 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:44:03,459 	Text Reference  :	it was difficult for icc to disqualify the afghan team at the last minute so they    included them as        per      the   schedule
2024-02-06 00:44:03,459 	Text Hypothesis :	** *** ********* *** *** ** ********** *** ****** **** ** the ban  will   be applied when     the  company's finances staff salaries
2024-02-06 00:44:03,459 	Text Alignment  :	D  D   D         D   D   D  D          D   D      D    D      S    S      S  S       S        S    S         S        S     S       
2024-02-06 00:44:03,459 ========================================================================================================================
2024-02-06 00:44:03,460 Logging Sequence: 102_147.00
2024-02-06 00:44:03,460 	Gloss Reference :	A B+C+D+E
2024-02-06 00:44:03,460 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:44:03,460 	Gloss Alignment :	         
2024-02-06 00:44:03,460 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:44:03,461 	Text Reference  :	despite the muscle cramps this young boy lifted such a huge weight and      made the country proud by  securing a gold   medal
2024-02-06 00:44:03,462 	Text Hypothesis :	******* *** ****** ****** **** ***** *** ****** **** * **** she    competed in   the ******* games and secured  a silver medal
2024-02-06 00:44:03,462 	Text Alignment  :	D       D   D      D      D    D     D   D      D    D D    S      S        S        D       S     S   S          S           
2024-02-06 00:44:03,462 ========================================================================================================================
2024-02-06 00:44:03,462 Logging Sequence: 105_2.00
2024-02-06 00:44:03,462 	Gloss Reference :	A B+C+D+E
2024-02-06 00:44:03,462 	Gloss Hypothesis:	A B+C+D  
2024-02-06 00:44:03,462 	Gloss Alignment :	  S      
2024-02-06 00:44:03,462 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:44:03,463 	Text Reference  :	** *** **** the  airthings masters tournament is    an     online chess tournament
2024-02-06 00:44:03,463 	Text Hypothesis :	do you know that india     had     won        their rights for    its   series    
2024-02-06 00:44:03,463 	Text Alignment  :	I  I   I    S    S         S       S          S     S      S      S     S         
2024-02-06 00:44:03,464 ========================================================================================================================
2024-02-06 00:44:03,464 Logging Sequence: 96_31.00
2024-02-06 00:44:03,464 	Gloss Reference :	A B+C+D+E
2024-02-06 00:44:03,464 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:44:03,464 	Gloss Alignment :	         
2024-02-06 00:44:03,464 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:44:03,465 	Text Reference  :	and then 2 teams will go   on   to play the   final
2024-02-06 00:44:03,465 	Text Hypothesis :	*** **** * ***** **** that time a  very happy that 
2024-02-06 00:44:03,465 	Text Alignment  :	D   D    D D     D    S    S    S  S    S     S    
2024-02-06 00:44:03,465 ========================================================================================================================
2024-02-06 00:44:05,298 Epoch 1118: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.62 
2024-02-06 00:44:05,298 EPOCH 1119
2024-02-06 00:44:10,226 Epoch 1119: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.51 
2024-02-06 00:44:10,226 EPOCH 1120
2024-02-06 00:44:14,321 Epoch 1120: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.88 
2024-02-06 00:44:14,321 EPOCH 1121
2024-02-06 00:44:17,324 [Epoch: 1121 Step: 00038100] Batch Recognition Loss:   0.000355 => Gls Tokens per Sec:     2046 || Batch Translation Loss:   0.184235 => Txt Tokens per Sec:     5589 || Lr: 0.000100
2024-02-06 00:44:19,382 Epoch 1121: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.09 
2024-02-06 00:44:19,382 EPOCH 1122
2024-02-06 00:44:23,631 Epoch 1122: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.63 
2024-02-06 00:44:23,631 EPOCH 1123
2024-02-06 00:44:28,539 Epoch 1123: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.02 
2024-02-06 00:44:28,540 EPOCH 1124
2024-02-06 00:44:30,632 [Epoch: 1124 Step: 00038200] Batch Recognition Loss:   0.000866 => Gls Tokens per Sec:     2630 || Batch Translation Loss:   0.038986 => Txt Tokens per Sec:     7186 || Lr: 0.000100
2024-02-06 00:44:32,941 Epoch 1124: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.67 
2024-02-06 00:44:32,941 EPOCH 1125
2024-02-06 00:44:37,619 Epoch 1125: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.97 
2024-02-06 00:44:37,620 EPOCH 1126
2024-02-06 00:44:42,114 Epoch 1126: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.55 
2024-02-06 00:44:42,114 EPOCH 1127
2024-02-06 00:44:43,956 [Epoch: 1127 Step: 00038300] Batch Recognition Loss:   0.000851 => Gls Tokens per Sec:     2640 || Batch Translation Loss:   0.029453 => Txt Tokens per Sec:     7120 || Lr: 0.000100
2024-02-06 00:44:46,737 Epoch 1127: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.08 
2024-02-06 00:44:46,738 EPOCH 1128
2024-02-06 00:44:51,424 Epoch 1128: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.92 
2024-02-06 00:44:51,425 EPOCH 1129
2024-02-06 00:44:56,421 Epoch 1129: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.93 
2024-02-06 00:44:56,421 EPOCH 1130
2024-02-06 00:44:58,193 [Epoch: 1130 Step: 00038400] Batch Recognition Loss:   0.000239 => Gls Tokens per Sec:     2383 || Batch Translation Loss:   0.013383 => Txt Tokens per Sec:     6588 || Lr: 0.000100
2024-02-06 00:45:00,994 Epoch 1130: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.81 
2024-02-06 00:45:00,994 EPOCH 1131
2024-02-06 00:45:05,397 Epoch 1131: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.81 
2024-02-06 00:45:05,397 EPOCH 1132
2024-02-06 00:45:10,187 Epoch 1132: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.76 
2024-02-06 00:45:10,187 EPOCH 1133
2024-02-06 00:45:11,963 [Epoch: 1133 Step: 00038500] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     2165 || Batch Translation Loss:   0.019327 => Txt Tokens per Sec:     6205 || Lr: 0.000100
2024-02-06 00:45:14,395 Epoch 1133: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.66 
2024-02-06 00:45:14,395 EPOCH 1134
2024-02-06 00:45:19,303 Epoch 1134: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-06 00:45:19,303 EPOCH 1135
2024-02-06 00:45:23,512 Epoch 1135: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.40 
2024-02-06 00:45:23,513 EPOCH 1136
2024-02-06 00:45:25,361 [Epoch: 1136 Step: 00038600] Batch Recognition Loss:   0.000148 => Gls Tokens per Sec:     1733 || Batch Translation Loss:   0.014563 => Txt Tokens per Sec:     5033 || Lr: 0.000100
2024-02-06 00:45:28,451 Epoch 1136: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.19 
2024-02-06 00:45:28,451 EPOCH 1137
2024-02-06 00:45:32,784 Epoch 1137: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.77 
2024-02-06 00:45:32,785 EPOCH 1138
2024-02-06 00:45:36,867 Epoch 1138: Total Training Recognition Loss 0.07  Total Training Translation Loss 13.84 
2024-02-06 00:45:36,867 EPOCH 1139
2024-02-06 00:45:37,562 [Epoch: 1139 Step: 00038700] Batch Recognition Loss:   0.003797 => Gls Tokens per Sec:     3686 || Batch Translation Loss:   0.833550 => Txt Tokens per Sec:     8509 || Lr: 0.000100
2024-02-06 00:45:41,013 Epoch 1139: Total Training Recognition Loss 0.18  Total Training Translation Loss 13.64 
2024-02-06 00:45:41,013 EPOCH 1140
2024-02-06 00:45:45,104 Epoch 1140: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.12 
2024-02-06 00:45:45,104 EPOCH 1141
2024-02-06 00:45:49,995 Epoch 1141: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.85 
2024-02-06 00:45:49,996 EPOCH 1142
2024-02-06 00:45:50,611 [Epoch: 1142 Step: 00038800] Batch Recognition Loss:   0.001153 => Gls Tokens per Sec:     3127 || Batch Translation Loss:   0.025614 => Txt Tokens per Sec:     8397 || Lr: 0.000100
2024-02-06 00:45:54,369 Epoch 1142: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.90 
2024-02-06 00:45:54,369 EPOCH 1143
2024-02-06 00:45:59,014 Epoch 1143: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.49 
2024-02-06 00:45:59,014 EPOCH 1144
2024-02-06 00:46:03,687 Epoch 1144: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.50 
2024-02-06 00:46:03,688 EPOCH 1145
2024-02-06 00:46:04,088 [Epoch: 1145 Step: 00038900] Batch Recognition Loss:   0.000540 => Gls Tokens per Sec:     3200 || Batch Translation Loss:   0.013668 => Txt Tokens per Sec:     8515 || Lr: 0.000100
2024-02-06 00:46:08,203 Epoch 1145: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.93 
2024-02-06 00:46:08,204 EPOCH 1146
2024-02-06 00:46:12,928 Epoch 1146: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.91 
2024-02-06 00:46:12,928 EPOCH 1147
2024-02-06 00:46:17,242 Epoch 1147: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.78 
2024-02-06 00:46:17,243 EPOCH 1148
2024-02-06 00:46:17,674 [Epoch: 1148 Step: 00039000] Batch Recognition Loss:   0.000284 => Gls Tokens per Sec:     1488 || Batch Translation Loss:   0.018799 => Txt Tokens per Sec:     4558 || Lr: 0.000100
2024-02-06 00:46:22,160 Epoch 1148: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.74 
2024-02-06 00:46:22,160 EPOCH 1149
2024-02-06 00:46:26,270 Epoch 1149: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-06 00:46:26,270 EPOCH 1150
2024-02-06 00:46:31,276 [Epoch: 1150 Step: 00039100] Batch Recognition Loss:   0.000354 => Gls Tokens per Sec:     2122 || Batch Translation Loss:   0.017052 => Txt Tokens per Sec:     5871 || Lr: 0.000100
2024-02-06 00:46:31,276 Epoch 1150: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-06 00:46:31,276 EPOCH 1151
2024-02-06 00:46:35,492 Epoch 1151: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.73 
2024-02-06 00:46:35,492 EPOCH 1152
2024-02-06 00:46:40,360 Epoch 1152: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-06 00:46:40,361 EPOCH 1153
2024-02-06 00:46:44,533 [Epoch: 1153 Step: 00039200] Batch Recognition Loss:   0.000514 => Gls Tokens per Sec:     2456 || Batch Translation Loss:   0.009717 => Txt Tokens per Sec:     6839 || Lr: 0.000100
2024-02-06 00:46:44,714 Epoch 1153: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.77 
2024-02-06 00:46:44,714 EPOCH 1154
2024-02-06 00:46:49,504 Epoch 1154: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.66 
2024-02-06 00:46:49,505 EPOCH 1155
2024-02-06 00:46:53,944 Epoch 1155: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-06 00:46:53,944 EPOCH 1156
2024-02-06 00:46:57,969 [Epoch: 1156 Step: 00039300] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:     2321 || Batch Translation Loss:   0.077348 => Txt Tokens per Sec:     6383 || Lr: 0.000100
2024-02-06 00:46:58,549 Epoch 1156: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.80 
2024-02-06 00:46:58,549 EPOCH 1157
2024-02-06 00:47:03,265 Epoch 1157: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.10 
2024-02-06 00:47:03,265 EPOCH 1158
2024-02-06 00:47:08,283 Epoch 1158: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-06 00:47:08,284 EPOCH 1159
2024-02-06 00:47:12,158 [Epoch: 1159 Step: 00039400] Batch Recognition Loss:   0.000239 => Gls Tokens per Sec:     2246 || Batch Translation Loss:   0.032335 => Txt Tokens per Sec:     6207 || Lr: 0.000100
2024-02-06 00:47:12,820 Epoch 1159: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.32 
2024-02-06 00:47:12,820 EPOCH 1160
2024-02-06 00:47:17,250 Epoch 1160: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.01 
2024-02-06 00:47:17,250 EPOCH 1161
2024-02-06 00:47:22,206 Epoch 1161: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-06 00:47:22,207 EPOCH 1162
2024-02-06 00:47:25,853 [Epoch: 1162 Step: 00039500] Batch Recognition Loss:   0.000330 => Gls Tokens per Sec:     2283 || Batch Translation Loss:   0.034239 => Txt Tokens per Sec:     6322 || Lr: 0.000100
2024-02-06 00:47:27,188 Epoch 1162: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.36 
2024-02-06 00:47:27,188 EPOCH 1163
2024-02-06 00:47:31,556 Epoch 1163: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.01 
2024-02-06 00:47:31,556 EPOCH 1164
2024-02-06 00:47:35,878 Epoch 1164: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.11 
2024-02-06 00:47:35,879 EPOCH 1165
2024-02-06 00:47:39,413 [Epoch: 1165 Step: 00039600] Batch Recognition Loss:   0.000807 => Gls Tokens per Sec:     2100 || Batch Translation Loss:   0.021873 => Txt Tokens per Sec:     5851 || Lr: 0.000100
2024-02-06 00:47:40,757 Epoch 1165: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-06 00:47:40,757 EPOCH 1166
2024-02-06 00:47:44,930 Epoch 1166: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-06 00:47:44,930 EPOCH 1167
2024-02-06 00:47:49,778 Epoch 1167: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.93 
2024-02-06 00:47:49,779 EPOCH 1168
2024-02-06 00:47:52,906 [Epoch: 1168 Step: 00039700] Batch Recognition Loss:   0.000633 => Gls Tokens per Sec:     2253 || Batch Translation Loss:   0.026881 => Txt Tokens per Sec:     6515 || Lr: 0.000100
2024-02-06 00:47:54,176 Epoch 1168: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-06 00:47:54,176 EPOCH 1169
2024-02-06 00:47:58,766 Epoch 1169: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-06 00:47:58,767 EPOCH 1170
2024-02-06 00:48:03,476 Epoch 1170: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-06 00:48:03,476 EPOCH 1171
2024-02-06 00:48:06,082 [Epoch: 1171 Step: 00039800] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:     2457 || Batch Translation Loss:   0.014372 => Txt Tokens per Sec:     6902 || Lr: 0.000100
2024-02-06 00:48:07,919 Epoch 1171: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.12 
2024-02-06 00:48:07,920 EPOCH 1172
2024-02-06 00:48:12,757 Epoch 1172: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-06 00:48:12,757 EPOCH 1173
2024-02-06 00:48:16,998 Epoch 1173: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-06 00:48:16,999 EPOCH 1174
2024-02-06 00:48:19,943 [Epoch: 1174 Step: 00039900] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:     1957 || Batch Translation Loss:   0.025420 => Txt Tokens per Sec:     5632 || Lr: 0.000100
2024-02-06 00:48:21,916 Epoch 1174: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-06 00:48:21,916 EPOCH 1175
2024-02-06 00:48:26,092 Epoch 1175: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.71 
2024-02-06 00:48:26,092 EPOCH 1176
2024-02-06 00:48:31,033 Epoch 1176: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.92 
2024-02-06 00:48:31,034 EPOCH 1177
2024-02-06 00:48:32,987 [Epoch: 1177 Step: 00040000] Batch Recognition Loss:   0.000312 => Gls Tokens per Sec:     2490 || Batch Translation Loss:   0.046411 => Txt Tokens per Sec:     6636 || Lr: 0.000100
2024-02-06 00:48:42,159 Validation result at epoch 1177, step    40000: duration: 9.1725s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.36420	Translation Loss: 92801.75000	PPL: 10604.68945
	Eval Metric: BLEU
	WER 3.46	(DEL: 0.00,	INS: 0.00,	SUB: 3.46)
	BLEU-4 0.33	(BLEU-1: 9.90,	BLEU-2: 2.84,	BLEU-3: 0.94,	BLEU-4: 0.33)
	CHRF 16.51	ROUGE 8.10
2024-02-06 00:48:42,160 Logging Recognition and Translation Outputs
2024-02-06 00:48:42,160 ========================================================================================================================
2024-02-06 00:48:42,160 Logging Sequence: 86_84.00
2024-02-06 00:48:42,160 	Gloss Reference :	A B+C+D+E
2024-02-06 00:48:42,161 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:48:42,161 	Gloss Alignment :	         
2024-02-06 00:48:42,161 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:48:42,162 	Text Reference  :	amassing 8933 runs which included 21 centuries with    a       highest score of  201    not out 
2024-02-06 00:48:42,162 	Text Hypothesis :	******** **** **** ***** ******** vs jammu     kashmir schools match   he    had scored 260 runs
2024-02-06 00:48:42,162 	Text Alignment  :	D        D    D    D     D        S  S         S       S       S       S     S   S      S   S   
2024-02-06 00:48:42,162 ========================================================================================================================
2024-02-06 00:48:42,163 Logging Sequence: 179_110.00
2024-02-06 00:48:42,163 	Gloss Reference :	A B+C+D+E
2024-02-06 00:48:42,163 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:48:42,163 	Gloss Alignment :	         
2024-02-06 00:48:42,163 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:48:42,165 	Text Reference  :	**** ****** ****** *** * ******* *** **** ***** ****** phogat refused to       stay in the same    room with other indian female wrestlers
2024-02-06 00:48:42,165 	Text Hypothesis :	even though sunday was a holiday wfi kept their office and    sonam   received it   at the airport on   the  day   of     her    departure
2024-02-06 00:48:42,165 	Text Alignment  :	I    I      I      I   I I       I   I    I     I      S      S       S        S    S      S       S    S    S     S      S      S        
2024-02-06 00:48:42,165 ========================================================================================================================
2024-02-06 00:48:42,165 Logging Sequence: 102_2.00
2024-02-06 00:48:42,166 	Gloss Reference :	A B+C+D+E    
2024-02-06 00:48:42,166 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-06 00:48:42,166 	Gloss Alignment :	  S          
2024-02-06 00:48:42,166 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:48:42,167 	Text Reference  :	** commonwealth games are  among the world's most recognised gaming championships after   the olympics
2024-02-06 00:48:42,167 	Text Hypothesis :	on 23rd         march 2021 at    the ******* **** 1st        match  against       england in  japan   
2024-02-06 00:48:42,167 	Text Alignment  :	I  S            S     S    S         D       D    S          S      S             S       S   S       
2024-02-06 00:48:42,167 ========================================================================================================================
2024-02-06 00:48:42,167 Logging Sequence: 60_195.00
2024-02-06 00:48:42,168 	Gloss Reference :	A B+C+D+E
2024-02-06 00:48:42,168 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:48:42,168 	Gloss Alignment :	         
2024-02-06 00:48:42,168 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:48:42,169 	Text Reference  :	******** ****** people loved to    watch his aggressive expressions and  his bowling
2024-02-06 00:48:42,169 	Text Hypothesis :	recently during india  -     nepal asia  cup match      when        play was caught 
2024-02-06 00:48:42,169 	Text Alignment  :	I        I      S      S     S     S     S   S          S           S    S   S      
2024-02-06 00:48:42,169 ========================================================================================================================
2024-02-06 00:48:42,169 Logging Sequence: 70_200.00
2024-02-06 00:48:42,169 	Gloss Reference :	A B+C+D+E
2024-02-06 00:48:42,170 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:48:42,170 	Gloss Alignment :	         
2024-02-06 00:48:42,170 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:48:42,170 	Text Reference  :	showing ronaldo whole-heartedly endorsing the    brand     
2024-02-06 00:48:42,170 	Text Hypothesis :	in      june    is              an        ardent bike-lover
2024-02-06 00:48:42,170 	Text Alignment  :	S       S       S               S         S      S         
2024-02-06 00:48:42,171 ========================================================================================================================
2024-02-06 00:48:44,924 Epoch 1177: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.25 
2024-02-06 00:48:44,925 EPOCH 1178
2024-02-06 00:48:49,828 Epoch 1178: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.22 
2024-02-06 00:48:49,828 EPOCH 1179
2024-02-06 00:48:54,160 Epoch 1179: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.87 
2024-02-06 00:48:54,161 EPOCH 1180
2024-02-06 00:48:55,880 [Epoch: 1180 Step: 00040100] Batch Recognition Loss:   0.000934 => Gls Tokens per Sec:     2608 || Batch Translation Loss:   0.078731 => Txt Tokens per Sec:     7031 || Lr: 0.000100
2024-02-06 00:48:58,797 Epoch 1180: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.98 
2024-02-06 00:48:58,797 EPOCH 1181
2024-02-06 00:49:03,480 Epoch 1181: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.94 
2024-02-06 00:49:03,481 EPOCH 1182
2024-02-06 00:49:08,492 Epoch 1182: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.96 
2024-02-06 00:49:08,492 EPOCH 1183
2024-02-06 00:49:10,422 [Epoch: 1183 Step: 00040200] Batch Recognition Loss:   0.000483 => Gls Tokens per Sec:     1991 || Batch Translation Loss:   0.030232 => Txt Tokens per Sec:     6024 || Lr: 0.000100
2024-02-06 00:49:13,165 Epoch 1183: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.32 
2024-02-06 00:49:13,166 EPOCH 1184
2024-02-06 00:49:18,069 Epoch 1184: Total Training Recognition Loss 0.13  Total Training Translation Loss 2.85 
2024-02-06 00:49:18,069 EPOCH 1185
2024-02-06 00:49:22,640 Epoch 1185: Total Training Recognition Loss 2.58  Total Training Translation Loss 4.43 
2024-02-06 00:49:22,641 EPOCH 1186
2024-02-06 00:49:24,099 [Epoch: 1186 Step: 00040300] Batch Recognition Loss:   0.225675 => Gls Tokens per Sec:     2197 || Batch Translation Loss:   0.096992 => Txt Tokens per Sec:     6296 || Lr: 0.000100
2024-02-06 00:49:27,504 Epoch 1186: Total Training Recognition Loss 14.90  Total Training Translation Loss 5.28 
2024-02-06 00:49:27,505 EPOCH 1187
2024-02-06 00:49:31,704 Epoch 1187: Total Training Recognition Loss 2.12  Total Training Translation Loss 4.72 
2024-02-06 00:49:31,704 EPOCH 1188
2024-02-06 00:49:36,437 Epoch 1188: Total Training Recognition Loss 0.82  Total Training Translation Loss 1.98 
2024-02-06 00:49:36,438 EPOCH 1189
2024-02-06 00:49:37,555 [Epoch: 1189 Step: 00040400] Batch Recognition Loss:   0.000387 => Gls Tokens per Sec:     2294 || Batch Translation Loss:   0.030837 => Txt Tokens per Sec:     6316 || Lr: 0.000100
2024-02-06 00:49:40,942 Epoch 1189: Total Training Recognition Loss 0.39  Total Training Translation Loss 1.21 
2024-02-06 00:49:40,942 EPOCH 1190
2024-02-06 00:49:45,518 Epoch 1190: Total Training Recognition Loss 0.10  Total Training Translation Loss 0.77 
2024-02-06 00:49:45,519 EPOCH 1191
2024-02-06 00:49:50,119 Epoch 1191: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.75 
2024-02-06 00:49:50,119 EPOCH 1192
2024-02-06 00:49:50,667 [Epoch: 1192 Step: 00040500] Batch Recognition Loss:   0.000251 => Gls Tokens per Sec:     3510 || Batch Translation Loss:   0.012635 => Txt Tokens per Sec:     9055 || Lr: 0.000100
2024-02-06 00:49:54,621 Epoch 1192: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.65 
2024-02-06 00:49:54,622 EPOCH 1193
2024-02-06 00:49:59,497 Epoch 1193: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.72 
2024-02-06 00:49:59,497 EPOCH 1194
2024-02-06 00:50:03,765 Epoch 1194: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.68 
2024-02-06 00:50:03,765 EPOCH 1195
2024-02-06 00:50:04,387 [Epoch: 1195 Step: 00040600] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:     2061 || Batch Translation Loss:   0.055151 => Txt Tokens per Sec:     5855 || Lr: 0.000100
2024-02-06 00:50:08,680 Epoch 1195: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.73 
2024-02-06 00:50:08,681 EPOCH 1196
2024-02-06 00:50:12,783 Epoch 1196: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.64 
2024-02-06 00:50:12,783 EPOCH 1197
2024-02-06 00:50:17,596 Epoch 1197: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.58 
2024-02-06 00:50:17,597 EPOCH 1198
2024-02-06 00:50:17,838 [Epoch: 1198 Step: 00040700] Batch Recognition Loss:   0.012589 => Gls Tokens per Sec:     2667 || Batch Translation Loss:   0.018083 => Txt Tokens per Sec:     7750 || Lr: 0.000100
2024-02-06 00:50:22,027 Epoch 1198: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.68 
2024-02-06 00:50:22,027 EPOCH 1199
2024-02-06 00:50:26,926 Epoch 1199: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.87 
2024-02-06 00:50:26,927 EPOCH 1200
2024-02-06 00:50:31,311 [Epoch: 1200 Step: 00040800] Batch Recognition Loss:   0.000115 => Gls Tokens per Sec:     2424 || Batch Translation Loss:   0.021570 => Txt Tokens per Sec:     6705 || Lr: 0.000100
2024-02-06 00:50:31,311 Epoch 1200: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.67 
2024-02-06 00:50:31,311 EPOCH 1201
2024-02-06 00:50:36,258 Epoch 1201: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-06 00:50:36,259 EPOCH 1202
2024-02-06 00:50:40,973 Epoch 1202: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.56 
2024-02-06 00:50:40,973 EPOCH 1203
2024-02-06 00:50:44,829 [Epoch: 1203 Step: 00040900] Batch Recognition Loss:   0.006556 => Gls Tokens per Sec:     2588 || Batch Translation Loss:   0.025336 => Txt Tokens per Sec:     7130 || Lr: 0.000100
2024-02-06 00:50:45,093 Epoch 1203: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.66 
2024-02-06 00:50:45,094 EPOCH 1204
2024-02-06 00:50:50,038 Epoch 1204: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.97 
2024-02-06 00:50:50,039 EPOCH 1205
2024-02-06 00:50:54,340 Epoch 1205: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.95 
2024-02-06 00:50:54,340 EPOCH 1206
2024-02-06 00:50:58,226 [Epoch: 1206 Step: 00041000] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:     2404 || Batch Translation Loss:   0.051988 => Txt Tokens per Sec:     6497 || Lr: 0.000100
2024-02-06 00:50:59,183 Epoch 1206: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.60 
2024-02-06 00:50:59,183 EPOCH 1207
2024-02-06 00:51:03,523 Epoch 1207: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.07 
2024-02-06 00:51:03,523 EPOCH 1208
2024-02-06 00:51:08,279 Epoch 1208: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.46 
2024-02-06 00:51:08,280 EPOCH 1209
2024-02-06 00:51:12,187 [Epoch: 1209 Step: 00041100] Batch Recognition Loss:   0.000246 => Gls Tokens per Sec:     2294 || Batch Translation Loss:   0.025091 => Txt Tokens per Sec:     6462 || Lr: 0.000100
2024-02-06 00:51:12,867 Epoch 1209: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.26 
2024-02-06 00:51:12,867 EPOCH 1210
2024-02-06 00:51:17,704 Epoch 1210: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.05 
2024-02-06 00:51:17,704 EPOCH 1211
2024-02-06 00:51:22,172 Epoch 1211: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.11 
2024-02-06 00:51:22,172 EPOCH 1212
2024-02-06 00:51:25,540 [Epoch: 1212 Step: 00041200] Batch Recognition Loss:   0.000213 => Gls Tokens per Sec:     2471 || Batch Translation Loss:   0.024963 => Txt Tokens per Sec:     6711 || Lr: 0.000100
2024-02-06 00:51:26,709 Epoch 1212: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.98 
2024-02-06 00:51:26,709 EPOCH 1213
2024-02-06 00:51:31,416 Epoch 1213: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.44 
2024-02-06 00:51:31,416 EPOCH 1214
2024-02-06 00:51:35,825 Epoch 1214: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.46 
2024-02-06 00:51:35,825 EPOCH 1215
2024-02-06 00:51:39,104 [Epoch: 1215 Step: 00041300] Batch Recognition Loss:   0.002937 => Gls Tokens per Sec:     2343 || Batch Translation Loss:   0.038979 => Txt Tokens per Sec:     6318 || Lr: 0.000100
2024-02-06 00:51:40,679 Epoch 1215: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.34 
2024-02-06 00:51:40,679 EPOCH 1216
2024-02-06 00:51:44,757 Epoch 1216: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.49 
2024-02-06 00:51:44,757 EPOCH 1217
2024-02-06 00:51:49,730 Epoch 1217: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.59 
2024-02-06 00:51:49,731 EPOCH 1218
2024-02-06 00:51:52,659 [Epoch: 1218 Step: 00041400] Batch Recognition Loss:   0.000395 => Gls Tokens per Sec:     2317 || Batch Translation Loss:   0.275632 => Txt Tokens per Sec:     6455 || Lr: 0.000100
2024-02-06 00:51:54,033 Epoch 1218: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.32 
2024-02-06 00:51:54,033 EPOCH 1219
2024-02-06 00:51:58,963 Epoch 1219: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.81 
2024-02-06 00:51:58,964 EPOCH 1220
2024-02-06 00:52:03,272 Epoch 1220: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.80 
2024-02-06 00:52:03,272 EPOCH 1221
2024-02-06 00:52:05,980 [Epoch: 1221 Step: 00041500] Batch Recognition Loss:   0.000396 => Gls Tokens per Sec:     2268 || Batch Translation Loss:   0.018518 => Txt Tokens per Sec:     6385 || Lr: 0.000100
2024-02-06 00:52:08,087 Epoch 1221: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.92 
2024-02-06 00:52:08,087 EPOCH 1222
2024-02-06 00:52:12,711 Epoch 1222: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.56 
2024-02-06 00:52:12,711 EPOCH 1223
2024-02-06 00:52:17,662 Epoch 1223: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.88 
2024-02-06 00:52:17,663 EPOCH 1224
2024-02-06 00:52:19,737 [Epoch: 1224 Step: 00041600] Batch Recognition Loss:   0.000595 => Gls Tokens per Sec:     2779 || Batch Translation Loss:   0.027694 => Txt Tokens per Sec:     7484 || Lr: 0.000100
2024-02-06 00:52:22,040 Epoch 1224: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.71 
2024-02-06 00:52:22,040 EPOCH 1225
2024-02-06 00:52:26,620 Epoch 1225: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.40 
2024-02-06 00:52:26,621 EPOCH 1226
2024-02-06 00:52:31,273 Epoch 1226: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.71 
2024-02-06 00:52:31,273 EPOCH 1227
2024-02-06 00:52:33,255 [Epoch: 1227 Step: 00041700] Batch Recognition Loss:   0.000526 => Gls Tokens per Sec:     2585 || Batch Translation Loss:   0.023565 => Txt Tokens per Sec:     6973 || Lr: 0.000100
2024-02-06 00:52:35,765 Epoch 1227: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.15 
2024-02-06 00:52:35,765 EPOCH 1228
2024-02-06 00:52:40,534 Epoch 1228: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.92 
2024-02-06 00:52:40,535 EPOCH 1229
2024-02-06 00:52:45,004 Epoch 1229: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.70 
2024-02-06 00:52:45,004 EPOCH 1230
2024-02-06 00:52:46,774 [Epoch: 1230 Step: 00041800] Batch Recognition Loss:   0.000731 => Gls Tokens per Sec:     2533 || Batch Translation Loss:   0.023849 => Txt Tokens per Sec:     6653 || Lr: 0.000100
2024-02-06 00:52:49,810 Epoch 1230: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.61 
2024-02-06 00:52:49,811 EPOCH 1231
2024-02-06 00:52:53,987 Epoch 1231: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.62 
2024-02-06 00:52:53,987 EPOCH 1232
2024-02-06 00:52:58,930 Epoch 1232: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.60 
2024-02-06 00:52:58,930 EPOCH 1233
2024-02-06 00:53:00,389 [Epoch: 1233 Step: 00041900] Batch Recognition Loss:   0.000366 => Gls Tokens per Sec:     2634 || Batch Translation Loss:   0.012820 => Txt Tokens per Sec:     7528 || Lr: 0.000100
2024-02-06 00:53:03,628 Epoch 1233: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.73 
2024-02-06 00:53:03,629 EPOCH 1234
2024-02-06 00:53:08,561 Epoch 1234: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.64 
2024-02-06 00:53:08,562 EPOCH 1235
2024-02-06 00:53:13,183 Epoch 1235: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.80 
2024-02-06 00:53:13,183 EPOCH 1236
2024-02-06 00:53:14,720 [Epoch: 1236 Step: 00042000] Batch Recognition Loss:   0.000257 => Gls Tokens per Sec:     2085 || Batch Translation Loss:   0.013900 => Txt Tokens per Sec:     6145 || Lr: 0.000100
2024-02-06 00:53:23,259 Validation result at epoch 1236, step    42000: duration: 8.5380s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.19254	Translation Loss: 93493.27344	PPL: 11363.04102
	Eval Metric: BLEU
	WER 2.68	(DEL: 0.00,	INS: 0.00,	SUB: 2.68)
	BLEU-4 0.65	(BLEU-1: 11.27,	BLEU-2: 3.53,	BLEU-3: 1.36,	BLEU-4: 0.65)
	CHRF 17.57	ROUGE 9.34
2024-02-06 00:53:23,260 Logging Recognition and Translation Outputs
2024-02-06 00:53:23,260 ========================================================================================================================
2024-02-06 00:53:23,260 Logging Sequence: 154_94.00
2024-02-06 00:53:23,260 	Gloss Reference :	A B+C+D+E
2024-02-06 00:53:23,261 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:53:23,261 	Gloss Alignment :	         
2024-02-06 00:53:23,261 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:53:23,262 	Text Reference  :	***** the ipl will also be held  in uae  from september 19        to  october 15       
2024-02-06 00:53:23,262 	Text Hypothesis :	after the *** **** **** ** start of july new  zealand   singapore and then    restarted
2024-02-06 00:53:23,262 	Text Alignment  :	I         D   D    D    D  S     S  S    S    S         S         S   S       S        
2024-02-06 00:53:23,262 ========================================================================================================================
2024-02-06 00:53:23,262 Logging Sequence: 118_2.00
2024-02-06 00:53:23,263 	Gloss Reference :	A B+C+D+E
2024-02-06 00:53:23,263 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:53:23,263 	Gloss Alignment :	         
2024-02-06 00:53:23,263 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:53:23,264 	Text Reference  :	yesterday was a   very   exciting day      people across the   world were  watching
2024-02-06 00:53:23,264 	Text Hypothesis :	********* *** the people started  trolling was    being  being being being made    
2024-02-06 00:53:23,264 	Text Alignment  :	D         D   S   S      S        S        S      S      S     S     S     S       
2024-02-06 00:53:23,264 ========================================================================================================================
2024-02-06 00:53:23,264 Logging Sequence: 165_453.00
2024-02-06 00:53:23,265 	Gloss Reference :	A B+C+D+E
2024-02-06 00:53:23,265 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:53:23,265 	Gloss Alignment :	         
2024-02-06 00:53:23,265 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:53:23,266 	Text Reference  :	icc     did not agree to   sehwag' decision of   wearing a   numberless jersey 
2024-02-06 00:53:23,266 	Text Hypothesis :	however he  has only  wore this    wicket   when he      got india      england
2024-02-06 00:53:23,266 	Text Alignment  :	S       S   S   S     S    S       S        S    S       S   S          S      
2024-02-06 00:53:23,266 ========================================================================================================================
2024-02-06 00:53:23,266 Logging Sequence: 126_163.00
2024-02-06 00:53:23,267 	Gloss Reference :	A B+C+D+E
2024-02-06 00:53:23,267 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:53:23,267 	Gloss Alignment :	         
2024-02-06 00:53:23,267 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:53:23,268 	Text Reference  :	*** your hard     work has helped secure a     medal at the tokyo olympics  
2024-02-06 00:53:23,268 	Text Hypothesis :	for the  olympics one  day and    i      never spoke to the ***** government
2024-02-06 00:53:23,268 	Text Alignment  :	I   S    S        S    S   S      S      S     S     S      D     S         
2024-02-06 00:53:23,268 ========================================================================================================================
2024-02-06 00:53:23,268 Logging Sequence: 84_2.00
2024-02-06 00:53:23,269 	Gloss Reference :	A B+C+D+E
2024-02-06 00:53:23,269 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:53:23,269 	Gloss Alignment :	         
2024-02-06 00:53:23,269 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:53:23,270 	Text Reference  :	the ********* 2022 fifa  football world cup ****** is     going on      in qatar from 20th november 2022 to 18th december 2022 
2024-02-06 00:53:23,271 	Text Hypothesis :	the cricketer was  about the      world cup trophy former rcb   captain in ***** **** **** ******** **** ** **** ******** dubai
2024-02-06 00:53:23,271 	Text Alignment  :	    I         S    S     S                  I      S      S     S          D     D    D    D        D    D  D    D        S    
2024-02-06 00:53:23,271 ========================================================================================================================
2024-02-06 00:53:26,864 Epoch 1236: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-06 00:53:26,865 EPOCH 1237
2024-02-06 00:53:31,354 Epoch 1237: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.79 
2024-02-06 00:53:31,354 EPOCH 1238
2024-02-06 00:53:36,015 Epoch 1238: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-06 00:53:36,016 EPOCH 1239
2024-02-06 00:53:36,965 [Epoch: 1239 Step: 00042100] Batch Recognition Loss:   0.000483 => Gls Tokens per Sec:     2426 || Batch Translation Loss:   0.018937 => Txt Tokens per Sec:     6072 || Lr: 0.000100
2024-02-06 00:53:40,599 Epoch 1239: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.71 
2024-02-06 00:53:40,599 EPOCH 1240
2024-02-06 00:53:44,710 Epoch 1240: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.86 
2024-02-06 00:53:44,710 EPOCH 1241
2024-02-06 00:53:48,798 Epoch 1241: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.93 
2024-02-06 00:53:48,798 EPOCH 1242
2024-02-06 00:53:49,746 [Epoch: 1242 Step: 00042200] Batch Recognition Loss:   0.000284 => Gls Tokens per Sec:     2027 || Batch Translation Loss:   0.010832 => Txt Tokens per Sec:     5817 || Lr: 0.000100
2024-02-06 00:53:53,664 Epoch 1242: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-06 00:53:53,665 EPOCH 1243
2024-02-06 00:53:58,041 Epoch 1243: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.00 
2024-02-06 00:53:58,041 EPOCH 1244
2024-02-06 00:54:02,635 Epoch 1244: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-06 00:54:02,635 EPOCH 1245
2024-02-06 00:54:03,284 [Epoch: 1245 Step: 00042300] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     1978 || Batch Translation Loss:   0.090505 => Txt Tokens per Sec:     5921 || Lr: 0.000100
2024-02-06 00:54:07,235 Epoch 1245: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.10 
2024-02-06 00:54:07,235 EPOCH 1246
2024-02-06 00:54:11,732 Epoch 1246: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.42 
2024-02-06 00:54:11,733 EPOCH 1247
2024-02-06 00:54:16,499 Epoch 1247: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.41 
2024-02-06 00:54:16,499 EPOCH 1248
2024-02-06 00:54:16,686 [Epoch: 1248 Step: 00042400] Batch Recognition Loss:   0.000618 => Gls Tokens per Sec:     3441 || Batch Translation Loss:   0.025888 => Txt Tokens per Sec:     9376 || Lr: 0.000100
2024-02-06 00:54:20,906 Epoch 1248: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.97 
2024-02-06 00:54:20,907 EPOCH 1249
2024-02-06 00:54:25,711 Epoch 1249: Total Training Recognition Loss 0.08  Total Training Translation Loss 12.52 
2024-02-06 00:54:25,711 EPOCH 1250
2024-02-06 00:54:29,854 [Epoch: 1250 Step: 00042500] Batch Recognition Loss:   0.000842 => Gls Tokens per Sec:     2565 || Batch Translation Loss:   0.137567 => Txt Tokens per Sec:     7095 || Lr: 0.000100
2024-02-06 00:54:29,854 Epoch 1250: Total Training Recognition Loss 0.06  Total Training Translation Loss 6.55 
2024-02-06 00:54:29,855 EPOCH 1251
2024-02-06 00:54:34,839 Epoch 1251: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.05 
2024-02-06 00:54:34,839 EPOCH 1252
2024-02-06 00:54:39,057 Epoch 1252: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.15 
2024-02-06 00:54:39,057 EPOCH 1253
2024-02-06 00:54:43,395 [Epoch: 1253 Step: 00042600] Batch Recognition Loss:   0.000608 => Gls Tokens per Sec:     2301 || Batch Translation Loss:   0.019228 => Txt Tokens per Sec:     6281 || Lr: 0.000100
2024-02-06 00:54:43,992 Epoch 1253: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.78 
2024-02-06 00:54:43,992 EPOCH 1254
2024-02-06 00:54:48,283 Epoch 1254: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.73 
2024-02-06 00:54:48,284 EPOCH 1255
2024-02-06 00:54:52,967 Epoch 1255: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.10 
2024-02-06 00:54:52,968 EPOCH 1256
2024-02-06 00:54:56,890 [Epoch: 1256 Step: 00042700] Batch Recognition Loss:   0.000994 => Gls Tokens per Sec:     2383 || Batch Translation Loss:   0.022586 => Txt Tokens per Sec:     6534 || Lr: 0.000100
2024-02-06 00:54:57,503 Epoch 1256: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-06 00:54:57,503 EPOCH 1257
2024-02-06 00:55:01,779 Epoch 1257: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-06 00:55:01,780 EPOCH 1258
2024-02-06 00:55:06,699 Epoch 1258: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-06 00:55:06,699 EPOCH 1259
2024-02-06 00:55:10,282 [Epoch: 1259 Step: 00042800] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     2429 || Batch Translation Loss:   0.014212 => Txt Tokens per Sec:     6773 || Lr: 0.000100
2024-02-06 00:55:10,845 Epoch 1259: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-06 00:55:10,845 EPOCH 1260
2024-02-06 00:55:15,792 Epoch 1260: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-06 00:55:15,792 EPOCH 1261
2024-02-06 00:55:20,056 Epoch 1261: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.50 
2024-02-06 00:55:20,057 EPOCH 1262
2024-02-06 00:55:23,719 [Epoch: 1262 Step: 00042900] Batch Recognition Loss:   0.000277 => Gls Tokens per Sec:     2272 || Batch Translation Loss:   0.015682 => Txt Tokens per Sec:     6433 || Lr: 0.000100
2024-02-06 00:55:24,931 Epoch 1262: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.44 
2024-02-06 00:55:24,931 EPOCH 1263
2024-02-06 00:55:29,379 Epoch 1263: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.94 
2024-02-06 00:55:29,379 EPOCH 1264
2024-02-06 00:55:34,399 Epoch 1264: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.95 
2024-02-06 00:55:34,399 EPOCH 1265
2024-02-06 00:55:37,645 [Epoch: 1265 Step: 00043000] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     2286 || Batch Translation Loss:   0.014671 => Txt Tokens per Sec:     6350 || Lr: 0.000100
2024-02-06 00:55:39,006 Epoch 1265: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.82 
2024-02-06 00:55:39,006 EPOCH 1266
2024-02-06 00:55:43,147 Epoch 1266: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.87 
2024-02-06 00:55:43,148 EPOCH 1267
2024-02-06 00:55:48,121 Epoch 1267: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.42 
2024-02-06 00:55:48,122 EPOCH 1268
2024-02-06 00:55:50,541 [Epoch: 1268 Step: 00043100] Batch Recognition Loss:   0.000772 => Gls Tokens per Sec:     2911 || Batch Translation Loss:   0.050344 => Txt Tokens per Sec:     7759 || Lr: 0.000100
2024-02-06 00:55:52,312 Epoch 1268: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.30 
2024-02-06 00:55:52,312 EPOCH 1269
2024-02-06 00:55:57,178 Epoch 1269: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.18 
2024-02-06 00:55:57,179 EPOCH 1270
2024-02-06 00:56:01,524 Epoch 1270: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.82 
2024-02-06 00:56:01,524 EPOCH 1271
2024-02-06 00:56:04,364 [Epoch: 1271 Step: 00043200] Batch Recognition Loss:   0.001902 => Gls Tokens per Sec:     2163 || Batch Translation Loss:   0.129843 => Txt Tokens per Sec:     6247 || Lr: 0.000100
2024-02-06 00:56:06,292 Epoch 1271: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.38 
2024-02-06 00:56:06,292 EPOCH 1272
2024-02-06 00:56:10,821 Epoch 1272: Total Training Recognition Loss 0.03  Total Training Translation Loss 6.48 
2024-02-06 00:56:10,821 EPOCH 1273
2024-02-06 00:56:15,474 Epoch 1273: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.18 
2024-02-06 00:56:15,475 EPOCH 1274
2024-02-06 00:56:17,772 [Epoch: 1274 Step: 00043300] Batch Recognition Loss:   0.000482 => Gls Tokens per Sec:     2509 || Batch Translation Loss:   0.035256 => Txt Tokens per Sec:     6880 || Lr: 0.000100
2024-02-06 00:56:20,083 Epoch 1274: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.34 
2024-02-06 00:56:20,083 EPOCH 1275
2024-02-06 00:56:24,214 Epoch 1275: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.72 
2024-02-06 00:56:24,214 EPOCH 1276
2024-02-06 00:56:29,170 Epoch 1276: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.02 
2024-02-06 00:56:29,170 EPOCH 1277
2024-02-06 00:56:31,147 [Epoch: 1277 Step: 00043400] Batch Recognition Loss:   0.000904 => Gls Tokens per Sec:     2591 || Batch Translation Loss:   0.022092 => Txt Tokens per Sec:     7352 || Lr: 0.000100
2024-02-06 00:56:33,405 Epoch 1277: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.93 
2024-02-06 00:56:33,406 EPOCH 1278
2024-02-06 00:56:37,798 Epoch 1278: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.85 
2024-02-06 00:56:37,799 EPOCH 1279
2024-02-06 00:56:42,634 Epoch 1279: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.37 
2024-02-06 00:56:42,634 EPOCH 1280
2024-02-06 00:56:44,468 [Epoch: 1280 Step: 00043500] Batch Recognition Loss:   0.000726 => Gls Tokens per Sec:     2302 || Batch Translation Loss:   0.077132 => Txt Tokens per Sec:     6498 || Lr: 0.000100
2024-02-06 00:56:46,816 Epoch 1280: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.59 
2024-02-06 00:56:46,816 EPOCH 1281
2024-02-06 00:56:51,820 Epoch 1281: Total Training Recognition Loss 0.04  Total Training Translation Loss 5.71 
2024-02-06 00:56:51,821 EPOCH 1282
2024-02-06 00:56:56,509 Epoch 1282: Total Training Recognition Loss 0.15  Total Training Translation Loss 6.62 
2024-02-06 00:56:56,510 EPOCH 1283
2024-02-06 00:56:57,957 [Epoch: 1283 Step: 00043600] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:     2653 || Batch Translation Loss:   0.042650 => Txt Tokens per Sec:     7117 || Lr: 0.000100
2024-02-06 00:57:01,304 Epoch 1283: Total Training Recognition Loss 0.83  Total Training Translation Loss 2.25 
2024-02-06 00:57:01,304 EPOCH 1284
2024-02-06 00:57:06,223 Epoch 1284: Total Training Recognition Loss 1.87  Total Training Translation Loss 1.78 
2024-02-06 00:57:06,224 EPOCH 1285
2024-02-06 00:57:11,045 Epoch 1285: Total Training Recognition Loss 2.45  Total Training Translation Loss 1.64 
2024-02-06 00:57:11,046 EPOCH 1286
2024-02-06 00:57:12,193 [Epoch: 1286 Step: 00043700] Batch Recognition Loss:   0.000274 => Gls Tokens per Sec:     2567 || Batch Translation Loss:   0.023341 => Txt Tokens per Sec:     7235 || Lr: 0.000100
2024-02-06 00:57:15,172 Epoch 1286: Total Training Recognition Loss 2.32  Total Training Translation Loss 1.07 
2024-02-06 00:57:15,172 EPOCH 1287
2024-02-06 00:57:19,781 Epoch 1287: Total Training Recognition Loss 0.20  Total Training Translation Loss 0.83 
2024-02-06 00:57:19,782 EPOCH 1288
2024-02-06 00:57:24,833 Epoch 1288: Total Training Recognition Loss 0.12  Total Training Translation Loss 0.81 
2024-02-06 00:57:24,834 EPOCH 1289
2024-02-06 00:57:25,760 [Epoch: 1289 Step: 00043800] Batch Recognition Loss:   0.000678 => Gls Tokens per Sec:     2773 || Batch Translation Loss:   0.007020 => Txt Tokens per Sec:     6645 || Lr: 0.000100
2024-02-06 00:57:29,701 Epoch 1289: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.97 
2024-02-06 00:57:29,701 EPOCH 1290
2024-02-06 00:57:34,411 Epoch 1290: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.63 
2024-02-06 00:57:34,411 EPOCH 1291
2024-02-06 00:57:39,343 Epoch 1291: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.58 
2024-02-06 00:57:39,344 EPOCH 1292
2024-02-06 00:57:40,274 [Epoch: 1292 Step: 00043900] Batch Recognition Loss:   0.000831 => Gls Tokens per Sec:     2066 || Batch Translation Loss:   0.014892 => Txt Tokens per Sec:     6264 || Lr: 0.000100
2024-02-06 00:57:43,969 Epoch 1292: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.56 
2024-02-06 00:57:43,970 EPOCH 1293
2024-02-06 00:57:48,846 Epoch 1293: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.54 
2024-02-06 00:57:48,846 EPOCH 1294
2024-02-06 00:57:53,589 Epoch 1294: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.53 
2024-02-06 00:57:53,589 EPOCH 1295
2024-02-06 00:57:53,975 [Epoch: 1295 Step: 00044000] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     3320 || Batch Translation Loss:   0.010381 => Txt Tokens per Sec:     7936 || Lr: 0.000100
2024-02-06 00:58:02,769 Validation result at epoch 1295, step    44000: duration: 8.7934s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.05552	Translation Loss: 94673.21875	PPL: 12784.31836
	Eval Metric: BLEU
	WER 2.40	(DEL: 0.00,	INS: 0.00,	SUB: 2.40)
	BLEU-4 0.47	(BLEU-1: 10.13,	BLEU-2: 2.81,	BLEU-3: 1.04,	BLEU-4: 0.47)
	CHRF 16.62	ROUGE 8.81
2024-02-06 00:58:02,770 Logging Recognition and Translation Outputs
2024-02-06 00:58:02,770 ========================================================================================================================
2024-02-06 00:58:02,770 Logging Sequence: 57_104.00
2024-02-06 00:58:02,771 	Gloss Reference :	A B+C+D+E
2024-02-06 00:58:02,771 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:58:02,771 	Gloss Alignment :	         
2024-02-06 00:58:02,771 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:58:02,773 	Text Reference  :	*** the next day kohli and kl rahul continued from  where they  had left and   displayed amazing batting performance without losing  their wickets
2024-02-06 00:58:02,774 	Text Hypothesis :	for the **** *** ***** *** ** ***** ********* first time  india won the  world cup       while   each    other       in      colombo sri   lanka  
2024-02-06 00:58:02,774 	Text Alignment  :	I       D    D   D     D   D  D     D         S     S     S     S   S    S     S         S       S       S           S       S       S     S      
2024-02-06 00:58:02,774 ========================================================================================================================
2024-02-06 00:58:02,774 Logging Sequence: 136_64.00
2024-02-06 00:58:02,774 	Gloss Reference :	A B+C+D+E
2024-02-06 00:58:02,774 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:58:02,774 	Gloss Alignment :	         
2024-02-06 00:58:02,775 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:58:02,775 	Text Reference  :	* ** **** ***** in all she   has won       2  medals
2024-02-06 00:58:02,775 	Text Hypothesis :	i am very sorry it was never my  intention to bowl  
2024-02-06 00:58:02,776 	Text Alignment  :	I I  I    I     S  S   S     S   S         S  S     
2024-02-06 00:58:02,776 ========================================================================================================================
2024-02-06 00:58:02,776 Logging Sequence: 54_123.00
2024-02-06 00:58:02,776 	Gloss Reference :	A B+C+D+E
2024-02-06 00:58:02,776 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:58:02,776 	Gloss Alignment :	         
2024-02-06 00:58:02,776 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:58:02,777 	Text Reference  :	vips sponsors international cricket groups have    already booked their hotel rooms
2024-02-06 00:58:02,777 	Text Hypothesis :	**** ******** there         are     no     clarity over    when   the   hotel *****
2024-02-06 00:58:02,777 	Text Alignment  :	D    D        S             S       S      S       S       S      S           D    
2024-02-06 00:58:02,777 ========================================================================================================================
2024-02-06 00:58:02,778 Logging Sequence: 168_115.00
2024-02-06 00:58:02,778 	Gloss Reference :	A B+C+D+E
2024-02-06 00:58:02,778 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:58:02,778 	Gloss Alignment :	         
2024-02-06 00:58:02,778 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:58:02,779 	Text Reference  :	****** **** this    has sparked a   major discussion on    social     media
2024-02-06 00:58:02,779 	Text Hypothesis :	people were shocked to  see     the game  that       their respective teams
2024-02-06 00:58:02,779 	Text Alignment  :	I      I    S       S   S       S   S     S          S     S          S    
2024-02-06 00:58:02,779 ========================================================================================================================
2024-02-06 00:58:02,779 Logging Sequence: 121_132.00
2024-02-06 00:58:02,780 	Gloss Reference :	A B+C+D+E
2024-02-06 00:58:02,780 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 00:58:02,780 	Gloss Alignment :	         
2024-02-06 00:58:02,780 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 00:58:02,781 	Text Reference  :	which is why they will be      retesting her to   check if  she consumed any stamina enhancing drugs
2024-02-06 00:58:02,781 	Text Hypothesis :	***** ** *** **** now  finally he        is  part of    the cwg lawn     be  held    in        july 
2024-02-06 00:58:02,782 	Text Alignment  :	D     D  D   D    S    S       S         S   S    S     S   S   S        S   S       S         S    
2024-02-06 00:58:02,782 ========================================================================================================================
2024-02-06 00:58:07,167 Epoch 1295: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-06 00:58:07,168 EPOCH 1296
2024-02-06 00:58:12,045 Epoch 1296: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.46 
2024-02-06 00:58:12,046 EPOCH 1297
2024-02-06 00:58:16,367 Epoch 1297: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-06 00:58:16,367 EPOCH 1298
2024-02-06 00:58:16,505 [Epoch: 1298 Step: 00044100] Batch Recognition Loss:   0.002972 => Gls Tokens per Sec:     4672 || Batch Translation Loss:   0.007812 => Txt Tokens per Sec:    10015 || Lr: 0.000050
2024-02-06 00:58:21,192 Epoch 1298: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.44 
2024-02-06 00:58:21,193 EPOCH 1299
2024-02-06 00:58:25,628 Epoch 1299: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.43 
2024-02-06 00:58:25,629 EPOCH 1300
2024-02-06 00:58:30,286 [Epoch: 1300 Step: 00044200] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:     2281 || Batch Translation Loss:   0.008997 => Txt Tokens per Sec:     6310 || Lr: 0.000050
2024-02-06 00:58:30,287 Epoch 1300: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 00:58:30,287 EPOCH 1301
2024-02-06 00:58:34,858 Epoch 1301: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 00:58:34,858 EPOCH 1302
2024-02-06 00:58:39,431 Epoch 1302: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.41 
2024-02-06 00:58:39,432 EPOCH 1303
2024-02-06 00:58:44,067 [Epoch: 1303 Step: 00044300] Batch Recognition Loss:   0.000537 => Gls Tokens per Sec:     2154 || Batch Translation Loss:   0.014841 => Txt Tokens per Sec:     6122 || Lr: 0.000050
2024-02-06 00:58:44,197 Epoch 1303: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.40 
2024-02-06 00:58:44,198 EPOCH 1304
2024-02-06 00:58:48,470 Epoch 1304: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 00:58:48,471 EPOCH 1305
2024-02-06 00:58:53,368 Epoch 1305: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 00:58:53,368 EPOCH 1306
2024-02-06 00:58:57,151 [Epoch: 1306 Step: 00044400] Batch Recognition Loss:   0.000531 => Gls Tokens per Sec:     2469 || Batch Translation Loss:   0.011077 => Txt Tokens per Sec:     6965 || Lr: 0.000050
2024-02-06 00:58:57,454 Epoch 1306: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.40 
2024-02-06 00:58:57,455 EPOCH 1307
2024-02-06 00:59:02,484 Epoch 1307: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-06 00:59:02,485 EPOCH 1308
2024-02-06 00:59:06,782 Epoch 1308: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.38 
2024-02-06 00:59:06,783 EPOCH 1309
2024-02-06 00:59:10,813 [Epoch: 1309 Step: 00044500] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     2159 || Batch Translation Loss:   0.012202 => Txt Tokens per Sec:     5922 || Lr: 0.000050
2024-02-06 00:59:11,662 Epoch 1309: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.37 
2024-02-06 00:59:11,662 EPOCH 1310
2024-02-06 00:59:16,028 Epoch 1310: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.39 
2024-02-06 00:59:16,029 EPOCH 1311
2024-02-06 00:59:20,783 Epoch 1311: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.38 
2024-02-06 00:59:20,783 EPOCH 1312
2024-02-06 00:59:24,276 [Epoch: 1312 Step: 00044600] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     2308 || Batch Translation Loss:   0.010179 => Txt Tokens per Sec:     6527 || Lr: 0.000050
2024-02-06 00:59:25,231 Epoch 1312: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 00:59:25,232 EPOCH 1313
2024-02-06 00:59:29,850 Epoch 1313: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-06 00:59:29,851 EPOCH 1314
2024-02-06 00:59:34,470 Epoch 1314: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 00:59:34,470 EPOCH 1315
2024-02-06 00:59:37,217 [Epoch: 1315 Step: 00044700] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     2797 || Batch Translation Loss:   0.015278 => Txt Tokens per Sec:     7531 || Lr: 0.000050
2024-02-06 00:59:38,891 Epoch 1315: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 00:59:38,891 EPOCH 1316
2024-02-06 00:59:43,663 Epoch 1316: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 00:59:43,663 EPOCH 1317
2024-02-06 00:59:47,967 Epoch 1317: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-06 00:59:47,967 EPOCH 1318
2024-02-06 00:59:51,208 [Epoch: 1318 Step: 00044800] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     2092 || Batch Translation Loss:   0.010141 => Txt Tokens per Sec:     5891 || Lr: 0.000050
2024-02-06 00:59:52,876 Epoch 1318: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 00:59:52,876 EPOCH 1319
2024-02-06 00:59:57,120 Epoch 1319: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 00:59:57,121 EPOCH 1320
2024-02-06 01:00:02,106 Epoch 1320: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-06 01:00:02,107 EPOCH 1321
2024-02-06 01:00:04,691 [Epoch: 1321 Step: 00044900] Batch Recognition Loss:   0.000098 => Gls Tokens per Sec:     2478 || Batch Translation Loss:   0.010743 => Txt Tokens per Sec:     6775 || Lr: 0.000050
2024-02-06 01:00:06,325 Epoch 1321: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.43 
2024-02-06 01:00:06,325 EPOCH 1322
2024-02-06 01:00:11,176 Epoch 1322: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 01:00:11,176 EPOCH 1323
2024-02-06 01:00:15,557 Epoch 1323: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-06 01:00:15,557 EPOCH 1324
2024-02-06 01:00:17,884 [Epoch: 1324 Step: 00045000] Batch Recognition Loss:   0.000125 => Gls Tokens per Sec:     2366 || Batch Translation Loss:   0.009635 => Txt Tokens per Sec:     6732 || Lr: 0.000050
2024-02-06 01:00:20,311 Epoch 1324: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-06 01:00:20,311 EPOCH 1325
2024-02-06 01:00:24,840 Epoch 1325: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.45 
2024-02-06 01:00:24,840 EPOCH 1326
2024-02-06 01:00:29,407 Epoch 1326: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 01:00:29,408 EPOCH 1327
2024-02-06 01:00:31,441 [Epoch: 1327 Step: 00045100] Batch Recognition Loss:   0.000083 => Gls Tokens per Sec:     2392 || Batch Translation Loss:   0.009958 => Txt Tokens per Sec:     6434 || Lr: 0.000050
2024-02-06 01:00:34,069 Epoch 1327: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 01:00:34,069 EPOCH 1328
2024-02-06 01:00:38,609 Epoch 1328: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-06 01:00:38,609 EPOCH 1329
2024-02-06 01:00:43,339 Epoch 1329: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-06 01:00:43,340 EPOCH 1330
2024-02-06 01:00:44,845 [Epoch: 1330 Step: 00045200] Batch Recognition Loss:   0.000166 => Gls Tokens per Sec:     2978 || Batch Translation Loss:   0.031304 => Txt Tokens per Sec:     7874 || Lr: 0.000050
2024-02-06 01:00:47,640 Epoch 1330: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.60 
2024-02-06 01:00:47,640 EPOCH 1331
2024-02-06 01:00:52,542 Epoch 1331: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-06 01:00:52,542 EPOCH 1332
2024-02-06 01:00:56,713 Epoch 1332: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-06 01:00:56,714 EPOCH 1333
2024-02-06 01:00:58,297 [Epoch: 1333 Step: 00045300] Batch Recognition Loss:   0.000096 => Gls Tokens per Sec:     2262 || Batch Translation Loss:   0.013356 => Txt Tokens per Sec:     5762 || Lr: 0.000050
2024-02-06 01:01:01,771 Epoch 1333: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-06 01:01:01,771 EPOCH 1334
2024-02-06 01:01:06,600 Epoch 1334: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-06 01:01:06,600 EPOCH 1335
2024-02-06 01:01:11,395 Epoch 1335: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.16 
2024-02-06 01:01:11,395 EPOCH 1336
2024-02-06 01:01:12,549 [Epoch: 1336 Step: 00045400] Batch Recognition Loss:   0.000189 => Gls Tokens per Sec:     2550 || Batch Translation Loss:   0.032204 => Txt Tokens per Sec:     6995 || Lr: 0.000050
2024-02-06 01:01:15,984 Epoch 1336: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.55 
2024-02-06 01:01:15,985 EPOCH 1337
2024-02-06 01:01:20,862 Epoch 1337: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-06 01:01:20,863 EPOCH 1338
2024-02-06 01:01:25,060 Epoch 1338: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.76 
2024-02-06 01:01:25,060 EPOCH 1339
2024-02-06 01:01:25,794 [Epoch: 1339 Step: 00045500] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     3496 || Batch Translation Loss:   0.018681 => Txt Tokens per Sec:     8264 || Lr: 0.000050
2024-02-06 01:01:29,914 Epoch 1339: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-06 01:01:29,915 EPOCH 1340
2024-02-06 01:01:34,310 Epoch 1340: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-06 01:01:34,310 EPOCH 1341
2024-02-06 01:01:38,946 Epoch 1341: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-06 01:01:38,946 EPOCH 1342
2024-02-06 01:01:39,767 [Epoch: 1342 Step: 00045600] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     2344 || Batch Translation Loss:   0.013003 => Txt Tokens per Sec:     6538 || Lr: 0.000050
2024-02-06 01:01:43,565 Epoch 1342: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-06 01:01:43,565 EPOCH 1343
2024-02-06 01:01:48,165 Epoch 1343: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.70 
2024-02-06 01:01:48,165 EPOCH 1344
2024-02-06 01:01:52,875 Epoch 1344: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-06 01:01:52,876 EPOCH 1345
2024-02-06 01:01:53,279 [Epoch: 1345 Step: 00045700] Batch Recognition Loss:   0.000152 => Gls Tokens per Sec:     3176 || Batch Translation Loss:   0.011402 => Txt Tokens per Sec:     7983 || Lr: 0.000050
2024-02-06 01:01:57,339 Epoch 1345: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-06 01:01:57,339 EPOCH 1346
2024-02-06 01:02:02,178 Epoch 1346: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 01:02:02,179 EPOCH 1347
2024-02-06 01:02:06,450 Epoch 1347: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.11 
2024-02-06 01:02:06,451 EPOCH 1348
2024-02-06 01:02:06,803 [Epoch: 1348 Step: 00045800] Batch Recognition Loss:   0.000143 => Gls Tokens per Sec:     1829 || Batch Translation Loss:   0.013708 => Txt Tokens per Sec:     5389 || Lr: 0.000050
2024-02-06 01:02:11,415 Epoch 1348: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.75 
2024-02-06 01:02:11,415 EPOCH 1349
2024-02-06 01:02:15,562 Epoch 1349: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-06 01:02:15,562 EPOCH 1350
2024-02-06 01:02:20,475 [Epoch: 1350 Step: 00045900] Batch Recognition Loss:   0.000245 => Gls Tokens per Sec:     2163 || Batch Translation Loss:   0.018587 => Txt Tokens per Sec:     5983 || Lr: 0.000050
2024-02-06 01:02:20,475 Epoch 1350: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-06 01:02:20,476 EPOCH 1351
2024-02-06 01:02:24,788 Epoch 1351: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-06 01:02:24,788 EPOCH 1352
2024-02-06 01:02:29,729 Epoch 1352: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.48 
2024-02-06 01:02:29,729 EPOCH 1353
2024-02-06 01:02:33,892 [Epoch: 1353 Step: 00046000] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     2399 || Batch Translation Loss:   0.017991 => Txt Tokens per Sec:     6701 || Lr: 0.000050
2024-02-06 01:02:42,626 Validation result at epoch 1353, step    46000: duration: 8.7342s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.09474	Translation Loss: 93311.10938	PPL: 11158.15918
	Eval Metric: BLEU
	WER 2.68	(DEL: 0.00,	INS: 0.00,	SUB: 2.68)
	BLEU-4 0.61	(BLEU-1: 10.50,	BLEU-2: 3.05,	BLEU-3: 1.26,	BLEU-4: 0.61)
	CHRF 17.00	ROUGE 8.90
2024-02-06 01:02:42,627 Logging Recognition and Translation Outputs
2024-02-06 01:02:42,627 ========================================================================================================================
2024-02-06 01:02:42,627 Logging Sequence: 87_207.00
2024-02-06 01:02:42,627 	Gloss Reference :	A B+C+D+E
2024-02-06 01:02:42,628 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:02:42,628 	Gloss Alignment :	         
2024-02-06 01:02:42,628 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:02:42,629 	Text Reference  :	**** *********** ** there were    2-3  pakistanis who  were speaking anti-india things and things on *** ****** kashmir
2024-02-06 01:02:42,629 	Text Hypothesis :	this information on been  sharing your story      like pubg call     of         duty   and others on his mobile ipad   
2024-02-06 01:02:42,629 	Text Alignment  :	I    I           I  S     S       S    S          S    S    S        S          S          S         I   I      S      
2024-02-06 01:02:42,630 ========================================================================================================================
2024-02-06 01:02:42,630 Logging Sequence: 67_73.00
2024-02-06 01:02:42,630 	Gloss Reference :	A B+C+D+E
2024-02-06 01:02:42,630 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:02:42,630 	Gloss Alignment :	         
2024-02-06 01:02:42,630 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:02:42,631 	Text Reference  :	*** in    his     tweet he      also said   
2024-02-06 01:02:42,631 	Text Hypothesis :	the match started its   batting and  england
2024-02-06 01:02:42,631 	Text Alignment  :	I   S     S       S     S       S    S      
2024-02-06 01:02:42,631 ========================================================================================================================
2024-02-06 01:02:42,631 Logging Sequence: 172_267.00
2024-02-06 01:02:42,631 	Gloss Reference :	A B+C+D+E
2024-02-06 01:02:42,631 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:02:42,632 	Gloss Alignment :	         
2024-02-06 01:02:42,632 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:02:42,632 	Text Reference  :	*** ***** ********* *** *** such provisions have been made    
2024-02-06 01:02:42,632 	Text Hypothesis :	and media personnel are all wear clothes    will be   lifelong
2024-02-06 01:02:42,632 	Text Alignment  :	I   I     I         I   I   S    S          S    S    S       
2024-02-06 01:02:42,633 ========================================================================================================================
2024-02-06 01:02:42,633 Logging Sequence: 144_23.00
2024-02-06 01:02:42,633 	Gloss Reference :	A B+C+D+E
2024-02-06 01:02:42,633 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:02:42,633 	Gloss Alignment :	         
2024-02-06 01:02:42,633 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:02:42,635 	Text Reference  :	the girl is    14-year-old mumal mehar and she is     from kanasar village of  barmer in  rajasthan
2024-02-06 01:02:42,635 	Text Hypothesis :	*** **** singh also        said  it    was a   screen test before  the     big final  and japan    
2024-02-06 01:02:42,635 	Text Alignment  :	D   D    S     S           S     S     S   S   S      S    S       S       S   S      S   S        
2024-02-06 01:02:42,635 ========================================================================================================================
2024-02-06 01:02:42,635 Logging Sequence: 133_202.00
2024-02-06 01:02:42,636 	Gloss Reference :	A B+C+D+E
2024-02-06 01:02:42,636 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:02:42,636 	Gloss Alignment :	         
2024-02-06 01:02:42,636 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:02:42,637 	Text Reference  :	australia has already qualified for the       final   if   india   wins it  will face australia
2024-02-06 01:02:42,637 	Text Hypothesis :	********* *** ******* ********* *** pakistani batsmen kept scoring run  but lost 5    times    
2024-02-06 01:02:42,637 	Text Alignment  :	D         D   D       D         D   S         S       S    S       S    S   S    S    S        
2024-02-06 01:02:42,637 ========================================================================================================================
2024-02-06 01:02:42,838 Epoch 1353: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-06 01:02:42,838 EPOCH 1354
2024-02-06 01:02:47,890 Epoch 1354: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 01:02:47,891 EPOCH 1355
2024-02-06 01:02:52,453 Epoch 1355: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 01:02:52,453 EPOCH 1356
2024-02-06 01:02:56,124 [Epoch: 1356 Step: 00046100] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:     2545 || Batch Translation Loss:   0.012875 => Txt Tokens per Sec:     7034 || Lr: 0.000050
2024-02-06 01:02:56,710 Epoch 1356: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.42 
2024-02-06 01:02:56,710 EPOCH 1357
2024-02-06 01:03:01,592 Epoch 1357: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 01:03:01,592 EPOCH 1358
2024-02-06 01:03:05,833 Epoch 1358: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 01:03:05,834 EPOCH 1359
2024-02-06 01:03:09,638 [Epoch: 1359 Step: 00046200] Batch Recognition Loss:   0.000111 => Gls Tokens per Sec:     2289 || Batch Translation Loss:   0.014535 => Txt Tokens per Sec:     6150 || Lr: 0.000050
2024-02-06 01:03:10,830 Epoch 1359: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 01:03:10,830 EPOCH 1360
2024-02-06 01:03:14,997 Epoch 1360: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 01:03:14,997 EPOCH 1361
2024-02-06 01:03:19,939 Epoch 1361: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 01:03:19,939 EPOCH 1362
2024-02-06 01:03:23,631 [Epoch: 1362 Step: 00046300] Batch Recognition Loss:   0.000906 => Gls Tokens per Sec:     2255 || Batch Translation Loss:   0.015507 => Txt Tokens per Sec:     6264 || Lr: 0.000050
2024-02-06 01:03:24,621 Epoch 1362: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 01:03:24,621 EPOCH 1363
2024-02-06 01:03:29,447 Epoch 1363: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 01:03:29,448 EPOCH 1364
2024-02-06 01:03:34,057 Epoch 1364: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.60 
2024-02-06 01:03:34,058 EPOCH 1365
2024-02-06 01:03:37,424 [Epoch: 1365 Step: 00046400] Batch Recognition Loss:   0.000102 => Gls Tokens per Sec:     2206 || Batch Translation Loss:   0.015120 => Txt Tokens per Sec:     6048 || Lr: 0.000050
2024-02-06 01:03:38,970 Epoch 1365: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.11 
2024-02-06 01:03:38,971 EPOCH 1366
2024-02-06 01:03:43,541 Epoch 1366: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.42 
2024-02-06 01:03:43,541 EPOCH 1367
2024-02-06 01:03:48,496 Epoch 1367: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-06 01:03:48,496 EPOCH 1368
2024-02-06 01:03:51,471 [Epoch: 1368 Step: 00046500] Batch Recognition Loss:   0.000396 => Gls Tokens per Sec:     2281 || Batch Translation Loss:   0.034719 => Txt Tokens per Sec:     6438 || Lr: 0.000050
2024-02-06 01:03:53,013 Epoch 1368: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-06 01:03:53,014 EPOCH 1369
2024-02-06 01:03:58,119 Epoch 1369: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-06 01:03:58,120 EPOCH 1370
2024-02-06 01:04:02,963 Epoch 1370: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-06 01:04:02,964 EPOCH 1371
2024-02-06 01:04:05,353 [Epoch: 1371 Step: 00046600] Batch Recognition Loss:   0.000175 => Gls Tokens per Sec:     2679 || Batch Translation Loss:   0.013491 => Txt Tokens per Sec:     7321 || Lr: 0.000050
2024-02-06 01:04:07,076 Epoch 1371: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-06 01:04:07,076 EPOCH 1372
2024-02-06 01:04:11,839 Epoch 1372: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-06 01:04:11,840 EPOCH 1373
2024-02-06 01:04:16,508 Epoch 1373: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.60 
2024-02-06 01:04:16,508 EPOCH 1374
2024-02-06 01:04:19,465 [Epoch: 1374 Step: 00046700] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     1949 || Batch Translation Loss:   0.034850 => Txt Tokens per Sec:     5563 || Lr: 0.000050
2024-02-06 01:04:21,493 Epoch 1374: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-06 01:04:21,493 EPOCH 1375
2024-02-06 01:04:25,913 Epoch 1375: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-06 01:04:25,913 EPOCH 1376
2024-02-06 01:04:30,481 Epoch 1376: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 01:04:30,482 EPOCH 1377
2024-02-06 01:04:32,466 [Epoch: 1377 Step: 00046800] Batch Recognition Loss:   0.000209 => Gls Tokens per Sec:     2582 || Batch Translation Loss:   0.012766 => Txt Tokens per Sec:     6763 || Lr: 0.000050
2024-02-06 01:04:35,185 Epoch 1377: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-06 01:04:35,186 EPOCH 1378
2024-02-06 01:04:39,554 Epoch 1378: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.60 
2024-02-06 01:04:39,555 EPOCH 1379
2024-02-06 01:04:44,345 Epoch 1379: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.87 
2024-02-06 01:04:44,345 EPOCH 1380
2024-02-06 01:04:45,900 [Epoch: 1380 Step: 00046900] Batch Recognition Loss:   0.000163 => Gls Tokens per Sec:     2882 || Batch Translation Loss:   0.015791 => Txt Tokens per Sec:     7758 || Lr: 0.000050
2024-02-06 01:04:48,495 Epoch 1380: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.09 
2024-02-06 01:04:48,496 EPOCH 1381
2024-02-06 01:04:53,509 Epoch 1381: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.34 
2024-02-06 01:04:53,510 EPOCH 1382
2024-02-06 01:04:57,725 Epoch 1382: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.15 
2024-02-06 01:04:57,725 EPOCH 1383
2024-02-06 01:04:59,313 [Epoch: 1383 Step: 00047000] Batch Recognition Loss:   0.000208 => Gls Tokens per Sec:     2421 || Batch Translation Loss:   0.036428 => Txt Tokens per Sec:     6851 || Lr: 0.000050
2024-02-06 01:05:02,607 Epoch 1383: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.18 
2024-02-06 01:05:02,608 EPOCH 1384
2024-02-06 01:05:06,950 Epoch 1384: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.15 
2024-02-06 01:05:06,950 EPOCH 1385
2024-02-06 01:05:11,733 Epoch 1385: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-06 01:05:11,734 EPOCH 1386
2024-02-06 01:05:13,008 [Epoch: 1386 Step: 00047100] Batch Recognition Loss:   0.000326 => Gls Tokens per Sec:     2514 || Batch Translation Loss:   0.025783 => Txt Tokens per Sec:     6992 || Lr: 0.000050
2024-02-06 01:05:16,210 Epoch 1386: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.01 
2024-02-06 01:05:16,210 EPOCH 1387
2024-02-06 01:05:20,913 Epoch 1387: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.73 
2024-02-06 01:05:20,914 EPOCH 1388
2024-02-06 01:05:25,454 Epoch 1388: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-06 01:05:25,455 EPOCH 1389
2024-02-06 01:05:26,536 [Epoch: 1389 Step: 00047200] Batch Recognition Loss:   0.000540 => Gls Tokens per Sec:     2372 || Batch Translation Loss:   0.029464 => Txt Tokens per Sec:     6142 || Lr: 0.000050
2024-02-06 01:05:29,992 Epoch 1389: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-06 01:05:29,993 EPOCH 1390
2024-02-06 01:05:34,734 Epoch 1390: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.03 
2024-02-06 01:05:34,734 EPOCH 1391
2024-02-06 01:05:39,038 Epoch 1391: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-06 01:05:39,038 EPOCH 1392
2024-02-06 01:05:40,198 [Epoch: 1392 Step: 00047300] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     1658 || Batch Translation Loss:   0.022652 => Txt Tokens per Sec:     4943 || Lr: 0.000050
2024-02-06 01:05:44,006 Epoch 1392: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.18 
2024-02-06 01:05:44,006 EPOCH 1393
2024-02-06 01:05:48,086 Epoch 1393: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-06 01:05:48,087 EPOCH 1394
2024-02-06 01:05:53,038 Epoch 1394: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-06 01:05:53,038 EPOCH 1395
2024-02-06 01:05:53,363 [Epoch: 1395 Step: 00047400] Batch Recognition Loss:   0.000311 => Gls Tokens per Sec:     3148 || Batch Translation Loss:   0.002954 => Txt Tokens per Sec:     6463 || Lr: 0.000050
2024-02-06 01:05:57,262 Epoch 1395: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-06 01:05:57,262 EPOCH 1396
2024-02-06 01:06:02,121 Epoch 1396: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 01:06:02,121 EPOCH 1397
2024-02-06 01:06:06,769 Epoch 1397: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-06 01:06:06,769 EPOCH 1398
2024-02-06 01:06:07,012 [Epoch: 1398 Step: 00047500] Batch Recognition Loss:   0.000373 => Gls Tokens per Sec:     2656 || Batch Translation Loss:   0.014356 => Txt Tokens per Sec:     7519 || Lr: 0.000050
2024-02-06 01:06:11,689 Epoch 1398: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-06 01:06:11,690 EPOCH 1399
2024-02-06 01:06:15,989 Epoch 1399: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-06 01:06:15,989 EPOCH 1400
2024-02-06 01:06:20,687 [Epoch: 1400 Step: 00047600] Batch Recognition Loss:   0.000279 => Gls Tokens per Sec:     2261 || Batch Translation Loss:   0.005418 => Txt Tokens per Sec:     6255 || Lr: 0.000050
2024-02-06 01:06:20,688 Epoch 1400: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-06 01:06:20,688 EPOCH 1401
2024-02-06 01:06:25,333 Epoch 1401: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-06 01:06:25,334 EPOCH 1402
2024-02-06 01:06:29,865 Epoch 1402: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-06 01:06:29,865 EPOCH 1403
2024-02-06 01:06:34,289 [Epoch: 1403 Step: 00047700] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:     2257 || Batch Translation Loss:   0.016478 => Txt Tokens per Sec:     6197 || Lr: 0.000050
2024-02-06 01:06:34,607 Epoch 1403: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-06 01:06:34,607 EPOCH 1404
2024-02-06 01:06:39,149 Epoch 1404: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-06 01:06:39,149 EPOCH 1405
2024-02-06 01:06:43,859 Epoch 1405: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-06 01:06:43,859 EPOCH 1406
2024-02-06 01:06:47,756 [Epoch: 1406 Step: 00047800] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     2397 || Batch Translation Loss:   0.010733 => Txt Tokens per Sec:     6655 || Lr: 0.000050
2024-02-06 01:06:48,255 Epoch 1406: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 01:06:48,255 EPOCH 1407
2024-02-06 01:06:53,094 Epoch 1407: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.45 
2024-02-06 01:06:53,094 EPOCH 1408
2024-02-06 01:06:57,220 Epoch 1408: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 01:06:57,220 EPOCH 1409
2024-02-06 01:07:01,294 [Epoch: 1409 Step: 00047900] Batch Recognition Loss:   0.000423 => Gls Tokens per Sec:     2201 || Batch Translation Loss:   0.004289 => Txt Tokens per Sec:     5983 || Lr: 0.000050
2024-02-06 01:07:02,245 Epoch 1409: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-06 01:07:02,245 EPOCH 1410
2024-02-06 01:07:06,486 Epoch 1410: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-06 01:07:06,486 EPOCH 1411
2024-02-06 01:07:11,330 Epoch 1411: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-06 01:07:11,330 EPOCH 1412
2024-02-06 01:07:14,655 [Epoch: 1412 Step: 00048000] Batch Recognition Loss:   0.000105 => Gls Tokens per Sec:     2425 || Batch Translation Loss:   0.024362 => Txt Tokens per Sec:     6752 || Lr: 0.000050
2024-02-06 01:07:23,551 Validation result at epoch 1412, step    48000: duration: 8.8957s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.16978	Translation Loss: 94163.46094	PPL: 12149.70020
	Eval Metric: BLEU
	WER 2.68	(DEL: 0.00,	INS: 0.00,	SUB: 2.68)
	BLEU-4 0.52	(BLEU-1: 9.79,	BLEU-2: 2.75,	BLEU-3: 1.08,	BLEU-4: 0.52)
	CHRF 16.62	ROUGE 8.28
2024-02-06 01:07:23,552 Logging Recognition and Translation Outputs
2024-02-06 01:07:23,552 ========================================================================================================================
2024-02-06 01:07:23,553 Logging Sequence: 96_93.00
2024-02-06 01:07:23,553 	Gloss Reference :	A B+C+D+E
2024-02-06 01:07:23,553 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:07:23,553 	Gloss Alignment :	         
2024-02-06 01:07:23,553 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:07:23,554 	Text Reference  :	bhuvneshwar kumar took 4   wickets and    hardik pandya took 3    wickets  wonderful
2024-02-06 01:07:23,554 	Text Hypothesis :	*********** this  is   why the     finals were   played well with pakistan cricket  
2024-02-06 01:07:23,554 	Text Alignment  :	D           S     S    S   S       S      S      S      S    S    S        S        
2024-02-06 01:07:23,555 ========================================================================================================================
2024-02-06 01:07:23,555 Logging Sequence: 144_2.00
2024-02-06 01:07:23,555 	Gloss Reference :	A B+C+D+E  
2024-02-06 01:07:23,555 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-06 01:07:23,555 	Gloss Alignment :	  S        
2024-02-06 01:07:23,556 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:07:23,557 	Text Reference  :	a girl posted a     video of herself  playing cricket on  a        village      farm    on social media the video has gone viral
2024-02-06 01:07:23,557 	Text Hypothesis :	* **** ****** after being in lockdown for     months  the football championship started on ****** ***** *** ***** *** 11th june 
2024-02-06 01:07:23,557 	Text Alignment  :	D D    D      S     S     S  S        S       S       S   S        S            S          D      D     D   D     D   S    S    
2024-02-06 01:07:23,558 ========================================================================================================================
2024-02-06 01:07:23,558 Logging Sequence: 178_83.00
2024-02-06 01:07:23,558 	Gloss Reference :	A B+C+D+E
2024-02-06 01:07:23,558 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:07:23,558 	Gloss Alignment :	         
2024-02-06 01:07:23,559 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:07:23,559 	Text Reference  :	and the   police    still haven't     apprehended the wrestler  
2024-02-06 01:07:23,560 	Text Hypothesis :	and media personnel are   immediately after       the tournament
2024-02-06 01:07:23,560 	Text Alignment  :	    S     S         S     S           S               S         
2024-02-06 01:07:23,560 ========================================================================================================================
2024-02-06 01:07:23,560 Logging Sequence: 169_214.00
2024-02-06 01:07:23,560 	Gloss Reference :	A B+C+D+E
2024-02-06 01:07:23,560 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:07:23,561 	Gloss Alignment :	         
2024-02-06 01:07:23,561 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:07:23,562 	Text Reference  :	virat kohli said that though arshdeep dropped the       catch    he          is still  a    strong part   of  the    indian     team
2024-02-06 01:07:23,562 	Text Hypothesis :	***** you   know that ****** ******** ******* wikipedia provides information on celebs like their  height age family background etc 
2024-02-06 01:07:23,563 	Text Alignment  :	D     S     S         D      D        D       S         S        S           S  S      S    S      S      S   S      S          S   
2024-02-06 01:07:23,563 ========================================================================================================================
2024-02-06 01:07:23,563 Logging Sequence: 147_202.00
2024-02-06 01:07:23,563 	Gloss Reference :	A B+C+D+E
2024-02-06 01:07:23,563 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:07:23,563 	Gloss Alignment :	         
2024-02-06 01:07:23,564 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:07:23,566 	Text Reference  :	*** ******* ******* were **** impressed   that she  took    the difficult decision to withdraw from the olympics and focus on   her  mental health
2024-02-06 01:07:23,566 	Text Hypothesis :	new zealand players were seen celebrating and  then lifting the ********* ******** ** trophy   for  the ******** *** first time have a      look  
2024-02-06 01:07:23,566 	Text Alignment  :	I   I       I            I    S           S    S    S           D         D        D  S        S        D        D   S     S    S    S      S     
2024-02-06 01:07:23,566 ========================================================================================================================
2024-02-06 01:07:24,722 Epoch 1412: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.34 
2024-02-06 01:07:24,722 EPOCH 1413
2024-02-06 01:07:29,722 Epoch 1413: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.53 
2024-02-06 01:07:29,723 EPOCH 1414
2024-02-06 01:07:34,296 Epoch 1414: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.51 
2024-02-06 01:07:34,296 EPOCH 1415
2024-02-06 01:07:37,353 [Epoch: 1415 Step: 00048100] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:     2428 || Batch Translation Loss:   0.020099 => Txt Tokens per Sec:     6673 || Lr: 0.000050
2024-02-06 01:07:38,969 Epoch 1415: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.70 
2024-02-06 01:07:38,969 EPOCH 1416
2024-02-06 01:07:43,645 Epoch 1416: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-06 01:07:43,645 EPOCH 1417
2024-02-06 01:07:48,176 Epoch 1417: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.87 
2024-02-06 01:07:48,176 EPOCH 1418
2024-02-06 01:07:51,490 [Epoch: 1418 Step: 00048200] Batch Recognition Loss:   0.000588 => Gls Tokens per Sec:     2047 || Batch Translation Loss:   0.023748 => Txt Tokens per Sec:     5699 || Lr: 0.000050
2024-02-06 01:07:52,951 Epoch 1418: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-06 01:07:52,951 EPOCH 1419
2024-02-06 01:07:57,218 Epoch 1419: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.77 
2024-02-06 01:07:57,218 EPOCH 1420
2024-02-06 01:08:02,141 Epoch 1420: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-06 01:08:02,141 EPOCH 1421
2024-02-06 01:08:04,644 [Epoch: 1421 Step: 00048300] Batch Recognition Loss:   0.000312 => Gls Tokens per Sec:     2455 || Batch Translation Loss:   0.012165 => Txt Tokens per Sec:     6958 || Lr: 0.000050
2024-02-06 01:08:06,278 Epoch 1421: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-06 01:08:06,278 EPOCH 1422
2024-02-06 01:08:11,308 Epoch 1422: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 01:08:11,308 EPOCH 1423
2024-02-06 01:08:15,569 Epoch 1423: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 01:08:15,569 EPOCH 1424
2024-02-06 01:08:17,628 [Epoch: 1424 Step: 00048400] Batch Recognition Loss:   0.000107 => Gls Tokens per Sec:     2799 || Batch Translation Loss:   0.009272 => Txt Tokens per Sec:     7211 || Lr: 0.000050
2024-02-06 01:08:20,594 Epoch 1424: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 01:08:20,595 EPOCH 1425
2024-02-06 01:08:25,148 Epoch 1425: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-06 01:08:25,149 EPOCH 1426
2024-02-06 01:08:29,418 Epoch 1426: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-06 01:08:29,418 EPOCH 1427
2024-02-06 01:08:31,993 [Epoch: 1427 Step: 00048500] Batch Recognition Loss:   0.000509 => Gls Tokens per Sec:     1989 || Batch Translation Loss:   0.021151 => Txt Tokens per Sec:     5532 || Lr: 0.000050
2024-02-06 01:08:34,364 Epoch 1427: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 01:08:34,365 EPOCH 1428
2024-02-06 01:08:39,224 Epoch 1428: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 01:08:39,224 EPOCH 1429
2024-02-06 01:08:44,075 Epoch 1429: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 01:08:44,076 EPOCH 1430
2024-02-06 01:08:45,732 [Epoch: 1430 Step: 00048600] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     2709 || Batch Translation Loss:   0.010753 => Txt Tokens per Sec:     7676 || Lr: 0.000050
2024-02-06 01:08:48,155 Epoch 1430: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 01:08:48,156 EPOCH 1431
2024-02-06 01:08:53,025 Epoch 1431: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-06 01:08:53,026 EPOCH 1432
2024-02-06 01:08:57,445 Epoch 1432: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.22 
2024-02-06 01:08:57,445 EPOCH 1433
2024-02-06 01:08:58,855 [Epoch: 1433 Step: 00048700] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:     2541 || Batch Translation Loss:   0.019562 => Txt Tokens per Sec:     7138 || Lr: 0.000050
2024-02-06 01:09:02,257 Epoch 1433: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.96 
2024-02-06 01:09:02,258 EPOCH 1434
2024-02-06 01:09:06,955 Epoch 1434: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.60 
2024-02-06 01:09:06,955 EPOCH 1435
2024-02-06 01:09:12,031 Epoch 1435: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-06 01:09:12,031 EPOCH 1436
2024-02-06 01:09:13,338 [Epoch: 1436 Step: 00048800] Batch Recognition Loss:   0.000163 => Gls Tokens per Sec:     2451 || Batch Translation Loss:   0.013489 => Txt Tokens per Sec:     6616 || Lr: 0.000050
2024-02-06 01:09:16,775 Epoch 1436: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-06 01:09:16,775 EPOCH 1437
2024-02-06 01:09:20,914 Epoch 1437: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-06 01:09:20,915 EPOCH 1438
2024-02-06 01:09:25,857 Epoch 1438: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-06 01:09:25,858 EPOCH 1439
2024-02-06 01:09:26,907 [Epoch: 1439 Step: 00048900] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     2442 || Batch Translation Loss:   0.020496 => Txt Tokens per Sec:     6181 || Lr: 0.000050
2024-02-06 01:09:30,281 Epoch 1439: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.77 
2024-02-06 01:09:30,281 EPOCH 1440
2024-02-06 01:09:35,242 Epoch 1440: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-06 01:09:35,242 EPOCH 1441
2024-02-06 01:09:39,419 Epoch 1441: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-06 01:09:39,419 EPOCH 1442
2024-02-06 01:09:40,089 [Epoch: 1442 Step: 00049000] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     2869 || Batch Translation Loss:   0.026622 => Txt Tokens per Sec:     8031 || Lr: 0.000050
2024-02-06 01:09:44,207 Epoch 1442: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-06 01:09:44,207 EPOCH 1443
2024-02-06 01:09:48,700 Epoch 1443: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 01:09:48,700 EPOCH 1444
2024-02-06 01:09:53,306 Epoch 1444: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 01:09:53,306 EPOCH 1445
2024-02-06 01:09:54,077 [Epoch: 1445 Step: 00049100] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     1662 || Batch Translation Loss:   0.012184 => Txt Tokens per Sec:     5185 || Lr: 0.000050
2024-02-06 01:09:57,927 Epoch 1445: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 01:09:57,927 EPOCH 1446
2024-02-06 01:10:02,457 Epoch 1446: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-06 01:10:02,457 EPOCH 1447
2024-02-06 01:10:07,245 Epoch 1447: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-06 01:10:07,245 EPOCH 1448
2024-02-06 01:10:07,419 [Epoch: 1448 Step: 00049200] Batch Recognition Loss:   0.004963 => Gls Tokens per Sec:     3699 || Batch Translation Loss:   0.030850 => Txt Tokens per Sec:     9122 || Lr: 0.000050
2024-02-06 01:10:11,666 Epoch 1448: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.05 
2024-02-06 01:10:11,667 EPOCH 1449
2024-02-06 01:10:16,700 Epoch 1449: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.33 
2024-02-06 01:10:16,701 EPOCH 1450
2024-02-06 01:10:20,935 [Epoch: 1450 Step: 00049300] Batch Recognition Loss:   0.000678 => Gls Tokens per Sec:     2509 || Batch Translation Loss:   0.086281 => Txt Tokens per Sec:     6942 || Lr: 0.000050
2024-02-06 01:10:20,935 Epoch 1450: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.87 
2024-02-06 01:10:20,935 EPOCH 1451
2024-02-06 01:10:25,711 Epoch 1451: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.53 
2024-02-06 01:10:25,712 EPOCH 1452
2024-02-06 01:10:30,176 Epoch 1452: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.94 
2024-02-06 01:10:30,176 EPOCH 1453
2024-02-06 01:10:34,395 [Epoch: 1453 Step: 00049400] Batch Recognition Loss:   0.000157 => Gls Tokens per Sec:     2366 || Batch Translation Loss:   0.017132 => Txt Tokens per Sec:     6506 || Lr: 0.000050
2024-02-06 01:10:34,744 Epoch 1453: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-06 01:10:34,744 EPOCH 1454
2024-02-06 01:10:39,393 Epoch 1454: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-06 01:10:39,393 EPOCH 1455
2024-02-06 01:10:43,495 Epoch 1455: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-06 01:10:43,495 EPOCH 1456
2024-02-06 01:10:47,852 [Epoch: 1456 Step: 00049500] Batch Recognition Loss:   0.000224 => Gls Tokens per Sec:     2144 || Batch Translation Loss:   0.021859 => Txt Tokens per Sec:     5853 || Lr: 0.000050
2024-02-06 01:10:48,472 Epoch 1456: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.76 
2024-02-06 01:10:48,472 EPOCH 1457
2024-02-06 01:10:52,783 Epoch 1457: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.04 
2024-02-06 01:10:52,783 EPOCH 1458
2024-02-06 01:10:57,657 Epoch 1458: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.87 
2024-02-06 01:10:57,657 EPOCH 1459
2024-02-06 01:11:01,518 [Epoch: 1459 Step: 00049600] Batch Recognition Loss:   0.000213 => Gls Tokens per Sec:     2254 || Batch Translation Loss:   0.024004 => Txt Tokens per Sec:     6455 || Lr: 0.000050
2024-02-06 01:11:02,043 Epoch 1459: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-06 01:11:02,043 EPOCH 1460
2024-02-06 01:11:06,806 Epoch 1460: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-06 01:11:06,807 EPOCH 1461
2024-02-06 01:11:11,289 Epoch 1461: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.99 
2024-02-06 01:11:11,289 EPOCH 1462
2024-02-06 01:11:14,434 [Epoch: 1462 Step: 00049700] Batch Recognition Loss:   0.000892 => Gls Tokens per Sec:     2564 || Batch Translation Loss:   0.039248 => Txt Tokens per Sec:     6877 || Lr: 0.000050
2024-02-06 01:11:15,904 Epoch 1462: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.13 
2024-02-06 01:11:15,905 EPOCH 1463
2024-02-06 01:11:20,574 Epoch 1463: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-06 01:11:20,574 EPOCH 1464
2024-02-06 01:11:24,948 Epoch 1464: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.64 
2024-02-06 01:11:24,949 EPOCH 1465
2024-02-06 01:11:28,340 [Epoch: 1465 Step: 00049800] Batch Recognition Loss:   0.000420 => Gls Tokens per Sec:     2189 || Batch Translation Loss:   0.052872 => Txt Tokens per Sec:     5985 || Lr: 0.000050
2024-02-06 01:11:29,779 Epoch 1465: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.02 
2024-02-06 01:11:29,779 EPOCH 1466
2024-02-06 01:11:34,014 Epoch 1466: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.90 
2024-02-06 01:11:34,014 EPOCH 1467
2024-02-06 01:11:38,930 Epoch 1467: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.99 
2024-02-06 01:11:38,931 EPOCH 1468
2024-02-06 01:11:41,731 [Epoch: 1468 Step: 00049900] Batch Recognition Loss:   0.000533 => Gls Tokens per Sec:     2515 || Batch Translation Loss:   0.053699 => Txt Tokens per Sec:     6999 || Lr: 0.000050
2024-02-06 01:11:43,110 Epoch 1468: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.58 
2024-02-06 01:11:43,110 EPOCH 1469
2024-02-06 01:11:47,906 Epoch 1469: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.99 
2024-02-06 01:11:47,907 EPOCH 1470
2024-02-06 01:11:52,317 Epoch 1470: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-06 01:11:52,317 EPOCH 1471
2024-02-06 01:11:54,808 [Epoch: 1471 Step: 00050000] Batch Recognition Loss:   0.000387 => Gls Tokens per Sec:     2466 || Batch Translation Loss:   0.028416 => Txt Tokens per Sec:     6704 || Lr: 0.000050
2024-02-06 01:12:03,330 Validation result at epoch 1471, step    50000: duration: 8.5206s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.19205	Translation Loss: 94714.31250	PPL: 12836.90039
	Eval Metric: BLEU
	WER 3.04	(DEL: 0.00,	INS: 0.00,	SUB: 3.04)
	BLEU-4 0.31	(BLEU-1: 9.88,	BLEU-2: 2.67,	BLEU-3: 0.86,	BLEU-4: 0.31)
	CHRF 16.71	ROUGE 8.01
2024-02-06 01:12:03,331 Logging Recognition and Translation Outputs
2024-02-06 01:12:03,331 ========================================================================================================================
2024-02-06 01:12:03,331 Logging Sequence: 178_157.00
2024-02-06 01:12:03,331 	Gloss Reference :	A B+C+D+E
2024-02-06 01:12:03,332 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:12:03,332 	Gloss Alignment :	         
2024-02-06 01:12:03,332 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:12:03,332 	Text Reference  :	this is   why sushil kumar will have to       be      arrested 
2024-02-06 01:12:03,333 	Text Hypothesis :	**** that her win    had   a    huge argument between argentina
2024-02-06 01:12:03,333 	Text Alignment  :	D    S    S   S      S     S    S    S        S       S        
2024-02-06 01:12:03,333 ========================================================================================================================
2024-02-06 01:12:03,333 Logging Sequence: 118_111.00
2024-02-06 01:12:03,333 	Gloss Reference :	A B+C+D+E
2024-02-06 01:12:03,333 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:12:03,334 	Gloss Alignment :	         
2024-02-06 01:12:03,334 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:12:03,334 	Text Reference  :	and people encourage him have hope for the  next      world cup 
2024-02-06 01:12:03,334 	Text Hypothesis :	*** people ********* *** **** **** *** were jubiliant over  this
2024-02-06 01:12:03,334 	Text Alignment  :	D          D         D   D    D    D   S    S         S     S   
2024-02-06 01:12:03,334 ========================================================================================================================
2024-02-06 01:12:03,335 Logging Sequence: 148_2.00
2024-02-06 01:12:03,335 	Gloss Reference :	A B+C+D+E
2024-02-06 01:12:03,335 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:12:03,335 	Gloss Alignment :	         
2024-02-06 01:12:03,335 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:12:03,337 	Text Reference  :	the final of the asia  cup 2023 cricket tournament was played between india       and ******** sri   lanka on 17th  september   2023     
2024-02-06 01:12:03,337 	Text Hypothesis :	*** ***** ** the world cup will held    in         uae deaf   cricket association and pakistan could score of royal challengers bangalore
2024-02-06 01:12:03,337 	Text Alignment  :	D   D     D      S         S    S       S          S   S      S       S               I        S     S     S  S     S           S        
2024-02-06 01:12:03,338 ========================================================================================================================
2024-02-06 01:12:03,338 Logging Sequence: 83_129.00
2024-02-06 01:12:03,338 	Gloss Reference :	A B+C+D+E
2024-02-06 01:12:03,338 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:12:03,338 	Gloss Alignment :	         
2024-02-06 01:12:03,338 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:12:03,339 	Text Reference  :	later the denmark football association tweeted
2024-02-06 01:12:03,339 	Text Hypothesis :	***** a   new     zealand  went        viral  
2024-02-06 01:12:03,339 	Text Alignment  :	D     S   S       S        S           S      
2024-02-06 01:12:03,339 ========================================================================================================================
2024-02-06 01:12:03,339 Logging Sequence: 99_158.00
2024-02-06 01:12:03,339 	Gloss Reference :	A B+C+D+E
2024-02-06 01:12:03,339 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:12:03,340 	Gloss Alignment :	         
2024-02-06 01:12:03,340 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:12:03,340 	Text Reference  :	*** the  incident occured in  dubai and  it **** was extremely shameful  
2024-02-06 01:12:03,341 	Text Hypothesis :	his fans want     to      see him   play it will be  very      protective
2024-02-06 01:12:03,341 	Text Alignment  :	I   S    S        S       S   S     S       I    S   S         S         
2024-02-06 01:12:03,341 ========================================================================================================================
2024-02-06 01:12:05,557 Epoch 1471: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.76 
2024-02-06 01:12:05,558 EPOCH 1472
2024-02-06 01:12:10,499 Epoch 1472: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-06 01:12:10,499 EPOCH 1473
2024-02-06 01:12:14,633 Epoch 1473: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-06 01:12:14,634 EPOCH 1474
2024-02-06 01:12:17,036 [Epoch: 1474 Step: 00050100] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:     2290 || Batch Translation Loss:   0.012436 => Txt Tokens per Sec:     6724 || Lr: 0.000050
2024-02-06 01:12:18,678 Epoch 1474: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.60 
2024-02-06 01:12:18,678 EPOCH 1475
2024-02-06 01:12:23,413 Epoch 1475: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-06 01:12:23,414 EPOCH 1476
2024-02-06 01:12:27,921 Epoch 1476: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.82 
2024-02-06 01:12:27,921 EPOCH 1477
2024-02-06 01:12:30,024 [Epoch: 1477 Step: 00050200] Batch Recognition Loss:   0.000348 => Gls Tokens per Sec:     2436 || Batch Translation Loss:   0.011757 => Txt Tokens per Sec:     6695 || Lr: 0.000050
2024-02-06 01:12:32,599 Epoch 1477: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-06 01:12:32,600 EPOCH 1478
2024-02-06 01:12:37,142 Epoch 1478: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-06 01:12:37,143 EPOCH 1479
2024-02-06 01:12:41,715 Epoch 1479: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-06 01:12:41,715 EPOCH 1480
2024-02-06 01:12:44,083 [Epoch: 1480 Step: 00050300] Batch Recognition Loss:   0.000180 => Gls Tokens per Sec:     1893 || Batch Translation Loss:   0.015660 => Txt Tokens per Sec:     5687 || Lr: 0.000050
2024-02-06 01:12:46,400 Epoch 1480: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 01:12:46,400 EPOCH 1481
2024-02-06 01:12:50,832 Epoch 1481: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-06 01:12:50,832 EPOCH 1482
2024-02-06 01:12:55,648 Epoch 1482: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-06 01:12:55,648 EPOCH 1483
2024-02-06 01:12:57,447 [Epoch: 1483 Step: 00050400] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     2137 || Batch Translation Loss:   0.010691 => Txt Tokens per Sec:     6519 || Lr: 0.000050
2024-02-06 01:12:59,831 Epoch 1483: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.65 
2024-02-06 01:12:59,832 EPOCH 1484
2024-02-06 01:13:04,862 Epoch 1484: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 01:13:04,863 EPOCH 1485
2024-02-06 01:13:09,084 Epoch 1485: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 01:13:09,084 EPOCH 1486
2024-02-06 01:13:10,563 [Epoch: 1486 Step: 00050500] Batch Recognition Loss:   0.000208 => Gls Tokens per Sec:     2166 || Batch Translation Loss:   0.017307 => Txt Tokens per Sec:     5964 || Lr: 0.000050
2024-02-06 01:13:14,043 Epoch 1486: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 01:13:14,043 EPOCH 1487
2024-02-06 01:13:18,326 Epoch 1487: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 01:13:18,326 EPOCH 1488
2024-02-06 01:13:23,155 Epoch 1488: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-06 01:13:23,155 EPOCH 1489
2024-02-06 01:13:24,106 [Epoch: 1489 Step: 00050600] Batch Recognition Loss:   0.000319 => Gls Tokens per Sec:     2422 || Batch Translation Loss:   0.016481 => Txt Tokens per Sec:     6754 || Lr: 0.000050
2024-02-06 01:13:27,682 Epoch 1489: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-06 01:13:27,683 EPOCH 1490
2024-02-06 01:13:32,363 Epoch 1490: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-06 01:13:32,363 EPOCH 1491
2024-02-06 01:13:36,957 Epoch 1491: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-06 01:13:36,958 EPOCH 1492
2024-02-06 01:13:37,652 [Epoch: 1492 Step: 00050700] Batch Recognition Loss:   0.000350 => Gls Tokens per Sec:     2775 || Batch Translation Loss:   0.012045 => Txt Tokens per Sec:     7747 || Lr: 0.000050
2024-02-06 01:13:41,524 Epoch 1492: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 01:13:41,524 EPOCH 1493
2024-02-06 01:13:46,178 Epoch 1493: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-06 01:13:46,178 EPOCH 1494
2024-02-06 01:13:50,593 Epoch 1494: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-06 01:13:50,594 EPOCH 1495
2024-02-06 01:13:51,128 [Epoch: 1495 Step: 00050800] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:     2402 || Batch Translation Loss:   0.012383 => Txt Tokens per Sec:     6578 || Lr: 0.000050
2024-02-06 01:13:55,437 Epoch 1495: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.77 
2024-02-06 01:13:55,438 EPOCH 1496
2024-02-06 01:13:59,635 Epoch 1496: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-06 01:13:59,635 EPOCH 1497
2024-02-06 01:14:04,596 Epoch 1497: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-06 01:14:04,597 EPOCH 1498
2024-02-06 01:14:04,883 [Epoch: 1498 Step: 00050900] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:     2238 || Batch Translation Loss:   0.012467 => Txt Tokens per Sec:     6654 || Lr: 0.000050
2024-02-06 01:14:09,244 Epoch 1498: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-06 01:14:09,244 EPOCH 1499
2024-02-06 01:14:14,198 Epoch 1499: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-06 01:14:14,199 EPOCH 1500
2024-02-06 01:14:18,860 [Epoch: 1500 Step: 00051000] Batch Recognition Loss:   0.000467 => Gls Tokens per Sec:     2280 || Batch Translation Loss:   0.011121 => Txt Tokens per Sec:     6307 || Lr: 0.000050
2024-02-06 01:14:18,860 Epoch 1500: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-06 01:14:18,860 EPOCH 1501
2024-02-06 01:14:23,783 Epoch 1501: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 01:14:23,784 EPOCH 1502
2024-02-06 01:14:27,991 Epoch 1502: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.56 
2024-02-06 01:14:27,991 EPOCH 1503
2024-02-06 01:14:32,524 [Epoch: 1503 Step: 00051100] Batch Recognition Loss:   0.000515 => Gls Tokens per Sec:     2202 || Batch Translation Loss:   0.021364 => Txt Tokens per Sec:     6050 || Lr: 0.000050
2024-02-06 01:14:32,820 Epoch 1503: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-06 01:14:32,821 EPOCH 1504
2024-02-06 01:14:37,249 Epoch 1504: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.67 
2024-02-06 01:14:37,249 EPOCH 1505
2024-02-06 01:14:41,770 Epoch 1505: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.35 
2024-02-06 01:14:41,770 EPOCH 1506
2024-02-06 01:14:46,132 [Epoch: 1506 Step: 00051200] Batch Recognition Loss:   0.000317 => Gls Tokens per Sec:     2142 || Batch Translation Loss:   0.091014 => Txt Tokens per Sec:     6042 || Lr: 0.000050
2024-02-06 01:14:46,493 Epoch 1506: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.39 
2024-02-06 01:14:46,493 EPOCH 1507
2024-02-06 01:14:50,750 Epoch 1507: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.44 
2024-02-06 01:14:50,751 EPOCH 1508
2024-02-06 01:14:55,668 Epoch 1508: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.07 
2024-02-06 01:14:55,668 EPOCH 1509
2024-02-06 01:14:59,588 [Epoch: 1509 Step: 00051300] Batch Recognition Loss:   0.000294 => Gls Tokens per Sec:     2220 || Batch Translation Loss:   0.005084 => Txt Tokens per Sec:     6134 || Lr: 0.000050
2024-02-06 01:15:00,527 Epoch 1509: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-06 01:15:00,528 EPOCH 1510
2024-02-06 01:15:05,302 Epoch 1510: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-06 01:15:05,302 EPOCH 1511
2024-02-06 01:15:09,424 Epoch 1511: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-06 01:15:09,424 EPOCH 1512
2024-02-06 01:15:13,090 [Epoch: 1512 Step: 00051400] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:     2270 || Batch Translation Loss:   0.012331 => Txt Tokens per Sec:     6243 || Lr: 0.000050
2024-02-06 01:15:14,248 Epoch 1512: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-06 01:15:14,249 EPOCH 1513
2024-02-06 01:15:18,692 Epoch 1513: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-06 01:15:18,692 EPOCH 1514
2024-02-06 01:15:23,372 Epoch 1514: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-06 01:15:23,372 EPOCH 1515
2024-02-06 01:15:26,517 [Epoch: 1515 Step: 00051500] Batch Recognition Loss:   0.000250 => Gls Tokens per Sec:     2362 || Batch Translation Loss:   0.021064 => Txt Tokens per Sec:     6729 || Lr: 0.000050
2024-02-06 01:15:27,934 Epoch 1515: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.93 
2024-02-06 01:15:27,934 EPOCH 1516
2024-02-06 01:15:32,415 Epoch 1516: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.87 
2024-02-06 01:15:32,415 EPOCH 1517
2024-02-06 01:15:37,191 Epoch 1517: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.87 
2024-02-06 01:15:37,191 EPOCH 1518
2024-02-06 01:15:39,717 [Epoch: 1518 Step: 00051600] Batch Recognition Loss:   0.000335 => Gls Tokens per Sec:     2685 || Batch Translation Loss:   0.104909 => Txt Tokens per Sec:     7441 || Lr: 0.000050
2024-02-06 01:15:41,486 Epoch 1518: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.00 
2024-02-06 01:15:41,486 EPOCH 1519
2024-02-06 01:15:46,408 Epoch 1519: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-06 01:15:46,408 EPOCH 1520
2024-02-06 01:15:50,579 Epoch 1520: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-06 01:15:50,579 EPOCH 1521
2024-02-06 01:15:53,502 [Epoch: 1521 Step: 00051700] Batch Recognition Loss:   0.000271 => Gls Tokens per Sec:     2101 || Batch Translation Loss:   0.019197 => Txt Tokens per Sec:     5905 || Lr: 0.000050
2024-02-06 01:15:55,601 Epoch 1521: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-06 01:15:55,601 EPOCH 1522
2024-02-06 01:15:59,837 Epoch 1522: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 01:15:59,837 EPOCH 1523
2024-02-06 01:16:03,946 Epoch 1523: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-06 01:16:03,946 EPOCH 1524
2024-02-06 01:16:06,482 [Epoch: 1524 Step: 00051800] Batch Recognition Loss:   0.000282 => Gls Tokens per Sec:     2170 || Batch Translation Loss:   0.019704 => Txt Tokens per Sec:     6312 || Lr: 0.000050
2024-02-06 01:16:08,067 Epoch 1524: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-06 01:16:08,068 EPOCH 1525
2024-02-06 01:16:12,173 Epoch 1525: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.70 
2024-02-06 01:16:12,173 EPOCH 1526
2024-02-06 01:16:17,015 Epoch 1526: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-06 01:16:17,016 EPOCH 1527
2024-02-06 01:16:19,176 [Epoch: 1527 Step: 00051900] Batch Recognition Loss:   0.002213 => Gls Tokens per Sec:     2251 || Batch Translation Loss:   0.013898 => Txt Tokens per Sec:     6358 || Lr: 0.000050
2024-02-06 01:16:21,362 Epoch 1527: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-06 01:16:21,362 EPOCH 1528
2024-02-06 01:16:25,911 Epoch 1528: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-06 01:16:25,912 EPOCH 1529
2024-02-06 01:16:30,638 Epoch 1529: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-06 01:16:30,638 EPOCH 1530
2024-02-06 01:16:32,435 [Epoch: 1530 Step: 00052000] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     2494 || Batch Translation Loss:   0.022440 => Txt Tokens per Sec:     7145 || Lr: 0.000050
2024-02-06 01:16:41,174 Validation result at epoch 1530, step    52000: duration: 8.7385s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.25656	Translation Loss: 95879.35156	PPL: 14421.02832
	Eval Metric: BLEU
	WER 2.82	(DEL: 0.00,	INS: 0.00,	SUB: 2.82)
	BLEU-4 0.56	(BLEU-1: 9.77,	BLEU-2: 3.00,	BLEU-3: 1.20,	BLEU-4: 0.56)
	CHRF 16.55	ROUGE 8.68
2024-02-06 01:16:41,175 Logging Recognition and Translation Outputs
2024-02-06 01:16:41,175 ========================================================================================================================
2024-02-06 01:16:41,175 Logging Sequence: 59_101.00
2024-02-06 01:16:41,176 	Gloss Reference :	A B+C+D+E
2024-02-06 01:16:41,176 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:16:41,176 	Gloss Alignment :	         
2024-02-06 01:16:41,176 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:16:41,178 	Text Reference  :	did you see the video fox said she won  her      medals because of a       condom and  is    very     happy 
2024-02-06 01:16:41,178 	Text Hypothesis :	*** *** in  the ***** *** **** *** 2022 athletes good   luck    in trouble at     each other domestic events
2024-02-06 01:16:41,178 	Text Alignment  :	D   D   S       D     D   D    D   S    S        S      S       S  S       S      S    S     S        S     
2024-02-06 01:16:41,178 ========================================================================================================================
2024-02-06 01:16:41,178 Logging Sequence: 103_112.00
2024-02-06 01:16:41,178 	Gloss Reference :	A B+C+D+E
2024-02-06 01:16:41,178 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:16:41,179 	Gloss Alignment :	         
2024-02-06 01:16:41,179 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:16:41,179 	Text Reference  :	you are aware that  earlier the britishers had colonized a lot of countries in the world
2024-02-06 01:16:41,180 	Text Hypothesis :	*** *** ***** there is      the ********** *** ********* * *** ** richest   in the world
2024-02-06 01:16:41,180 	Text Alignment  :	D   D   D     S     S           D          D   D         D D   D  S                     
2024-02-06 01:16:41,180 ========================================================================================================================
2024-02-06 01:16:41,180 Logging Sequence: 143_11.00
2024-02-06 01:16:41,180 	Gloss Reference :	A B+C+D+E
2024-02-06 01:16:41,180 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:16:41,180 	Gloss Alignment :	         
2024-02-06 01:16:41,181 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:16:41,182 	Text Reference  :	ronaldo has also become the first person to have 500 million followers on      instagram he is the   most    loved    footballer
2024-02-06 01:16:41,182 	Text Hypothesis :	******* *** **** ****** the ***** ****** ** **** ban will    be        applied when      he ** joins another football club      
2024-02-06 01:16:41,182 	Text Alignment  :	D       D   D    D          D     D      D  D    S   S       S         S       S            D  S     S       S        S         
2024-02-06 01:16:41,182 ========================================================================================================================
2024-02-06 01:16:41,182 Logging Sequence: 183_23.00
2024-02-06 01:16:41,183 	Gloss Reference :	A B+C+D+E
2024-02-06 01:16:41,183 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:16:41,183 	Gloss Alignment :	         
2024-02-06 01:16:41,183 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:16:41,184 	Text Reference  :	however everybody has been waiting for  them to     announce the name   of      the   child
2024-02-06 01:16:41,184 	Text Hypothesis :	like    india     has **** ******* bcci a    source from     the afghan cricket board said 
2024-02-06 01:16:41,184 	Text Alignment  :	S       S             D    D       S    S    S      S            S      S       S     S    
2024-02-06 01:16:41,184 ========================================================================================================================
2024-02-06 01:16:41,184 Logging Sequence: 169_165.00
2024-02-06 01:16:41,185 	Gloss Reference :	A B+C+D+E
2024-02-06 01:16:41,185 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:16:41,185 	Gloss Alignment :	         
2024-02-06 01:16:41,185 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:16:41,187 	Text Reference  :	***** ** the indian government was  outraged by the  incident and  these    changes were   undone  by wikipedia
2024-02-06 01:16:41,187 	Text Hypothesis :	kohli is the ****** first      time india    to play an       easy watching the     women' cricket or ipad     
2024-02-06 01:16:41,187 	Text Alignment  :	I     I      D      S          S    S        S  S    S        S    S        S       S      S       S  S        
2024-02-06 01:16:41,187 ========================================================================================================================
2024-02-06 01:16:43,897 Epoch 1530: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-06 01:16:43,898 EPOCH 1531
2024-02-06 01:16:48,882 Epoch 1531: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-06 01:16:48,882 EPOCH 1532
2024-02-06 01:16:53,069 Epoch 1532: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-06 01:16:53,069 EPOCH 1533
2024-02-06 01:16:54,703 [Epoch: 1533 Step: 00052100] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:     2353 || Batch Translation Loss:   0.011642 => Txt Tokens per Sec:     6538 || Lr: 0.000050
2024-02-06 01:16:57,962 Epoch 1533: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-06 01:16:57,963 EPOCH 1534
2024-02-06 01:17:02,387 Epoch 1534: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.64 
2024-02-06 01:17:02,387 EPOCH 1535
2024-02-06 01:17:07,159 Epoch 1535: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.21 
2024-02-06 01:17:07,160 EPOCH 1536
2024-02-06 01:17:08,162 [Epoch: 1536 Step: 00052200] Batch Recognition Loss:   0.000228 => Gls Tokens per Sec:     2934 || Batch Translation Loss:   0.015779 => Txt Tokens per Sec:     7393 || Lr: 0.000050
2024-02-06 01:17:11,621 Epoch 1536: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.00 
2024-02-06 01:17:11,621 EPOCH 1537
2024-02-06 01:17:16,301 Epoch 1537: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-06 01:17:16,301 EPOCH 1538
2024-02-06 01:17:20,907 Epoch 1538: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-06 01:17:20,907 EPOCH 1539
2024-02-06 01:17:21,682 [Epoch: 1539 Step: 00052300] Batch Recognition Loss:   0.000486 => Gls Tokens per Sec:     2972 || Batch Translation Loss:   0.022813 => Txt Tokens per Sec:     7335 || Lr: 0.000050
2024-02-06 01:17:25,478 Epoch 1539: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-06 01:17:25,479 EPOCH 1540
2024-02-06 01:17:30,113 Epoch 1540: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-06 01:17:30,113 EPOCH 1541
2024-02-06 01:17:34,527 Epoch 1541: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.96 
2024-02-06 01:17:34,528 EPOCH 1542
2024-02-06 01:17:35,254 [Epoch: 1542 Step: 00052400] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:     2652 || Batch Translation Loss:   0.029458 => Txt Tokens per Sec:     7135 || Lr: 0.000050
2024-02-06 01:17:39,280 Epoch 1542: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.90 
2024-02-06 01:17:39,280 EPOCH 1543
2024-02-06 01:17:43,514 Epoch 1543: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.79 
2024-02-06 01:17:43,514 EPOCH 1544
2024-02-06 01:17:48,435 Epoch 1544: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-06 01:17:48,435 EPOCH 1545
2024-02-06 01:17:48,948 [Epoch: 1545 Step: 00052500] Batch Recognition Loss:   0.000276 => Gls Tokens per Sec:     2498 || Batch Translation Loss:   0.022796 => Txt Tokens per Sec:     7011 || Lr: 0.000050
2024-02-06 01:17:52,564 Epoch 1545: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.76 
2024-02-06 01:17:52,564 EPOCH 1546
2024-02-06 01:17:57,508 Epoch 1546: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.87 
2024-02-06 01:17:57,509 EPOCH 1547
2024-02-06 01:18:01,800 Epoch 1547: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-06 01:18:01,800 EPOCH 1548
2024-02-06 01:18:01,945 [Epoch: 1548 Step: 00052600] Batch Recognition Loss:   0.000152 => Gls Tokens per Sec:     4475 || Batch Translation Loss:   0.008336 => Txt Tokens per Sec:     9105 || Lr: 0.000050
2024-02-06 01:18:06,587 Epoch 1548: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-06 01:18:06,588 EPOCH 1549
2024-02-06 01:18:11,021 Epoch 1549: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-06 01:18:11,021 EPOCH 1550
2024-02-06 01:18:15,650 [Epoch: 1550 Step: 00052700] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     2295 || Batch Translation Loss:   0.014792 => Txt Tokens per Sec:     6348 || Lr: 0.000050
2024-02-06 01:18:15,651 Epoch 1550: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-06 01:18:15,651 EPOCH 1551
2024-02-06 01:18:20,239 Epoch 1551: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-06 01:18:20,239 EPOCH 1552
2024-02-06 01:18:24,785 Epoch 1552: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 01:18:24,785 EPOCH 1553
2024-02-06 01:18:29,283 [Epoch: 1553 Step: 00052800] Batch Recognition Loss:   0.000285 => Gls Tokens per Sec:     2219 || Batch Translation Loss:   0.011288 => Txt Tokens per Sec:     6139 || Lr: 0.000050
2024-02-06 01:18:29,571 Epoch 1553: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 01:18:29,571 EPOCH 1554
2024-02-06 01:18:33,981 Epoch 1554: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-06 01:18:33,982 EPOCH 1555
2024-02-06 01:18:38,929 Epoch 1555: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-06 01:18:38,929 EPOCH 1556
2024-02-06 01:18:42,511 [Epoch: 1556 Step: 00052900] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     2608 || Batch Translation Loss:   0.013515 => Txt Tokens per Sec:     7113 || Lr: 0.000050
2024-02-06 01:18:43,105 Epoch 1556: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 01:18:43,106 EPOCH 1557
2024-02-06 01:18:48,292 Epoch 1557: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-06 01:18:48,293 EPOCH 1558
2024-02-06 01:18:52,734 Epoch 1558: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-06 01:18:52,735 EPOCH 1559
2024-02-06 01:18:56,392 [Epoch: 1559 Step: 00053000] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     2380 || Batch Translation Loss:   0.012455 => Txt Tokens per Sec:     6726 || Lr: 0.000050
2024-02-06 01:18:57,017 Epoch 1559: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-06 01:18:57,017 EPOCH 1560
2024-02-06 01:19:01,920 Epoch 1560: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.74 
2024-02-06 01:19:01,920 EPOCH 1561
2024-02-06 01:19:06,077 Epoch 1561: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.96 
2024-02-06 01:19:06,077 EPOCH 1562
2024-02-06 01:19:10,009 [Epoch: 1562 Step: 00053100] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:     2051 || Batch Translation Loss:   0.025375 => Txt Tokens per Sec:     5726 || Lr: 0.000050
2024-02-06 01:19:11,090 Epoch 1562: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-06 01:19:11,091 EPOCH 1563
2024-02-06 01:19:15,320 Epoch 1563: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-06 01:19:15,320 EPOCH 1564
2024-02-06 01:19:20,141 Epoch 1564: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-06 01:19:20,142 EPOCH 1565
2024-02-06 01:19:23,153 [Epoch: 1565 Step: 00053200] Batch Recognition Loss:   0.000743 => Gls Tokens per Sec:     2465 || Batch Translation Loss:   0.018197 => Txt Tokens per Sec:     6797 || Lr: 0.000050
2024-02-06 01:19:24,489 Epoch 1565: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.60 
2024-02-06 01:19:24,489 EPOCH 1566
2024-02-06 01:19:29,247 Epoch 1566: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-06 01:19:29,248 EPOCH 1567
2024-02-06 01:19:33,753 Epoch 1567: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.40 
2024-02-06 01:19:33,754 EPOCH 1568
2024-02-06 01:19:36,500 [Epoch: 1568 Step: 00053300] Batch Recognition Loss:   0.001837 => Gls Tokens per Sec:     2469 || Batch Translation Loss:   0.029215 => Txt Tokens per Sec:     6743 || Lr: 0.000050
2024-02-06 01:19:38,178 Epoch 1568: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-06 01:19:38,179 EPOCH 1569
2024-02-06 01:19:42,994 Epoch 1569: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-06 01:19:42,995 EPOCH 1570
2024-02-06 01:19:47,163 Epoch 1570: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-06 01:19:47,163 EPOCH 1571
2024-02-06 01:19:50,361 [Epoch: 1571 Step: 00053400] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     1921 || Batch Translation Loss:   0.012386 => Txt Tokens per Sec:     5400 || Lr: 0.000050
2024-02-06 01:19:52,174 Epoch 1571: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.91 
2024-02-06 01:19:52,174 EPOCH 1572
2024-02-06 01:19:56,386 Epoch 1572: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.39 
2024-02-06 01:19:56,386 EPOCH 1573
2024-02-06 01:20:01,350 Epoch 1573: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.27 
2024-02-06 01:20:01,351 EPOCH 1574
2024-02-06 01:20:03,470 [Epoch: 1574 Step: 00053500] Batch Recognition Loss:   0.000613 => Gls Tokens per Sec:     2597 || Batch Translation Loss:   0.114855 => Txt Tokens per Sec:     6934 || Lr: 0.000050
2024-02-06 01:20:05,657 Epoch 1574: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.91 
2024-02-06 01:20:05,657 EPOCH 1575
2024-02-06 01:20:10,452 Epoch 1575: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.65 
2024-02-06 01:20:10,453 EPOCH 1576
2024-02-06 01:20:14,848 Epoch 1576: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.84 
2024-02-06 01:20:14,848 EPOCH 1577
2024-02-06 01:20:16,996 [Epoch: 1577 Step: 00053600] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     2385 || Batch Translation Loss:   0.025620 => Txt Tokens per Sec:     6674 || Lr: 0.000050
2024-02-06 01:20:19,580 Epoch 1577: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.26 
2024-02-06 01:20:19,581 EPOCH 1578
2024-02-06 01:20:24,106 Epoch 1578: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.99 
2024-02-06 01:20:24,107 EPOCH 1579
2024-02-06 01:20:28,611 Epoch 1579: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.81 
2024-02-06 01:20:28,612 EPOCH 1580
2024-02-06 01:20:30,178 [Epoch: 1580 Step: 00053700] Batch Recognition Loss:   0.000316 => Gls Tokens per Sec:     2697 || Batch Translation Loss:   0.017652 => Txt Tokens per Sec:     6699 || Lr: 0.000050
2024-02-06 01:20:33,359 Epoch 1580: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-06 01:20:33,360 EPOCH 1581
2024-02-06 01:20:37,714 Epoch 1581: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-06 01:20:37,715 EPOCH 1582
2024-02-06 01:20:42,555 Epoch 1582: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-06 01:20:42,555 EPOCH 1583
2024-02-06 01:20:43,733 [Epoch: 1583 Step: 00053800] Batch Recognition Loss:   0.000348 => Gls Tokens per Sec:     3264 || Batch Translation Loss:   0.015766 => Txt Tokens per Sec:     8247 || Lr: 0.000050
2024-02-06 01:20:46,694 Epoch 1583: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-06 01:20:46,694 EPOCH 1584
2024-02-06 01:20:51,713 Epoch 1584: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-06 01:20:51,714 EPOCH 1585
2024-02-06 01:20:55,988 Epoch 1585: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-06 01:20:55,988 EPOCH 1586
2024-02-06 01:20:57,167 [Epoch: 1586 Step: 00053900] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     2718 || Batch Translation Loss:   0.016959 => Txt Tokens per Sec:     7180 || Lr: 0.000050
2024-02-06 01:21:00,925 Epoch 1586: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 01:21:00,925 EPOCH 1587
2024-02-06 01:21:05,502 Epoch 1587: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-06 01:21:05,502 EPOCH 1588
2024-02-06 01:21:10,272 Epoch 1588: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-06 01:21:10,272 EPOCH 1589
2024-02-06 01:21:11,303 [Epoch: 1589 Step: 00054000] Batch Recognition Loss:   0.000180 => Gls Tokens per Sec:     2233 || Batch Translation Loss:   0.016733 => Txt Tokens per Sec:     6423 || Lr: 0.000050
2024-02-06 01:21:20,294 Validation result at epoch 1589, step    54000: duration: 8.9902s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.08401	Translation Loss: 95509.49219	PPL: 13898.01367
	Eval Metric: BLEU
	WER 2.68	(DEL: 0.00,	INS: 0.00,	SUB: 2.68)
	BLEU-4 0.45	(BLEU-1: 10.42,	BLEU-2: 2.85,	BLEU-3: 0.98,	BLEU-4: 0.45)
	CHRF 16.88	ROUGE 8.71
2024-02-06 01:21:20,295 Logging Recognition and Translation Outputs
2024-02-06 01:21:20,295 ========================================================================================================================
2024-02-06 01:21:20,295 Logging Sequence: 166_243.00
2024-02-06 01:21:20,296 	Gloss Reference :	A B+C+D+E
2024-02-06 01:21:20,296 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:21:20,296 	Gloss Alignment :	         
2024-02-06 01:21:20,296 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:21:20,297 	Text Reference  :	*** **** ********* ****** ******* *** ***** icc       worked with members  boards like bcci pcb   cricket australia etc 
2024-02-06 01:21:20,298 	Text Hypothesis :	the bcci president sourav ganguly and board secretary jay    shah welcomed the    two  new  teams to      the       team
2024-02-06 01:21:20,298 	Text Alignment  :	I   I    I         I      I       I   I     S         S      S    S        S      S    S    S     S       S         S   
2024-02-06 01:21:20,298 ========================================================================================================================
2024-02-06 01:21:20,298 Logging Sequence: 179_409.00
2024-02-06 01:21:20,298 	Gloss Reference :	A B+C+D+E
2024-02-06 01:21:20,298 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:21:20,298 	Gloss Alignment :	         
2024-02-06 01:21:20,299 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:21:20,300 	Text Reference  :	** *** ********* **** the ******** ******* *** *** *** * ***** passport was at the  wfi    office in      delhi
2024-02-06 01:21:20,300 	Text Hypothesis :	if she continued with the athletes however now and has a other team     and 3  days before the    bottles now  
2024-02-06 01:21:20,300 	Text Alignment  :	I  I   I         I        I        I       I   I   I   I I     S        S   S  S    S      S      S       S    
2024-02-06 01:21:20,300 ========================================================================================================================
2024-02-06 01:21:20,300 Logging Sequence: 81_407.00
2024-02-06 01:21:20,301 	Gloss Reference :	A B+C+D+E
2024-02-06 01:21:20,301 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:21:20,301 	Gloss Alignment :	         
2024-02-06 01:21:20,301 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:21:20,302 	Text Reference  :	******* the government company - national buildings construction corporation and they  will complete them    in   a  time-bound manner
2024-02-06 01:21:20,302 	Text Hypothesis :	however the ********** ******* * ******** ********* bcci         told        you don't that it's     players have to rhiti      sports
2024-02-06 01:21:20,303 	Text Alignment  :	I           D          D       D D        D         S            S           S   S     S    S        S       S    S  S          S     
2024-02-06 01:21:20,303 ========================================================================================================================
2024-02-06 01:21:20,303 Logging Sequence: 96_31.00
2024-02-06 01:21:20,303 	Gloss Reference :	A B+C+D+E
2024-02-06 01:21:20,303 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:21:20,303 	Gloss Alignment :	         
2024-02-06 01:21:20,303 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:21:20,304 	Text Reference  :	and then 2 teams will go on      to  play the final
2024-02-06 01:21:20,304 	Text Hypothesis :	*** **** * ***** **** ** however the next day that 
2024-02-06 01:21:20,304 	Text Alignment  :	D   D    D D     D    D  S       S   S    S   S    
2024-02-06 01:21:20,304 ========================================================================================================================
2024-02-06 01:21:20,305 Logging Sequence: 160_87.00
2024-02-06 01:21:20,305 	Gloss Reference :	A B+C+D+E
2024-02-06 01:21:20,305 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:21:20,305 	Gloss Alignment :	         
2024-02-06 01:21:20,305 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:21:20,306 	Text Reference  :	***** **** *** *** kohli held a  press conference and said
2024-02-06 01:21:20,306 	Text Hypothesis :	after this was the first time he was   glued      to  see 
2024-02-06 01:21:20,306 	Text Alignment  :	I     I    I   I   S     S    S  S     S          S   S   
2024-02-06 01:21:20,306 ========================================================================================================================
2024-02-06 01:21:24,085 Epoch 1589: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 01:21:24,085 EPOCH 1590
2024-02-06 01:21:29,085 Epoch 1590: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 01:21:29,085 EPOCH 1591
2024-02-06 01:21:33,313 Epoch 1591: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-06 01:21:33,314 EPOCH 1592
2024-02-06 01:21:33,851 [Epoch: 1592 Step: 00054100] Batch Recognition Loss:   0.000616 => Gls Tokens per Sec:     3575 || Batch Translation Loss:   0.005086 => Txt Tokens per Sec:     8549 || Lr: 0.000050
2024-02-06 01:21:38,016 Epoch 1592: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-06 01:21:38,017 EPOCH 1593
2024-02-06 01:21:42,565 Epoch 1593: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-06 01:21:42,565 EPOCH 1594
2024-02-06 01:21:47,123 Epoch 1594: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-06 01:21:47,123 EPOCH 1595
2024-02-06 01:21:47,487 [Epoch: 1595 Step: 00054200] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     3526 || Batch Translation Loss:   0.011491 => Txt Tokens per Sec:     8780 || Lr: 0.000050
2024-02-06 01:21:51,778 Epoch 1595: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.46 
2024-02-06 01:21:51,779 EPOCH 1596
2024-02-06 01:21:56,235 Epoch 1596: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-06 01:21:56,236 EPOCH 1597
2024-02-06 01:22:01,032 Epoch 1597: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 01:22:01,032 EPOCH 1598
2024-02-06 01:22:01,282 [Epoch: 1598 Step: 00054300] Batch Recognition Loss:   0.000128 => Gls Tokens per Sec:     2570 || Batch Translation Loss:   0.010407 => Txt Tokens per Sec:     7490 || Lr: 0.000050
2024-02-06 01:22:05,967 Epoch 1598: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-06 01:22:05,968 EPOCH 1599
2024-02-06 01:22:10,649 Epoch 1599: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-06 01:22:10,649 EPOCH 1600
2024-02-06 01:22:15,635 [Epoch: 1600 Step: 00054400] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:     2130 || Batch Translation Loss:   0.016552 => Txt Tokens per Sec:     5894 || Lr: 0.000050
2024-02-06 01:22:15,636 Epoch 1600: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 01:22:15,636 EPOCH 1601
2024-02-06 01:22:20,285 Epoch 1601: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-06 01:22:20,285 EPOCH 1602
2024-02-06 01:22:24,486 Epoch 1602: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-06 01:22:24,487 EPOCH 1603
2024-02-06 01:22:29,214 [Epoch: 1603 Step: 00054500] Batch Recognition Loss:   0.000507 => Gls Tokens per Sec:     2112 || Batch Translation Loss:   0.030675 => Txt Tokens per Sec:     5882 || Lr: 0.000050
2024-02-06 01:22:29,411 Epoch 1603: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.39 
2024-02-06 01:22:29,411 EPOCH 1604
2024-02-06 01:22:33,594 Epoch 1604: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.57 
2024-02-06 01:22:33,594 EPOCH 1605
2024-02-06 01:22:38,355 Epoch 1605: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.05 
2024-02-06 01:22:38,355 EPOCH 1606
2024-02-06 01:22:42,388 [Epoch: 1606 Step: 00054600] Batch Recognition Loss:   0.000771 => Gls Tokens per Sec:     2381 || Batch Translation Loss:   0.048655 => Txt Tokens per Sec:     6703 || Lr: 0.000050
2024-02-06 01:22:42,816 Epoch 1606: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.22 
2024-02-06 01:22:42,816 EPOCH 1607
2024-02-06 01:22:47,353 Epoch 1607: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.72 
2024-02-06 01:22:47,354 EPOCH 1608
2024-02-06 01:22:51,963 Epoch 1608: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.67 
2024-02-06 01:22:51,964 EPOCH 1609
2024-02-06 01:22:55,663 [Epoch: 1609 Step: 00054700] Batch Recognition Loss:   0.000294 => Gls Tokens per Sec:     2423 || Batch Translation Loss:   0.022575 => Txt Tokens per Sec:     6782 || Lr: 0.000050
2024-02-06 01:22:56,437 Epoch 1609: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.13 
2024-02-06 01:22:56,437 EPOCH 1610
2024-02-06 01:23:01,287 Epoch 1610: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-06 01:23:01,287 EPOCH 1611
2024-02-06 01:23:05,556 Epoch 1611: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-06 01:23:05,557 EPOCH 1612
2024-02-06 01:23:09,304 [Epoch: 1612 Step: 00054800] Batch Recognition Loss:   0.000338 => Gls Tokens per Sec:     2221 || Batch Translation Loss:   0.014795 => Txt Tokens per Sec:     6183 || Lr: 0.000050
2024-02-06 01:23:10,512 Epoch 1612: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.67 
2024-02-06 01:23:10,512 EPOCH 1613
2024-02-06 01:23:14,673 Epoch 1613: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-06 01:23:14,673 EPOCH 1614
2024-02-06 01:23:19,604 Epoch 1614: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.85 
2024-02-06 01:23:19,604 EPOCH 1615
2024-02-06 01:23:22,452 [Epoch: 1615 Step: 00054900] Batch Recognition Loss:   0.000324 => Gls Tokens per Sec:     2606 || Batch Translation Loss:   0.023644 => Txt Tokens per Sec:     6951 || Lr: 0.000050
2024-02-06 01:23:23,847 Epoch 1615: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-06 01:23:23,847 EPOCH 1616
2024-02-06 01:23:28,704 Epoch 1616: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-06 01:23:28,704 EPOCH 1617
2024-02-06 01:23:33,093 Epoch 1617: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-06 01:23:33,093 EPOCH 1618
2024-02-06 01:23:36,228 [Epoch: 1618 Step: 00055000] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:     2247 || Batch Translation Loss:   0.017235 => Txt Tokens per Sec:     6307 || Lr: 0.000050
2024-02-06 01:23:37,813 Epoch 1618: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-06 01:23:37,814 EPOCH 1619
2024-02-06 01:23:42,382 Epoch 1619: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-06 01:23:42,382 EPOCH 1620
2024-02-06 01:23:46,996 Epoch 1620: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-06 01:23:46,997 EPOCH 1621
2024-02-06 01:23:49,893 [Epoch: 1621 Step: 00055100] Batch Recognition Loss:   0.000302 => Gls Tokens per Sec:     2211 || Batch Translation Loss:   0.014382 => Txt Tokens per Sec:     6020 || Lr: 0.000050
2024-02-06 01:23:51,953 Epoch 1621: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 01:23:51,953 EPOCH 1622
2024-02-06 01:23:56,137 Epoch 1622: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 01:23:56,137 EPOCH 1623
2024-02-06 01:24:00,939 Epoch 1623: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-06 01:24:00,940 EPOCH 1624
2024-02-06 01:24:03,348 [Epoch: 1624 Step: 00055200] Batch Recognition Loss:   0.000513 => Gls Tokens per Sec:     2285 || Batch Translation Loss:   0.026152 => Txt Tokens per Sec:     6170 || Lr: 0.000050
2024-02-06 01:24:05,408 Epoch 1624: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-06 01:24:05,409 EPOCH 1625
2024-02-06 01:24:10,074 Epoch 1625: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-06 01:24:10,075 EPOCH 1626
2024-02-06 01:24:14,617 Epoch 1626: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-06 01:24:14,617 EPOCH 1627
2024-02-06 01:24:16,148 [Epoch: 1627 Step: 00055300] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:     3347 || Batch Translation Loss:   0.020411 => Txt Tokens per Sec:     8431 || Lr: 0.000050
2024-02-06 01:24:19,065 Epoch 1627: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-06 01:24:19,066 EPOCH 1628
2024-02-06 01:24:23,800 Epoch 1628: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.70 
2024-02-06 01:24:23,800 EPOCH 1629
2024-02-06 01:24:28,076 Epoch 1629: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-06 01:24:28,076 EPOCH 1630
2024-02-06 01:24:30,390 [Epoch: 1630 Step: 00055400] Batch Recognition Loss:   0.000405 => Gls Tokens per Sec:     1824 || Batch Translation Loss:   0.020743 => Txt Tokens per Sec:     5241 || Lr: 0.000050
2024-02-06 01:24:32,976 Epoch 1630: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-06 01:24:32,976 EPOCH 1631
2024-02-06 01:24:37,104 Epoch 1631: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-06 01:24:37,105 EPOCH 1632
2024-02-06 01:24:42,030 Epoch 1632: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-06 01:24:42,031 EPOCH 1633
2024-02-06 01:24:43,472 [Epoch: 1633 Step: 00055500] Batch Recognition Loss:   0.000129 => Gls Tokens per Sec:     2666 || Batch Translation Loss:   0.025737 => Txt Tokens per Sec:     7382 || Lr: 0.000050
2024-02-06 01:24:46,353 Epoch 1633: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.44 
2024-02-06 01:24:46,353 EPOCH 1634
2024-02-06 01:24:51,203 Epoch 1634: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.97 
2024-02-06 01:24:51,204 EPOCH 1635
2024-02-06 01:24:55,579 Epoch 1635: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-06 01:24:55,579 EPOCH 1636
2024-02-06 01:24:56,931 [Epoch: 1636 Step: 00055600] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:     2176 || Batch Translation Loss:   0.010714 => Txt Tokens per Sec:     5884 || Lr: 0.000050
2024-02-06 01:25:00,276 Epoch 1636: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.70 
2024-02-06 01:25:00,277 EPOCH 1637
2024-02-06 01:25:04,752 Epoch 1637: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-06 01:25:04,753 EPOCH 1638
2024-02-06 01:25:09,248 Epoch 1638: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.43 
2024-02-06 01:25:09,248 EPOCH 1639
2024-02-06 01:25:10,385 [Epoch: 1639 Step: 00055700] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     2026 || Batch Translation Loss:   0.019638 => Txt Tokens per Sec:     5780 || Lr: 0.000050
2024-02-06 01:25:14,052 Epoch 1639: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.24 
2024-02-06 01:25:14,052 EPOCH 1640
2024-02-06 01:25:18,598 Epoch 1640: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.91 
2024-02-06 01:25:18,599 EPOCH 1641
2024-02-06 01:25:23,214 Epoch 1641: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.10 
2024-02-06 01:25:23,214 EPOCH 1642
2024-02-06 01:25:23,947 [Epoch: 1642 Step: 00055800] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:     2271 || Batch Translation Loss:   0.118136 => Txt Tokens per Sec:     6623 || Lr: 0.000050
2024-02-06 01:25:27,667 Epoch 1642: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.45 
2024-02-06 01:25:27,667 EPOCH 1643
2024-02-06 01:25:32,475 Epoch 1643: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.38 
2024-02-06 01:25:32,475 EPOCH 1644
2024-02-06 01:25:36,582 Epoch 1644: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.33 
2024-02-06 01:25:36,582 EPOCH 1645
2024-02-06 01:25:36,910 [Epoch: 1645 Step: 00055900] Batch Recognition Loss:   0.000267 => Gls Tokens per Sec:     3121 || Batch Translation Loss:   0.025378 => Txt Tokens per Sec:     7065 || Lr: 0.000050
2024-02-06 01:25:41,436 Epoch 1645: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.26 
2024-02-06 01:25:41,437 EPOCH 1646
2024-02-06 01:25:45,754 Epoch 1646: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.95 
2024-02-06 01:25:45,754 EPOCH 1647
2024-02-06 01:25:50,679 Epoch 1647: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-06 01:25:50,679 EPOCH 1648
2024-02-06 01:25:51,087 [Epoch: 1648 Step: 00056000] Batch Recognition Loss:   0.001067 => Gls Tokens per Sec:     1576 || Batch Translation Loss:   0.022066 => Txt Tokens per Sec:     5305 || Lr: 0.000050
2024-02-06 01:25:59,629 Validation result at epoch 1648, step    56000: duration: 8.5412s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.06745	Translation Loss: 94880.88281	PPL: 13052.25391
	Eval Metric: BLEU
	WER 2.54	(DEL: 0.00,	INS: 0.00,	SUB: 2.54)
	BLEU-4 0.73	(BLEU-1: 10.25,	BLEU-2: 3.19,	BLEU-3: 1.30,	BLEU-4: 0.73)
	CHRF 16.95	ROUGE 8.76
2024-02-06 01:25:59,630 Logging Recognition and Translation Outputs
2024-02-06 01:25:59,630 ========================================================================================================================
2024-02-06 01:25:59,630 Logging Sequence: 177_167.00
2024-02-06 01:25:59,630 	Gloss Reference :	A B+C+D+E
2024-02-06 01:25:59,630 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:25:59,630 	Gloss Alignment :	         
2024-02-06 01:25:59,631 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:25:59,632 	Text Reference  :	this is       because  sushil wanted      to   establish his fear to ensure no one   would oppose him   
2024-02-06 01:25:59,632 	Text Hypothesis :	and  arrested danushka for    encouraging even at        the age  of '‚¹    2  crore was   taken  tennis
2024-02-06 01:25:59,632 	Text Alignment  :	S    S        S        S      S           S    S         S   S    S  S      S  S     S     S      S     
2024-02-06 01:25:59,633 ========================================================================================================================
2024-02-06 01:25:59,633 Logging Sequence: 127_140.00
2024-02-06 01:25:59,633 	Gloss Reference :	A B+C+D+E
2024-02-06 01:25:59,633 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:25:59,633 	Gloss Alignment :	         
2024-02-06 01:25:59,633 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:25:59,635 	Text Reference  :	this is india' 3rd medal in the ****** world athletics championships he is  very   talented and his performance is  highly impressive
2024-02-06 01:25:59,635 	Text Hypothesis :	**** ** ****** *** ***** ** the indian team  was       india         by sri lankan batsmen  and *** during      the world  cup       
2024-02-06 01:25:59,635 	Text Alignment  :	D    D  D      D   D     D      I      S     S         S             S  S   S      S            D   S           S   S      S         
2024-02-06 01:25:59,635 ========================================================================================================================
2024-02-06 01:25:59,635 Logging Sequence: 126_200.00
2024-02-06 01:25:59,636 	Gloss Reference :	A B+C+D+E
2024-02-06 01:25:59,636 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:25:59,636 	Gloss Alignment :	         
2024-02-06 01:25:59,636 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:25:59,636 	Text Reference  :	let   me   tell you about them 
2024-02-06 01:25:59,637 	Text Hypothesis :	these were some of  the   match
2024-02-06 01:25:59,637 	Text Alignment  :	S     S    S    S   S     S    
2024-02-06 01:25:59,637 ========================================================================================================================
2024-02-06 01:25:59,637 Logging Sequence: 104_119.00
2024-02-06 01:25:59,637 	Gloss Reference :	A B+C+D+E
2024-02-06 01:25:59,637 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:25:59,637 	Gloss Alignment :	         
2024-02-06 01:25:59,638 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:25:59,640 	Text Reference  :	*** ** ****** *** famous       chess players like viswanathan anand and praggnanandhaa's coach r     b    ramesh congratulated him   for his impressive performance
2024-02-06 01:25:59,640 	Text Hypothesis :	the tv rights for broadcasting ipl   matches in   india       for   the next             5     years went to     star          india for rs  23575      crore      
2024-02-06 01:25:59,640 	Text Alignment  :	I   I  I      I   S            S     S       S    S           S     S   S                S     S     S    S      S             S         S   S          S          
2024-02-06 01:25:59,640 ========================================================================================================================
2024-02-06 01:25:59,640 Logging Sequence: 172_267.00
2024-02-06 01:25:59,641 	Gloss Reference :	A B+C+D+E
2024-02-06 01:25:59,641 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:25:59,641 	Gloss Alignment :	         
2024-02-06 01:25:59,641 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:25:59,642 	Text Reference  :	**** ** *** such provisions have  been made     
2024-02-06 01:25:59,642 	Text Hypothesis :	this is why the  police     filed a    statement
2024-02-06 01:25:59,642 	Text Alignment  :	I    I  I   S    S          S     S    S        
2024-02-06 01:25:59,642 ========================================================================================================================
2024-02-06 01:26:04,326 Epoch 1648: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.66 
2024-02-06 01:26:04,326 EPOCH 1649
2024-02-06 01:26:08,606 Epoch 1649: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-06 01:26:08,606 EPOCH 1650
2024-02-06 01:26:13,417 [Epoch: 1650 Step: 00056100] Batch Recognition Loss:   0.000256 => Gls Tokens per Sec:     2208 || Batch Translation Loss:   0.012842 => Txt Tokens per Sec:     6109 || Lr: 0.000050
2024-02-06 01:26:13,417 Epoch 1650: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 01:26:13,418 EPOCH 1651
2024-02-06 01:26:17,815 Epoch 1651: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 01:26:17,815 EPOCH 1652
2024-02-06 01:26:22,590 Epoch 1652: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.75 
2024-02-06 01:26:22,591 EPOCH 1653
2024-02-06 01:26:26,691 [Epoch: 1653 Step: 00056200] Batch Recognition Loss:   0.000208 => Gls Tokens per Sec:     2435 || Batch Translation Loss:   0.016580 => Txt Tokens per Sec:     6657 || Lr: 0.000050
2024-02-06 01:26:27,041 Epoch 1653: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-06 01:26:27,041 EPOCH 1654
2024-02-06 01:26:31,745 Epoch 1654: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.62 
2024-02-06 01:26:31,746 EPOCH 1655
2024-02-06 01:26:36,301 Epoch 1655: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-06 01:26:36,301 EPOCH 1656
2024-02-06 01:26:40,013 [Epoch: 1656 Step: 00056300] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     2587 || Batch Translation Loss:   0.013570 => Txt Tokens per Sec:     7102 || Lr: 0.000050
2024-02-06 01:26:40,672 Epoch 1656: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.78 
2024-02-06 01:26:40,673 EPOCH 1657
2024-02-06 01:26:45,495 Epoch 1657: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.56 
2024-02-06 01:26:45,495 EPOCH 1658
2024-02-06 01:26:49,669 Epoch 1658: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.71 
2024-02-06 01:26:49,669 EPOCH 1659
2024-02-06 01:26:53,529 [Epoch: 1659 Step: 00056400] Batch Recognition Loss:   0.000124 => Gls Tokens per Sec:     2255 || Batch Translation Loss:   0.012336 => Txt Tokens per Sec:     6164 || Lr: 0.000050
2024-02-06 01:26:54,639 Epoch 1659: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-06 01:26:54,640 EPOCH 1660
2024-02-06 01:26:58,803 Epoch 1660: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-06 01:26:58,803 EPOCH 1661
2024-02-06 01:27:03,843 Epoch 1661: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-06 01:27:03,843 EPOCH 1662
2024-02-06 01:27:07,922 [Epoch: 1662 Step: 00056500] Batch Recognition Loss:   0.000318 => Gls Tokens per Sec:     1978 || Batch Translation Loss:   0.021659 => Txt Tokens per Sec:     5567 || Lr: 0.000050
2024-02-06 01:27:08,865 Epoch 1662: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 01:27:08,865 EPOCH 1663
2024-02-06 01:27:13,508 Epoch 1663: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-06 01:27:13,508 EPOCH 1664
2024-02-06 01:27:17,626 Epoch 1664: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-06 01:27:17,626 EPOCH 1665
2024-02-06 01:27:21,067 [Epoch: 1665 Step: 00056600] Batch Recognition Loss:   0.000297 => Gls Tokens per Sec:     2156 || Batch Translation Loss:   0.012435 => Txt Tokens per Sec:     6066 || Lr: 0.000050
2024-02-06 01:27:22,350 Epoch 1665: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-06 01:27:22,350 EPOCH 1666
2024-02-06 01:27:27,219 Epoch 1666: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.84 
2024-02-06 01:27:27,219 EPOCH 1667
2024-02-06 01:27:31,373 Epoch 1667: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-06 01:27:31,373 EPOCH 1668
2024-02-06 01:27:34,622 [Epoch: 1668 Step: 00056700] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:     2088 || Batch Translation Loss:   0.024954 => Txt Tokens per Sec:     5844 || Lr: 0.000050
2024-02-06 01:27:36,272 Epoch 1668: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-06 01:27:36,272 EPOCH 1669
2024-02-06 01:27:40,605 Epoch 1669: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.80 
2024-02-06 01:27:40,605 EPOCH 1670
2024-02-06 01:27:45,358 Epoch 1670: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-06 01:27:45,359 EPOCH 1671
2024-02-06 01:27:47,849 [Epoch: 1671 Step: 00056800] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:     2571 || Batch Translation Loss:   0.022081 => Txt Tokens per Sec:     7039 || Lr: 0.000050
2024-02-06 01:27:49,832 Epoch 1671: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.86 
2024-02-06 01:27:49,832 EPOCH 1672
2024-02-06 01:27:54,457 Epoch 1672: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-06 01:27:54,457 EPOCH 1673
2024-02-06 01:27:59,088 Epoch 1673: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-06 01:27:59,088 EPOCH 1674
2024-02-06 01:28:01,356 [Epoch: 1674 Step: 00056900] Batch Recognition Loss:   0.000731 => Gls Tokens per Sec:     2426 || Batch Translation Loss:   0.019772 => Txt Tokens per Sec:     6749 || Lr: 0.000050
2024-02-06 01:28:03,510 Epoch 1674: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-06 01:28:03,510 EPOCH 1675
2024-02-06 01:28:08,328 Epoch 1675: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-06 01:28:08,328 EPOCH 1676
2024-02-06 01:28:12,583 Epoch 1676: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.71 
2024-02-06 01:28:12,583 EPOCH 1677
2024-02-06 01:28:14,975 [Epoch: 1677 Step: 00057000] Batch Recognition Loss:   0.000189 => Gls Tokens per Sec:     2032 || Batch Translation Loss:   0.015396 => Txt Tokens per Sec:     5525 || Lr: 0.000050
2024-02-06 01:28:17,776 Epoch 1677: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-06 01:28:17,777 EPOCH 1678
2024-02-06 01:28:22,380 Epoch 1678: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-06 01:28:22,380 EPOCH 1679
2024-02-06 01:28:26,630 Epoch 1679: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-06 01:28:26,630 EPOCH 1680
2024-02-06 01:28:28,730 [Epoch: 1680 Step: 00057100] Batch Recognition Loss:   0.000282 => Gls Tokens per Sec:     2012 || Batch Translation Loss:   0.024685 => Txt Tokens per Sec:     5580 || Lr: 0.000050
2024-02-06 01:28:31,519 Epoch 1680: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-06 01:28:31,520 EPOCH 1681
2024-02-06 01:28:35,695 Epoch 1681: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.94 
2024-02-06 01:28:35,695 EPOCH 1682
2024-02-06 01:28:40,674 Epoch 1682: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.34 
2024-02-06 01:28:40,675 EPOCH 1683
2024-02-06 01:28:42,257 [Epoch: 1683 Step: 00057200] Batch Recognition Loss:   0.000277 => Gls Tokens per Sec:     2265 || Batch Translation Loss:   0.030342 => Txt Tokens per Sec:     6532 || Lr: 0.000050
2024-02-06 01:28:45,337 Epoch 1683: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.46 
2024-02-06 01:28:45,337 EPOCH 1684
2024-02-06 01:28:50,214 Epoch 1684: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.99 
2024-02-06 01:28:50,214 EPOCH 1685
2024-02-06 01:28:54,416 Epoch 1685: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-06 01:28:54,417 EPOCH 1686
2024-02-06 01:28:55,631 [Epoch: 1686 Step: 00057300] Batch Recognition Loss:   0.000438 => Gls Tokens per Sec:     2424 || Batch Translation Loss:   0.006065 => Txt Tokens per Sec:     6686 || Lr: 0.000050
2024-02-06 01:28:59,229 Epoch 1686: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.44 
2024-02-06 01:28:59,230 EPOCH 1687
2024-02-06 01:29:03,767 Epoch 1687: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.64 
2024-02-06 01:29:03,767 EPOCH 1688
2024-02-06 01:29:08,223 Epoch 1688: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.49 
2024-02-06 01:29:08,224 EPOCH 1689
2024-02-06 01:29:09,607 [Epoch: 1689 Step: 00057400] Batch Recognition Loss:   0.000364 => Gls Tokens per Sec:     1664 || Batch Translation Loss:   0.045133 => Txt Tokens per Sec:     4959 || Lr: 0.000050
2024-02-06 01:29:13,028 Epoch 1689: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.97 
2024-02-06 01:29:13,028 EPOCH 1690
2024-02-06 01:29:17,152 Epoch 1690: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.79 
2024-02-06 01:29:17,152 EPOCH 1691
2024-02-06 01:29:21,317 Epoch 1691: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-06 01:29:21,318 EPOCH 1692
2024-02-06 01:29:21,990 [Epoch: 1692 Step: 00057500] Batch Recognition Loss:   0.000315 => Gls Tokens per Sec:     2868 || Batch Translation Loss:   0.014495 => Txt Tokens per Sec:     6733 || Lr: 0.000050
2024-02-06 01:29:26,290 Epoch 1692: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.66 
2024-02-06 01:29:26,291 EPOCH 1693
2024-02-06 01:29:30,453 Epoch 1693: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-06 01:29:30,453 EPOCH 1694
2024-02-06 01:29:35,083 Epoch 1694: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.97 
2024-02-06 01:29:35,084 EPOCH 1695
2024-02-06 01:29:35,627 [Epoch: 1695 Step: 00057600] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:     1882 || Batch Translation Loss:   0.020536 => Txt Tokens per Sec:     5441 || Lr: 0.000050
2024-02-06 01:29:39,723 Epoch 1695: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-06 01:29:39,723 EPOCH 1696
2024-02-06 01:29:44,267 Epoch 1696: Total Training Recognition Loss 0.04  Total Training Translation Loss 13.04 
2024-02-06 01:29:44,268 EPOCH 1697
2024-02-06 01:29:48,958 Epoch 1697: Total Training Recognition Loss 0.14  Total Training Translation Loss 7.14 
2024-02-06 01:29:48,958 EPOCH 1698
2024-02-06 01:29:49,176 [Epoch: 1698 Step: 00057700] Batch Recognition Loss:   0.000639 => Gls Tokens per Sec:     2949 || Batch Translation Loss:   0.055749 => Txt Tokens per Sec:     7705 || Lr: 0.000050
2024-02-06 01:29:53,215 Epoch 1698: Total Training Recognition Loss 0.09  Total Training Translation Loss 1.29 
2024-02-06 01:29:53,216 EPOCH 1699
2024-02-06 01:29:58,127 Epoch 1699: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.81 
2024-02-06 01:29:58,127 EPOCH 1700
2024-02-06 01:30:02,233 [Epoch: 1700 Step: 00057800] Batch Recognition Loss:   0.000272 => Gls Tokens per Sec:     2588 || Batch Translation Loss:   0.016967 => Txt Tokens per Sec:     7160 || Lr: 0.000050
2024-02-06 01:30:02,233 Epoch 1700: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.70 
2024-02-06 01:30:02,233 EPOCH 1701
2024-02-06 01:30:07,217 Epoch 1701: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.71 
2024-02-06 01:30:07,218 EPOCH 1702
2024-02-06 01:30:11,974 Epoch 1702: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-06 01:30:11,975 EPOCH 1703
2024-02-06 01:30:16,542 [Epoch: 1703 Step: 00057900] Batch Recognition Loss:   0.000734 => Gls Tokens per Sec:     2185 || Batch Translation Loss:   0.014374 => Txt Tokens per Sec:     6054 || Lr: 0.000050
2024-02-06 01:30:16,757 Epoch 1703: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-06 01:30:16,757 EPOCH 1704
2024-02-06 01:30:21,324 Epoch 1704: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-06 01:30:21,325 EPOCH 1705
2024-02-06 01:30:26,459 Epoch 1705: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-06 01:30:26,459 EPOCH 1706
2024-02-06 01:30:31,081 [Epoch: 1706 Step: 00058000] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     2021 || Batch Translation Loss:   0.019509 => Txt Tokens per Sec:     5633 || Lr: 0.000050
2024-02-06 01:30:39,739 Validation result at epoch 1706, step    58000: duration: 8.6564s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.11761	Translation Loss: 94992.92188	PPL: 13199.13281
	Eval Metric: BLEU
	WER 2.90	(DEL: 0.00,	INS: 0.00,	SUB: 2.90)
	BLEU-4 0.48	(BLEU-1: 10.60,	BLEU-2: 3.11,	BLEU-3: 1.07,	BLEU-4: 0.48)
	CHRF 16.83	ROUGE 8.99
2024-02-06 01:30:39,740 Logging Recognition and Translation Outputs
2024-02-06 01:30:39,740 ========================================================================================================================
2024-02-06 01:30:39,740 Logging Sequence: 60_264.00
2024-02-06 01:30:39,741 	Gloss Reference :	A B+C+D+E
2024-02-06 01:30:39,741 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:30:39,741 	Gloss Alignment :	         
2024-02-06 01:30:39,741 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:30:39,743 	Text Reference  :	** plus do   you       know that     a             sex  tape    of  his      with two    women had  gone  viral 
2024-02-06 01:30:39,743 	Text Hypothesis :	he has  been embroiled in   multiple controversies like affairs sex scandals with models and   many other issues
2024-02-06 01:30:39,743 	Text Alignment  :	I  S    S    S         S    S        S             S    S       S   S             S      S     S    S     S     
2024-02-06 01:30:39,743 ========================================================================================================================
2024-02-06 01:30:39,744 Logging Sequence: 100_50.00
2024-02-06 01:30:39,744 	Gloss Reference :	A B+C+D+E
2024-02-06 01:30:39,744 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:30:39,744 	Gloss Alignment :	         
2024-02-06 01:30:39,744 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:30:39,745 	Text Reference  :	***** with virat    kohli  as     the captain
2024-02-06 01:30:39,745 	Text Hypothesis :	after this incident yuvraj showed his yet    
2024-02-06 01:30:39,745 	Text Alignment  :	I     S    S        S      S      S   S      
2024-02-06 01:30:39,745 ========================================================================================================================
2024-02-06 01:30:39,745 Logging Sequence: 137_44.00
2024-02-06 01:30:39,746 	Gloss Reference :	A B+C+D+E
2024-02-06 01:30:39,746 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:30:39,746 	Gloss Alignment :	         
2024-02-06 01:30:39,746 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:30:39,748 	Text Reference  :	let me   tell you    the rules that qatar has announced for the fans    travelling for the world cup 
2024-02-06 01:30:39,748 	Text Hypothesis :	*** they also called the ***** **** ***** *** news      of  all crashed out        of  the long  time
2024-02-06 01:30:39,748 	Text Alignment  :	D   S    S    S          D     D    D     D   S         S   S   S       S          S       S     S   
2024-02-06 01:30:39,748 ========================================================================================================================
2024-02-06 01:30:39,748 Logging Sequence: 58_27.00
2024-02-06 01:30:39,748 	Gloss Reference :	A B+C+D+E
2024-02-06 01:30:39,749 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:30:39,749 	Gloss Alignment :	         
2024-02-06 01:30:39,749 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:30:39,750 	Text Reference  :	the 19th  asian games 2022 were to be  held in hangzhou china 
2024-02-06 01:30:39,750 	Text Hypothesis :	the first asian games **** **** ** are held in 39       sports
2024-02-06 01:30:39,750 	Text Alignment  :	    S                 D    D    D  S           S        S     
2024-02-06 01:30:39,750 ========================================================================================================================
2024-02-06 01:30:39,750 Logging Sequence: 75_255.00
2024-02-06 01:30:39,751 	Gloss Reference :	A B+C+D+E  
2024-02-06 01:30:39,751 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-06 01:30:39,751 	Gloss Alignment :	  S        
2024-02-06 01:30:39,751 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:30:39,752 	Text Reference  :	we miss our baby boy with this ronaldo' total baby   count has     reached 5 with  2  boys 3   girls
2024-02-06 01:30:39,753 	Text Hypothesis :	** **** *** **** *** **** **** ******** later rooney then  invited the     3 girls to his  vip booth
2024-02-06 01:30:39,753 	Text Alignment  :	D  D    D   D    D   D    D    D        S     S      S     S       S       S S     S  S    S   S    
2024-02-06 01:30:39,753 ========================================================================================================================
2024-02-06 01:30:40,285 Epoch 1706: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-06 01:30:40,285 EPOCH 1707
2024-02-06 01:30:45,208 Epoch 1707: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 01:30:45,208 EPOCH 1708
2024-02-06 01:30:50,189 Epoch 1708: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-06 01:30:50,190 EPOCH 1709
2024-02-06 01:30:53,814 [Epoch: 1709 Step: 00058100] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:     2401 || Batch Translation Loss:   0.015241 => Txt Tokens per Sec:     6548 || Lr: 0.000050
2024-02-06 01:30:54,882 Epoch 1709: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 01:30:54,882 EPOCH 1710
2024-02-06 01:30:59,700 Epoch 1710: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-06 01:30:59,701 EPOCH 1711
2024-02-06 01:31:04,367 Epoch 1711: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 01:31:04,367 EPOCH 1712
2024-02-06 01:31:08,005 [Epoch: 1712 Step: 00058200] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     2216 || Batch Translation Loss:   0.014035 => Txt Tokens per Sec:     6073 || Lr: 0.000050
2024-02-06 01:31:09,282 Epoch 1712: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 01:31:09,282 EPOCH 1713
2024-02-06 01:31:14,017 Epoch 1713: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-06 01:31:14,017 EPOCH 1714
2024-02-06 01:31:18,107 Epoch 1714: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 01:31:18,107 EPOCH 1715
2024-02-06 01:31:21,756 [Epoch: 1715 Step: 00058300] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:     2034 || Batch Translation Loss:   0.017634 => Txt Tokens per Sec:     5689 || Lr: 0.000050
2024-02-06 01:31:23,057 Epoch 1715: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-06 01:31:23,058 EPOCH 1716
2024-02-06 01:31:27,310 Epoch 1716: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-06 01:31:27,311 EPOCH 1717
2024-02-06 01:31:31,619 Epoch 1717: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-06 01:31:31,620 EPOCH 1718
2024-02-06 01:31:34,592 [Epoch: 1718 Step: 00058400] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:     2282 || Batch Translation Loss:   0.022285 => Txt Tokens per Sec:     6080 || Lr: 0.000050
2024-02-06 01:31:36,549 Epoch 1718: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-06 01:31:36,550 EPOCH 1719
2024-02-06 01:31:40,672 Epoch 1719: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-06 01:31:40,673 EPOCH 1720
2024-02-06 01:31:45,710 Epoch 1720: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.71 
2024-02-06 01:31:45,710 EPOCH 1721
2024-02-06 01:31:48,099 [Epoch: 1721 Step: 00058500] Batch Recognition Loss:   0.000454 => Gls Tokens per Sec:     2571 || Batch Translation Loss:   0.010811 => Txt Tokens per Sec:     6871 || Lr: 0.000050
2024-02-06 01:31:49,917 Epoch 1721: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-06 01:31:49,917 EPOCH 1722
2024-02-06 01:31:54,789 Epoch 1722: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-06 01:31:54,790 EPOCH 1723
2024-02-06 01:31:59,184 Epoch 1723: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 01:31:59,184 EPOCH 1724
2024-02-06 01:32:02,071 [Epoch: 1724 Step: 00058600] Batch Recognition Loss:   0.001317 => Gls Tokens per Sec:     1907 || Batch Translation Loss:   0.040340 => Txt Tokens per Sec:     5550 || Lr: 0.000050
2024-02-06 01:32:03,991 Epoch 1724: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 01:32:03,992 EPOCH 1725
2024-02-06 01:32:08,475 Epoch 1725: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-06 01:32:08,475 EPOCH 1726
2024-02-06 01:32:13,116 Epoch 1726: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-06 01:32:13,117 EPOCH 1727
2024-02-06 01:32:15,301 [Epoch: 1727 Step: 00058700] Batch Recognition Loss:   0.000997 => Gls Tokens per Sec:     2347 || Batch Translation Loss:   0.016224 => Txt Tokens per Sec:     6552 || Lr: 0.000050
2024-02-06 01:32:17,735 Epoch 1727: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-06 01:32:17,735 EPOCH 1728
2024-02-06 01:32:22,188 Epoch 1728: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-06 01:32:22,189 EPOCH 1729
2024-02-06 01:32:26,985 Epoch 1729: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-06 01:32:26,985 EPOCH 1730
2024-02-06 01:32:28,556 [Epoch: 1730 Step: 00058800] Batch Recognition Loss:   0.000306 => Gls Tokens per Sec:     2854 || Batch Translation Loss:   0.007689 => Txt Tokens per Sec:     7269 || Lr: 0.000050
2024-02-06 01:32:31,260 Epoch 1730: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-06 01:32:31,261 EPOCH 1731
2024-02-06 01:32:36,147 Epoch 1731: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-06 01:32:36,147 EPOCH 1732
2024-02-06 01:32:40,432 Epoch 1732: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-06 01:32:40,432 EPOCH 1733
2024-02-06 01:32:42,129 [Epoch: 1733 Step: 00058900] Batch Recognition Loss:   0.003095 => Gls Tokens per Sec:     2112 || Batch Translation Loss:   0.143185 => Txt Tokens per Sec:     5845 || Lr: 0.000050
2024-02-06 01:32:45,404 Epoch 1733: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.23 
2024-02-06 01:32:45,405 EPOCH 1734
2024-02-06 01:32:49,602 Epoch 1734: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.09 
2024-02-06 01:32:49,603 EPOCH 1735
2024-02-06 01:32:54,530 Epoch 1735: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.34 
2024-02-06 01:32:54,531 EPOCH 1736
2024-02-06 01:32:56,030 [Epoch: 1736 Step: 00059000] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:     2135 || Batch Translation Loss:   0.020129 => Txt Tokens per Sec:     6041 || Lr: 0.000050
2024-02-06 01:32:58,779 Epoch 1736: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-06 01:32:58,779 EPOCH 1737
2024-02-06 01:33:03,527 Epoch 1737: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-06 01:33:03,528 EPOCH 1738
2024-02-06 01:33:07,996 Epoch 1738: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.87 
2024-02-06 01:33:07,997 EPOCH 1739
2024-02-06 01:33:08,891 [Epoch: 1739 Step: 00059100] Batch Recognition Loss:   0.000295 => Gls Tokens per Sec:     2867 || Batch Translation Loss:   0.018452 => Txt Tokens per Sec:     7786 || Lr: 0.000050
2024-02-06 01:33:12,665 Epoch 1739: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-06 01:33:12,665 EPOCH 1740
2024-02-06 01:33:17,212 Epoch 1740: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-06 01:33:17,213 EPOCH 1741
2024-02-06 01:33:21,715 Epoch 1741: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-06 01:33:21,715 EPOCH 1742
2024-02-06 01:33:22,784 [Epoch: 1742 Step: 00059200] Batch Recognition Loss:   0.000792 => Gls Tokens per Sec:     1799 || Batch Translation Loss:   0.008025 => Txt Tokens per Sec:     4912 || Lr: 0.000050
2024-02-06 01:33:26,467 Epoch 1742: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-06 01:33:26,467 EPOCH 1743
2024-02-06 01:33:30,786 Epoch 1743: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-06 01:33:30,786 EPOCH 1744
2024-02-06 01:33:35,721 Epoch 1744: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.00 
2024-02-06 01:33:35,721 EPOCH 1745
2024-02-06 01:33:36,302 [Epoch: 1745 Step: 00059300] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:     2207 || Batch Translation Loss:   0.032291 => Txt Tokens per Sec:     6500 || Lr: 0.000050
2024-02-06 01:33:39,814 Epoch 1745: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-06 01:33:39,814 EPOCH 1746
2024-02-06 01:33:44,824 Epoch 1746: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-06 01:33:44,825 EPOCH 1747
2024-02-06 01:33:49,086 Epoch 1747: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-06 01:33:49,086 EPOCH 1748
2024-02-06 01:33:49,323 [Epoch: 1748 Step: 00059400] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:     2723 || Batch Translation Loss:   0.015300 => Txt Tokens per Sec:     8089 || Lr: 0.000050
2024-02-06 01:33:53,786 Epoch 1748: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-06 01:33:53,787 EPOCH 1749
2024-02-06 01:33:58,331 Epoch 1749: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-06 01:33:58,331 EPOCH 1750
2024-02-06 01:34:02,864 [Epoch: 1750 Step: 00059500] Batch Recognition Loss:   0.000282 => Gls Tokens per Sec:     2344 || Batch Translation Loss:   0.024646 => Txt Tokens per Sec:     6484 || Lr: 0.000050
2024-02-06 01:34:02,864 Epoch 1750: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.16 
2024-02-06 01:34:02,864 EPOCH 1751
2024-02-06 01:34:07,515 Epoch 1751: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.30 
2024-02-06 01:34:07,515 EPOCH 1752
2024-02-06 01:34:11,879 Epoch 1752: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.54 
2024-02-06 01:34:11,880 EPOCH 1753
2024-02-06 01:34:16,521 [Epoch: 1753 Step: 00059600] Batch Recognition Loss:   0.000219 => Gls Tokens per Sec:     2151 || Batch Translation Loss:   0.026080 => Txt Tokens per Sec:     6027 || Lr: 0.000050
2024-02-06 01:34:16,698 Epoch 1753: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.80 
2024-02-06 01:34:16,698 EPOCH 1754
2024-02-06 01:34:20,782 Epoch 1754: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.98 
2024-02-06 01:34:20,783 EPOCH 1755
2024-02-06 01:34:25,863 Epoch 1755: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-06 01:34:25,864 EPOCH 1756
2024-02-06 01:34:29,642 [Epoch: 1756 Step: 00059700] Batch Recognition Loss:   0.000265 => Gls Tokens per Sec:     2473 || Batch Translation Loss:   0.015248 => Txt Tokens per Sec:     6793 || Lr: 0.000050
2024-02-06 01:34:30,085 Epoch 1756: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.79 
2024-02-06 01:34:30,086 EPOCH 1757
2024-02-06 01:34:35,018 Epoch 1757: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-06 01:34:35,019 EPOCH 1758
2024-02-06 01:34:39,405 Epoch 1758: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-06 01:34:39,406 EPOCH 1759
2024-02-06 01:34:43,313 [Epoch: 1759 Step: 00059800] Batch Recognition Loss:   0.000490 => Gls Tokens per Sec:     2294 || Batch Translation Loss:   0.020299 => Txt Tokens per Sec:     6289 || Lr: 0.000050
2024-02-06 01:34:44,184 Epoch 1759: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.77 
2024-02-06 01:34:44,184 EPOCH 1760
2024-02-06 01:34:48,665 Epoch 1760: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-06 01:34:48,665 EPOCH 1761
2024-02-06 01:34:53,346 Epoch 1761: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-06 01:34:53,347 EPOCH 1762
2024-02-06 01:34:56,724 [Epoch: 1762 Step: 00059900] Batch Recognition Loss:   0.000411 => Gls Tokens per Sec:     2464 || Batch Translation Loss:   0.015205 => Txt Tokens per Sec:     6791 || Lr: 0.000050
2024-02-06 01:34:57,966 Epoch 1762: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-06 01:34:57,967 EPOCH 1763
2024-02-06 01:35:02,512 Epoch 1763: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-06 01:35:02,513 EPOCH 1764
2024-02-06 01:35:07,256 Epoch 1764: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-06 01:35:07,256 EPOCH 1765
2024-02-06 01:35:10,138 [Epoch: 1765 Step: 00060000] Batch Recognition Loss:   0.000733 => Gls Tokens per Sec:     2576 || Batch Translation Loss:   0.010985 => Txt Tokens per Sec:     7055 || Lr: 0.000050
2024-02-06 01:35:19,183 Validation result at epoch 1765, step    60000: duration: 9.0445s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.12387	Translation Loss: 94963.17969	PPL: 13159.98145
	Eval Metric: BLEU
	WER 3.11	(DEL: 0.00,	INS: 0.00,	SUB: 3.11)
	BLEU-4 0.64	(BLEU-1: 10.57,	BLEU-2: 3.12,	BLEU-3: 1.25,	BLEU-4: 0.64)
	CHRF 17.20	ROUGE 8.70
2024-02-06 01:35:19,184 Logging Recognition and Translation Outputs
2024-02-06 01:35:19,184 ========================================================================================================================
2024-02-06 01:35:19,184 Logging Sequence: 75_58.00
2024-02-06 01:35:19,184 	Gloss Reference :	A B+C+D+E
2024-02-06 01:35:19,184 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:35:19,185 	Gloss Alignment :	         
2024-02-06 01:35:19,185 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:35:19,186 	Text Reference  :	it seems like he like to date women and does not want to      get married people seem    to respect his       choices
2024-02-06 01:35:19,186 	Text Hypothesis :	** ***** **** ** **** ** **** ***** *** **** *** we   request the media   for    privacy at this    difficult time   
2024-02-06 01:35:19,186 	Text Alignment  :	D  D     D    D  D    D  D    D     D   D    D   S    S       S   S       S      S       S  S       S         S      
2024-02-06 01:35:19,186 ========================================================================================================================
2024-02-06 01:35:19,186 Logging Sequence: 152_113.00
2024-02-06 01:35:19,187 	Gloss Reference :	A B+C+D+E
2024-02-06 01:35:19,187 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:35:19,187 	Gloss Alignment :	         
2024-02-06 01:35:19,187 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:35:19,188 	Text Reference  :	indians hoping for a         victory were   distraught at     the defeat
2024-02-06 01:35:19,188 	Text Hypothesis :	when    it     was pakistan' turn    lionel messi      scored his bat   
2024-02-06 01:35:19,188 	Text Alignment  :	S       S      S   S         S       S      S          S      S   S     
2024-02-06 01:35:19,188 ========================================================================================================================
2024-02-06 01:35:19,188 Logging Sequence: 176_41.00
2024-02-06 01:35:19,189 	Gloss Reference :	A B+C+D+E
2024-02-06 01:35:19,189 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:35:19,189 	Gloss Alignment :	         
2024-02-06 01:35:19,189 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:35:19,190 	Text Reference  :	dahiya did not   loose  hope and   put up    a    strong fight   
2024-02-06 01:35:19,190 	Text Hypothesis :	after  the match people go   viral 3   balls will be     refunded
2024-02-06 01:35:19,190 	Text Alignment  :	S      S   S     S      S    S     S   S     S    S      S       
2024-02-06 01:35:19,190 ========================================================================================================================
2024-02-06 01:35:19,190 Logging Sequence: 77_190.00
2024-02-06 01:35:19,191 	Gloss Reference :	A B+C+D+E
2024-02-06 01:35:19,191 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:35:19,191 	Gloss Alignment :	         
2024-02-06 01:35:19,191 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:35:19,192 	Text Reference  :	* ** **** ***** there are many batsmen who      have scrored 36  runs in        6   balls   
2024-02-06 01:35:19,192 	Text Hypothesis :	i am very grate to    my  fans and     everyone to   see     him to   celebrate the position
2024-02-06 01:35:19,192 	Text Alignment  :	I I  I    I     S     S   S    S       S        S    S       S   S    S         S   S       
2024-02-06 01:35:19,192 ========================================================================================================================
2024-02-06 01:35:19,193 Logging Sequence: 155_170.00
2024-02-06 01:35:19,193 	Gloss Reference :	A B+C+D+E
2024-02-06 01:35:19,193 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:35:19,193 	Gloss Alignment :	         
2024-02-06 01:35:19,193 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:35:19,194 	Text Reference  :	india lost the matches and  could    not secure a      place   in the semi final 
2024-02-06 01:35:19,194 	Text Hypothesis :	***** **** i   am      very grateful to  my     family friends of the **** sports
2024-02-06 01:35:19,194 	Text Alignment  :	D     D    S   S       S    S        S   S      S      S       S      D    S     
2024-02-06 01:35:19,194 ========================================================================================================================
2024-02-06 01:35:20,633 Epoch 1765: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.90 
2024-02-06 01:35:20,633 EPOCH 1766
2024-02-06 01:35:25,687 Epoch 1766: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.94 
2024-02-06 01:35:25,688 EPOCH 1767
2024-02-06 01:35:29,869 Epoch 1767: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.65 
2024-02-06 01:35:29,870 EPOCH 1768
2024-02-06 01:35:32,716 [Epoch: 1768 Step: 00060100] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:     2383 || Batch Translation Loss:   0.019077 => Txt Tokens per Sec:     6507 || Lr: 0.000050
2024-02-06 01:35:34,763 Epoch 1768: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-06 01:35:34,763 EPOCH 1769
2024-02-06 01:35:39,115 Epoch 1769: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-06 01:35:39,115 EPOCH 1770
2024-02-06 01:35:43,862 Epoch 1770: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-06 01:35:43,863 EPOCH 1771
2024-02-06 01:35:46,490 [Epoch: 1771 Step: 00060200] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:     2338 || Batch Translation Loss:   0.013388 => Txt Tokens per Sec:     6531 || Lr: 0.000050
2024-02-06 01:35:48,367 Epoch 1771: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-06 01:35:48,368 EPOCH 1772
2024-02-06 01:35:53,038 Epoch 1772: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-06 01:35:53,039 EPOCH 1773
2024-02-06 01:35:57,614 Epoch 1773: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-06 01:35:57,614 EPOCH 1774
2024-02-06 01:35:59,889 [Epoch: 1774 Step: 00060300] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     2533 || Batch Translation Loss:   0.015070 => Txt Tokens per Sec:     7171 || Lr: 0.000050
2024-02-06 01:36:02,058 Epoch 1774: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-06 01:36:02,059 EPOCH 1775
2024-02-06 01:36:06,817 Epoch 1775: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-06 01:36:06,818 EPOCH 1776
2024-02-06 01:36:11,028 Epoch 1776: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-06 01:36:11,029 EPOCH 1777
2024-02-06 01:36:12,927 [Epoch: 1777 Step: 00060400] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:     2697 || Batch Translation Loss:   0.019007 => Txt Tokens per Sec:     6842 || Lr: 0.000050
2024-02-06 01:36:15,928 Epoch 1777: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-06 01:36:15,928 EPOCH 1778
2024-02-06 01:36:20,086 Epoch 1778: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.19 
2024-02-06 01:36:20,086 EPOCH 1779
2024-02-06 01:36:25,052 Epoch 1779: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.14 
2024-02-06 01:36:25,052 EPOCH 1780
2024-02-06 01:36:26,887 [Epoch: 1780 Step: 00060500] Batch Recognition Loss:   0.000288 => Gls Tokens per Sec:     2302 || Batch Translation Loss:   0.039940 => Txt Tokens per Sec:     6426 || Lr: 0.000050
2024-02-06 01:36:29,311 Epoch 1780: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-06 01:36:29,311 EPOCH 1781
2024-02-06 01:36:34,112 Epoch 1781: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.81 
2024-02-06 01:36:34,112 EPOCH 1782
2024-02-06 01:36:38,534 Epoch 1782: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.19 
2024-02-06 01:36:38,534 EPOCH 1783
2024-02-06 01:36:39,868 [Epoch: 1783 Step: 00060600] Batch Recognition Loss:   0.000448 => Gls Tokens per Sec:     2685 || Batch Translation Loss:   0.053189 => Txt Tokens per Sec:     7055 || Lr: 0.000050
2024-02-06 01:36:43,183 Epoch 1783: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.31 
2024-02-06 01:36:43,184 EPOCH 1784
2024-02-06 01:36:47,806 Epoch 1784: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.46 
2024-02-06 01:36:47,806 EPOCH 1785
2024-02-06 01:36:52,823 Epoch 1785: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.85 
2024-02-06 01:36:52,824 EPOCH 1786
2024-02-06 01:36:54,070 [Epoch: 1786 Step: 00060700] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     2570 || Batch Translation Loss:   0.015055 => Txt Tokens per Sec:     7260 || Lr: 0.000050
2024-02-06 01:36:57,359 Epoch 1786: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-06 01:36:57,359 EPOCH 1787
2024-02-06 01:37:01,786 Epoch 1787: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-06 01:37:01,787 EPOCH 1788
2024-02-06 01:37:06,621 Epoch 1788: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.77 
2024-02-06 01:37:06,621 EPOCH 1789
2024-02-06 01:37:07,617 [Epoch: 1789 Step: 00060800] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:     2313 || Batch Translation Loss:   0.024883 => Txt Tokens per Sec:     6577 || Lr: 0.000050
2024-02-06 01:37:10,728 Epoch 1789: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-06 01:37:10,728 EPOCH 1790
2024-02-06 01:37:15,633 Epoch 1790: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.91 
2024-02-06 01:37:15,634 EPOCH 1791
2024-02-06 01:37:19,974 Epoch 1791: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.68 
2024-02-06 01:37:19,975 EPOCH 1792
2024-02-06 01:37:20,739 [Epoch: 1792 Step: 00060900] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:     2515 || Batch Translation Loss:   0.013870 => Txt Tokens per Sec:     7200 || Lr: 0.000050
2024-02-06 01:37:24,770 Epoch 1792: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.81 
2024-02-06 01:37:24,770 EPOCH 1793
2024-02-06 01:37:29,200 Epoch 1793: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-06 01:37:29,201 EPOCH 1794
2024-02-06 01:37:33,929 Epoch 1794: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.16 
2024-02-06 01:37:33,930 EPOCH 1795
2024-02-06 01:37:34,582 [Epoch: 1795 Step: 00061000] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:     1969 || Batch Translation Loss:   0.036871 => Txt Tokens per Sec:     5903 || Lr: 0.000050
2024-02-06 01:37:38,495 Epoch 1795: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-06 01:37:38,495 EPOCH 1796
2024-02-06 01:37:43,036 Epoch 1796: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.85 
2024-02-06 01:37:43,037 EPOCH 1797
2024-02-06 01:37:47,766 Epoch 1797: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.12 
2024-02-06 01:37:47,767 EPOCH 1798
2024-02-06 01:37:47,945 [Epoch: 1798 Step: 00061100] Batch Recognition Loss:   0.000196 => Gls Tokens per Sec:     3616 || Batch Translation Loss:   0.024264 => Txt Tokens per Sec:     9249 || Lr: 0.000050
2024-02-06 01:37:51,855 Epoch 1798: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.35 
2024-02-06 01:37:51,855 EPOCH 1799
2024-02-06 01:37:56,779 Epoch 1799: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.09 
2024-02-06 01:37:56,779 EPOCH 1800
2024-02-06 01:38:01,123 [Epoch: 1800 Step: 00061200] Batch Recognition Loss:   0.000437 => Gls Tokens per Sec:     2446 || Batch Translation Loss:   0.059450 => Txt Tokens per Sec:     6768 || Lr: 0.000050
2024-02-06 01:38:01,124 Epoch 1800: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-06 01:38:01,124 EPOCH 1801
2024-02-06 01:38:05,960 Epoch 1801: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.96 
2024-02-06 01:38:05,961 EPOCH 1802
2024-02-06 01:38:10,305 Epoch 1802: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.28 
2024-02-06 01:38:10,305 EPOCH 1803
2024-02-06 01:38:14,769 [Epoch: 1803 Step: 00061300] Batch Recognition Loss:   0.000478 => Gls Tokens per Sec:     2236 || Batch Translation Loss:   0.032191 => Txt Tokens per Sec:     6194 || Lr: 0.000050
2024-02-06 01:38:14,979 Epoch 1803: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.50 
2024-02-06 01:38:14,980 EPOCH 1804
2024-02-06 01:38:19,497 Epoch 1804: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.86 
2024-02-06 01:38:19,497 EPOCH 1805
2024-02-06 01:38:24,087 Epoch 1805: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-06 01:38:24,088 EPOCH 1806
2024-02-06 01:38:28,197 [Epoch: 1806 Step: 00061400] Batch Recognition Loss:   0.000550 => Gls Tokens per Sec:     2273 || Batch Translation Loss:   0.013998 => Txt Tokens per Sec:     6244 || Lr: 0.000050
2024-02-06 01:38:28,855 Epoch 1806: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.74 
2024-02-06 01:38:28,856 EPOCH 1807
2024-02-06 01:38:33,183 Epoch 1807: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.76 
2024-02-06 01:38:33,183 EPOCH 1808
2024-02-06 01:38:38,017 Epoch 1808: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.65 
2024-02-06 01:38:38,017 EPOCH 1809
2024-02-06 01:38:41,424 [Epoch: 1809 Step: 00061500] Batch Recognition Loss:   0.000304 => Gls Tokens per Sec:     2631 || Batch Translation Loss:   0.022929 => Txt Tokens per Sec:     7246 || Lr: 0.000050
2024-02-06 01:38:42,270 Epoch 1809: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-06 01:38:42,271 EPOCH 1810
2024-02-06 01:38:47,215 Epoch 1810: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 01:38:47,216 EPOCH 1811
2024-02-06 01:38:51,382 Epoch 1811: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 01:38:51,382 EPOCH 1812
2024-02-06 01:38:55,221 [Epoch: 1812 Step: 00061600] Batch Recognition Loss:   0.000333 => Gls Tokens per Sec:     2101 || Batch Translation Loss:   0.007192 => Txt Tokens per Sec:     5825 || Lr: 0.000050
2024-02-06 01:38:56,318 Epoch 1812: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-06 01:38:56,318 EPOCH 1813
2024-02-06 01:39:00,724 Epoch 1813: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-06 01:39:00,725 EPOCH 1814
2024-02-06 01:39:05,743 Epoch 1814: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 01:39:05,743 EPOCH 1815
2024-02-06 01:39:08,978 [Epoch: 1815 Step: 00061700] Batch Recognition Loss:   0.000237 => Gls Tokens per Sec:     2294 || Batch Translation Loss:   0.015778 => Txt Tokens per Sec:     6618 || Lr: 0.000050
2024-02-06 01:39:09,937 Epoch 1815: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-06 01:39:09,937 EPOCH 1816
2024-02-06 01:39:14,706 Epoch 1816: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.90 
2024-02-06 01:39:14,707 EPOCH 1817
2024-02-06 01:39:19,169 Epoch 1817: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-06 01:39:19,169 EPOCH 1818
2024-02-06 01:39:22,307 [Epoch: 1818 Step: 00061800] Batch Recognition Loss:   0.001097 => Gls Tokens per Sec:     2161 || Batch Translation Loss:   0.025979 => Txt Tokens per Sec:     6265 || Lr: 0.000050
2024-02-06 01:39:23,735 Epoch 1818: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-06 01:39:23,735 EPOCH 1819
2024-02-06 01:39:28,350 Epoch 1819: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-06 01:39:28,350 EPOCH 1820
2024-02-06 01:39:32,707 Epoch 1820: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-06 01:39:32,708 EPOCH 1821
2024-02-06 01:39:35,777 [Epoch: 1821 Step: 00061900] Batch Recognition Loss:   0.000605 => Gls Tokens per Sec:     2087 || Batch Translation Loss:   0.021486 => Txt Tokens per Sec:     5677 || Lr: 0.000050
2024-02-06 01:39:37,640 Epoch 1821: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-06 01:39:37,640 EPOCH 1822
2024-02-06 01:39:41,868 Epoch 1822: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 01:39:41,869 EPOCH 1823
2024-02-06 01:39:46,791 Epoch 1823: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-06 01:39:46,791 EPOCH 1824
2024-02-06 01:39:48,851 [Epoch: 1824 Step: 00062000] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     2798 || Batch Translation Loss:   0.015785 => Txt Tokens per Sec:     7575 || Lr: 0.000050
2024-02-06 01:39:58,070 Validation result at epoch 1824, step    62000: duration: 9.2189s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.08389	Translation Loss: 96123.72656	PPL: 14777.35254
	Eval Metric: BLEU
	WER 2.75	(DEL: 0.00,	INS: 0.00,	SUB: 2.75)
	BLEU-4 0.43	(BLEU-1: 9.97,	BLEU-2: 2.79,	BLEU-3: 1.06,	BLEU-4: 0.43)
	CHRF 16.93	ROUGE 8.40
2024-02-06 01:39:58,071 Logging Recognition and Translation Outputs
2024-02-06 01:39:58,071 ========================================================================================================================
2024-02-06 01:39:58,071 Logging Sequence: 165_523.00
2024-02-06 01:39:58,071 	Gloss Reference :	A B+C+D+E
2024-02-06 01:39:58,071 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:39:58,072 	Gloss Alignment :	         
2024-02-06 01:39:58,072 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:39:58,072 	Text Reference  :	as he believed that his team might lose if   he takes off his batting pads    
2024-02-06 01:39:58,073 	Text Hypothesis :	** ** ******** **** *** **** ***** **** when he came  out to  penalty shootout
2024-02-06 01:39:58,073 	Text Alignment  :	D  D  D        D    D   D    D     D    S       S     S   S   S       S       
2024-02-06 01:39:58,073 ========================================================================================================================
2024-02-06 01:39:58,073 Logging Sequence: 165_233.00
2024-02-06 01:39:58,073 	Gloss Reference :	A B+C+D+E
2024-02-06 01:39:58,073 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:39:58,073 	Gloss Alignment :	         
2024-02-06 01:39:58,074 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:39:58,075 	Text Reference  :	irrespective of whether he was playing the match or not he always sat with his bag he was happy when    the          team     won    
2024-02-06 01:39:58,075 	Text Hypothesis :	************ ** ******* ** *** ******* *** ***** ** *** ** ****** *** **** *** *** ** to  play  against kazakhstan's nurislam sanayev
2024-02-06 01:39:58,075 	Text Alignment  :	D            D  D       D  D   D       D   D     D  D   D  D      D   D    D   D   D  S   S     S       S            S        S      
2024-02-06 01:39:58,075 ========================================================================================================================
2024-02-06 01:39:58,075 Logging Sequence: 169_214.00
2024-02-06 01:39:58,075 	Gloss Reference :	A B+C+D+E
2024-02-06 01:39:58,076 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:39:58,076 	Gloss Alignment :	         
2024-02-06 01:39:58,076 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:39:58,078 	Text Reference  :	virat kohli said that though arshdeep dropped the       catch    he          is still  a    strong part   of  the    indian     team
2024-02-06 01:39:58,078 	Text Hypothesis :	do    you   know that ****** ******** ******* wikipedia provides information on celebs like their  height age family background etc 
2024-02-06 01:39:58,078 	Text Alignment  :	S     S     S         D      D        D       S         S        S           S  S      S    S      S      S   S      S          S   
2024-02-06 01:39:58,078 ========================================================================================================================
2024-02-06 01:39:58,078 Logging Sequence: 88_67.00
2024-02-06 01:39:58,078 	Gloss Reference :	A B+C+D+E    
2024-02-06 01:39:58,078 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-06 01:39:58,079 	Gloss Alignment :	  S          
2024-02-06 01:39:58,079 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:39:58,080 	Text Reference  :	pablo javkin the         mayor of   rosario is  also      a        drug   trafficker so  he  won't    take  care of       you        
2024-02-06 01:39:58,081 	Text Hypothesis :	***** police authorities said  that they    are reviewing security camera footage    and his culprits would be   punished accordingly
2024-02-06 01:39:58,081 	Text Alignment  :	D     S      S           S     S    S       S   S         S        S      S          S   S   S        S     S    S        S          
2024-02-06 01:39:58,081 ========================================================================================================================
2024-02-06 01:39:58,081 Logging Sequence: 69_95.00
2024-02-06 01:39:58,081 	Gloss Reference :	A B+C+D+E
2024-02-06 01:39:58,081 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:39:58,081 	Gloss Alignment :	         
2024-02-06 01:39:58,081 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:39:58,083 	Text Reference  :	***** ** a six and a   four sealed csk's    victory     and ********** ****** ***** the team won the match
2024-02-06 01:39:58,083 	Text Hypothesis :	there is a end of  the you  all    football association and merseyside police filed the end  of  the field
2024-02-06 01:39:58,083 	Text Alignment  :	I     I    S   S   S   S    S      S        S               I          I      I         S    S       S    
2024-02-06 01:39:58,083 ========================================================================================================================
2024-02-06 01:40:00,491 Epoch 1824: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-06 01:40:00,491 EPOCH 1825
2024-02-06 01:40:05,464 Epoch 1825: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.95 
2024-02-06 01:40:05,465 EPOCH 1826
2024-02-06 01:40:09,828 Epoch 1826: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.02 
2024-02-06 01:40:09,828 EPOCH 1827
2024-02-06 01:40:12,192 [Epoch: 1827 Step: 00062100] Batch Recognition Loss:   0.000653 => Gls Tokens per Sec:     2056 || Batch Translation Loss:   0.016331 => Txt Tokens per Sec:     5952 || Lr: 0.000050
2024-02-06 01:40:14,517 Epoch 1827: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.00 
2024-02-06 01:40:14,518 EPOCH 1828
2024-02-06 01:40:19,055 Epoch 1828: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.00 
2024-02-06 01:40:19,056 EPOCH 1829
2024-02-06 01:40:23,631 Epoch 1829: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.92 
2024-02-06 01:40:23,631 EPOCH 1830
2024-02-06 01:40:25,395 [Epoch: 1830 Step: 00062200] Batch Recognition Loss:   0.000175 => Gls Tokens per Sec:     2542 || Batch Translation Loss:   0.016781 => Txt Tokens per Sec:     6416 || Lr: 0.000050
2024-02-06 01:40:28,272 Epoch 1830: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-06 01:40:28,272 EPOCH 1831
2024-02-06 01:40:32,696 Epoch 1831: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.97 
2024-02-06 01:40:32,697 EPOCH 1832
2024-02-06 01:40:37,545 Epoch 1832: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-06 01:40:37,545 EPOCH 1833
2024-02-06 01:40:39,152 [Epoch: 1833 Step: 00062300] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     2229 || Batch Translation Loss:   0.019335 => Txt Tokens per Sec:     6365 || Lr: 0.000050
2024-02-06 01:40:41,709 Epoch 1833: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-06 01:40:41,710 EPOCH 1834
2024-02-06 01:40:46,814 Epoch 1834: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.81 
2024-02-06 01:40:46,815 EPOCH 1835
2024-02-06 01:40:51,505 Epoch 1835: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.89 
2024-02-06 01:40:51,506 EPOCH 1836
2024-02-06 01:40:52,981 [Epoch: 1836 Step: 00062400] Batch Recognition Loss:   0.000654 => Gls Tokens per Sec:     2171 || Batch Translation Loss:   0.026725 => Txt Tokens per Sec:     5968 || Lr: 0.000050
2024-02-06 01:40:56,383 Epoch 1836: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-06 01:40:56,383 EPOCH 1837
2024-02-06 01:41:00,495 Epoch 1837: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-06 01:41:00,495 EPOCH 1838
2024-02-06 01:41:05,315 Epoch 1838: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.61 
2024-02-06 01:41:05,315 EPOCH 1839
2024-02-06 01:41:06,323 [Epoch: 1839 Step: 00062500] Batch Recognition Loss:   0.000202 => Gls Tokens per Sec:     2544 || Batch Translation Loss:   0.015854 => Txt Tokens per Sec:     7141 || Lr: 0.000050
2024-02-06 01:41:09,747 Epoch 1839: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-06 01:41:09,748 EPOCH 1840
2024-02-06 01:41:14,462 Epoch 1840: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-06 01:41:14,463 EPOCH 1841
2024-02-06 01:41:18,922 Epoch 1841: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-06 01:41:18,922 EPOCH 1842
2024-02-06 01:41:19,601 [Epoch: 1842 Step: 00062600] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:     2452 || Batch Translation Loss:   0.006666 => Txt Tokens per Sec:     6464 || Lr: 0.000050
2024-02-06 01:41:23,510 Epoch 1842: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 01:41:23,511 EPOCH 1843
2024-02-06 01:41:28,672 Epoch 1843: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 01:41:28,673 EPOCH 1844
2024-02-06 01:41:33,307 Epoch 1844: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 01:41:33,308 EPOCH 1845
2024-02-06 01:41:33,591 [Epoch: 1845 Step: 00062700] Batch Recognition Loss:   0.000117 => Gls Tokens per Sec:     4549 || Batch Translation Loss:   0.007003 => Txt Tokens per Sec:     9506 || Lr: 0.000050
2024-02-06 01:41:37,776 Epoch 1845: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 01:41:37,777 EPOCH 1846
2024-02-06 01:41:42,501 Epoch 1846: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 01:41:42,501 EPOCH 1847
2024-02-06 01:41:46,765 Epoch 1847: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 01:41:46,765 EPOCH 1848
2024-02-06 01:41:47,247 [Epoch: 1848 Step: 00062800] Batch Recognition Loss:   0.000137 => Gls Tokens per Sec:     1337 || Batch Translation Loss:   0.015882 => Txt Tokens per Sec:     4520 || Lr: 0.000050
2024-02-06 01:41:51,691 Epoch 1848: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-06 01:41:51,691 EPOCH 1849
2024-02-06 01:41:55,816 Epoch 1849: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-06 01:41:55,817 EPOCH 1850
2024-02-06 01:42:00,822 [Epoch: 1850 Step: 00062900] Batch Recognition Loss:   0.000270 => Gls Tokens per Sec:     2122 || Batch Translation Loss:   0.247056 => Txt Tokens per Sec:     5870 || Lr: 0.000050
2024-02-06 01:42:00,823 Epoch 1850: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.18 
2024-02-06 01:42:00,823 EPOCH 1851
2024-02-06 01:42:05,093 Epoch 1851: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.20 
2024-02-06 01:42:05,094 EPOCH 1852
2024-02-06 01:42:09,181 Epoch 1852: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.58 
2024-02-06 01:42:09,181 EPOCH 1853
2024-02-06 01:42:13,114 [Epoch: 1853 Step: 00063000] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     2538 || Batch Translation Loss:   0.009729 => Txt Tokens per Sec:     7054 || Lr: 0.000050
2024-02-06 01:42:13,331 Epoch 1853: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-06 01:42:13,332 EPOCH 1854
2024-02-06 01:42:17,788 Epoch 1854: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.87 
2024-02-06 01:42:17,789 EPOCH 1855
2024-02-06 01:42:22,570 Epoch 1855: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-06 01:42:22,570 EPOCH 1856
2024-02-06 01:42:25,890 [Epoch: 1856 Step: 00063100] Batch Recognition Loss:   0.000266 => Gls Tokens per Sec:     2814 || Batch Translation Loss:   0.043924 => Txt Tokens per Sec:     7654 || Lr: 0.000050
2024-02-06 01:42:26,723 Epoch 1856: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.66 
2024-02-06 01:42:26,724 EPOCH 1857
2024-02-06 01:42:31,632 Epoch 1857: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.23 
2024-02-06 01:42:31,632 EPOCH 1858
2024-02-06 01:42:35,833 Epoch 1858: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-06 01:42:35,834 EPOCH 1859
2024-02-06 01:42:40,032 [Epoch: 1859 Step: 00063200] Batch Recognition Loss:   0.000346 => Gls Tokens per Sec:     2134 || Batch Translation Loss:   0.026214 => Txt Tokens per Sec:     5934 || Lr: 0.000050
2024-02-06 01:42:40,754 Epoch 1859: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.71 
2024-02-06 01:42:40,754 EPOCH 1860
2024-02-06 01:42:45,142 Epoch 1860: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.71 
2024-02-06 01:42:45,143 EPOCH 1861
2024-02-06 01:42:49,932 Epoch 1861: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.29 
2024-02-06 01:42:49,933 EPOCH 1862
2024-02-06 01:42:53,298 [Epoch: 1862 Step: 00063300] Batch Recognition Loss:   0.000295 => Gls Tokens per Sec:     2473 || Batch Translation Loss:   0.015289 => Txt Tokens per Sec:     6813 || Lr: 0.000050
2024-02-06 01:42:54,366 Epoch 1862: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.81 
2024-02-06 01:42:54,367 EPOCH 1863
2024-02-06 01:42:58,980 Epoch 1863: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.66 
2024-02-06 01:42:58,980 EPOCH 1864
2024-02-06 01:43:03,572 Epoch 1864: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-06 01:43:03,573 EPOCH 1865
2024-02-06 01:43:06,592 [Epoch: 1865 Step: 00063400] Batch Recognition Loss:   0.003390 => Gls Tokens per Sec:     2459 || Batch Translation Loss:   0.020776 => Txt Tokens per Sec:     6952 || Lr: 0.000050
2024-02-06 01:43:07,983 Epoch 1865: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-06 01:43:07,984 EPOCH 1866
2024-02-06 01:43:12,713 Epoch 1866: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-06 01:43:12,713 EPOCH 1867
2024-02-06 01:43:16,829 Epoch 1867: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.78 
2024-02-06 01:43:16,829 EPOCH 1868
2024-02-06 01:43:19,913 [Epoch: 1868 Step: 00063500] Batch Recognition Loss:   0.000502 => Gls Tokens per Sec:     2200 || Batch Translation Loss:   0.019423 => Txt Tokens per Sec:     5930 || Lr: 0.000050
2024-02-06 01:43:21,867 Epoch 1868: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-06 01:43:21,867 EPOCH 1869
2024-02-06 01:43:26,057 Epoch 1869: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-06 01:43:26,057 EPOCH 1870
2024-02-06 01:43:30,992 Epoch 1870: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.39 
2024-02-06 01:43:30,993 EPOCH 1871
2024-02-06 01:43:33,482 [Epoch: 1871 Step: 00063600] Batch Recognition Loss:   0.000275 => Gls Tokens per Sec:     2467 || Batch Translation Loss:   0.234231 => Txt Tokens per Sec:     6753 || Lr: 0.000050
2024-02-06 01:43:35,317 Epoch 1871: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.63 
2024-02-06 01:43:35,318 EPOCH 1872
2024-02-06 01:43:40,155 Epoch 1872: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.23 
2024-02-06 01:43:40,155 EPOCH 1873
2024-02-06 01:43:44,516 Epoch 1873: Total Training Recognition Loss 0.13  Total Training Translation Loss 0.79 
2024-02-06 01:43:44,516 EPOCH 1874
2024-02-06 01:43:46,864 [Epoch: 1874 Step: 00063700] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:     2343 || Batch Translation Loss:   0.010603 => Txt Tokens per Sec:     6734 || Lr: 0.000050
2024-02-06 01:43:49,017 Epoch 1874: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.64 
2024-02-06 01:43:49,018 EPOCH 1875
2024-02-06 01:43:53,731 Epoch 1875: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.64 
2024-02-06 01:43:53,731 EPOCH 1876
2024-02-06 01:43:57,996 Epoch 1876: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.70 
2024-02-06 01:43:57,997 EPOCH 1877
2024-02-06 01:44:00,480 [Epoch: 1877 Step: 00063800] Batch Recognition Loss:   0.029884 => Gls Tokens per Sec:     1958 || Batch Translation Loss:   0.023049 => Txt Tokens per Sec:     5474 || Lr: 0.000050
2024-02-06 01:44:02,972 Epoch 1877: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.76 
2024-02-06 01:44:02,972 EPOCH 1878
2024-02-06 01:44:07,085 Epoch 1878: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.59 
2024-02-06 01:44:07,085 EPOCH 1879
2024-02-06 01:44:12,045 Epoch 1879: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-06 01:44:12,045 EPOCH 1880
2024-02-06 01:44:13,766 [Epoch: 1880 Step: 00063900] Batch Recognition Loss:   0.000336 => Gls Tokens per Sec:     2454 || Batch Translation Loss:   0.009507 => Txt Tokens per Sec:     6718 || Lr: 0.000050
2024-02-06 01:44:16,353 Epoch 1880: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-06 01:44:16,353 EPOCH 1881
2024-02-06 01:44:21,408 Epoch 1881: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-06 01:44:21,409 EPOCH 1882
2024-02-06 01:44:26,041 Epoch 1882: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.81 
2024-02-06 01:44:26,041 EPOCH 1883
2024-02-06 01:44:27,571 [Epoch: 1883 Step: 00064000] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:     2511 || Batch Translation Loss:   0.021230 => Txt Tokens per Sec:     7135 || Lr: 0.000050
2024-02-06 01:44:36,551 Validation result at epoch 1883, step    64000: duration: 8.9782s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.94836	Translation Loss: 96630.06250	PPL: 15543.91309
	Eval Metric: BLEU
	WER 2.90	(DEL: 0.00,	INS: 0.00,	SUB: 2.90)
	BLEU-4 0.46	(BLEU-1: 10.38,	BLEU-2: 3.09,	BLEU-3: 1.03,	BLEU-4: 0.46)
	CHRF 17.15	ROUGE 8.70
2024-02-06 01:44:36,552 Logging Recognition and Translation Outputs
2024-02-06 01:44:36,552 ========================================================================================================================
2024-02-06 01:44:36,552 Logging Sequence: 122_86.00
2024-02-06 01:44:36,552 	Gloss Reference :	A B+C+D+E
2024-02-06 01:44:36,552 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:44:36,552 	Gloss Alignment :	         
2024-02-06 01:44:36,553 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:44:36,553 	Text Reference  :	** after winning chanu spoke   to  the    media and       said  
2024-02-06 01:44:36,553 	Text Hypothesis :	he said  that    the   ronaldo has issued a     statement saying
2024-02-06 01:44:36,553 	Text Alignment  :	I  S     S       S     S       S   S      S     S         S     
2024-02-06 01:44:36,554 ========================================================================================================================
2024-02-06 01:44:36,554 Logging Sequence: 82_81.00
2024-02-06 01:44:36,554 	Gloss Reference :	A B+C+D+E
2024-02-06 01:44:36,554 	Gloss Hypothesis:	A B+C+D  
2024-02-06 01:44:36,554 	Gloss Alignment :	  S      
2024-02-06 01:44:36,554 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:44:36,555 	Text Reference  :	since the couple were residents of mumbai the       mumbai police  cyber cell began investigating the matter
2024-02-06 01:44:36,556 	Text Hypothesis :	***** the ****** **** ********* ** bcci   president sourav ganguly along with board secretary     jay shah  
2024-02-06 01:44:36,556 	Text Alignment  :	D         D      D    D         D  S      S         S      S       S     S    S     S             S   S     
2024-02-06 01:44:36,556 ========================================================================================================================
2024-02-06 01:44:36,556 Logging Sequence: 61_65.00
2024-02-06 01:44:36,556 	Gloss Reference :	A B+C+D+E
2024-02-06 01:44:36,556 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:44:36,556 	Gloss Alignment :	         
2024-02-06 01:44:36,557 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:44:36,557 	Text Reference  :	the name seems indian   but  whether it  has been made by  an  indian
2024-02-06 01:44:36,558 	Text Hypothesis :	*** **** ***** everyone were all     out and not  know who had tested
2024-02-06 01:44:36,558 	Text Alignment  :	D   D    D     S        S    S       S   S   S    S    S   S   S     
2024-02-06 01:44:36,558 ========================================================================================================================
2024-02-06 01:44:36,558 Logging Sequence: 179_126.00
2024-02-06 01:44:36,558 	Gloss Reference :	A B+C+D+E
2024-02-06 01:44:36,558 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:44:36,558 	Gloss Alignment :	         
2024-02-06 01:44:36,558 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:44:36,560 	Text Reference  :	vinesh argued that she might      contract coronavirus since  these wrestlers travelled from india where there       are many infections
2024-02-06 01:44:36,560 	Text Hypothesis :	****** ****** and  the federation or       sai         people are   well      to        see  him   a     deaflympics at  his  residence 
2024-02-06 01:44:36,560 	Text Alignment  :	D      D      S    S   S          S        S           S      S     S         S         S    S     S     S           S   S    S         
2024-02-06 01:44:36,560 ========================================================================================================================
2024-02-06 01:44:36,561 Logging Sequence: 62_24.00
2024-02-06 01:44:36,561 	Gloss Reference :	A B+C+D+E
2024-02-06 01:44:36,561 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:44:36,561 	Gloss Alignment :	         
2024-02-06 01:44:36,561 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:44:36,564 	Text Reference  :	*** *** **** now  the ***** ********** *** *** women's cricket team too    is   giving    splendid performances which are at      par with the men's         team   
2024-02-06 01:44:36,564 	Text Hypothesis :	you all know that the women cricketers can see india   england in   ensure that cricketer was      very         b     by  parents and know in  international matches
2024-02-06 01:44:36,564 	Text Alignment  :	I   I   I    S        I     I          I   I   S       S       S    S      S    S         S        S            S     S   S       S   S    S   S             S      
2024-02-06 01:44:36,564 ========================================================================================================================
2024-02-06 01:44:39,643 Epoch 1883: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.98 
2024-02-06 01:44:39,644 EPOCH 1884
2024-02-06 01:44:44,614 Epoch 1884: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-06 01:44:44,614 EPOCH 1885
2024-02-06 01:44:48,908 Epoch 1885: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-06 01:44:48,908 EPOCH 1886
2024-02-06 01:44:50,173 [Epoch: 1886 Step: 00064100] Batch Recognition Loss:   0.000228 => Gls Tokens per Sec:     2533 || Batch Translation Loss:   0.026529 => Txt Tokens per Sec:     7046 || Lr: 0.000050
2024-02-06 01:44:53,802 Epoch 1886: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-06 01:44:53,802 EPOCH 1887
2024-02-06 01:44:58,182 Epoch 1887: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 01:44:58,183 EPOCH 1888
2024-02-06 01:45:02,296 Epoch 1888: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-06 01:45:02,296 EPOCH 1889
2024-02-06 01:45:03,124 [Epoch: 1889 Step: 00064200] Batch Recognition Loss:   0.000224 => Gls Tokens per Sec:     2781 || Batch Translation Loss:   0.020544 => Txt Tokens per Sec:     7030 || Lr: 0.000050
2024-02-06 01:45:07,188 Epoch 1889: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-06 01:45:07,189 EPOCH 1890
2024-02-06 01:45:11,547 Epoch 1890: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-06 01:45:11,547 EPOCH 1891
2024-02-06 01:45:16,112 Epoch 1891: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.39 
2024-02-06 01:45:16,112 EPOCH 1892
2024-02-06 01:45:16,952 [Epoch: 1892 Step: 00064300] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:     2288 || Batch Translation Loss:   0.024736 => Txt Tokens per Sec:     6398 || Lr: 0.000050
2024-02-06 01:45:20,752 Epoch 1892: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.83 
2024-02-06 01:45:20,752 EPOCH 1893
2024-02-06 01:45:25,167 Epoch 1893: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.66 
2024-02-06 01:45:25,168 EPOCH 1894
2024-02-06 01:45:30,052 Epoch 1894: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.81 
2024-02-06 01:45:30,052 EPOCH 1895
2024-02-06 01:45:30,448 [Epoch: 1895 Step: 00064400] Batch Recognition Loss:   0.000311 => Gls Tokens per Sec:     3245 || Batch Translation Loss:   0.043285 => Txt Tokens per Sec:     9153 || Lr: 0.000050
2024-02-06 01:45:34,208 Epoch 1895: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.20 
2024-02-06 01:45:34,209 EPOCH 1896
2024-02-06 01:45:39,232 Epoch 1896: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.10 
2024-02-06 01:45:39,232 EPOCH 1897
2024-02-06 01:45:43,440 Epoch 1897: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.74 
2024-02-06 01:45:43,440 EPOCH 1898
2024-02-06 01:45:43,671 [Epoch: 1898 Step: 00064500] Batch Recognition Loss:   0.000268 => Gls Tokens per Sec:     2783 || Batch Translation Loss:   0.018394 => Txt Tokens per Sec:     6343 || Lr: 0.000050
2024-02-06 01:45:48,380 Epoch 1898: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-06 01:45:48,380 EPOCH 1899
2024-02-06 01:45:53,200 Epoch 1899: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 01:45:53,201 EPOCH 1900
2024-02-06 01:45:57,829 [Epoch: 1900 Step: 00064600] Batch Recognition Loss:   0.000225 => Gls Tokens per Sec:     2295 || Batch Translation Loss:   0.013576 => Txt Tokens per Sec:     6350 || Lr: 0.000050
2024-02-06 01:45:57,830 Epoch 1900: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 01:45:57,830 EPOCH 1901
2024-02-06 01:46:01,948 Epoch 1901: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-06 01:46:01,948 EPOCH 1902
2024-02-06 01:46:06,859 Epoch 1902: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 01:46:06,859 EPOCH 1903
2024-02-06 01:46:11,014 [Epoch: 1903 Step: 00064700] Batch Recognition Loss:   0.000178 => Gls Tokens per Sec:     2403 || Batch Translation Loss:   0.040154 => Txt Tokens per Sec:     6708 || Lr: 0.000050
2024-02-06 01:46:11,182 Epoch 1903: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-06 01:46:11,182 EPOCH 1904
2024-02-06 01:46:15,859 Epoch 1904: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-06 01:46:15,859 EPOCH 1905
2024-02-06 01:46:20,387 Epoch 1905: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-06 01:46:20,387 EPOCH 1906
2024-02-06 01:46:24,549 [Epoch: 1906 Step: 00064800] Batch Recognition Loss:   0.000202 => Gls Tokens per Sec:     2307 || Batch Translation Loss:   0.045871 => Txt Tokens per Sec:     6402 || Lr: 0.000050
2024-02-06 01:46:24,981 Epoch 1906: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-06 01:46:24,981 EPOCH 1907
2024-02-06 01:46:29,709 Epoch 1907: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-06 01:46:29,709 EPOCH 1908
2024-02-06 01:46:34,248 Epoch 1908: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-06 01:46:34,249 EPOCH 1909
2024-02-06 01:46:38,007 [Epoch: 1909 Step: 00064900] Batch Recognition Loss:   0.000345 => Gls Tokens per Sec:     2316 || Batch Translation Loss:   0.015946 => Txt Tokens per Sec:     6430 || Lr: 0.000050
2024-02-06 01:46:38,996 Epoch 1909: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-06 01:46:38,996 EPOCH 1910
2024-02-06 01:46:43,408 Epoch 1910: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.24 
2024-02-06 01:46:43,408 EPOCH 1911
2024-02-06 01:46:48,281 Epoch 1911: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.28 
2024-02-06 01:46:48,282 EPOCH 1912
2024-02-06 01:46:51,522 [Epoch: 1912 Step: 00065000] Batch Recognition Loss:   0.000752 => Gls Tokens per Sec:     2488 || Batch Translation Loss:   0.036296 => Txt Tokens per Sec:     7002 || Lr: 0.000050
2024-02-06 01:46:52,384 Epoch 1912: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.20 
2024-02-06 01:46:52,385 EPOCH 1913
2024-02-06 01:46:57,561 Epoch 1913: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.28 
2024-02-06 01:46:57,562 EPOCH 1914
2024-02-06 01:47:01,987 Epoch 1914: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.88 
2024-02-06 01:47:01,987 EPOCH 1915
2024-02-06 01:47:05,039 [Epoch: 1915 Step: 00065100] Batch Recognition Loss:   0.000469 => Gls Tokens per Sec:     2518 || Batch Translation Loss:   0.016061 => Txt Tokens per Sec:     7177 || Lr: 0.000050
2024-02-06 01:47:06,417 Epoch 1915: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-06 01:47:06,418 EPOCH 1916
2024-02-06 01:47:11,219 Epoch 1916: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-06 01:47:11,219 EPOCH 1917
2024-02-06 01:47:15,409 Epoch 1917: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-06 01:47:15,410 EPOCH 1918
2024-02-06 01:47:18,736 [Epoch: 1918 Step: 00065200] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:     2039 || Batch Translation Loss:   0.011195 => Txt Tokens per Sec:     5713 || Lr: 0.000050
2024-02-06 01:47:20,291 Epoch 1918: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-06 01:47:20,291 EPOCH 1919
2024-02-06 01:47:24,502 Epoch 1919: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-06 01:47:24,502 EPOCH 1920
2024-02-06 01:47:29,291 Epoch 1920: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-06 01:47:29,292 EPOCH 1921
2024-02-06 01:47:31,896 [Epoch: 1921 Step: 00065300] Batch Recognition Loss:   0.001705 => Gls Tokens per Sec:     2359 || Batch Translation Loss:   0.010455 => Txt Tokens per Sec:     6551 || Lr: 0.000050
2024-02-06 01:47:33,743 Epoch 1921: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-06 01:47:33,743 EPOCH 1922
2024-02-06 01:47:38,415 Epoch 1922: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.51 
2024-02-06 01:47:38,416 EPOCH 1923
2024-02-06 01:47:43,074 Epoch 1923: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-06 01:47:43,075 EPOCH 1924
2024-02-06 01:47:45,619 [Epoch: 1924 Step: 00065400] Batch Recognition Loss:   0.000226 => Gls Tokens per Sec:     2266 || Batch Translation Loss:   0.022617 => Txt Tokens per Sec:     6209 || Lr: 0.000050
2024-02-06 01:47:48,066 Epoch 1924: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-06 01:47:48,067 EPOCH 1925
2024-02-06 01:47:52,608 Epoch 1925: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-06 01:47:52,608 EPOCH 1926
2024-02-06 01:47:56,713 Epoch 1926: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-06 01:47:56,713 EPOCH 1927
2024-02-06 01:47:59,279 [Epoch: 1927 Step: 00065500] Batch Recognition Loss:   0.002726 => Gls Tokens per Sec:     1998 || Batch Translation Loss:   0.024298 => Txt Tokens per Sec:     5501 || Lr: 0.000050
2024-02-06 01:48:01,761 Epoch 1927: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-06 01:48:01,762 EPOCH 1928
2024-02-06 01:48:05,967 Epoch 1928: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-06 01:48:05,967 EPOCH 1929
2024-02-06 01:48:10,899 Epoch 1929: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-06 01:48:10,900 EPOCH 1930
2024-02-06 01:48:12,687 [Epoch: 1930 Step: 00065600] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:     2510 || Batch Translation Loss:   0.014756 => Txt Tokens per Sec:     7001 || Lr: 0.000050
2024-02-06 01:48:15,303 Epoch 1930: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.41 
2024-02-06 01:48:15,304 EPOCH 1931
2024-02-06 01:48:20,083 Epoch 1931: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.37 
2024-02-06 01:48:20,084 EPOCH 1932
2024-02-06 01:48:24,531 Epoch 1932: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.51 
2024-02-06 01:48:24,531 EPOCH 1933
2024-02-06 01:48:25,741 [Epoch: 1933 Step: 00065700] Batch Recognition Loss:   0.000404 => Gls Tokens per Sec:     2963 || Batch Translation Loss:   0.041298 => Txt Tokens per Sec:     7739 || Lr: 0.000050
2024-02-06 01:48:29,157 Epoch 1933: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.84 
2024-02-06 01:48:29,158 EPOCH 1934
2024-02-06 01:48:33,796 Epoch 1934: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.53 
2024-02-06 01:48:33,796 EPOCH 1935
2024-02-06 01:48:38,202 Epoch 1935: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.95 
2024-02-06 01:48:38,203 EPOCH 1936
2024-02-06 01:48:39,660 [Epoch: 1936 Step: 00065800] Batch Recognition Loss:   0.000489 => Gls Tokens per Sec:     2197 || Batch Translation Loss:   0.029333 => Txt Tokens per Sec:     6251 || Lr: 0.000050
2024-02-06 01:48:43,034 Epoch 1936: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-06 01:48:43,034 EPOCH 1937
2024-02-06 01:48:47,236 Epoch 1937: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-06 01:48:47,237 EPOCH 1938
2024-02-06 01:48:52,200 Epoch 1938: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.68 
2024-02-06 01:48:52,200 EPOCH 1939
2024-02-06 01:48:53,047 [Epoch: 1939 Step: 00065900] Batch Recognition Loss:   0.000309 => Gls Tokens per Sec:     2718 || Batch Translation Loss:   0.010857 => Txt Tokens per Sec:     6967 || Lr: 0.000050
2024-02-06 01:48:56,489 Epoch 1939: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-06 01:48:56,489 EPOCH 1940
2024-02-06 01:49:01,419 Epoch 1940: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-06 01:49:01,420 EPOCH 1941
2024-02-06 01:49:05,562 Epoch 1941: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.61 
2024-02-06 01:49:05,562 EPOCH 1942
2024-02-06 01:49:06,550 [Epoch: 1942 Step: 00066000] Batch Recognition Loss:   0.001446 => Gls Tokens per Sec:     1946 || Batch Translation Loss:   0.029204 => Txt Tokens per Sec:     5595 || Lr: 0.000050
2024-02-06 01:49:15,277 Validation result at epoch 1942, step    66000: duration: 8.7256s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.99667	Translation Loss: 97896.29688	PPL: 17639.50000
	Eval Metric: BLEU
	WER 2.33	(DEL: 0.00,	INS: 0.00,	SUB: 2.33)
	BLEU-4 0.38	(BLEU-1: 10.11,	BLEU-2: 2.75,	BLEU-3: 0.92,	BLEU-4: 0.38)
	CHRF 16.70	ROUGE 8.68
2024-02-06 01:49:15,278 Logging Recognition and Translation Outputs
2024-02-06 01:49:15,278 ========================================================================================================================
2024-02-06 01:49:15,278 Logging Sequence: 85_97.00
2024-02-06 01:49:15,279 	Gloss Reference :	A B+C+D+E
2024-02-06 01:49:15,279 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:49:15,279 	Gloss Alignment :	         
2024-02-06 01:49:15,279 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:49:15,280 	Text Reference  :	******* *** ****** * ********* ** ** like  india's bcci australia has cricket australia
2024-02-06 01:49:15,280 	Text Hypothesis :	symonds has scored 2 centuries in 26 tests that    he   played    for his     country  
2024-02-06 01:49:15,280 	Text Alignment  :	I       I   I      I I         I  I  S     S       S    S         S   S       S        
2024-02-06 01:49:15,280 ========================================================================================================================
2024-02-06 01:49:15,280 Logging Sequence: 53_161.00
2024-02-06 01:49:15,281 	Gloss Reference :	A B+C+D+E
2024-02-06 01:49:15,281 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:49:15,281 	Gloss Alignment :	         
2024-02-06 01:49:15,281 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:49:15,283 	Text Reference  :	rashid has     also     been urging people  to donate   to    his      rashid khan foundation and     afghanistan cricket association
2024-02-06 01:49:15,283 	Text Hypothesis :	****** another incident was  a      picture of mohammed shami fielding with   a    towel      wrapped over        his     jersey     
2024-02-06 01:49:15,283 	Text Alignment  :	D      S       S        S    S      S       S  S        S     S        S      S    S          S       S           S       S          
2024-02-06 01:49:15,284 ========================================================================================================================
2024-02-06 01:49:15,284 Logging Sequence: 101_97.00
2024-02-06 01:49:15,284 	Gloss Reference :	A B+C+D+E
2024-02-06 01:49:15,284 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:49:15,284 	Gloss Alignment :	         
2024-02-06 01:49:15,285 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:49:15,287 	Text Reference  :	2 of the best indian bowlers who bowled very well were raj     bawa    who     took 5   wickets and   ravi kumar who    took 4  wickets
2024-02-06 01:49:15,287 	Text Hypothesis :	* ** *** **** ****** ******* *** ****** **** **** when england started batting the  won the     first time a     scored only 50 runs   
2024-02-06 01:49:15,287 	Text Alignment  :	D D  D   D    D      D       D   D      D    D    S    S       S       S       S    S   S       S     S    S     S      S    S  S      
2024-02-06 01:49:15,287 ========================================================================================================================
2024-02-06 01:49:15,287 Logging Sequence: 118_130.00
2024-02-06 01:49:15,287 	Gloss Reference :	A B+C+D+E
2024-02-06 01:49:15,288 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:49:15,288 	Gloss Alignment :	         
2024-02-06 01:49:15,288 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:49:15,288 	Text Reference  :	messi's fans the entire team were in tears everyone was overwhelmed by       the    victory
2024-02-06 01:49:15,289 	Text Hypothesis :	******* **** *** ****** **** **** ** ***** this     was krunal      pandya's maiden odi    
2024-02-06 01:49:15,289 	Text Alignment  :	D       D    D   D      D    D    D  D     S            S           S        S      S      
2024-02-06 01:49:15,289 ========================================================================================================================
2024-02-06 01:49:15,289 Logging Sequence: 170_195.00
2024-02-06 01:49:15,289 	Gloss Reference :	A B+C+D+E
2024-02-06 01:49:15,289 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:49:15,289 	Gloss Alignment :	         
2024-02-06 01:49:15,289 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:49:15,290 	Text Reference  :	i moved to   rajasthan royals team  as  they      paid me     8     crores
2024-02-06 01:49:15,290 	Text Hypothesis :	* they  will be        told   their fan following the  social media 2023  
2024-02-06 01:49:15,291 	Text Alignment  :	D S     S    S         S      S     S   S         S    S      S     S     
2024-02-06 01:49:15,291 ========================================================================================================================
2024-02-06 01:49:19,263 Epoch 1942: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.81 
2024-02-06 01:49:19,264 EPOCH 1943
2024-02-06 01:49:23,746 Epoch 1943: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.03 
2024-02-06 01:49:23,746 EPOCH 1944
2024-02-06 01:49:28,249 Epoch 1944: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.48 
2024-02-06 01:49:28,250 EPOCH 1945
2024-02-06 01:49:28,890 [Epoch: 1945 Step: 00066100] Batch Recognition Loss:   0.000646 => Gls Tokens per Sec:     2003 || Batch Translation Loss:   0.029350 => Txt Tokens per Sec:     5714 || Lr: 0.000050
2024-02-06 01:49:33,242 Epoch 1945: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.06 
2024-02-06 01:49:33,242 EPOCH 1946
2024-02-06 01:49:37,598 Epoch 1946: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-06 01:49:37,599 EPOCH 1947
2024-02-06 01:49:42,482 Epoch 1947: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.75 
2024-02-06 01:49:42,482 EPOCH 1948
2024-02-06 01:49:42,649 [Epoch: 1948 Step: 00066200] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:     3879 || Batch Translation Loss:   0.018097 => Txt Tokens per Sec:     9630 || Lr: 0.000050
2024-02-06 01:49:46,579 Epoch 1948: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-06 01:49:46,579 EPOCH 1949
2024-02-06 01:49:51,442 Epoch 1949: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.45 
2024-02-06 01:49:51,443 EPOCH 1950
2024-02-06 01:49:55,855 [Epoch: 1950 Step: 00066300] Batch Recognition Loss:   0.000301 => Gls Tokens per Sec:     2407 || Batch Translation Loss:   0.020203 => Txt Tokens per Sec:     6660 || Lr: 0.000050
2024-02-06 01:49:55,855 Epoch 1950: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.89 
2024-02-06 01:49:55,856 EPOCH 1951
2024-02-06 01:50:00,754 Epoch 1951: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-06 01:50:00,755 EPOCH 1952
2024-02-06 01:50:05,184 Epoch 1952: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-06 01:50:05,184 EPOCH 1953
2024-02-06 01:50:09,688 [Epoch: 1953 Step: 00066400] Batch Recognition Loss:   0.000219 => Gls Tokens per Sec:     2216 || Batch Translation Loss:   0.035344 => Txt Tokens per Sec:     6158 || Lr: 0.000050
2024-02-06 01:50:09,883 Epoch 1953: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.56 
2024-02-06 01:50:09,883 EPOCH 1954
2024-02-06 01:50:14,435 Epoch 1954: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-06 01:50:14,435 EPOCH 1955
2024-02-06 01:50:18,949 Epoch 1955: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-06 01:50:18,949 EPOCH 1956
2024-02-06 01:50:23,205 [Epoch: 1956 Step: 00066500] Batch Recognition Loss:   0.001175 => Gls Tokens per Sec:     2195 || Batch Translation Loss:   0.025109 => Txt Tokens per Sec:     6063 || Lr: 0.000050
2024-02-06 01:50:23,693 Epoch 1956: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-06 01:50:23,693 EPOCH 1957
2024-02-06 01:50:28,005 Epoch 1957: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-06 01:50:28,005 EPOCH 1958
2024-02-06 01:50:33,099 Epoch 1958: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-06 01:50:33,099 EPOCH 1959
2024-02-06 01:50:36,963 [Epoch: 1959 Step: 00066600] Batch Recognition Loss:   0.000619 => Gls Tokens per Sec:     2319 || Batch Translation Loss:   0.034685 => Txt Tokens per Sec:     6550 || Lr: 0.000050
2024-02-06 01:50:37,725 Epoch 1959: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-06 01:50:37,726 EPOCH 1960
2024-02-06 01:50:42,480 Epoch 1960: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-06 01:50:42,480 EPOCH 1961
2024-02-06 01:50:46,993 Epoch 1961: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-06 01:50:46,993 EPOCH 1962
2024-02-06 01:50:50,568 [Epoch: 1962 Step: 00066700] Batch Recognition Loss:   0.000182 => Gls Tokens per Sec:     2255 || Batch Translation Loss:   0.015399 => Txt Tokens per Sec:     6131 || Lr: 0.000050
2024-02-06 01:50:51,986 Epoch 1962: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 01:50:51,986 EPOCH 1963
2024-02-06 01:50:56,586 Epoch 1963: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-06 01:50:56,587 EPOCH 1964
2024-02-06 01:51:01,560 Epoch 1964: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-06 01:51:01,561 EPOCH 1965
2024-02-06 01:51:04,966 [Epoch: 1965 Step: 00066800] Batch Recognition Loss:   0.000125 => Gls Tokens per Sec:     2256 || Batch Translation Loss:   0.016414 => Txt Tokens per Sec:     6237 || Lr: 0.000050
2024-02-06 01:51:06,182 Epoch 1965: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-06 01:51:06,183 EPOCH 1966
2024-02-06 01:51:10,271 Epoch 1966: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-06 01:51:10,272 EPOCH 1967
2024-02-06 01:51:15,270 Epoch 1967: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-06 01:51:15,270 EPOCH 1968
2024-02-06 01:51:18,272 [Epoch: 1968 Step: 00066900] Batch Recognition Loss:   0.000232 => Gls Tokens per Sec:     2259 || Batch Translation Loss:   0.045994 => Txt Tokens per Sec:     6478 || Lr: 0.000050
2024-02-06 01:51:19,485 Epoch 1968: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-06 01:51:19,485 EPOCH 1969
2024-02-06 01:51:24,573 Epoch 1969: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.42 
2024-02-06 01:51:24,574 EPOCH 1970
2024-02-06 01:51:29,360 Epoch 1970: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.97 
2024-02-06 01:51:29,360 EPOCH 1971
2024-02-06 01:51:32,442 [Epoch: 1971 Step: 00067000] Batch Recognition Loss:   0.000328 => Gls Tokens per Sec:     2078 || Batch Translation Loss:   0.103140 => Txt Tokens per Sec:     5769 || Lr: 0.000050
2024-02-06 01:51:34,342 Epoch 1971: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.32 
2024-02-06 01:51:34,343 EPOCH 1972
2024-02-06 01:51:39,156 Epoch 1972: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.56 
2024-02-06 01:51:39,156 EPOCH 1973
2024-02-06 01:51:43,311 Epoch 1973: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.10 
2024-02-06 01:51:43,311 EPOCH 1974
2024-02-06 01:51:45,936 [Epoch: 1974 Step: 00067100] Batch Recognition Loss:   0.000305 => Gls Tokens per Sec:     2196 || Batch Translation Loss:   0.021560 => Txt Tokens per Sec:     6111 || Lr: 0.000050
2024-02-06 01:51:48,243 Epoch 1974: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.85 
2024-02-06 01:51:48,244 EPOCH 1975
2024-02-06 01:51:52,411 Epoch 1975: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.71 
2024-02-06 01:51:52,412 EPOCH 1976
2024-02-06 01:51:57,301 Epoch 1976: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-06 01:51:57,302 EPOCH 1977
2024-02-06 01:51:59,307 [Epoch: 1977 Step: 00067200] Batch Recognition Loss:   0.000132 => Gls Tokens per Sec:     2555 || Batch Translation Loss:   0.010970 => Txt Tokens per Sec:     6911 || Lr: 0.000050
2024-02-06 01:52:01,632 Epoch 1977: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-06 01:52:01,632 EPOCH 1978
2024-02-06 01:52:06,313 Epoch 1978: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-06 01:52:06,314 EPOCH 1979
2024-02-06 01:52:10,807 Epoch 1979: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-06 01:52:10,807 EPOCH 1980
2024-02-06 01:52:12,706 [Epoch: 1980 Step: 00067300] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:     2362 || Batch Translation Loss:   0.018790 => Txt Tokens per Sec:     6728 || Lr: 0.000050
2024-02-06 01:52:15,382 Epoch 1980: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-06 01:52:15,383 EPOCH 1981
2024-02-06 01:52:20,140 Epoch 1981: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-06 01:52:20,140 EPOCH 1982
2024-02-06 01:52:25,075 Epoch 1982: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-06 01:52:25,075 EPOCH 1983
2024-02-06 01:52:26,745 [Epoch: 1983 Step: 00067400] Batch Recognition Loss:   0.002600 => Gls Tokens per Sec:     2145 || Batch Translation Loss:   0.015766 => Txt Tokens per Sec:     6303 || Lr: 0.000050
2024-02-06 01:52:29,526 Epoch 1983: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 01:52:29,526 EPOCH 1984
2024-02-06 01:52:33,943 Epoch 1984: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-06 01:52:33,944 EPOCH 1985
2024-02-06 01:52:38,729 Epoch 1985: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 01:52:38,730 EPOCH 1986
2024-02-06 01:52:39,849 [Epoch: 1986 Step: 00067500] Batch Recognition Loss:   0.000261 => Gls Tokens per Sec:     2862 || Batch Translation Loss:   0.010996 => Txt Tokens per Sec:     7885 || Lr: 0.000050
2024-02-06 01:52:42,942 Epoch 1986: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 01:52:42,943 EPOCH 1987
2024-02-06 01:52:47,881 Epoch 1987: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 01:52:47,882 EPOCH 1988
2024-02-06 01:52:52,050 Epoch 1988: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-06 01:52:52,050 EPOCH 1989
2024-02-06 01:52:53,268 [Epoch: 1989 Step: 00067600] Batch Recognition Loss:   0.000148 => Gls Tokens per Sec:     1890 || Batch Translation Loss:   0.014677 => Txt Tokens per Sec:     5706 || Lr: 0.000050
2024-02-06 01:52:56,951 Epoch 1989: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.43 
2024-02-06 01:52:56,951 EPOCH 1990
2024-02-06 01:53:01,302 Epoch 1990: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 01:53:01,302 EPOCH 1991
2024-02-06 01:53:06,069 Epoch 1991: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.44 
2024-02-06 01:53:06,070 EPOCH 1992
2024-02-06 01:53:06,823 [Epoch: 1992 Step: 00067700] Batch Recognition Loss:   0.000100 => Gls Tokens per Sec:     2553 || Batch Translation Loss:   0.011527 => Txt Tokens per Sec:     6649 || Lr: 0.000050
2024-02-06 01:53:10,562 Epoch 1992: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-06 01:53:10,562 EPOCH 1993
2024-02-06 01:53:15,218 Epoch 1993: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-06 01:53:15,218 EPOCH 1994
2024-02-06 01:53:19,724 Epoch 1994: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.41 
2024-02-06 01:53:19,724 EPOCH 1995
2024-02-06 01:53:20,231 [Epoch: 1995 Step: 00067800] Batch Recognition Loss:   0.000865 => Gls Tokens per Sec:     2530 || Batch Translation Loss:   0.016575 => Txt Tokens per Sec:     7350 || Lr: 0.000050
2024-02-06 01:53:24,307 Epoch 1995: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.47 
2024-02-06 01:53:24,307 EPOCH 1996
2024-02-06 01:53:29,034 Epoch 1996: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.63 
2024-02-06 01:53:29,034 EPOCH 1997
2024-02-06 01:53:33,257 Epoch 1997: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-06 01:53:33,258 EPOCH 1998
2024-02-06 01:53:33,507 [Epoch: 1998 Step: 00067900] Batch Recognition Loss:   0.000683 => Gls Tokens per Sec:     2587 || Batch Translation Loss:   0.013327 => Txt Tokens per Sec:     6117 || Lr: 0.000050
2024-02-06 01:53:38,212 Epoch 1998: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-06 01:53:38,213 EPOCH 1999
2024-02-06 01:53:42,374 Epoch 1999: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 01:53:42,374 EPOCH 2000
2024-02-06 01:53:47,132 [Epoch: 2000 Step: 00068000] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:     2233 || Batch Translation Loss:   0.012663 => Txt Tokens per Sec:     6176 || Lr: 0.000050
2024-02-06 01:53:55,731 Validation result at epoch 2000, step    68000: duration: 8.5980s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.13869	Translation Loss: 99311.90625	PPL: 20318.49414
	Eval Metric: BLEU
	WER 2.26	(DEL: 0.00,	INS: 0.00,	SUB: 2.26)
	BLEU-4 0.55	(BLEU-1: 9.76,	BLEU-2: 2.70,	BLEU-3: 1.10,	BLEU-4: 0.55)
	CHRF 15.98	ROUGE 8.66
2024-02-06 01:53:55,732 Logging Recognition and Translation Outputs
2024-02-06 01:53:55,733 ========================================================================================================================
2024-02-06 01:53:55,733 Logging Sequence: 148_186.00
2024-02-06 01:53:55,733 	Gloss Reference :	A B+C+D+E
2024-02-06 01:53:55,733 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:53:55,733 	Gloss Alignment :	         
2024-02-06 01:53:55,733 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:53:55,735 	Text Reference  :	siraj also took four   wickets  in     1      over thus    becoming the record-holder for          most wickets in an     over in    odis  
2024-02-06 01:53:55,735 	Text Hypothesis :	***** **** the  sports minister anurag thakur also present at       the ************* participated in   several 5  months of   south africa
2024-02-06 01:53:55,736 	Text Alignment  :	D     D    S    S      S        S      S      S    S       S            D             S            S    S       S  S      S    S     S     
2024-02-06 01:53:55,736 ========================================================================================================================
2024-02-06 01:53:55,736 Logging Sequence: 61_181.00
2024-02-06 01:53:55,736 	Gloss Reference :	A B+C+D+E
2024-02-06 01:53:55,736 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:53:55,736 	Gloss Alignment :	         
2024-02-06 01:53:55,736 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:53:55,738 	Text Reference  :	one other fan said     it is  babar's personal chat    we   should focuc on   this       cricketing career  and not his personal life
2024-02-06 01:53:55,739 	Text Hypothesis :	*** ***** *** shocking he has now     being    brought down as     one   more viewership by         parents and *** *** ******** 2019
2024-02-06 01:53:55,739 	Text Alignment  :	D   D     D   S        S  S   S       S        S       S    S      S     S    S          S          S           D   D   D        S   
2024-02-06 01:53:55,739 ========================================================================================================================
2024-02-06 01:53:55,739 Logging Sequence: 123_24.00
2024-02-06 01:53:55,739 	Gloss Reference :	A B+C+D+E
2024-02-06 01:53:55,739 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:53:55,739 	Gloss Alignment :	         
2024-02-06 01:53:55,739 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:53:55,740 	Text Reference  :	did you know that other than cricket dhoni  has  another passion
2024-02-06 01:53:55,740 	Text Hypothesis :	*** *** **** **** he    was  then    calmed down by      head   
2024-02-06 01:53:55,740 	Text Alignment  :	D   D   D    D    S     S    S       S      S    S       S      
2024-02-06 01:53:55,740 ========================================================================================================================
2024-02-06 01:53:55,741 Logging Sequence: 84_76.00
2024-02-06 01:53:55,741 	Gloss Reference :	A B+C+D+E
2024-02-06 01:53:55,741 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:53:55,741 	Gloss Alignment :	         
2024-02-06 01:53:55,741 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:53:55,742 	Text Reference  :	** *** the  teams wanted to    support but     were refused 
2024-02-06 01:53:55,742 	Text Hypothesis :	do you know that  the    match and     respect was  shocking
2024-02-06 01:53:55,742 	Text Alignment  :	I  I   S    S     S      S     S       S       S    S       
2024-02-06 01:53:55,742 ========================================================================================================================
2024-02-06 01:53:55,742 Logging Sequence: 126_188.00
2024-02-06 01:53:55,743 	Gloss Reference :	A B+C+D+E
2024-02-06 01:53:55,743 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:53:55,743 	Gloss Alignment :	         
2024-02-06 01:53:55,743 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:53:55,744 	Text Reference  :	now he  has become a    gold  medalist at   the   2020 tokyo olympics
2024-02-06 01:53:55,744 	Text Hypothesis :	*** and the second time india were     very happy in   gold  medal   
2024-02-06 01:53:55,744 	Text Alignment  :	D   S   S   S      S    S     S        S    S     S    S     S       
2024-02-06 01:53:55,744 ========================================================================================================================
2024-02-06 01:53:55,748 Epoch 2000: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 01:53:55,748 EPOCH 2001
2024-02-06 01:54:00,634 Epoch 2001: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-06 01:54:00,634 EPOCH 2002
2024-02-06 01:54:05,073 Epoch 2002: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.82 
2024-02-06 01:54:05,073 EPOCH 2003
2024-02-06 01:54:09,611 [Epoch: 2003 Step: 00068100] Batch Recognition Loss:   0.000276 => Gls Tokens per Sec:     2200 || Batch Translation Loss:   0.075695 => Txt Tokens per Sec:     6030 || Lr: 0.000050
2024-02-06 01:54:09,945 Epoch 2003: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.90 
2024-02-06 01:54:09,945 EPOCH 2004
2024-02-06 01:54:14,168 Epoch 2004: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.21 
2024-02-06 01:54:14,168 EPOCH 2005
2024-02-06 01:54:19,132 Epoch 2005: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.41 
2024-02-06 01:54:19,132 EPOCH 2006
2024-02-06 01:54:23,024 [Epoch: 2006 Step: 00068200] Batch Recognition Loss:   0.002041 => Gls Tokens per Sec:     2400 || Batch Translation Loss:   0.031511 => Txt Tokens per Sec:     6753 || Lr: 0.000050
2024-02-06 01:54:23,368 Epoch 2006: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.01 
2024-02-06 01:54:23,369 EPOCH 2007
2024-02-06 01:54:28,334 Epoch 2007: Total Training Recognition Loss 0.14  Total Training Translation Loss 0.80 
2024-02-06 01:54:28,335 EPOCH 2008
2024-02-06 01:54:33,416 Epoch 2008: Total Training Recognition Loss 0.46  Total Training Translation Loss 0.95 
2024-02-06 01:54:33,417 EPOCH 2009
2024-02-06 01:54:37,139 [Epoch: 2009 Step: 00068300] Batch Recognition Loss:   0.000453 => Gls Tokens per Sec:     2407 || Batch Translation Loss:   0.033822 => Txt Tokens per Sec:     6634 || Lr: 0.000050
2024-02-06 01:54:37,948 Epoch 2009: Total Training Recognition Loss 0.13  Total Training Translation Loss 1.12 
2024-02-06 01:54:37,948 EPOCH 2010
2024-02-06 01:54:42,190 Epoch 2010: Total Training Recognition Loss 0.23  Total Training Translation Loss 0.87 
2024-02-06 01:54:42,191 EPOCH 2011
2024-02-06 01:54:47,108 Epoch 2011: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.96 
2024-02-06 01:54:47,108 EPOCH 2012
2024-02-06 01:54:50,228 [Epoch: 2012 Step: 00068400] Batch Recognition Loss:   0.000433 => Gls Tokens per Sec:     2585 || Batch Translation Loss:   0.019397 => Txt Tokens per Sec:     7112 || Lr: 0.000050
2024-02-06 01:54:51,333 Epoch 2012: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.73 
2024-02-06 01:54:51,333 EPOCH 2013
2024-02-06 01:54:56,251 Epoch 2013: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.71 
2024-02-06 01:54:56,251 EPOCH 2014
2024-02-06 01:55:00,573 Epoch 2014: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-06 01:55:00,574 EPOCH 2015
2024-02-06 01:55:03,945 [Epoch: 2015 Step: 00068500] Batch Recognition Loss:   0.001202 => Gls Tokens per Sec:     2202 || Batch Translation Loss:   0.004523 => Txt Tokens per Sec:     6092 || Lr: 0.000050
2024-02-06 01:55:05,233 Epoch 2015: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-06 01:55:05,233 EPOCH 2016
2024-02-06 01:55:09,848 Epoch 2016: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-06 01:55:09,848 EPOCH 2017
2024-02-06 01:55:14,322 Epoch 2017: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-06 01:55:14,322 EPOCH 2018
2024-02-06 01:55:17,445 [Epoch: 2018 Step: 00068600] Batch Recognition Loss:   0.000825 => Gls Tokens per Sec:     2173 || Batch Translation Loss:   0.026829 => Txt Tokens per Sec:     6076 || Lr: 0.000050
2024-02-06 01:55:19,027 Epoch 2018: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 01:55:19,027 EPOCH 2019
2024-02-06 01:55:23,289 Epoch 2019: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-06 01:55:23,290 EPOCH 2020
2024-02-06 01:55:28,282 Epoch 2020: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-06 01:55:28,282 EPOCH 2021
2024-02-06 01:55:30,657 [Epoch: 2021 Step: 00068700] Batch Recognition Loss:   0.000208 => Gls Tokens per Sec:     2587 || Batch Translation Loss:   0.016114 => Txt Tokens per Sec:     7113 || Lr: 0.000050
2024-02-06 01:55:32,466 Epoch 2021: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-06 01:55:32,467 EPOCH 2022
2024-02-06 01:55:37,462 Epoch 2022: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-06 01:55:37,463 EPOCH 2023
2024-02-06 01:55:41,724 Epoch 2023: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-06 01:55:41,724 EPOCH 2024
2024-02-06 01:55:44,091 [Epoch: 2024 Step: 00068800] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:     2436 || Batch Translation Loss:   0.018727 => Txt Tokens per Sec:     6520 || Lr: 0.000050
2024-02-06 01:55:46,554 Epoch 2024: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.28 
2024-02-06 01:55:46,554 EPOCH 2025
2024-02-06 01:55:50,940 Epoch 2025: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.20 
2024-02-06 01:55:50,940 EPOCH 2026
2024-02-06 01:55:55,548 Epoch 2026: Total Training Recognition Loss 0.05  Total Training Translation Loss 6.68 
2024-02-06 01:55:55,549 EPOCH 2027
2024-02-06 01:55:57,805 [Epoch: 2027 Step: 00068900] Batch Recognition Loss:   0.000425 => Gls Tokens per Sec:     2156 || Batch Translation Loss:   0.017829 => Txt Tokens per Sec:     6075 || Lr: 0.000050
2024-02-06 01:56:00,138 Epoch 2027: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.72 
2024-02-06 01:56:00,138 EPOCH 2028
2024-02-06 01:56:04,686 Epoch 2028: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.83 
2024-02-06 01:56:04,687 EPOCH 2029
2024-02-06 01:56:09,423 Epoch 2029: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.65 
2024-02-06 01:56:09,423 EPOCH 2030
2024-02-06 01:56:11,280 [Epoch: 2030 Step: 00069000] Batch Recognition Loss:   0.000444 => Gls Tokens per Sec:     2275 || Batch Translation Loss:   0.011898 => Txt Tokens per Sec:     6420 || Lr: 0.000050
2024-02-06 01:56:13,813 Epoch 2030: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.62 
2024-02-06 01:56:13,814 EPOCH 2031
2024-02-06 01:56:18,672 Epoch 2031: Total Training Recognition Loss 0.28  Total Training Translation Loss 0.60 
2024-02-06 01:56:18,672 EPOCH 2032
2024-02-06 01:56:22,821 Epoch 2032: Total Training Recognition Loss 0.68  Total Training Translation Loss 0.87 
2024-02-06 01:56:22,821 EPOCH 2033
2024-02-06 01:56:24,480 [Epoch: 2033 Step: 00069100] Batch Recognition Loss:   0.000573 => Gls Tokens per Sec:     2160 || Batch Translation Loss:   0.013649 => Txt Tokens per Sec:     5844 || Lr: 0.000050
2024-02-06 01:56:27,731 Epoch 2033: Total Training Recognition Loss 0.16  Total Training Translation Loss 0.61 
2024-02-06 01:56:27,732 EPOCH 2034
2024-02-06 01:56:32,146 Epoch 2034: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-06 01:56:32,146 EPOCH 2035
2024-02-06 01:56:37,184 Epoch 2035: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-06 01:56:37,185 EPOCH 2036
2024-02-06 01:56:38,403 [Epoch: 2036 Step: 00069200] Batch Recognition Loss:   0.000241 => Gls Tokens per Sec:     2414 || Batch Translation Loss:   0.011504 => Txt Tokens per Sec:     6629 || Lr: 0.000050
2024-02-06 01:56:41,383 Epoch 2036: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-06 01:56:41,383 EPOCH 2037
2024-02-06 01:56:46,173 Epoch 2037: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-06 01:56:46,173 EPOCH 2038
2024-02-06 01:56:50,659 Epoch 2038: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-06 01:56:50,659 EPOCH 2039
2024-02-06 01:56:51,822 [Epoch: 2039 Step: 00069300] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:     2202 || Batch Translation Loss:   0.014829 => Txt Tokens per Sec:     6391 || Lr: 0.000050
2024-02-06 01:56:55,317 Epoch 2039: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-06 01:56:55,318 EPOCH 2040
2024-02-06 01:56:59,971 Epoch 2040: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-06 01:56:59,972 EPOCH 2041
2024-02-06 01:57:05,019 Epoch 2041: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 01:57:05,020 EPOCH 2042
2024-02-06 01:57:05,598 [Epoch: 2042 Step: 00069400] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:     3333 || Batch Translation Loss:   0.015055 => Txt Tokens per Sec:     8175 || Lr: 0.000050
2024-02-06 01:57:09,509 Epoch 2042: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-06 01:57:09,510 EPOCH 2043
2024-02-06 01:57:13,858 Epoch 2043: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-06 01:57:13,859 EPOCH 2044
2024-02-06 01:57:18,733 Epoch 2044: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-06 01:57:18,734 EPOCH 2045
2024-02-06 01:57:19,214 [Epoch: 2045 Step: 00069500] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     2670 || Batch Translation Loss:   0.012640 => Txt Tokens per Sec:     7909 || Lr: 0.000050
2024-02-06 01:57:22,852 Epoch 2045: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-06 01:57:22,853 EPOCH 2046
2024-02-06 01:57:27,866 Epoch 2046: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-06 01:57:27,866 EPOCH 2047
2024-02-06 01:57:32,122 Epoch 2047: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-06 01:57:32,123 EPOCH 2048
2024-02-06 01:57:32,298 [Epoch: 2048 Step: 00069600] Batch Recognition Loss:   0.000129 => Gls Tokens per Sec:     3657 || Batch Translation Loss:   0.010875 => Txt Tokens per Sec:     9600 || Lr: 0.000050
2024-02-06 01:57:36,987 Epoch 2048: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-06 01:57:36,988 EPOCH 2049
2024-02-06 01:57:41,630 Epoch 2049: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.53 
2024-02-06 01:57:41,631 EPOCH 2050
2024-02-06 01:57:46,579 [Epoch: 2050 Step: 00069700] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:     2147 || Batch Translation Loss:   0.015269 => Txt Tokens per Sec:     5939 || Lr: 0.000050
2024-02-06 01:57:46,580 Epoch 2050: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-06 01:57:46,580 EPOCH 2051
2024-02-06 01:57:51,086 Epoch 2051: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.68 
2024-02-06 01:57:51,086 EPOCH 2052
2024-02-06 01:57:56,031 Epoch 2052: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-06 01:57:56,031 EPOCH 2053
2024-02-06 01:58:00,296 [Epoch: 2053 Step: 00069800] Batch Recognition Loss:   0.000553 => Gls Tokens per Sec:     2341 || Batch Translation Loss:   0.027292 => Txt Tokens per Sec:     6498 || Lr: 0.000050
2024-02-06 01:58:00,564 Epoch 2053: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-06 01:58:00,564 EPOCH 2054
2024-02-06 01:58:05,543 Epoch 2054: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.00 
2024-02-06 01:58:05,543 EPOCH 2055
2024-02-06 01:58:09,992 Epoch 2055: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.22 
2024-02-06 01:58:09,992 EPOCH 2056
2024-02-06 01:58:13,864 [Epoch: 2056 Step: 00069900] Batch Recognition Loss:   0.000437 => Gls Tokens per Sec:     2414 || Batch Translation Loss:   0.117740 => Txt Tokens per Sec:     6694 || Lr: 0.000050
2024-02-06 01:58:14,407 Epoch 2056: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.04 
2024-02-06 01:58:14,408 EPOCH 2057
2024-02-06 01:58:19,200 Epoch 2057: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.68 
2024-02-06 01:58:19,200 EPOCH 2058
2024-02-06 01:58:23,390 Epoch 2058: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.13 
2024-02-06 01:58:23,390 EPOCH 2059
2024-02-06 01:58:27,607 [Epoch: 2059 Step: 00070000] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:     2064 || Batch Translation Loss:   0.025627 => Txt Tokens per Sec:     5659 || Lr: 0.000050
2024-02-06 01:58:36,526 Validation result at epoch 2059, step    70000: duration: 8.9178s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 1.09741	Translation Loss: 97299.81250	PPL: 16619.28320
	Eval Metric: BLEU
	WER 2.40	(DEL: 0.00,	INS: 0.00,	SUB: 2.40)
	BLEU-4 0.58	(BLEU-1: 10.35,	BLEU-2: 3.11,	BLEU-3: 1.17,	BLEU-4: 0.58)
	CHRF 16.79	ROUGE 8.74
2024-02-06 01:58:36,527 Logging Recognition and Translation Outputs
2024-02-06 01:58:36,528 ========================================================================================================================
2024-02-06 01:58:36,528 Logging Sequence: 129_90.00
2024-02-06 01:58:36,528 	Gloss Reference :	A B+C+D+E
2024-02-06 01:58:36,528 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:58:36,528 	Gloss Alignment :	         
2024-02-06 01:58:36,529 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:58:36,530 	Text Reference  :	******** ******* **** **** **** **** however because of    the     emergency games will now be held    without any  spectators
2024-02-06 01:58:36,530 	Text Hypothesis :	everyone thought that they have been glued   to      their screens as        they  will *** be similar to      euro 2020      
2024-02-06 01:58:36,530 	Text Alignment  :	I        I       I    I    I    I    S       S       S     S       S         S          D      S       S       S    S         
2024-02-06 01:58:36,531 ========================================================================================================================
2024-02-06 01:58:36,531 Logging Sequence: 179_378.00
2024-02-06 01:58:36,531 	Gloss Reference :	A B+C+D+E
2024-02-06 01:58:36,531 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:58:36,531 	Gloss Alignment :	         
2024-02-06 01:58:36,531 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:58:36,533 	Text Reference  :	these kids think that they are      going to  the ***** olympics so     they've become some kind of   stars 
2024-02-06 01:58:36,533 	Text Hypothesis :	***** **** ***** **** the  response is    not the covid pandemic people are     now    get  a    huge amount
2024-02-06 01:58:36,533 	Text Alignment  :	D     D    D     D    S    S        S     S       I     S        S      S       S      S    S    S    S     
2024-02-06 01:58:36,533 ========================================================================================================================
2024-02-06 01:58:36,533 Logging Sequence: 162_20.00
2024-02-06 01:58:36,534 	Gloss Reference :	A B+C+D+E
2024-02-06 01:58:36,534 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:58:36,534 	Gloss Alignment :	         
2024-02-06 01:58:36,534 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:58:36,535 	Text Reference  :	not only this but they     blamed mohammed shami's religion as  the reason for  india's   loss 
2024-02-06 01:58:36,535 	Text Hypothesis :	*** **** he   is  survived by     his      wife    their    son and a      very captaincy crore
2024-02-06 01:58:36,536 	Text Alignment  :	D   D    S    S   S        S      S        S       S        S   S   S      S    S         S    
2024-02-06 01:58:36,536 ========================================================================================================================
2024-02-06 01:58:36,536 Logging Sequence: 106_169.00
2024-02-06 01:58:36,536 	Gloss Reference :	A B+C+D+E
2024-02-06 01:58:36,536 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:58:36,537 	Gloss Alignment :	         
2024-02-06 01:58:36,537 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:58:36,538 	Text Reference  :	prime minister narendra modi also expressed his happiness while congratulating the   team on   twitter
2024-02-06 01:58:36,538 	Text Hypothesis :	***** ******** ******** **** **** we        won the       toss  and            chose to   bowl indian 
2024-02-06 01:58:36,538 	Text Alignment  :	D     D        D        D    D    S         S   S         S     S              S     S    S    S      
2024-02-06 01:58:36,538 ========================================================================================================================
2024-02-06 01:58:36,538 Logging Sequence: 65_77.00
2024-02-06 01:58:36,539 	Gloss Reference :	A B+C+D+E
2024-02-06 01:58:36,539 	Gloss Hypothesis:	A B+C+D+E
2024-02-06 01:58:36,539 	Gloss Alignment :	         
2024-02-06 01:58:36,539 	--------------------------------------------------------------------------------------------------------------------
2024-02-06 01:58:36,539 	Text Reference  :	** *** ***** indian team travelling included 16    players   
2024-02-06 01:58:36,540 	Text Hypothesis :	in the match he     was  held       in       quick succession
2024-02-06 01:58:36,540 	Text Alignment  :	I  I   I     S      S    S          S        S     S         
2024-02-06 01:58:36,540 ========================================================================================================================
2024-02-06 01:58:36,544 Training ended since there were no improvements inthe last learning rate step: 0.000050
2024-02-06 01:58:36,544 Best validation result at step    18000:   0.92 eval_metric.
2024-02-06 01:59:04,205 ------------------------------------------------------------
2024-02-06 01:59:04,206 [DEV] partition [RECOGNITION] experiment [BW]: 1
2024-02-06 01:59:12,928 finished in 8.7217s 
2024-02-06 01:59:12,929 ************************************************************
2024-02-06 01:59:12,929 [DEV] partition [RECOGNITION] results:
	New Best CTC Decode Beam Size: 1
	WER 6.57	(DEL: 0.00,	INS: 0.00,	SUB: 6.57)
2024-02-06 01:59:12,929 ************************************************************
2024-02-06 01:59:12,929 ------------------------------------------------------------
2024-02-06 01:59:12,929 [DEV] partition [RECOGNITION] experiment [BW]: 2
2024-02-06 01:59:21,481 finished in 8.5521s 
2024-02-06 01:59:21,481 ************************************************************
2024-02-06 01:59:21,482 [DEV] partition [RECOGNITION] results:
	New Best CTC Decode Beam Size: 2
	WER 6.43	(DEL: 0.00,	INS: 0.00,	SUB: 6.43)
2024-02-06 01:59:21,482 ************************************************************
2024-02-06 01:59:21,482 ------------------------------------------------------------
2024-02-06 01:59:21,482 [DEV] partition [RECOGNITION] experiment [BW]: 3
2024-02-06 01:59:29,870 finished in 8.3881s 
2024-02-06 01:59:29,870 ------------------------------------------------------------
2024-02-06 01:59:29,870 [DEV] partition [RECOGNITION] experiment [BW]: 4
2024-02-06 01:59:38,373 finished in 8.5034s 
2024-02-06 01:59:38,373 ------------------------------------------------------------
2024-02-06 01:59:38,374 [DEV] partition [RECOGNITION] experiment [BW]: 5
2024-02-06 01:59:46,886 finished in 8.5115s 
2024-02-06 01:59:46,887 ------------------------------------------------------------
2024-02-06 01:59:46,887 [DEV] partition [RECOGNITION] experiment [BW]: 6
2024-02-06 01:59:55,681 finished in 8.7940s 
2024-02-06 01:59:55,681 ------------------------------------------------------------
2024-02-06 01:59:55,681 [DEV] partition [RECOGNITION] experiment [BW]: 7
2024-02-06 02:00:04,211 finished in 8.5299s 
2024-02-06 02:00:04,212 ------------------------------------------------------------
2024-02-06 02:00:04,212 [DEV] partition [RECOGNITION] experiment [BW]: 8
2024-02-06 02:00:13,058 finished in 8.8457s 
2024-02-06 02:00:13,059 ------------------------------------------------------------
2024-02-06 02:00:13,059 [DEV] partition [RECOGNITION] experiment [BW]: 9
2024-02-06 02:00:21,765 finished in 8.7061s 
2024-02-06 02:00:21,765 ------------------------------------------------------------
2024-02-06 02:00:21,766 [DEV] partition [RECOGNITION] experiment [BW]: 10
2024-02-06 02:00:30,489 finished in 8.7234s 
2024-02-06 02:00:30,490 ============================================================
2024-02-06 02:00:38,988 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 1 and Alpha: -1
	BLEU-4 0.92	(BLEU-1: 11.47,	BLEU-2: 3.78,	BLEU-3: 1.66,	BLEU-4: 0.92)
	CHRF 17.31	ROUGE 9.69
2024-02-06 02:00:38,989 ------------------------------------------------------------
2024-02-06 02:05:10,121 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 4 and Alpha: 3
	BLEU-4 0.95	(BLEU-1: 10.81,	BLEU-2: 3.75,	BLEU-3: 1.65,	BLEU-4: 0.95)
	CHRF 16.65	ROUGE 9.28
2024-02-06 02:05:10,122 ------------------------------------------------------------
2024-02-06 02:18:26,108 ************************************************************
2024-02-06 02:18:26,108 [DEV] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 2
	Best Translation Beam Size: 4 and Alpha: 3
	WER 6.43	(DEL: 0.00,	INS: 0.00,	SUB: 6.43)
	BLEU-4 0.95	(BLEU-1: 10.81,	BLEU-2: 3.75,	BLEU-3: 1.65,	BLEU-4: 0.95)
	CHRF 16.65	ROUGE 9.28
2024-02-06 02:18:26,109 ************************************************************
2024-02-06 02:18:39,125 [TEST] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 2
	Best Translation Beam Size: 4 and Alpha: 3
	WER 5.44	(DEL: 0.00,	INS: 0.00,	SUB: 5.44)
	BLEU-4 0.67	(BLEU-1: 10.09,	BLEU-2: 3.24,	BLEU-3: 1.30,	BLEU-4: 0.67)
	CHRF 16.65	ROUGE 8.74
2024-02-06 02:18:39,126 ************************************************************
