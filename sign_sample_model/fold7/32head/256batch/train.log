2024-02-09 21:54:59,820 Hello! This is Joey-NMT.
2024-02-09 21:54:59,826 Total params: 25642504
2024-02-09 21:54:59,827 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'gloss_output_layer.bias', 'gloss_output_layer.weight', 'sgn_embed.ln.bias', 'sgn_embed.ln.weight', 'sgn_embed.norm.norm.bias', 'sgn_embed.norm.norm.weight', 'txt_embed.norm.norm.bias', 'txt_embed.norm.norm.weight']
2024-02-09 21:55:01,090 cfg.name                           : sign_experiment
2024-02-09 21:55:01,091 cfg.data.data_path                 : ./data/Sports_dataset/7/
2024-02-09 21:55:01,091 cfg.data.version                   : phoenix_2014_trans
2024-02-09 21:55:01,091 cfg.data.sgn                       : sign
2024-02-09 21:55:01,091 cfg.data.txt                       : text
2024-02-09 21:55:01,091 cfg.data.gls                       : gloss
2024-02-09 21:55:01,091 cfg.data.train                     : excel_data.train
2024-02-09 21:55:01,091 cfg.data.dev                       : excel_data.dev
2024-02-09 21:55:01,091 cfg.data.test                      : excel_data.test
2024-02-09 21:55:01,092 cfg.data.feature_size              : 2560
2024-02-09 21:55:01,092 cfg.data.level                     : word
2024-02-09 21:55:01,092 cfg.data.txt_lowercase             : True
2024-02-09 21:55:01,092 cfg.data.max_sent_length           : 500
2024-02-09 21:55:01,092 cfg.data.random_train_subset       : -1
2024-02-09 21:55:01,092 cfg.data.random_dev_subset         : -1
2024-02-09 21:55:01,092 cfg.testing.recognition_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-09 21:55:01,092 cfg.testing.translation_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2024-02-09 21:55:01,093 cfg.testing.translation_beam_alphas : [-1, 0, 1, 2, 3, 4, 5]
2024-02-09 21:55:01,093 cfg.training.reset_best_ckpt       : False
2024-02-09 21:55:01,093 cfg.training.reset_scheduler       : False
2024-02-09 21:55:01,093 cfg.training.reset_optimizer       : False
2024-02-09 21:55:01,093 cfg.training.random_seed           : 42
2024-02-09 21:55:01,093 cfg.training.model_dir             : ./sign_sample_model/fold7/32head/256batch
2024-02-09 21:55:01,093 cfg.training.recognition_loss_weight : 1.0
2024-02-09 21:55:01,093 cfg.training.translation_loss_weight : 1.0
2024-02-09 21:55:01,093 cfg.training.eval_metric           : bleu
2024-02-09 21:55:01,094 cfg.training.optimizer             : adam
2024-02-09 21:55:01,094 cfg.training.learning_rate         : 0.0001
2024-02-09 21:55:01,094 cfg.training.batch_size            : 256
2024-02-09 21:55:01,094 cfg.training.num_valid_log         : 5
2024-02-09 21:55:01,094 cfg.training.epochs                : 50000
2024-02-09 21:55:01,094 cfg.training.early_stopping_metric : eval_metric
2024-02-09 21:55:01,094 cfg.training.batch_type            : sentence
2024-02-09 21:55:01,094 cfg.training.translation_normalization : batch
2024-02-09 21:55:01,095 cfg.training.eval_recognition_beam_size : 1
2024-02-09 21:55:01,095 cfg.training.eval_translation_beam_size : 1
2024-02-09 21:55:01,095 cfg.training.eval_translation_beam_alpha : -1
2024-02-09 21:55:01,095 cfg.training.overwrite             : True
2024-02-09 21:55:01,095 cfg.training.shuffle               : True
2024-02-09 21:55:01,095 cfg.training.use_cuda              : True
2024-02-09 21:55:01,095 cfg.training.translation_max_output_length : 40
2024-02-09 21:55:01,095 cfg.training.keep_last_ckpts       : 1
2024-02-09 21:55:01,095 cfg.training.batch_multiplier      : 1
2024-02-09 21:55:01,096 cfg.training.logging_freq          : 100
2024-02-09 21:55:01,096 cfg.training.validation_freq       : 2000
2024-02-09 21:55:01,096 cfg.training.betas                 : [0.9, 0.998]
2024-02-09 21:55:01,096 cfg.training.scheduling            : plateau
2024-02-09 21:55:01,096 cfg.training.learning_rate_min     : 1e-08
2024-02-09 21:55:01,096 cfg.training.weight_decay          : 0.0001
2024-02-09 21:55:01,096 cfg.training.patience              : 12
2024-02-09 21:55:01,096 cfg.training.decrease_factor       : 0.5
2024-02-09 21:55:01,097 cfg.training.label_smoothing       : 0.0
2024-02-09 21:55:01,097 cfg.model.initializer              : xavier
2024-02-09 21:55:01,097 cfg.model.bias_initializer         : zeros
2024-02-09 21:55:01,097 cfg.model.init_gain                : 1.0
2024-02-09 21:55:01,097 cfg.model.embed_initializer        : xavier
2024-02-09 21:55:01,097 cfg.model.embed_init_gain          : 1.0
2024-02-09 21:55:01,097 cfg.model.tied_softmax             : True
2024-02-09 21:55:01,097 cfg.model.encoder.type             : transformer
2024-02-09 21:55:01,098 cfg.model.encoder.num_layers       : 3
2024-02-09 21:55:01,098 cfg.model.encoder.num_heads        : 32
2024-02-09 21:55:01,098 cfg.model.encoder.embeddings.embedding_dim : 512
2024-02-09 21:55:01,098 cfg.model.encoder.embeddings.scale : False
2024-02-09 21:55:01,098 cfg.model.encoder.embeddings.dropout : 0.1
2024-02-09 21:55:01,098 cfg.model.encoder.embeddings.norm_type : batch
2024-02-09 21:55:01,098 cfg.model.encoder.embeddings.activation_type : softsign
2024-02-09 21:55:01,098 cfg.model.encoder.hidden_size      : 512
2024-02-09 21:55:01,098 cfg.model.encoder.ff_size          : 2048
2024-02-09 21:55:01,099 cfg.model.encoder.dropout          : 0.1
2024-02-09 21:55:01,099 cfg.model.decoder.type             : transformer
2024-02-09 21:55:01,099 cfg.model.decoder.num_layers       : 3
2024-02-09 21:55:01,099 cfg.model.decoder.num_heads        : 32
2024-02-09 21:55:01,099 cfg.model.decoder.embeddings.embedding_dim : 512
2024-02-09 21:55:01,099 cfg.model.decoder.embeddings.scale : False
2024-02-09 21:55:01,099 cfg.model.decoder.embeddings.dropout : 0.1
2024-02-09 21:55:01,099 cfg.model.decoder.embeddings.norm_type : batch
2024-02-09 21:55:01,100 cfg.model.decoder.embeddings.activation_type : softsign
2024-02-09 21:55:01,100 cfg.model.decoder.hidden_size      : 512
2024-02-09 21:55:01,100 cfg.model.decoder.ff_size          : 2048
2024-02-09 21:55:01,100 cfg.model.decoder.dropout          : 0.1
2024-02-09 21:55:01,100 Data set sizes: 
	train 2124,
	valid 708,
	test 708
2024-02-09 21:55:01,100 First training example:
	[GLS] A B C D E
	[TXT] how did she become a champion
2024-02-09 21:55:01,100 First 10 words (gls): (0) <si> (1) <unk> (2) <pad> (3) A (4) B (5) C (6) D (7) E
2024-02-09 21:55:01,100 First 10 words (txt): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) and (6) to (7) in (8) a (9) of
2024-02-09 21:55:01,101 Number of unique glosses (types): 8
2024-02-09 21:55:01,101 Number of unique words (types): 4402
2024-02-09 21:55:01,101 SignModel(
	encoder=TransformerEncoder(num_layers=3, num_heads=32),
	decoder=TransformerDecoder(num_layers=3, num_heads=32),
	sgn_embed=SpatialEmbeddings(embedding_dim=512, input_size=2560),
	txt_embed=Embeddings(embedding_dim=512, vocab_size=4402))
2024-02-09 21:55:01,104 EPOCH 1
2024-02-09 21:55:23,793 Epoch   1: Total Training Recognition Loss 84.41  Total Training Translation Loss 994.47 
2024-02-09 21:55:23,793 EPOCH 2
2024-02-09 21:55:42,621 Epoch   2: Total Training Recognition Loss 41.38  Total Training Translation Loss 919.63 
2024-02-09 21:55:42,622 EPOCH 3
2024-02-09 21:55:59,150 Epoch   3: Total Training Recognition Loss 35.91  Total Training Translation Loss 882.69 
2024-02-09 21:55:59,150 EPOCH 4
2024-02-09 21:56:18,722 Epoch   4: Total Training Recognition Loss 24.37  Total Training Translation Loss 853.91 
2024-02-09 21:56:18,722 EPOCH 5
2024-02-09 21:56:36,927 Epoch   5: Total Training Recognition Loss 18.90  Total Training Translation Loss 833.48 
2024-02-09 21:56:36,928 EPOCH 6
2024-02-09 21:56:53,165 Epoch   6: Total Training Recognition Loss 15.62  Total Training Translation Loss 820.54 
2024-02-09 21:56:53,166 EPOCH 7
2024-02-09 21:57:09,888 Epoch   7: Total Training Recognition Loss 13.45  Total Training Translation Loss 812.37 
2024-02-09 21:57:09,889 EPOCH 8
2024-02-09 21:57:26,316 Epoch   8: Total Training Recognition Loss 12.01  Total Training Translation Loss 807.51 
2024-02-09 21:57:26,316 EPOCH 9
2024-02-09 21:57:42,408 Epoch   9: Total Training Recognition Loss 10.75  Total Training Translation Loss 804.43 
2024-02-09 21:57:42,409 EPOCH 10
2024-02-09 21:57:58,531 Epoch  10: Total Training Recognition Loss 9.78  Total Training Translation Loss 801.18 
2024-02-09 21:57:58,531 EPOCH 11
2024-02-09 21:58:14,827 Epoch  11: Total Training Recognition Loss 9.01  Total Training Translation Loss 796.29 
2024-02-09 21:58:14,828 EPOCH 12
2024-02-09 21:58:21,000 [Epoch: 012 Step: 00000100] Batch Recognition Loss:   1.139669 => Gls Tokens per Sec:      207 || Batch Translation Loss: 110.594482 => Txt Tokens per Sec:      714 || Lr: 0.000100
2024-02-09 21:58:33,193 Epoch  12: Total Training Recognition Loss 8.24  Total Training Translation Loss 791.81 
2024-02-09 21:58:33,194 EPOCH 13
2024-02-09 21:58:52,777 Epoch  13: Total Training Recognition Loss 7.86  Total Training Translation Loss 784.89 
2024-02-09 21:58:52,777 EPOCH 14
2024-02-09 21:59:09,357 Epoch  14: Total Training Recognition Loss 7.22  Total Training Translation Loss 779.03 
2024-02-09 21:59:09,357 EPOCH 15
2024-02-09 21:59:25,921 Epoch  15: Total Training Recognition Loss 6.84  Total Training Translation Loss 770.49 
2024-02-09 21:59:25,921 EPOCH 16
2024-02-09 21:59:42,478 Epoch  16: Total Training Recognition Loss 6.39  Total Training Translation Loss 761.45 
2024-02-09 21:59:42,479 EPOCH 17
2024-02-09 21:59:59,066 Epoch  17: Total Training Recognition Loss 6.76  Total Training Translation Loss 753.10 
2024-02-09 21:59:59,066 EPOCH 18
2024-02-09 22:00:15,779 Epoch  18: Total Training Recognition Loss 6.53  Total Training Translation Loss 741.94 
2024-02-09 22:00:15,780 EPOCH 19
2024-02-09 22:00:33,352 Epoch  19: Total Training Recognition Loss 5.96  Total Training Translation Loss 733.82 
2024-02-09 22:00:33,353 EPOCH 20
2024-02-09 22:00:56,325 Epoch  20: Total Training Recognition Loss 5.97  Total Training Translation Loss 719.31 
2024-02-09 22:00:56,326 EPOCH 21
2024-02-09 22:01:15,903 Epoch  21: Total Training Recognition Loss 5.36  Total Training Translation Loss 707.77 
2024-02-09 22:01:15,903 EPOCH 22
2024-02-09 22:01:35,240 Epoch  22: Total Training Recognition Loss 5.47  Total Training Translation Loss 697.00 
2024-02-09 22:01:35,240 EPOCH 23
2024-02-09 22:01:42,624 [Epoch: 023 Step: 00000200] Batch Recognition Loss:   0.746001 => Gls Tokens per Sec:      347 || Batch Translation Loss:  80.147163 => Txt Tokens per Sec:     1099 || Lr: 0.000100
2024-02-09 22:01:53,558 Epoch  23: Total Training Recognition Loss 6.29  Total Training Translation Loss 686.00 
2024-02-09 22:01:53,559 EPOCH 24
2024-02-09 22:02:11,862 Epoch  24: Total Training Recognition Loss 10.51  Total Training Translation Loss 677.77 
2024-02-09 22:02:11,863 EPOCH 25
2024-02-09 22:02:30,390 Epoch  25: Total Training Recognition Loss 6.87  Total Training Translation Loss 666.14 
2024-02-09 22:02:30,391 EPOCH 26
2024-02-09 22:02:47,818 Epoch  26: Total Training Recognition Loss 5.91  Total Training Translation Loss 659.56 
2024-02-09 22:02:47,819 EPOCH 27
2024-02-09 22:03:05,131 Epoch  27: Total Training Recognition Loss 6.49  Total Training Translation Loss 650.37 
2024-02-09 22:03:05,131 EPOCH 28
2024-02-09 22:03:23,389 Epoch  28: Total Training Recognition Loss 4.60  Total Training Translation Loss 640.70 
2024-02-09 22:03:23,390 EPOCH 29
2024-02-09 22:03:39,758 Epoch  29: Total Training Recognition Loss 4.07  Total Training Translation Loss 634.41 
2024-02-09 22:03:39,758 EPOCH 30
2024-02-09 22:03:55,898 Epoch  30: Total Training Recognition Loss 4.08  Total Training Translation Loss 626.10 
2024-02-09 22:03:55,898 EPOCH 31
2024-02-09 22:04:12,221 Epoch  31: Total Training Recognition Loss 3.61  Total Training Translation Loss 614.22 
2024-02-09 22:04:12,222 EPOCH 32
2024-02-09 22:04:28,900 Epoch  32: Total Training Recognition Loss 3.52  Total Training Translation Loss 602.68 
2024-02-09 22:04:28,900 EPOCH 33
2024-02-09 22:04:44,889 Epoch  33: Total Training Recognition Loss 3.31  Total Training Translation Loss 593.55 
2024-02-09 22:04:44,890 EPOCH 34
2024-02-09 22:04:54,440 [Epoch: 034 Step: 00000300] Batch Recognition Loss:   0.623495 => Gls Tokens per Sec:      402 || Batch Translation Loss:  84.808983 => Txt Tokens per Sec:     1259 || Lr: 0.000100
2024-02-09 22:05:01,118 Epoch  34: Total Training Recognition Loss 3.40  Total Training Translation Loss 590.37 
2024-02-09 22:05:01,118 EPOCH 35
2024-02-09 22:05:17,346 Epoch  35: Total Training Recognition Loss 3.25  Total Training Translation Loss 584.03 
2024-02-09 22:05:17,347 EPOCH 36
2024-02-09 22:05:33,840 Epoch  36: Total Training Recognition Loss 3.04  Total Training Translation Loss 573.17 
2024-02-09 22:05:33,840 EPOCH 37
2024-02-09 22:05:50,233 Epoch  37: Total Training Recognition Loss 2.80  Total Training Translation Loss 563.28 
2024-02-09 22:05:50,233 EPOCH 38
2024-02-09 22:06:06,581 Epoch  38: Total Training Recognition Loss 2.72  Total Training Translation Loss 558.74 
2024-02-09 22:06:06,582 EPOCH 39
2024-02-09 22:06:22,718 Epoch  39: Total Training Recognition Loss 2.63  Total Training Translation Loss 558.25 
2024-02-09 22:06:22,718 EPOCH 40
2024-02-09 22:06:38,853 Epoch  40: Total Training Recognition Loss 2.61  Total Training Translation Loss 547.81 
2024-02-09 22:06:38,853 EPOCH 41
2024-02-09 22:06:55,153 Epoch  41: Total Training Recognition Loss 2.53  Total Training Translation Loss 537.88 
2024-02-09 22:06:55,154 EPOCH 42
2024-02-09 22:07:11,633 Epoch  42: Total Training Recognition Loss 2.32  Total Training Translation Loss 525.38 
2024-02-09 22:07:11,633 EPOCH 43
2024-02-09 22:07:27,785 Epoch  43: Total Training Recognition Loss 2.32  Total Training Translation Loss 519.81 
2024-02-09 22:07:27,785 EPOCH 44
2024-02-09 22:07:44,572 Epoch  44: Total Training Recognition Loss 2.23  Total Training Translation Loss 510.33 
2024-02-09 22:07:44,573 EPOCH 45
2024-02-09 22:07:55,620 [Epoch: 045 Step: 00000400] Batch Recognition Loss:   0.458852 => Gls Tokens per Sec:      382 || Batch Translation Loss:  73.955307 => Txt Tokens per Sec:     1176 || Lr: 0.000100
2024-02-09 22:08:00,778 Epoch  45: Total Training Recognition Loss 2.13  Total Training Translation Loss 502.51 
2024-02-09 22:08:00,778 EPOCH 46
2024-02-09 22:08:17,180 Epoch  46: Total Training Recognition Loss 2.04  Total Training Translation Loss 493.19 
2024-02-09 22:08:17,181 EPOCH 47
2024-02-09 22:08:33,747 Epoch  47: Total Training Recognition Loss 1.87  Total Training Translation Loss 485.99 
2024-02-09 22:08:33,747 EPOCH 48
2024-02-09 22:08:50,034 Epoch  48: Total Training Recognition Loss 1.82  Total Training Translation Loss 482.57 
2024-02-09 22:08:50,035 EPOCH 49
2024-02-09 22:09:06,368 Epoch  49: Total Training Recognition Loss 1.79  Total Training Translation Loss 472.81 
2024-02-09 22:09:06,368 EPOCH 50
2024-02-09 22:09:22,711 Epoch  50: Total Training Recognition Loss 1.77  Total Training Translation Loss 468.16 
2024-02-09 22:09:22,712 EPOCH 51
2024-02-09 22:09:39,283 Epoch  51: Total Training Recognition Loss 1.76  Total Training Translation Loss 463.58 
2024-02-09 22:09:39,284 EPOCH 52
2024-02-09 22:09:57,530 Epoch  52: Total Training Recognition Loss 1.78  Total Training Translation Loss 461.15 
2024-02-09 22:09:57,530 EPOCH 53
2024-02-09 22:10:14,463 Epoch  53: Total Training Recognition Loss 1.80  Total Training Translation Loss 451.32 
2024-02-09 22:10:14,463 EPOCH 54
2024-02-09 22:10:30,891 Epoch  54: Total Training Recognition Loss 1.66  Total Training Translation Loss 443.69 
2024-02-09 22:10:30,892 EPOCH 55
2024-02-09 22:10:47,629 Epoch  55: Total Training Recognition Loss 1.59  Total Training Translation Loss 435.70 
2024-02-09 22:10:47,630 EPOCH 56
2024-02-09 22:10:58,707 [Epoch: 056 Step: 00000500] Batch Recognition Loss:   0.166601 => Gls Tokens per Sec:      578 || Batch Translation Loss:  54.220665 => Txt Tokens per Sec:     1748 || Lr: 0.000100
2024-02-09 22:11:04,157 Epoch  56: Total Training Recognition Loss 1.56  Total Training Translation Loss 429.66 
2024-02-09 22:11:04,157 EPOCH 57
2024-02-09 22:11:20,472 Epoch  57: Total Training Recognition Loss 1.56  Total Training Translation Loss 427.60 
2024-02-09 22:11:20,472 EPOCH 58
2024-02-09 22:11:36,849 Epoch  58: Total Training Recognition Loss 1.59  Total Training Translation Loss 424.86 
2024-02-09 22:11:36,850 EPOCH 59
2024-02-09 22:11:53,019 Epoch  59: Total Training Recognition Loss 1.49  Total Training Translation Loss 421.20 
2024-02-09 22:11:53,019 EPOCH 60
2024-02-09 22:12:09,332 Epoch  60: Total Training Recognition Loss 1.51  Total Training Translation Loss 413.37 
2024-02-09 22:12:09,333 EPOCH 61
2024-02-09 22:12:25,983 Epoch  61: Total Training Recognition Loss 1.48  Total Training Translation Loss 408.74 
2024-02-09 22:12:25,984 EPOCH 62
2024-02-09 22:12:43,737 Epoch  62: Total Training Recognition Loss 1.48  Total Training Translation Loss 414.64 
2024-02-09 22:12:43,738 EPOCH 63
2024-02-09 22:12:59,985 Epoch  63: Total Training Recognition Loss 1.42  Total Training Translation Loss 398.54 
2024-02-09 22:12:59,986 EPOCH 64
2024-02-09 22:13:16,351 Epoch  64: Total Training Recognition Loss 1.43  Total Training Translation Loss 389.62 
2024-02-09 22:13:16,351 EPOCH 65
2024-02-09 22:13:32,415 Epoch  65: Total Training Recognition Loss 1.34  Total Training Translation Loss 379.86 
2024-02-09 22:13:32,415 EPOCH 66
2024-02-09 22:13:49,105 Epoch  66: Total Training Recognition Loss 1.33  Total Training Translation Loss 372.64 
2024-02-09 22:13:49,106 EPOCH 67
2024-02-09 22:14:01,209 [Epoch: 067 Step: 00000600] Batch Recognition Loss:   0.269614 => Gls Tokens per Sec:      560 || Batch Translation Loss:  55.315395 => Txt Tokens per Sec:     1582 || Lr: 0.000100
2024-02-09 22:14:05,336 Epoch  67: Total Training Recognition Loss 1.28  Total Training Translation Loss 366.04 
2024-02-09 22:14:05,336 EPOCH 68
2024-02-09 22:14:21,694 Epoch  68: Total Training Recognition Loss 1.22  Total Training Translation Loss 358.34 
2024-02-09 22:14:21,694 EPOCH 69
2024-02-09 22:14:38,106 Epoch  69: Total Training Recognition Loss 1.16  Total Training Translation Loss 352.11 
2024-02-09 22:14:38,107 EPOCH 70
2024-02-09 22:14:54,317 Epoch  70: Total Training Recognition Loss 1.21  Total Training Translation Loss 347.73 
2024-02-09 22:14:54,317 EPOCH 71
2024-02-09 22:15:10,925 Epoch  71: Total Training Recognition Loss 1.19  Total Training Translation Loss 344.75 
2024-02-09 22:15:10,926 EPOCH 72
2024-02-09 22:15:27,154 Epoch  72: Total Training Recognition Loss 1.17  Total Training Translation Loss 341.38 
2024-02-09 22:15:27,155 EPOCH 73
2024-02-09 22:15:43,576 Epoch  73: Total Training Recognition Loss 1.16  Total Training Translation Loss 335.35 
2024-02-09 22:15:43,577 EPOCH 74
2024-02-09 22:15:59,732 Epoch  74: Total Training Recognition Loss 1.16  Total Training Translation Loss 331.77 
2024-02-09 22:15:59,732 EPOCH 75
2024-02-09 22:16:15,845 Epoch  75: Total Training Recognition Loss 1.12  Total Training Translation Loss 325.27 
2024-02-09 22:16:15,845 EPOCH 76
2024-02-09 22:16:32,434 Epoch  76: Total Training Recognition Loss 1.12  Total Training Translation Loss 317.44 
2024-02-09 22:16:32,434 EPOCH 77
2024-02-09 22:16:48,551 Epoch  77: Total Training Recognition Loss 1.12  Total Training Translation Loss 310.08 
2024-02-09 22:16:48,552 EPOCH 78
2024-02-09 22:16:57,927 [Epoch: 078 Step: 00000700] Batch Recognition Loss:   0.144842 => Gls Tokens per Sec:      860 || Batch Translation Loss:  42.606747 => Txt Tokens per Sec:     2274 || Lr: 0.000100
2024-02-09 22:17:04,320 Epoch  78: Total Training Recognition Loss 1.04  Total Training Translation Loss 303.19 
2024-02-09 22:17:04,321 EPOCH 79
2024-02-09 22:17:20,661 Epoch  79: Total Training Recognition Loss 1.10  Total Training Translation Loss 299.83 
2024-02-09 22:17:20,662 EPOCH 80
2024-02-09 22:17:36,793 Epoch  80: Total Training Recognition Loss 1.01  Total Training Translation Loss 294.63 
2024-02-09 22:17:36,793 EPOCH 81
2024-02-09 22:17:53,190 Epoch  81: Total Training Recognition Loss 1.07  Total Training Translation Loss 291.59 
2024-02-09 22:17:53,190 EPOCH 82
2024-02-09 22:18:09,388 Epoch  82: Total Training Recognition Loss 1.03  Total Training Translation Loss 291.35 
2024-02-09 22:18:09,389 EPOCH 83
2024-02-09 22:18:25,589 Epoch  83: Total Training Recognition Loss 1.13  Total Training Translation Loss 301.81 
2024-02-09 22:18:25,590 EPOCH 84
2024-02-09 22:18:41,721 Epoch  84: Total Training Recognition Loss 1.11  Total Training Translation Loss 292.18 
2024-02-09 22:18:41,721 EPOCH 85
2024-02-09 22:18:58,071 Epoch  85: Total Training Recognition Loss 1.11  Total Training Translation Loss 282.87 
2024-02-09 22:18:58,071 EPOCH 86
2024-02-09 22:19:15,227 Epoch  86: Total Training Recognition Loss 1.05  Total Training Translation Loss 272.97 
2024-02-09 22:19:15,228 EPOCH 87
2024-02-09 22:19:31,304 Epoch  87: Total Training Recognition Loss 1.05  Total Training Translation Loss 267.34 
2024-02-09 22:19:31,305 EPOCH 88
2024-02-09 22:19:47,156 Epoch  88: Total Training Recognition Loss 1.06  Total Training Translation Loss 263.15 
2024-02-09 22:19:47,157 EPOCH 89
2024-02-09 22:20:02,998 [Epoch: 089 Step: 00000800] Batch Recognition Loss:   0.121871 => Gls Tokens per Sec:      590 || Batch Translation Loss:  15.676558 => Txt Tokens per Sec:     1709 || Lr: 0.000100
2024-02-09 22:20:03,282 Epoch  89: Total Training Recognition Loss 0.96  Total Training Translation Loss 265.12 
2024-02-09 22:20:03,283 EPOCH 90
2024-02-09 22:20:19,264 Epoch  90: Total Training Recognition Loss 1.02  Total Training Translation Loss 259.54 
2024-02-09 22:20:19,265 EPOCH 91
2024-02-09 22:20:35,366 Epoch  91: Total Training Recognition Loss 0.96  Total Training Translation Loss 251.60 
2024-02-09 22:20:35,367 EPOCH 92
2024-02-09 22:20:51,619 Epoch  92: Total Training Recognition Loss 0.96  Total Training Translation Loss 246.71 
2024-02-09 22:20:51,620 EPOCH 93
2024-02-09 22:21:07,928 Epoch  93: Total Training Recognition Loss 0.98  Total Training Translation Loss 243.40 
2024-02-09 22:21:07,928 EPOCH 94
2024-02-09 22:21:24,216 Epoch  94: Total Training Recognition Loss 0.96  Total Training Translation Loss 239.18 
2024-02-09 22:21:24,216 EPOCH 95
2024-02-09 22:21:40,488 Epoch  95: Total Training Recognition Loss 1.00  Total Training Translation Loss 241.72 
2024-02-09 22:21:40,488 EPOCH 96
2024-02-09 22:21:56,986 Epoch  96: Total Training Recognition Loss 1.02  Total Training Translation Loss 233.29 
2024-02-09 22:21:56,987 EPOCH 97
2024-02-09 22:22:13,091 Epoch  97: Total Training Recognition Loss 0.94  Total Training Translation Loss 224.78 
2024-02-09 22:22:13,092 EPOCH 98
2024-02-09 22:22:29,254 Epoch  98: Total Training Recognition Loss 0.94  Total Training Translation Loss 218.57 
2024-02-09 22:22:29,254 EPOCH 99
2024-02-09 22:22:45,048 Epoch  99: Total Training Recognition Loss 0.90  Total Training Translation Loss 217.38 
2024-02-09 22:22:45,048 EPOCH 100
2024-02-09 22:23:01,152 [Epoch: 100 Step: 00000900] Batch Recognition Loss:   0.141578 => Gls Tokens per Sec:      660 || Batch Translation Loss:  10.370111 => Txt Tokens per Sec:     1825 || Lr: 0.000100
2024-02-09 22:23:01,152 Epoch 100: Total Training Recognition Loss 0.95  Total Training Translation Loss 212.26 
2024-02-09 22:23:01,152 EPOCH 101
2024-02-09 22:23:17,401 Epoch 101: Total Training Recognition Loss 0.92  Total Training Translation Loss 210.27 
2024-02-09 22:23:17,402 EPOCH 102
2024-02-09 22:23:33,650 Epoch 102: Total Training Recognition Loss 0.91  Total Training Translation Loss 205.30 
2024-02-09 22:23:33,651 EPOCH 103
2024-02-09 22:23:49,779 Epoch 103: Total Training Recognition Loss 0.93  Total Training Translation Loss 202.93 
2024-02-09 22:23:49,780 EPOCH 104
2024-02-09 22:24:05,497 Epoch 104: Total Training Recognition Loss 0.87  Total Training Translation Loss 195.66 
2024-02-09 22:24:05,497 EPOCH 105
2024-02-09 22:24:21,776 Epoch 105: Total Training Recognition Loss 0.88  Total Training Translation Loss 191.13 
2024-02-09 22:24:21,777 EPOCH 106
2024-02-09 22:24:38,009 Epoch 106: Total Training Recognition Loss 0.83  Total Training Translation Loss 185.40 
2024-02-09 22:24:38,009 EPOCH 107
2024-02-09 22:24:53,985 Epoch 107: Total Training Recognition Loss 0.85  Total Training Translation Loss 182.99 
2024-02-09 22:24:53,985 EPOCH 108
2024-02-09 22:25:09,973 Epoch 108: Total Training Recognition Loss 0.87  Total Training Translation Loss 178.26 
2024-02-09 22:25:09,973 EPOCH 109
2024-02-09 22:25:25,972 Epoch 109: Total Training Recognition Loss 0.89  Total Training Translation Loss 178.21 
2024-02-09 22:25:25,973 EPOCH 110
2024-02-09 22:25:41,948 Epoch 110: Total Training Recognition Loss 0.88  Total Training Translation Loss 174.29 
2024-02-09 22:25:41,949 EPOCH 111
2024-02-09 22:25:58,307 Epoch 111: Total Training Recognition Loss 0.84  Total Training Translation Loss 168.74 
2024-02-09 22:25:58,308 EPOCH 112
2024-02-09 22:25:58,741 [Epoch: 112 Step: 00001000] Batch Recognition Loss:   0.061712 => Gls Tokens per Sec:     2963 || Batch Translation Loss:  20.029348 => Txt Tokens per Sec:     8164 || Lr: 0.000100
2024-02-09 22:26:14,303 Epoch 112: Total Training Recognition Loss 0.86  Total Training Translation Loss 164.43 
2024-02-09 22:26:14,303 EPOCH 113
2024-02-09 22:26:30,287 Epoch 113: Total Training Recognition Loss 0.83  Total Training Translation Loss 158.89 
2024-02-09 22:26:30,287 EPOCH 114
2024-02-09 22:26:46,755 Epoch 114: Total Training Recognition Loss 0.83  Total Training Translation Loss 154.09 
2024-02-09 22:26:46,756 EPOCH 115
2024-02-09 22:27:02,976 Epoch 115: Total Training Recognition Loss 0.78  Total Training Translation Loss 148.96 
2024-02-09 22:27:02,977 EPOCH 116
2024-02-09 22:27:19,273 Epoch 116: Total Training Recognition Loss 0.81  Total Training Translation Loss 145.30 
2024-02-09 22:27:19,274 EPOCH 117
2024-02-09 22:27:35,419 Epoch 117: Total Training Recognition Loss 0.77  Total Training Translation Loss 142.88 
2024-02-09 22:27:35,419 EPOCH 118
2024-02-09 22:27:51,773 Epoch 118: Total Training Recognition Loss 0.77  Total Training Translation Loss 139.02 
2024-02-09 22:27:51,774 EPOCH 119
2024-02-09 22:28:07,849 Epoch 119: Total Training Recognition Loss 0.78  Total Training Translation Loss 136.94 
2024-02-09 22:28:07,850 EPOCH 120
2024-02-09 22:28:24,375 Epoch 120: Total Training Recognition Loss 0.75  Total Training Translation Loss 134.62 
2024-02-09 22:28:24,375 EPOCH 121
2024-02-09 22:28:40,572 Epoch 121: Total Training Recognition Loss 0.75  Total Training Translation Loss 130.70 
2024-02-09 22:28:40,573 EPOCH 122
2024-02-09 22:28:56,583 Epoch 122: Total Training Recognition Loss 0.74  Total Training Translation Loss 127.83 
2024-02-09 22:28:56,583 EPOCH 123
2024-02-09 22:28:57,558 [Epoch: 123 Step: 00001100] Batch Recognition Loss:   0.078323 => Gls Tokens per Sec:     2628 || Batch Translation Loss:  11.786918 => Txt Tokens per Sec:     6566 || Lr: 0.000100
2024-02-09 22:29:12,985 Epoch 123: Total Training Recognition Loss 0.75  Total Training Translation Loss 128.54 
2024-02-09 22:29:12,985 EPOCH 124
2024-02-09 22:29:29,428 Epoch 124: Total Training Recognition Loss 0.76  Total Training Translation Loss 127.29 
2024-02-09 22:29:29,429 EPOCH 125
2024-02-09 22:29:46,260 Epoch 125: Total Training Recognition Loss 0.74  Total Training Translation Loss 123.83 
2024-02-09 22:29:46,261 EPOCH 126
2024-02-09 22:30:02,382 Epoch 126: Total Training Recognition Loss 0.77  Total Training Translation Loss 120.54 
2024-02-09 22:30:02,383 EPOCH 127
2024-02-09 22:30:18,252 Epoch 127: Total Training Recognition Loss 0.77  Total Training Translation Loss 122.78 
2024-02-09 22:30:18,252 EPOCH 128
2024-02-09 22:30:34,514 Epoch 128: Total Training Recognition Loss 0.77  Total Training Translation Loss 118.97 
2024-02-09 22:30:34,514 EPOCH 129
2024-02-09 22:30:50,799 Epoch 129: Total Training Recognition Loss 0.79  Total Training Translation Loss 113.89 
2024-02-09 22:30:50,800 EPOCH 130
2024-02-09 22:31:06,818 Epoch 130: Total Training Recognition Loss 0.74  Total Training Translation Loss 114.61 
2024-02-09 22:31:06,818 EPOCH 131
2024-02-09 22:31:23,148 Epoch 131: Total Training Recognition Loss 0.76  Total Training Translation Loss 111.63 
2024-02-09 22:31:23,149 EPOCH 132
2024-02-09 22:31:41,380 Epoch 132: Total Training Recognition Loss 0.79  Total Training Translation Loss 110.96 
2024-02-09 22:31:41,380 EPOCH 133
2024-02-09 22:31:58,299 Epoch 133: Total Training Recognition Loss 0.78  Total Training Translation Loss 104.44 
2024-02-09 22:31:58,300 EPOCH 134
2024-02-09 22:32:03,396 [Epoch: 134 Step: 00001200] Batch Recognition Loss:   0.103157 => Gls Tokens per Sec:      577 || Batch Translation Loss:   3.636260 => Txt Tokens per Sec:     1444 || Lr: 0.000100
2024-02-09 22:32:14,807 Epoch 134: Total Training Recognition Loss 0.77  Total Training Translation Loss 100.00 
2024-02-09 22:32:14,808 EPOCH 135
2024-02-09 22:32:31,478 Epoch 135: Total Training Recognition Loss 0.71  Total Training Translation Loss 96.81 
2024-02-09 22:32:31,478 EPOCH 136
2024-02-09 22:32:47,866 Epoch 136: Total Training Recognition Loss 0.73  Total Training Translation Loss 95.31 
2024-02-09 22:32:47,866 EPOCH 137
2024-02-09 22:33:04,028 Epoch 137: Total Training Recognition Loss 0.69  Total Training Translation Loss 96.92 
2024-02-09 22:33:04,029 EPOCH 138
2024-02-09 22:33:20,044 Epoch 138: Total Training Recognition Loss 0.73  Total Training Translation Loss 102.18 
2024-02-09 22:33:20,045 EPOCH 139
2024-02-09 22:33:36,318 Epoch 139: Total Training Recognition Loss 0.73  Total Training Translation Loss 98.03 
2024-02-09 22:33:36,318 EPOCH 140
2024-02-09 22:33:52,330 Epoch 140: Total Training Recognition Loss 0.70  Total Training Translation Loss 89.90 
2024-02-09 22:33:52,330 EPOCH 141
2024-02-09 22:34:08,757 Epoch 141: Total Training Recognition Loss 0.71  Total Training Translation Loss 84.43 
2024-02-09 22:34:08,758 EPOCH 142
2024-02-09 22:34:24,880 Epoch 142: Total Training Recognition Loss 0.72  Total Training Translation Loss 80.50 
2024-02-09 22:34:24,880 EPOCH 143
2024-02-09 22:34:41,403 Epoch 143: Total Training Recognition Loss 0.71  Total Training Translation Loss 77.23 
2024-02-09 22:34:41,403 EPOCH 144
2024-02-09 22:34:57,625 Epoch 144: Total Training Recognition Loss 0.65  Total Training Translation Loss 74.43 
2024-02-09 22:34:57,626 EPOCH 145
2024-02-09 22:34:59,278 [Epoch: 145 Step: 00001300] Batch Recognition Loss:   0.063095 => Gls Tokens per Sec:     3099 || Batch Translation Loss:   9.887820 => Txt Tokens per Sec:     7982 || Lr: 0.000100
2024-02-09 22:35:13,565 Epoch 145: Total Training Recognition Loss 0.62  Total Training Translation Loss 72.97 
2024-02-09 22:35:13,566 EPOCH 146
2024-02-09 22:35:29,803 Epoch 146: Total Training Recognition Loss 0.63  Total Training Translation Loss 69.76 
2024-02-09 22:35:29,804 EPOCH 147
2024-02-09 22:35:45,496 Epoch 147: Total Training Recognition Loss 0.59  Total Training Translation Loss 68.17 
2024-02-09 22:35:45,496 EPOCH 148
2024-02-09 22:36:01,776 Epoch 148: Total Training Recognition Loss 0.60  Total Training Translation Loss 68.03 
2024-02-09 22:36:01,777 EPOCH 149
2024-02-09 22:36:17,925 Epoch 149: Total Training Recognition Loss 0.60  Total Training Translation Loss 64.58 
2024-02-09 22:36:17,925 EPOCH 150
2024-02-09 22:36:33,976 Epoch 150: Total Training Recognition Loss 0.59  Total Training Translation Loss 63.16 
2024-02-09 22:36:33,976 EPOCH 151
2024-02-09 22:36:49,881 Epoch 151: Total Training Recognition Loss 0.58  Total Training Translation Loss 60.63 
2024-02-09 22:36:49,881 EPOCH 152
2024-02-09 22:37:06,429 Epoch 152: Total Training Recognition Loss 0.56  Total Training Translation Loss 59.88 
2024-02-09 22:37:06,430 EPOCH 153
2024-02-09 22:37:22,137 Epoch 153: Total Training Recognition Loss 0.55  Total Training Translation Loss 58.41 
2024-02-09 22:37:22,138 EPOCH 154
2024-02-09 22:37:38,191 Epoch 154: Total Training Recognition Loss 0.57  Total Training Translation Loss 58.91 
2024-02-09 22:37:38,192 EPOCH 155
2024-02-09 22:37:54,168 Epoch 155: Total Training Recognition Loss 0.57  Total Training Translation Loss 59.48 
2024-02-09 22:37:54,168 EPOCH 156
2024-02-09 22:38:00,162 [Epoch: 156 Step: 00001400] Batch Recognition Loss:   0.063583 => Gls Tokens per Sec:      918 || Batch Translation Loss:   5.507606 => Txt Tokens per Sec:     2251 || Lr: 0.000100
2024-02-09 22:38:10,332 Epoch 156: Total Training Recognition Loss 0.60  Total Training Translation Loss 57.86 
2024-02-09 22:38:10,332 EPOCH 157
2024-02-09 22:38:26,636 Epoch 157: Total Training Recognition Loss 0.60  Total Training Translation Loss 55.81 
2024-02-09 22:38:26,636 EPOCH 158
2024-02-09 22:38:42,454 Epoch 158: Total Training Recognition Loss 0.54  Total Training Translation Loss 53.41 
2024-02-09 22:38:42,455 EPOCH 159
2024-02-09 22:38:58,695 Epoch 159: Total Training Recognition Loss 0.57  Total Training Translation Loss 51.76 
2024-02-09 22:38:58,696 EPOCH 160
2024-02-09 22:39:14,863 Epoch 160: Total Training Recognition Loss 0.55  Total Training Translation Loss 51.14 
2024-02-09 22:39:14,864 EPOCH 161
2024-02-09 22:39:31,120 Epoch 161: Total Training Recognition Loss 0.55  Total Training Translation Loss 48.86 
2024-02-09 22:39:31,120 EPOCH 162
2024-02-09 22:39:47,268 Epoch 162: Total Training Recognition Loss 0.55  Total Training Translation Loss 49.12 
2024-02-09 22:39:47,269 EPOCH 163
2024-02-09 22:40:03,607 Epoch 163: Total Training Recognition Loss 0.55  Total Training Translation Loss 47.87 
2024-02-09 22:40:03,608 EPOCH 164
2024-02-09 22:40:19,841 Epoch 164: Total Training Recognition Loss 0.53  Total Training Translation Loss 45.04 
2024-02-09 22:40:19,842 EPOCH 165
2024-02-09 22:40:36,257 Epoch 165: Total Training Recognition Loss 0.50  Total Training Translation Loss 43.05 
2024-02-09 22:40:36,258 EPOCH 166
2024-02-09 22:40:52,226 Epoch 166: Total Training Recognition Loss 0.48  Total Training Translation Loss 41.56 
2024-02-09 22:40:52,227 EPOCH 167
2024-02-09 22:41:06,727 [Epoch: 167 Step: 00001500] Batch Recognition Loss:   0.035015 => Gls Tokens per Sec:      468 || Batch Translation Loss:   4.599648 => Txt Tokens per Sec:     1309 || Lr: 0.000100
2024-02-09 22:41:08,541 Epoch 167: Total Training Recognition Loss 0.48  Total Training Translation Loss 40.87 
2024-02-09 22:41:08,541 EPOCH 168
2024-02-09 22:41:24,558 Epoch 168: Total Training Recognition Loss 0.49  Total Training Translation Loss 39.26 
2024-02-09 22:41:24,558 EPOCH 169
2024-02-09 22:41:40,650 Epoch 169: Total Training Recognition Loss 0.52  Total Training Translation Loss 38.01 
2024-02-09 22:41:40,650 EPOCH 170
2024-02-09 22:41:56,594 Epoch 170: Total Training Recognition Loss 0.47  Total Training Translation Loss 38.02 
2024-02-09 22:41:56,595 EPOCH 171
2024-02-09 22:42:12,473 Epoch 171: Total Training Recognition Loss 0.50  Total Training Translation Loss 36.55 
2024-02-09 22:42:12,473 EPOCH 172
2024-02-09 22:42:28,297 Epoch 172: Total Training Recognition Loss 0.45  Total Training Translation Loss 36.31 
2024-02-09 22:42:28,298 EPOCH 173
2024-02-09 22:42:44,523 Epoch 173: Total Training Recognition Loss 0.45  Total Training Translation Loss 35.44 
2024-02-09 22:42:44,524 EPOCH 174
2024-02-09 22:43:00,294 Epoch 174: Total Training Recognition Loss 0.44  Total Training Translation Loss 33.79 
2024-02-09 22:43:00,294 EPOCH 175
2024-02-09 22:43:16,374 Epoch 175: Total Training Recognition Loss 0.46  Total Training Translation Loss 33.47 
2024-02-09 22:43:16,375 EPOCH 176
2024-02-09 22:43:32,421 Epoch 176: Total Training Recognition Loss 0.42  Total Training Translation Loss 32.52 
2024-02-09 22:43:32,421 EPOCH 177
2024-02-09 22:43:48,690 Epoch 177: Total Training Recognition Loss 0.41  Total Training Translation Loss 31.53 
2024-02-09 22:43:48,691 EPOCH 178
2024-02-09 22:44:03,796 [Epoch: 178 Step: 00001600] Batch Recognition Loss:   0.066248 => Gls Tokens per Sec:      534 || Batch Translation Loss:   2.177579 => Txt Tokens per Sec:     1498 || Lr: 0.000100
2024-02-09 22:44:04,862 Epoch 178: Total Training Recognition Loss 0.42  Total Training Translation Loss 31.00 
2024-02-09 22:44:04,862 EPOCH 179
2024-02-09 22:44:21,105 Epoch 179: Total Training Recognition Loss 0.41  Total Training Translation Loss 30.63 
2024-02-09 22:44:21,106 EPOCH 180
2024-02-09 22:44:37,134 Epoch 180: Total Training Recognition Loss 0.41  Total Training Translation Loss 28.97 
2024-02-09 22:44:37,134 EPOCH 181
2024-02-09 22:44:53,077 Epoch 181: Total Training Recognition Loss 0.42  Total Training Translation Loss 29.24 
2024-02-09 22:44:53,078 EPOCH 182
2024-02-09 22:45:09,029 Epoch 182: Total Training Recognition Loss 0.42  Total Training Translation Loss 29.04 
2024-02-09 22:45:09,029 EPOCH 183
2024-02-09 22:45:25,615 Epoch 183: Total Training Recognition Loss 0.41  Total Training Translation Loss 27.97 
2024-02-09 22:45:25,615 EPOCH 184
2024-02-09 22:45:41,967 Epoch 184: Total Training Recognition Loss 0.39  Total Training Translation Loss 27.35 
2024-02-09 22:45:41,967 EPOCH 185
2024-02-09 22:45:58,361 Epoch 185: Total Training Recognition Loss 0.41  Total Training Translation Loss 28.17 
2024-02-09 22:45:58,362 EPOCH 186
2024-02-09 22:46:14,817 Epoch 186: Total Training Recognition Loss 0.41  Total Training Translation Loss 26.29 
2024-02-09 22:46:14,817 EPOCH 187
2024-02-09 22:46:31,138 Epoch 187: Total Training Recognition Loss 0.37  Total Training Translation Loss 25.89 
2024-02-09 22:46:31,139 EPOCH 188
2024-02-09 22:46:47,730 Epoch 188: Total Training Recognition Loss 0.39  Total Training Translation Loss 25.07 
2024-02-09 22:46:47,731 EPOCH 189
2024-02-09 22:47:00,689 [Epoch: 189 Step: 00001700] Batch Recognition Loss:   0.020670 => Gls Tokens per Sec:      721 || Batch Translation Loss:   2.838775 => Txt Tokens per Sec:     1950 || Lr: 0.000100
2024-02-09 22:47:03,857 Epoch 189: Total Training Recognition Loss 0.39  Total Training Translation Loss 24.94 
2024-02-09 22:47:03,857 EPOCH 190
2024-02-09 22:47:19,822 Epoch 190: Total Training Recognition Loss 0.36  Total Training Translation Loss 24.42 
2024-02-09 22:47:19,823 EPOCH 191
2024-02-09 22:47:36,326 Epoch 191: Total Training Recognition Loss 0.36  Total Training Translation Loss 24.51 
2024-02-09 22:47:36,327 EPOCH 192
2024-02-09 22:47:52,215 Epoch 192: Total Training Recognition Loss 0.37  Total Training Translation Loss 23.48 
2024-02-09 22:47:52,216 EPOCH 193
2024-02-09 22:48:08,179 Epoch 193: Total Training Recognition Loss 0.36  Total Training Translation Loss 22.37 
2024-02-09 22:48:08,179 EPOCH 194
2024-02-09 22:48:24,284 Epoch 194: Total Training Recognition Loss 0.35  Total Training Translation Loss 21.98 
2024-02-09 22:48:24,285 EPOCH 195
2024-02-09 22:48:40,519 Epoch 195: Total Training Recognition Loss 0.33  Total Training Translation Loss 21.05 
2024-02-09 22:48:40,519 EPOCH 196
2024-02-09 22:48:56,486 Epoch 196: Total Training Recognition Loss 0.34  Total Training Translation Loss 20.57 
2024-02-09 22:48:56,486 EPOCH 197
2024-02-09 22:49:12,552 Epoch 197: Total Training Recognition Loss 0.32  Total Training Translation Loss 20.28 
2024-02-09 22:49:12,553 EPOCH 198
2024-02-09 22:49:28,739 Epoch 198: Total Training Recognition Loss 0.33  Total Training Translation Loss 19.92 
2024-02-09 22:49:28,740 EPOCH 199
2024-02-09 22:49:44,824 Epoch 199: Total Training Recognition Loss 0.31  Total Training Translation Loss 19.84 
2024-02-09 22:49:44,825 EPOCH 200
2024-02-09 22:50:00,785 [Epoch: 200 Step: 00001800] Batch Recognition Loss:   0.033208 => Gls Tokens per Sec:      665 || Batch Translation Loss:   2.961499 => Txt Tokens per Sec:     1841 || Lr: 0.000100
2024-02-09 22:50:00,786 Epoch 200: Total Training Recognition Loss 0.32  Total Training Translation Loss 20.30 
2024-02-09 22:50:00,786 EPOCH 201
2024-02-09 22:50:16,840 Epoch 201: Total Training Recognition Loss 0.32  Total Training Translation Loss 19.19 
2024-02-09 22:50:16,840 EPOCH 202
2024-02-09 22:50:32,823 Epoch 202: Total Training Recognition Loss 0.33  Total Training Translation Loss 19.21 
2024-02-09 22:50:32,824 EPOCH 203
2024-02-09 22:50:49,371 Epoch 203: Total Training Recognition Loss 0.32  Total Training Translation Loss 18.44 
2024-02-09 22:50:49,371 EPOCH 204
2024-02-09 22:51:07,708 Epoch 204: Total Training Recognition Loss 0.31  Total Training Translation Loss 17.58 
2024-02-09 22:51:07,709 EPOCH 205
2024-02-09 22:51:24,259 Epoch 205: Total Training Recognition Loss 0.30  Total Training Translation Loss 17.32 
2024-02-09 22:51:24,260 EPOCH 206
2024-02-09 22:51:40,423 Epoch 206: Total Training Recognition Loss 0.31  Total Training Translation Loss 16.77 
2024-02-09 22:51:40,423 EPOCH 207
2024-02-09 22:51:57,054 Epoch 207: Total Training Recognition Loss 0.30  Total Training Translation Loss 16.44 
2024-02-09 22:51:57,055 EPOCH 208
2024-02-09 22:52:14,464 Epoch 208: Total Training Recognition Loss 0.29  Total Training Translation Loss 15.91 
2024-02-09 22:52:14,464 EPOCH 209
2024-02-09 22:52:31,050 Epoch 209: Total Training Recognition Loss 0.29  Total Training Translation Loss 15.82 
2024-02-09 22:52:31,050 EPOCH 210
2024-02-09 22:52:47,694 Epoch 210: Total Training Recognition Loss 0.27  Total Training Translation Loss 15.92 
2024-02-09 22:52:47,694 EPOCH 211
2024-02-09 22:53:04,172 Epoch 211: Total Training Recognition Loss 0.29  Total Training Translation Loss 15.29 
2024-02-09 22:53:04,173 EPOCH 212
2024-02-09 22:53:04,787 [Epoch: 212 Step: 00001900] Batch Recognition Loss:   0.036859 => Gls Tokens per Sec:     2085 || Batch Translation Loss:   2.066308 => Txt Tokens per Sec:     6249 || Lr: 0.000100
2024-02-09 22:53:20,759 Epoch 212: Total Training Recognition Loss 0.29  Total Training Translation Loss 15.17 
2024-02-09 22:53:20,760 EPOCH 213
2024-02-09 22:53:36,986 Epoch 213: Total Training Recognition Loss 0.27  Total Training Translation Loss 14.93 
2024-02-09 22:53:36,987 EPOCH 214
2024-02-09 22:53:53,234 Epoch 214: Total Training Recognition Loss 0.30  Total Training Translation Loss 14.44 
2024-02-09 22:53:53,235 EPOCH 215
2024-02-09 22:54:09,630 Epoch 215: Total Training Recognition Loss 0.27  Total Training Translation Loss 14.45 
2024-02-09 22:54:09,631 EPOCH 216
2024-02-09 22:54:25,925 Epoch 216: Total Training Recognition Loss 0.27  Total Training Translation Loss 13.56 
2024-02-09 22:54:25,925 EPOCH 217
2024-02-09 22:54:42,108 Epoch 217: Total Training Recognition Loss 0.24  Total Training Translation Loss 13.73 
2024-02-09 22:54:42,109 EPOCH 218
2024-02-09 22:54:58,611 Epoch 218: Total Training Recognition Loss 0.28  Total Training Translation Loss 13.31 
2024-02-09 22:54:58,612 EPOCH 219
2024-02-09 22:55:14,463 Epoch 219: Total Training Recognition Loss 0.26  Total Training Translation Loss 13.46 
2024-02-09 22:55:14,464 EPOCH 220
2024-02-09 22:55:30,723 Epoch 220: Total Training Recognition Loss 0.26  Total Training Translation Loss 12.73 
2024-02-09 22:55:30,724 EPOCH 221
2024-02-09 22:55:46,637 Epoch 221: Total Training Recognition Loss 0.25  Total Training Translation Loss 12.72 
2024-02-09 22:55:46,638 EPOCH 222
2024-02-09 22:56:02,912 Epoch 222: Total Training Recognition Loss 0.22  Total Training Translation Loss 12.35 
2024-02-09 22:56:02,912 EPOCH 223
2024-02-09 22:56:03,627 [Epoch: 223 Step: 00002000] Batch Recognition Loss:   0.024515 => Gls Tokens per Sec:     3585 || Batch Translation Loss:   1.171681 => Txt Tokens per Sec:     8571 || Lr: 0.000100
2024-02-09 22:57:59,413 Hooray! New best validation result [eval_metric]!
2024-02-09 22:57:59,415 Saving new checkpoint.
2024-02-09 22:57:59,744 Validation result at epoch 223, step     2000: duration: 116.1173s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.55171	Translation Loss: 76063.57812	PPL: 1992.69031
	Eval Metric: BLEU
	WER 7.84	(DEL: 0.00,	INS: 0.00,	SUB: 7.84)
	BLEU-4 0.80	(BLEU-1: 13.23,	BLEU-2: 4.53,	BLEU-3: 1.79,	BLEU-4: 0.80)
	CHRF 17.64	ROUGE 10.72
2024-02-09 22:57:59,745 Logging Recognition and Translation Outputs
2024-02-09 22:57:59,745 ========================================================================================================================
2024-02-09 22:57:59,745 Logging Sequence: 165_414.00
2024-02-09 22:57:59,745 	Gloss Reference :	A B+C+D+E
2024-02-09 22:57:59,746 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 22:57:59,746 	Gloss Alignment :	         
2024-02-09 22:57:59,746 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 22:57:59,747 	Text Reference  :	he felt sachin was      lucky so       he always gave his     sweater to give it   to the umpire 
2024-02-09 22:57:59,747 	Text Hypothesis :	** **** delhi  capitals were  supposed to be     an   threats as      he not  take a  new zealand
2024-02-09 22:57:59,748 	Text Alignment  :	D  D    S      S        S     S        S  S      S    S       S       S  S    S    S  S   S      
2024-02-09 22:57:59,748 ========================================================================================================================
2024-02-09 22:57:59,748 Logging Sequence: 169_268.00
2024-02-09 22:57:59,748 	Gloss Reference :	A B+C+D+E
2024-02-09 22:57:59,748 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 22:57:59,748 	Gloss Alignment :	         
2024-02-09 22:57:59,748 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 22:57:59,749 	Text Reference  :	shami supports arshdeep and  many fans    supported him   as  well 
2024-02-09 22:57:59,749 	Text Hypothesis :	***** a        panel    will be   created to        solve the issue
2024-02-09 22:57:59,749 	Text Alignment  :	D     S        S        S    S    S       S         S     S   S    
2024-02-09 22:57:59,750 ========================================================================================================================
2024-02-09 22:57:59,750 Logging Sequence: 172_15.00
2024-02-09 22:57:59,750 	Gloss Reference :	A B+C+D+E
2024-02-09 22:57:59,750 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 22:57:59,750 	Gloss Alignment :	         
2024-02-09 22:57:59,750 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 22:57:59,752 	Text Reference  :	now in the final match on 28 may   2023   the  two teams were  up  against each other at       the same *** venue
2024-02-09 22:57:59,752 	Text Hypothesis :	*** ** *** ***** ***** ** ** after losing this was an    world cup trophy  he   was   unvieled the same and cafes
2024-02-09 22:57:59,752 	Text Alignment  :	D   D  D   D     D     D  D  S     S      S    S   S     S     S   S       S    S     S                 I   S    
2024-02-09 22:57:59,752 ========================================================================================================================
2024-02-09 22:57:59,753 Logging Sequence: 96_158.00
2024-02-09 22:57:59,753 	Gloss Reference :	A B+C+D+E
2024-02-09 22:57:59,753 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 22:57:59,753 	Gloss Alignment :	         
2024-02-09 22:57:59,753 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 22:57:59,754 	Text Reference  :	after this   pandya fell    on his knees in   disappointment
2024-02-09 22:57:59,754 	Text Hypothesis :	the   couple were   shocked to see the   next wicket        
2024-02-09 22:57:59,754 	Text Alignment  :	S     S      S      S       S  S   S     S    S             
2024-02-09 22:57:59,754 ========================================================================================================================
2024-02-09 22:57:59,754 Logging Sequence: 152_73.00
2024-02-09 22:57:59,755 	Gloss Reference :	A B+C+D+E
2024-02-09 22:57:59,755 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 22:57:59,755 	Gloss Alignment :	         
2024-02-09 22:57:59,755 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 22:57:59,756 	Text Reference  :	******* *** ***** *** ********* *** ******* ** *** ***** eventually he   too got  out by   shaheen afridi
2024-02-09 22:57:59,756 	Text Hypothesis :	however the match was cancelled and managed to bat first but        will be  held in  just 175     overs 
2024-02-09 22:57:59,756 	Text Alignment  :	I       I   I     I   I         I   I       I  I   I     S          S    S   S    S   S    S       S     
2024-02-09 22:57:59,757 ========================================================================================================================
2024-02-09 22:58:16,252 Epoch 223: Total Training Recognition Loss 0.23  Total Training Translation Loss 12.37 
2024-02-09 22:58:16,253 EPOCH 224
2024-02-09 22:58:32,559 Epoch 224: Total Training Recognition Loss 0.24  Total Training Translation Loss 12.19 
2024-02-09 22:58:32,560 EPOCH 225
2024-02-09 22:58:48,543 Epoch 225: Total Training Recognition Loss 0.25  Total Training Translation Loss 12.04 
2024-02-09 22:58:48,544 EPOCH 226
2024-02-09 22:59:04,594 Epoch 226: Total Training Recognition Loss 0.23  Total Training Translation Loss 11.83 
2024-02-09 22:59:04,595 EPOCH 227
2024-02-09 22:59:20,706 Epoch 227: Total Training Recognition Loss 0.24  Total Training Translation Loss 11.64 
2024-02-09 22:59:20,706 EPOCH 228
2024-02-09 22:59:37,070 Epoch 228: Total Training Recognition Loss 0.24  Total Training Translation Loss 11.29 
2024-02-09 22:59:37,071 EPOCH 229
2024-02-09 22:59:53,038 Epoch 229: Total Training Recognition Loss 0.21  Total Training Translation Loss 10.85 
2024-02-09 22:59:53,039 EPOCH 230
2024-02-09 23:00:09,273 Epoch 230: Total Training Recognition Loss 0.22  Total Training Translation Loss 10.80 
2024-02-09 23:00:09,273 EPOCH 231
2024-02-09 23:00:25,506 Epoch 231: Total Training Recognition Loss 0.22  Total Training Translation Loss 10.68 
2024-02-09 23:00:25,507 EPOCH 232
2024-02-09 23:00:41,854 Epoch 232: Total Training Recognition Loss 0.20  Total Training Translation Loss 10.44 
2024-02-09 23:00:41,854 EPOCH 233
2024-02-09 23:00:58,380 Epoch 233: Total Training Recognition Loss 0.22  Total Training Translation Loss 10.52 
2024-02-09 23:00:58,380 EPOCH 234
2024-02-09 23:01:02,087 [Epoch: 234 Step: 00002100] Batch Recognition Loss:   0.032524 => Gls Tokens per Sec:     1036 || Batch Translation Loss:   0.679814 => Txt Tokens per Sec:     2494 || Lr: 0.000100
2024-02-09 23:01:14,369 Epoch 234: Total Training Recognition Loss 0.21  Total Training Translation Loss 10.20 
2024-02-09 23:01:14,369 EPOCH 235
2024-02-09 23:01:30,376 Epoch 235: Total Training Recognition Loss 0.21  Total Training Translation Loss 10.19 
2024-02-09 23:01:30,377 EPOCH 236
2024-02-09 23:01:46,425 Epoch 236: Total Training Recognition Loss 0.21  Total Training Translation Loss 10.03 
2024-02-09 23:01:46,427 EPOCH 237
2024-02-09 23:02:02,576 Epoch 237: Total Training Recognition Loss 0.19  Total Training Translation Loss 9.78 
2024-02-09 23:02:02,577 EPOCH 238
2024-02-09 23:02:18,774 Epoch 238: Total Training Recognition Loss 0.20  Total Training Translation Loss 9.61 
2024-02-09 23:02:18,774 EPOCH 239
2024-02-09 23:02:34,744 Epoch 239: Total Training Recognition Loss 0.19  Total Training Translation Loss 9.67 
2024-02-09 23:02:34,745 EPOCH 240
2024-02-09 23:02:51,058 Epoch 240: Total Training Recognition Loss 0.18  Total Training Translation Loss 9.55 
2024-02-09 23:02:51,059 EPOCH 241
2024-02-09 23:03:07,076 Epoch 241: Total Training Recognition Loss 0.19  Total Training Translation Loss 9.27 
2024-02-09 23:03:07,077 EPOCH 242
2024-02-09 23:03:23,155 Epoch 242: Total Training Recognition Loss 0.18  Total Training Translation Loss 9.17 
2024-02-09 23:03:23,155 EPOCH 243
2024-02-09 23:03:39,473 Epoch 243: Total Training Recognition Loss 0.18  Total Training Translation Loss 9.16 
2024-02-09 23:03:39,473 EPOCH 244
2024-02-09 23:03:55,242 Epoch 244: Total Training Recognition Loss 0.18  Total Training Translation Loss 9.03 
2024-02-09 23:03:55,244 EPOCH 245
2024-02-09 23:04:05,405 [Epoch: 245 Step: 00002200] Batch Recognition Loss:   0.012449 => Gls Tokens per Sec:      504 || Batch Translation Loss:   1.040008 => Txt Tokens per Sec:     1550 || Lr: 0.000100
2024-02-09 23:04:11,580 Epoch 245: Total Training Recognition Loss 0.18  Total Training Translation Loss 8.65 
2024-02-09 23:04:11,580 EPOCH 246
2024-02-09 23:04:27,291 Epoch 246: Total Training Recognition Loss 0.17  Total Training Translation Loss 8.46 
2024-02-09 23:04:27,291 EPOCH 247
2024-02-09 23:04:43,417 Epoch 247: Total Training Recognition Loss 0.18  Total Training Translation Loss 8.45 
2024-02-09 23:04:43,418 EPOCH 248
2024-02-09 23:04:59,278 Epoch 248: Total Training Recognition Loss 0.19  Total Training Translation Loss 8.45 
2024-02-09 23:04:59,279 EPOCH 249
2024-02-09 23:05:15,311 Epoch 249: Total Training Recognition Loss 0.18  Total Training Translation Loss 8.35 
2024-02-09 23:05:15,311 EPOCH 250
2024-02-09 23:05:31,550 Epoch 250: Total Training Recognition Loss 0.19  Total Training Translation Loss 8.29 
2024-02-09 23:05:31,552 EPOCH 251
2024-02-09 23:05:47,646 Epoch 251: Total Training Recognition Loss 0.17  Total Training Translation Loss 8.06 
2024-02-09 23:05:47,646 EPOCH 252
2024-02-09 23:06:03,915 Epoch 252: Total Training Recognition Loss 0.16  Total Training Translation Loss 7.98 
2024-02-09 23:06:03,916 EPOCH 253
2024-02-09 23:06:20,397 Epoch 253: Total Training Recognition Loss 0.18  Total Training Translation Loss 7.88 
2024-02-09 23:06:20,398 EPOCH 254
2024-02-09 23:06:36,241 Epoch 254: Total Training Recognition Loss 0.16  Total Training Translation Loss 7.67 
2024-02-09 23:06:36,242 EPOCH 255
2024-02-09 23:06:52,135 Epoch 255: Total Training Recognition Loss 0.16  Total Training Translation Loss 7.73 
2024-02-09 23:06:52,136 EPOCH 256
2024-02-09 23:06:54,241 [Epoch: 256 Step: 00002300] Batch Recognition Loss:   0.012348 => Gls Tokens per Sec:     3043 || Batch Translation Loss:   0.915804 => Txt Tokens per Sec:     7438 || Lr: 0.000100
2024-02-09 23:07:08,210 Epoch 256: Total Training Recognition Loss 0.17  Total Training Translation Loss 7.67 
2024-02-09 23:07:08,211 EPOCH 257
2024-02-09 23:07:24,483 Epoch 257: Total Training Recognition Loss 0.17  Total Training Translation Loss 7.85 
2024-02-09 23:07:24,484 EPOCH 258
2024-02-09 23:07:40,510 Epoch 258: Total Training Recognition Loss 0.16  Total Training Translation Loss 7.33 
2024-02-09 23:07:40,511 EPOCH 259
2024-02-09 23:07:56,690 Epoch 259: Total Training Recognition Loss 0.17  Total Training Translation Loss 7.45 
2024-02-09 23:07:56,691 EPOCH 260
2024-02-09 23:08:12,820 Epoch 260: Total Training Recognition Loss 0.16  Total Training Translation Loss 7.27 
2024-02-09 23:08:12,822 EPOCH 261
2024-02-09 23:08:29,296 Epoch 261: Total Training Recognition Loss 0.15  Total Training Translation Loss 7.20 
2024-02-09 23:08:29,296 EPOCH 262
2024-02-09 23:08:45,380 Epoch 262: Total Training Recognition Loss 0.15  Total Training Translation Loss 6.99 
2024-02-09 23:08:45,380 EPOCH 263
2024-02-09 23:09:01,597 Epoch 263: Total Training Recognition Loss 0.16  Total Training Translation Loss 6.91 
2024-02-09 23:09:01,598 EPOCH 264
2024-02-09 23:09:17,753 Epoch 264: Total Training Recognition Loss 0.15  Total Training Translation Loss 6.68 
2024-02-09 23:09:17,754 EPOCH 265
2024-02-09 23:09:33,666 Epoch 265: Total Training Recognition Loss 0.16  Total Training Translation Loss 6.75 
2024-02-09 23:09:33,668 EPOCH 266
2024-02-09 23:09:49,828 Epoch 266: Total Training Recognition Loss 0.15  Total Training Translation Loss 6.54 
2024-02-09 23:09:49,828 EPOCH 267
2024-02-09 23:09:59,308 [Epoch: 267 Step: 00002400] Batch Recognition Loss:   0.015596 => Gls Tokens per Sec:      715 || Batch Translation Loss:   0.949863 => Txt Tokens per Sec:     1927 || Lr: 0.000100
2024-02-09 23:10:06,156 Epoch 267: Total Training Recognition Loss 0.14  Total Training Translation Loss 6.48 
2024-02-09 23:10:06,156 EPOCH 268
2024-02-09 23:10:22,338 Epoch 268: Total Training Recognition Loss 0.15  Total Training Translation Loss 6.50 
2024-02-09 23:10:22,339 EPOCH 269
2024-02-09 23:10:38,428 Epoch 269: Total Training Recognition Loss 0.16  Total Training Translation Loss 6.56 
2024-02-09 23:10:38,430 EPOCH 270
2024-02-09 23:10:54,468 Epoch 270: Total Training Recognition Loss 0.15  Total Training Translation Loss 6.31 
2024-02-09 23:10:54,469 EPOCH 271
2024-02-09 23:11:10,726 Epoch 271: Total Training Recognition Loss 0.15  Total Training Translation Loss 6.27 
2024-02-09 23:11:10,727 EPOCH 272
2024-02-09 23:11:26,955 Epoch 272: Total Training Recognition Loss 0.13  Total Training Translation Loss 6.24 
2024-02-09 23:11:26,957 EPOCH 273
2024-02-09 23:11:43,194 Epoch 273: Total Training Recognition Loss 0.14  Total Training Translation Loss 6.07 
2024-02-09 23:11:43,194 EPOCH 274
2024-02-09 23:11:59,017 Epoch 274: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.77 
2024-02-09 23:11:59,018 EPOCH 275
2024-02-09 23:12:15,843 Epoch 275: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.96 
2024-02-09 23:12:15,843 EPOCH 276
2024-02-09 23:12:31,896 Epoch 276: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.79 
2024-02-09 23:12:31,897 EPOCH 277
2024-02-09 23:12:47,833 Epoch 277: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.78 
2024-02-09 23:12:47,833 EPOCH 278
2024-02-09 23:12:57,531 [Epoch: 278 Step: 00002500] Batch Recognition Loss:   0.013395 => Gls Tokens per Sec:      831 || Batch Translation Loss:   0.762261 => Txt Tokens per Sec:     2239 || Lr: 0.000100
2024-02-09 23:13:03,912 Epoch 278: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.57 
2024-02-09 23:13:03,912 EPOCH 279
2024-02-09 23:13:20,278 Epoch 279: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.64 
2024-02-09 23:13:20,279 EPOCH 280
2024-02-09 23:13:36,320 Epoch 280: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.99 
2024-02-09 23:13:36,320 EPOCH 281
2024-02-09 23:13:52,316 Epoch 281: Total Training Recognition Loss 0.12  Total Training Translation Loss 5.89 
2024-02-09 23:13:52,316 EPOCH 282
2024-02-09 23:14:08,239 Epoch 282: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.59 
2024-02-09 23:14:08,240 EPOCH 283
2024-02-09 23:14:24,374 Epoch 283: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.41 
2024-02-09 23:14:24,375 EPOCH 284
2024-02-09 23:14:40,667 Epoch 284: Total Training Recognition Loss 0.13  Total Training Translation Loss 5.46 
2024-02-09 23:14:40,667 EPOCH 285
2024-02-09 23:14:56,835 Epoch 285: Total Training Recognition Loss 0.14  Total Training Translation Loss 5.40 
2024-02-09 23:14:56,836 EPOCH 286
2024-02-09 23:15:12,974 Epoch 286: Total Training Recognition Loss 0.11  Total Training Translation Loss 5.14 
2024-02-09 23:15:12,974 EPOCH 287
2024-02-09 23:15:29,032 Epoch 287: Total Training Recognition Loss 0.12  Total Training Translation Loss 5.59 
2024-02-09 23:15:29,033 EPOCH 288
2024-02-09 23:15:44,852 Epoch 288: Total Training Recognition Loss 0.12  Total Training Translation Loss 5.67 
2024-02-09 23:15:44,853 EPOCH 289
2024-02-09 23:16:00,971 [Epoch: 289 Step: 00002600] Batch Recognition Loss:   0.025227 => Gls Tokens per Sec:      580 || Batch Translation Loss:   0.381376 => Txt Tokens per Sec:     1619 || Lr: 0.000100
2024-02-09 23:16:01,386 Epoch 289: Total Training Recognition Loss 0.12  Total Training Translation Loss 5.50 
2024-02-09 23:16:01,386 EPOCH 290
2024-02-09 23:16:17,463 Epoch 290: Total Training Recognition Loss 0.11  Total Training Translation Loss 6.10 
2024-02-09 23:16:17,463 EPOCH 291
2024-02-09 23:16:33,669 Epoch 291: Total Training Recognition Loss 0.12  Total Training Translation Loss 6.79 
2024-02-09 23:16:33,671 EPOCH 292
2024-02-09 23:16:50,218 Epoch 292: Total Training Recognition Loss 0.16  Total Training Translation Loss 17.16 
2024-02-09 23:16:50,219 EPOCH 293
2024-02-09 23:17:06,739 Epoch 293: Total Training Recognition Loss 0.43  Total Training Translation Loss 50.39 
2024-02-09 23:17:06,739 EPOCH 294
2024-02-09 23:17:22,835 Epoch 294: Total Training Recognition Loss 0.37  Total Training Translation Loss 40.32 
2024-02-09 23:17:22,836 EPOCH 295
2024-02-09 23:17:38,789 Epoch 295: Total Training Recognition Loss 0.42  Total Training Translation Loss 28.52 
2024-02-09 23:17:38,789 EPOCH 296
2024-02-09 23:17:55,152 Epoch 296: Total Training Recognition Loss 0.36  Total Training Translation Loss 18.00 
2024-02-09 23:17:55,153 EPOCH 297
2024-02-09 23:18:11,196 Epoch 297: Total Training Recognition Loss 0.40  Total Training Translation Loss 13.42 
2024-02-09 23:18:11,198 EPOCH 298
2024-02-09 23:18:27,729 Epoch 298: Total Training Recognition Loss 0.32  Total Training Translation Loss 10.50 
2024-02-09 23:18:27,730 EPOCH 299
2024-02-09 23:18:43,720 Epoch 299: Total Training Recognition Loss 0.25  Total Training Translation Loss 8.87 
2024-02-09 23:18:43,721 EPOCH 300
2024-02-09 23:18:59,834 [Epoch: 300 Step: 00002700] Batch Recognition Loss:   0.033360 => Gls Tokens per Sec:      659 || Batch Translation Loss:   1.025169 => Txt Tokens per Sec:     1824 || Lr: 0.000100
2024-02-09 23:18:59,835 Epoch 300: Total Training Recognition Loss 0.19  Total Training Translation Loss 7.26 
2024-02-09 23:18:59,836 EPOCH 301
2024-02-09 23:19:16,562 Epoch 301: Total Training Recognition Loss 0.16  Total Training Translation Loss 6.71 
2024-02-09 23:19:16,563 EPOCH 302
2024-02-09 23:19:32,776 Epoch 302: Total Training Recognition Loss 0.16  Total Training Translation Loss 5.90 
2024-02-09 23:19:32,777 EPOCH 303
2024-02-09 23:19:48,762 Epoch 303: Total Training Recognition Loss 0.17  Total Training Translation Loss 5.51 
2024-02-09 23:19:48,763 EPOCH 304
2024-02-09 23:20:04,743 Epoch 304: Total Training Recognition Loss 0.15  Total Training Translation Loss 5.23 
2024-02-09 23:20:04,744 EPOCH 305
2024-02-09 23:20:20,965 Epoch 305: Total Training Recognition Loss 0.16  Total Training Translation Loss 4.88 
2024-02-09 23:20:20,966 EPOCH 306
2024-02-09 23:20:37,119 Epoch 306: Total Training Recognition Loss 0.14  Total Training Translation Loss 4.78 
2024-02-09 23:20:37,119 EPOCH 307
2024-02-09 23:20:53,412 Epoch 307: Total Training Recognition Loss 0.13  Total Training Translation Loss 4.63 
2024-02-09 23:20:53,413 EPOCH 308
2024-02-09 23:21:09,651 Epoch 308: Total Training Recognition Loss 0.14  Total Training Translation Loss 4.44 
2024-02-09 23:21:09,652 EPOCH 309
2024-02-09 23:21:25,814 Epoch 309: Total Training Recognition Loss 0.13  Total Training Translation Loss 4.34 
2024-02-09 23:21:25,814 EPOCH 310
2024-02-09 23:21:42,073 Epoch 310: Total Training Recognition Loss 0.12  Total Training Translation Loss 4.27 
2024-02-09 23:21:42,073 EPOCH 311
2024-02-09 23:21:58,445 Epoch 311: Total Training Recognition Loss 0.13  Total Training Translation Loss 4.23 
2024-02-09 23:21:58,447 EPOCH 312
2024-02-09 23:21:58,763 [Epoch: 312 Step: 00002800] Batch Recognition Loss:   0.008578 => Gls Tokens per Sec:     4064 || Batch Translation Loss:   0.439157 => Txt Tokens per Sec:    10375 || Lr: 0.000100
2024-02-09 23:22:14,515 Epoch 312: Total Training Recognition Loss 0.14  Total Training Translation Loss 4.10 
2024-02-09 23:22:14,515 EPOCH 313
2024-02-09 23:22:30,700 Epoch 313: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.98 
2024-02-09 23:22:30,701 EPOCH 314
2024-02-09 23:22:46,598 Epoch 314: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.92 
2024-02-09 23:22:46,598 EPOCH 315
2024-02-09 23:23:02,782 Epoch 315: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.87 
2024-02-09 23:23:02,783 EPOCH 316
2024-02-09 23:23:18,810 Epoch 316: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.82 
2024-02-09 23:23:18,811 EPOCH 317
2024-02-09 23:23:35,023 Epoch 317: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.91 
2024-02-09 23:23:35,023 EPOCH 318
2024-02-09 23:23:51,102 Epoch 318: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.64 
2024-02-09 23:23:51,103 EPOCH 319
2024-02-09 23:24:07,570 Epoch 319: Total Training Recognition Loss 0.13  Total Training Translation Loss 3.65 
2024-02-09 23:24:07,570 EPOCH 320
2024-02-09 23:24:23,553 Epoch 320: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.67 
2024-02-09 23:24:23,554 EPOCH 321
2024-02-09 23:24:39,481 Epoch 321: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.54 
2024-02-09 23:24:39,481 EPOCH 322
2024-02-09 23:24:55,889 Epoch 322: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.44 
2024-02-09 23:24:55,890 EPOCH 323
2024-02-09 23:24:56,789 [Epoch: 323 Step: 00002900] Batch Recognition Loss:   0.014959 => Gls Tokens per Sec:     2848 || Batch Translation Loss:   0.252264 => Txt Tokens per Sec:     6877 || Lr: 0.000100
2024-02-09 23:25:11,744 Epoch 323: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.38 
2024-02-09 23:25:11,745 EPOCH 324
2024-02-09 23:25:27,904 Epoch 324: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.38 
2024-02-09 23:25:27,905 EPOCH 325
2024-02-09 23:25:44,139 Epoch 325: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.39 
2024-02-09 23:25:44,140 EPOCH 326
2024-02-09 23:26:00,220 Epoch 326: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.29 
2024-02-09 23:26:00,221 EPOCH 327
2024-02-09 23:26:16,471 Epoch 327: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.32 
2024-02-09 23:26:16,472 EPOCH 328
2024-02-09 23:26:32,504 Epoch 328: Total Training Recognition Loss 0.11  Total Training Translation Loss 3.17 
2024-02-09 23:26:32,504 EPOCH 329
2024-02-09 23:26:48,727 Epoch 329: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.11 
2024-02-09 23:26:48,727 EPOCH 330
2024-02-09 23:27:04,806 Epoch 330: Total Training Recognition Loss 0.10  Total Training Translation Loss 3.07 
2024-02-09 23:27:04,807 EPOCH 331
2024-02-09 23:27:21,349 Epoch 331: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.99 
2024-02-09 23:27:21,349 EPOCH 332
2024-02-09 23:27:37,402 Epoch 332: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.06 
2024-02-09 23:27:37,403 EPOCH 333
2024-02-09 23:27:53,731 Epoch 333: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.07 
2024-02-09 23:27:53,733 EPOCH 334
2024-02-09 23:27:55,188 [Epoch: 334 Step: 00003000] Batch Recognition Loss:   0.006111 => Gls Tokens per Sec:     2642 || Batch Translation Loss:   0.387442 => Txt Tokens per Sec:     6879 || Lr: 0.000100
2024-02-09 23:28:09,856 Epoch 334: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.06 
2024-02-09 23:28:09,856 EPOCH 335
2024-02-09 23:28:25,819 Epoch 335: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.99 
2024-02-09 23:28:25,820 EPOCH 336
2024-02-09 23:28:42,106 Epoch 336: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.94 
2024-02-09 23:28:42,106 EPOCH 337
2024-02-09 23:28:58,178 Epoch 337: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.94 
2024-02-09 23:28:58,178 EPOCH 338
2024-02-09 23:29:14,179 Epoch 338: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.94 
2024-02-09 23:29:14,179 EPOCH 339
2024-02-09 23:29:30,633 Epoch 339: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.99 
2024-02-09 23:29:30,635 EPOCH 340
2024-02-09 23:29:46,854 Epoch 340: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.86 
2024-02-09 23:29:46,855 EPOCH 341
2024-02-09 23:30:03,202 Epoch 341: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.80 
2024-02-09 23:30:03,203 EPOCH 342
2024-02-09 23:30:19,602 Epoch 342: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.87 
2024-02-09 23:30:19,603 EPOCH 343
2024-02-09 23:30:36,072 Epoch 343: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.82 
2024-02-09 23:30:36,072 EPOCH 344
2024-02-09 23:30:52,201 Epoch 344: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.73 
2024-02-09 23:30:52,203 EPOCH 345
2024-02-09 23:31:03,044 [Epoch: 345 Step: 00003100] Batch Recognition Loss:   0.019042 => Gls Tokens per Sec:      389 || Batch Translation Loss:   0.397592 => Txt Tokens per Sec:     1070 || Lr: 0.000100
2024-02-09 23:31:08,601 Epoch 345: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.66 
2024-02-09 23:31:08,602 EPOCH 346
2024-02-09 23:31:24,951 Epoch 346: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.69 
2024-02-09 23:31:24,951 EPOCH 347
2024-02-09 23:31:41,339 Epoch 347: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.62 
2024-02-09 23:31:41,340 EPOCH 348
2024-02-09 23:31:57,720 Epoch 348: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.56 
2024-02-09 23:31:57,721 EPOCH 349
2024-02-09 23:32:14,172 Epoch 349: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.58 
2024-02-09 23:32:14,174 EPOCH 350
2024-02-09 23:32:30,417 Epoch 350: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.57 
2024-02-09 23:32:30,417 EPOCH 351
2024-02-09 23:32:46,489 Epoch 351: Total Training Recognition Loss 0.09  Total Training Translation Loss 2.48 
2024-02-09 23:32:46,490 EPOCH 352
2024-02-09 23:33:02,470 Epoch 352: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.55 
2024-02-09 23:33:02,471 EPOCH 353
2024-02-09 23:33:18,592 Epoch 353: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.51 
2024-02-09 23:33:18,593 EPOCH 354
2024-02-09 23:33:34,753 Epoch 354: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.42 
2024-02-09 23:33:34,753 EPOCH 355
2024-02-09 23:33:50,660 Epoch 355: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.57 
2024-02-09 23:33:50,661 EPOCH 356
2024-02-09 23:33:58,357 [Epoch: 356 Step: 00003200] Batch Recognition Loss:   0.003808 => Gls Tokens per Sec:      832 || Batch Translation Loss:   0.246455 => Txt Tokens per Sec:     2277 || Lr: 0.000100
2024-02-09 23:34:06,619 Epoch 356: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.41 
2024-02-09 23:34:06,619 EPOCH 357
2024-02-09 23:34:22,917 Epoch 357: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.38 
2024-02-09 23:34:22,919 EPOCH 358
2024-02-09 23:34:39,210 Epoch 358: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.31 
2024-02-09 23:34:39,211 EPOCH 359
2024-02-09 23:34:55,006 Epoch 359: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.41 
2024-02-09 23:34:55,007 EPOCH 360
2024-02-09 23:35:11,304 Epoch 360: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.37 
2024-02-09 23:35:11,305 EPOCH 361
2024-02-09 23:35:27,220 Epoch 361: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.40 
2024-02-09 23:35:27,220 EPOCH 362
2024-02-09 23:35:43,512 Epoch 362: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.36 
2024-02-09 23:35:43,513 EPOCH 363
2024-02-09 23:35:59,702 Epoch 363: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.29 
2024-02-09 23:35:59,702 EPOCH 364
2024-02-09 23:36:16,089 Epoch 364: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.29 
2024-02-09 23:36:16,090 EPOCH 365
2024-02-09 23:36:32,026 Epoch 365: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.30 
2024-02-09 23:36:32,026 EPOCH 366
2024-02-09 23:36:48,001 Epoch 366: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.26 
2024-02-09 23:36:48,003 EPOCH 367
2024-02-09 23:37:02,810 [Epoch: 367 Step: 00003300] Batch Recognition Loss:   0.014008 => Gls Tokens per Sec:      458 || Batch Translation Loss:   0.360855 => Txt Tokens per Sec:     1307 || Lr: 0.000100
2024-02-09 23:37:04,462 Epoch 367: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.30 
2024-02-09 23:37:04,462 EPOCH 368
2024-02-09 23:37:20,837 Epoch 368: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.21 
2024-02-09 23:37:20,838 EPOCH 369
2024-02-09 23:37:37,114 Epoch 369: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.22 
2024-02-09 23:37:37,114 EPOCH 370
2024-02-09 23:37:53,514 Epoch 370: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.16 
2024-02-09 23:37:53,514 EPOCH 371
2024-02-09 23:38:09,451 Epoch 371: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.23 
2024-02-09 23:38:09,451 EPOCH 372
2024-02-09 23:38:25,710 Epoch 372: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.10 
2024-02-09 23:38:25,711 EPOCH 373
2024-02-09 23:38:41,621 Epoch 373: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.09 
2024-02-09 23:38:41,622 EPOCH 374
2024-02-09 23:38:57,528 Epoch 374: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.05 
2024-02-09 23:38:57,529 EPOCH 375
2024-02-09 23:39:13,610 Epoch 375: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.08 
2024-02-09 23:39:13,611 EPOCH 376
2024-02-09 23:39:29,849 Epoch 376: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.98 
2024-02-09 23:39:29,849 EPOCH 377
2024-02-09 23:39:45,813 Epoch 377: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.10 
2024-02-09 23:39:45,813 EPOCH 378
2024-02-09 23:40:01,061 [Epoch: 378 Step: 00003400] Batch Recognition Loss:   0.003452 => Gls Tokens per Sec:      529 || Batch Translation Loss:   0.269743 => Txt Tokens per Sec:     1458 || Lr: 0.000100
2024-02-09 23:40:02,257 Epoch 378: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.16 
2024-02-09 23:40:02,257 EPOCH 379
2024-02-09 23:40:18,196 Epoch 379: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.28 
2024-02-09 23:40:18,198 EPOCH 380
2024-02-09 23:40:34,749 Epoch 380: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.35 
2024-02-09 23:40:34,750 EPOCH 381
2024-02-09 23:40:51,077 Epoch 381: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.48 
2024-02-09 23:40:51,079 EPOCH 382
2024-02-09 23:41:07,557 Epoch 382: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.32 
2024-02-09 23:41:07,559 EPOCH 383
2024-02-09 23:41:23,732 Epoch 383: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.23 
2024-02-09 23:41:23,733 EPOCH 384
2024-02-09 23:41:39,914 Epoch 384: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.20 
2024-02-09 23:41:39,915 EPOCH 385
2024-02-09 23:41:56,344 Epoch 385: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.04 
2024-02-09 23:41:56,345 EPOCH 386
2024-02-09 23:42:12,729 Epoch 386: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.05 
2024-02-09 23:42:12,729 EPOCH 387
2024-02-09 23:42:28,976 Epoch 387: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.18 
2024-02-09 23:42:28,978 EPOCH 388
2024-02-09 23:42:45,247 Epoch 388: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.14 
2024-02-09 23:42:45,248 EPOCH 389
2024-02-09 23:43:01,082 [Epoch: 389 Step: 00003500] Batch Recognition Loss:   0.005307 => Gls Tokens per Sec:      590 || Batch Translation Loss:   0.206062 => Txt Tokens per Sec:     1711 || Lr: 0.000100
2024-02-09 23:43:01,331 Epoch 389: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.98 
2024-02-09 23:43:01,331 EPOCH 390
2024-02-09 23:43:16,826 Epoch 390: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.06 
2024-02-09 23:43:16,826 EPOCH 391
2024-02-09 23:43:32,707 Epoch 391: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.10 
2024-02-09 23:43:32,708 EPOCH 392
2024-02-09 23:43:49,117 Epoch 392: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.93 
2024-02-09 23:43:49,119 EPOCH 393
2024-02-09 23:44:05,383 Epoch 393: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.94 
2024-02-09 23:44:05,384 EPOCH 394
2024-02-09 23:44:21,694 Epoch 394: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.94 
2024-02-09 23:44:21,694 EPOCH 395
2024-02-09 23:44:37,657 Epoch 395: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.80 
2024-02-09 23:44:37,657 EPOCH 396
2024-02-09 23:44:54,035 Epoch 396: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.74 
2024-02-09 23:44:54,037 EPOCH 397
2024-02-09 23:45:10,449 Epoch 397: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.86 
2024-02-09 23:45:10,451 EPOCH 398
2024-02-09 23:45:26,577 Epoch 398: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.88 
2024-02-09 23:45:26,577 EPOCH 399
2024-02-09 23:45:42,910 Epoch 399: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.71 
2024-02-09 23:45:42,912 EPOCH 400
2024-02-09 23:45:59,380 [Epoch: 400 Step: 00003600] Batch Recognition Loss:   0.006489 => Gls Tokens per Sec:      645 || Batch Translation Loss:   0.242859 => Txt Tokens per Sec:     1784 || Lr: 0.000100
2024-02-09 23:45:59,381 Epoch 400: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.72 
2024-02-09 23:45:59,381 EPOCH 401
2024-02-09 23:46:15,105 Epoch 401: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.72 
2024-02-09 23:46:15,106 EPOCH 402
2024-02-09 23:46:31,233 Epoch 402: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.80 
2024-02-09 23:46:31,234 EPOCH 403
2024-02-09 23:46:47,247 Epoch 403: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.79 
2024-02-09 23:46:47,248 EPOCH 404
2024-02-09 23:47:03,633 Epoch 404: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.79 
2024-02-09 23:47:03,634 EPOCH 405
2024-02-09 23:47:20,017 Epoch 405: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.81 
2024-02-09 23:47:20,018 EPOCH 406
2024-02-09 23:47:36,194 Epoch 406: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.86 
2024-02-09 23:47:36,195 EPOCH 407
2024-02-09 23:47:52,113 Epoch 407: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.84 
2024-02-09 23:47:52,114 EPOCH 408
2024-02-09 23:48:08,376 Epoch 408: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.01 
2024-02-09 23:48:08,377 EPOCH 409
2024-02-09 23:48:24,301 Epoch 409: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.50 
2024-02-09 23:48:24,302 EPOCH 410
2024-02-09 23:48:40,770 Epoch 410: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.61 
2024-02-09 23:48:40,771 EPOCH 411
2024-02-09 23:48:56,967 Epoch 411: Total Training Recognition Loss 0.06  Total Training Translation Loss 3.71 
2024-02-09 23:48:56,968 EPOCH 412
2024-02-09 23:49:02,675 [Epoch: 412 Step: 00003700] Batch Recognition Loss:   0.009205 => Gls Tokens per Sec:      224 || Batch Translation Loss:   1.825756 => Txt Tokens per Sec:      772 || Lr: 0.000100
2024-02-09 23:49:13,057 Epoch 412: Total Training Recognition Loss 0.07  Total Training Translation Loss 7.43 
2024-02-09 23:49:13,057 EPOCH 413
2024-02-09 23:49:29,216 Epoch 413: Total Training Recognition Loss 0.07  Total Training Translation Loss 4.61 
2024-02-09 23:49:29,218 EPOCH 414
2024-02-09 23:49:45,673 Epoch 414: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.99 
2024-02-09 23:49:45,674 EPOCH 415
2024-02-09 23:50:01,573 Epoch 415: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.31 
2024-02-09 23:50:01,574 EPOCH 416
2024-02-09 23:50:17,911 Epoch 416: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.70 
2024-02-09 23:50:17,912 EPOCH 417
2024-02-09 23:50:34,305 Epoch 417: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.62 
2024-02-09 23:50:34,306 EPOCH 418
2024-02-09 23:50:50,477 Epoch 418: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.78 
2024-02-09 23:50:50,478 EPOCH 419
2024-02-09 23:51:06,756 Epoch 419: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.77 
2024-02-09 23:51:06,757 EPOCH 420
2024-02-09 23:51:22,862 Epoch 420: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.73 
2024-02-09 23:51:22,863 EPOCH 421
2024-02-09 23:51:39,366 Epoch 421: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.28 
2024-02-09 23:51:39,367 EPOCH 422
2024-02-09 23:51:55,676 Epoch 422: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.97 
2024-02-09 23:51:55,678 EPOCH 423
2024-02-09 23:51:56,662 [Epoch: 423 Step: 00003800] Batch Recognition Loss:   0.004883 => Gls Tokens per Sec:     2607 || Batch Translation Loss:   0.235060 => Txt Tokens per Sec:     6836 || Lr: 0.000100
2024-02-09 23:52:11,815 Epoch 423: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.58 
2024-02-09 23:52:11,816 EPOCH 424
2024-02-09 23:52:27,874 Epoch 424: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.50 
2024-02-09 23:52:27,874 EPOCH 425
2024-02-09 23:52:43,512 Epoch 425: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.21 
2024-02-09 23:52:43,513 EPOCH 426
2024-02-09 23:52:59,391 Epoch 426: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.39 
2024-02-09 23:52:59,392 EPOCH 427
2024-02-09 23:53:15,416 Epoch 427: Total Training Recognition Loss 0.07  Total Training Translation Loss 5.33 
2024-02-09 23:53:15,417 EPOCH 428
2024-02-09 23:53:31,703 Epoch 428: Total Training Recognition Loss 0.10  Total Training Translation Loss 5.02 
2024-02-09 23:53:31,705 EPOCH 429
2024-02-09 23:53:47,741 Epoch 429: Total Training Recognition Loss 0.08  Total Training Translation Loss 3.90 
2024-02-09 23:53:47,742 EPOCH 430
2024-02-09 23:54:04,079 Epoch 430: Total Training Recognition Loss 0.09  Total Training Translation Loss 3.22 
2024-02-09 23:54:04,080 EPOCH 431
2024-02-09 23:54:20,253 Epoch 431: Total Training Recognition Loss 0.07  Total Training Translation Loss 2.52 
2024-02-09 23:54:20,254 EPOCH 432
2024-02-09 23:54:36,387 Epoch 432: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.21 
2024-02-09 23:54:36,389 EPOCH 433
2024-02-09 23:54:52,912 Epoch 433: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.88 
2024-02-09 23:54:52,914 EPOCH 434
2024-02-09 23:54:57,096 [Epoch: 434 Step: 00003900] Batch Recognition Loss:   0.003482 => Gls Tokens per Sec:      918 || Batch Translation Loss:   0.195440 => Txt Tokens per Sec:     2606 || Lr: 0.000100
2024-02-09 23:55:09,453 Epoch 434: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.68 
2024-02-09 23:55:09,453 EPOCH 435
2024-02-09 23:55:25,463 Epoch 435: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.57 
2024-02-09 23:55:25,463 EPOCH 436
2024-02-09 23:55:41,533 Epoch 436: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.59 
2024-02-09 23:55:41,535 EPOCH 437
2024-02-09 23:55:57,667 Epoch 437: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.44 
2024-02-09 23:55:57,667 EPOCH 438
2024-02-09 23:56:13,692 Epoch 438: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.40 
2024-02-09 23:56:13,693 EPOCH 439
2024-02-09 23:56:29,904 Epoch 439: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.35 
2024-02-09 23:56:29,905 EPOCH 440
2024-02-09 23:56:46,315 Epoch 440: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.32 
2024-02-09 23:56:46,316 EPOCH 441
2024-02-09 23:57:02,155 Epoch 441: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.28 
2024-02-09 23:57:02,156 EPOCH 442
2024-02-09 23:57:18,188 Epoch 442: Total Training Recognition Loss 0.06  Total Training Translation Loss 1.23 
2024-02-09 23:57:18,190 EPOCH 443
2024-02-09 23:57:34,525 Epoch 443: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.23 
2024-02-09 23:57:34,526 EPOCH 444
2024-02-09 23:57:50,704 Epoch 444: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.26 
2024-02-09 23:57:50,704 EPOCH 445
2024-02-09 23:57:57,331 [Epoch: 445 Step: 00004000] Batch Recognition Loss:   0.005814 => Gls Tokens per Sec:      773 || Batch Translation Loss:   0.187525 => Txt Tokens per Sec:     1937 || Lr: 0.000100
2024-02-09 23:59:09,432 Validation result at epoch 445, step     4000: duration: 72.1008s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.54551	Translation Loss: 83530.37500	PPL: 4200.77881
	Eval Metric: BLEU
	WER 6.64	(DEL: 0.00,	INS: 0.00,	SUB: 6.64)
	BLEU-4 0.44	(BLEU-1: 12.55,	BLEU-2: 3.99,	BLEU-3: 1.38,	BLEU-4: 0.44)
	CHRF 17.88	ROUGE 10.19
2024-02-09 23:59:09,433 Logging Recognition and Translation Outputs
2024-02-09 23:59:09,434 ========================================================================================================================
2024-02-09 23:59:09,434 Logging Sequence: 112_165.00
2024-02-09 23:59:09,434 	Gloss Reference :	A B+C+D+E
2024-02-09 23:59:09,434 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 23:59:09,434 	Gloss Alignment :	         
2024-02-09 23:59:09,434 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 23:59:09,436 	Text Reference  :	**** ***** ******** the **** narendra modi stadium will  be    the  home  for the     ahmedabad-based franchise
2024-02-09 23:59:09,436 	Text Hypothesis :	this group includes the same as       the  delhi   final match semi final and weekend double          headers  
2024-02-09 23:59:09,436 	Text Alignment  :	I    I     I            I    S        S    S       S     S     S    S     S   S       S               S        
2024-02-09 23:59:09,436 ========================================================================================================================
2024-02-09 23:59:09,436 Logging Sequence: 176_154.00
2024-02-09 23:59:09,436 	Gloss Reference :	A B+C+D+E
2024-02-09 23:59:09,436 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 23:59:09,437 	Gloss Alignment :	         
2024-02-09 23:59:09,437 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 23:59:09,437 	Text Reference  :	***** dahiya could  potentially bring home india's second gold medal 
2024-02-09 23:59:09,438 	Text Hypothesis :	sadly the    indian team        lost  to   qualify for    the  finals
2024-02-09 23:59:09,438 	Text Alignment  :	I     S      S      S           S     S    S       S      S    S     
2024-02-09 23:59:09,438 ========================================================================================================================
2024-02-09 23:59:09,438 Logging Sequence: 94_2.00
2024-02-09 23:59:09,438 	Gloss Reference :	A B+C+D+E
2024-02-09 23:59:09,438 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 23:59:09,438 	Gloss Alignment :	         
2024-02-09 23:59:09,439 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 23:59:09,440 	Text Reference  :	***** ** *** ** the icc odi    men'    world cup 2023 will be hosted by india on    5th october 2023 
2024-02-09 23:59:09,440 	Text Hypothesis :	india is now at the *** women' cricket world cup **** **** ** ****** ** 6     balls two tough   balls
2024-02-09 23:59:09,440 	Text Alignment  :	I     I  I   I      D   S      S                 D    D    D  D      D  S     S     S   S       S    
2024-02-09 23:59:09,440 ========================================================================================================================
2024-02-09 23:59:09,440 Logging Sequence: 165_453.00
2024-02-09 23:59:09,441 	Gloss Reference :	A B+C+D+E
2024-02-09 23:59:09,441 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 23:59:09,441 	Gloss Alignment :	         
2024-02-09 23:59:09,441 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 23:59:09,442 	Text Reference  :	*** **** icc did not **** *** agree to sehwag' decision of  wearing a    numberless jersey
2024-02-09 23:59:09,442 	Text Hypothesis :	sai said he  did not know who is    a  strong  but      did not     find the        umpire
2024-02-09 23:59:09,442 	Text Alignment  :	I   I    S           I    I   S     S  S       S        S   S       S    S          S     
2024-02-09 23:59:09,443 ========================================================================================================================
2024-02-09 23:59:09,443 Logging Sequence: 139_46.00
2024-02-09 23:59:09,443 	Gloss Reference :	A B+C+D+E
2024-02-09 23:59:09,443 	Gloss Hypothesis:	A B+C+D+E
2024-02-09 23:59:09,443 	Gloss Alignment :	         
2024-02-09 23:59:09,443 	--------------------------------------------------------------------------------------------------------------------
2024-02-09 23:59:09,445 	Text Reference  :	everyone thought it would be a         one    sided match   because    morocco is       an amateur team    and belgium ranks 2nd in the        world
2024-02-09 23:59:09,445 	Text Hypothesis :	******** ******* ** ***** ** moroccans living in    belgium celebrated by      resorted to arson   rioting and ******* ***** *** ** destroying cars 
2024-02-09 23:59:09,445 	Text Alignment  :	D        D       D  D     D  S         S      S     S       S          S       S        S  S       S           D       D     D   D  S          S    
2024-02-09 23:59:09,446 ========================================================================================================================
2024-02-09 23:59:19,486 Epoch 445: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.39 
2024-02-09 23:59:19,487 EPOCH 446
2024-02-09 23:59:35,791 Epoch 446: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.31 
2024-02-09 23:59:35,791 EPOCH 447
2024-02-09 23:59:51,913 Epoch 447: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.33 
2024-02-09 23:59:51,914 EPOCH 448
2024-02-10 00:00:07,928 Epoch 448: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.22 
2024-02-10 00:00:07,928 EPOCH 449
2024-02-10 00:00:24,151 Epoch 449: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.14 
2024-02-10 00:00:24,151 EPOCH 450
2024-02-10 00:00:40,370 Epoch 450: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.14 
2024-02-10 00:00:40,371 EPOCH 451
2024-02-10 00:00:56,750 Epoch 451: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.10 
2024-02-10 00:00:56,751 EPOCH 452
2024-02-10 00:01:12,626 Epoch 452: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.07 
2024-02-10 00:01:12,627 EPOCH 453
2024-02-10 00:01:29,117 Epoch 453: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.06 
2024-02-10 00:01:29,118 EPOCH 454
2024-02-10 00:01:45,141 Epoch 454: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.03 
2024-02-10 00:01:45,141 EPOCH 455
2024-02-10 00:02:01,107 Epoch 455: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.05 
2024-02-10 00:02:01,107 EPOCH 456
2024-02-10 00:02:11,986 [Epoch: 456 Step: 00004100] Batch Recognition Loss:   0.004497 => Gls Tokens per Sec:      588 || Batch Translation Loss:   0.150436 => Txt Tokens per Sec:     1801 || Lr: 0.000100
2024-02-10 00:02:17,397 Epoch 456: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.08 
2024-02-10 00:02:17,397 EPOCH 457
2024-02-10 00:02:33,448 Epoch 457: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.07 
2024-02-10 00:02:33,448 EPOCH 458
2024-02-10 00:02:49,402 Epoch 458: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.04 
2024-02-10 00:02:49,402 EPOCH 459
2024-02-10 00:03:05,733 Epoch 459: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.02 
2024-02-10 00:03:05,734 EPOCH 460
2024-02-10 00:03:21,839 Epoch 460: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.99 
2024-02-10 00:03:21,840 EPOCH 461
2024-02-10 00:03:37,733 Epoch 461: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.07 
2024-02-10 00:03:37,734 EPOCH 462
2024-02-10 00:03:53,895 Epoch 462: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.00 
2024-02-10 00:03:53,895 EPOCH 463
2024-02-10 00:04:09,848 Epoch 463: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.97 
2024-02-10 00:04:09,848 EPOCH 464
2024-02-10 00:04:26,113 Epoch 464: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.97 
2024-02-10 00:04:26,113 EPOCH 465
2024-02-10 00:04:42,357 Epoch 465: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.00 
2024-02-10 00:04:42,357 EPOCH 466
2024-02-10 00:04:58,425 Epoch 466: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.98 
2024-02-10 00:04:58,425 EPOCH 467
2024-02-10 00:05:09,458 [Epoch: 467 Step: 00004200] Batch Recognition Loss:   0.003798 => Gls Tokens per Sec:      696 || Batch Translation Loss:   0.132766 => Txt Tokens per Sec:     2016 || Lr: 0.000100
2024-02-10 00:05:14,395 Epoch 467: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.95 
2024-02-10 00:05:14,395 EPOCH 468
2024-02-10 00:05:30,848 Epoch 468: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.94 
2024-02-10 00:05:30,849 EPOCH 469
2024-02-10 00:05:46,791 Epoch 469: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.98 
2024-02-10 00:05:46,792 EPOCH 470
2024-02-10 00:06:03,000 Epoch 470: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.93 
2024-02-10 00:06:03,001 EPOCH 471
2024-02-10 00:06:19,143 Epoch 471: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.92 
2024-02-10 00:06:19,143 EPOCH 472
2024-02-10 00:06:35,449 Epoch 472: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.92 
2024-02-10 00:06:35,450 EPOCH 473
2024-02-10 00:06:51,198 Epoch 473: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.96 
2024-02-10 00:06:51,199 EPOCH 474
2024-02-10 00:07:07,739 Epoch 474: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.95 
2024-02-10 00:07:07,739 EPOCH 475
2024-02-10 00:07:23,896 Epoch 475: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.99 
2024-02-10 00:07:23,896 EPOCH 476
2024-02-10 00:07:39,913 Epoch 476: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.95 
2024-02-10 00:07:39,913 EPOCH 477
2024-02-10 00:07:55,802 Epoch 477: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.09 
2024-02-10 00:07:55,803 EPOCH 478
2024-02-10 00:08:07,444 [Epoch: 478 Step: 00004300] Batch Recognition Loss:   0.005186 => Gls Tokens per Sec:      770 || Batch Translation Loss:   0.089509 => Txt Tokens per Sec:     2209 || Lr: 0.000100
2024-02-10 00:08:12,261 Epoch 478: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.94 
2024-02-10 00:08:12,261 EPOCH 479
2024-02-10 00:08:28,527 Epoch 479: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.98 
2024-02-10 00:08:28,528 EPOCH 480
2024-02-10 00:08:44,645 Epoch 480: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.01 
2024-02-10 00:08:44,646 EPOCH 481
2024-02-10 00:09:00,691 Epoch 481: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.04 
2024-02-10 00:09:00,691 EPOCH 482
2024-02-10 00:09:16,642 Epoch 482: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.01 
2024-02-10 00:09:16,643 EPOCH 483
2024-02-10 00:09:32,695 Epoch 483: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.98 
2024-02-10 00:09:32,695 EPOCH 484
2024-02-10 00:09:48,677 Epoch 484: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.98 
2024-02-10 00:09:48,678 EPOCH 485
2024-02-10 00:10:04,907 Epoch 485: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.98 
2024-02-10 00:10:04,907 EPOCH 486
2024-02-10 00:10:21,125 Epoch 486: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.99 
2024-02-10 00:10:21,126 EPOCH 487
2024-02-10 00:10:37,189 Epoch 487: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.02 
2024-02-10 00:10:37,189 EPOCH 488
2024-02-10 00:10:53,182 Epoch 488: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.17 
2024-02-10 00:10:53,182 EPOCH 489
2024-02-10 00:11:08,944 [Epoch: 489 Step: 00004400] Batch Recognition Loss:   0.001340 => Gls Tokens per Sec:      593 || Batch Translation Loss:   0.153856 => Txt Tokens per Sec:     1683 || Lr: 0.000100
2024-02-10 00:11:09,260 Epoch 489: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.24 
2024-02-10 00:11:09,260 EPOCH 490
2024-02-10 00:11:25,566 Epoch 490: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.16 
2024-02-10 00:11:25,566 EPOCH 491
2024-02-10 00:11:41,714 Epoch 491: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.27 
2024-02-10 00:11:41,714 EPOCH 492
2024-02-10 00:11:58,104 Epoch 492: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.16 
2024-02-10 00:11:58,104 EPOCH 493
2024-02-10 00:12:14,188 Epoch 493: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.17 
2024-02-10 00:12:14,189 EPOCH 494
2024-02-10 00:12:30,108 Epoch 494: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.11 
2024-02-10 00:12:30,108 EPOCH 495
2024-02-10 00:12:46,226 Epoch 495: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.11 
2024-02-10 00:12:46,227 EPOCH 496
2024-02-10 00:13:02,349 Epoch 496: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.43 
2024-02-10 00:13:02,350 EPOCH 497
2024-02-10 00:13:18,628 Epoch 497: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.31 
2024-02-10 00:13:18,629 EPOCH 498
2024-02-10 00:13:35,027 Epoch 498: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.20 
2024-02-10 00:13:35,027 EPOCH 499
2024-02-10 00:13:50,992 Epoch 499: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.27 
2024-02-10 00:13:50,993 EPOCH 500
2024-02-10 00:14:07,062 [Epoch: 500 Step: 00004500] Batch Recognition Loss:   0.001988 => Gls Tokens per Sec:      661 || Batch Translation Loss:   0.119016 => Txt Tokens per Sec:     1828 || Lr: 0.000100
2024-02-10 00:14:07,063 Epoch 500: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.22 
2024-02-10 00:14:07,063 EPOCH 501
2024-02-10 00:14:23,062 Epoch 501: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.23 
2024-02-10 00:14:23,063 EPOCH 502
2024-02-10 00:14:39,348 Epoch 502: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.29 
2024-02-10 00:14:39,349 EPOCH 503
2024-02-10 00:14:55,361 Epoch 503: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.13 
2024-02-10 00:14:55,361 EPOCH 504
2024-02-10 00:15:11,648 Epoch 504: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.08 
2024-02-10 00:15:11,649 EPOCH 505
2024-02-10 00:15:27,685 Epoch 505: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.08 
2024-02-10 00:15:27,686 EPOCH 506
2024-02-10 00:15:43,961 Epoch 506: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.15 
2024-02-10 00:15:43,961 EPOCH 507
2024-02-10 00:15:59,997 Epoch 507: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.99 
2024-02-10 00:15:59,997 EPOCH 508
2024-02-10 00:16:15,537 Epoch 508: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.02 
2024-02-10 00:16:15,538 EPOCH 509
2024-02-10 00:16:31,744 Epoch 509: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.01 
2024-02-10 00:16:31,745 EPOCH 510
2024-02-10 00:16:48,026 Epoch 510: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.15 
2024-02-10 00:16:48,027 EPOCH 511
2024-02-10 00:17:04,393 Epoch 511: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.22 
2024-02-10 00:17:04,394 EPOCH 512
2024-02-10 00:17:04,789 [Epoch: 512 Step: 00004600] Batch Recognition Loss:   0.001154 => Gls Tokens per Sec:     3249 || Batch Translation Loss:   0.129299 => Txt Tokens per Sec:     8180 || Lr: 0.000100
2024-02-10 00:17:20,637 Epoch 512: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.12 
2024-02-10 00:17:20,638 EPOCH 513
2024-02-10 00:17:36,761 Epoch 513: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-10 00:17:36,761 EPOCH 514
2024-02-10 00:17:53,009 Epoch 514: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.20 
2024-02-10 00:17:53,009 EPOCH 515
2024-02-10 00:18:09,013 Epoch 515: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.18 
2024-02-10 00:18:09,014 EPOCH 516
2024-02-10 00:18:25,196 Epoch 516: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.14 
2024-02-10 00:18:25,196 EPOCH 517
2024-02-10 00:18:41,308 Epoch 517: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.01 
2024-02-10 00:18:41,308 EPOCH 518
2024-02-10 00:18:57,107 Epoch 518: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.98 
2024-02-10 00:18:57,107 EPOCH 519
2024-02-10 00:19:13,260 Epoch 519: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.04 
2024-02-10 00:19:13,261 EPOCH 520
2024-02-10 00:19:30,200 Epoch 520: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.96 
2024-02-10 00:19:30,201 EPOCH 521
2024-02-10 00:19:46,309 Epoch 521: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.93 
2024-02-10 00:19:46,309 EPOCH 522
2024-02-10 00:20:02,362 Epoch 522: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.89 
2024-02-10 00:20:02,362 EPOCH 523
2024-02-10 00:20:07,045 [Epoch: 523 Step: 00004700] Batch Recognition Loss:   0.001016 => Gls Tokens per Sec:      355 || Batch Translation Loss:   0.112218 => Txt Tokens per Sec:      995 || Lr: 0.000100
2024-02-10 00:20:18,466 Epoch 523: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.89 
2024-02-10 00:20:18,466 EPOCH 524
2024-02-10 00:20:34,413 Epoch 524: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.87 
2024-02-10 00:20:34,413 EPOCH 525
2024-02-10 00:20:50,499 Epoch 525: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.82 
2024-02-10 00:20:50,499 EPOCH 526
2024-02-10 00:21:06,792 Epoch 526: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.83 
2024-02-10 00:21:06,793 EPOCH 527
2024-02-10 00:21:23,144 Epoch 527: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.88 
2024-02-10 00:21:23,144 EPOCH 528
2024-02-10 00:21:39,179 Epoch 528: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.91 
2024-02-10 00:21:39,179 EPOCH 529
2024-02-10 00:21:55,202 Epoch 529: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.83 
2024-02-10 00:21:55,203 EPOCH 530
2024-02-10 00:22:11,256 Epoch 530: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.88 
2024-02-10 00:22:11,257 EPOCH 531
2024-02-10 00:22:27,117 Epoch 531: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.85 
2024-02-10 00:22:27,118 EPOCH 532
2024-02-10 00:22:42,952 Epoch 532: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.84 
2024-02-10 00:22:42,952 EPOCH 533
2024-02-10 00:22:59,078 Epoch 533: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.80 
2024-02-10 00:22:59,078 EPOCH 534
2024-02-10 00:23:00,496 [Epoch: 534 Step: 00004800] Batch Recognition Loss:   0.006695 => Gls Tokens per Sec:     2710 || Batch Translation Loss:   0.055763 => Txt Tokens per Sec:     6658 || Lr: 0.000100
2024-02-10 00:23:15,130 Epoch 534: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.98 
2024-02-10 00:23:15,131 EPOCH 535
2024-02-10 00:23:31,042 Epoch 535: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.99 
2024-02-10 00:23:31,042 EPOCH 536
2024-02-10 00:23:47,212 Epoch 536: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.01 
2024-02-10 00:23:47,213 EPOCH 537
2024-02-10 00:24:03,416 Epoch 537: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.94 
2024-02-10 00:24:03,417 EPOCH 538
2024-02-10 00:24:19,549 Epoch 538: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.92 
2024-02-10 00:24:19,549 EPOCH 539
2024-02-10 00:24:35,519 Epoch 539: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.90 
2024-02-10 00:24:35,520 EPOCH 540
2024-02-10 00:24:52,109 Epoch 540: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.97 
2024-02-10 00:24:52,109 EPOCH 541
2024-02-10 00:25:07,948 Epoch 541: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.01 
2024-02-10 00:25:07,949 EPOCH 542
2024-02-10 00:25:24,151 Epoch 542: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.98 
2024-02-10 00:25:24,152 EPOCH 543
2024-02-10 00:25:40,361 Epoch 543: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.98 
2024-02-10 00:25:40,362 EPOCH 544
2024-02-10 00:25:56,200 Epoch 544: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.08 
2024-02-10 00:25:56,201 EPOCH 545
2024-02-10 00:26:07,300 [Epoch: 545 Step: 00004900] Batch Recognition Loss:   0.001765 => Gls Tokens per Sec:      380 || Batch Translation Loss:   0.180295 => Txt Tokens per Sec:     1147 || Lr: 0.000100
2024-02-10 00:26:12,377 Epoch 545: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.14 
2024-02-10 00:26:12,378 EPOCH 546
2024-02-10 00:26:28,491 Epoch 546: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.08 
2024-02-10 00:26:28,492 EPOCH 547
2024-02-10 00:26:44,399 Epoch 547: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.02 
2024-02-10 00:26:44,399 EPOCH 548
2024-02-10 00:27:00,516 Epoch 548: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.15 
2024-02-10 00:27:00,517 EPOCH 549
2024-02-10 00:27:16,556 Epoch 549: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.18 
2024-02-10 00:27:16,556 EPOCH 550
2024-02-10 00:27:32,505 Epoch 550: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.37 
2024-02-10 00:27:32,506 EPOCH 551
2024-02-10 00:27:48,667 Epoch 551: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.83 
2024-02-10 00:27:48,667 EPOCH 552
2024-02-10 00:28:04,678 Epoch 552: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.23 
2024-02-10 00:28:04,679 EPOCH 553
2024-02-10 00:28:20,999 Epoch 553: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.70 
2024-02-10 00:28:20,999 EPOCH 554
2024-02-10 00:28:36,967 Epoch 554: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.03 
2024-02-10 00:28:36,967 EPOCH 555
2024-02-10 00:28:52,911 Epoch 555: Total Training Recognition Loss 0.05  Total Training Translation Loss 8.31 
2024-02-10 00:28:52,911 EPOCH 556
2024-02-10 00:28:55,420 [Epoch: 556 Step: 00005000] Batch Recognition Loss:   0.006058 => Gls Tokens per Sec:     2552 || Batch Translation Loss:   0.940291 => Txt Tokens per Sec:     6646 || Lr: 0.000100
2024-02-10 00:29:09,005 Epoch 556: Total Training Recognition Loss 0.10  Total Training Translation Loss 10.37 
2024-02-10 00:29:09,005 EPOCH 557
2024-02-10 00:29:25,436 Epoch 557: Total Training Recognition Loss 0.16  Total Training Translation Loss 14.99 
2024-02-10 00:29:25,436 EPOCH 558
2024-02-10 00:29:41,581 Epoch 558: Total Training Recognition Loss 0.63  Total Training Translation Loss 12.92 
2024-02-10 00:29:41,582 EPOCH 559
2024-02-10 00:29:58,340 Epoch 559: Total Training Recognition Loss 2.24  Total Training Translation Loss 9.91 
2024-02-10 00:29:58,341 EPOCH 560
2024-02-10 00:30:15,149 Epoch 560: Total Training Recognition Loss 0.61  Total Training Translation Loss 7.54 
2024-02-10 00:30:15,150 EPOCH 561
2024-02-10 00:30:31,104 Epoch 561: Total Training Recognition Loss 0.15  Total Training Translation Loss 4.69 
2024-02-10 00:30:31,104 EPOCH 562
2024-02-10 00:30:47,294 Epoch 562: Total Training Recognition Loss 0.10  Total Training Translation Loss 2.81 
2024-02-10 00:30:47,296 EPOCH 563
2024-02-10 00:31:03,468 Epoch 563: Total Training Recognition Loss 0.08  Total Training Translation Loss 2.01 
2024-02-10 00:31:03,469 EPOCH 564
2024-02-10 00:31:19,323 Epoch 564: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.52 
2024-02-10 00:31:19,324 EPOCH 565
2024-02-10 00:31:35,521 Epoch 565: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.32 
2024-02-10 00:31:35,522 EPOCH 566
2024-02-10 00:31:51,560 Epoch 566: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.30 
2024-02-10 00:31:51,561 EPOCH 567
2024-02-10 00:32:06,706 [Epoch: 567 Step: 00005100] Batch Recognition Loss:   0.002299 => Gls Tokens per Sec:      448 || Batch Translation Loss:   0.080300 => Txt Tokens per Sec:     1300 || Lr: 0.000100
2024-02-10 00:32:08,319 Epoch 567: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.13 
2024-02-10 00:32:08,319 EPOCH 568
2024-02-10 00:32:24,348 Epoch 568: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.04 
2024-02-10 00:32:24,348 EPOCH 569
2024-02-10 00:32:40,625 Epoch 569: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.06 
2024-02-10 00:32:40,625 EPOCH 570
2024-02-10 00:32:56,702 Epoch 570: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.89 
2024-02-10 00:32:56,703 EPOCH 571
2024-02-10 00:33:13,216 Epoch 571: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.87 
2024-02-10 00:33:13,217 EPOCH 572
2024-02-10 00:33:29,431 Epoch 572: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.83 
2024-02-10 00:33:29,431 EPOCH 573
2024-02-10 00:33:45,555 Epoch 573: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.82 
2024-02-10 00:33:45,556 EPOCH 574
2024-02-10 00:34:01,644 Epoch 574: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.77 
2024-02-10 00:34:01,644 EPOCH 575
2024-02-10 00:34:17,681 Epoch 575: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.72 
2024-02-10 00:34:17,682 EPOCH 576
2024-02-10 00:34:33,707 Epoch 576: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.67 
2024-02-10 00:34:33,707 EPOCH 577
2024-02-10 00:34:49,820 Epoch 577: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.63 
2024-02-10 00:34:49,820 EPOCH 578
2024-02-10 00:35:05,005 [Epoch: 578 Step: 00005200] Batch Recognition Loss:   0.002838 => Gls Tokens per Sec:      531 || Batch Translation Loss:   0.028973 => Txt Tokens per Sec:     1504 || Lr: 0.000100
2024-02-10 00:35:05,991 Epoch 578: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.66 
2024-02-10 00:35:05,992 EPOCH 579
2024-02-10 00:35:22,265 Epoch 579: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.66 
2024-02-10 00:35:22,265 EPOCH 580
2024-02-10 00:35:38,512 Epoch 580: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.61 
2024-02-10 00:35:38,513 EPOCH 581
2024-02-10 00:35:54,746 Epoch 581: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-10 00:35:54,746 EPOCH 582
2024-02-10 00:36:10,925 Epoch 582: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.61 
2024-02-10 00:36:10,926 EPOCH 583
2024-02-10 00:36:26,933 Epoch 583: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-10 00:36:26,934 EPOCH 584
2024-02-10 00:36:43,115 Epoch 584: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.61 
2024-02-10 00:36:43,115 EPOCH 585
2024-02-10 00:36:59,225 Epoch 585: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.58 
2024-02-10 00:36:59,226 EPOCH 586
2024-02-10 00:37:15,332 Epoch 586: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.61 
2024-02-10 00:37:15,333 EPOCH 587
2024-02-10 00:37:31,567 Epoch 587: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-10 00:37:31,568 EPOCH 588
2024-02-10 00:37:47,566 Epoch 588: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.59 
2024-02-10 00:37:47,566 EPOCH 589
2024-02-10 00:38:03,377 [Epoch: 589 Step: 00005300] Batch Recognition Loss:   0.001430 => Gls Tokens per Sec:      591 || Batch Translation Loss:   0.023602 => Txt Tokens per Sec:     1712 || Lr: 0.000100
2024-02-10 00:38:03,709 Epoch 589: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.56 
2024-02-10 00:38:03,709 EPOCH 590
2024-02-10 00:38:19,907 Epoch 590: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-10 00:38:19,908 EPOCH 591
2024-02-10 00:38:36,006 Epoch 591: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-10 00:38:36,006 EPOCH 592
2024-02-10 00:38:51,678 Epoch 592: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-10 00:38:51,678 EPOCH 593
2024-02-10 00:39:07,797 Epoch 593: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.55 
2024-02-10 00:39:07,798 EPOCH 594
2024-02-10 00:39:23,887 Epoch 594: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-10 00:39:23,887 EPOCH 595
2024-02-10 00:39:39,678 Epoch 595: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-10 00:39:39,679 EPOCH 596
2024-02-10 00:39:56,025 Epoch 596: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-10 00:39:56,025 EPOCH 597
2024-02-10 00:40:12,132 Epoch 597: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-10 00:40:12,133 EPOCH 598
2024-02-10 00:40:28,048 Epoch 598: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-10 00:40:28,048 EPOCH 599
2024-02-10 00:40:44,052 Epoch 599: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-10 00:40:44,052 EPOCH 600
2024-02-10 00:41:00,265 [Epoch: 600 Step: 00005400] Batch Recognition Loss:   0.001429 => Gls Tokens per Sec:      655 || Batch Translation Loss:   0.065746 => Txt Tokens per Sec:     1812 || Lr: 0.000100
2024-02-10 00:41:00,266 Epoch 600: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-10 00:41:00,266 EPOCH 601
2024-02-10 00:41:16,522 Epoch 601: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-10 00:41:16,522 EPOCH 602
2024-02-10 00:41:32,725 Epoch 602: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-10 00:41:32,725 EPOCH 603
2024-02-10 00:41:48,790 Epoch 603: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-10 00:41:48,791 EPOCH 604
2024-02-10 00:42:04,709 Epoch 604: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.49 
2024-02-10 00:42:04,710 EPOCH 605
2024-02-10 00:42:20,847 Epoch 605: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.49 
2024-02-10 00:42:20,848 EPOCH 606
2024-02-10 00:42:36,877 Epoch 606: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-10 00:42:36,878 EPOCH 607
2024-02-10 00:42:52,854 Epoch 607: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-10 00:42:52,855 EPOCH 608
2024-02-10 00:43:09,044 Epoch 608: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-10 00:43:09,044 EPOCH 609
2024-02-10 00:43:25,340 Epoch 609: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-10 00:43:25,341 EPOCH 610
2024-02-10 00:43:41,244 Epoch 610: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.51 
2024-02-10 00:43:41,245 EPOCH 611
2024-02-10 00:43:57,374 Epoch 611: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-10 00:43:57,374 EPOCH 612
2024-02-10 00:44:01,703 [Epoch: 612 Step: 00005500] Batch Recognition Loss:   0.001099 => Gls Tokens per Sec:       88 || Batch Translation Loss:   0.022428 => Txt Tokens per Sec:      314 || Lr: 0.000100
2024-02-10 00:44:13,552 Epoch 612: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-10 00:44:13,552 EPOCH 613
2024-02-10 00:44:29,567 Epoch 613: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-10 00:44:29,568 EPOCH 614
2024-02-10 00:44:45,737 Epoch 614: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.45 
2024-02-10 00:44:45,737 EPOCH 615
2024-02-10 00:45:02,044 Epoch 615: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-10 00:45:02,044 EPOCH 616
2024-02-10 00:45:17,636 Epoch 616: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-10 00:45:17,637 EPOCH 617
2024-02-10 00:45:33,974 Epoch 617: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-10 00:45:33,974 EPOCH 618
2024-02-10 00:45:50,375 Epoch 618: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-10 00:45:50,376 EPOCH 619
2024-02-10 00:46:06,571 Epoch 619: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-10 00:46:06,571 EPOCH 620
2024-02-10 00:46:22,521 Epoch 620: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-10 00:46:22,521 EPOCH 621
2024-02-10 00:46:38,563 Epoch 621: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-10 00:46:38,563 EPOCH 622
2024-02-10 00:46:54,532 Epoch 622: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.51 
2024-02-10 00:46:54,533 EPOCH 623
2024-02-10 00:46:55,154 [Epoch: 623 Step: 00005600] Batch Recognition Loss:   0.005292 => Gls Tokens per Sec:     4129 || Batch Translation Loss:   0.037709 => Txt Tokens per Sec:     8306 || Lr: 0.000100
2024-02-10 00:47:10,847 Epoch 623: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-10 00:47:10,848 EPOCH 624
2024-02-10 00:47:27,172 Epoch 624: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.49 
2024-02-10 00:47:27,172 EPOCH 625
2024-02-10 00:47:43,237 Epoch 625: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-10 00:47:43,238 EPOCH 626
2024-02-10 00:47:59,493 Epoch 626: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.45 
2024-02-10 00:47:59,494 EPOCH 627
2024-02-10 00:48:15,513 Epoch 627: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.43 
2024-02-10 00:48:15,513 EPOCH 628
2024-02-10 00:48:31,601 Epoch 628: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-10 00:48:31,601 EPOCH 629
2024-02-10 00:48:47,730 Epoch 629: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-10 00:48:47,731 EPOCH 630
2024-02-10 00:49:04,005 Epoch 630: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-10 00:49:04,006 EPOCH 631
2024-02-10 00:49:20,068 Epoch 631: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-10 00:49:20,068 EPOCH 632
2024-02-10 00:49:36,306 Epoch 632: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.44 
2024-02-10 00:49:36,307 EPOCH 633
2024-02-10 00:49:52,363 Epoch 633: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.43 
2024-02-10 00:49:52,364 EPOCH 634
2024-02-10 00:50:02,670 [Epoch: 634 Step: 00005700] Batch Recognition Loss:   0.002628 => Gls Tokens per Sec:      285 || Batch Translation Loss:   0.065867 => Txt Tokens per Sec:      834 || Lr: 0.000100
2024-02-10 00:50:08,410 Epoch 634: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-10 00:50:08,410 EPOCH 635
2024-02-10 00:50:24,510 Epoch 635: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-10 00:50:24,511 EPOCH 636
2024-02-10 00:50:40,691 Epoch 636: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-10 00:50:40,692 EPOCH 637
2024-02-10 00:50:56,741 Epoch 637: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-10 00:50:56,742 EPOCH 638
2024-02-10 00:51:12,856 Epoch 638: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-10 00:51:12,857 EPOCH 639
2024-02-10 00:51:28,910 Epoch 639: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-10 00:51:28,911 EPOCH 640
2024-02-10 00:51:44,991 Epoch 640: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-10 00:51:44,991 EPOCH 641
2024-02-10 00:52:00,683 Epoch 641: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-10 00:52:00,684 EPOCH 642
2024-02-10 00:52:16,643 Epoch 642: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-10 00:52:16,643 EPOCH 643
2024-02-10 00:52:32,661 Epoch 643: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-10 00:52:32,661 EPOCH 644
2024-02-10 00:52:48,955 Epoch 644: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-10 00:52:48,956 EPOCH 645
2024-02-10 00:52:54,783 [Epoch: 645 Step: 00005800] Batch Recognition Loss:   0.001008 => Gls Tokens per Sec:      724 || Batch Translation Loss:   0.040563 => Txt Tokens per Sec:     1966 || Lr: 0.000100
2024-02-10 00:53:05,256 Epoch 645: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.38 
2024-02-10 00:53:05,256 EPOCH 646
2024-02-10 00:53:21,453 Epoch 646: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.39 
2024-02-10 00:53:21,453 EPOCH 647
2024-02-10 00:53:37,548 Epoch 647: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-10 00:53:37,549 EPOCH 648
2024-02-10 00:53:53,774 Epoch 648: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-10 00:53:53,775 EPOCH 649
2024-02-10 00:54:10,082 Epoch 649: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.42 
2024-02-10 00:54:10,082 EPOCH 650
2024-02-10 00:54:26,401 Epoch 650: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.40 
2024-02-10 00:54:26,401 EPOCH 651
2024-02-10 00:54:42,584 Epoch 651: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-10 00:54:42,584 EPOCH 652
2024-02-10 00:54:58,851 Epoch 652: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-10 00:54:58,852 EPOCH 653
2024-02-10 00:55:15,064 Epoch 653: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.80 
2024-02-10 00:55:15,065 EPOCH 654
2024-02-10 00:55:31,241 Epoch 654: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.85 
2024-02-10 00:55:31,241 EPOCH 655
2024-02-10 00:55:47,330 Epoch 655: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.76 
2024-02-10 00:55:47,331 EPOCH 656
2024-02-10 00:56:01,793 [Epoch: 656 Step: 00005900] Batch Recognition Loss:   0.002515 => Gls Tokens per Sec:      380 || Batch Translation Loss:   0.073978 => Txt Tokens per Sec:     1175 || Lr: 0.000100
2024-02-10 00:56:03,588 Epoch 656: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-10 00:56:03,588 EPOCH 657
2024-02-10 00:56:20,036 Epoch 657: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-10 00:56:20,038 EPOCH 658
2024-02-10 00:56:36,271 Epoch 658: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-10 00:56:36,271 EPOCH 659
2024-02-10 00:56:52,411 Epoch 659: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.60 
2024-02-10 00:56:52,412 EPOCH 660
2024-02-10 00:57:08,305 Epoch 660: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-10 00:57:08,306 EPOCH 661
2024-02-10 00:57:24,721 Epoch 661: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.59 
2024-02-10 00:57:24,722 EPOCH 662
2024-02-10 00:57:40,593 Epoch 662: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-10 00:57:40,594 EPOCH 663
2024-02-10 00:57:56,564 Epoch 663: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-10 00:57:56,564 EPOCH 664
2024-02-10 00:58:12,584 Epoch 664: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.56 
2024-02-10 00:58:12,584 EPOCH 665
2024-02-10 00:58:28,708 Epoch 665: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-10 00:58:28,709 EPOCH 666
2024-02-10 00:58:44,864 Epoch 666: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-10 00:58:44,865 EPOCH 667
2024-02-10 00:58:59,763 [Epoch: 667 Step: 00006000] Batch Recognition Loss:   0.001245 => Gls Tokens per Sec:      455 || Batch Translation Loss:   0.063381 => Txt Tokens per Sec:     1379 || Lr: 0.000100
2024-02-10 01:00:11,926 Validation result at epoch 667, step     6000: duration: 72.1622s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.55238	Translation Loss: 88206.43750	PPL: 6701.39502
	Eval Metric: BLEU
	WER 5.79	(DEL: 0.00,	INS: 0.00,	SUB: 5.79)
	BLEU-4 0.41	(BLEU-1: 11.80,	BLEU-2: 3.77,	BLEU-3: 1.26,	BLEU-4: 0.41)
	CHRF 17.22	ROUGE 9.87
2024-02-10 01:00:11,928 Logging Recognition and Translation Outputs
2024-02-10 01:00:11,928 ========================================================================================================================
2024-02-10 01:00:11,928 Logging Sequence: 160_153.00
2024-02-10 01:00:11,929 	Gloss Reference :	A B+C+D+E
2024-02-10 01:00:11,929 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 01:00:11,929 	Gloss Alignment :	         
2024-02-10 01:00:11,929 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 01:00:11,931 	Text Reference  :	i have no hard feelings towards rohit sharma and he   will  always   have my    full support as    he   is my   teammate
2024-02-10 01:00:11,931 	Text Hypothesis :	* **** ** **** ******** ******* ***** ****** *** this group includes the  first time when    kohli made a  t20i captain 
2024-02-10 01:00:11,931 	Text Alignment  :	D D    D  D    D        D       D     D      D   S    S     S        S    S     S    S       S     S    S  S    S       
2024-02-10 01:00:11,931 ========================================================================================================================
2024-02-10 01:00:11,931 Logging Sequence: 103_253.00
2024-02-10 01:00:11,932 	Gloss Reference :	A B+C+D+E
2024-02-10 01:00:11,932 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 01:00:11,932 	Gloss Alignment :	         
2024-02-10 01:00:11,932 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 01:00:11,933 	Text Reference  :	canada is *** *** ***** ***** ** 3rd  with 92 medals
2024-02-10 01:00:11,933 	Text Hypothesis :	this   is not the first count of them in   an games 
2024-02-10 01:00:11,933 	Text Alignment  :	S         I   I   I     I     I  S    S    S  S     
2024-02-10 01:00:11,933 ========================================================================================================================
2024-02-10 01:00:11,933 Logging Sequence: 155_25.00
2024-02-10 01:00:11,933 	Gloss Reference :	A B+C+D+E
2024-02-10 01:00:11,934 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 01:00:11,934 	Gloss Alignment :	         
2024-02-10 01:00:11,934 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 01:00:11,935 	Text Reference  :	this is because taliban overthrew the afghan government and took over   the country
2024-02-10 01:00:11,935 	Text Hypothesis :	**** ** ******* ******* i         am  very   grateful   to  my   family as  well   
2024-02-10 01:00:11,935 	Text Alignment  :	D    D  D       D       S         S   S      S          S   S    S      S   S      
2024-02-10 01:00:11,935 ========================================================================================================================
2024-02-10 01:00:11,935 Logging Sequence: 81_105.00
2024-02-10 01:00:11,935 	Gloss Reference :	A B+C+D+E
2024-02-10 01:00:11,935 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 01:00:11,936 	Gloss Alignment :	         
2024-02-10 01:00:11,936 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 01:00:11,937 	Text Reference  :	** ** *** ***** dhoni was tagged in      multiple such posts as       he    was the      brand   ambassador
2024-02-10 01:00:11,937 	Text Hypothesis :	it is not known if    any court  seeking to       do   so    amrapali group so  amrapali group's sports    
2024-02-10 01:00:11,937 	Text Alignment  :	I  I  I   I     S     S   S      S       S        S    S     S        S     S   S        S       S         
2024-02-10 01:00:11,937 ========================================================================================================================
2024-02-10 01:00:11,938 Logging Sequence: 105_136.00
2024-02-10 01:00:11,938 	Gloss Reference :	A B+C+D+E
2024-02-10 01:00:11,938 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 01:00:11,938 	Gloss Alignment :	         
2024-02-10 01:00:11,938 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 01:00:11,938 	Text Reference  :	beating him once is my   biggest dream    
2024-02-10 01:00:11,939 	Text Hypothesis :	******* *** **** ** what a       wonderful
2024-02-10 01:00:11,939 	Text Alignment  :	D       D   D    D  S    S       S        
2024-02-10 01:00:11,939 ========================================================================================================================
2024-02-10 01:00:13,404 Epoch 667: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-10 01:00:13,404 EPOCH 668
2024-02-10 01:00:29,974 Epoch 668: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-10 01:00:29,975 EPOCH 669
2024-02-10 01:00:46,231 Epoch 669: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-10 01:00:46,232 EPOCH 670
2024-02-10 01:01:02,141 Epoch 670: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-10 01:01:02,142 EPOCH 671
2024-02-10 01:01:18,024 Epoch 671: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-10 01:01:18,024 EPOCH 672
2024-02-10 01:01:34,364 Epoch 672: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-10 01:01:34,364 EPOCH 673
2024-02-10 01:01:50,262 Epoch 673: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-10 01:01:50,262 EPOCH 674
2024-02-10 01:02:06,328 Epoch 674: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-10 01:02:06,328 EPOCH 675
2024-02-10 01:02:22,236 Epoch 675: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-10 01:02:22,237 EPOCH 676
2024-02-10 01:02:38,396 Epoch 676: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-10 01:02:38,397 EPOCH 677
2024-02-10 01:02:54,787 Epoch 677: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-10 01:02:54,787 EPOCH 678
2024-02-10 01:03:10,103 [Epoch: 678 Step: 00006100] Batch Recognition Loss:   0.000361 => Gls Tokens per Sec:      526 || Batch Translation Loss:   0.024810 => Txt Tokens per Sec:     1519 || Lr: 0.000100
2024-02-10 01:03:10,914 Epoch 678: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-10 01:03:10,915 EPOCH 679
2024-02-10 01:03:27,357 Epoch 679: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-10 01:03:27,358 EPOCH 680
2024-02-10 01:03:43,346 Epoch 680: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-10 01:03:43,346 EPOCH 681
2024-02-10 01:03:59,299 Epoch 681: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.43 
2024-02-10 01:03:59,300 EPOCH 682
2024-02-10 01:04:15,345 Epoch 682: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-10 01:04:15,346 EPOCH 683
2024-02-10 01:04:31,289 Epoch 683: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-10 01:04:31,290 EPOCH 684
2024-02-10 01:04:47,465 Epoch 684: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-10 01:04:47,465 EPOCH 685
2024-02-10 01:05:03,286 Epoch 685: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-10 01:05:03,287 EPOCH 686
2024-02-10 01:05:19,091 Epoch 686: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-10 01:05:19,091 EPOCH 687
2024-02-10 01:05:35,263 Epoch 687: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-10 01:05:35,264 EPOCH 688
2024-02-10 01:05:51,059 Epoch 688: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-10 01:05:51,060 EPOCH 689
2024-02-10 01:06:06,933 [Epoch: 689 Step: 00006200] Batch Recognition Loss:   0.002973 => Gls Tokens per Sec:      588 || Batch Translation Loss:   0.107330 => Txt Tokens per Sec:     1645 || Lr: 0.000100
2024-02-10 01:06:07,321 Epoch 689: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-10 01:06:07,321 EPOCH 690
2024-02-10 01:06:23,482 Epoch 690: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-10 01:06:23,483 EPOCH 691
2024-02-10 01:06:39,623 Epoch 691: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.62 
2024-02-10 01:06:39,624 EPOCH 692
2024-02-10 01:06:55,899 Epoch 692: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-10 01:06:55,899 EPOCH 693
2024-02-10 01:07:12,136 Epoch 693: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.51 
2024-02-10 01:07:12,137 EPOCH 694
2024-02-10 01:07:28,090 Epoch 694: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-10 01:07:28,090 EPOCH 695
2024-02-10 01:07:44,548 Epoch 695: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-10 01:07:44,549 EPOCH 696
2024-02-10 01:08:01,016 Epoch 696: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-10 01:08:01,017 EPOCH 697
2024-02-10 01:08:17,106 Epoch 697: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-10 01:08:17,107 EPOCH 698
2024-02-10 01:08:33,372 Epoch 698: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-10 01:08:33,372 EPOCH 699
2024-02-10 01:08:49,435 Epoch 699: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-10 01:08:49,436 EPOCH 700
2024-02-10 01:09:05,432 [Epoch: 700 Step: 00006300] Batch Recognition Loss:   0.001453 => Gls Tokens per Sec:      664 || Batch Translation Loss:   0.078498 => Txt Tokens per Sec:     1837 || Lr: 0.000100
2024-02-10 01:09:05,432 Epoch 700: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-10 01:09:05,432 EPOCH 701
2024-02-10 01:09:21,613 Epoch 701: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.85 
2024-02-10 01:09:21,613 EPOCH 702
2024-02-10 01:09:37,484 Epoch 702: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-10 01:09:37,484 EPOCH 703
2024-02-10 01:09:53,743 Epoch 703: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-10 01:09:53,744 EPOCH 704
2024-02-10 01:10:09,520 Epoch 704: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.93 
2024-02-10 01:10:09,521 EPOCH 705
2024-02-10 01:10:25,457 Epoch 705: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.17 
2024-02-10 01:10:25,457 EPOCH 706
2024-02-10 01:10:41,789 Epoch 706: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.30 
2024-02-10 01:10:41,789 EPOCH 707
2024-02-10 01:10:57,704 Epoch 707: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.53 
2024-02-10 01:10:57,704 EPOCH 708
2024-02-10 01:11:13,563 Epoch 708: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.49 
2024-02-10 01:11:13,563 EPOCH 709
2024-02-10 01:11:29,680 Epoch 709: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.81 
2024-02-10 01:11:29,681 EPOCH 710
2024-02-10 01:11:45,673 Epoch 710: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.66 
2024-02-10 01:11:45,674 EPOCH 711
2024-02-10 01:12:01,640 Epoch 711: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.65 
2024-02-10 01:12:01,641 EPOCH 712
2024-02-10 01:12:01,865 [Epoch: 712 Step: 00006400] Batch Recognition Loss:   0.006864 => Gls Tokens per Sec:     5766 || Batch Translation Loss:   0.271701 => Txt Tokens per Sec:    10270 || Lr: 0.000100
2024-02-10 01:12:17,916 Epoch 712: Total Training Recognition Loss 0.07  Total Training Translation Loss 13.58 
2024-02-10 01:12:17,916 EPOCH 713
2024-02-10 01:12:34,174 Epoch 713: Total Training Recognition Loss 0.08  Total Training Translation Loss 9.52 
2024-02-10 01:12:34,176 EPOCH 714
2024-02-10 01:12:50,192 Epoch 714: Total Training Recognition Loss 0.17  Total Training Translation Loss 10.76 
2024-02-10 01:12:50,192 EPOCH 715
2024-02-10 01:13:06,461 Epoch 715: Total Training Recognition Loss 0.16  Total Training Translation Loss 9.35 
2024-02-10 01:13:06,461 EPOCH 716
2024-02-10 01:13:22,224 Epoch 716: Total Training Recognition Loss 0.24  Total Training Translation Loss 6.02 
2024-02-10 01:13:22,224 EPOCH 717
2024-02-10 01:13:38,198 Epoch 717: Total Training Recognition Loss 0.16  Total Training Translation Loss 3.73 
2024-02-10 01:13:38,199 EPOCH 718
2024-02-10 01:13:54,595 Epoch 718: Total Training Recognition Loss 0.20  Total Training Translation Loss 2.46 
2024-02-10 01:13:54,596 EPOCH 719
2024-02-10 01:14:10,684 Epoch 719: Total Training Recognition Loss 0.41  Total Training Translation Loss 1.43 
2024-02-10 01:14:10,684 EPOCH 720
2024-02-10 01:14:26,840 Epoch 720: Total Training Recognition Loss 0.62  Total Training Translation Loss 1.09 
2024-02-10 01:14:26,841 EPOCH 721
2024-02-10 01:14:42,892 Epoch 721: Total Training Recognition Loss 1.27  Total Training Translation Loss 1.06 
2024-02-10 01:14:42,893 EPOCH 722
2024-02-10 01:14:58,912 Epoch 722: Total Training Recognition Loss 6.18  Total Training Translation Loss 1.55 
2024-02-10 01:14:58,913 EPOCH 723
2024-02-10 01:15:03,967 [Epoch: 723 Step: 00006500] Batch Recognition Loss:   0.030852 => Gls Tokens per Sec:      329 || Batch Translation Loss:   0.374455 => Txt Tokens per Sec:     1033 || Lr: 0.000100
2024-02-10 01:15:14,953 Epoch 723: Total Training Recognition Loss 10.87  Total Training Translation Loss 2.99 
2024-02-10 01:15:14,953 EPOCH 724
2024-02-10 01:15:31,069 Epoch 724: Total Training Recognition Loss 8.52  Total Training Translation Loss 3.17 
2024-02-10 01:15:31,070 EPOCH 725
2024-02-10 01:15:46,882 Epoch 725: Total Training Recognition Loss 2.44  Total Training Translation Loss 2.37 
2024-02-10 01:15:46,883 EPOCH 726
2024-02-10 01:16:02,717 Epoch 726: Total Training Recognition Loss 0.70  Total Training Translation Loss 1.70 
2024-02-10 01:16:02,717 EPOCH 727
2024-02-10 01:16:19,128 Epoch 727: Total Training Recognition Loss 0.39  Total Training Translation Loss 1.22 
2024-02-10 01:16:19,129 EPOCH 728
2024-02-10 01:16:35,313 Epoch 728: Total Training Recognition Loss 0.20  Total Training Translation Loss 0.95 
2024-02-10 01:16:35,314 EPOCH 729
2024-02-10 01:16:51,576 Epoch 729: Total Training Recognition Loss 0.14  Total Training Translation Loss 0.76 
2024-02-10 01:16:51,576 EPOCH 730
2024-02-10 01:17:07,759 Epoch 730: Total Training Recognition Loss 0.11  Total Training Translation Loss 0.70 
2024-02-10 01:17:07,759 EPOCH 731
2024-02-10 01:17:23,708 Epoch 731: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.64 
2024-02-10 01:17:23,708 EPOCH 732
2024-02-10 01:17:39,993 Epoch 732: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.58 
2024-02-10 01:17:39,994 EPOCH 733
2024-02-10 01:17:56,181 Epoch 733: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.54 
2024-02-10 01:17:56,181 EPOCH 734
2024-02-10 01:18:01,424 [Epoch: 734 Step: 00006600] Batch Recognition Loss:   0.010452 => Gls Tokens per Sec:      561 || Batch Translation Loss:   0.024879 => Txt Tokens per Sec:     1553 || Lr: 0.000100
2024-02-10 01:18:12,228 Epoch 734: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.51 
2024-02-10 01:18:12,229 EPOCH 735
2024-02-10 01:18:28,263 Epoch 735: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.49 
2024-02-10 01:18:28,264 EPOCH 736
2024-02-10 01:18:44,678 Epoch 736: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.49 
2024-02-10 01:18:44,679 EPOCH 737
2024-02-10 01:19:00,680 Epoch 737: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.47 
2024-02-10 01:19:00,680 EPOCH 738
2024-02-10 01:19:16,706 Epoch 738: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.45 
2024-02-10 01:19:16,706 EPOCH 739
2024-02-10 01:19:33,553 Epoch 739: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.44 
2024-02-10 01:19:33,553 EPOCH 740
2024-02-10 01:19:49,557 Epoch 740: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.45 
2024-02-10 01:19:49,558 EPOCH 741
2024-02-10 01:20:05,773 Epoch 741: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.43 
2024-02-10 01:20:05,773 EPOCH 742
2024-02-10 01:20:21,896 Epoch 742: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.41 
2024-02-10 01:20:21,897 EPOCH 743
2024-02-10 01:20:37,866 Epoch 743: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.40 
2024-02-10 01:20:37,866 EPOCH 744
2024-02-10 01:20:54,022 Epoch 744: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.41 
2024-02-10 01:20:54,022 EPOCH 745
2024-02-10 01:21:03,758 [Epoch: 745 Step: 00006700] Batch Recognition Loss:   0.003373 => Gls Tokens per Sec:      526 || Batch Translation Loss:   0.026921 => Txt Tokens per Sec:     1476 || Lr: 0.000100
2024-02-10 01:21:09,987 Epoch 745: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.42 
2024-02-10 01:21:09,987 EPOCH 746
2024-02-10 01:21:25,762 Epoch 746: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.40 
2024-02-10 01:21:25,763 EPOCH 747
2024-02-10 01:21:41,683 Epoch 747: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.38 
2024-02-10 01:21:41,684 EPOCH 748
2024-02-10 01:21:58,293 Epoch 748: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.39 
2024-02-10 01:21:58,295 EPOCH 749
2024-02-10 01:22:14,612 Epoch 749: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.38 
2024-02-10 01:22:14,613 EPOCH 750
2024-02-10 01:22:30,578 Epoch 750: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.37 
2024-02-10 01:22:30,579 EPOCH 751
2024-02-10 01:22:46,837 Epoch 751: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.37 
2024-02-10 01:22:46,838 EPOCH 752
2024-02-10 01:23:02,734 Epoch 752: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.38 
2024-02-10 01:23:02,734 EPOCH 753
2024-02-10 01:23:18,555 Epoch 753: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.34 
2024-02-10 01:23:18,556 EPOCH 754
2024-02-10 01:23:34,693 Epoch 754: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.33 
2024-02-10 01:23:34,694 EPOCH 755
2024-02-10 01:23:50,699 Epoch 755: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.36 
2024-02-10 01:23:50,699 EPOCH 756
2024-02-10 01:24:05,010 [Epoch: 756 Step: 00006800] Batch Recognition Loss:   0.002187 => Gls Tokens per Sec:      384 || Batch Translation Loss:   0.044271 => Txt Tokens per Sec:     1156 || Lr: 0.000100
2024-02-10 01:24:06,658 Epoch 756: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.32 
2024-02-10 01:24:06,658 EPOCH 757
2024-02-10 01:24:22,699 Epoch 757: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.33 
2024-02-10 01:24:22,700 EPOCH 758
2024-02-10 01:24:38,850 Epoch 758: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.36 
2024-02-10 01:24:38,850 EPOCH 759
2024-02-10 01:24:54,677 Epoch 759: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.31 
2024-02-10 01:24:54,677 EPOCH 760
2024-02-10 01:25:10,886 Epoch 760: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.32 
2024-02-10 01:25:10,887 EPOCH 761
2024-02-10 01:25:26,948 Epoch 761: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.34 
2024-02-10 01:25:26,948 EPOCH 762
2024-02-10 01:25:42,893 Epoch 762: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.34 
2024-02-10 01:25:42,894 EPOCH 763
2024-02-10 01:25:59,168 Epoch 763: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.33 
2024-02-10 01:25:59,169 EPOCH 764
2024-02-10 01:26:15,019 Epoch 764: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.30 
2024-02-10 01:26:15,020 EPOCH 765
2024-02-10 01:26:31,369 Epoch 765: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.32 
2024-02-10 01:26:31,370 EPOCH 766
2024-02-10 01:26:46,966 Epoch 766: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.33 
2024-02-10 01:26:46,967 EPOCH 767
2024-02-10 01:26:58,981 [Epoch: 767 Step: 00006900] Batch Recognition Loss:   0.000539 => Gls Tokens per Sec:      564 || Batch Translation Loss:   0.035970 => Txt Tokens per Sec:     1603 || Lr: 0.000100
2024-02-10 01:27:03,136 Epoch 767: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 01:27:03,137 EPOCH 768
2024-02-10 01:27:19,255 Epoch 768: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.32 
2024-02-10 01:27:19,256 EPOCH 769
2024-02-10 01:27:35,343 Epoch 769: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 01:27:35,343 EPOCH 770
2024-02-10 01:27:51,276 Epoch 770: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 01:27:51,277 EPOCH 771
2024-02-10 01:28:07,333 Epoch 771: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.29 
2024-02-10 01:28:07,334 EPOCH 772
2024-02-10 01:28:23,594 Epoch 772: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.28 
2024-02-10 01:28:23,594 EPOCH 773
2024-02-10 01:28:39,442 Epoch 773: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.30 
2024-02-10 01:28:39,443 EPOCH 774
2024-02-10 01:28:55,833 Epoch 774: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.30 
2024-02-10 01:28:55,833 EPOCH 775
2024-02-10 01:29:11,636 Epoch 775: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.28 
2024-02-10 01:29:11,637 EPOCH 776
2024-02-10 01:29:27,818 Epoch 776: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.31 
2024-02-10 01:29:27,818 EPOCH 777
2024-02-10 01:29:43,948 Epoch 777: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.28 
2024-02-10 01:29:43,948 EPOCH 778
2024-02-10 01:29:55,251 [Epoch: 778 Step: 00007000] Batch Recognition Loss:   0.001461 => Gls Tokens per Sec:      793 || Batch Translation Loss:   0.025202 => Txt Tokens per Sec:     2137 || Lr: 0.000100
2024-02-10 01:30:00,301 Epoch 778: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 01:30:00,302 EPOCH 779
2024-02-10 01:30:17,055 Epoch 779: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.29 
2024-02-10 01:30:17,056 EPOCH 780
2024-02-10 01:30:33,198 Epoch 780: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.31 
2024-02-10 01:30:33,199 EPOCH 781
2024-02-10 01:30:49,150 Epoch 781: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.29 
2024-02-10 01:30:49,151 EPOCH 782
2024-02-10 01:31:05,111 Epoch 782: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.32 
2024-02-10 01:31:05,111 EPOCH 783
2024-02-10 01:31:21,198 Epoch 783: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 01:31:21,198 EPOCH 784
2024-02-10 01:31:37,159 Epoch 784: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 01:31:37,160 EPOCH 785
2024-02-10 01:31:53,159 Epoch 785: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 01:31:53,159 EPOCH 786
2024-02-10 01:32:08,913 Epoch 786: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 01:32:08,914 EPOCH 787
2024-02-10 01:32:25,067 Epoch 787: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 01:32:25,068 EPOCH 788
2024-02-10 01:32:40,850 Epoch 788: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 01:32:40,850 EPOCH 789
2024-02-10 01:32:56,656 [Epoch: 789 Step: 00007100] Batch Recognition Loss:   0.001639 => Gls Tokens per Sec:      591 || Batch Translation Loss:   0.024745 => Txt Tokens per Sec:     1714 || Lr: 0.000100
2024-02-10 01:32:56,906 Epoch 789: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 01:32:56,906 EPOCH 790
2024-02-10 01:33:12,761 Epoch 790: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 01:33:12,762 EPOCH 791
2024-02-10 01:33:28,864 Epoch 791: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 01:33:28,865 EPOCH 792
2024-02-10 01:33:44,592 Epoch 792: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 01:33:44,593 EPOCH 793
2024-02-10 01:34:00,988 Epoch 793: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 01:34:00,989 EPOCH 794
2024-02-10 01:34:17,026 Epoch 794: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 01:34:17,027 EPOCH 795
2024-02-10 01:34:33,255 Epoch 795: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 01:34:33,256 EPOCH 796
2024-02-10 01:34:49,155 Epoch 796: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 01:34:49,155 EPOCH 797
2024-02-10 01:35:04,928 Epoch 797: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 01:35:04,929 EPOCH 798
2024-02-10 01:35:21,158 Epoch 798: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.30 
2024-02-10 01:35:21,159 EPOCH 799
2024-02-10 01:35:37,524 Epoch 799: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 01:35:37,525 EPOCH 800
2024-02-10 01:35:53,547 [Epoch: 800 Step: 00007200] Batch Recognition Loss:   0.002852 => Gls Tokens per Sec:      663 || Batch Translation Loss:   0.023433 => Txt Tokens per Sec:     1834 || Lr: 0.000100
2024-02-10 01:35:53,549 Epoch 800: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.28 
2024-02-10 01:35:53,549 EPOCH 801
2024-02-10 01:36:10,012 Epoch 801: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 01:36:10,012 EPOCH 802
2024-02-10 01:36:25,979 Epoch 802: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 01:36:25,979 EPOCH 803
2024-02-10 01:36:41,944 Epoch 803: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 01:36:41,944 EPOCH 804
2024-02-10 01:36:58,070 Epoch 804: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 01:36:58,070 EPOCH 805
2024-02-10 01:37:14,414 Epoch 805: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 01:37:14,415 EPOCH 806
2024-02-10 01:37:30,649 Epoch 806: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 01:37:30,649 EPOCH 807
2024-02-10 01:37:46,584 Epoch 807: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 01:37:46,585 EPOCH 808
2024-02-10 01:38:02,524 Epoch 808: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 01:38:02,525 EPOCH 809
2024-02-10 01:38:18,477 Epoch 809: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 01:38:18,478 EPOCH 810
2024-02-10 01:38:34,674 Epoch 810: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 01:38:34,675 EPOCH 811
2024-02-10 01:38:50,682 Epoch 811: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 01:38:50,683 EPOCH 812
2024-02-10 01:38:51,059 [Epoch: 812 Step: 00007300] Batch Recognition Loss:   0.000274 => Gls Tokens per Sec:     3413 || Batch Translation Loss:   0.025714 => Txt Tokens per Sec:     8635 || Lr: 0.000100
2024-02-10 01:39:07,001 Epoch 812: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 01:39:07,002 EPOCH 813
2024-02-10 01:39:22,712 Epoch 813: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 01:39:22,712 EPOCH 814
2024-02-10 01:39:39,040 Epoch 814: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 01:39:39,040 EPOCH 815
2024-02-10 01:39:55,156 Epoch 815: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 01:39:55,156 EPOCH 816
2024-02-10 01:40:11,229 Epoch 816: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 01:40:11,230 EPOCH 817
2024-02-10 01:40:27,427 Epoch 817: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 01:40:27,427 EPOCH 818
2024-02-10 01:40:43,639 Epoch 818: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 01:40:43,639 EPOCH 819
2024-02-10 01:40:59,436 Epoch 819: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 01:40:59,437 EPOCH 820
2024-02-10 01:41:15,609 Epoch 820: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 01:41:15,610 EPOCH 821
2024-02-10 01:41:31,706 Epoch 821: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 01:41:31,707 EPOCH 822
2024-02-10 01:41:47,795 Epoch 822: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 01:41:47,795 EPOCH 823
2024-02-10 01:41:48,430 [Epoch: 823 Step: 00007400] Batch Recognition Loss:   0.000869 => Gls Tokens per Sec:     4038 || Batch Translation Loss:   0.014925 => Txt Tokens per Sec:     9156 || Lr: 0.000100
2024-02-10 01:42:03,720 Epoch 823: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 01:42:03,721 EPOCH 824
2024-02-10 01:42:20,199 Epoch 824: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 01:42:20,200 EPOCH 825
2024-02-10 01:42:36,471 Epoch 825: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 01:42:36,471 EPOCH 826
2024-02-10 01:42:52,741 Epoch 826: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 01:42:52,741 EPOCH 827
2024-02-10 01:43:08,688 Epoch 827: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 01:43:08,689 EPOCH 828
2024-02-10 01:43:24,909 Epoch 828: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 01:43:24,910 EPOCH 829
2024-02-10 01:43:41,292 Epoch 829: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 01:43:41,292 EPOCH 830
2024-02-10 01:43:57,307 Epoch 830: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 01:43:57,307 EPOCH 831
2024-02-10 01:44:13,658 Epoch 831: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-10 01:44:13,658 EPOCH 832
2024-02-10 01:44:30,008 Epoch 832: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-10 01:44:30,009 EPOCH 833
2024-02-10 01:44:46,234 Epoch 833: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 01:44:46,236 EPOCH 834
2024-02-10 01:44:52,753 [Epoch: 834 Step: 00007500] Batch Recognition Loss:   0.000648 => Gls Tokens per Sec:      589 || Batch Translation Loss:   0.023216 => Txt Tokens per Sec:     1465 || Lr: 0.000100
2024-02-10 01:45:02,625 Epoch 834: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-10 01:45:02,626 EPOCH 835
2024-02-10 01:45:18,840 Epoch 835: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 01:45:18,840 EPOCH 836
2024-02-10 01:45:34,703 Epoch 836: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 01:45:34,704 EPOCH 837
2024-02-10 01:45:51,110 Epoch 837: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 01:45:51,110 EPOCH 838
2024-02-10 01:46:07,162 Epoch 838: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 01:46:07,163 EPOCH 839
2024-02-10 01:46:22,889 Epoch 839: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 01:46:22,890 EPOCH 840
2024-02-10 01:46:38,948 Epoch 840: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 01:46:38,948 EPOCH 841
2024-02-10 01:46:55,148 Epoch 841: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 01:46:55,148 EPOCH 842
2024-02-10 01:47:11,214 Epoch 842: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 01:47:11,215 EPOCH 843
2024-02-10 01:47:27,291 Epoch 843: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 01:47:27,292 EPOCH 844
2024-02-10 01:47:43,810 Epoch 844: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 01:47:43,810 EPOCH 845
2024-02-10 01:47:49,554 [Epoch: 845 Step: 00007600] Batch Recognition Loss:   0.000540 => Gls Tokens per Sec:      735 || Batch Translation Loss:   0.037321 => Txt Tokens per Sec:     1971 || Lr: 0.000100
2024-02-10 01:48:00,005 Epoch 845: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 01:48:00,006 EPOCH 846
2024-02-10 01:48:16,008 Epoch 846: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-10 01:48:16,008 EPOCH 847
2024-02-10 01:48:32,194 Epoch 847: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-10 01:48:32,195 EPOCH 848
2024-02-10 01:48:48,368 Epoch 848: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-10 01:48:48,368 EPOCH 849
2024-02-10 01:49:04,306 Epoch 849: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-10 01:49:04,307 EPOCH 850
2024-02-10 01:49:20,649 Epoch 850: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-10 01:49:20,650 EPOCH 851
2024-02-10 01:49:36,600 Epoch 851: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-10 01:49:36,600 EPOCH 852
2024-02-10 01:49:52,590 Epoch 852: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.69 
2024-02-10 01:49:52,590 EPOCH 853
2024-02-10 01:50:08,739 Epoch 853: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.77 
2024-02-10 01:50:08,740 EPOCH 854
2024-02-10 01:50:24,791 Epoch 854: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-10 01:50:24,792 EPOCH 855
2024-02-10 01:50:40,746 Epoch 855: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-10 01:50:40,747 EPOCH 856
2024-02-10 01:50:42,793 [Epoch: 856 Step: 00007700] Batch Recognition Loss:   0.000814 => Gls Tokens per Sec:     3130 || Batch Translation Loss:   0.089581 => Txt Tokens per Sec:     7655 || Lr: 0.000100
2024-02-10 01:50:56,735 Epoch 856: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-10 01:50:56,735 EPOCH 857
2024-02-10 01:51:12,711 Epoch 857: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-10 01:51:12,712 EPOCH 858
2024-02-10 01:51:28,888 Epoch 858: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-10 01:51:28,890 EPOCH 859
2024-02-10 01:51:44,818 Epoch 859: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-10 01:51:44,819 EPOCH 860
2024-02-10 01:52:01,217 Epoch 860: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.73 
2024-02-10 01:52:01,217 EPOCH 861
2024-02-10 01:52:17,105 Epoch 861: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.55 
2024-02-10 01:52:17,106 EPOCH 862
2024-02-10 01:52:33,904 Epoch 862: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.89 
2024-02-10 01:52:33,906 EPOCH 863
2024-02-10 01:52:49,958 Epoch 863: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.92 
2024-02-10 01:52:49,958 EPOCH 864
2024-02-10 01:53:05,830 Epoch 864: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.02 
2024-02-10 01:53:05,831 EPOCH 865
2024-02-10 01:53:21,734 Epoch 865: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.98 
2024-02-10 01:53:21,735 EPOCH 866
2024-02-10 01:53:38,098 Epoch 866: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.34 
2024-02-10 01:53:38,098 EPOCH 867
2024-02-10 01:53:52,596 [Epoch: 867 Step: 00007800] Batch Recognition Loss:   0.003326 => Gls Tokens per Sec:      468 || Batch Translation Loss:   0.205168 => Txt Tokens per Sec:     1348 || Lr: 0.000100
2024-02-10 01:53:54,347 Epoch 867: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.35 
2024-02-10 01:53:54,347 EPOCH 868
2024-02-10 01:54:10,483 Epoch 868: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.31 
2024-02-10 01:54:10,483 EPOCH 869
2024-02-10 01:54:26,771 Epoch 869: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.04 
2024-02-10 01:54:26,771 EPOCH 870
2024-02-10 01:54:42,990 Epoch 870: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.72 
2024-02-10 01:54:42,991 EPOCH 871
2024-02-10 01:54:59,369 Epoch 871: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.65 
2024-02-10 01:54:59,369 EPOCH 872
2024-02-10 01:55:15,553 Epoch 872: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.50 
2024-02-10 01:55:15,554 EPOCH 873
2024-02-10 01:55:31,842 Epoch 873: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-10 01:55:31,843 EPOCH 874
2024-02-10 01:55:48,028 Epoch 874: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.51 
2024-02-10 01:55:48,029 EPOCH 875
2024-02-10 01:56:04,312 Epoch 875: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.49 
2024-02-10 01:56:04,313 EPOCH 876
2024-02-10 01:56:20,594 Epoch 876: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-10 01:56:20,595 EPOCH 877
2024-02-10 01:56:36,522 Epoch 877: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.49 
2024-02-10 01:56:36,522 EPOCH 878
2024-02-10 01:56:48,119 [Epoch: 878 Step: 00007900] Batch Recognition Loss:   0.001046 => Gls Tokens per Sec:      773 || Batch Translation Loss:   0.045375 => Txt Tokens per Sec:     2111 || Lr: 0.000100
2024-02-10 01:56:52,842 Epoch 878: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-10 01:56:52,842 EPOCH 879
2024-02-10 01:57:08,870 Epoch 879: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.40 
2024-02-10 01:57:08,871 EPOCH 880
2024-02-10 01:57:24,992 Epoch 880: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-10 01:57:24,993 EPOCH 881
2024-02-10 01:57:40,628 Epoch 881: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.97 
2024-02-10 01:57:40,628 EPOCH 882
2024-02-10 01:57:56,931 Epoch 882: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-10 01:57:56,931 EPOCH 883
2024-02-10 01:58:12,904 Epoch 883: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.65 
2024-02-10 01:58:12,905 EPOCH 884
2024-02-10 01:58:28,988 Epoch 884: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.55 
2024-02-10 01:58:28,989 EPOCH 885
2024-02-10 01:58:45,085 Epoch 885: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-10 01:58:45,086 EPOCH 886
2024-02-10 01:59:01,114 Epoch 886: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-10 01:59:01,114 EPOCH 887
2024-02-10 01:59:17,623 Epoch 887: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-10 01:59:17,624 EPOCH 888
2024-02-10 01:59:33,855 Epoch 888: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-10 01:59:33,856 EPOCH 889
2024-02-10 01:59:49,784 [Epoch: 889 Step: 00008000] Batch Recognition Loss:   0.001074 => Gls Tokens per Sec:      586 || Batch Translation Loss:   0.219799 => Txt Tokens per Sec:     1641 || Lr: 0.000100
2024-02-10 02:01:01,213 Validation result at epoch 889, step     8000: duration: 71.4261s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.45098	Translation Loss: 90285.59375	PPL: 8248.09375
	Eval Metric: BLEU
	WER 5.37	(DEL: 0.00,	INS: 0.00,	SUB: 5.37)
	BLEU-4 0.70	(BLEU-1: 11.73,	BLEU-2: 3.81,	BLEU-3: 1.49,	BLEU-4: 0.70)
	CHRF 17.16	ROUGE 9.75
2024-02-10 02:01:01,215 Logging Recognition and Translation Outputs
2024-02-10 02:01:01,216 ========================================================================================================================
2024-02-10 02:01:01,216 Logging Sequence: 180_236.00
2024-02-10 02:01:01,216 	Gloss Reference :	A B+C+D+E
2024-02-10 02:01:01,216 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 02:01:01,216 	Gloss Alignment :	         
2024-02-10 02:01:01,217 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 02:01:01,217 	Text Reference  :	however the wrestlers returned to      the  protest site at   jantar   mantar  with thier     demands
2024-02-10 02:01:01,218 	Text Hypothesis :	******* *** ********* ******** earlier they were    a    huge argument however 9    wrestlers said   
2024-02-10 02:01:01,218 	Text Alignment  :	D       D   D         D        S       S    S       S    S    S        S       S    S         S      
2024-02-10 02:01:01,218 ========================================================================================================================
2024-02-10 02:01:01,218 Logging Sequence: 111_154.00
2024-02-10 02:01:01,218 	Gloss Reference :	A B+C+D+E  
2024-02-10 02:01:01,219 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-10 02:01:01,219 	Gloss Alignment :	  S        
2024-02-10 02:01:01,219 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 02:01:01,220 	Text Reference  :	***** *** due   to       csk's  slow over   rate dhoni   was fined rs 12 lakh *** **** *********
2024-02-10 02:01:01,220 	Text Hypothesis :	after the first instance during a    season the  captain is  fined rs 12 lakh for slow over-rate
2024-02-10 02:01:01,221 	Text Alignment  :	I     I   S     S        S      S    S      S    S       S                    I   I    I        
2024-02-10 02:01:01,221 ========================================================================================================================
2024-02-10 02:01:01,221 Logging Sequence: 118_314.00
2024-02-10 02:01:01,221 	Gloss Reference :	A B+C+D+E
2024-02-10 02:01:01,221 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 02:01:01,221 	Gloss Alignment :	         
2024-02-10 02:01:01,222 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 02:01:01,222 	Text Reference  :	wow even the president had    come to    watch
2024-02-10 02:01:01,222 	Text Hypothesis :	*** what a   proud     moment for  these match
2024-02-10 02:01:01,222 	Text Alignment  :	D   S    S   S         S      S    S     S    
2024-02-10 02:01:01,223 ========================================================================================================================
2024-02-10 02:01:01,223 Logging Sequence: 156_197.00
2024-02-10 02:01:01,223 	Gloss Reference :	A B+C+D+E
2024-02-10 02:01:01,223 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 02:01:01,223 	Gloss Alignment :	         
2024-02-10 02:01:01,223 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 02:01:01,225 	Text Reference  :	*** ******* ***** **** **** ******** *** seattle orcas sor  is owned by   many      investors including satya nadella microsoft ceo      
2024-02-10 02:01:01,225 	Text Hypothesis :	kkr batters could have many sponsors but at      the   2020 is ***** very different but       they      have  from    their     passports
2024-02-10 02:01:01,225 	Text Alignment  :	I   I       I     I    I    I        I   S       S     S       D     S    S         S         S         S     S       S         S        
2024-02-10 02:01:01,225 ========================================================================================================================
2024-02-10 02:01:01,225 Logging Sequence: 183_159.00
2024-02-10 02:01:01,226 	Gloss Reference :	A B+C+D+E
2024-02-10 02:01:01,226 	Gloss Hypothesis:	A B+C+D  
2024-02-10 02:01:01,226 	Gloss Alignment :	  S      
2024-02-10 02:01:01,226 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 02:01:01,228 	Text Reference  :	*** however an   exception to this     is   virat          kohli    and his wife anushka sharma who refuse to share images of    their daughter
2024-02-10 02:01:01,228 	Text Hypothesis :	the picture went viral     it received many congratulatory messages and his **** ******* ****** *** ****** ** ***** son    orion keech singh   
2024-02-10 02:01:01,228 	Text Alignment  :	I   S       S    S         S  S        S    S              S                D    D       D      D   D      D  D     S      S     S     S       
2024-02-10 02:01:01,228 ========================================================================================================================
2024-02-10 02:01:01,863 Epoch 889: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-10 02:01:01,863 EPOCH 890
2024-02-10 02:01:18,654 Epoch 890: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-10 02:01:18,654 EPOCH 891
2024-02-10 02:01:34,860 Epoch 891: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-10 02:01:34,860 EPOCH 892
2024-02-10 02:01:51,034 Epoch 892: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-10 02:01:51,034 EPOCH 893
2024-02-10 02:02:07,055 Epoch 893: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-10 02:02:07,056 EPOCH 894
2024-02-10 02:02:23,179 Epoch 894: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 02:02:23,180 EPOCH 895
2024-02-10 02:02:39,297 Epoch 895: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 02:02:39,298 EPOCH 896
2024-02-10 02:02:55,608 Epoch 896: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-10 02:02:55,609 EPOCH 897
2024-02-10 02:03:11,691 Epoch 897: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 02:03:11,692 EPOCH 898
2024-02-10 02:03:27,806 Epoch 898: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-10 02:03:27,807 EPOCH 899
2024-02-10 02:03:43,778 Epoch 899: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-10 02:03:43,778 EPOCH 900
2024-02-10 02:03:59,883 [Epoch: 900 Step: 00008100] Batch Recognition Loss:   0.000964 => Gls Tokens per Sec:      659 || Batch Translation Loss:   0.030999 => Txt Tokens per Sec:     1825 || Lr: 0.000100
2024-02-10 02:03:59,884 Epoch 900: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 02:03:59,884 EPOCH 901
2024-02-10 02:04:16,247 Epoch 901: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 02:04:16,248 EPOCH 902
2024-02-10 02:04:32,410 Epoch 902: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 02:04:32,411 EPOCH 903
2024-02-10 02:04:48,462 Epoch 903: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 02:04:48,462 EPOCH 904
2024-02-10 02:05:04,196 Epoch 904: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 02:05:04,196 EPOCH 905
2024-02-10 02:05:20,206 Epoch 905: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-10 02:05:20,206 EPOCH 906
2024-02-10 02:05:36,182 Epoch 906: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-10 02:05:36,182 EPOCH 907
2024-02-10 02:05:52,463 Epoch 907: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-10 02:05:52,464 EPOCH 908
2024-02-10 02:06:08,405 Epoch 908: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-10 02:06:08,406 EPOCH 909
2024-02-10 02:06:24,452 Epoch 909: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-10 02:06:24,453 EPOCH 910
2024-02-10 02:06:40,651 Epoch 910: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 02:06:40,651 EPOCH 911
2024-02-10 02:06:56,774 Epoch 911: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 02:06:56,774 EPOCH 912
2024-02-10 02:06:57,145 [Epoch: 912 Step: 00008200] Batch Recognition Loss:   0.000757 => Gls Tokens per Sec:     3459 || Batch Translation Loss:   0.031619 => Txt Tokens per Sec:     9432 || Lr: 0.000100
2024-02-10 02:07:12,831 Epoch 912: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 02:07:12,831 EPOCH 913
2024-02-10 02:07:28,988 Epoch 913: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 02:07:28,989 EPOCH 914
2024-02-10 02:07:44,988 Epoch 914: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 02:07:44,988 EPOCH 915
2024-02-10 02:08:01,027 Epoch 915: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 02:08:01,027 EPOCH 916
2024-02-10 02:08:17,231 Epoch 916: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 02:08:17,232 EPOCH 917
2024-02-10 02:08:33,200 Epoch 917: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 02:08:33,200 EPOCH 918
2024-02-10 02:08:49,510 Epoch 918: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 02:08:49,510 EPOCH 919
2024-02-10 02:09:05,516 Epoch 919: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 02:09:05,516 EPOCH 920
2024-02-10 02:09:21,533 Epoch 920: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 02:09:21,534 EPOCH 921
2024-02-10 02:09:37,806 Epoch 921: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 02:09:37,806 EPOCH 922
2024-02-10 02:09:53,752 Epoch 922: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-10 02:09:53,753 EPOCH 923
2024-02-10 02:09:54,953 [Epoch: 923 Step: 00008300] Batch Recognition Loss:   0.000369 => Gls Tokens per Sec:     2135 || Batch Translation Loss:   0.030504 => Txt Tokens per Sec:     6210 || Lr: 0.000100
2024-02-10 02:10:09,705 Epoch 923: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 02:10:09,705 EPOCH 924
2024-02-10 02:10:25,862 Epoch 924: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 02:10:25,863 EPOCH 925
2024-02-10 02:10:42,226 Epoch 925: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 02:10:42,226 EPOCH 926
2024-02-10 02:10:58,514 Epoch 926: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 02:10:58,515 EPOCH 927
2024-02-10 02:11:14,638 Epoch 927: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 02:11:14,639 EPOCH 928
2024-02-10 02:11:30,789 Epoch 928: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-10 02:11:30,790 EPOCH 929
2024-02-10 02:11:46,934 Epoch 929: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 02:11:46,935 EPOCH 930
2024-02-10 02:12:02,941 Epoch 930: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 02:12:02,941 EPOCH 931
2024-02-10 02:12:18,668 Epoch 931: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 02:12:18,669 EPOCH 932
2024-02-10 02:12:34,845 Epoch 932: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-10 02:12:34,846 EPOCH 933
2024-02-10 02:12:51,038 Epoch 933: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 02:12:51,038 EPOCH 934
2024-02-10 02:13:00,479 [Epoch: 934 Step: 00008400] Batch Recognition Loss:   0.000558 => Gls Tokens per Sec:      407 || Batch Translation Loss:   0.025036 => Txt Tokens per Sec:     1202 || Lr: 0.000100
2024-02-10 02:13:07,296 Epoch 934: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 02:13:07,296 EPOCH 935
2024-02-10 02:13:23,356 Epoch 935: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 02:13:23,357 EPOCH 936
2024-02-10 02:13:39,439 Epoch 936: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 02:13:39,439 EPOCH 937
2024-02-10 02:13:55,486 Epoch 937: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 02:13:55,487 EPOCH 938
2024-02-10 02:14:11,559 Epoch 938: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-10 02:14:11,559 EPOCH 939
2024-02-10 02:14:27,737 Epoch 939: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-10 02:14:27,738 EPOCH 940
2024-02-10 02:14:43,733 Epoch 940: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-10 02:14:43,733 EPOCH 941
2024-02-10 02:15:00,204 Epoch 941: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-10 02:15:00,204 EPOCH 942
2024-02-10 02:15:16,099 Epoch 942: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-10 02:15:16,099 EPOCH 943
2024-02-10 02:15:32,034 Epoch 943: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-10 02:15:32,035 EPOCH 944
2024-02-10 02:15:48,493 Epoch 944: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-10 02:15:48,494 EPOCH 945
2024-02-10 02:15:59,504 [Epoch: 945 Step: 00008500] Batch Recognition Loss:   0.001538 => Gls Tokens per Sec:      383 || Batch Translation Loss:   0.052642 => Txt Tokens per Sec:     1120 || Lr: 0.000100
2024-02-10 02:16:04,556 Epoch 945: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-10 02:16:04,556 EPOCH 946
2024-02-10 02:16:20,291 Epoch 946: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-10 02:16:20,292 EPOCH 947
2024-02-10 02:16:36,366 Epoch 947: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-10 02:16:36,367 EPOCH 948
2024-02-10 02:16:52,391 Epoch 948: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-10 02:16:52,392 EPOCH 949
2024-02-10 02:17:08,222 Epoch 949: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-10 02:17:08,222 EPOCH 950
2024-02-10 02:17:24,315 Epoch 950: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-10 02:17:24,315 EPOCH 951
2024-02-10 02:17:40,546 Epoch 951: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-10 02:17:40,547 EPOCH 952
2024-02-10 02:17:57,096 Epoch 952: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-10 02:17:57,096 EPOCH 953
2024-02-10 02:18:13,213 Epoch 953: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-10 02:18:13,213 EPOCH 954
2024-02-10 02:18:29,315 Epoch 954: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-10 02:18:29,315 EPOCH 955
2024-02-10 02:18:45,590 Epoch 955: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-10 02:18:45,591 EPOCH 956
2024-02-10 02:18:54,144 [Epoch: 956 Step: 00008600] Batch Recognition Loss:   0.001128 => Gls Tokens per Sec:      643 || Batch Translation Loss:   0.042507 => Txt Tokens per Sec:     1620 || Lr: 0.000100
2024-02-10 02:19:01,643 Epoch 956: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-10 02:19:01,643 EPOCH 957
2024-02-10 02:19:17,781 Epoch 957: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-10 02:19:17,782 EPOCH 958
2024-02-10 02:19:34,192 Epoch 958: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-10 02:19:34,193 EPOCH 959
2024-02-10 02:19:51,112 Epoch 959: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-10 02:19:51,113 EPOCH 960
2024-02-10 02:20:07,449 Epoch 960: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-10 02:20:07,450 EPOCH 961
2024-02-10 02:20:23,453 Epoch 961: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-10 02:20:23,454 EPOCH 962
2024-02-10 02:20:39,729 Epoch 962: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-10 02:20:39,729 EPOCH 963
2024-02-10 02:20:55,937 Epoch 963: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-10 02:20:55,937 EPOCH 964
2024-02-10 02:21:11,966 Epoch 964: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-10 02:21:11,966 EPOCH 965
2024-02-10 02:21:27,952 Epoch 965: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-10 02:21:27,952 EPOCH 966
2024-02-10 02:21:43,970 Epoch 966: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-10 02:21:43,971 EPOCH 967
2024-02-10 02:21:53,325 [Epoch: 967 Step: 00008700] Batch Recognition Loss:   0.003190 => Gls Tokens per Sec:      725 || Batch Translation Loss:   0.066848 => Txt Tokens per Sec:     1970 || Lr: 0.000100
2024-02-10 02:22:00,127 Epoch 967: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-10 02:22:00,128 EPOCH 968
2024-02-10 02:22:15,961 Epoch 968: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-10 02:22:15,962 EPOCH 969
2024-02-10 02:22:32,168 Epoch 969: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-10 02:22:32,169 EPOCH 970
2024-02-10 02:22:48,203 Epoch 970: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-10 02:22:48,204 EPOCH 971
2024-02-10 02:23:04,351 Epoch 971: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-10 02:23:04,352 EPOCH 972
2024-02-10 02:23:20,404 Epoch 972: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 02:23:20,404 EPOCH 973
2024-02-10 02:23:36,581 Epoch 973: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 02:23:36,582 EPOCH 974
2024-02-10 02:23:52,727 Epoch 974: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-10 02:23:52,728 EPOCH 975
2024-02-10 02:24:08,898 Epoch 975: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-10 02:24:08,898 EPOCH 976
2024-02-10 02:24:25,028 Epoch 976: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-10 02:24:25,028 EPOCH 977
2024-02-10 02:24:41,346 Epoch 977: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.59 
2024-02-10 02:24:41,347 EPOCH 978
2024-02-10 02:24:56,389 [Epoch: 978 Step: 00008800] Batch Recognition Loss:   0.009945 => Gls Tokens per Sec:      536 || Batch Translation Loss:   4.887678 => Txt Tokens per Sec:     1502 || Lr: 0.000100
2024-02-10 02:24:57,235 Epoch 978: Total Training Recognition Loss 0.02  Total Training Translation Loss 12.59 
2024-02-10 02:24:57,235 EPOCH 979
2024-02-10 02:25:13,020 Epoch 979: Total Training Recognition Loss 0.07  Total Training Translation Loss 6.86 
2024-02-10 02:25:13,021 EPOCH 980
2024-02-10 02:25:29,572 Epoch 980: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.66 
2024-02-10 02:25:29,573 EPOCH 981
2024-02-10 02:25:45,769 Epoch 981: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.24 
2024-02-10 02:25:45,770 EPOCH 982
2024-02-10 02:26:02,077 Epoch 982: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.49 
2024-02-10 02:26:02,078 EPOCH 983
2024-02-10 02:26:18,220 Epoch 983: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.05 
2024-02-10 02:26:18,220 EPOCH 984
2024-02-10 02:26:34,160 Epoch 984: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.81 
2024-02-10 02:26:34,160 EPOCH 985
2024-02-10 02:26:50,253 Epoch 985: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-10 02:26:50,253 EPOCH 986
2024-02-10 02:27:06,443 Epoch 986: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.52 
2024-02-10 02:27:06,444 EPOCH 987
2024-02-10 02:27:22,624 Epoch 987: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.42 
2024-02-10 02:27:22,625 EPOCH 988
2024-02-10 02:27:38,292 Epoch 988: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.40 
2024-02-10 02:27:38,292 EPOCH 989
2024-02-10 02:27:49,993 [Epoch: 989 Step: 00008900] Batch Recognition Loss:   0.002391 => Gls Tokens per Sec:      875 || Batch Translation Loss:   0.038241 => Txt Tokens per Sec:     2396 || Lr: 0.000100
2024-02-10 02:27:54,283 Epoch 989: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.35 
2024-02-10 02:27:54,284 EPOCH 990
2024-02-10 02:28:10,462 Epoch 990: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.34 
2024-02-10 02:28:10,462 EPOCH 991
2024-02-10 02:28:26,384 Epoch 991: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-10 02:28:26,384 EPOCH 992
2024-02-10 02:28:42,262 Epoch 992: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.32 
2024-02-10 02:28:42,262 EPOCH 993
2024-02-10 02:28:58,809 Epoch 993: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-10 02:28:58,810 EPOCH 994
2024-02-10 02:29:14,889 Epoch 994: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.30 
2024-02-10 02:29:14,890 EPOCH 995
2024-02-10 02:29:30,658 Epoch 995: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 02:29:30,659 EPOCH 996
2024-02-10 02:29:47,219 Epoch 996: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 02:29:47,220 EPOCH 997
2024-02-10 02:30:03,190 Epoch 997: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 02:30:03,190 EPOCH 998
2024-02-10 02:30:19,676 Epoch 998: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 02:30:19,676 EPOCH 999
2024-02-10 02:30:36,042 Epoch 999: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-10 02:30:36,043 EPOCH 1000
2024-02-10 02:30:51,909 [Epoch: 1000 Step: 00009000] Batch Recognition Loss:   0.000980 => Gls Tokens per Sec:      669 || Batch Translation Loss:   0.041056 => Txt Tokens per Sec:     1852 || Lr: 0.000100
2024-02-10 02:30:51,909 Epoch 1000: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.38 
2024-02-10 02:30:51,909 EPOCH 1001
2024-02-10 02:31:07,943 Epoch 1001: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.26 
2024-02-10 02:31:07,943 EPOCH 1002
2024-02-10 02:31:24,224 Epoch 1002: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 02:31:24,224 EPOCH 1003
2024-02-10 02:31:40,341 Epoch 1003: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 02:31:40,342 EPOCH 1004
2024-02-10 02:31:56,728 Epoch 1004: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 02:31:56,728 EPOCH 1005
2024-02-10 02:32:12,927 Epoch 1005: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 02:32:12,928 EPOCH 1006
2024-02-10 02:32:29,108 Epoch 1006: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 02:32:29,108 EPOCH 1007
2024-02-10 02:32:44,895 Epoch 1007: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 02:32:44,896 EPOCH 1008
2024-02-10 02:33:00,969 Epoch 1008: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 02:33:00,970 EPOCH 1009
2024-02-10 02:33:16,987 Epoch 1009: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 02:33:16,988 EPOCH 1010
2024-02-10 02:33:33,011 Epoch 1010: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 02:33:33,012 EPOCH 1011
2024-02-10 02:33:49,102 Epoch 1011: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 02:33:49,103 EPOCH 1012
2024-02-10 02:33:49,400 [Epoch: 1012 Step: 00009100] Batch Recognition Loss:   0.000571 => Gls Tokens per Sec:     4324 || Batch Translation Loss:   0.031917 => Txt Tokens per Sec:     9625 || Lr: 0.000100
2024-02-10 02:34:05,240 Epoch 1012: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 02:34:05,241 EPOCH 1013
2024-02-10 02:34:21,282 Epoch 1013: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 02:34:21,283 EPOCH 1014
2024-02-10 02:34:37,512 Epoch 1014: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 02:34:37,513 EPOCH 1015
2024-02-10 02:34:53,868 Epoch 1015: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 02:34:53,869 EPOCH 1016
2024-02-10 02:35:09,836 Epoch 1016: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 02:35:09,837 EPOCH 1017
2024-02-10 02:35:26,335 Epoch 1017: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 02:35:26,335 EPOCH 1018
2024-02-10 02:35:42,263 Epoch 1018: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 02:35:42,264 EPOCH 1019
2024-02-10 02:35:58,511 Epoch 1019: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 02:35:58,511 EPOCH 1020
2024-02-10 02:36:14,540 Epoch 1020: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 02:36:14,541 EPOCH 1021
2024-02-10 02:36:31,024 Epoch 1021: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 02:36:31,026 EPOCH 1022
2024-02-10 02:36:47,243 Epoch 1022: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 02:36:47,243 EPOCH 1023
2024-02-10 02:36:48,255 [Epoch: 1023 Step: 00009200] Batch Recognition Loss:   0.000420 => Gls Tokens per Sec:     2532 || Batch Translation Loss:   0.017326 => Txt Tokens per Sec:     6590 || Lr: 0.000100
2024-02-10 02:37:03,170 Epoch 1023: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 02:37:03,170 EPOCH 1024
2024-02-10 02:37:19,331 Epoch 1024: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 02:37:19,331 EPOCH 1025
2024-02-10 02:37:35,701 Epoch 1025: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 02:37:35,702 EPOCH 1026
2024-02-10 02:37:51,828 Epoch 1026: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 02:37:51,828 EPOCH 1027
2024-02-10 02:38:07,808 Epoch 1027: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 02:38:07,809 EPOCH 1028
2024-02-10 02:38:24,125 Epoch 1028: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 02:38:24,126 EPOCH 1029
2024-02-10 02:38:40,272 Epoch 1029: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 02:38:40,273 EPOCH 1030
2024-02-10 02:38:56,603 Epoch 1030: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 02:38:56,603 EPOCH 1031
2024-02-10 02:39:12,796 Epoch 1031: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 02:39:12,797 EPOCH 1032
2024-02-10 02:39:28,518 Epoch 1032: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 02:39:28,519 EPOCH 1033
2024-02-10 02:39:44,630 Epoch 1033: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 02:39:44,631 EPOCH 1034
2024-02-10 02:39:51,320 [Epoch: 1034 Step: 00009300] Batch Recognition Loss:   0.000255 => Gls Tokens per Sec:      574 || Batch Translation Loss:   0.030168 => Txt Tokens per Sec:     1703 || Lr: 0.000100
2024-02-10 02:40:00,697 Epoch 1034: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 02:40:00,698 EPOCH 1035
2024-02-10 02:40:16,851 Epoch 1035: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 02:40:16,852 EPOCH 1036
2024-02-10 02:40:33,095 Epoch 1036: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 02:40:33,095 EPOCH 1037
2024-02-10 02:40:49,080 Epoch 1037: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 02:40:49,081 EPOCH 1038
2024-02-10 02:41:05,147 Epoch 1038: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 02:41:05,148 EPOCH 1039
2024-02-10 02:41:21,465 Epoch 1039: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 02:41:21,465 EPOCH 1040
2024-02-10 02:41:37,478 Epoch 1040: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 02:41:37,479 EPOCH 1041
2024-02-10 02:41:53,748 Epoch 1041: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 02:41:53,748 EPOCH 1042
2024-02-10 02:42:10,159 Epoch 1042: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 02:42:10,160 EPOCH 1043
2024-02-10 02:42:26,288 Epoch 1043: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 02:42:26,288 EPOCH 1044
2024-02-10 02:42:42,693 Epoch 1044: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 02:42:42,693 EPOCH 1045
2024-02-10 02:42:56,333 [Epoch: 1045 Step: 00009400] Batch Recognition Loss:   0.000548 => Gls Tokens per Sec:      309 || Batch Translation Loss:   0.023785 => Txt Tokens per Sec:      893 || Lr: 0.000100
2024-02-10 02:42:58,793 Epoch 1045: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 02:42:58,793 EPOCH 1046
2024-02-10 02:43:14,808 Epoch 1046: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-10 02:43:14,808 EPOCH 1047
2024-02-10 02:43:31,230 Epoch 1047: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 02:43:31,231 EPOCH 1048
2024-02-10 02:43:47,524 Epoch 1048: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 02:43:47,524 EPOCH 1049
2024-02-10 02:44:03,701 Epoch 1049: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 02:44:03,702 EPOCH 1050
2024-02-10 02:44:19,771 Epoch 1050: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 02:44:19,772 EPOCH 1051
2024-02-10 02:44:36,142 Epoch 1051: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 02:44:36,142 EPOCH 1052
2024-02-10 02:44:52,662 Epoch 1052: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 02:44:52,663 EPOCH 1053
2024-02-10 02:45:08,833 Epoch 1053: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 02:45:08,834 EPOCH 1054
2024-02-10 02:45:24,790 Epoch 1054: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 02:45:24,791 EPOCH 1055
2024-02-10 02:45:40,711 Epoch 1055: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 02:45:40,711 EPOCH 1056
2024-02-10 02:45:48,063 [Epoch: 1056 Step: 00009500] Batch Recognition Loss:   0.000953 => Gls Tokens per Sec:      871 || Batch Translation Loss:   0.012926 => Txt Tokens per Sec:     2344 || Lr: 0.000100
2024-02-10 02:45:56,771 Epoch 1056: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 02:45:56,771 EPOCH 1057
2024-02-10 02:46:12,733 Epoch 1057: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 02:46:12,733 EPOCH 1058
2024-02-10 02:46:28,679 Epoch 1058: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-10 02:46:28,679 EPOCH 1059
2024-02-10 02:46:44,917 Epoch 1059: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 02:46:44,917 EPOCH 1060
2024-02-10 02:47:01,194 Epoch 1060: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-10 02:47:01,195 EPOCH 1061
2024-02-10 02:47:17,283 Epoch 1061: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-10 02:47:17,284 EPOCH 1062
2024-02-10 02:47:33,435 Epoch 1062: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.49 
2024-02-10 02:47:33,435 EPOCH 1063
2024-02-10 02:47:49,386 Epoch 1063: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-10 02:47:49,386 EPOCH 1064
2024-02-10 02:48:05,692 Epoch 1064: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-10 02:48:05,693 EPOCH 1065
2024-02-10 02:48:22,074 Epoch 1065: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-10 02:48:22,075 EPOCH 1066
2024-02-10 02:48:38,320 Epoch 1066: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-10 02:48:38,321 EPOCH 1067
2024-02-10 02:48:49,988 [Epoch: 1067 Step: 00009600] Batch Recognition Loss:   0.001570 => Gls Tokens per Sec:      581 || Batch Translation Loss:   0.298906 => Txt Tokens per Sec:     1532 || Lr: 0.000100
2024-02-10 02:48:54,496 Epoch 1067: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.17 
2024-02-10 02:48:54,497 EPOCH 1068
2024-02-10 02:49:10,732 Epoch 1068: Total Training Recognition Loss 0.02  Total Training Translation Loss 6.59 
2024-02-10 02:49:10,733 EPOCH 1069
2024-02-10 02:49:26,857 Epoch 1069: Total Training Recognition Loss 0.05  Total Training Translation Loss 7.64 
2024-02-10 02:49:26,857 EPOCH 1070
2024-02-10 02:49:42,782 Epoch 1070: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.15 
2024-02-10 02:49:42,783 EPOCH 1071
2024-02-10 02:49:58,756 Epoch 1071: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.20 
2024-02-10 02:49:58,757 EPOCH 1072
2024-02-10 02:50:15,090 Epoch 1072: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.90 
2024-02-10 02:50:15,090 EPOCH 1073
2024-02-10 02:50:30,971 Epoch 1073: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.30 
2024-02-10 02:50:30,971 EPOCH 1074
2024-02-10 02:50:46,794 Epoch 1074: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.91 
2024-02-10 02:50:46,795 EPOCH 1075
2024-02-10 02:51:02,835 Epoch 1075: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.58 
2024-02-10 02:51:02,835 EPOCH 1076
2024-02-10 02:51:19,101 Epoch 1076: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-10 02:51:19,101 EPOCH 1077
2024-02-10 02:51:34,978 Epoch 1077: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.41 
2024-02-10 02:51:34,978 EPOCH 1078
2024-02-10 02:51:49,904 [Epoch: 1078 Step: 00009700] Batch Recognition Loss:   0.001160 => Gls Tokens per Sec:      540 || Batch Translation Loss:   0.015539 => Txt Tokens per Sec:     1484 || Lr: 0.000100
2024-02-10 02:51:51,002 Epoch 1078: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.35 
2024-02-10 02:51:51,002 EPOCH 1079
2024-02-10 02:52:07,231 Epoch 1079: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-10 02:52:07,232 EPOCH 1080
2024-02-10 02:52:23,170 Epoch 1080: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 02:52:23,170 EPOCH 1081
2024-02-10 02:52:39,008 Epoch 1081: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 02:52:39,009 EPOCH 1082
2024-02-10 02:52:55,102 Epoch 1082: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 02:52:55,102 EPOCH 1083
2024-02-10 02:53:11,072 Epoch 1083: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 02:53:11,073 EPOCH 1084
2024-02-10 02:53:26,899 Epoch 1084: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 02:53:26,900 EPOCH 1085
2024-02-10 02:53:43,043 Epoch 1085: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-10 02:53:43,044 EPOCH 1086
2024-02-10 02:53:59,323 Epoch 1086: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 02:53:59,324 EPOCH 1087
2024-02-10 02:54:15,317 Epoch 1087: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-10 02:54:15,317 EPOCH 1088
2024-02-10 02:54:31,537 Epoch 1088: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 02:54:31,537 EPOCH 1089
2024-02-10 02:54:47,131 [Epoch: 1089 Step: 00009800] Batch Recognition Loss:   0.000922 => Gls Tokens per Sec:      599 || Batch Translation Loss:   0.014279 => Txt Tokens per Sec:     1646 || Lr: 0.000100
2024-02-10 02:54:47,770 Epoch 1089: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 02:54:47,770 EPOCH 1090
2024-02-10 02:55:03,804 Epoch 1090: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-10 02:55:03,804 EPOCH 1091
2024-02-10 02:55:19,854 Epoch 1091: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 02:55:19,854 EPOCH 1092
2024-02-10 02:55:36,196 Epoch 1092: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 02:55:36,197 EPOCH 1093
2024-02-10 02:55:52,254 Epoch 1093: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 02:55:52,254 EPOCH 1094
2024-02-10 02:56:08,409 Epoch 1094: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 02:56:08,410 EPOCH 1095
2024-02-10 02:56:24,457 Epoch 1095: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 02:56:24,458 EPOCH 1096
2024-02-10 02:56:40,648 Epoch 1096: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-10 02:56:40,648 EPOCH 1097
2024-02-10 02:56:56,555 Epoch 1097: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 02:56:56,556 EPOCH 1098
2024-02-10 02:57:12,483 Epoch 1098: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 02:57:12,483 EPOCH 1099
2024-02-10 02:57:28,570 Epoch 1099: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 02:57:28,571 EPOCH 1100
2024-02-10 02:57:44,593 [Epoch: 1100 Step: 00009900] Batch Recognition Loss:   0.000461 => Gls Tokens per Sec:      663 || Batch Translation Loss:   0.023864 => Txt Tokens per Sec:     1834 || Lr: 0.000100
2024-02-10 02:57:44,594 Epoch 1100: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 02:57:44,594 EPOCH 1101
2024-02-10 02:58:00,337 Epoch 1101: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 02:58:00,337 EPOCH 1102
2024-02-10 02:58:16,487 Epoch 1102: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 02:58:16,488 EPOCH 1103
2024-02-10 02:58:32,721 Epoch 1103: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 02:58:32,721 EPOCH 1104
2024-02-10 02:58:48,608 Epoch 1104: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 02:58:48,608 EPOCH 1105
2024-02-10 02:59:04,701 Epoch 1105: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 02:59:04,701 EPOCH 1106
2024-02-10 02:59:21,133 Epoch 1106: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 02:59:21,134 EPOCH 1107
2024-02-10 02:59:37,153 Epoch 1107: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 02:59:37,154 EPOCH 1108
2024-02-10 02:59:53,328 Epoch 1108: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 02:59:53,329 EPOCH 1109
2024-02-10 03:00:09,552 Epoch 1109: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 03:00:09,552 EPOCH 1110
2024-02-10 03:00:25,735 Epoch 1110: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 03:00:25,736 EPOCH 1111
2024-02-10 03:00:41,342 Epoch 1111: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 03:00:41,343 EPOCH 1112
2024-02-10 03:00:41,955 [Epoch: 1112 Step: 00010000] Batch Recognition Loss:   0.000558 => Gls Tokens per Sec:     2095 || Batch Translation Loss:   0.022343 => Txt Tokens per Sec:     5776 || Lr: 0.000100
2024-02-10 03:01:53,837 Validation result at epoch 1112, step    10000: duration: 71.8807s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.53727	Translation Loss: 90832.82031	PPL: 8711.46582
	Eval Metric: BLEU
	WER 5.44	(DEL: 0.00,	INS: 0.00,	SUB: 5.44)
	BLEU-4 0.45	(BLEU-1: 11.93,	BLEU-2: 3.95,	BLEU-3: 1.44,	BLEU-4: 0.45)
	CHRF 17.65	ROUGE 10.20
2024-02-10 03:01:53,840 Logging Recognition and Translation Outputs
2024-02-10 03:01:53,840 ========================================================================================================================
2024-02-10 03:01:53,840 Logging Sequence: 123_147.00
2024-02-10 03:01:53,841 	Gloss Reference :	A B+C+D+E
2024-02-10 03:01:53,841 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 03:01:53,841 	Gloss Alignment :	         
2024-02-10 03:01:53,841 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 03:01:53,842 	Text Reference  :	the former captain also owns the pontiac firebird trans am  car worth    rs         68 lakh    
2024-02-10 03:01:53,842 	Text Hypothesis :	*** ****** ******* **** **** *** ******* ******** dhoni was a   stunning collection of vehicles
2024-02-10 03:01:53,842 	Text Alignment  :	D   D      D       D    D    D   D       D        S     S   S   S        S          S  S       
2024-02-10 03:01:53,842 ========================================================================================================================
2024-02-10 03:01:53,842 Logging Sequence: 58_196.00
2024-02-10 03:01:53,843 	Gloss Reference :	A B+C+D+E
2024-02-10 03:01:53,843 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 03:01:53,843 	Gloss Alignment :	         
2024-02-10 03:01:53,843 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 03:01:53,844 	Text Reference  :	the ***** *** talents   and   skills of our  athletes knows no    bounds
2024-02-10 03:01:53,844 	Text Hypothesis :	the games are currently being held   in many medals   in    tokyo games 
2024-02-10 03:01:53,844 	Text Alignment  :	    I     I   S         S     S      S  S    S        S     S     S     
2024-02-10 03:01:53,844 ========================================================================================================================
2024-02-10 03:01:53,844 Logging Sequence: 168_184.00
2024-02-10 03:01:53,845 	Gloss Reference :	A B+C+D+E    
2024-02-10 03:01:53,845 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-10 03:01:53,845 	Gloss Alignment :	  S          
2024-02-10 03:01:53,845 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 03:01:53,847 	Text Reference  :	people say  that we    may get     a  true glimpse of    vamika in   february 2022  when    she turns 1  year old 
2024-02-10 03:01:53,847 	Text Hypothesis :	****** they have asked the threats as they have    truly learnt from other    celeb parents and focus on his  team
2024-02-10 03:01:53,847 	Text Alignment  :	D      S    S    S     S   S       S  S    S       S     S      S    S        S     S       S   S     S  S    S   
2024-02-10 03:01:53,848 ========================================================================================================================
2024-02-10 03:01:53,848 Logging Sequence: 87_123.00
2024-02-10 03:01:53,848 	Gloss Reference :	A B+C+D+E
2024-02-10 03:01:53,848 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 03:01:53,848 	Gloss Alignment :	         
2024-02-10 03:01:53,848 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 03:01:53,849 	Text Reference  :	he said that he hoped   kl  rahul would   be fit   for     the upcoming world  cup 
2024-02-10 03:01:53,849 	Text Hypothesis :	** **** **** ** gambhir has been  accused of being jealous of  his      kohli' rise
2024-02-10 03:01:53,850 	Text Alignment  :	D  D    D    D  S       S   S     S       S  S     S       S   S        S      S   
2024-02-10 03:01:53,850 ========================================================================================================================
2024-02-10 03:01:53,850 Logging Sequence: 144_154.00
2024-02-10 03:01:53,850 	Gloss Reference :	A B+C+D+E
2024-02-10 03:01:53,850 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 03:01:53,850 	Gloss Alignment :	         
2024-02-10 03:01:53,850 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 03:01:53,852 	Text Reference  :	*** ****** she   also participated in the rural olympic games organised in   rajasthan a    few months
2024-02-10 03:01:53,852 	Text Hypothesis :	the second match was  held         at the end   of      the   national  team 3         days for help  
2024-02-10 03:01:53,852 	Text Alignment  :	I   I      S     S    S            S      S     S       S     S         S    S         S    S   S     
2024-02-10 03:01:53,852 ========================================================================================================================
2024-02-10 03:02:10,286 Epoch 1112: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 03:02:10,287 EPOCH 1113
2024-02-10 03:02:26,396 Epoch 1113: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 03:02:26,397 EPOCH 1114
2024-02-10 03:02:42,423 Epoch 1114: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 03:02:42,424 EPOCH 1115
2024-02-10 03:02:58,712 Epoch 1115: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 03:02:58,713 EPOCH 1116
2024-02-10 03:03:14,754 Epoch 1116: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 03:03:14,754 EPOCH 1117
2024-02-10 03:03:30,982 Epoch 1117: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 03:03:30,983 EPOCH 1118
2024-02-10 03:03:46,799 Epoch 1118: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 03:03:46,800 EPOCH 1119
2024-02-10 03:04:02,958 Epoch 1119: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-10 03:04:02,958 EPOCH 1120
2024-02-10 03:04:18,916 Epoch 1120: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 03:04:18,916 EPOCH 1121
2024-02-10 03:04:34,808 Epoch 1121: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 03:04:34,809 EPOCH 1122
2024-02-10 03:04:51,216 Epoch 1122: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 03:04:51,217 EPOCH 1123
2024-02-10 03:04:55,872 [Epoch: 1123 Step: 00010100] Batch Recognition Loss:   0.000579 => Gls Tokens per Sec:      357 || Batch Translation Loss:   0.018691 => Txt Tokens per Sec:     1001 || Lr: 0.000100
2024-02-10 03:05:07,215 Epoch 1123: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 03:05:07,216 EPOCH 1124
2024-02-10 03:05:23,034 Epoch 1124: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 03:05:23,034 EPOCH 1125
2024-02-10 03:05:38,921 Epoch 1125: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 03:05:38,922 EPOCH 1126
2024-02-10 03:05:55,110 Epoch 1126: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 03:05:55,111 EPOCH 1127
2024-02-10 03:06:10,982 Epoch 1127: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-10 03:06:10,982 EPOCH 1128
2024-02-10 03:06:27,410 Epoch 1128: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-10 03:06:27,411 EPOCH 1129
2024-02-10 03:06:45,475 Epoch 1129: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 03:06:45,476 EPOCH 1130
2024-02-10 03:07:01,962 Epoch 1130: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 03:07:01,963 EPOCH 1131
2024-02-10 03:07:17,984 Epoch 1131: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 03:07:17,985 EPOCH 1132
2024-02-10 03:07:33,797 Epoch 1132: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 03:07:33,798 EPOCH 1133
2024-02-10 03:07:49,887 Epoch 1133: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 03:07:49,888 EPOCH 1134
2024-02-10 03:07:51,313 [Epoch: 1134 Step: 00010200] Batch Recognition Loss:   0.001014 => Gls Tokens per Sec:     2695 || Batch Translation Loss:   0.024344 => Txt Tokens per Sec:     6913 || Lr: 0.000100
2024-02-10 03:08:05,870 Epoch 1134: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 03:08:05,871 EPOCH 1135
2024-02-10 03:08:21,834 Epoch 1135: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 03:08:21,835 EPOCH 1136
2024-02-10 03:08:38,121 Epoch 1136: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 03:08:38,122 EPOCH 1137
2024-02-10 03:08:54,236 Epoch 1137: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 03:08:54,237 EPOCH 1138
2024-02-10 03:09:10,536 Epoch 1138: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 03:09:10,536 EPOCH 1139
2024-02-10 03:09:26,559 Epoch 1139: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 03:09:26,559 EPOCH 1140
2024-02-10 03:09:42,534 Epoch 1140: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 03:09:42,534 EPOCH 1141
2024-02-10 03:09:58,740 Epoch 1141: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 03:09:58,741 EPOCH 1142
2024-02-10 03:10:15,166 Epoch 1142: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 03:10:15,167 EPOCH 1143
2024-02-10 03:10:31,254 Epoch 1143: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 03:10:31,256 EPOCH 1144
2024-02-10 03:10:47,518 Epoch 1144: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 03:10:47,518 EPOCH 1145
2024-02-10 03:10:49,186 [Epoch: 1145 Step: 00010300] Batch Recognition Loss:   0.000748 => Gls Tokens per Sec:     3071 || Batch Translation Loss:   0.012314 => Txt Tokens per Sec:     7631 || Lr: 0.000100
2024-02-10 03:11:03,307 Epoch 1145: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-10 03:11:03,308 EPOCH 1146
2024-02-10 03:11:19,386 Epoch 1146: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-10 03:11:19,386 EPOCH 1147
2024-02-10 03:11:35,349 Epoch 1147: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.93 
2024-02-10 03:11:35,350 EPOCH 1148
2024-02-10 03:11:51,607 Epoch 1148: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.01 
2024-02-10 03:11:51,608 EPOCH 1149
2024-02-10 03:12:07,614 Epoch 1149: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-10 03:12:07,615 EPOCH 1150
2024-02-10 03:12:24,192 Epoch 1150: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-10 03:12:24,193 EPOCH 1151
2024-02-10 03:12:40,120 Epoch 1151: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-10 03:12:40,120 EPOCH 1152
2024-02-10 03:12:56,102 Epoch 1152: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-10 03:12:56,103 EPOCH 1153
2024-02-10 03:13:12,292 Epoch 1153: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-10 03:13:12,293 EPOCH 1154
2024-02-10 03:13:28,225 Epoch 1154: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-10 03:13:28,226 EPOCH 1155
2024-02-10 03:13:44,480 Epoch 1155: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-10 03:13:44,480 EPOCH 1156
2024-02-10 03:13:50,761 [Epoch: 1156 Step: 00010400] Batch Recognition Loss:   0.000553 => Gls Tokens per Sec:      876 || Batch Translation Loss:   0.017076 => Txt Tokens per Sec:     2312 || Lr: 0.000100
2024-02-10 03:14:00,513 Epoch 1156: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 03:14:00,513 EPOCH 1157
2024-02-10 03:14:16,974 Epoch 1157: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 03:14:16,974 EPOCH 1158
2024-02-10 03:14:33,102 Epoch 1158: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 03:14:33,103 EPOCH 1159
2024-02-10 03:14:49,285 Epoch 1159: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 03:14:49,286 EPOCH 1160
2024-02-10 03:15:05,251 Epoch 1160: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-10 03:15:05,251 EPOCH 1161
2024-02-10 03:15:21,010 Epoch 1161: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-10 03:15:21,011 EPOCH 1162
2024-02-10 03:15:37,018 Epoch 1162: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 03:15:37,019 EPOCH 1163
2024-02-10 03:15:52,994 Epoch 1163: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 03:15:52,994 EPOCH 1164
2024-02-10 03:16:08,960 Epoch 1164: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 03:16:08,961 EPOCH 1165
2024-02-10 03:16:24,959 Epoch 1165: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 03:16:24,959 EPOCH 1166
2024-02-10 03:16:40,437 Epoch 1166: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 03:16:40,438 EPOCH 1167
2024-02-10 03:16:50,015 [Epoch: 1167 Step: 00010500] Batch Recognition Loss:   0.000259 => Gls Tokens per Sec:      708 || Batch Translation Loss:   0.011218 => Txt Tokens per Sec:     2002 || Lr: 0.000100
2024-02-10 03:16:56,763 Epoch 1167: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 03:16:56,763 EPOCH 1168
2024-02-10 03:17:12,902 Epoch 1168: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 03:17:12,903 EPOCH 1169
2024-02-10 03:17:29,228 Epoch 1169: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 03:17:29,228 EPOCH 1170
2024-02-10 03:17:45,209 Epoch 1170: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 03:17:45,210 EPOCH 1171
2024-02-10 03:18:01,178 Epoch 1171: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 03:18:01,178 EPOCH 1172
2024-02-10 03:18:17,305 Epoch 1172: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 03:18:17,306 EPOCH 1173
2024-02-10 03:18:33,347 Epoch 1173: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 03:18:33,348 EPOCH 1174
2024-02-10 03:18:49,411 Epoch 1174: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 03:18:49,411 EPOCH 1175
2024-02-10 03:19:05,772 Epoch 1175: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-10 03:19:05,773 EPOCH 1176
2024-02-10 03:19:21,748 Epoch 1176: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-10 03:19:21,748 EPOCH 1177
2024-02-10 03:19:37,559 Epoch 1177: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-10 03:19:37,560 EPOCH 1178
2024-02-10 03:19:53,936 [Epoch: 1178 Step: 00010600] Batch Recognition Loss:   0.000553 => Gls Tokens per Sec:      492 || Batch Translation Loss:   0.069205 => Txt Tokens per Sec:     1480 || Lr: 0.000100
2024-02-10 03:19:54,700 Epoch 1178: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-10 03:19:54,701 EPOCH 1179
2024-02-10 03:20:10,785 Epoch 1179: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-10 03:20:10,785 EPOCH 1180
2024-02-10 03:20:26,756 Epoch 1180: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-10 03:20:26,756 EPOCH 1181
2024-02-10 03:20:42,958 Epoch 1181: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-10 03:20:42,959 EPOCH 1182
2024-02-10 03:20:59,220 Epoch 1182: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-10 03:20:59,221 EPOCH 1183
2024-02-10 03:21:15,435 Epoch 1183: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-10 03:21:15,436 EPOCH 1184
2024-02-10 03:21:31,358 Epoch 1184: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-10 03:21:31,358 EPOCH 1185
2024-02-10 03:21:47,754 Epoch 1185: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-10 03:21:47,755 EPOCH 1186
2024-02-10 03:22:03,815 Epoch 1186: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-10 03:22:03,816 EPOCH 1187
2024-02-10 03:22:19,911 Epoch 1187: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-10 03:22:19,912 EPOCH 1188
2024-02-10 03:22:36,247 Epoch 1188: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-10 03:22:36,247 EPOCH 1189
2024-02-10 03:22:51,418 [Epoch: 1189 Step: 00010700] Batch Recognition Loss:   0.002359 => Gls Tokens per Sec:      616 || Batch Translation Loss:   0.066778 => Txt Tokens per Sec:     1682 || Lr: 0.000100
2024-02-10 03:22:52,182 Epoch 1189: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-10 03:22:52,182 EPOCH 1190
2024-02-10 03:23:08,123 Epoch 1190: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-10 03:23:08,124 EPOCH 1191
2024-02-10 03:23:24,412 Epoch 1191: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-10 03:23:24,413 EPOCH 1192
2024-02-10 03:23:40,583 Epoch 1192: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-10 03:23:40,583 EPOCH 1193
2024-02-10 03:23:56,579 Epoch 1193: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-10 03:23:56,579 EPOCH 1194
2024-02-10 03:24:12,882 Epoch 1194: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-10 03:24:12,883 EPOCH 1195
2024-02-10 03:24:28,767 Epoch 1195: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-10 03:24:28,768 EPOCH 1196
2024-02-10 03:24:44,493 Epoch 1196: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-10 03:24:44,494 EPOCH 1197
2024-02-10 03:25:00,875 Epoch 1197: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-10 03:25:00,875 EPOCH 1198
2024-02-10 03:25:17,043 Epoch 1198: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-10 03:25:17,043 EPOCH 1199
2024-02-10 03:25:33,182 Epoch 1199: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-10 03:25:33,182 EPOCH 1200
2024-02-10 03:25:49,321 [Epoch: 1200 Step: 00010800] Batch Recognition Loss:   0.000586 => Gls Tokens per Sec:      658 || Batch Translation Loss:   0.066245 => Txt Tokens per Sec:     1821 || Lr: 0.000100
2024-02-10 03:25:49,322 Epoch 1200: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-10 03:25:49,322 EPOCH 1201
2024-02-10 03:26:05,433 Epoch 1201: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-10 03:26:05,434 EPOCH 1202
2024-02-10 03:26:21,157 Epoch 1202: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-10 03:26:21,158 EPOCH 1203
2024-02-10 03:26:37,793 Epoch 1203: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-10 03:26:37,794 EPOCH 1204
2024-02-10 03:26:53,969 Epoch 1204: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-10 03:26:53,969 EPOCH 1205
2024-02-10 03:27:09,809 Epoch 1205: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-10 03:27:09,810 EPOCH 1206
2024-02-10 03:27:25,753 Epoch 1206: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-10 03:27:25,753 EPOCH 1207
2024-02-10 03:27:41,702 Epoch 1207: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-10 03:27:41,702 EPOCH 1208
2024-02-10 03:27:58,097 Epoch 1208: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 03:27:58,097 EPOCH 1209
2024-02-10 03:28:14,251 Epoch 1209: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-10 03:28:14,251 EPOCH 1210
2024-02-10 03:28:30,257 Epoch 1210: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-10 03:28:30,258 EPOCH 1211
2024-02-10 03:28:46,569 Epoch 1211: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-10 03:28:46,570 EPOCH 1212
2024-02-10 03:28:50,928 [Epoch: 1212 Step: 00010900] Batch Recognition Loss:   0.000674 => Gls Tokens per Sec:       87 || Batch Translation Loss:   0.010560 => Txt Tokens per Sec:      311 || Lr: 0.000100
2024-02-10 03:29:02,923 Epoch 1212: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-10 03:29:02,923 EPOCH 1213
2024-02-10 03:29:18,922 Epoch 1213: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 03:29:18,923 EPOCH 1214
2024-02-10 03:29:34,772 Epoch 1214: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 03:29:34,773 EPOCH 1215
2024-02-10 03:29:50,996 Epoch 1215: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-10 03:29:50,996 EPOCH 1216
2024-02-10 03:30:07,019 Epoch 1216: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-10 03:30:07,020 EPOCH 1217
2024-02-10 03:30:22,967 Epoch 1217: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-10 03:30:22,967 EPOCH 1218
2024-02-10 03:30:39,523 Epoch 1218: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-10 03:30:39,523 EPOCH 1219
2024-02-10 03:30:55,860 Epoch 1219: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-10 03:30:55,861 EPOCH 1220
2024-02-10 03:31:11,974 Epoch 1220: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-10 03:31:11,975 EPOCH 1221
2024-02-10 03:31:28,164 Epoch 1221: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-10 03:31:28,164 EPOCH 1222
2024-02-10 03:31:44,470 Epoch 1222: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-10 03:31:44,471 EPOCH 1223
2024-02-10 03:31:50,548 [Epoch: 1223 Step: 00011000] Batch Recognition Loss:   0.001162 => Gls Tokens per Sec:      421 || Batch Translation Loss:   0.062990 => Txt Tokens per Sec:     1102 || Lr: 0.000100
2024-02-10 03:32:00,589 Epoch 1223: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-10 03:32:00,589 EPOCH 1224
2024-02-10 03:32:16,460 Epoch 1224: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-10 03:32:16,461 EPOCH 1225
2024-02-10 03:32:32,414 Epoch 1225: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-10 03:32:32,415 EPOCH 1226
2024-02-10 03:32:48,541 Epoch 1226: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-10 03:32:48,541 EPOCH 1227
2024-02-10 03:33:04,939 Epoch 1227: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-10 03:33:04,940 EPOCH 1228
2024-02-10 03:33:20,816 Epoch 1228: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-10 03:33:20,816 EPOCH 1229
2024-02-10 03:33:36,656 Epoch 1229: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-10 03:33:36,656 EPOCH 1230
2024-02-10 03:33:53,327 Epoch 1230: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-10 03:33:53,328 EPOCH 1231
2024-02-10 03:34:09,449 Epoch 1231: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-10 03:34:09,449 EPOCH 1232
2024-02-10 03:34:25,603 Epoch 1232: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-10 03:34:25,604 EPOCH 1233
2024-02-10 03:34:41,826 Epoch 1233: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-10 03:34:41,826 EPOCH 1234
2024-02-10 03:34:48,463 [Epoch: 1234 Step: 00011100] Batch Recognition Loss:   0.003445 => Gls Tokens per Sec:      579 || Batch Translation Loss:   0.063680 => Txt Tokens per Sec:     1664 || Lr: 0.000100
2024-02-10 03:34:58,192 Epoch 1234: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-10 03:34:58,193 EPOCH 1235
2024-02-10 03:35:14,336 Epoch 1235: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-10 03:35:14,337 EPOCH 1236
2024-02-10 03:35:30,616 Epoch 1236: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-10 03:35:30,616 EPOCH 1237
2024-02-10 03:35:46,896 Epoch 1237: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-10 03:35:46,897 EPOCH 1238
2024-02-10 03:36:03,297 Epoch 1238: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-10 03:36:03,297 EPOCH 1239
2024-02-10 03:36:19,250 Epoch 1239: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-10 03:36:19,250 EPOCH 1240
2024-02-10 03:36:35,480 Epoch 1240: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-10 03:36:35,481 EPOCH 1241
2024-02-10 03:36:51,554 Epoch 1241: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-10 03:36:51,554 EPOCH 1242
2024-02-10 03:37:07,562 Epoch 1242: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-10 03:37:07,562 EPOCH 1243
2024-02-10 03:37:23,816 Epoch 1243: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-10 03:37:23,817 EPOCH 1244
2024-02-10 03:37:39,784 Epoch 1244: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-10 03:37:39,784 EPOCH 1245
2024-02-10 03:37:45,234 [Epoch: 1245 Step: 00011200] Batch Recognition Loss:   0.000240 => Gls Tokens per Sec:      774 || Batch Translation Loss:   0.051910 => Txt Tokens per Sec:     2018 || Lr: 0.000100
2024-02-10 03:37:55,917 Epoch 1245: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-10 03:37:55,917 EPOCH 1246
2024-02-10 03:38:12,171 Epoch 1246: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-10 03:38:12,172 EPOCH 1247
2024-02-10 03:38:28,044 Epoch 1247: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-10 03:38:28,045 EPOCH 1248
2024-02-10 03:38:44,225 Epoch 1248: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 03:38:44,226 EPOCH 1249
2024-02-10 03:39:00,538 Epoch 1249: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 03:39:00,539 EPOCH 1250
2024-02-10 03:39:16,763 Epoch 1250: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-10 03:39:16,764 EPOCH 1251
2024-02-10 03:39:32,876 Epoch 1251: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-10 03:39:32,877 EPOCH 1252
2024-02-10 03:39:48,778 Epoch 1252: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-10 03:39:48,779 EPOCH 1253
2024-02-10 03:40:04,915 Epoch 1253: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.71 
2024-02-10 03:40:04,915 EPOCH 1254
2024-02-10 03:40:21,293 Epoch 1254: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-10 03:40:21,293 EPOCH 1255
2024-02-10 03:40:37,563 Epoch 1255: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-10 03:40:37,564 EPOCH 1256
2024-02-10 03:40:45,206 [Epoch: 1256 Step: 00011300] Batch Recognition Loss:   0.000597 => Gls Tokens per Sec:      838 || Batch Translation Loss:   0.115660 => Txt Tokens per Sec:     2302 || Lr: 0.000100
2024-02-10 03:40:53,616 Epoch 1256: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-10 03:40:53,617 EPOCH 1257
2024-02-10 03:41:09,773 Epoch 1257: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.92 
2024-02-10 03:41:09,774 EPOCH 1258
2024-02-10 03:41:25,990 Epoch 1258: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-10 03:41:25,991 EPOCH 1259
2024-02-10 03:41:42,387 Epoch 1259: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-10 03:41:42,387 EPOCH 1260
2024-02-10 03:41:58,463 Epoch 1260: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-10 03:41:58,464 EPOCH 1261
2024-02-10 03:42:14,594 Epoch 1261: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.82 
2024-02-10 03:42:14,595 EPOCH 1262
2024-02-10 03:42:30,529 Epoch 1262: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-10 03:42:30,530 EPOCH 1263
2024-02-10 03:42:46,132 Epoch 1263: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.27 
2024-02-10 03:42:46,132 EPOCH 1264
2024-02-10 03:43:02,255 Epoch 1264: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.38 
2024-02-10 03:43:02,256 EPOCH 1265
2024-02-10 03:43:18,275 Epoch 1265: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.39 
2024-02-10 03:43:18,276 EPOCH 1266
2024-02-10 03:43:34,439 Epoch 1266: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.71 
2024-02-10 03:43:34,440 EPOCH 1267
2024-02-10 03:43:43,974 [Epoch: 1267 Step: 00011400] Batch Recognition Loss:   0.000966 => Gls Tokens per Sec:      711 || Batch Translation Loss:   0.190577 => Txt Tokens per Sec:     2038 || Lr: 0.000100
2024-02-10 03:43:50,691 Epoch 1267: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.41 
2024-02-10 03:43:50,691 EPOCH 1268
2024-02-10 03:44:06,836 Epoch 1268: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.82 
2024-02-10 03:44:06,836 EPOCH 1269
2024-02-10 03:44:22,921 Epoch 1269: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.63 
2024-02-10 03:44:22,922 EPOCH 1270
2024-02-10 03:44:38,547 Epoch 1270: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.70 
2024-02-10 03:44:38,548 EPOCH 1271
2024-02-10 03:44:55,009 Epoch 1271: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.23 
2024-02-10 03:44:55,010 EPOCH 1272
2024-02-10 03:45:10,939 Epoch 1272: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.62 
2024-02-10 03:45:10,940 EPOCH 1273
2024-02-10 03:45:26,985 Epoch 1273: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.99 
2024-02-10 03:45:26,985 EPOCH 1274
2024-02-10 03:45:43,328 Epoch 1274: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-10 03:45:43,328 EPOCH 1275
2024-02-10 03:45:59,443 Epoch 1275: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-10 03:45:59,443 EPOCH 1276
2024-02-10 03:46:15,889 Epoch 1276: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-10 03:46:15,890 EPOCH 1277
2024-02-10 03:46:32,226 Epoch 1277: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-10 03:46:32,226 EPOCH 1278
2024-02-10 03:46:43,592 [Epoch: 1278 Step: 00011500] Batch Recognition Loss:   0.001435 => Gls Tokens per Sec:      788 || Batch Translation Loss:   0.040339 => Txt Tokens per Sec:     2180 || Lr: 0.000100
2024-02-10 03:46:48,297 Epoch 1278: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-10 03:46:48,297 EPOCH 1279
2024-02-10 03:47:04,172 Epoch 1279: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-10 03:47:04,173 EPOCH 1280
2024-02-10 03:47:20,502 Epoch 1280: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-10 03:47:20,503 EPOCH 1281
2024-02-10 03:47:36,851 Epoch 1281: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-10 03:47:36,851 EPOCH 1282
2024-02-10 03:47:52,983 Epoch 1282: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-10 03:47:52,984 EPOCH 1283
2024-02-10 03:48:09,577 Epoch 1283: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 03:48:09,578 EPOCH 1284
2024-02-10 03:48:25,813 Epoch 1284: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 03:48:25,814 EPOCH 1285
2024-02-10 03:48:41,822 Epoch 1285: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 03:48:41,823 EPOCH 1286
2024-02-10 03:48:58,077 Epoch 1286: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 03:48:58,077 EPOCH 1287
2024-02-10 03:49:14,010 Epoch 1287: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 03:49:14,011 EPOCH 1288
2024-02-10 03:49:30,091 Epoch 1288: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 03:49:30,092 EPOCH 1289
2024-02-10 03:49:45,579 [Epoch: 1289 Step: 00011600] Batch Recognition Loss:   0.000251 => Gls Tokens per Sec:      603 || Batch Translation Loss:   0.017720 => Txt Tokens per Sec:     1659 || Lr: 0.000100
2024-02-10 03:49:46,154 Epoch 1289: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 03:49:46,154 EPOCH 1290
2024-02-10 03:50:01,921 Epoch 1290: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 03:50:01,922 EPOCH 1291
2024-02-10 03:50:18,020 Epoch 1291: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 03:50:18,020 EPOCH 1292
2024-02-10 03:50:34,177 Epoch 1292: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 03:50:34,178 EPOCH 1293
2024-02-10 03:50:50,230 Epoch 1293: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 03:50:50,230 EPOCH 1294
2024-02-10 03:51:06,385 Epoch 1294: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 03:51:06,386 EPOCH 1295
2024-02-10 03:51:22,414 Epoch 1295: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 03:51:22,415 EPOCH 1296
2024-02-10 03:51:38,673 Epoch 1296: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 03:51:38,674 EPOCH 1297
2024-02-10 03:51:54,549 Epoch 1297: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 03:51:54,550 EPOCH 1298
2024-02-10 03:52:10,998 Epoch 1298: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 03:52:10,999 EPOCH 1299
2024-02-10 03:52:27,168 Epoch 1299: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 03:52:27,169 EPOCH 1300
2024-02-10 03:52:43,011 [Epoch: 1300 Step: 00011700] Batch Recognition Loss:   0.003430 => Gls Tokens per Sec:      670 || Batch Translation Loss:   0.024065 => Txt Tokens per Sec:     1855 || Lr: 0.000100
2024-02-10 03:52:43,011 Epoch 1300: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 03:52:43,011 EPOCH 1301
2024-02-10 03:52:59,279 Epoch 1301: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 03:52:59,280 EPOCH 1302
2024-02-10 03:53:15,243 Epoch 1302: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 03:53:15,243 EPOCH 1303
2024-02-10 03:53:31,324 Epoch 1303: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 03:53:31,325 EPOCH 1304
2024-02-10 03:53:47,821 Epoch 1304: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 03:53:47,821 EPOCH 1305
2024-02-10 03:54:03,693 Epoch 1305: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 03:54:03,694 EPOCH 1306
2024-02-10 03:54:20,069 Epoch 1306: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 03:54:20,070 EPOCH 1307
2024-02-10 03:54:36,323 Epoch 1307: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 03:54:36,323 EPOCH 1308
2024-02-10 03:54:52,529 Epoch 1308: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 03:54:52,530 EPOCH 1309
2024-02-10 03:55:08,554 Epoch 1309: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 03:55:08,555 EPOCH 1310
2024-02-10 03:55:24,801 Epoch 1310: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 03:55:24,802 EPOCH 1311
2024-02-10 03:55:40,792 Epoch 1311: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 03:55:40,792 EPOCH 1312
2024-02-10 03:55:41,065 [Epoch: 1312 Step: 00011800] Batch Recognition Loss:   0.000365 => Gls Tokens per Sec:     4706 || Batch Translation Loss:   0.015769 => Txt Tokens per Sec:    10559 || Lr: 0.000100
2024-02-10 03:55:56,624 Epoch 1312: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 03:55:56,624 EPOCH 1313
2024-02-10 03:56:12,650 Epoch 1313: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 03:56:12,651 EPOCH 1314
2024-02-10 03:56:28,892 Epoch 1314: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 03:56:28,892 EPOCH 1315
2024-02-10 03:56:44,876 Epoch 1315: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 03:56:44,877 EPOCH 1316
2024-02-10 03:57:00,906 Epoch 1316: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 03:57:00,906 EPOCH 1317
2024-02-10 03:57:16,951 Epoch 1317: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 03:57:16,952 EPOCH 1318
2024-02-10 03:57:33,505 Epoch 1318: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 03:57:33,506 EPOCH 1319
2024-02-10 03:57:49,661 Epoch 1319: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 03:57:49,662 EPOCH 1320
2024-02-10 03:58:06,020 Epoch 1320: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 03:58:06,021 EPOCH 1321
2024-02-10 03:58:22,340 Epoch 1321: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 03:58:22,340 EPOCH 1322
2024-02-10 03:58:38,486 Epoch 1322: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 03:58:38,487 EPOCH 1323
2024-02-10 03:58:43,665 [Epoch: 1323 Step: 00011900] Batch Recognition Loss:   0.000692 => Gls Tokens per Sec:      321 || Batch Translation Loss:   0.028750 => Txt Tokens per Sec:     1008 || Lr: 0.000100
2024-02-10 03:58:54,871 Epoch 1323: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 03:58:54,872 EPOCH 1324
2024-02-10 03:59:11,252 Epoch 1324: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 03:59:11,253 EPOCH 1325
2024-02-10 03:59:27,420 Epoch 1325: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 03:59:27,420 EPOCH 1326
2024-02-10 03:59:43,698 Epoch 1326: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 03:59:43,699 EPOCH 1327
2024-02-10 04:00:00,392 Epoch 1327: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 04:00:00,393 EPOCH 1328
2024-02-10 04:00:16,534 Epoch 1328: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 04:00:16,535 EPOCH 1329
2024-02-10 04:00:32,488 Epoch 1329: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 04:00:32,489 EPOCH 1330
2024-02-10 04:00:48,739 Epoch 1330: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 04:00:48,740 EPOCH 1331
2024-02-10 04:01:04,972 Epoch 1331: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 04:01:04,973 EPOCH 1332
2024-02-10 04:01:20,955 Epoch 1332: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 04:01:20,956 EPOCH 1333
2024-02-10 04:01:36,841 Epoch 1333: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 04:01:36,842 EPOCH 1334
2024-02-10 04:01:41,263 [Epoch: 1334 Step: 00012000] Batch Recognition Loss:   0.001931 => Gls Tokens per Sec:      869 || Batch Translation Loss:   0.022912 => Txt Tokens per Sec:     2596 || Lr: 0.000100
2024-02-10 04:02:53,490 Validation result at epoch 1334, step    12000: duration: 72.2263s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.58935	Translation Loss: 92596.48438	PPL: 10389.48438
	Eval Metric: BLEU
	WER 5.01	(DEL: 0.00,	INS: 0.00,	SUB: 5.01)
	BLEU-4 0.00	(BLEU-1: 12.06,	BLEU-2: 3.75,	BLEU-3: 1.25,	BLEU-4: 0.00)
	CHRF 17.89	ROUGE 9.72
2024-02-10 04:02:53,492 Logging Recognition and Translation Outputs
2024-02-10 04:02:53,492 ========================================================================================================================
2024-02-10 04:02:53,492 Logging Sequence: 168_56.00
2024-02-10 04:02:53,493 	Gloss Reference :	A B+C+D+E
2024-02-10 04:02:53,493 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 04:02:53,493 	Gloss Alignment :	         
2024-02-10 04:02:53,493 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 04:02:53,495 	Text Reference  :	fans have  been waiting to ***** ** *** ***** *** **** *** *** ******* *** ** see   vamika for  a  long time
2024-02-10 04:02:53,495 	Text Hypothesis :	**** kohli has  refused to focus on the child and they are the parents and it never take   part of the  game
2024-02-10 04:02:53,495 	Text Alignment  :	D    S     S    S          I     I  I   I     I   I    I   I   I       I   I  S     S      S    S  S    S   
2024-02-10 04:02:53,495 ========================================================================================================================
2024-02-10 04:02:53,495 Logging Sequence: 161_74.00
2024-02-10 04:02:53,495 	Gloss Reference :	A B+C+D+E
2024-02-10 04:02:53,495 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 04:02:53,496 	Gloss Alignment :	         
2024-02-10 04:02:53,496 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 04:02:53,497 	Text Reference  :	*** **** *** *** ******* ** i    am      proud of      the indian team's   achievements
2024-02-10 04:02:53,497 	Text Hypothesis :	the bcci can not allowed to wear clothes that  shocked the ****** football team        
2024-02-10 04:02:53,497 	Text Alignment  :	I   I    I   I   I       I  S    S       S     S           D      S        S           
2024-02-10 04:02:53,497 ========================================================================================================================
2024-02-10 04:02:53,497 Logging Sequence: 111_83.00
2024-02-10 04:02:53,497 	Gloss Reference :	A B+C+D+E
2024-02-10 04:02:53,497 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 04:02:53,498 	Gloss Alignment :	         
2024-02-10 04:02:53,498 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 04:02:53,499 	Text Reference  :	and   the other 10 team members are       fined 25   of the  match fee or rs    6  lakh 
2024-02-10 04:02:53,499 	Text Hypothesis :	after the ***** ** bcci is      extremely fit   when i  want to    win a  medal in delhi
2024-02-10 04:02:53,499 	Text Alignment  :	S         D     D  S    S       S         S     S    S  S    S     S   S  S     S  S    
2024-02-10 04:02:53,500 ========================================================================================================================
2024-02-10 04:02:53,500 Logging Sequence: 61_218.00
2024-02-10 04:02:53,500 	Gloss Reference :	A B+C+D+E
2024-02-10 04:02:53,500 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 04:02:53,500 	Gloss Alignment :	         
2024-02-10 04:02:53,500 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 04:02:53,501 	Text Reference  :	***** in 2020  a         woman had said         at the  press conference
2024-02-10 04:02:53,501 	Text Hypothesis :	after 28 years argentina won   the similarities in just world cup       
2024-02-10 04:02:53,501 	Text Alignment  :	I     S  S     S         S     S   S            S  S    S     S         
2024-02-10 04:02:53,502 ========================================================================================================================
2024-02-10 04:02:53,502 Logging Sequence: 94_123.00
2024-02-10 04:02:53,502 	Gloss Reference :	A B+C+D+E
2024-02-10 04:02:53,502 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 04:02:53,502 	Gloss Alignment :	         
2024-02-10 04:02:53,502 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 04:02:53,505 	Text Reference  :	* ****** the     venue narendra modi stadium for the india-pakistan match has    been      kept    the   same people   can book    flights etc  
2024-02-10 04:02:53,505 	Text Hypothesis :	a police officer said  that     the  plans   for the ************** ***** eighth encounter between india and  pakistan had playing the     plans
2024-02-10 04:02:53,505 	Text Alignment  :	I I      S       S     S        S    S               D              D     S      S         S       S     S    S        S   S       S       S    
2024-02-10 04:02:53,505 ========================================================================================================================
2024-02-10 04:03:05,744 Epoch 1334: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 04:03:05,745 EPOCH 1335
2024-02-10 04:03:21,768 Epoch 1335: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 04:03:21,769 EPOCH 1336
2024-02-10 04:03:37,798 Epoch 1336: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 04:03:37,799 EPOCH 1337
2024-02-10 04:03:53,884 Epoch 1337: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 04:03:53,884 EPOCH 1338
2024-02-10 04:04:10,166 Epoch 1338: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 04:04:10,167 EPOCH 1339
2024-02-10 04:04:26,371 Epoch 1339: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 04:04:26,371 EPOCH 1340
2024-02-10 04:04:42,780 Epoch 1340: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 04:04:42,781 EPOCH 1341
2024-02-10 04:04:58,862 Epoch 1341: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 04:04:58,863 EPOCH 1342
2024-02-10 04:05:15,117 Epoch 1342: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 04:05:15,118 EPOCH 1343
2024-02-10 04:05:31,389 Epoch 1343: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 04:05:31,390 EPOCH 1344
2024-02-10 04:05:47,823 Epoch 1344: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 04:05:47,824 EPOCH 1345
2024-02-10 04:05:49,362 [Epoch: 1345 Step: 00012100] Batch Recognition Loss:   0.000209 => Gls Tokens per Sec:     3331 || Batch Translation Loss:   0.017138 => Txt Tokens per Sec:     8041 || Lr: 0.000100
2024-02-10 04:06:03,874 Epoch 1345: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 04:06:03,875 EPOCH 1346
2024-02-10 04:06:20,076 Epoch 1346: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 04:06:20,077 EPOCH 1347
2024-02-10 04:06:35,769 Epoch 1347: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 04:06:35,769 EPOCH 1348
2024-02-10 04:06:51,991 Epoch 1348: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 04:06:51,992 EPOCH 1349
2024-02-10 04:07:08,000 Epoch 1349: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 04:07:08,000 EPOCH 1350
2024-02-10 04:07:23,963 Epoch 1350: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.09 
2024-02-10 04:07:23,963 EPOCH 1351
2024-02-10 04:07:40,125 Epoch 1351: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-10 04:07:40,125 EPOCH 1352
2024-02-10 04:07:56,242 Epoch 1352: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-10 04:07:56,243 EPOCH 1353
2024-02-10 04:08:12,194 Epoch 1353: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-10 04:08:12,195 EPOCH 1354
2024-02-10 04:08:28,302 Epoch 1354: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-10 04:08:28,302 EPOCH 1355
2024-02-10 04:08:44,160 Epoch 1355: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.43 
2024-02-10 04:08:44,160 EPOCH 1356
2024-02-10 04:08:50,242 [Epoch: 1356 Step: 00012200] Batch Recognition Loss:   0.000872 => Gls Tokens per Sec:      905 || Batch Translation Loss:   0.132436 => Txt Tokens per Sec:     2269 || Lr: 0.000100
2024-02-10 04:09:00,295 Epoch 1356: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.74 
2024-02-10 04:09:00,296 EPOCH 1357
2024-02-10 04:09:16,565 Epoch 1357: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.64 
2024-02-10 04:09:16,566 EPOCH 1358
2024-02-10 04:09:32,547 Epoch 1358: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-10 04:09:32,548 EPOCH 1359
2024-02-10 04:09:48,493 Epoch 1359: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-10 04:09:48,494 EPOCH 1360
2024-02-10 04:10:04,838 Epoch 1360: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-10 04:10:04,839 EPOCH 1361
2024-02-10 04:10:20,725 Epoch 1361: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-10 04:10:20,725 EPOCH 1362
2024-02-10 04:10:36,974 Epoch 1362: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-10 04:10:36,974 EPOCH 1363
2024-02-10 04:10:53,402 Epoch 1363: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-10 04:10:53,403 EPOCH 1364
2024-02-10 04:11:09,363 Epoch 1364: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-10 04:11:09,364 EPOCH 1365
2024-02-10 04:11:25,357 Epoch 1365: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-10 04:11:25,357 EPOCH 1366
2024-02-10 04:11:41,626 Epoch 1366: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-10 04:11:41,626 EPOCH 1367
2024-02-10 04:11:51,220 [Epoch: 1367 Step: 00012300] Batch Recognition Loss:   0.000849 => Gls Tokens per Sec:      707 || Batch Translation Loss:   0.043730 => Txt Tokens per Sec:     1997 || Lr: 0.000100
2024-02-10 04:11:57,666 Epoch 1367: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 04:11:57,666 EPOCH 1368
2024-02-10 04:12:14,022 Epoch 1368: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 04:12:14,023 EPOCH 1369
2024-02-10 04:12:30,311 Epoch 1369: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 04:12:30,311 EPOCH 1370
2024-02-10 04:12:46,593 Epoch 1370: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 04:12:46,593 EPOCH 1371
2024-02-10 04:13:02,757 Epoch 1371: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 04:13:02,757 EPOCH 1372
2024-02-10 04:13:19,009 Epoch 1372: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 04:13:19,010 EPOCH 1373
2024-02-10 04:13:35,613 Epoch 1373: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-10 04:13:35,614 EPOCH 1374
2024-02-10 04:13:51,633 Epoch 1374: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-10 04:13:51,634 EPOCH 1375
2024-02-10 04:14:07,717 Epoch 1375: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-10 04:14:07,718 EPOCH 1376
2024-02-10 04:14:23,774 Epoch 1376: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-10 04:14:23,775 EPOCH 1377
2024-02-10 04:14:39,928 Epoch 1377: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-10 04:14:39,928 EPOCH 1378
2024-02-10 04:14:52,688 [Epoch: 1378 Step: 00012400] Batch Recognition Loss:   0.001377 => Gls Tokens per Sec:      632 || Batch Translation Loss:   0.034374 => Txt Tokens per Sec:     1804 || Lr: 0.000100
2024-02-10 04:14:56,118 Epoch 1378: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 04:14:56,118 EPOCH 1379
2024-02-10 04:15:12,194 Epoch 1379: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-10 04:15:12,194 EPOCH 1380
2024-02-10 04:15:28,348 Epoch 1380: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 04:15:28,349 EPOCH 1381
2024-02-10 04:15:44,583 Epoch 1381: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 04:15:44,584 EPOCH 1382
2024-02-10 04:16:00,490 Epoch 1382: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 04:16:00,491 EPOCH 1383
2024-02-10 04:16:16,910 Epoch 1383: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 04:16:16,910 EPOCH 1384
2024-02-10 04:16:33,343 Epoch 1384: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 04:16:33,343 EPOCH 1385
2024-02-10 04:16:49,650 Epoch 1385: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 04:16:49,650 EPOCH 1386
2024-02-10 04:17:05,910 Epoch 1386: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 04:17:05,911 EPOCH 1387
2024-02-10 04:17:22,180 Epoch 1387: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 04:17:22,180 EPOCH 1388
2024-02-10 04:17:38,363 Epoch 1388: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 04:17:38,363 EPOCH 1389
2024-02-10 04:17:48,858 [Epoch: 1389 Step: 00012500] Batch Recognition Loss:   0.000515 => Gls Tokens per Sec:      890 || Batch Translation Loss:   0.021577 => Txt Tokens per Sec:     2379 || Lr: 0.000100
2024-02-10 04:17:54,627 Epoch 1389: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 04:17:54,627 EPOCH 1390
2024-02-10 04:18:11,182 Epoch 1390: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 04:18:11,183 EPOCH 1391
2024-02-10 04:18:27,544 Epoch 1391: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 04:18:27,544 EPOCH 1392
2024-02-10 04:18:43,426 Epoch 1392: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 04:18:43,427 EPOCH 1393
2024-02-10 04:18:59,725 Epoch 1393: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 04:18:59,726 EPOCH 1394
2024-02-10 04:19:16,059 Epoch 1394: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 04:19:16,059 EPOCH 1395
2024-02-10 04:19:32,425 Epoch 1395: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 04:19:32,426 EPOCH 1396
2024-02-10 04:19:48,610 Epoch 1396: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 04:19:48,610 EPOCH 1397
2024-02-10 04:20:05,687 Epoch 1397: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 04:20:05,687 EPOCH 1398
2024-02-10 04:20:22,115 Epoch 1398: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 04:20:22,115 EPOCH 1399
2024-02-10 04:20:38,244 Epoch 1399: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 04:20:38,244 EPOCH 1400
2024-02-10 04:20:53,965 [Epoch: 1400 Step: 00012600] Batch Recognition Loss:   0.000389 => Gls Tokens per Sec:      676 || Batch Translation Loss:   0.060857 => Txt Tokens per Sec:     1869 || Lr: 0.000100
2024-02-10 04:20:53,966 Epoch 1400: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-10 04:20:53,966 EPOCH 1401
2024-02-10 04:21:10,163 Epoch 1401: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 04:21:10,163 EPOCH 1402
2024-02-10 04:21:26,562 Epoch 1402: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 04:21:26,562 EPOCH 1403
2024-02-10 04:21:42,745 Epoch 1403: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-10 04:21:42,746 EPOCH 1404
2024-02-10 04:21:59,033 Epoch 1404: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-10 04:21:59,034 EPOCH 1405
2024-02-10 04:22:15,253 Epoch 1405: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-10 04:22:15,254 EPOCH 1406
2024-02-10 04:22:31,187 Epoch 1406: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-10 04:22:31,188 EPOCH 1407
2024-02-10 04:22:47,545 Epoch 1407: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 04:22:47,546 EPOCH 1408
2024-02-10 04:23:03,863 Epoch 1408: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 04:23:03,863 EPOCH 1409
2024-02-10 04:23:20,242 Epoch 1409: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 04:23:20,243 EPOCH 1410
2024-02-10 04:23:36,421 Epoch 1410: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.20 
2024-02-10 04:23:36,422 EPOCH 1411
2024-02-10 04:23:52,705 Epoch 1411: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 04:23:52,706 EPOCH 1412
2024-02-10 04:23:53,052 [Epoch: 1412 Step: 00012700] Batch Recognition Loss:   0.000943 => Gls Tokens per Sec:     3721 || Batch Translation Loss:   0.011925 => Txt Tokens per Sec:     6718 || Lr: 0.000100
2024-02-10 04:24:08,843 Epoch 1412: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 04:24:08,843 EPOCH 1413
2024-02-10 04:24:24,986 Epoch 1413: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 04:24:24,986 EPOCH 1414
2024-02-10 04:24:40,929 Epoch 1414: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 04:24:40,929 EPOCH 1415
2024-02-10 04:24:57,315 Epoch 1415: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 04:24:57,316 EPOCH 1416
2024-02-10 04:25:13,620 Epoch 1416: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 04:25:13,621 EPOCH 1417
2024-02-10 04:25:29,580 Epoch 1417: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 04:25:29,581 EPOCH 1418
2024-02-10 04:25:45,992 Epoch 1418: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 04:25:45,993 EPOCH 1419
2024-02-10 04:26:02,026 Epoch 1419: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 04:26:02,027 EPOCH 1420
2024-02-10 04:26:18,167 Epoch 1420: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-10 04:26:18,167 EPOCH 1421
2024-02-10 04:26:34,037 Epoch 1421: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-10 04:26:34,038 EPOCH 1422
2024-02-10 04:26:50,305 Epoch 1422: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 04:26:50,306 EPOCH 1423
2024-02-10 04:26:56,367 [Epoch: 1423 Step: 00012800] Batch Recognition Loss:   0.001729 => Gls Tokens per Sec:      422 || Batch Translation Loss:   0.030197 => Txt Tokens per Sec:     1105 || Lr: 0.000100
2024-02-10 04:27:06,623 Epoch 1423: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 04:27:06,624 EPOCH 1424
2024-02-10 04:27:22,845 Epoch 1424: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-10 04:27:22,846 EPOCH 1425
2024-02-10 04:27:38,907 Epoch 1425: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 04:27:38,908 EPOCH 1426
2024-02-10 04:27:54,954 Epoch 1426: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 04:27:54,954 EPOCH 1427
2024-02-10 04:28:10,867 Epoch 1427: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-10 04:28:10,867 EPOCH 1428
2024-02-10 04:28:26,964 Epoch 1428: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.52 
2024-02-10 04:28:26,964 EPOCH 1429
2024-02-10 04:28:42,738 Epoch 1429: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.83 
2024-02-10 04:28:42,738 EPOCH 1430
2024-02-10 04:28:58,991 Epoch 1430: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-10 04:28:58,992 EPOCH 1431
2024-02-10 04:29:15,200 Epoch 1431: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-10 04:29:15,201 EPOCH 1432
2024-02-10 04:29:31,381 Epoch 1432: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-10 04:29:31,381 EPOCH 1433
2024-02-10 04:29:47,423 Epoch 1433: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.49 
2024-02-10 04:29:47,423 EPOCH 1434
2024-02-10 04:29:51,573 [Epoch: 1434 Step: 00012900] Batch Recognition Loss:   0.001585 => Gls Tokens per Sec:      926 || Batch Translation Loss:   0.058787 => Txt Tokens per Sec:     2722 || Lr: 0.000100
2024-02-10 04:30:03,591 Epoch 1434: Total Training Recognition Loss 0.11  Total Training Translation Loss 0.44 
2024-02-10 04:30:03,592 EPOCH 1435
2024-02-10 04:30:19,909 Epoch 1435: Total Training Recognition Loss 2.33  Total Training Translation Loss 0.56 
2024-02-10 04:30:19,910 EPOCH 1436
2024-02-10 04:30:35,910 Epoch 1436: Total Training Recognition Loss 3.68  Total Training Translation Loss 0.97 
2024-02-10 04:30:35,911 EPOCH 1437
2024-02-10 04:30:52,727 Epoch 1437: Total Training Recognition Loss 2.26  Total Training Translation Loss 1.92 
2024-02-10 04:30:52,727 EPOCH 1438
2024-02-10 04:31:08,995 Epoch 1438: Total Training Recognition Loss 0.76  Total Training Translation Loss 2.79 
2024-02-10 04:31:08,996 EPOCH 1439
2024-02-10 04:31:24,867 Epoch 1439: Total Training Recognition Loss 0.11  Total Training Translation Loss 5.23 
2024-02-10 04:31:24,867 EPOCH 1440
2024-02-10 04:31:40,818 Epoch 1440: Total Training Recognition Loss 0.12  Total Training Translation Loss 9.33 
2024-02-10 04:31:40,819 EPOCH 1441
2024-02-10 04:31:56,798 Epoch 1441: Total Training Recognition Loss 0.09  Total Training Translation Loss 4.89 
2024-02-10 04:31:56,799 EPOCH 1442
2024-02-10 04:32:12,910 Epoch 1442: Total Training Recognition Loss 0.08  Total Training Translation Loss 4.08 
2024-02-10 04:32:12,910 EPOCH 1443
2024-02-10 04:32:28,947 Epoch 1443: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.82 
2024-02-10 04:32:28,947 EPOCH 1444
2024-02-10 04:32:45,081 Epoch 1444: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.02 
2024-02-10 04:32:45,081 EPOCH 1445
2024-02-10 04:32:50,915 [Epoch: 1445 Step: 00013000] Batch Recognition Loss:   0.002198 => Gls Tokens per Sec:      723 || Batch Translation Loss:   0.088596 => Txt Tokens per Sec:     1780 || Lr: 0.000100
2024-02-10 04:33:01,493 Epoch 1445: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.73 
2024-02-10 04:33:01,494 EPOCH 1446
2024-02-10 04:33:17,546 Epoch 1446: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.55 
2024-02-10 04:33:17,547 EPOCH 1447
2024-02-10 04:33:33,379 Epoch 1447: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.38 
2024-02-10 04:33:33,380 EPOCH 1448
2024-02-10 04:33:49,889 Epoch 1448: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.34 
2024-02-10 04:33:49,889 EPOCH 1449
2024-02-10 04:34:05,777 Epoch 1449: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.28 
2024-02-10 04:34:05,778 EPOCH 1450
2024-02-10 04:34:21,782 Epoch 1450: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.25 
2024-02-10 04:34:21,782 EPOCH 1451
2024-02-10 04:34:38,087 Epoch 1451: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.25 
2024-02-10 04:34:38,088 EPOCH 1452
2024-02-10 04:34:54,195 Epoch 1452: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.24 
2024-02-10 04:34:54,195 EPOCH 1453
2024-02-10 04:35:10,289 Epoch 1453: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.24 
2024-02-10 04:35:10,289 EPOCH 1454
2024-02-10 04:35:26,533 Epoch 1454: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.21 
2024-02-10 04:35:26,534 EPOCH 1455
2024-02-10 04:35:42,529 Epoch 1455: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 04:35:42,530 EPOCH 1456
2024-02-10 04:35:47,730 [Epoch: 1456 Step: 00013100] Batch Recognition Loss:   0.000695 => Gls Tokens per Sec:     1231 || Batch Translation Loss:   0.027221 => Txt Tokens per Sec:     3148 || Lr: 0.000100
2024-02-10 04:35:58,908 Epoch 1456: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 04:35:58,909 EPOCH 1457
2024-02-10 04:36:15,001 Epoch 1457: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.21 
2024-02-10 04:36:15,002 EPOCH 1458
2024-02-10 04:36:31,414 Epoch 1458: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 04:36:31,415 EPOCH 1459
2024-02-10 04:36:47,574 Epoch 1459: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.26 
2024-02-10 04:36:47,575 EPOCH 1460
2024-02-10 04:37:03,825 Epoch 1460: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.24 
2024-02-10 04:37:03,826 EPOCH 1461
2024-02-10 04:37:20,242 Epoch 1461: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 04:37:20,243 EPOCH 1462
2024-02-10 04:37:36,176 Epoch 1462: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 04:37:36,177 EPOCH 1463
2024-02-10 04:37:52,223 Epoch 1463: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.20 
2024-02-10 04:37:52,224 EPOCH 1464
2024-02-10 04:38:08,443 Epoch 1464: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 04:38:08,444 EPOCH 1465
2024-02-10 04:38:24,488 Epoch 1465: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 04:38:24,489 EPOCH 1466
2024-02-10 04:38:40,340 Epoch 1466: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 04:38:40,340 EPOCH 1467
2024-02-10 04:38:49,824 [Epoch: 1467 Step: 00013200] Batch Recognition Loss:   0.006144 => Gls Tokens per Sec:      715 || Batch Translation Loss:   0.008296 => Txt Tokens per Sec:     2041 || Lr: 0.000100
2024-02-10 04:38:56,408 Epoch 1467: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 04:38:56,409 EPOCH 1468
2024-02-10 04:39:12,549 Epoch 1468: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 04:39:12,549 EPOCH 1469
2024-02-10 04:39:28,798 Epoch 1469: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 04:39:28,799 EPOCH 1470
2024-02-10 04:39:44,970 Epoch 1470: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 04:39:44,971 EPOCH 1471
2024-02-10 04:40:00,961 Epoch 1471: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 04:40:00,961 EPOCH 1472
2024-02-10 04:40:16,998 Epoch 1472: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 04:40:16,999 EPOCH 1473
2024-02-10 04:40:32,997 Epoch 1473: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 04:40:32,998 EPOCH 1474
2024-02-10 04:40:49,361 Epoch 1474: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 04:40:49,361 EPOCH 1475
2024-02-10 04:41:05,527 Epoch 1475: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 04:41:05,528 EPOCH 1476
2024-02-10 04:41:21,804 Epoch 1476: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 04:41:21,805 EPOCH 1477
2024-02-10 04:41:37,568 Epoch 1477: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 04:41:37,569 EPOCH 1478
2024-02-10 04:41:52,402 [Epoch: 1478 Step: 00013300] Batch Recognition Loss:   0.000855 => Gls Tokens per Sec:      543 || Batch Translation Loss:   0.014617 => Txt Tokens per Sec:     1492 || Lr: 0.000100
2024-02-10 04:41:53,515 Epoch 1478: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 04:41:53,516 EPOCH 1479
2024-02-10 04:42:09,643 Epoch 1479: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 04:42:09,643 EPOCH 1480
2024-02-10 04:42:25,603 Epoch 1480: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 04:42:25,603 EPOCH 1481
2024-02-10 04:42:41,545 Epoch 1481: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 04:42:41,546 EPOCH 1482
2024-02-10 04:42:57,494 Epoch 1482: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 04:42:57,494 EPOCH 1483
2024-02-10 04:43:13,809 Epoch 1483: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 04:43:13,809 EPOCH 1484
2024-02-10 04:43:29,881 Epoch 1484: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 04:43:29,881 EPOCH 1485
2024-02-10 04:43:45,953 Epoch 1485: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 04:43:45,953 EPOCH 1486
2024-02-10 04:44:01,726 Epoch 1486: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 04:44:01,727 EPOCH 1487
2024-02-10 04:44:17,637 Epoch 1487: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 04:44:17,637 EPOCH 1488
2024-02-10 04:44:33,773 Epoch 1488: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 04:44:33,774 EPOCH 1489
2024-02-10 04:44:43,835 [Epoch: 1489 Step: 00013400] Batch Recognition Loss:   0.000238 => Gls Tokens per Sec:      928 || Batch Translation Loss:   0.017104 => Txt Tokens per Sec:     2482 || Lr: 0.000100
2024-02-10 04:44:49,560 Epoch 1489: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 04:44:49,561 EPOCH 1490
2024-02-10 04:45:05,768 Epoch 1490: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 04:45:05,768 EPOCH 1491
2024-02-10 04:45:21,625 Epoch 1491: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 04:45:21,626 EPOCH 1492
2024-02-10 04:45:37,667 Epoch 1492: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 04:45:37,668 EPOCH 1493
2024-02-10 04:45:53,585 Epoch 1493: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 04:45:53,585 EPOCH 1494
2024-02-10 04:46:09,976 Epoch 1494: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 04:46:09,978 EPOCH 1495
2024-02-10 04:46:26,295 Epoch 1495: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 04:46:26,295 EPOCH 1496
2024-02-10 04:46:42,081 Epoch 1496: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 04:46:42,081 EPOCH 1497
2024-02-10 04:46:58,151 Epoch 1497: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-10 04:46:58,152 EPOCH 1498
2024-02-10 04:47:14,092 Epoch 1498: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-10 04:47:14,093 EPOCH 1499
2024-02-10 04:47:30,259 Epoch 1499: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-10 04:47:30,259 EPOCH 1500
2024-02-10 04:47:46,260 [Epoch: 1500 Step: 00013500] Batch Recognition Loss:   0.002002 => Gls Tokens per Sec:      664 || Batch Translation Loss:   0.051332 => Txt Tokens per Sec:     1836 || Lr: 0.000100
2024-02-10 04:47:46,260 Epoch 1500: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-10 04:47:46,261 EPOCH 1501
2024-02-10 04:48:02,105 Epoch 1501: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-10 04:48:02,106 EPOCH 1502
2024-02-10 04:48:18,517 Epoch 1502: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-10 04:48:18,518 EPOCH 1503
2024-02-10 04:48:34,383 Epoch 1503: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-10 04:48:34,384 EPOCH 1504
2024-02-10 04:48:50,620 Epoch 1504: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 04:48:50,621 EPOCH 1505
2024-02-10 04:49:06,865 Epoch 1505: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 04:49:06,865 EPOCH 1506
2024-02-10 04:49:22,561 Epoch 1506: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-10 04:49:22,561 EPOCH 1507
2024-02-10 04:49:38,257 Epoch 1507: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 04:49:38,258 EPOCH 1508
2024-02-10 04:49:54,268 Epoch 1508: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-10 04:49:54,268 EPOCH 1509
2024-02-10 04:50:10,528 Epoch 1509: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 04:50:10,529 EPOCH 1510
2024-02-10 04:50:26,707 Epoch 1510: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-10 04:50:26,707 EPOCH 1511
2024-02-10 04:50:42,655 Epoch 1511: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 04:50:42,656 EPOCH 1512
2024-02-10 04:50:48,383 [Epoch: 1512 Step: 00013600] Batch Recognition Loss:   0.000769 => Gls Tokens per Sec:      224 || Batch Translation Loss:   0.036726 => Txt Tokens per Sec:      771 || Lr: 0.000100
2024-02-10 04:50:58,632 Epoch 1512: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 04:50:58,632 EPOCH 1513
2024-02-10 04:51:14,811 Epoch 1513: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 04:51:14,811 EPOCH 1514
2024-02-10 04:51:31,250 Epoch 1514: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 04:51:31,251 EPOCH 1515
2024-02-10 04:51:47,736 Epoch 1515: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 04:51:47,736 EPOCH 1516
2024-02-10 04:52:03,921 Epoch 1516: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 04:52:03,922 EPOCH 1517
2024-02-10 04:52:19,742 Epoch 1517: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 04:52:19,742 EPOCH 1518
2024-02-10 04:52:35,864 Epoch 1518: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-10 04:52:35,864 EPOCH 1519
2024-02-10 04:52:51,903 Epoch 1519: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 04:52:51,904 EPOCH 1520
2024-02-10 04:53:07,962 Epoch 1520: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-10 04:53:07,962 EPOCH 1521
2024-02-10 04:53:23,902 Epoch 1521: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 04:53:23,902 EPOCH 1522
2024-02-10 04:53:39,866 Epoch 1522: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 04:53:39,866 EPOCH 1523
2024-02-10 04:53:40,580 [Epoch: 1523 Step: 00013700] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:     3596 || Batch Translation Loss:   0.025928 => Txt Tokens per Sec:     8987 || Lr: 0.000100
2024-02-10 04:53:55,799 Epoch 1523: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 04:53:55,800 EPOCH 1524
2024-02-10 04:54:11,888 Epoch 1524: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 04:54:11,889 EPOCH 1525
2024-02-10 04:54:28,137 Epoch 1525: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-10 04:54:28,138 EPOCH 1526
2024-02-10 04:54:44,537 Epoch 1526: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 04:54:44,537 EPOCH 1527
2024-02-10 04:55:00,658 Epoch 1527: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 04:55:00,659 EPOCH 1528
2024-02-10 04:55:16,898 Epoch 1528: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-10 04:55:16,899 EPOCH 1529
2024-02-10 04:55:32,842 Epoch 1529: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 04:55:32,842 EPOCH 1530
2024-02-10 04:55:49,058 Epoch 1530: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 04:55:49,059 EPOCH 1531
2024-02-10 04:56:05,437 Epoch 1531: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 04:56:05,438 EPOCH 1532
2024-02-10 04:56:21,633 Epoch 1532: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 04:56:21,633 EPOCH 1533
2024-02-10 04:56:37,770 Epoch 1533: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 04:56:37,771 EPOCH 1534
2024-02-10 04:56:42,843 [Epoch: 1534 Step: 00013800] Batch Recognition Loss:   0.000375 => Gls Tokens per Sec:      580 || Batch Translation Loss:   0.009452 => Txt Tokens per Sec:     1283 || Lr: 0.000100
2024-02-10 04:56:54,064 Epoch 1534: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 04:56:54,065 EPOCH 1535
2024-02-10 04:57:10,014 Epoch 1535: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 04:57:10,014 EPOCH 1536
2024-02-10 04:57:26,130 Epoch 1536: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 04:57:26,131 EPOCH 1537
2024-02-10 04:57:42,412 Epoch 1537: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 04:57:42,412 EPOCH 1538
2024-02-10 04:57:58,348 Epoch 1538: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 04:57:58,349 EPOCH 1539
2024-02-10 04:58:14,224 Epoch 1539: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 04:58:14,225 EPOCH 1540
2024-02-10 04:58:30,531 Epoch 1540: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 04:58:30,532 EPOCH 1541
2024-02-10 04:58:46,595 Epoch 1541: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 04:58:46,596 EPOCH 1542
2024-02-10 04:59:02,535 Epoch 1542: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 04:59:02,536 EPOCH 1543
2024-02-10 04:59:19,220 Epoch 1543: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 04:59:19,221 EPOCH 1544
2024-02-10 04:59:35,268 Epoch 1544: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 04:59:35,268 EPOCH 1545
2024-02-10 04:59:42,370 [Epoch: 1545 Step: 00013900] Batch Recognition Loss:   0.000596 => Gls Tokens per Sec:      721 || Batch Translation Loss:   0.008957 => Txt Tokens per Sec:     1905 || Lr: 0.000100
2024-02-10 04:59:51,587 Epoch 1545: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 04:59:51,588 EPOCH 1546
2024-02-10 05:00:07,613 Epoch 1546: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 05:00:07,614 EPOCH 1547
2024-02-10 05:00:23,532 Epoch 1547: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 05:00:23,532 EPOCH 1548
2024-02-10 05:00:39,780 Epoch 1548: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 05:00:39,781 EPOCH 1549
2024-02-10 05:00:55,734 Epoch 1549: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 05:00:55,735 EPOCH 1550
2024-02-10 05:01:11,712 Epoch 1550: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 05:01:11,713 EPOCH 1551
2024-02-10 05:01:27,958 Epoch 1551: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 05:01:27,959 EPOCH 1552
2024-02-10 05:01:43,780 Epoch 1552: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 05:01:43,781 EPOCH 1553
2024-02-10 05:01:59,849 Epoch 1553: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 05:01:59,849 EPOCH 1554
2024-02-10 05:02:16,002 Epoch 1554: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 05:02:16,003 EPOCH 1555
2024-02-10 05:02:31,931 Epoch 1555: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-10 05:02:31,932 EPOCH 1556
2024-02-10 05:02:38,504 [Epoch: 1556 Step: 00014000] Batch Recognition Loss:   0.002847 => Gls Tokens per Sec:      837 || Batch Translation Loss:   0.040282 => Txt Tokens per Sec:     2398 || Lr: 0.000100
2024-02-10 05:03:50,356 Validation result at epoch 1556, step    14000: duration: 71.8500s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.45651	Translation Loss: 92948.84375	PPL: 10761.64648
	Eval Metric: BLEU
	WER 4.87	(DEL: 0.00,	INS: 0.00,	SUB: 4.87)
	BLEU-4 0.39	(BLEU-1: 11.24,	BLEU-2: 3.47,	BLEU-3: 1.19,	BLEU-4: 0.39)
	CHRF 17.15	ROUGE 9.20
2024-02-10 05:03:50,358 Logging Recognition and Translation Outputs
2024-02-10 05:03:50,358 ========================================================================================================================
2024-02-10 05:03:50,358 Logging Sequence: 177_50.00
2024-02-10 05:03:50,359 	Gloss Reference :	A B+C+D+E
2024-02-10 05:03:50,359 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 05:03:50,359 	Gloss Alignment :	         
2024-02-10 05:03:50,359 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 05:03:50,361 	Text Reference  :	**** ** ** ***** a        similar reward of  rs  50000 was announced for    information against his  associate ajay  kumar
2024-02-10 05:03:50,361 	Text Hypothesis :	will be in tokyo olympics sushil  kumar  who led to    the motive    behind the         brawl   that killed    sagar rana 
2024-02-10 05:03:50,361 	Text Alignment  :	I    I  I  I     S        S       S      S   S   S     S   S         S      S           S       S    S         S     S    
2024-02-10 05:03:50,361 ========================================================================================================================
2024-02-10 05:03:50,361 Logging Sequence: 136_175.00
2024-02-10 05:03:50,362 	Gloss Reference :	A B+C+D+E  
2024-02-10 05:03:50,362 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-10 05:03:50,362 	Gloss Alignment :	  S        
2024-02-10 05:03:50,362 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 05:03:50,363 	Text Reference  :	after 49     years india' hockey team          beat britain and       qualified for the        semi-finals
2024-02-10 05:03:50,363 	Text Hypothesis :	4     anyone found to     be     proselytizing for  other   religions would     be  criminally prosecuted 
2024-02-10 05:03:50,363 	Text Alignment  :	S     S      S     S      S      S             S    S       S         S         S   S          S          
2024-02-10 05:03:50,364 ========================================================================================================================
2024-02-10 05:03:50,364 Logging Sequence: 126_159.00
2024-02-10 05:03:50,364 	Gloss Reference :	A B+C+D+E
2024-02-10 05:03:50,364 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 05:03:50,364 	Gloss Alignment :	         
2024-02-10 05:03:50,364 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 05:03:50,365 	Text Reference  :	despite multiple challenges and injuries you  did   not give up  
2024-02-10 05:03:50,365 	Text Hypothesis :	******* ******** ********** he  played   very grate to  his  fans
2024-02-10 05:03:50,365 	Text Alignment  :	D       D        D          S   S        S    S     S   S    S   
2024-02-10 05:03:50,366 ========================================================================================================================
2024-02-10 05:03:50,366 Logging Sequence: 70_88.00
2024-02-10 05:03:50,366 	Gloss Reference :	A B+C+D+E
2024-02-10 05:03:50,366 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 05:03:50,366 	Gloss Alignment :	         
2024-02-10 05:03:50,366 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 05:03:50,367 	Text Reference  :	two coca-cola bottles were  placed on   the ***** table next    to the ****** mic 
2024-02-10 05:03:50,367 	Text Hypothesis :	*** ********* ******* sadly the    lost the match and   england at the second time
2024-02-10 05:03:50,368 	Text Alignment  :	D   D         D       S     S      S        I     S     S       S      I      S   
2024-02-10 05:03:50,368 ========================================================================================================================
2024-02-10 05:03:50,368 Logging Sequence: 54_201.00
2024-02-10 05:03:50,368 	Gloss Reference :	A B+C+D+E
2024-02-10 05:03:50,368 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 05:03:50,368 	Gloss Alignment :	         
2024-02-10 05:03:50,369 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 05:03:50,372 	Text Reference  :	there is a huge demand       mostly from    non-resident indians nris who  are    excited   to see          the match ******* and they    have booked the       hotel ******* ***** ** rooms
2024-02-10 05:03:50,372 	Text Hypothesis :	***** ** * the  middle-class fans   usually decide       at      the  last moment depending on availability of  match tickets if  tickets are  not    available hotel booking would go waste
2024-02-10 05:03:50,372 	Text Alignment  :	D     D  D S    S            S      S       S            S       S    S    S      S         S  S            S         I       S   S       S    S      S               I       I     I  S    
2024-02-10 05:03:50,372 ========================================================================================================================
2024-02-10 05:04:00,488 Epoch 1556: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 05:04:00,489 EPOCH 1557
2024-02-10 05:04:16,838 Epoch 1557: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 05:04:16,839 EPOCH 1558
2024-02-10 05:04:32,852 Epoch 1558: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 05:04:32,852 EPOCH 1559
2024-02-10 05:04:48,646 Epoch 1559: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 05:04:48,647 EPOCH 1560
2024-02-10 05:05:04,421 Epoch 1560: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 05:05:04,422 EPOCH 1561
2024-02-10 05:05:21,092 Epoch 1561: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 05:05:21,093 EPOCH 1562
2024-02-10 05:05:37,015 Epoch 1562: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 05:05:37,016 EPOCH 1563
2024-02-10 05:05:52,940 Epoch 1563: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 05:05:52,940 EPOCH 1564
2024-02-10 05:06:09,384 Epoch 1564: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 05:06:09,384 EPOCH 1565
2024-02-10 05:06:25,411 Epoch 1565: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 05:06:25,411 EPOCH 1566
2024-02-10 05:06:41,232 Epoch 1566: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 05:06:41,233 EPOCH 1567
2024-02-10 05:06:50,672 [Epoch: 1567 Step: 00014100] Batch Recognition Loss:   0.000672 => Gls Tokens per Sec:      718 || Batch Translation Loss:   0.028221 => Txt Tokens per Sec:     1996 || Lr: 0.000100
2024-02-10 05:06:57,194 Epoch 1567: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 05:06:57,195 EPOCH 1568
2024-02-10 05:07:13,591 Epoch 1568: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 05:07:13,592 EPOCH 1569
2024-02-10 05:07:29,201 Epoch 1569: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 05:07:29,201 EPOCH 1570
2024-02-10 05:07:45,681 Epoch 1570: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 05:07:45,681 EPOCH 1571
2024-02-10 05:08:01,294 Epoch 1571: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 05:08:01,295 EPOCH 1572
2024-02-10 05:08:17,586 Epoch 1572: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-10 05:08:17,586 EPOCH 1573
2024-02-10 05:08:33,365 Epoch 1573: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-10 05:08:33,365 EPOCH 1574
2024-02-10 05:08:49,355 Epoch 1574: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-10 05:08:49,356 EPOCH 1575
2024-02-10 05:09:05,263 Epoch 1575: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-10 05:09:05,264 EPOCH 1576
2024-02-10 05:09:21,417 Epoch 1576: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-10 05:09:21,418 EPOCH 1577
2024-02-10 05:09:37,771 Epoch 1577: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-10 05:09:37,771 EPOCH 1578
2024-02-10 05:09:44,606 [Epoch: 1578 Step: 00014200] Batch Recognition Loss:   0.000333 => Gls Tokens per Sec:     1180 || Batch Translation Loss:   0.038827 => Txt Tokens per Sec:     3057 || Lr: 0.000100
2024-02-10 05:09:53,793 Epoch 1578: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 05:09:53,794 EPOCH 1579
2024-02-10 05:10:09,981 Epoch 1579: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-10 05:10:09,981 EPOCH 1580
2024-02-10 05:10:26,108 Epoch 1580: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 05:10:26,109 EPOCH 1581
2024-02-10 05:10:42,178 Epoch 1581: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 05:10:42,178 EPOCH 1582
2024-02-10 05:10:58,284 Epoch 1582: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 05:10:58,285 EPOCH 1583
2024-02-10 05:11:14,163 Epoch 1583: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-10 05:11:14,164 EPOCH 1584
2024-02-10 05:11:30,156 Epoch 1584: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 05:11:30,157 EPOCH 1585
2024-02-10 05:11:46,234 Epoch 1585: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-10 05:11:46,235 EPOCH 1586
2024-02-10 05:12:02,213 Epoch 1586: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 05:12:02,214 EPOCH 1587
2024-02-10 05:12:18,328 Epoch 1587: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-10 05:12:18,328 EPOCH 1588
2024-02-10 05:12:34,726 Epoch 1588: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 05:12:34,727 EPOCH 1589
2024-02-10 05:12:50,348 [Epoch: 1589 Step: 00014300] Batch Recognition Loss:   0.000328 => Gls Tokens per Sec:      598 || Batch Translation Loss:   0.023837 => Txt Tokens per Sec:     1701 || Lr: 0.000100
2024-02-10 05:12:50,657 Epoch 1589: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 05:12:50,657 EPOCH 1590
2024-02-10 05:13:06,783 Epoch 1590: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 05:13:06,784 EPOCH 1591
2024-02-10 05:13:22,804 Epoch 1591: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 05:13:22,804 EPOCH 1592
2024-02-10 05:13:38,802 Epoch 1592: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 05:13:38,803 EPOCH 1593
2024-02-10 05:13:54,795 Epoch 1593: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 05:13:54,795 EPOCH 1594
2024-02-10 05:14:10,942 Epoch 1594: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 05:14:10,943 EPOCH 1595
2024-02-10 05:14:27,043 Epoch 1595: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-10 05:14:27,044 EPOCH 1596
2024-02-10 05:14:43,193 Epoch 1596: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-10 05:14:43,194 EPOCH 1597
2024-02-10 05:14:59,645 Epoch 1597: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-10 05:14:59,646 EPOCH 1598
2024-02-10 05:15:15,688 Epoch 1598: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-10 05:15:15,689 EPOCH 1599
2024-02-10 05:15:31,689 Epoch 1599: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.11 
2024-02-10 05:15:31,690 EPOCH 1600
2024-02-10 05:15:47,721 [Epoch: 1600 Step: 00014400] Batch Recognition Loss:   0.001932 => Gls Tokens per Sec:      663 || Batch Translation Loss:   0.344339 => Txt Tokens per Sec:     1833 || Lr: 0.000100
2024-02-10 05:15:47,721 Epoch 1600: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.31 
2024-02-10 05:15:47,721 EPOCH 1601
2024-02-10 05:16:03,980 Epoch 1601: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.68 
2024-02-10 05:16:03,980 EPOCH 1602
2024-02-10 05:16:19,813 Epoch 1602: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.12 
2024-02-10 05:16:19,813 EPOCH 1603
2024-02-10 05:16:36,095 Epoch 1603: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.34 
2024-02-10 05:16:36,095 EPOCH 1604
2024-02-10 05:16:52,190 Epoch 1604: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.46 
2024-02-10 05:16:52,191 EPOCH 1605
2024-02-10 05:17:08,063 Epoch 1605: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.22 
2024-02-10 05:17:08,064 EPOCH 1606
2024-02-10 05:17:24,313 Epoch 1606: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-10 05:17:24,314 EPOCH 1607
2024-02-10 05:17:40,269 Epoch 1607: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.94 
2024-02-10 05:17:40,270 EPOCH 1608
2024-02-10 05:17:56,322 Epoch 1608: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.75 
2024-02-10 05:17:56,322 EPOCH 1609
2024-02-10 05:18:12,452 Epoch 1609: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-10 05:18:12,452 EPOCH 1610
2024-02-10 05:18:28,372 Epoch 1610: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-10 05:18:28,372 EPOCH 1611
2024-02-10 05:18:44,495 Epoch 1611: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-10 05:18:44,496 EPOCH 1612
2024-02-10 05:18:44,987 [Epoch: 1612 Step: 00014500] Batch Recognition Loss:   0.000734 => Gls Tokens per Sec:     2618 || Batch Translation Loss:   0.033096 => Txt Tokens per Sec:     6652 || Lr: 0.000100
2024-02-10 05:19:01,229 Epoch 1612: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-10 05:19:01,229 EPOCH 1613
2024-02-10 05:19:17,234 Epoch 1613: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-10 05:19:17,235 EPOCH 1614
2024-02-10 05:19:33,408 Epoch 1614: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-10 05:19:33,408 EPOCH 1615
2024-02-10 05:19:49,017 Epoch 1615: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-10 05:19:49,017 EPOCH 1616
2024-02-10 05:20:05,587 Epoch 1616: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-10 05:20:05,588 EPOCH 1617
2024-02-10 05:20:21,281 Epoch 1617: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-10 05:20:21,282 EPOCH 1618
2024-02-10 05:20:37,526 Epoch 1618: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-10 05:20:37,526 EPOCH 1619
2024-02-10 05:20:53,296 Epoch 1619: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-10 05:20:53,296 EPOCH 1620
2024-02-10 05:21:09,640 Epoch 1620: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-10 05:21:09,640 EPOCH 1621
2024-02-10 05:21:25,475 Epoch 1621: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-10 05:21:25,475 EPOCH 1622
2024-02-10 05:21:41,410 Epoch 1622: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 05:21:41,411 EPOCH 1623
2024-02-10 05:21:42,466 [Epoch: 1623 Step: 00014600] Batch Recognition Loss:   0.000625 => Gls Tokens per Sec:     2429 || Batch Translation Loss:   0.029261 => Txt Tokens per Sec:     6198 || Lr: 0.000100
2024-02-10 05:21:57,566 Epoch 1623: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 05:21:57,566 EPOCH 1624
2024-02-10 05:22:14,060 Epoch 1624: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 05:22:14,061 EPOCH 1625
2024-02-10 05:22:30,110 Epoch 1625: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 05:22:30,111 EPOCH 1626
2024-02-10 05:22:45,997 Epoch 1626: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 05:22:45,997 EPOCH 1627
2024-02-10 05:23:02,121 Epoch 1627: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 05:23:02,122 EPOCH 1628
2024-02-10 05:23:18,322 Epoch 1628: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 05:23:18,322 EPOCH 1629
2024-02-10 05:23:34,252 Epoch 1629: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 05:23:34,252 EPOCH 1630
2024-02-10 05:23:50,171 Epoch 1630: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-10 05:23:50,171 EPOCH 1631
2024-02-10 05:24:06,167 Epoch 1631: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-10 05:24:06,168 EPOCH 1632
2024-02-10 05:24:22,162 Epoch 1632: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-10 05:24:22,163 EPOCH 1633
2024-02-10 05:24:38,363 Epoch 1633: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-10 05:24:38,364 EPOCH 1634
2024-02-10 05:24:39,788 [Epoch: 1634 Step: 00014700] Batch Recognition Loss:   0.000297 => Gls Tokens per Sec:     2699 || Batch Translation Loss:   0.048527 => Txt Tokens per Sec:     7082 || Lr: 0.000100
2024-02-10 05:24:54,554 Epoch 1634: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-10 05:24:54,554 EPOCH 1635
2024-02-10 05:25:10,641 Epoch 1635: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-10 05:25:10,642 EPOCH 1636
2024-02-10 05:25:26,694 Epoch 1636: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 05:25:26,695 EPOCH 1637
2024-02-10 05:25:43,341 Epoch 1637: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 05:25:43,343 EPOCH 1638
2024-02-10 05:25:59,498 Epoch 1638: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 05:25:59,498 EPOCH 1639
2024-02-10 05:26:15,576 Epoch 1639: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 05:26:15,576 EPOCH 1640
2024-02-10 05:26:31,286 Epoch 1640: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 05:26:31,287 EPOCH 1641
2024-02-10 05:26:47,531 Epoch 1641: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 05:26:47,531 EPOCH 1642
2024-02-10 05:27:03,498 Epoch 1642: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 05:27:03,499 EPOCH 1643
2024-02-10 05:27:19,672 Epoch 1643: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 05:27:19,673 EPOCH 1644
2024-02-10 05:27:35,881 Epoch 1644: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 05:27:35,881 EPOCH 1645
2024-02-10 05:27:37,866 [Epoch: 1645 Step: 00014800] Batch Recognition Loss:   0.000230 => Gls Tokens per Sec:     2581 || Batch Translation Loss:   0.016902 => Txt Tokens per Sec:     6759 || Lr: 0.000100
2024-02-10 05:27:51,811 Epoch 1645: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 05:27:51,811 EPOCH 1646
2024-02-10 05:28:08,012 Epoch 1646: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 05:28:08,013 EPOCH 1647
2024-02-10 05:28:24,148 Epoch 1647: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 05:28:24,149 EPOCH 1648
2024-02-10 05:28:40,436 Epoch 1648: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 05:28:40,437 EPOCH 1649
2024-02-10 05:28:56,487 Epoch 1649: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 05:28:56,487 EPOCH 1650
2024-02-10 05:29:12,759 Epoch 1650: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 05:29:12,760 EPOCH 1651
2024-02-10 05:29:28,828 Epoch 1651: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 05:29:28,828 EPOCH 1652
2024-02-10 05:29:44,984 Epoch 1652: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 05:29:44,985 EPOCH 1653
2024-02-10 05:30:00,991 Epoch 1653: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 05:30:00,992 EPOCH 1654
2024-02-10 05:30:17,050 Epoch 1654: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 05:30:17,050 EPOCH 1655
2024-02-10 05:30:33,246 Epoch 1655: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 05:30:33,246 EPOCH 1656
2024-02-10 05:30:45,329 [Epoch: 1656 Step: 00014900] Batch Recognition Loss:   0.000596 => Gls Tokens per Sec:      455 || Batch Translation Loss:   0.020348 => Txt Tokens per Sec:     1360 || Lr: 0.000100
2024-02-10 05:30:49,662 Epoch 1656: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 05:30:49,662 EPOCH 1657
2024-02-10 05:31:05,755 Epoch 1657: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 05:31:05,756 EPOCH 1658
2024-02-10 05:31:22,191 Epoch 1658: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 05:31:22,192 EPOCH 1659
2024-02-10 05:31:38,233 Epoch 1659: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 05:31:38,234 EPOCH 1660
2024-02-10 05:31:54,101 Epoch 1660: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 05:31:54,101 EPOCH 1661
2024-02-10 05:32:10,725 Epoch 1661: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 05:32:10,726 EPOCH 1662
2024-02-10 05:32:27,061 Epoch 1662: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 05:32:27,061 EPOCH 1663
2024-02-10 05:32:43,179 Epoch 1663: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 05:32:43,179 EPOCH 1664
2024-02-10 05:32:59,386 Epoch 1664: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 05:32:59,387 EPOCH 1665
2024-02-10 05:33:15,783 Epoch 1665: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 05:33:15,784 EPOCH 1666
2024-02-10 05:33:31,621 Epoch 1666: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 05:33:31,622 EPOCH 1667
2024-02-10 05:33:42,536 [Epoch: 1667 Step: 00015000] Batch Recognition Loss:   0.000247 => Gls Tokens per Sec:      704 || Batch Translation Loss:   0.016426 => Txt Tokens per Sec:     1948 || Lr: 0.000100
2024-02-10 05:33:47,859 Epoch 1667: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 05:33:47,859 EPOCH 1668
2024-02-10 05:34:03,997 Epoch 1668: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 05:34:03,998 EPOCH 1669
2024-02-10 05:34:19,995 Epoch 1669: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 05:34:19,996 EPOCH 1670
2024-02-10 05:34:36,238 Epoch 1670: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 05:34:36,239 EPOCH 1671
2024-02-10 05:34:51,940 Epoch 1671: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 05:34:51,940 EPOCH 1672
2024-02-10 05:35:08,176 Epoch 1672: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 05:35:08,177 EPOCH 1673
2024-02-10 05:35:24,146 Epoch 1673: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 05:35:24,148 EPOCH 1674
2024-02-10 05:35:40,260 Epoch 1674: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 05:35:40,261 EPOCH 1675
2024-02-10 05:35:56,646 Epoch 1675: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 05:35:56,647 EPOCH 1676
2024-02-10 05:36:12,725 Epoch 1676: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 05:36:12,725 EPOCH 1677
2024-02-10 05:36:28,660 Epoch 1677: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 05:36:28,660 EPOCH 1678
2024-02-10 05:36:43,572 [Epoch: 1678 Step: 00015100] Batch Recognition Loss:   0.000348 => Gls Tokens per Sec:      541 || Batch Translation Loss:   0.023130 => Txt Tokens per Sec:     1519 || Lr: 0.000100
2024-02-10 05:36:44,664 Epoch 1678: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 05:36:44,664 EPOCH 1679
2024-02-10 05:37:00,667 Epoch 1679: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 05:37:00,668 EPOCH 1680
2024-02-10 05:37:16,977 Epoch 1680: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 05:37:16,978 EPOCH 1681
2024-02-10 05:37:33,052 Epoch 1681: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 05:37:33,053 EPOCH 1682
2024-02-10 05:37:49,053 Epoch 1682: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 05:37:49,053 EPOCH 1683
2024-02-10 05:38:05,291 Epoch 1683: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 05:38:05,291 EPOCH 1684
2024-02-10 05:38:21,280 Epoch 1684: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 05:38:21,281 EPOCH 1685
2024-02-10 05:38:37,253 Epoch 1685: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 05:38:37,254 EPOCH 1686
2024-02-10 05:38:53,223 Epoch 1686: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 05:38:53,224 EPOCH 1687
2024-02-10 05:39:09,369 Epoch 1687: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 05:39:09,370 EPOCH 1688
2024-02-10 05:39:25,530 Epoch 1688: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 05:39:25,531 EPOCH 1689
2024-02-10 05:39:37,405 [Epoch: 1689 Step: 00015200] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:      863 || Batch Translation Loss:   0.020326 => Txt Tokens per Sec:     2360 || Lr: 0.000100
2024-02-10 05:39:41,675 Epoch 1689: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 05:39:41,675 EPOCH 1690
2024-02-10 05:39:57,771 Epoch 1690: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 05:39:57,771 EPOCH 1691
2024-02-10 05:40:13,744 Epoch 1691: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 05:40:13,745 EPOCH 1692
2024-02-10 05:40:29,427 Epoch 1692: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 05:40:29,428 EPOCH 1693
2024-02-10 05:40:45,342 Epoch 1693: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 05:40:45,343 EPOCH 1694
2024-02-10 05:41:01,580 Epoch 1694: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 05:41:01,581 EPOCH 1695
2024-02-10 05:41:17,838 Epoch 1695: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 05:41:17,838 EPOCH 1696
2024-02-10 05:41:33,980 Epoch 1696: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 05:41:33,980 EPOCH 1697
2024-02-10 05:41:50,091 Epoch 1697: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 05:41:50,092 EPOCH 1698
2024-02-10 05:42:05,917 Epoch 1698: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 05:42:05,918 EPOCH 1699
2024-02-10 05:42:22,019 Epoch 1699: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 05:42:22,020 EPOCH 1700
2024-02-10 05:42:38,352 [Epoch: 1700 Step: 00015300] Batch Recognition Loss:   0.000238 => Gls Tokens per Sec:      650 || Batch Translation Loss:   0.018750 => Txt Tokens per Sec:     1799 || Lr: 0.000100
2024-02-10 05:42:38,352 Epoch 1700: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-10 05:42:38,352 EPOCH 1701
2024-02-10 05:42:54,340 Epoch 1701: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-10 05:42:54,340 EPOCH 1702
2024-02-10 05:43:09,999 Epoch 1702: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.83 
2024-02-10 05:43:10,000 EPOCH 1703
2024-02-10 05:43:26,280 Epoch 1703: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.05 
2024-02-10 05:43:26,281 EPOCH 1704
2024-02-10 05:43:42,235 Epoch 1704: Total Training Recognition Loss 0.03  Total Training Translation Loss 10.67 
2024-02-10 05:43:42,236 EPOCH 1705
2024-02-10 05:43:57,779 Epoch 1705: Total Training Recognition Loss 0.06  Total Training Translation Loss 7.72 
2024-02-10 05:43:57,779 EPOCH 1706
2024-02-10 05:44:13,935 Epoch 1706: Total Training Recognition Loss 0.10  Total Training Translation Loss 16.53 
2024-02-10 05:44:13,936 EPOCH 1707
2024-02-10 05:44:29,765 Epoch 1707: Total Training Recognition Loss 0.08  Total Training Translation Loss 7.97 
2024-02-10 05:44:29,766 EPOCH 1708
2024-02-10 05:44:45,841 Epoch 1708: Total Training Recognition Loss 0.06  Total Training Translation Loss 5.76 
2024-02-10 05:44:45,842 EPOCH 1709
2024-02-10 05:45:02,148 Epoch 1709: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.46 
2024-02-10 05:45:02,148 EPOCH 1710
2024-02-10 05:45:18,634 Epoch 1710: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.43 
2024-02-10 05:45:18,634 EPOCH 1711
2024-02-10 05:45:34,730 Epoch 1711: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.84 
2024-02-10 05:45:34,731 EPOCH 1712
2024-02-10 05:45:35,625 [Epoch: 1712 Step: 00015400] Batch Recognition Loss:   0.002432 => Gls Tokens per Sec:     1437 || Batch Translation Loss:   0.080506 => Txt Tokens per Sec:     4366 || Lr: 0.000100
2024-02-10 05:45:51,069 Epoch 1712: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-10 05:45:51,069 EPOCH 1713
2024-02-10 05:46:07,175 Epoch 1713: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.46 
2024-02-10 05:46:07,176 EPOCH 1714
2024-02-10 05:46:23,499 Epoch 1714: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-10 05:46:23,501 EPOCH 1715
2024-02-10 05:46:39,747 Epoch 1715: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-10 05:46:39,748 EPOCH 1716
2024-02-10 05:46:55,861 Epoch 1716: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-10 05:46:55,862 EPOCH 1717
2024-02-10 05:47:11,842 Epoch 1717: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 05:47:11,843 EPOCH 1718
2024-02-10 05:47:28,137 Epoch 1718: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 05:47:28,137 EPOCH 1719
2024-02-10 05:47:44,005 Epoch 1719: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 05:47:44,006 EPOCH 1720
2024-02-10 05:47:59,850 Epoch 1720: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 05:47:59,851 EPOCH 1721
2024-02-10 05:48:16,169 Epoch 1721: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 05:48:16,170 EPOCH 1722
2024-02-10 05:48:32,514 Epoch 1722: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 05:48:32,515 EPOCH 1723
2024-02-10 05:48:35,935 [Epoch: 1723 Step: 00015500] Batch Recognition Loss:   0.000520 => Gls Tokens per Sec:      749 || Batch Translation Loss:   0.017755 => Txt Tokens per Sec:     1877 || Lr: 0.000100
2024-02-10 05:48:48,522 Epoch 1723: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 05:48:48,523 EPOCH 1724
2024-02-10 05:49:05,175 Epoch 1724: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 05:49:05,175 EPOCH 1725
2024-02-10 05:49:21,658 Epoch 1725: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 05:49:21,658 EPOCH 1726
2024-02-10 05:49:37,388 Epoch 1726: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 05:49:37,388 EPOCH 1727
2024-02-10 05:49:53,738 Epoch 1727: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 05:49:53,739 EPOCH 1728
2024-02-10 05:50:10,207 Epoch 1728: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 05:50:10,209 EPOCH 1729
2024-02-10 05:50:26,478 Epoch 1729: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 05:50:26,478 EPOCH 1730
2024-02-10 05:50:42,414 Epoch 1730: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 05:50:42,414 EPOCH 1731
2024-02-10 05:50:58,334 Epoch 1731: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 05:50:58,334 EPOCH 1732
2024-02-10 05:51:14,530 Epoch 1732: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 05:51:14,530 EPOCH 1733
2024-02-10 05:51:30,765 Epoch 1733: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 05:51:30,766 EPOCH 1734
2024-02-10 05:51:31,923 [Epoch: 1734 Step: 00015600] Batch Recognition Loss:   0.000464 => Gls Tokens per Sec:     3322 || Batch Translation Loss:   0.017809 => Txt Tokens per Sec:     8366 || Lr: 0.000100
2024-02-10 05:51:46,715 Epoch 1734: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 05:51:46,716 EPOCH 1735
2024-02-10 05:52:02,949 Epoch 1735: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 05:52:02,950 EPOCH 1736
2024-02-10 05:52:19,195 Epoch 1736: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 05:52:19,195 EPOCH 1737
2024-02-10 05:52:35,127 Epoch 1737: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 05:52:35,127 EPOCH 1738
2024-02-10 05:52:51,321 Epoch 1738: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 05:52:51,322 EPOCH 1739
2024-02-10 05:53:08,033 Epoch 1739: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 05:53:08,034 EPOCH 1740
2024-02-10 05:53:23,878 Epoch 1740: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 05:53:23,879 EPOCH 1741
2024-02-10 05:53:39,988 Epoch 1741: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 05:53:39,989 EPOCH 1742
2024-02-10 05:53:56,269 Epoch 1742: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 05:53:56,269 EPOCH 1743
2024-02-10 05:54:12,344 Epoch 1743: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 05:54:12,345 EPOCH 1744
2024-02-10 05:54:28,406 Epoch 1744: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 05:54:28,407 EPOCH 1745
2024-02-10 05:54:36,563 [Epoch: 1745 Step: 00015700] Batch Recognition Loss:   0.000389 => Gls Tokens per Sec:      517 || Batch Translation Loss:   0.016869 => Txt Tokens per Sec:     1352 || Lr: 0.000100
2024-02-10 05:54:44,544 Epoch 1745: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 05:54:44,545 EPOCH 1746
2024-02-10 05:55:00,326 Epoch 1746: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 05:55:00,327 EPOCH 1747
2024-02-10 05:55:16,569 Epoch 1747: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 05:55:16,570 EPOCH 1748
2024-02-10 05:55:32,736 Epoch 1748: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 05:55:32,737 EPOCH 1749
2024-02-10 05:55:48,799 Epoch 1749: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 05:55:48,800 EPOCH 1750
2024-02-10 05:56:05,190 Epoch 1750: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 05:56:05,192 EPOCH 1751
2024-02-10 05:56:21,510 Epoch 1751: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 05:56:21,511 EPOCH 1752
2024-02-10 05:56:37,533 Epoch 1752: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 05:56:37,533 EPOCH 1753
2024-02-10 05:56:53,610 Epoch 1753: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 05:56:53,611 EPOCH 1754
2024-02-10 05:57:09,730 Epoch 1754: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 05:57:09,731 EPOCH 1755
2024-02-10 05:57:26,010 Epoch 1755: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 05:57:26,010 EPOCH 1756
2024-02-10 05:57:33,909 [Epoch: 1756 Step: 00015800] Batch Recognition Loss:   0.000259 => Gls Tokens per Sec:      810 || Batch Translation Loss:   0.017921 => Txt Tokens per Sec:     2178 || Lr: 0.000100
2024-02-10 05:57:42,339 Epoch 1756: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 05:57:42,339 EPOCH 1757
2024-02-10 05:57:58,653 Epoch 1757: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 05:57:58,654 EPOCH 1758
2024-02-10 05:58:14,627 Epoch 1758: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 05:58:14,627 EPOCH 1759
2024-02-10 05:58:30,724 Epoch 1759: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 05:58:30,725 EPOCH 1760
2024-02-10 05:58:46,814 Epoch 1760: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 05:58:46,815 EPOCH 1761
2024-02-10 05:59:02,951 Epoch 1761: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 05:59:02,952 EPOCH 1762
2024-02-10 05:59:19,001 Epoch 1762: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 05:59:19,001 EPOCH 1763
2024-02-10 05:59:35,151 Epoch 1763: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 05:59:35,151 EPOCH 1764
2024-02-10 05:59:50,800 Epoch 1764: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 05:59:50,801 EPOCH 1765
2024-02-10 06:00:07,030 Epoch 1765: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 06:00:07,031 EPOCH 1766
2024-02-10 06:00:22,885 Epoch 1766: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 06:00:22,886 EPOCH 1767
2024-02-10 06:00:37,946 [Epoch: 1767 Step: 00015900] Batch Recognition Loss:   0.000259 => Gls Tokens per Sec:      450 || Batch Translation Loss:   0.015227 => Txt Tokens per Sec:     1347 || Lr: 0.000100
2024-02-10 06:00:39,069 Epoch 1767: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 06:00:39,069 EPOCH 1768
2024-02-10 06:00:55,054 Epoch 1768: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 06:00:55,054 EPOCH 1769
2024-02-10 06:01:11,275 Epoch 1769: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 06:01:11,275 EPOCH 1770
2024-02-10 06:01:27,160 Epoch 1770: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 06:01:27,161 EPOCH 1771
2024-02-10 06:01:43,047 Epoch 1771: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 06:01:43,048 EPOCH 1772
2024-02-10 06:01:59,325 Epoch 1772: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 06:01:59,325 EPOCH 1773
2024-02-10 06:02:15,115 Epoch 1773: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 06:02:15,116 EPOCH 1774
2024-02-10 06:02:31,427 Epoch 1774: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 06:02:31,427 EPOCH 1775
2024-02-10 06:02:47,302 Epoch 1775: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 06:02:47,302 EPOCH 1776
2024-02-10 06:03:03,306 Epoch 1776: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 06:03:03,307 EPOCH 1777
2024-02-10 06:03:19,567 Epoch 1777: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 06:03:19,568 EPOCH 1778
2024-02-10 06:03:35,081 [Epoch: 1778 Step: 00016000] Batch Recognition Loss:   0.001872 => Gls Tokens per Sec:      520 || Batch Translation Loss:   0.008562 => Txt Tokens per Sec:     1456 || Lr: 0.000100
2024-02-10 06:04:47,056 Validation result at epoch 1778, step    16000: duration: 71.9737s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.49468	Translation Loss: 92087.25781	PPL: 9874.27441
	Eval Metric: BLEU
	WER 4.87	(DEL: 0.00,	INS: 0.00,	SUB: 4.87)
	BLEU-4 0.38	(BLEU-1: 11.66,	BLEU-2: 3.58,	BLEU-3: 1.14,	BLEU-4: 0.38)
	CHRF 17.24	ROUGE 9.59
2024-02-10 06:04:47,058 Logging Recognition and Translation Outputs
2024-02-10 06:04:47,059 ========================================================================================================================
2024-02-10 06:04:47,059 Logging Sequence: 163_116.00
2024-02-10 06:04:47,059 	Gloss Reference :	A B+C+D+E
2024-02-10 06:04:47,059 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 06:04:47,059 	Gloss Alignment :	         
2024-02-10 06:04:47,060 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 06:04:47,061 	Text Reference  :	******** **** ***** ***** ******** ***** ** people said   that she   looked similar to       virat
2024-02-10 06:04:47,061 	Text Hypothesis :	whenever they would share pictures taken at his    family and  there was    very    vamika's head 
2024-02-10 06:04:47,061 	Text Alignment  :	I        I    I     I     I        I     I  S      S      S    S     S      S       S        S    
2024-02-10 06:04:47,061 ========================================================================================================================
2024-02-10 06:04:47,061 Logging Sequence: 53_161.00
2024-02-10 06:04:47,061 	Gloss Reference :	A B+C+D+E
2024-02-10 06:04:47,062 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 06:04:47,062 	Gloss Alignment :	         
2024-02-10 06:04:47,062 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 06:04:47,063 	Text Reference  :	rashid has also been urging people      to donate to  his rashid khan foundation and afghanistan cricket association
2024-02-10 06:04:47,064 	Text Hypothesis :	he     has now  been ****** afghanistan to ****** win but will   be   available  for the         trent   rockets    
2024-02-10 06:04:47,064 	Text Alignment  :	S          S         D      S              D      S   S   S      S    S          S   S           S       S          
2024-02-10 06:04:47,064 ========================================================================================================================
2024-02-10 06:04:47,064 Logging Sequence: 67_73.00
2024-02-10 06:04:47,064 	Gloss Reference :	A B+C+D+E
2024-02-10 06:04:47,064 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 06:04:47,064 	Gloss Alignment :	         
2024-02-10 06:04:47,065 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 06:04:47,065 	Text Reference  :	*** ****** in   his        tweet   he  also said
2024-02-10 06:04:47,065 	Text Hypothesis :	the indian team absolutely admires the all  out 
2024-02-10 06:04:47,065 	Text Alignment  :	I   I      S    S          S       S   S    S   
2024-02-10 06:04:47,066 ========================================================================================================================
2024-02-10 06:04:47,066 Logging Sequence: 137_44.00
2024-02-10 06:04:47,066 	Gloss Reference :	A B+C+D+E
2024-02-10 06:04:47,066 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 06:04:47,066 	Gloss Alignment :	         
2024-02-10 06:04:47,067 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 06:04:47,068 	Text Reference  :	let me tell you the rules that qatar has announced for the      fans travelling  for       the world  cup       
2024-02-10 06:04:47,068 	Text Hypothesis :	*** ** **** *** *** ***** **** they  all posed     for pictures amid celebratory fireworks the indian supporters
2024-02-10 06:04:47,068 	Text Alignment  :	D   D  D    D   D   D     D    S     S   S             S        S    S           S             S      S         
2024-02-10 06:04:47,068 ========================================================================================================================
2024-02-10 06:04:47,068 Logging Sequence: 99_158.00
2024-02-10 06:04:47,069 	Gloss Reference :	A B+C+D+E
2024-02-10 06:04:47,069 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 06:04:47,069 	Gloss Alignment :	         
2024-02-10 06:04:47,069 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 06:04:47,070 	Text Reference  :	the incident occured in  dubai and     it  was  extremely shameful
2024-02-10 06:04:47,070 	Text Hypothesis :	*** shocking he      has now   retired but will be        true    
2024-02-10 06:04:47,070 	Text Alignment  :	D   S        S       S   S     S       S   S    S         S       
2024-02-10 06:04:47,070 ========================================================================================================================
2024-02-10 06:04:48,244 Epoch 1778: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 06:04:48,244 EPOCH 1779
2024-02-10 06:05:04,532 Epoch 1779: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 06:05:04,533 EPOCH 1780
2024-02-10 06:05:20,914 Epoch 1780: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 06:05:20,914 EPOCH 1781
2024-02-10 06:05:37,381 Epoch 1781: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 06:05:37,381 EPOCH 1782
2024-02-10 06:05:53,387 Epoch 1782: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 06:05:53,388 EPOCH 1783
2024-02-10 06:06:09,375 Epoch 1783: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 06:06:09,376 EPOCH 1784
2024-02-10 06:06:25,809 Epoch 1784: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 06:06:25,810 EPOCH 1785
2024-02-10 06:06:42,065 Epoch 1785: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 06:06:42,065 EPOCH 1786
2024-02-10 06:06:58,579 Epoch 1786: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 06:06:58,579 EPOCH 1787
2024-02-10 06:07:14,493 Epoch 1787: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 06:07:14,494 EPOCH 1788
2024-02-10 06:07:30,876 Epoch 1788: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 06:07:30,876 EPOCH 1789
2024-02-10 06:07:46,605 [Epoch: 1789 Step: 00016100] Batch Recognition Loss:   0.000421 => Gls Tokens per Sec:      594 || Batch Translation Loss:   0.021153 => Txt Tokens per Sec:     1721 || Lr: 0.000100
2024-02-10 06:07:46,997 Epoch 1789: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 06:07:46,997 EPOCH 1790
2024-02-10 06:08:03,416 Epoch 1790: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 06:08:03,416 EPOCH 1791
2024-02-10 06:08:19,579 Epoch 1791: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 06:08:19,579 EPOCH 1792
2024-02-10 06:08:35,840 Epoch 1792: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 06:08:35,841 EPOCH 1793
2024-02-10 06:08:52,121 Epoch 1793: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 06:08:52,122 EPOCH 1794
2024-02-10 06:09:08,334 Epoch 1794: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 06:09:08,335 EPOCH 1795
2024-02-10 06:09:24,357 Epoch 1795: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 06:09:24,357 EPOCH 1796
2024-02-10 06:09:40,433 Epoch 1796: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 06:09:40,434 EPOCH 1797
2024-02-10 06:09:56,479 Epoch 1797: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 06:09:56,480 EPOCH 1798
2024-02-10 06:10:12,373 Epoch 1798: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 06:10:12,373 EPOCH 1799
2024-02-10 06:10:28,463 Epoch 1799: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 06:10:28,464 EPOCH 1800
2024-02-10 06:10:45,188 [Epoch: 1800 Step: 00016200] Batch Recognition Loss:   0.000256 => Gls Tokens per Sec:      635 || Batch Translation Loss:   0.016595 => Txt Tokens per Sec:     1757 || Lr: 0.000100
2024-02-10 06:10:45,189 Epoch 1800: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 06:10:45,189 EPOCH 1801
2024-02-10 06:11:01,136 Epoch 1801: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 06:11:01,136 EPOCH 1802
2024-02-10 06:11:17,411 Epoch 1802: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 06:11:17,412 EPOCH 1803
2024-02-10 06:11:33,188 Epoch 1803: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 06:11:33,188 EPOCH 1804
2024-02-10 06:11:49,150 Epoch 1804: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 06:11:49,150 EPOCH 1805
2024-02-10 06:12:05,068 Epoch 1805: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 06:12:05,068 EPOCH 1806
2024-02-10 06:12:21,147 Epoch 1806: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 06:12:21,148 EPOCH 1807
2024-02-10 06:12:37,256 Epoch 1807: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 06:12:37,257 EPOCH 1808
2024-02-10 06:12:53,153 Epoch 1808: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 06:12:53,154 EPOCH 1809
2024-02-10 06:13:09,438 Epoch 1809: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 06:13:09,438 EPOCH 1810
2024-02-10 06:13:25,307 Epoch 1810: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 06:13:25,308 EPOCH 1811
2024-02-10 06:13:41,390 Epoch 1811: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 06:13:41,391 EPOCH 1812
2024-02-10 06:13:42,016 [Epoch: 1812 Step: 00016300] Batch Recognition Loss:   0.000604 => Gls Tokens per Sec:     2055 || Batch Translation Loss:   0.019485 => Txt Tokens per Sec:     6202 || Lr: 0.000100
2024-02-10 06:13:57,268 Epoch 1812: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 06:13:57,268 EPOCH 1813
2024-02-10 06:14:13,395 Epoch 1813: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 06:14:13,395 EPOCH 1814
2024-02-10 06:14:29,567 Epoch 1814: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 06:14:29,568 EPOCH 1815
2024-02-10 06:14:45,581 Epoch 1815: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 06:14:45,582 EPOCH 1816
2024-02-10 06:15:01,381 Epoch 1816: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 06:15:01,382 EPOCH 1817
2024-02-10 06:15:17,486 Epoch 1817: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 06:15:17,487 EPOCH 1818
2024-02-10 06:15:33,345 Epoch 1818: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 06:15:33,345 EPOCH 1819
2024-02-10 06:15:49,321 Epoch 1819: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 06:15:49,321 EPOCH 1820
2024-02-10 06:16:05,629 Epoch 1820: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 06:16:05,630 EPOCH 1821
2024-02-10 06:16:21,607 Epoch 1821: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 06:16:21,608 EPOCH 1822
2024-02-10 06:16:37,882 Epoch 1822: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 06:16:37,883 EPOCH 1823
2024-02-10 06:16:47,074 [Epoch: 1823 Step: 00016400] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:      279 || Batch Translation Loss:   0.027965 => Txt Tokens per Sec:      925 || Lr: 0.000100
2024-02-10 06:16:54,176 Epoch 1823: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-10 06:16:54,177 EPOCH 1824
2024-02-10 06:17:10,451 Epoch 1824: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-10 06:17:10,451 EPOCH 1825
2024-02-10 06:17:26,568 Epoch 1825: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-10 06:17:26,568 EPOCH 1826
2024-02-10 06:17:42,723 Epoch 1826: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-10 06:17:42,723 EPOCH 1827
2024-02-10 06:17:58,872 Epoch 1827: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-10 06:17:58,873 EPOCH 1828
2024-02-10 06:18:14,629 Epoch 1828: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-10 06:18:14,630 EPOCH 1829
2024-02-10 06:18:30,665 Epoch 1829: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.27 
2024-02-10 06:18:30,666 EPOCH 1830
2024-02-10 06:18:46,633 Epoch 1830: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-10 06:18:46,634 EPOCH 1831
2024-02-10 06:19:03,437 Epoch 1831: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-10 06:19:03,437 EPOCH 1832
2024-02-10 06:19:19,689 Epoch 1832: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-10 06:19:19,689 EPOCH 1833
2024-02-10 06:19:35,688 Epoch 1833: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-10 06:19:35,688 EPOCH 1834
2024-02-10 06:19:39,562 [Epoch: 1834 Step: 00016500] Batch Recognition Loss:   0.000347 => Gls Tokens per Sec:      991 || Batch Translation Loss:   0.028460 => Txt Tokens per Sec:     2392 || Lr: 0.000100
2024-02-10 06:19:51,855 Epoch 1834: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-10 06:19:51,855 EPOCH 1835
2024-02-10 06:20:07,766 Epoch 1835: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-10 06:20:07,767 EPOCH 1836
2024-02-10 06:20:24,519 Epoch 1836: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-10 06:20:24,519 EPOCH 1837
2024-02-10 06:20:40,580 Epoch 1837: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 06:20:40,581 EPOCH 1838
2024-02-10 06:20:56,721 Epoch 1838: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-10 06:20:56,721 EPOCH 1839
2024-02-10 06:21:13,143 Epoch 1839: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 06:21:13,144 EPOCH 1840
2024-02-10 06:21:29,052 Epoch 1840: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-10 06:21:29,053 EPOCH 1841
2024-02-10 06:21:44,972 Epoch 1841: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 06:21:44,972 EPOCH 1842
2024-02-10 06:22:01,481 Epoch 1842: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 06:22:01,481 EPOCH 1843
2024-02-10 06:22:17,686 Epoch 1843: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 06:22:17,687 EPOCH 1844
2024-02-10 06:22:33,741 Epoch 1844: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 06:22:33,742 EPOCH 1845
2024-02-10 06:22:42,475 [Epoch: 1845 Step: 00016600] Batch Recognition Loss:   0.000325 => Gls Tokens per Sec:      483 || Batch Translation Loss:   0.010173 => Txt Tokens per Sec:     1403 || Lr: 0.000100
2024-02-10 06:22:50,109 Epoch 1845: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 06:22:50,110 EPOCH 1846
2024-02-10 06:23:06,568 Epoch 1846: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-10 06:23:06,569 EPOCH 1847
2024-02-10 06:23:22,750 Epoch 1847: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-10 06:23:22,750 EPOCH 1848
2024-02-10 06:23:38,723 Epoch 1848: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 06:23:38,724 EPOCH 1849
2024-02-10 06:23:55,104 Epoch 1849: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 06:23:55,105 EPOCH 1850
2024-02-10 06:24:11,258 Epoch 1850: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 06:24:11,259 EPOCH 1851
2024-02-10 06:24:27,211 Epoch 1851: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 06:24:27,212 EPOCH 1852
2024-02-10 06:24:43,557 Epoch 1852: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 06:24:43,558 EPOCH 1853
2024-02-10 06:24:59,593 Epoch 1853: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 06:24:59,593 EPOCH 1854
2024-02-10 06:25:15,658 Epoch 1854: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 06:25:15,659 EPOCH 1855
2024-02-10 06:25:31,824 Epoch 1855: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 06:25:31,824 EPOCH 1856
2024-02-10 06:25:41,943 [Epoch: 1856 Step: 00016700] Batch Recognition Loss:   0.000515 => Gls Tokens per Sec:      633 || Batch Translation Loss:   0.022353 => Txt Tokens per Sec:     1771 || Lr: 0.000100
2024-02-10 06:25:47,695 Epoch 1856: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 06:25:47,695 EPOCH 1857
2024-02-10 06:26:03,559 Epoch 1857: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 06:26:03,560 EPOCH 1858
2024-02-10 06:26:19,812 Epoch 1858: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 06:26:19,813 EPOCH 1859
2024-02-10 06:26:35,806 Epoch 1859: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 06:26:35,807 EPOCH 1860
2024-02-10 06:26:51,751 Epoch 1860: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 06:26:51,752 EPOCH 1861
2024-02-10 06:27:07,633 Epoch 1861: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 06:27:07,634 EPOCH 1862
2024-02-10 06:27:23,954 Epoch 1862: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 06:27:23,954 EPOCH 1863
2024-02-10 06:27:39,910 Epoch 1863: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 06:27:39,911 EPOCH 1864
2024-02-10 06:27:56,140 Epoch 1864: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 06:27:56,140 EPOCH 1865
2024-02-10 06:28:12,239 Epoch 1865: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 06:28:12,240 EPOCH 1866
2024-02-10 06:28:28,493 Epoch 1866: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 06:28:28,493 EPOCH 1867
2024-02-10 06:28:43,611 [Epoch: 1867 Step: 00016800] Batch Recognition Loss:   0.000272 => Gls Tokens per Sec:      449 || Batch Translation Loss:   0.019766 => Txt Tokens per Sec:     1386 || Lr: 0.000100
2024-02-10 06:28:44,566 Epoch 1867: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 06:28:44,566 EPOCH 1868
2024-02-10 06:29:00,554 Epoch 1868: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 06:29:00,555 EPOCH 1869
2024-02-10 06:29:16,587 Epoch 1869: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 06:29:16,588 EPOCH 1870
2024-02-10 06:29:32,704 Epoch 1870: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.14 
2024-02-10 06:29:32,704 EPOCH 1871
2024-02-10 06:29:48,492 Epoch 1871: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.16 
2024-02-10 06:29:48,493 EPOCH 1872
2024-02-10 06:30:04,831 Epoch 1872: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 06:30:04,832 EPOCH 1873
2024-02-10 06:30:20,705 Epoch 1873: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 06:30:20,706 EPOCH 1874
2024-02-10 06:30:36,832 Epoch 1874: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 06:30:36,833 EPOCH 1875
2024-02-10 06:30:54,017 Epoch 1875: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 06:30:54,018 EPOCH 1876
2024-02-10 06:31:10,034 Epoch 1876: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 06:31:10,034 EPOCH 1877
2024-02-10 06:31:26,056 Epoch 1877: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 06:31:26,057 EPOCH 1878
2024-02-10 06:31:38,616 [Epoch: 1878 Step: 00016900] Batch Recognition Loss:   0.000364 => Gls Tokens per Sec:      642 || Batch Translation Loss:   0.024948 => Txt Tokens per Sec:     1755 || Lr: 0.000100
2024-02-10 06:31:42,227 Epoch 1878: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 06:31:42,228 EPOCH 1879
2024-02-10 06:31:58,228 Epoch 1879: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 06:31:58,228 EPOCH 1880
2024-02-10 06:32:14,366 Epoch 1880: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 06:32:14,367 EPOCH 1881
2024-02-10 06:32:30,680 Epoch 1881: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 06:32:30,681 EPOCH 1882
2024-02-10 06:32:46,858 Epoch 1882: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 06:32:46,858 EPOCH 1883
2024-02-10 06:33:02,969 Epoch 1883: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 06:33:02,969 EPOCH 1884
2024-02-10 06:33:19,230 Epoch 1884: Total Training Recognition Loss 0.00  Total Training Translation Loss 2.36 
2024-02-10 06:33:19,231 EPOCH 1885
2024-02-10 06:33:35,473 Epoch 1885: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.29 
2024-02-10 06:33:35,474 EPOCH 1886
2024-02-10 06:33:51,383 Epoch 1886: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.77 
2024-02-10 06:33:51,384 EPOCH 1887
2024-02-10 06:34:07,595 Epoch 1887: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.22 
2024-02-10 06:34:07,595 EPOCH 1888
2024-02-10 06:34:23,881 Epoch 1888: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.51 
2024-02-10 06:34:23,882 EPOCH 1889
2024-02-10 06:34:36,785 [Epoch: 1889 Step: 00017000] Batch Recognition Loss:   0.001277 => Gls Tokens per Sec:      724 || Batch Translation Loss:   0.090328 => Txt Tokens per Sec:     1959 || Lr: 0.000100
2024-02-10 06:34:39,898 Epoch 1889: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.20 
2024-02-10 06:34:39,898 EPOCH 1890
2024-02-10 06:34:55,996 Epoch 1890: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.00 
2024-02-10 06:34:55,997 EPOCH 1891
2024-02-10 06:35:12,253 Epoch 1891: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-10 06:35:12,253 EPOCH 1892
2024-02-10 06:35:28,421 Epoch 1892: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-10 06:35:28,422 EPOCH 1893
2024-02-10 06:35:44,351 Epoch 1893: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-10 06:35:44,352 EPOCH 1894
2024-02-10 06:36:00,175 Epoch 1894: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-10 06:36:00,176 EPOCH 1895
2024-02-10 06:36:15,901 Epoch 1895: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 06:36:15,902 EPOCH 1896
2024-02-10 06:36:31,889 Epoch 1896: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 06:36:31,890 EPOCH 1897
2024-02-10 06:36:48,150 Epoch 1897: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 06:36:48,151 EPOCH 1898
2024-02-10 06:37:04,230 Epoch 1898: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 06:37:04,231 EPOCH 1899
2024-02-10 06:37:20,407 Epoch 1899: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 06:37:20,407 EPOCH 1900
2024-02-10 06:37:36,548 [Epoch: 1900 Step: 00017100] Batch Recognition Loss:   0.000733 => Gls Tokens per Sec:      658 || Batch Translation Loss:   0.013629 => Txt Tokens per Sec:     1820 || Lr: 0.000100
2024-02-10 06:37:36,549 Epoch 1900: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 06:37:36,549 EPOCH 1901
2024-02-10 06:37:52,589 Epoch 1901: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 06:37:52,590 EPOCH 1902
2024-02-10 06:38:08,812 Epoch 1902: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 06:38:08,813 EPOCH 1903
2024-02-10 06:38:24,778 Epoch 1903: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 06:38:24,778 EPOCH 1904
2024-02-10 06:38:40,763 Epoch 1904: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 06:38:40,764 EPOCH 1905
2024-02-10 06:38:57,150 Epoch 1905: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 06:38:57,151 EPOCH 1906
2024-02-10 06:39:13,011 Epoch 1906: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-10 06:39:13,011 EPOCH 1907
2024-02-10 06:39:28,917 Epoch 1907: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-10 06:39:28,917 EPOCH 1908
2024-02-10 06:39:45,151 Epoch 1908: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-10 06:39:45,152 EPOCH 1909
2024-02-10 06:40:01,333 Epoch 1909: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-10 06:40:01,333 EPOCH 1910
2024-02-10 06:40:17,258 Epoch 1910: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-10 06:40:17,259 EPOCH 1911
2024-02-10 06:40:33,272 Epoch 1911: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-10 06:40:33,273 EPOCH 1912
2024-02-10 06:40:33,933 [Epoch: 1912 Step: 00017200] Batch Recognition Loss:   0.000495 => Gls Tokens per Sec:     1942 || Batch Translation Loss:   0.052756 => Txt Tokens per Sec:     5862 || Lr: 0.000100
2024-02-10 06:40:49,616 Epoch 1912: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-10 06:40:49,616 EPOCH 1913
2024-02-10 06:41:05,776 Epoch 1913: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 06:41:05,777 EPOCH 1914
2024-02-10 06:41:22,035 Epoch 1914: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 06:41:22,036 EPOCH 1915
2024-02-10 06:41:38,096 Epoch 1915: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 06:41:38,096 EPOCH 1916
2024-02-10 06:41:54,379 Epoch 1916: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 06:41:54,380 EPOCH 1917
2024-02-10 06:42:10,513 Epoch 1917: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 06:42:10,514 EPOCH 1918
2024-02-10 06:42:26,852 Epoch 1918: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.20 
2024-02-10 06:42:26,852 EPOCH 1919
2024-02-10 06:42:42,821 Epoch 1919: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 06:42:42,822 EPOCH 1920
2024-02-10 06:42:58,559 Epoch 1920: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.20 
2024-02-10 06:42:58,560 EPOCH 1921
2024-02-10 06:43:15,080 Epoch 1921: Total Training Recognition Loss 0.38  Total Training Translation Loss 0.22 
2024-02-10 06:43:15,081 EPOCH 1922
2024-02-10 06:43:31,318 Epoch 1922: Total Training Recognition Loss 0.31  Total Training Translation Loss 0.26 
2024-02-10 06:43:31,318 EPOCH 1923
2024-02-10 06:43:36,185 [Epoch: 1923 Step: 00017300] Batch Recognition Loss:   0.001685 => Gls Tokens per Sec:      341 || Batch Translation Loss:   0.030130 => Txt Tokens per Sec:      996 || Lr: 0.000100
2024-02-10 06:43:47,455 Epoch 1923: Total Training Recognition Loss 0.81  Total Training Translation Loss 0.30 
2024-02-10 06:43:47,455 EPOCH 1924
2024-02-10 06:44:03,671 Epoch 1924: Total Training Recognition Loss 1.07  Total Training Translation Loss 0.32 
2024-02-10 06:44:03,672 EPOCH 1925
2024-02-10 06:44:19,740 Epoch 1925: Total Training Recognition Loss 0.95  Total Training Translation Loss 0.33 
2024-02-10 06:44:19,741 EPOCH 1926
2024-02-10 06:44:35,925 Epoch 1926: Total Training Recognition Loss 3.01  Total Training Translation Loss 0.56 
2024-02-10 06:44:35,926 EPOCH 1927
2024-02-10 06:44:51,965 Epoch 1927: Total Training Recognition Loss 1.41  Total Training Translation Loss 0.78 
2024-02-10 06:44:51,966 EPOCH 1928
2024-02-10 06:45:07,919 Epoch 1928: Total Training Recognition Loss 0.42  Total Training Translation Loss 0.72 
2024-02-10 06:45:07,919 EPOCH 1929
2024-02-10 06:45:24,011 Epoch 1929: Total Training Recognition Loss 0.11  Total Training Translation Loss 0.60 
2024-02-10 06:45:24,011 EPOCH 1930
2024-02-10 06:45:40,055 Epoch 1930: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.65 
2024-02-10 06:45:40,055 EPOCH 1931
2024-02-10 06:45:56,337 Epoch 1931: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.43 
2024-02-10 06:45:56,338 EPOCH 1932
2024-02-10 06:46:12,547 Epoch 1932: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.38 
2024-02-10 06:46:12,548 EPOCH 1933
2024-02-10 06:46:28,451 Epoch 1933: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.33 
2024-02-10 06:46:28,452 EPOCH 1934
2024-02-10 06:46:36,603 [Epoch: 1934 Step: 00017400] Batch Recognition Loss:   0.000773 => Gls Tokens per Sec:      361 || Batch Translation Loss:   0.036475 => Txt Tokens per Sec:     1142 || Lr: 0.000100
2024-02-10 06:46:44,578 Epoch 1934: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 06:46:44,579 EPOCH 1935
2024-02-10 06:47:00,786 Epoch 1935: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 06:47:00,786 EPOCH 1936
2024-02-10 06:47:16,786 Epoch 1936: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.23 
2024-02-10 06:47:16,787 EPOCH 1937
2024-02-10 06:47:33,118 Epoch 1937: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.23 
2024-02-10 06:47:33,118 EPOCH 1938
2024-02-10 06:47:49,056 Epoch 1938: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 06:47:49,057 EPOCH 1939
2024-02-10 06:48:05,068 Epoch 1939: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 06:48:05,068 EPOCH 1940
2024-02-10 06:48:20,962 Epoch 1940: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 06:48:20,962 EPOCH 1941
2024-02-10 06:48:36,771 Epoch 1941: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 06:48:36,772 EPOCH 1942
2024-02-10 06:48:52,896 Epoch 1942: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 06:48:52,897 EPOCH 1943
2024-02-10 06:49:08,803 Epoch 1943: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 06:49:08,804 EPOCH 1944
2024-02-10 06:49:25,294 Epoch 1944: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 06:49:25,294 EPOCH 1945
2024-02-10 06:49:26,734 [Epoch: 1945 Step: 00017500] Batch Recognition Loss:   0.000240 => Gls Tokens per Sec:     3558 || Batch Translation Loss:   0.022375 => Txt Tokens per Sec:     8404 || Lr: 0.000100
2024-02-10 06:49:41,267 Epoch 1945: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 06:49:41,268 EPOCH 1946
2024-02-10 06:49:57,318 Epoch 1946: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 06:49:57,318 EPOCH 1947
2024-02-10 06:50:13,700 Epoch 1947: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 06:50:13,701 EPOCH 1948
2024-02-10 06:50:29,731 Epoch 1948: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 06:50:29,732 EPOCH 1949
2024-02-10 06:50:45,732 Epoch 1949: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 06:50:45,733 EPOCH 1950
2024-02-10 06:51:01,744 Epoch 1950: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 06:51:01,745 EPOCH 1951
2024-02-10 06:51:17,646 Epoch 1951: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 06:51:17,646 EPOCH 1952
2024-02-10 06:51:33,587 Epoch 1952: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 06:51:33,588 EPOCH 1953
2024-02-10 06:51:49,767 Epoch 1953: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 06:51:49,768 EPOCH 1954
2024-02-10 06:52:06,059 Epoch 1954: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 06:52:06,060 EPOCH 1955
2024-02-10 06:52:22,137 Epoch 1955: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 06:52:22,138 EPOCH 1956
2024-02-10 06:52:27,551 [Epoch: 1956 Step: 00017600] Batch Recognition Loss:   0.000381 => Gls Tokens per Sec:     1183 || Batch Translation Loss:   0.013910 => Txt Tokens per Sec:     3411 || Lr: 0.000100
2024-02-10 06:52:38,469 Epoch 1956: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 06:52:38,470 EPOCH 1957
2024-02-10 06:52:54,454 Epoch 1957: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 06:52:54,454 EPOCH 1958
2024-02-10 06:53:10,392 Epoch 1958: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 06:53:10,393 EPOCH 1959
2024-02-10 06:53:26,393 Epoch 1959: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 06:53:26,394 EPOCH 1960
2024-02-10 06:53:42,182 Epoch 1960: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 06:53:42,182 EPOCH 1961
2024-02-10 06:53:58,112 Epoch 1961: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 06:53:58,113 EPOCH 1962
2024-02-10 06:54:14,286 Epoch 1962: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 06:54:14,286 EPOCH 1963
2024-02-10 06:54:30,426 Epoch 1963: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 06:54:30,427 EPOCH 1964
2024-02-10 06:54:46,225 Epoch 1964: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 06:54:46,226 EPOCH 1965
2024-02-10 06:55:02,107 Epoch 1965: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 06:55:02,108 EPOCH 1966
2024-02-10 06:55:18,114 Epoch 1966: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 06:55:18,115 EPOCH 1967
2024-02-10 06:55:24,754 [Epoch: 1967 Step: 00017700] Batch Recognition Loss:   0.000700 => Gls Tokens per Sec:     1022 || Batch Translation Loss:   0.010211 => Txt Tokens per Sec:     2658 || Lr: 0.000100
2024-02-10 06:55:34,004 Epoch 1967: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 06:55:34,005 EPOCH 1968
2024-02-10 06:55:50,391 Epoch 1968: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 06:55:50,392 EPOCH 1969
2024-02-10 06:56:06,395 Epoch 1969: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 06:56:06,396 EPOCH 1970
2024-02-10 06:56:22,543 Epoch 1970: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-10 06:56:22,544 EPOCH 1971
2024-02-10 06:56:38,678 Epoch 1971: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.33 
2024-02-10 06:56:38,679 EPOCH 1972
2024-02-10 06:56:54,919 Epoch 1972: Total Training Recognition Loss 0.01  Total Training Translation Loss 8.95 
2024-02-10 06:56:54,919 EPOCH 1973
2024-02-10 06:57:10,984 Epoch 1973: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.23 
2024-02-10 06:57:10,984 EPOCH 1974
2024-02-10 06:57:27,128 Epoch 1974: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.93 
2024-02-10 06:57:27,129 EPOCH 1975
2024-02-10 06:57:43,246 Epoch 1975: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.17 
2024-02-10 06:57:43,246 EPOCH 1976
2024-02-10 06:57:59,474 Epoch 1976: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-10 06:57:59,475 EPOCH 1977
2024-02-10 06:58:15,814 Epoch 1977: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.02 
2024-02-10 06:58:15,815 EPOCH 1978
2024-02-10 06:58:28,125 [Epoch: 1978 Step: 00017800] Batch Recognition Loss:   0.001559 => Gls Tokens per Sec:      655 || Batch Translation Loss:   0.097689 => Txt Tokens per Sec:     1754 || Lr: 0.000100
2024-02-10 06:58:31,856 Epoch 1978: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-10 06:58:31,856 EPOCH 1979
2024-02-10 06:58:47,879 Epoch 1979: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-10 06:58:47,880 EPOCH 1980
2024-02-10 06:59:03,958 Epoch 1980: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-10 06:59:03,959 EPOCH 1981
2024-02-10 06:59:20,342 Epoch 1981: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-10 06:59:20,343 EPOCH 1982
2024-02-10 06:59:36,241 Epoch 1982: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-10 06:59:36,241 EPOCH 1983
2024-02-10 06:59:52,098 Epoch 1983: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 06:59:52,098 EPOCH 1984
2024-02-10 07:00:08,522 Epoch 1984: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 07:00:08,522 EPOCH 1985
2024-02-10 07:00:24,356 Epoch 1985: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 07:00:24,356 EPOCH 1986
2024-02-10 07:00:40,558 Epoch 1986: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 07:00:40,559 EPOCH 1987
2024-02-10 07:00:56,663 Epoch 1987: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 07:00:56,664 EPOCH 1988
2024-02-10 07:01:12,771 Epoch 1988: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 07:01:12,772 EPOCH 1989
2024-02-10 07:01:28,330 [Epoch: 1989 Step: 00017900] Batch Recognition Loss:   0.001115 => Gls Tokens per Sec:      600 || Batch Translation Loss:   0.022242 => Txt Tokens per Sec:     1677 || Lr: 0.000100
2024-02-10 07:01:28,699 Epoch 1989: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 07:01:28,699 EPOCH 1990
2024-02-10 07:01:44,766 Epoch 1990: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 07:01:44,767 EPOCH 1991
2024-02-10 07:02:00,902 Epoch 1991: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 07:02:00,903 EPOCH 1992
2024-02-10 07:02:17,085 Epoch 1992: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 07:02:17,086 EPOCH 1993
2024-02-10 07:02:33,149 Epoch 1993: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 07:02:33,150 EPOCH 1994
2024-02-10 07:02:49,127 Epoch 1994: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 07:02:49,127 EPOCH 1995
2024-02-10 07:03:05,255 Epoch 1995: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 07:03:05,256 EPOCH 1996
2024-02-10 07:03:21,311 Epoch 1996: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 07:03:21,312 EPOCH 1997
2024-02-10 07:03:37,219 Epoch 1997: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 07:03:37,219 EPOCH 1998
2024-02-10 07:03:53,248 Epoch 1998: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 07:03:53,249 EPOCH 1999
2024-02-10 07:04:09,526 Epoch 1999: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 07:04:09,527 EPOCH 2000
2024-02-10 07:04:25,276 [Epoch: 2000 Step: 00018000] Batch Recognition Loss:   0.000781 => Gls Tokens per Sec:      674 || Batch Translation Loss:   0.026637 => Txt Tokens per Sec:     1866 || Lr: 0.000100
2024-02-10 07:05:37,209 Validation result at epoch 2000, step    18000: duration: 71.9310s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.40410	Translation Loss: 93589.39062	PPL: 11472.64844
	Eval Metric: BLEU
	WER 4.38	(DEL: 0.00,	INS: 0.00,	SUB: 4.38)
	BLEU-4 0.67	(BLEU-1: 11.47,	BLEU-2: 3.91,	BLEU-3: 1.43,	BLEU-4: 0.67)
	CHRF 17.48	ROUGE 9.65
2024-02-10 07:05:37,211 Logging Recognition and Translation Outputs
2024-02-10 07:05:37,211 ========================================================================================================================
2024-02-10 07:05:37,211 Logging Sequence: 179_309.00
2024-02-10 07:05:37,212 	Gloss Reference :	A B+C+D+E
2024-02-10 07:05:37,212 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 07:05:37,212 	Gloss Alignment :	         
2024-02-10 07:05:37,212 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 07:05:37,213 	Text Reference  :	** before the ioa could    send the  notice wfi has asked    phogat to   explain her indiscipline
2024-02-10 07:05:37,214 	Text Hypothesis :	we hope   the *** response to   help she    is  so  athletes be     held in      her departure   
2024-02-10 07:05:37,214 	Text Alignment  :	I  S          D   S        S    S    S      S   S   S        S      S    S           S           
2024-02-10 07:05:37,214 ========================================================================================================================
2024-02-10 07:05:37,214 Logging Sequence: 156_35.00
2024-02-10 07:05:37,214 	Gloss Reference :	A B+C+D+E
2024-02-10 07:05:37,214 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 07:05:37,215 	Gloss Alignment :	         
2024-02-10 07:05:37,215 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 07:05:37,217 	Text Reference  :	the first season   of      mlc began  on      13th     july   2023 and  ended on    30th july  2023  with     six  teams
2024-02-10 07:05:37,217 	Text Hypothesis :	*** miny' original captain was kieron pollard nicholas pooran was  held in    place in   place which pakistan were 126  
2024-02-10 07:05:37,217 	Text Alignment  :	D   S     S        S       S   S      S       S        S      S    S    S     S     S    S     S     S        S    S    
2024-02-10 07:05:37,217 ========================================================================================================================
2024-02-10 07:05:37,217 Logging Sequence: 129_45.00
2024-02-10 07:05:37,217 	Gloss Reference :	A B+C+D+E
2024-02-10 07:05:37,218 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 07:05:37,218 	Gloss Alignment :	         
2024-02-10 07:05:37,218 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 07:05:37,220 	Text Reference  :	suga then         announced that from 5        july onwards japan  will be      in  a      state of *** emergency
2024-02-10 07:05:37,220 	Text Hypothesis :	the  postponement of        the  2020 olympics by   15      months has  stalled the income flow  of the ioc      
2024-02-10 07:05:37,220 	Text Alignment  :	S    S            S         S    S    S        S    S       S      S    S       S   S      S        I   S        
2024-02-10 07:05:37,220 ========================================================================================================================
2024-02-10 07:05:37,220 Logging Sequence: 56_17.00
2024-02-10 07:05:37,220 	Gloss Reference :	A B+C+D+E  
2024-02-10 07:05:37,221 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-10 07:05:37,221 	Gloss Alignment :	  S        
2024-02-10 07:05:37,221 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 07:05:37,221 	Text Reference  :	*** it     was *** held at  mumbai's wankhede stadium
2024-02-10 07:05:37,221 	Text Hypothesis :	the reason was not the  ipl despite  their    screens
2024-02-10 07:05:37,222 	Text Alignment  :	I   S          I   S    S   S        S        S      
2024-02-10 07:05:37,222 ========================================================================================================================
2024-02-10 07:05:37,222 Logging Sequence: 152_73.00
2024-02-10 07:05:37,222 	Gloss Reference :	A B+C+D+E
2024-02-10 07:05:37,222 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 07:05:37,222 	Gloss Alignment :	         
2024-02-10 07:05:37,223 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 07:05:37,223 	Text Reference  :	**** ****** *** ********* **** ** eventually he      too      got out by    shaheen afridi  
2024-02-10 07:05:37,224 	Text Hypothesis :	when sharma was pakistan' turn to bat        against pakistan in  any place with    pakistan
2024-02-10 07:05:37,224 	Text Alignment  :	I    I      I   I         I    I  S          S       S        S   S   S     S       S       
2024-02-10 07:05:37,224 ========================================================================================================================
2024-02-10 07:05:37,228 Epoch 2000: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 07:05:37,228 EPOCH 2001
2024-02-10 07:05:53,636 Epoch 2001: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 07:05:53,636 EPOCH 2002
2024-02-10 07:06:09,706 Epoch 2002: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 07:06:09,706 EPOCH 2003
2024-02-10 07:06:26,539 Epoch 2003: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 07:06:26,539 EPOCH 2004
2024-02-10 07:06:42,526 Epoch 2004: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 07:06:42,527 EPOCH 2005
2024-02-10 07:06:58,647 Epoch 2005: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 07:06:58,647 EPOCH 2006
2024-02-10 07:07:14,940 Epoch 2006: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 07:07:14,941 EPOCH 2007
2024-02-10 07:07:31,118 Epoch 2007: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 07:07:31,119 EPOCH 2008
2024-02-10 07:07:47,330 Epoch 2008: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 07:07:47,331 EPOCH 2009
2024-02-10 07:08:03,523 Epoch 2009: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 07:08:03,523 EPOCH 2010
2024-02-10 07:08:19,784 Epoch 2010: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 07:08:19,785 EPOCH 2011
2024-02-10 07:08:35,689 Epoch 2011: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 07:08:35,689 EPOCH 2012
2024-02-10 07:08:36,175 [Epoch: 2012 Step: 00018100] Batch Recognition Loss:   0.000285 => Gls Tokens per Sec:     2645 || Batch Translation Loss:   0.021506 => Txt Tokens per Sec:     7256 || Lr: 0.000100
2024-02-10 07:08:51,784 Epoch 2012: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 07:08:51,785 EPOCH 2013
2024-02-10 07:09:08,206 Epoch 2013: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 07:09:08,206 EPOCH 2014
2024-02-10 07:09:24,372 Epoch 2014: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 07:09:24,373 EPOCH 2015
2024-02-10 07:09:40,499 Epoch 2015: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 07:09:40,500 EPOCH 2016
2024-02-10 07:09:56,405 Epoch 2016: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 07:09:56,406 EPOCH 2017
2024-02-10 07:10:12,568 Epoch 2017: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 07:10:12,568 EPOCH 2018
2024-02-10 07:10:28,730 Epoch 2018: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 07:10:28,731 EPOCH 2019
2024-02-10 07:10:44,922 Epoch 2019: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 07:10:44,923 EPOCH 2020
2024-02-10 07:11:00,775 Epoch 2020: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-10 07:11:00,775 EPOCH 2021
2024-02-10 07:11:17,158 Epoch 2021: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 07:11:17,158 EPOCH 2022
2024-02-10 07:11:33,185 Epoch 2022: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-10 07:11:33,185 EPOCH 2023
2024-02-10 07:11:38,121 [Epoch: 2023 Step: 00018200] Batch Recognition Loss:   0.000333 => Gls Tokens per Sec:      336 || Batch Translation Loss:   0.010643 => Txt Tokens per Sec:     1023 || Lr: 0.000100
2024-02-10 07:11:49,267 Epoch 2023: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-10 07:11:49,268 EPOCH 2024
2024-02-10 07:12:05,215 Epoch 2024: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 07:12:05,215 EPOCH 2025
2024-02-10 07:12:21,488 Epoch 2025: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 07:12:21,489 EPOCH 2026
2024-02-10 07:12:37,526 Epoch 2026: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 07:12:37,526 EPOCH 2027
2024-02-10 07:12:53,878 Epoch 2027: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 07:12:53,879 EPOCH 2028
2024-02-10 07:13:09,998 Epoch 2028: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 07:13:09,999 EPOCH 2029
2024-02-10 07:13:26,178 Epoch 2029: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 07:13:26,179 EPOCH 2030
2024-02-10 07:13:42,231 Epoch 2030: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 07:13:42,231 EPOCH 2031
2024-02-10 07:13:58,570 Epoch 2031: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 07:13:58,571 EPOCH 2032
2024-02-10 07:14:14,739 Epoch 2032: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 07:14:14,740 EPOCH 2033
2024-02-10 07:14:31,059 Epoch 2033: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 07:14:31,060 EPOCH 2034
2024-02-10 07:14:35,134 [Epoch: 2034 Step: 00018300] Batch Recognition Loss:   0.000959 => Gls Tokens per Sec:      943 || Batch Translation Loss:   0.019257 => Txt Tokens per Sec:     2516 || Lr: 0.000100
2024-02-10 07:14:47,526 Epoch 2034: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 07:14:47,527 EPOCH 2035
2024-02-10 07:15:03,640 Epoch 2035: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 07:15:03,640 EPOCH 2036
2024-02-10 07:15:19,660 Epoch 2036: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 07:15:19,660 EPOCH 2037
2024-02-10 07:15:36,119 Epoch 2037: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 07:15:36,120 EPOCH 2038
2024-02-10 07:15:52,477 Epoch 2038: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 07:15:52,478 EPOCH 2039
2024-02-10 07:16:08,629 Epoch 2039: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 07:16:08,629 EPOCH 2040
2024-02-10 07:16:24,746 Epoch 2040: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 07:16:24,747 EPOCH 2041
2024-02-10 07:16:40,821 Epoch 2041: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 07:16:40,822 EPOCH 2042
2024-02-10 07:16:56,945 Epoch 2042: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 07:16:56,946 EPOCH 2043
2024-02-10 07:17:12,728 Epoch 2043: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 07:17:12,729 EPOCH 2044
2024-02-10 07:17:29,065 Epoch 2044: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 07:17:29,066 EPOCH 2045
2024-02-10 07:17:39,388 [Epoch: 2045 Step: 00018400] Batch Recognition Loss:   0.000419 => Gls Tokens per Sec:      496 || Batch Translation Loss:   0.020034 => Txt Tokens per Sec:     1538 || Lr: 0.000100
2024-02-10 07:17:45,282 Epoch 2045: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 07:17:45,283 EPOCH 2046
2024-02-10 07:18:01,188 Epoch 2046: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 07:18:01,188 EPOCH 2047
2024-02-10 07:18:17,207 Epoch 2047: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-10 07:18:17,207 EPOCH 2048
2024-02-10 07:18:32,798 Epoch 2048: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 07:18:32,798 EPOCH 2049
2024-02-10 07:18:48,891 Epoch 2049: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-10 07:18:48,891 EPOCH 2050
2024-02-10 07:19:04,881 Epoch 2050: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-10 07:19:04,882 EPOCH 2051
2024-02-10 07:19:21,061 Epoch 2051: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.55 
2024-02-10 07:19:21,062 EPOCH 2052
2024-02-10 07:19:37,565 Epoch 2052: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-10 07:19:37,565 EPOCH 2053
2024-02-10 07:19:53,512 Epoch 2053: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.77 
2024-02-10 07:19:53,512 EPOCH 2054
2024-02-10 07:20:09,535 Epoch 2054: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.81 
2024-02-10 07:20:09,536 EPOCH 2055
2024-02-10 07:20:26,706 Epoch 2055: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-10 07:20:26,706 EPOCH 2056
2024-02-10 07:20:38,122 [Epoch: 2056 Step: 00018500] Batch Recognition Loss:   0.000618 => Gls Tokens per Sec:      482 || Batch Translation Loss:   0.038347 => Txt Tokens per Sec:     1352 || Lr: 0.000100
2024-02-10 07:20:42,841 Epoch 2056: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-10 07:20:42,841 EPOCH 2057
2024-02-10 07:20:58,850 Epoch 2057: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-10 07:20:58,851 EPOCH 2058
2024-02-10 07:21:15,251 Epoch 2058: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 07:21:15,252 EPOCH 2059
2024-02-10 07:21:31,586 Epoch 2059: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-10 07:21:31,587 EPOCH 2060
2024-02-10 07:21:47,616 Epoch 2060: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-10 07:21:47,617 EPOCH 2061
2024-02-10 07:22:03,409 Epoch 2061: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.86 
2024-02-10 07:22:03,409 EPOCH 2062
2024-02-10 07:22:19,487 Epoch 2062: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-10 07:22:19,488 EPOCH 2063
2024-02-10 07:22:35,674 Epoch 2063: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.21 
2024-02-10 07:22:35,674 EPOCH 2064
2024-02-10 07:22:51,959 Epoch 2064: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.99 
2024-02-10 07:22:51,959 EPOCH 2065
2024-02-10 07:23:08,176 Epoch 2065: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.08 
2024-02-10 07:23:08,177 EPOCH 2066
2024-02-10 07:23:24,580 Epoch 2066: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-10 07:23:24,581 EPOCH 2067
2024-02-10 07:23:36,697 [Epoch: 2067 Step: 00018600] Batch Recognition Loss:   0.001225 => Gls Tokens per Sec:      560 || Batch Translation Loss:   0.086066 => Txt Tokens per Sec:     1514 || Lr: 0.000100
2024-02-10 07:23:40,784 Epoch 2067: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-10 07:23:40,784 EPOCH 2068
2024-02-10 07:23:57,353 Epoch 2068: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-10 07:23:57,353 EPOCH 2069
2024-02-10 07:24:13,637 Epoch 2069: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-10 07:24:13,637 EPOCH 2070
2024-02-10 07:24:29,698 Epoch 2070: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.70 
2024-02-10 07:24:29,698 EPOCH 2071
2024-02-10 07:24:45,850 Epoch 2071: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.22 
2024-02-10 07:24:45,851 EPOCH 2072
2024-02-10 07:25:01,680 Epoch 2072: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.76 
2024-02-10 07:25:01,681 EPOCH 2073
2024-02-10 07:25:17,744 Epoch 2073: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.22 
2024-02-10 07:25:17,745 EPOCH 2074
2024-02-10 07:25:33,640 Epoch 2074: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-10 07:25:33,641 EPOCH 2075
2024-02-10 07:25:50,037 Epoch 2075: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-10 07:25:50,037 EPOCH 2076
2024-02-10 07:26:06,123 Epoch 2076: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-10 07:26:06,124 EPOCH 2077
2024-02-10 07:26:22,117 Epoch 2077: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.74 
2024-02-10 07:26:22,117 EPOCH 2078
2024-02-10 07:26:31,502 [Epoch: 2078 Step: 00018700] Batch Recognition Loss:   0.000458 => Gls Tokens per Sec:      859 || Batch Translation Loss:   0.085639 => Txt Tokens per Sec:     2249 || Lr: 0.000100
2024-02-10 07:26:37,945 Epoch 2078: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-10 07:26:37,946 EPOCH 2079
2024-02-10 07:26:54,445 Epoch 2079: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-10 07:26:54,445 EPOCH 2080
2024-02-10 07:27:10,586 Epoch 2080: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-10 07:27:10,587 EPOCH 2081
2024-02-10 07:27:26,731 Epoch 2081: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-10 07:27:26,732 EPOCH 2082
2024-02-10 07:27:42,649 Epoch 2082: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-10 07:27:42,650 EPOCH 2083
2024-02-10 07:27:58,355 Epoch 2083: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-10 07:27:58,356 EPOCH 2084
2024-02-10 07:28:14,641 Epoch 2084: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-10 07:28:14,642 EPOCH 2085
2024-02-10 07:28:30,753 Epoch 2085: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 07:28:30,753 EPOCH 2086
2024-02-10 07:28:47,100 Epoch 2086: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 07:28:47,100 EPOCH 2087
2024-02-10 07:29:03,316 Epoch 2087: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 07:29:03,316 EPOCH 2088
2024-02-10 07:29:19,508 Epoch 2088: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 07:29:19,509 EPOCH 2089
2024-02-10 07:29:35,073 [Epoch: 2089 Step: 00018800] Batch Recognition Loss:   0.000664 => Gls Tokens per Sec:      600 || Batch Translation Loss:   0.021982 => Txt Tokens per Sec:     1661 || Lr: 0.000100
2024-02-10 07:29:35,612 Epoch 2089: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 07:29:35,612 EPOCH 2090
2024-02-10 07:29:51,875 Epoch 2090: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 07:29:51,875 EPOCH 2091
2024-02-10 07:30:07,894 Epoch 2091: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 07:30:07,894 EPOCH 2092
2024-02-10 07:30:23,888 Epoch 2092: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 07:30:23,889 EPOCH 2093
2024-02-10 07:30:40,084 Epoch 2093: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 07:30:40,085 EPOCH 2094
2024-02-10 07:30:56,321 Epoch 2094: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 07:30:56,322 EPOCH 2095
2024-02-10 07:31:13,369 Epoch 2095: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 07:31:13,370 EPOCH 2096
2024-02-10 07:31:29,399 Epoch 2096: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 07:31:29,399 EPOCH 2097
2024-02-10 07:31:45,446 Epoch 2097: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 07:31:45,447 EPOCH 2098
2024-02-10 07:32:01,400 Epoch 2098: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 07:32:01,401 EPOCH 2099
2024-02-10 07:32:17,921 Epoch 2099: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 07:32:17,921 EPOCH 2100
2024-02-10 07:32:33,757 [Epoch: 2100 Step: 00018900] Batch Recognition Loss:   0.000431 => Gls Tokens per Sec:      671 || Batch Translation Loss:   0.020326 => Txt Tokens per Sec:     1855 || Lr: 0.000100
2024-02-10 07:32:33,758 Epoch 2100: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 07:32:33,758 EPOCH 2101
2024-02-10 07:32:49,989 Epoch 2101: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 07:32:49,989 EPOCH 2102
2024-02-10 07:33:06,351 Epoch 2102: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 07:33:06,351 EPOCH 2103
2024-02-10 07:33:22,567 Epoch 2103: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 07:33:22,568 EPOCH 2104
2024-02-10 07:33:38,966 Epoch 2104: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 07:33:38,967 EPOCH 2105
2024-02-10 07:33:55,029 Epoch 2105: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 07:33:55,030 EPOCH 2106
2024-02-10 07:34:11,059 Epoch 2106: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 07:34:11,059 EPOCH 2107
2024-02-10 07:34:27,496 Epoch 2107: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 07:34:27,497 EPOCH 2108
2024-02-10 07:34:43,807 Epoch 2108: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 07:34:43,807 EPOCH 2109
2024-02-10 07:34:59,971 Epoch 2109: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 07:34:59,972 EPOCH 2110
2024-02-10 07:35:16,280 Epoch 2110: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 07:35:16,280 EPOCH 2111
2024-02-10 07:35:32,228 Epoch 2111: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 07:35:32,229 EPOCH 2112
2024-02-10 07:35:32,569 [Epoch: 2112 Step: 00019000] Batch Recognition Loss:   0.000322 => Gls Tokens per Sec:     3765 || Batch Translation Loss:   0.011406 => Txt Tokens per Sec:     8329 || Lr: 0.000100
2024-02-10 07:35:48,545 Epoch 2112: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 07:35:48,546 EPOCH 2113
2024-02-10 07:36:04,522 Epoch 2113: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 07:36:04,522 EPOCH 2114
2024-02-10 07:36:20,579 Epoch 2114: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 07:36:20,579 EPOCH 2115
2024-02-10 07:36:36,821 Epoch 2115: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 07:36:36,822 EPOCH 2116
2024-02-10 07:36:52,795 Epoch 2116: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 07:36:52,796 EPOCH 2117
2024-02-10 07:37:09,191 Epoch 2117: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-10 07:37:09,192 EPOCH 2118
2024-02-10 07:37:25,343 Epoch 2118: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-10 07:37:25,344 EPOCH 2119
2024-02-10 07:37:41,360 Epoch 2119: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-10 07:37:41,361 EPOCH 2120
2024-02-10 07:37:57,303 Epoch 2120: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.03 
2024-02-10 07:37:57,304 EPOCH 2121
2024-02-10 07:38:13,390 Epoch 2121: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-10 07:38:13,391 EPOCH 2122
2024-02-10 07:38:29,475 Epoch 2122: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-10 07:38:29,475 EPOCH 2123
2024-02-10 07:38:30,007 [Epoch: 2123 Step: 00019100] Batch Recognition Loss:   0.000748 => Gls Tokens per Sec:     4830 || Batch Translation Loss:   0.058418 => Txt Tokens per Sec:    10509 || Lr: 0.000100
2024-02-10 07:38:45,118 Epoch 2123: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-10 07:38:45,118 EPOCH 2124
2024-02-10 07:39:01,248 Epoch 2124: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-10 07:39:01,249 EPOCH 2125
2024-02-10 07:39:17,854 Epoch 2125: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-10 07:39:17,854 EPOCH 2126
2024-02-10 07:39:33,851 Epoch 2126: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 07:39:33,852 EPOCH 2127
2024-02-10 07:39:49,733 Epoch 2127: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-10 07:39:49,733 EPOCH 2128
2024-02-10 07:40:06,238 Epoch 2128: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 07:40:06,239 EPOCH 2129
2024-02-10 07:40:22,435 Epoch 2129: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 07:40:22,436 EPOCH 2130
2024-02-10 07:40:38,450 Epoch 2130: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 07:40:38,451 EPOCH 2131
2024-02-10 07:40:54,841 Epoch 2131: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 07:40:54,842 EPOCH 2132
2024-02-10 07:41:10,948 Epoch 2132: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 07:41:10,949 EPOCH 2133
2024-02-10 07:41:26,990 Epoch 2133: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 07:41:26,991 EPOCH 2134
2024-02-10 07:41:28,562 [Epoch: 2134 Step: 00019200] Batch Recognition Loss:   0.000332 => Gls Tokens per Sec:     2446 || Batch Translation Loss:   0.017871 => Txt Tokens per Sec:     6435 || Lr: 0.000100
2024-02-10 07:41:43,345 Epoch 2134: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 07:41:43,345 EPOCH 2135
2024-02-10 07:41:59,559 Epoch 2135: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 07:41:59,559 EPOCH 2136
2024-02-10 07:42:15,745 Epoch 2136: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 07:42:15,745 EPOCH 2137
2024-02-10 07:42:32,053 Epoch 2137: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 07:42:32,053 EPOCH 2138
2024-02-10 07:42:48,402 Epoch 2138: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 07:42:48,402 EPOCH 2139
2024-02-10 07:43:04,813 Epoch 2139: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 07:43:04,814 EPOCH 2140
2024-02-10 07:43:21,063 Epoch 2140: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 07:43:21,063 EPOCH 2141
2024-02-10 07:43:37,052 Epoch 2141: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 07:43:37,053 EPOCH 2142
2024-02-10 07:43:53,576 Epoch 2142: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 07:43:53,577 EPOCH 2143
2024-02-10 07:44:10,025 Epoch 2143: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 07:44:10,025 EPOCH 2144
2024-02-10 07:44:26,153 Epoch 2144: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 07:44:26,153 EPOCH 2145
2024-02-10 07:44:33,189 [Epoch: 2145 Step: 00019300] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:      728 || Batch Translation Loss:   0.012078 => Txt Tokens per Sec:     2017 || Lr: 0.000100
2024-02-10 07:44:42,150 Epoch 2145: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 07:44:42,150 EPOCH 2146
2024-02-10 07:44:58,419 Epoch 2146: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 07:44:58,419 EPOCH 2147
2024-02-10 07:45:14,676 Epoch 2147: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 07:45:14,677 EPOCH 2148
2024-02-10 07:45:30,926 Epoch 2148: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 07:45:30,927 EPOCH 2149
2024-02-10 07:45:47,065 Epoch 2149: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 07:45:47,066 EPOCH 2150
2024-02-10 07:46:03,026 Epoch 2150: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 07:46:03,027 EPOCH 2151
2024-02-10 07:46:18,977 Epoch 2151: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 07:46:18,978 EPOCH 2152
2024-02-10 07:46:34,847 Epoch 2152: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 07:46:34,848 EPOCH 2153
2024-02-10 07:46:50,894 Epoch 2153: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 07:46:50,895 EPOCH 2154
2024-02-10 07:47:07,133 Epoch 2154: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 07:47:07,134 EPOCH 2155
2024-02-10 07:47:23,228 Epoch 2155: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 07:47:23,229 EPOCH 2156
2024-02-10 07:47:37,273 [Epoch: 2156 Step: 00019400] Batch Recognition Loss:   0.000359 => Gls Tokens per Sec:      392 || Batch Translation Loss:   0.016701 => Txt Tokens per Sec:     1100 || Lr: 0.000100
2024-02-10 07:47:39,304 Epoch 2156: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 07:47:39,305 EPOCH 2157
2024-02-10 07:47:55,572 Epoch 2157: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 07:47:55,572 EPOCH 2158
2024-02-10 07:48:11,656 Epoch 2158: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 07:48:11,656 EPOCH 2159
2024-02-10 07:48:27,470 Epoch 2159: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 07:48:27,470 EPOCH 2160
2024-02-10 07:48:42,754 Epoch 2160: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 07:48:42,755 EPOCH 2161
2024-02-10 07:48:58,998 Epoch 2161: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 07:48:58,999 EPOCH 2162
2024-02-10 07:49:15,238 Epoch 2162: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 07:49:15,239 EPOCH 2163
2024-02-10 07:49:31,017 Epoch 2163: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 07:49:31,017 EPOCH 2164
2024-02-10 07:49:47,344 Epoch 2164: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 07:49:47,345 EPOCH 2165
2024-02-10 07:50:03,626 Epoch 2165: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 07:50:03,627 EPOCH 2166
2024-02-10 07:50:19,736 Epoch 2166: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 07:50:19,736 EPOCH 2167
2024-02-10 07:50:30,685 [Epoch: 2167 Step: 00019500] Batch Recognition Loss:   0.000153 => Gls Tokens per Sec:      701 || Batch Translation Loss:   0.011466 => Txt Tokens per Sec:     2027 || Lr: 0.000100
2024-02-10 07:50:35,890 Epoch 2167: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 07:50:35,891 EPOCH 2168
2024-02-10 07:50:52,212 Epoch 2168: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 07:50:52,212 EPOCH 2169
2024-02-10 07:51:08,354 Epoch 2169: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 07:51:08,354 EPOCH 2170
2024-02-10 07:51:24,185 Epoch 2170: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 07:51:24,186 EPOCH 2171
2024-02-10 07:51:40,506 Epoch 2171: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 07:51:40,507 EPOCH 2172
2024-02-10 07:51:56,808 Epoch 2172: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 07:51:56,808 EPOCH 2173
2024-02-10 07:52:13,195 Epoch 2173: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 07:52:13,195 EPOCH 2174
2024-02-10 07:52:29,141 Epoch 2174: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 07:52:29,141 EPOCH 2175
2024-02-10 07:52:44,765 Epoch 2175: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 07:52:44,765 EPOCH 2176
2024-02-10 07:53:01,176 Epoch 2176: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 07:53:01,177 EPOCH 2177
2024-02-10 07:53:17,467 Epoch 2177: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-10 07:53:17,467 EPOCH 2178
2024-02-10 07:53:32,511 [Epoch: 2178 Step: 00019600] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:      536 || Batch Translation Loss:   0.077397 => Txt Tokens per Sec:     1545 || Lr: 0.000100
2024-02-10 07:53:33,630 Epoch 2178: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-10 07:53:33,631 EPOCH 2179
2024-02-10 07:53:49,720 Epoch 2179: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-10 07:53:49,721 EPOCH 2180
2024-02-10 07:54:05,478 Epoch 2180: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-10 07:54:05,479 EPOCH 2181
2024-02-10 07:54:21,767 Epoch 2181: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-10 07:54:21,767 EPOCH 2182
2024-02-10 07:54:38,195 Epoch 2182: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-10 07:54:38,196 EPOCH 2183
2024-02-10 07:54:54,438 Epoch 2183: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-10 07:54:54,439 EPOCH 2184
2024-02-10 07:55:10,442 Epoch 2184: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-10 07:55:10,443 EPOCH 2185
2024-02-10 07:55:26,405 Epoch 2185: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-10 07:55:26,405 EPOCH 2186
2024-02-10 07:55:42,574 Epoch 2186: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-10 07:55:42,575 EPOCH 2187
2024-02-10 07:55:58,717 Epoch 2187: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-10 07:55:58,718 EPOCH 2188
2024-02-10 07:56:14,342 Epoch 2188: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-10 07:56:14,343 EPOCH 2189
2024-02-10 07:56:30,120 [Epoch: 2189 Step: 00019700] Batch Recognition Loss:   0.000694 => Gls Tokens per Sec:      592 || Batch Translation Loss:   0.085421 => Txt Tokens per Sec:     1683 || Lr: 0.000100
2024-02-10 07:56:30,457 Epoch 2189: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-10 07:56:30,457 EPOCH 2190
2024-02-10 07:56:46,432 Epoch 2190: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-10 07:56:46,432 EPOCH 2191
2024-02-10 07:57:02,725 Epoch 2191: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-10 07:57:02,725 EPOCH 2192
2024-02-10 07:57:18,948 Epoch 2192: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-10 07:57:18,949 EPOCH 2193
2024-02-10 07:57:34,930 Epoch 2193: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-10 07:57:34,930 EPOCH 2194
2024-02-10 07:57:51,080 Epoch 2194: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-10 07:57:51,080 EPOCH 2195
2024-02-10 07:58:06,992 Epoch 2195: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-10 07:58:06,992 EPOCH 2196
2024-02-10 07:58:22,987 Epoch 2196: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-10 07:58:22,987 EPOCH 2197
2024-02-10 07:58:38,852 Epoch 2197: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-10 07:58:38,853 EPOCH 2198
2024-02-10 07:58:55,080 Epoch 2198: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 07:58:55,081 EPOCH 2199
2024-02-10 07:59:11,401 Epoch 2199: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 07:59:11,401 EPOCH 2200
2024-02-10 07:59:27,446 [Epoch: 2200 Step: 00019800] Batch Recognition Loss:   0.000416 => Gls Tokens per Sec:      662 || Batch Translation Loss:   0.024636 => Txt Tokens per Sec:     1831 || Lr: 0.000100
2024-02-10 07:59:27,447 Epoch 2200: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 07:59:27,447 EPOCH 2201
2024-02-10 07:59:43,732 Epoch 2201: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 07:59:43,732 EPOCH 2202
2024-02-10 07:59:59,815 Epoch 2202: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 07:59:59,815 EPOCH 2203
2024-02-10 08:00:15,316 Epoch 2203: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 08:00:15,316 EPOCH 2204
2024-02-10 08:00:31,643 Epoch 2204: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 08:00:31,644 EPOCH 2205
2024-02-10 08:00:47,343 Epoch 2205: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 08:00:47,343 EPOCH 2206
2024-02-10 08:01:03,621 Epoch 2206: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 08:01:03,622 EPOCH 2207
2024-02-10 08:01:19,841 Epoch 2207: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-10 08:01:19,841 EPOCH 2208
2024-02-10 08:01:36,174 Epoch 2208: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-10 08:01:36,175 EPOCH 2209
2024-02-10 08:01:52,049 Epoch 2209: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-10 08:01:52,049 EPOCH 2210
2024-02-10 08:02:08,218 Epoch 2210: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-10 08:02:08,219 EPOCH 2211
2024-02-10 08:02:24,354 Epoch 2211: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-10 08:02:24,355 EPOCH 2212
2024-02-10 08:02:24,815 [Epoch: 2212 Step: 00019900] Batch Recognition Loss:   0.000494 => Gls Tokens per Sec:     2789 || Batch Translation Loss:   0.061284 => Txt Tokens per Sec:     7022 || Lr: 0.000100
2024-02-10 08:02:40,342 Epoch 2212: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-10 08:02:40,342 EPOCH 2213
2024-02-10 08:02:56,395 Epoch 2213: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-10 08:02:56,396 EPOCH 2214
2024-02-10 08:03:12,395 Epoch 2214: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-10 08:03:12,396 EPOCH 2215
2024-02-10 08:03:28,571 Epoch 2215: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-10 08:03:28,571 EPOCH 2216
2024-02-10 08:03:44,573 Epoch 2216: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-10 08:03:44,574 EPOCH 2217
2024-02-10 08:04:00,525 Epoch 2217: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-10 08:04:00,526 EPOCH 2218
2024-02-10 08:04:16,742 Epoch 2218: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-10 08:04:16,742 EPOCH 2219
2024-02-10 08:04:32,879 Epoch 2219: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-10 08:04:32,880 EPOCH 2220
2024-02-10 08:04:48,839 Epoch 2220: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-10 08:04:48,840 EPOCH 2221
2024-02-10 08:05:04,914 Epoch 2221: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 08:05:04,915 EPOCH 2222
2024-02-10 08:05:21,250 Epoch 2222: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 08:05:21,251 EPOCH 2223
2024-02-10 08:05:21,874 [Epoch: 2223 Step: 00020000] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:     4122 || Batch Translation Loss:   0.024072 => Txt Tokens per Sec:     8317 || Lr: 0.000100
2024-02-10 08:06:33,731 Validation result at epoch 2223, step    20000: duration: 71.8556s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.41454	Translation Loss: 94226.20312	PPL: 12226.07715
	Eval Metric: BLEU
	WER 3.88	(DEL: 0.00,	INS: 0.00,	SUB: 3.88)
	BLEU-4 0.63	(BLEU-1: 11.60,	BLEU-2: 3.91,	BLEU-3: 1.40,	BLEU-4: 0.63)
	CHRF 17.48	ROUGE 9.49
2024-02-10 08:06:33,733 Logging Recognition and Translation Outputs
2024-02-10 08:06:33,733 ========================================================================================================================
2024-02-10 08:06:33,734 Logging Sequence: 120_7.00
2024-02-10 08:06:33,734 	Gloss Reference :	A B+C+D+E
2024-02-10 08:06:33,734 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 08:06:33,734 	Gloss Alignment :	         
2024-02-10 08:06:33,735 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 08:06:33,735 	Text Reference  :	he *** had tested positive for  covid-19 on        may     19
2024-02-10 08:06:33,736 	Text Hypothesis :	he was in  the    first    time a        wrestling stadium in
2024-02-10 08:06:33,736 	Text Alignment  :	   I   S   S      S        S    S        S         S       S 
2024-02-10 08:06:33,736 ========================================================================================================================
2024-02-10 08:06:33,736 Logging Sequence: 148_186.00
2024-02-10 08:06:33,736 	Gloss Reference :	A B+C+D+E
2024-02-10 08:06:33,736 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 08:06:33,737 	Gloss Alignment :	         
2024-02-10 08:06:33,737 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 08:06:33,739 	Text Reference  :	siraj also took  four wickets in 1 over  thus becoming the record-holder for    most   wickets      in an  over  in        odis         
2024-02-10 08:06:33,739 	Text Hypothesis :	***** **** india had  won     in a world cup  trophy   in  2022          neeraj chopra participated in the world athletics championships
2024-02-10 08:06:33,739 	Text Alignment  :	D     D    S     S    S          S S     S    S        S   S             S      S      S               S   S     S         S            
2024-02-10 08:06:33,739 ========================================================================================================================
2024-02-10 08:06:33,739 Logging Sequence: 67_73.00
2024-02-10 08:06:33,740 	Gloss Reference :	A B+C+D+E
2024-02-10 08:06:33,740 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 08:06:33,740 	Gloss Alignment :	         
2024-02-10 08:06:33,740 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 08:06:33,741 	Text Reference  :	** in   his tweet he   also said   
2024-02-10 08:06:33,741 	Text Hypothesis :	we have won the   toss and  captain
2024-02-10 08:06:33,741 	Text Alignment  :	I  S    S   S     S    S    S      
2024-02-10 08:06:33,741 ========================================================================================================================
2024-02-10 08:06:33,741 Logging Sequence: 164_526.00
2024-02-10 08:06:33,741 	Gloss Reference :	A B+C+D+E
2024-02-10 08:06:33,742 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 08:06:33,742 	Gloss Alignment :	         
2024-02-10 08:06:33,742 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 08:06:33,743 	Text Reference  :	you are aware that viacom18 bought the broadcast rights of ** ipl   
2024-02-10 08:06:33,743 	Text Hypothesis :	*** *** ***** **** in       t20is  and different types  of in sports
2024-02-10 08:06:33,743 	Text Alignment  :	D   D   D     D    S        S      S   S         S         I  S     
2024-02-10 08:06:33,743 ========================================================================================================================
2024-02-10 08:06:33,743 Logging Sequence: 108_28.00
2024-02-10 08:06:33,743 	Gloss Reference :	A B+C+D+E
2024-02-10 08:06:33,744 	Gloss Hypothesis:	A B+C+D  
2024-02-10 08:06:33,744 	Gloss Alignment :	  S      
2024-02-10 08:06:33,744 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 08:06:33,746 	Text Reference  :	the 10 teams bought 204  players including 67  foreign players after spending      a   total of     rs    55170 crore
2024-02-10 08:06:33,746 	Text Hypothesis :	*** ** ***** ****** many of      you       may believe in      such  superstitions and khan  always share price tag  
2024-02-10 08:06:33,746 	Text Alignment  :	D   D  D     D      S    S       S         S   S       S       S     S             S   S     S      S     S     S    
2024-02-10 08:06:33,746 ========================================================================================================================
2024-02-10 08:06:50,000 Epoch 2223: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 08:06:50,000 EPOCH 2224
2024-02-10 08:07:06,293 Epoch 2224: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-10 08:07:06,293 EPOCH 2225
2024-02-10 08:07:22,403 Epoch 2225: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-10 08:07:22,403 EPOCH 2226
2024-02-10 08:07:38,628 Epoch 2226: Total Training Recognition Loss 0.01  Total Training Translation Loss 6.32 
2024-02-10 08:07:38,628 EPOCH 2227
2024-02-10 08:07:55,062 Epoch 2227: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.13 
2024-02-10 08:07:55,062 EPOCH 2228
2024-02-10 08:08:11,514 Epoch 2228: Total Training Recognition Loss 0.04  Total Training Translation Loss 6.77 
2024-02-10 08:08:11,514 EPOCH 2229
2024-02-10 08:08:27,666 Epoch 2229: Total Training Recognition Loss 0.04  Total Training Translation Loss 4.68 
2024-02-10 08:08:27,666 EPOCH 2230
2024-02-10 08:08:43,924 Epoch 2230: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.48 
2024-02-10 08:08:43,925 EPOCH 2231
2024-02-10 08:08:59,980 Epoch 2231: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.05 
2024-02-10 08:08:59,980 EPOCH 2232
2024-02-10 08:09:16,261 Epoch 2232: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.17 
2024-02-10 08:09:16,261 EPOCH 2233
2024-02-10 08:09:32,408 Epoch 2233: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-10 08:09:32,408 EPOCH 2234
2024-02-10 08:09:37,693 [Epoch: 2234 Step: 00020100] Batch Recognition Loss:   0.001203 => Gls Tokens per Sec:      556 || Batch Translation Loss:   0.054713 => Txt Tokens per Sec:     1531 || Lr: 0.000100
2024-02-10 08:09:48,511 Epoch 2234: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.48 
2024-02-10 08:09:48,511 EPOCH 2235
2024-02-10 08:10:04,951 Epoch 2235: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-10 08:10:04,952 EPOCH 2236
2024-02-10 08:10:20,992 Epoch 2236: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-10 08:10:20,992 EPOCH 2237
2024-02-10 08:10:37,276 Epoch 2237: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 08:10:37,277 EPOCH 2238
2024-02-10 08:10:53,546 Epoch 2238: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 08:10:53,547 EPOCH 2239
2024-02-10 08:11:08,947 Epoch 2239: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 08:11:08,948 EPOCH 2240
2024-02-10 08:11:25,017 Epoch 2240: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 08:11:25,017 EPOCH 2241
2024-02-10 08:11:41,061 Epoch 2241: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 08:11:41,062 EPOCH 2242
2024-02-10 08:11:57,220 Epoch 2242: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 08:11:57,221 EPOCH 2243
2024-02-10 08:12:13,283 Epoch 2243: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 08:12:13,283 EPOCH 2244
2024-02-10 08:12:29,496 Epoch 2244: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 08:12:29,496 EPOCH 2245
2024-02-10 08:12:36,859 [Epoch: 2245 Step: 00020200] Batch Recognition Loss:   0.000373 => Gls Tokens per Sec:      696 || Batch Translation Loss:   0.018198 => Txt Tokens per Sec:     1964 || Lr: 0.000100
2024-02-10 08:12:46,019 Epoch 2245: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 08:12:46,019 EPOCH 2246
2024-02-10 08:13:02,247 Epoch 2246: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 08:13:02,248 EPOCH 2247
2024-02-10 08:13:18,034 Epoch 2247: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 08:13:18,035 EPOCH 2248
2024-02-10 08:13:33,130 Epoch 2248: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 08:13:33,130 EPOCH 2249
2024-02-10 08:13:48,205 Epoch 2249: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 08:13:48,205 EPOCH 2250
2024-02-10 08:14:03,934 Epoch 2250: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 08:14:03,935 EPOCH 2251
2024-02-10 08:14:20,187 Epoch 2251: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 08:14:20,187 EPOCH 2252
2024-02-10 08:14:36,181 Epoch 2252: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 08:14:36,182 EPOCH 2253
2024-02-10 08:14:52,126 Epoch 2253: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 08:14:52,126 EPOCH 2254
2024-02-10 08:15:08,330 Epoch 2254: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 08:15:08,330 EPOCH 2255
2024-02-10 08:15:24,519 Epoch 2255: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 08:15:24,520 EPOCH 2256
2024-02-10 08:15:30,405 [Epoch: 2256 Step: 00020300] Batch Recognition Loss:   0.000280 => Gls Tokens per Sec:      935 || Batch Translation Loss:   0.015335 => Txt Tokens per Sec:     2358 || Lr: 0.000100
2024-02-10 08:15:40,622 Epoch 2256: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 08:15:40,623 EPOCH 2257
2024-02-10 08:15:56,644 Epoch 2257: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 08:15:56,644 EPOCH 2258
2024-02-10 08:16:12,812 Epoch 2258: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 08:16:12,812 EPOCH 2259
2024-02-10 08:16:29,021 Epoch 2259: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 08:16:29,022 EPOCH 2260
2024-02-10 08:16:45,059 Epoch 2260: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 08:16:45,060 EPOCH 2261
2024-02-10 08:17:01,357 Epoch 2261: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 08:17:01,358 EPOCH 2262
2024-02-10 08:17:17,390 Epoch 2262: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 08:17:17,391 EPOCH 2263
2024-02-10 08:17:33,081 Epoch 2263: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 08:17:33,082 EPOCH 2264
2024-02-10 08:17:49,379 Epoch 2264: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 08:17:49,380 EPOCH 2265
2024-02-10 08:18:05,532 Epoch 2265: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 08:18:05,532 EPOCH 2266
2024-02-10 08:18:21,785 Epoch 2266: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 08:18:21,785 EPOCH 2267
2024-02-10 08:18:30,972 [Epoch: 2267 Step: 00020400] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:      738 || Batch Translation Loss:   0.014736 => Txt Tokens per Sec:     2046 || Lr: 0.000100
2024-02-10 08:18:37,656 Epoch 2267: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 08:18:37,656 EPOCH 2268
2024-02-10 08:18:53,864 Epoch 2268: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 08:18:53,864 EPOCH 2269
2024-02-10 08:19:09,942 Epoch 2269: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 08:19:09,943 EPOCH 2270
2024-02-10 08:19:26,451 Epoch 2270: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 08:19:26,451 EPOCH 2271
2024-02-10 08:19:42,670 Epoch 2271: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 08:19:42,670 EPOCH 2272
2024-02-10 08:19:59,064 Epoch 2272: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 08:19:59,064 EPOCH 2273
2024-02-10 08:20:15,135 Epoch 2273: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 08:20:15,136 EPOCH 2274
2024-02-10 08:20:31,911 Epoch 2274: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 08:20:31,911 EPOCH 2275
2024-02-10 08:20:48,115 Epoch 2275: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 08:20:48,116 EPOCH 2276
2024-02-10 08:21:04,615 Epoch 2276: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 08:21:04,615 EPOCH 2277
2024-02-10 08:21:20,756 Epoch 2277: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 08:21:20,757 EPOCH 2278
2024-02-10 08:21:36,026 [Epoch: 2278 Step: 00020500] Batch Recognition Loss:   0.000293 => Gls Tokens per Sec:      528 || Batch Translation Loss:   0.010088 => Txt Tokens per Sec:     1451 || Lr: 0.000100
2024-02-10 08:21:37,055 Epoch 2278: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 08:21:37,056 EPOCH 2279
2024-02-10 08:21:53,250 Epoch 2279: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 08:21:53,251 EPOCH 2280
2024-02-10 08:22:09,421 Epoch 2280: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 08:22:09,422 EPOCH 2281
2024-02-10 08:22:25,282 Epoch 2281: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 08:22:25,283 EPOCH 2282
2024-02-10 08:22:41,507 Epoch 2282: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 08:22:41,507 EPOCH 2283
2024-02-10 08:22:57,696 Epoch 2283: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 08:22:57,696 EPOCH 2284
2024-02-10 08:23:13,418 Epoch 2284: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 08:23:13,419 EPOCH 2285
2024-02-10 08:23:29,663 Epoch 2285: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 08:23:29,663 EPOCH 2286
2024-02-10 08:23:45,923 Epoch 2286: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 08:23:45,924 EPOCH 2287
2024-02-10 08:24:02,050 Epoch 2287: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 08:24:02,051 EPOCH 2288
2024-02-10 08:24:17,908 Epoch 2288: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 08:24:17,909 EPOCH 2289
2024-02-10 08:24:33,548 [Epoch: 2289 Step: 00020600] Batch Recognition Loss:   0.000210 => Gls Tokens per Sec:      597 || Batch Translation Loss:   0.013814 => Txt Tokens per Sec:     1633 || Lr: 0.000100
2024-02-10 08:24:34,284 Epoch 2289: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 08:24:34,284 EPOCH 2290
2024-02-10 08:24:50,264 Epoch 2290: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 08:24:50,265 EPOCH 2291
2024-02-10 08:25:06,292 Epoch 2291: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 08:25:06,293 EPOCH 2292
2024-02-10 08:25:22,335 Epoch 2292: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 08:25:22,336 EPOCH 2293
2024-02-10 08:25:38,495 Epoch 2293: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 08:25:38,495 EPOCH 2294
2024-02-10 08:25:54,268 Epoch 2294: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 08:25:54,268 EPOCH 2295
2024-02-10 08:26:10,242 Epoch 2295: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 08:26:10,243 EPOCH 2296
2024-02-10 08:26:26,115 Epoch 2296: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 08:26:26,116 EPOCH 2297
2024-02-10 08:26:42,327 Epoch 2297: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 08:26:42,328 EPOCH 2298
2024-02-10 08:26:58,670 Epoch 2298: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 08:26:58,671 EPOCH 2299
2024-02-10 08:27:14,624 Epoch 2299: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 08:27:14,625 EPOCH 2300
2024-02-10 08:27:30,498 [Epoch: 2300 Step: 00020700] Batch Recognition Loss:   0.000681 => Gls Tokens per Sec:      669 || Batch Translation Loss:   0.019432 => Txt Tokens per Sec:     1851 || Lr: 0.000100
2024-02-10 08:27:30,498 Epoch 2300: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 08:27:30,498 EPOCH 2301
2024-02-10 08:27:46,882 Epoch 2301: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 08:27:46,883 EPOCH 2302
2024-02-10 08:28:02,847 Epoch 2302: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 08:28:02,847 EPOCH 2303
2024-02-10 08:28:18,883 Epoch 2303: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 08:28:18,884 EPOCH 2304
2024-02-10 08:28:34,778 Epoch 2304: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 08:28:34,779 EPOCH 2305
2024-02-10 08:28:50,729 Epoch 2305: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 08:28:50,729 EPOCH 2306
2024-02-10 08:29:06,795 Epoch 2306: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 08:29:06,797 EPOCH 2307
2024-02-10 08:29:23,078 Epoch 2307: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 08:29:23,078 EPOCH 2308
2024-02-10 08:29:39,094 Epoch 2308: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 08:29:39,094 EPOCH 2309
2024-02-10 08:29:55,147 Epoch 2309: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 08:29:55,148 EPOCH 2310
2024-02-10 08:30:10,974 Epoch 2310: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 08:30:10,975 EPOCH 2311
2024-02-10 08:30:26,836 Epoch 2311: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 08:30:26,837 EPOCH 2312
2024-02-10 08:30:30,980 [Epoch: 2312 Step: 00020800] Batch Recognition Loss:   0.000159 => Gls Tokens per Sec:       92 || Batch Translation Loss:   0.007009 => Txt Tokens per Sec:      328 || Lr: 0.000100
2024-02-10 08:30:42,655 Epoch 2312: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 08:30:42,656 EPOCH 2313
2024-02-10 08:30:58,641 Epoch 2313: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 08:30:58,642 EPOCH 2314
2024-02-10 08:31:14,848 Epoch 2314: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 08:31:14,849 EPOCH 2315
2024-02-10 08:31:31,183 Epoch 2315: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 08:31:31,184 EPOCH 2316
2024-02-10 08:31:47,427 Epoch 2316: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 08:31:47,428 EPOCH 2317
2024-02-10 08:32:03,758 Epoch 2317: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 08:32:03,759 EPOCH 2318
2024-02-10 08:32:20,029 Epoch 2318: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 08:32:20,030 EPOCH 2319
2024-02-10 08:32:36,098 Epoch 2319: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 08:32:36,098 EPOCH 2320
2024-02-10 08:32:51,867 Epoch 2320: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 08:32:51,867 EPOCH 2321
2024-02-10 08:33:07,934 Epoch 2321: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 08:33:07,935 EPOCH 2322
2024-02-10 08:33:23,794 Epoch 2322: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 08:33:23,794 EPOCH 2323
2024-02-10 08:33:27,333 [Epoch: 2323 Step: 00020900] Batch Recognition Loss:   0.000669 => Gls Tokens per Sec:      724 || Batch Translation Loss:   0.019728 => Txt Tokens per Sec:     2076 || Lr: 0.000100
2024-02-10 08:33:39,942 Epoch 2323: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 08:33:39,943 EPOCH 2324
2024-02-10 08:33:56,195 Epoch 2324: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 08:33:56,195 EPOCH 2325
2024-02-10 08:34:12,443 Epoch 2325: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 08:34:12,444 EPOCH 2326
2024-02-10 08:34:28,570 Epoch 2326: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 08:34:28,571 EPOCH 2327
2024-02-10 08:34:44,630 Epoch 2327: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 08:34:44,630 EPOCH 2328
2024-02-10 08:35:00,795 Epoch 2328: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 08:35:00,796 EPOCH 2329
2024-02-10 08:35:16,809 Epoch 2329: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 08:35:16,810 EPOCH 2330
2024-02-10 08:35:32,773 Epoch 2330: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 08:35:32,774 EPOCH 2331
2024-02-10 08:35:49,163 Epoch 2331: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-10 08:35:49,164 EPOCH 2332
2024-02-10 08:36:05,617 Epoch 2332: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 08:36:05,617 EPOCH 2333
2024-02-10 08:36:21,755 Epoch 2333: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 08:36:21,755 EPOCH 2334
2024-02-10 08:36:31,293 [Epoch: 2334 Step: 00021000] Batch Recognition Loss:   0.000346 => Gls Tokens per Sec:      403 || Batch Translation Loss:   0.022304 => Txt Tokens per Sec:     1282 || Lr: 0.000100
2024-02-10 08:36:38,001 Epoch 2334: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-10 08:36:38,001 EPOCH 2335
2024-02-10 08:36:54,141 Epoch 2335: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-10 08:36:54,141 EPOCH 2336
2024-02-10 08:37:10,354 Epoch 2336: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-10 08:37:10,355 EPOCH 2337
2024-02-10 08:37:26,355 Epoch 2337: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-10 08:37:26,356 EPOCH 2338
2024-02-10 08:37:42,363 Epoch 2338: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-10 08:37:42,364 EPOCH 2339
2024-02-10 08:37:58,283 Epoch 2339: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.92 
2024-02-10 08:37:58,284 EPOCH 2340
2024-02-10 08:38:14,656 Epoch 2340: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.19 
2024-02-10 08:38:14,657 EPOCH 2341
2024-02-10 08:38:30,477 Epoch 2341: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.32 
2024-02-10 08:38:30,478 EPOCH 2342
2024-02-10 08:38:46,776 Epoch 2342: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.89 
2024-02-10 08:38:46,777 EPOCH 2343
2024-02-10 08:39:02,937 Epoch 2343: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.95 
2024-02-10 08:39:02,937 EPOCH 2344
2024-02-10 08:39:18,743 Epoch 2344: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.42 
2024-02-10 08:39:18,743 EPOCH 2345
2024-02-10 08:39:28,636 [Epoch: 2345 Step: 00021100] Batch Recognition Loss:   0.002771 => Gls Tokens per Sec:      518 || Batch Translation Loss:   0.089628 => Txt Tokens per Sec:     1378 || Lr: 0.000100
2024-02-10 08:39:35,219 Epoch 2345: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.14 
2024-02-10 08:39:35,220 EPOCH 2346
2024-02-10 08:39:51,186 Epoch 2346: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-10 08:39:51,187 EPOCH 2347
2024-02-10 08:40:07,562 Epoch 2347: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.80 
2024-02-10 08:40:07,562 EPOCH 2348
2024-02-10 08:40:23,524 Epoch 2348: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-10 08:40:23,524 EPOCH 2349
2024-02-10 08:40:39,901 Epoch 2349: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-10 08:40:39,902 EPOCH 2350
2024-02-10 08:40:56,051 Epoch 2350: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-10 08:40:56,052 EPOCH 2351
2024-02-10 08:41:12,012 Epoch 2351: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-10 08:41:12,013 EPOCH 2352
2024-02-10 08:41:28,273 Epoch 2352: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 08:41:28,274 EPOCH 2353
2024-02-10 08:41:44,237 Epoch 2353: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 08:41:44,237 EPOCH 2354
2024-02-10 08:42:00,014 Epoch 2354: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 08:42:00,014 EPOCH 2355
2024-02-10 08:42:15,959 Epoch 2355: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 08:42:15,959 EPOCH 2356
2024-02-10 08:42:21,239 [Epoch: 2356 Step: 00021200] Batch Recognition Loss:   0.000427 => Gls Tokens per Sec:     1212 || Batch Translation Loss:   0.014762 => Txt Tokens per Sec:     3315 || Lr: 0.000100
2024-02-10 08:42:32,291 Epoch 2356: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 08:42:32,292 EPOCH 2357
2024-02-10 08:42:48,011 Epoch 2357: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 08:42:48,011 EPOCH 2358
2024-02-10 08:43:04,058 Epoch 2358: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 08:43:04,059 EPOCH 2359
2024-02-10 08:43:20,199 Epoch 2359: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 08:43:20,199 EPOCH 2360
2024-02-10 08:43:36,503 Epoch 2360: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 08:43:36,503 EPOCH 2361
2024-02-10 08:43:52,735 Epoch 2361: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 08:43:52,735 EPOCH 2362
2024-02-10 08:44:08,667 Epoch 2362: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 08:44:08,668 EPOCH 2363
2024-02-10 08:44:24,775 Epoch 2363: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 08:44:24,776 EPOCH 2364
2024-02-10 08:44:41,113 Epoch 2364: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 08:44:41,114 EPOCH 2365
2024-02-10 08:44:56,932 Epoch 2365: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 08:44:56,933 EPOCH 2366
2024-02-10 08:45:13,175 Epoch 2366: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 08:45:13,176 EPOCH 2367
2024-02-10 08:45:22,474 [Epoch: 2367 Step: 00021300] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:      729 || Batch Translation Loss:   0.015681 => Txt Tokens per Sec:     1968 || Lr: 0.000100
2024-02-10 08:45:29,448 Epoch 2367: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 08:45:29,448 EPOCH 2368
2024-02-10 08:45:45,607 Epoch 2368: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 08:45:45,608 EPOCH 2369
2024-02-10 08:46:01,958 Epoch 2369: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 08:46:01,958 EPOCH 2370
2024-02-10 08:46:18,038 Epoch 2370: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 08:46:18,039 EPOCH 2371
2024-02-10 08:46:34,261 Epoch 2371: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 08:46:34,262 EPOCH 2372
2024-02-10 08:46:50,260 Epoch 2372: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 08:46:50,260 EPOCH 2373
2024-02-10 08:47:06,334 Epoch 2373: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 08:47:06,335 EPOCH 2374
2024-02-10 08:47:22,416 Epoch 2374: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 08:47:22,417 EPOCH 2375
2024-02-10 08:47:38,341 Epoch 2375: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 08:47:38,341 EPOCH 2376
2024-02-10 08:47:54,441 Epoch 2376: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 08:47:54,441 EPOCH 2377
2024-02-10 08:48:10,648 Epoch 2377: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 08:48:10,649 EPOCH 2378
2024-02-10 08:48:16,612 [Epoch: 2378 Step: 00021400] Batch Recognition Loss:   0.000327 => Gls Tokens per Sec:     1503 || Batch Translation Loss:   0.017928 => Txt Tokens per Sec:     3959 || Lr: 0.000100
2024-02-10 08:48:26,542 Epoch 2378: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 08:48:26,542 EPOCH 2379
2024-02-10 08:48:42,933 Epoch 2379: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 08:48:42,934 EPOCH 2380
2024-02-10 08:48:58,668 Epoch 2380: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 08:48:58,668 EPOCH 2381
2024-02-10 08:49:14,587 Epoch 2381: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 08:49:14,588 EPOCH 2382
2024-02-10 08:49:30,621 Epoch 2382: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 08:49:30,621 EPOCH 2383
2024-02-10 08:49:46,636 Epoch 2383: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 08:49:46,637 EPOCH 2384
2024-02-10 08:50:02,715 Epoch 2384: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 08:50:02,715 EPOCH 2385
2024-02-10 08:50:18,812 Epoch 2385: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 08:50:18,813 EPOCH 2386
2024-02-10 08:50:34,940 Epoch 2386: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 08:50:34,941 EPOCH 2387
2024-02-10 08:50:50,994 Epoch 2387: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 08:50:50,994 EPOCH 2388
2024-02-10 08:51:07,006 Epoch 2388: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 08:51:07,007 EPOCH 2389
2024-02-10 08:51:22,226 [Epoch: 2389 Step: 00021500] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:      614 || Batch Translation Loss:   0.015578 => Txt Tokens per Sec:     1687 || Lr: 0.000100
2024-02-10 08:51:22,766 Epoch 2389: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 08:51:22,766 EPOCH 2390
2024-02-10 08:51:39,125 Epoch 2390: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 08:51:39,125 EPOCH 2391
2024-02-10 08:51:55,337 Epoch 2391: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 08:51:55,338 EPOCH 2392
2024-02-10 08:52:11,310 Epoch 2392: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 08:52:11,311 EPOCH 2393
2024-02-10 08:52:27,387 Epoch 2393: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 08:52:27,388 EPOCH 2394
2024-02-10 08:52:43,513 Epoch 2394: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 08:52:43,513 EPOCH 2395
2024-02-10 08:52:59,292 Epoch 2395: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 08:52:59,292 EPOCH 2396
2024-02-10 08:53:15,461 Epoch 2396: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 08:53:15,462 EPOCH 2397
2024-02-10 08:53:31,663 Epoch 2397: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-10 08:53:31,664 EPOCH 2398
2024-02-10 08:53:47,647 Epoch 2398: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-10 08:53:47,648 EPOCH 2399
2024-02-10 08:54:03,751 Epoch 2399: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 08:54:03,752 EPOCH 2400
2024-02-10 08:54:19,650 [Epoch: 2400 Step: 00021600] Batch Recognition Loss:   0.000231 => Gls Tokens per Sec:      668 || Batch Translation Loss:   0.040382 => Txt Tokens per Sec:     1848 || Lr: 0.000100
2024-02-10 08:54:19,651 Epoch 2400: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-10 08:54:19,651 EPOCH 2401
2024-02-10 08:54:36,043 Epoch 2401: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-10 08:54:36,044 EPOCH 2402
2024-02-10 08:54:51,895 Epoch 2402: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 08:54:51,896 EPOCH 2403
2024-02-10 08:55:07,736 Epoch 2403: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 08:55:07,737 EPOCH 2404
2024-02-10 08:55:23,511 Epoch 2404: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 08:55:23,512 EPOCH 2405
2024-02-10 08:55:39,720 Epoch 2405: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-10 08:55:39,721 EPOCH 2406
2024-02-10 08:55:55,894 Epoch 2406: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-10 08:55:55,895 EPOCH 2407
2024-02-10 08:56:12,270 Epoch 2407: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-10 08:56:12,271 EPOCH 2408
2024-02-10 08:56:28,260 Epoch 2408: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-10 08:56:28,260 EPOCH 2409
2024-02-10 08:56:44,261 Epoch 2409: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-10 08:56:44,262 EPOCH 2410
2024-02-10 08:57:00,535 Epoch 2410: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-10 08:57:00,535 EPOCH 2411
2024-02-10 08:57:16,418 Epoch 2411: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-10 08:57:16,418 EPOCH 2412
2024-02-10 08:57:16,913 [Epoch: 2412 Step: 00021700] Batch Recognition Loss:   0.000485 => Gls Tokens per Sec:     2591 || Batch Translation Loss:   0.072205 => Txt Tokens per Sec:     7567 || Lr: 0.000100
2024-02-10 08:57:32,341 Epoch 2412: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.55 
2024-02-10 08:57:32,342 EPOCH 2413
2024-02-10 08:57:48,458 Epoch 2413: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.16 
2024-02-10 08:57:48,459 EPOCH 2414
2024-02-10 08:58:04,554 Epoch 2414: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.05 
2024-02-10 08:58:04,554 EPOCH 2415
2024-02-10 08:58:20,813 Epoch 2415: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.19 
2024-02-10 08:58:20,813 EPOCH 2416
2024-02-10 08:58:36,818 Epoch 2416: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.48 
2024-02-10 08:58:36,819 EPOCH 2417
2024-02-10 08:58:52,821 Epoch 2417: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.57 
2024-02-10 08:58:52,822 EPOCH 2418
2024-02-10 08:59:09,116 Epoch 2418: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.06 
2024-02-10 08:59:09,116 EPOCH 2419
2024-02-10 08:59:25,255 Epoch 2419: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-10 08:59:25,256 EPOCH 2420
2024-02-10 08:59:41,243 Epoch 2420: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-10 08:59:41,244 EPOCH 2421
2024-02-10 08:59:57,427 Epoch 2421: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-10 08:59:57,427 EPOCH 2422
2024-02-10 09:00:13,406 Epoch 2422: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-10 09:00:13,406 EPOCH 2423
2024-02-10 09:00:14,889 [Epoch: 2423 Step: 00021800] Batch Recognition Loss:   0.000889 => Gls Tokens per Sec:     1729 || Batch Translation Loss:   0.037368 => Txt Tokens per Sec:     5119 || Lr: 0.000100
2024-02-10 09:00:29,622 Epoch 2423: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 09:00:29,623 EPOCH 2424
2024-02-10 09:00:45,705 Epoch 2424: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 09:00:45,706 EPOCH 2425
2024-02-10 09:01:01,563 Epoch 2425: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 09:01:01,564 EPOCH 2426
2024-02-10 09:01:17,822 Epoch 2426: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 09:01:17,822 EPOCH 2427
2024-02-10 09:01:34,002 Epoch 2427: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 09:01:34,003 EPOCH 2428
2024-02-10 09:01:49,925 Epoch 2428: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 09:01:49,926 EPOCH 2429
2024-02-10 09:02:06,038 Epoch 2429: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 09:02:06,039 EPOCH 2430
2024-02-10 09:02:22,045 Epoch 2430: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 09:02:22,046 EPOCH 2431
2024-02-10 09:02:37,938 Epoch 2431: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 09:02:37,938 EPOCH 2432
2024-02-10 09:02:54,136 Epoch 2432: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 09:02:54,137 EPOCH 2433
2024-02-10 09:03:10,120 Epoch 2433: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 09:03:10,121 EPOCH 2434
2024-02-10 09:03:15,172 [Epoch: 2434 Step: 00021900] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:      582 || Batch Translation Loss:   0.008770 => Txt Tokens per Sec:     1616 || Lr: 0.000100
2024-02-10 09:03:26,228 Epoch 2434: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 09:03:26,229 EPOCH 2435
2024-02-10 09:03:42,382 Epoch 2435: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 09:03:42,383 EPOCH 2436
2024-02-10 09:03:58,489 Epoch 2436: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 09:03:58,490 EPOCH 2437
2024-02-10 09:04:14,371 Epoch 2437: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 09:04:14,371 EPOCH 2438
2024-02-10 09:04:30,476 Epoch 2438: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 09:04:30,476 EPOCH 2439
2024-02-10 09:04:46,457 Epoch 2439: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 09:04:46,458 EPOCH 2440
2024-02-10 09:05:02,589 Epoch 2440: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 09:05:02,589 EPOCH 2441
2024-02-10 09:05:18,768 Epoch 2441: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 09:05:18,769 EPOCH 2442
2024-02-10 09:05:35,029 Epoch 2442: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 09:05:35,029 EPOCH 2443
2024-02-10 09:05:51,010 Epoch 2443: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 09:05:51,010 EPOCH 2444
2024-02-10 09:06:07,054 Epoch 2444: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 09:06:07,055 EPOCH 2445
2024-02-10 09:06:09,273 [Epoch: 2445 Step: 00022000] Batch Recognition Loss:   0.000565 => Gls Tokens per Sec:     2310 || Batch Translation Loss:   0.017860 => Txt Tokens per Sec:     6167 || Lr: 0.000100
2024-02-10 09:07:20,765 Hooray! New best validation result [eval_metric]!
2024-02-10 09:07:20,766 Saving new checkpoint.
2024-02-10 09:07:21,131 Validation result at epoch 2445, step    22000: duration: 71.8578s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.44279	Translation Loss: 93292.56250	PPL: 11137.51270
	Eval Metric: BLEU
	WER 3.88	(DEL: 0.00,	INS: 0.00,	SUB: 3.88)
	BLEU-4 0.86	(BLEU-1: 11.24,	BLEU-2: 3.76,	BLEU-3: 1.63,	BLEU-4: 0.86)
	CHRF 17.26	ROUGE 9.55
2024-02-10 09:07:21,133 Logging Recognition and Translation Outputs
2024-02-10 09:07:21,133 ========================================================================================================================
2024-02-10 09:07:21,133 Logging Sequence: 179_2.00
2024-02-10 09:07:21,134 	Gloss Reference :	A B+C+D+E
2024-02-10 09:07:21,134 	Gloss Hypothesis:	A B+C+D  
2024-02-10 09:07:21,134 	Gloss Alignment :	  S      
2024-02-10 09:07:21,134 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 09:07:21,135 	Text Reference  :	vinesh phogat is  a    well known wrestler
2024-02-10 09:07:21,135 	Text Hypothesis :	****** the    duo were very sorry you     
2024-02-10 09:07:21,135 	Text Alignment  :	D      S      S   S    S    S     S       
2024-02-10 09:07:21,136 ========================================================================================================================
2024-02-10 09:07:21,136 Logging Sequence: 55_124.00
2024-02-10 09:07:21,136 	Gloss Reference :	A B+C+D+E
2024-02-10 09:07:21,136 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 09:07:21,136 	Gloss Alignment :	         
2024-02-10 09:07:21,137 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 09:07:21,137 	Text Reference  :	next to  him with    the        patel jersey was ajaz  patel
2024-02-10 09:07:21,137 	Text Hypothesis :	it   was a   strange experience on    the    4th march 2023 
2024-02-10 09:07:21,138 	Text Alignment  :	S    S   S   S       S          S     S      S   S     S    
2024-02-10 09:07:21,138 ========================================================================================================================
2024-02-10 09:07:21,138 Logging Sequence: 148_105.00
2024-02-10 09:07:21,138 	Gloss Reference :	A B+C+D+E
2024-02-10 09:07:21,138 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 09:07:21,139 	Gloss Alignment :	         
2024-02-10 09:07:21,139 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 09:07:21,141 	Text Reference  :	later with amazing bowling   by     hardik pandya and kuldeep yadav sri lanka were all out  in just 50  runs   
2024-02-10 09:07:21,141 	Text Hypothesis :	***** even india'  legendary scored 63     runs   off 31      balls had also  took her face in **** 321 innings
2024-02-10 09:07:21,141 	Text Alignment  :	D     S    S       S         S      S      S      S   S       S     S   S     S    S   S       D    S   S      
2024-02-10 09:07:21,141 ========================================================================================================================
2024-02-10 09:07:21,141 Logging Sequence: 125_165.00
2024-02-10 09:07:21,142 	Gloss Reference :	A B+C+D+E
2024-02-10 09:07:21,142 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 09:07:21,142 	Gloss Alignment :	         
2024-02-10 09:07:21,142 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 09:07:21,143 	Text Reference  :	please do not target nadeem we speak to  each other and        share a     good bond    
2024-02-10 09:07:21,143 	Text Hypothesis :	****** ** *** ****** ****** ** while one were many  youngsters in    india who  practise
2024-02-10 09:07:21,143 	Text Alignment  :	D      D  D   D      D      D  S     S   S    S     S          S     S     S    S       
2024-02-10 09:07:21,143 ========================================================================================================================
2024-02-10 09:07:21,143 Logging Sequence: 77_52.00
2024-02-10 09:07:21,144 	Gloss Reference :	A B+C+D+E
2024-02-10 09:07:21,144 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 09:07:21,144 	Gloss Alignment :	         
2024-02-10 09:07:21,144 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 09:07:21,146 	Text Reference  :	kane williamson held down the  fort for hyderabad by   scoring   66  runs    and  ended the match  in a tie   
2024-02-10 09:07:21,146 	Text Hypothesis :	**** ********** the  8    runs were an  8         days wonderful but england lost to    an  option of 6 medals
2024-02-10 09:07:21,146 	Text Alignment  :	D    D          S    S    S    S    S   S         S    S         S   S       S    S     S   S      S  S S     
2024-02-10 09:07:21,146 ========================================================================================================================
2024-02-10 09:07:35,866 Epoch 2445: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 09:07:35,867 EPOCH 2446
2024-02-10 09:07:52,380 Epoch 2446: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 09:07:52,380 EPOCH 2447
2024-02-10 09:08:08,723 Epoch 2447: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 09:08:08,723 EPOCH 2448
2024-02-10 09:08:24,903 Epoch 2448: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 09:08:24,904 EPOCH 2449
2024-02-10 09:08:40,989 Epoch 2449: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 09:08:40,989 EPOCH 2450
2024-02-10 09:08:57,049 Epoch 2450: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 09:08:57,049 EPOCH 2451
2024-02-10 09:09:13,027 Epoch 2451: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 09:09:13,028 EPOCH 2452
2024-02-10 09:09:29,176 Epoch 2452: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 09:09:29,177 EPOCH 2453
2024-02-10 09:09:45,361 Epoch 2453: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 09:09:45,362 EPOCH 2454
2024-02-10 09:10:01,288 Epoch 2454: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 09:10:01,289 EPOCH 2455
2024-02-10 09:10:17,014 Epoch 2455: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 09:10:17,015 EPOCH 2456
2024-02-10 09:10:23,071 [Epoch: 2456 Step: 00022100] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:      908 || Batch Translation Loss:   0.008049 => Txt Tokens per Sec:     2428 || Lr: 0.000100
2024-02-10 09:10:33,092 Epoch 2456: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 09:10:33,093 EPOCH 2457
2024-02-10 09:10:49,180 Epoch 2457: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 09:10:49,181 EPOCH 2458
2024-02-10 09:11:05,418 Epoch 2458: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 09:11:05,419 EPOCH 2459
2024-02-10 09:11:21,730 Epoch 2459: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 09:11:21,731 EPOCH 2460
2024-02-10 09:11:37,796 Epoch 2460: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 09:11:37,797 EPOCH 2461
2024-02-10 09:11:54,004 Epoch 2461: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 09:11:54,004 EPOCH 2462
2024-02-10 09:12:09,866 Epoch 2462: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 09:12:09,866 EPOCH 2463
2024-02-10 09:12:26,686 Epoch 2463: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 09:12:26,686 EPOCH 2464
2024-02-10 09:12:43,049 Epoch 2464: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 09:12:43,050 EPOCH 2465
2024-02-10 09:12:59,044 Epoch 2465: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 09:12:59,045 EPOCH 2466
2024-02-10 09:13:15,250 Epoch 2466: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 09:13:15,251 EPOCH 2467
2024-02-10 09:13:27,217 [Epoch: 2467 Step: 00022200] Batch Recognition Loss:   0.000297 => Gls Tokens per Sec:      567 || Batch Translation Loss:   0.022292 => Txt Tokens per Sec:     1627 || Lr: 0.000100
2024-02-10 09:13:31,144 Epoch 2467: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 09:13:31,144 EPOCH 2468
2024-02-10 09:13:47,141 Epoch 2468: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 09:13:47,142 EPOCH 2469
2024-02-10 09:14:03,057 Epoch 2469: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 09:14:03,057 EPOCH 2470
2024-02-10 09:14:19,384 Epoch 2470: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 09:14:19,384 EPOCH 2471
2024-02-10 09:14:35,469 Epoch 2471: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 09:14:35,470 EPOCH 2472
2024-02-10 09:14:51,672 Epoch 2472: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 09:14:51,673 EPOCH 2473
2024-02-10 09:15:07,719 Epoch 2473: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 09:15:07,720 EPOCH 2474
2024-02-10 09:15:23,564 Epoch 2474: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 09:15:23,564 EPOCH 2475
2024-02-10 09:15:39,916 Epoch 2475: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 09:15:39,917 EPOCH 2476
2024-02-10 09:15:56,291 Epoch 2476: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 09:15:56,291 EPOCH 2477
2024-02-10 09:16:12,308 Epoch 2477: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 09:16:12,309 EPOCH 2478
2024-02-10 09:16:23,695 [Epoch: 2478 Step: 00022300] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:      787 || Batch Translation Loss:   0.019906 => Txt Tokens per Sec:     2172 || Lr: 0.000100
2024-02-10 09:16:28,372 Epoch 2478: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 09:16:28,373 EPOCH 2479
2024-02-10 09:16:44,486 Epoch 2479: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 09:16:44,486 EPOCH 2480
2024-02-10 09:17:00,928 Epoch 2480: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 09:17:00,929 EPOCH 2481
2024-02-10 09:17:17,156 Epoch 2481: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 09:17:17,156 EPOCH 2482
2024-02-10 09:17:33,224 Epoch 2482: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 09:17:33,225 EPOCH 2483
2024-02-10 09:17:49,442 Epoch 2483: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 09:17:49,442 EPOCH 2484
2024-02-10 09:18:05,782 Epoch 2484: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 09:18:05,783 EPOCH 2485
2024-02-10 09:18:21,928 Epoch 2485: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 09:18:21,928 EPOCH 2486
2024-02-10 09:18:37,805 Epoch 2486: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 09:18:37,806 EPOCH 2487
2024-02-10 09:18:54,118 Epoch 2487: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 09:18:54,118 EPOCH 2488
2024-02-10 09:19:10,063 Epoch 2488: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 09:19:10,063 EPOCH 2489
2024-02-10 09:19:20,350 [Epoch: 2489 Step: 00022400] Batch Recognition Loss:   0.000217 => Gls Tokens per Sec:      908 || Batch Translation Loss:   0.012718 => Txt Tokens per Sec:     2426 || Lr: 0.000100
2024-02-10 09:19:26,128 Epoch 2489: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 09:19:26,128 EPOCH 2490
2024-02-10 09:19:42,344 Epoch 2490: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 09:19:42,344 EPOCH 2491
2024-02-10 09:19:58,715 Epoch 2491: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 09:19:58,716 EPOCH 2492
2024-02-10 09:20:14,805 Epoch 2492: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 09:20:14,806 EPOCH 2493
2024-02-10 09:20:31,921 Epoch 2493: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 09:20:31,922 EPOCH 2494
2024-02-10 09:20:48,820 Epoch 2494: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 09:20:48,821 EPOCH 2495
2024-02-10 09:21:04,962 Epoch 2495: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 09:21:04,963 EPOCH 2496
2024-02-10 09:21:21,211 Epoch 2496: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 09:21:21,211 EPOCH 2497
2024-02-10 09:21:37,253 Epoch 2497: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 09:21:37,254 EPOCH 2498
2024-02-10 09:21:53,289 Epoch 2498: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 09:21:53,290 EPOCH 2499
2024-02-10 09:22:09,232 Epoch 2499: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 09:22:09,233 EPOCH 2500
2024-02-10 09:22:25,204 [Epoch: 2500 Step: 00022500] Batch Recognition Loss:   0.000263 => Gls Tokens per Sec:      665 || Batch Translation Loss:   0.016113 => Txt Tokens per Sec:     1840 || Lr: 0.000100
2024-02-10 09:22:25,205 Epoch 2500: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 09:22:25,205 EPOCH 2501
2024-02-10 09:22:41,459 Epoch 2501: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 09:22:41,459 EPOCH 2502
2024-02-10 09:22:57,716 Epoch 2502: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 09:22:57,717 EPOCH 2503
2024-02-10 09:23:13,690 Epoch 2503: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 09:23:13,691 EPOCH 2504
2024-02-10 09:23:29,686 Epoch 2504: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 09:23:29,687 EPOCH 2505
2024-02-10 09:23:45,841 Epoch 2505: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 09:23:45,841 EPOCH 2506
2024-02-10 09:24:01,830 Epoch 2506: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 09:24:01,831 EPOCH 2507
2024-02-10 09:24:17,817 Epoch 2507: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 09:24:17,818 EPOCH 2508
2024-02-10 09:24:33,940 Epoch 2508: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-10 09:24:33,941 EPOCH 2509
2024-02-10 09:24:50,170 Epoch 2509: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-10 09:24:50,171 EPOCH 2510
2024-02-10 09:25:06,312 Epoch 2510: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-10 09:25:06,313 EPOCH 2511
2024-02-10 09:25:22,898 Epoch 2511: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-10 09:25:22,899 EPOCH 2512
2024-02-10 09:25:23,227 [Epoch: 2512 Step: 00022600] Batch Recognition Loss:   0.000414 => Gls Tokens per Sec:     3902 || Batch Translation Loss:   0.038699 => Txt Tokens per Sec:     8686 || Lr: 0.000100
2024-02-10 09:25:38,979 Epoch 2512: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-10 09:25:38,980 EPOCH 2513
2024-02-10 09:25:54,981 Epoch 2513: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-10 09:25:54,981 EPOCH 2514
2024-02-10 09:26:11,039 Epoch 2514: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-10 09:26:11,040 EPOCH 2515
2024-02-10 09:26:27,279 Epoch 2515: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-10 09:26:27,280 EPOCH 2516
2024-02-10 09:26:43,197 Epoch 2516: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-10 09:26:43,198 EPOCH 2517
2024-02-10 09:26:59,074 Epoch 2517: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-10 09:26:59,074 EPOCH 2518
2024-02-10 09:27:15,060 Epoch 2518: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-10 09:27:15,060 EPOCH 2519
2024-02-10 09:27:31,104 Epoch 2519: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-10 09:27:31,105 EPOCH 2520
2024-02-10 09:27:47,177 Epoch 2520: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 09:27:47,178 EPOCH 2521
2024-02-10 09:28:03,552 Epoch 2521: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 09:28:03,553 EPOCH 2522
2024-02-10 09:28:19,624 Epoch 2522: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 09:28:19,625 EPOCH 2523
2024-02-10 09:28:24,577 [Epoch: 2523 Step: 00022700] Batch Recognition Loss:   0.001149 => Gls Tokens per Sec:      335 || Batch Translation Loss:   0.014864 => Txt Tokens per Sec:      982 || Lr: 0.000100
2024-02-10 09:28:35,982 Epoch 2523: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-10 09:28:35,983 EPOCH 2524
2024-02-10 09:28:52,284 Epoch 2524: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 09:28:52,285 EPOCH 2525
2024-02-10 09:29:08,480 Epoch 2525: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 09:29:08,481 EPOCH 2526
2024-02-10 09:29:24,394 Epoch 2526: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-10 09:29:24,395 EPOCH 2527
2024-02-10 09:29:39,911 Epoch 2527: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-10 09:29:39,912 EPOCH 2528
2024-02-10 09:29:56,205 Epoch 2528: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 09:29:56,205 EPOCH 2529
2024-02-10 09:30:12,372 Epoch 2529: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 09:30:12,372 EPOCH 2530
2024-02-10 09:30:28,340 Epoch 2530: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 09:30:28,340 EPOCH 2531
2024-02-10 09:30:44,319 Epoch 2531: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 09:30:44,319 EPOCH 2532
2024-02-10 09:31:00,648 Epoch 2532: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 09:31:00,649 EPOCH 2533
2024-02-10 09:31:17,991 Epoch 2533: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-10 09:31:17,991 EPOCH 2534
2024-02-10 09:31:24,975 [Epoch: 2534 Step: 00022800] Batch Recognition Loss:   0.000273 => Gls Tokens per Sec:      550 || Batch Translation Loss:   0.027599 => Txt Tokens per Sec:     1568 || Lr: 0.000100
2024-02-10 09:31:34,275 Epoch 2534: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-10 09:31:34,276 EPOCH 2535
2024-02-10 09:31:50,309 Epoch 2535: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 09:31:50,310 EPOCH 2536
2024-02-10 09:32:06,433 Epoch 2536: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 09:32:06,433 EPOCH 2537
2024-02-10 09:32:22,495 Epoch 2537: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 09:32:22,496 EPOCH 2538
2024-02-10 09:32:38,711 Epoch 2538: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 09:32:38,712 EPOCH 2539
2024-02-10 09:32:54,889 Epoch 2539: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 09:32:54,889 EPOCH 2540
2024-02-10 09:33:10,832 Epoch 2540: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 09:33:10,832 EPOCH 2541
2024-02-10 09:33:27,110 Epoch 2541: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 09:33:27,111 EPOCH 2542
2024-02-10 09:33:43,301 Epoch 2542: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 09:33:43,301 EPOCH 2543
2024-02-10 09:33:59,644 Epoch 2543: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 09:33:59,644 EPOCH 2544
2024-02-10 09:34:15,505 Epoch 2544: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-10 09:34:15,505 EPOCH 2545
2024-02-10 09:34:29,035 [Epoch: 2545 Step: 00022900] Batch Recognition Loss:   0.003835 => Gls Tokens per Sec:      312 || Batch Translation Loss:   0.027333 => Txt Tokens per Sec:      967 || Lr: 0.000100
2024-02-10 09:34:31,280 Epoch 2545: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 09:34:31,281 EPOCH 2546
2024-02-10 09:34:47,587 Epoch 2546: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-10 09:34:47,588 EPOCH 2547
2024-02-10 09:35:03,689 Epoch 2547: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-10 09:35:03,690 EPOCH 2548
2024-02-10 09:35:20,047 Epoch 2548: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.53 
2024-02-10 09:35:20,048 EPOCH 2549
2024-02-10 09:35:36,015 Epoch 2549: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-10 09:35:36,015 EPOCH 2550
2024-02-10 09:35:51,959 Epoch 2550: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.42 
2024-02-10 09:35:51,960 EPOCH 2551
2024-02-10 09:36:07,888 Epoch 2551: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-10 09:36:07,888 EPOCH 2552
2024-02-10 09:36:23,968 Epoch 2552: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-10 09:36:23,969 EPOCH 2553
2024-02-10 09:36:40,110 Epoch 2553: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-10 09:36:40,111 EPOCH 2554
2024-02-10 09:36:56,171 Epoch 2554: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-10 09:36:56,172 EPOCH 2555
2024-02-10 09:37:12,206 Epoch 2555: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-10 09:37:12,206 EPOCH 2556
2024-02-10 09:37:18,642 [Epoch: 2556 Step: 00023000] Batch Recognition Loss:   0.000397 => Gls Tokens per Sec:      855 || Batch Translation Loss:   0.087294 => Txt Tokens per Sec:     2443 || Lr: 0.000100
2024-02-10 09:37:28,265 Epoch 2556: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.92 
2024-02-10 09:37:28,265 EPOCH 2557
2024-02-10 09:37:44,436 Epoch 2557: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.57 
2024-02-10 09:37:44,437 EPOCH 2558
2024-02-10 09:38:00,358 Epoch 2558: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.84 
2024-02-10 09:38:00,359 EPOCH 2559
2024-02-10 09:38:16,716 Epoch 2559: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.47 
2024-02-10 09:38:16,717 EPOCH 2560
2024-02-10 09:38:32,835 Epoch 2560: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.94 
2024-02-10 09:38:32,835 EPOCH 2561
2024-02-10 09:38:48,766 Epoch 2561: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.36 
2024-02-10 09:38:48,766 EPOCH 2562
2024-02-10 09:39:04,514 Epoch 2562: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.06 
2024-02-10 09:39:04,514 EPOCH 2563
2024-02-10 09:39:20,408 Epoch 2563: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.71 
2024-02-10 09:39:20,408 EPOCH 2564
2024-02-10 09:39:36,767 Epoch 2564: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.96 
2024-02-10 09:39:36,767 EPOCH 2565
2024-02-10 09:39:52,782 Epoch 2565: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.71 
2024-02-10 09:39:52,783 EPOCH 2566
2024-02-10 09:40:09,106 Epoch 2566: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.41 
2024-02-10 09:40:09,107 EPOCH 2567
2024-02-10 09:40:17,306 [Epoch: 2567 Step: 00023100] Batch Recognition Loss:   0.000724 => Gls Tokens per Sec:      937 || Batch Translation Loss:   0.033321 => Txt Tokens per Sec:     2573 || Lr: 0.000100
2024-02-10 09:40:25,220 Epoch 2567: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-10 09:40:25,220 EPOCH 2568
2024-02-10 09:40:41,258 Epoch 2568: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-10 09:40:41,259 EPOCH 2569
2024-02-10 09:40:57,375 Epoch 2569: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 09:40:57,376 EPOCH 2570
2024-02-10 09:41:13,682 Epoch 2570: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 09:41:13,682 EPOCH 2571
2024-02-10 09:41:29,581 Epoch 2571: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 09:41:29,582 EPOCH 2572
2024-02-10 09:41:45,408 Epoch 2572: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 09:41:45,409 EPOCH 2573
2024-02-10 09:42:01,819 Epoch 2573: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 09:42:01,819 EPOCH 2574
2024-02-10 09:42:18,099 Epoch 2574: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 09:42:18,100 EPOCH 2575
2024-02-10 09:42:34,395 Epoch 2575: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 09:42:34,396 EPOCH 2576
2024-02-10 09:42:50,589 Epoch 2576: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 09:42:50,590 EPOCH 2577
2024-02-10 09:43:06,608 Epoch 2577: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 09:43:06,608 EPOCH 2578
2024-02-10 09:43:21,697 [Epoch: 2578 Step: 00023200] Batch Recognition Loss:   0.000516 => Gls Tokens per Sec:      534 || Batch Translation Loss:   0.024421 => Txt Tokens per Sec:     1537 || Lr: 0.000100
2024-02-10 09:43:22,664 Epoch 2578: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 09:43:22,664 EPOCH 2579
2024-02-10 09:43:38,409 Epoch 2579: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 09:43:38,409 EPOCH 2580
2024-02-10 09:43:54,561 Epoch 2580: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 09:43:54,562 EPOCH 2581
2024-02-10 09:44:10,722 Epoch 2581: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 09:44:10,723 EPOCH 2582
2024-02-10 09:44:26,780 Epoch 2582: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 09:44:26,781 EPOCH 2583
2024-02-10 09:44:42,767 Epoch 2583: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 09:44:42,767 EPOCH 2584
2024-02-10 09:44:58,660 Epoch 2584: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 09:44:58,660 EPOCH 2585
2024-02-10 09:45:14,964 Epoch 2585: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 09:45:14,964 EPOCH 2586
2024-02-10 09:45:30,966 Epoch 2586: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 09:45:30,967 EPOCH 2587
2024-02-10 09:45:47,241 Epoch 2587: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 09:45:47,242 EPOCH 2588
2024-02-10 09:46:03,383 Epoch 2588: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 09:46:03,384 EPOCH 2589
2024-02-10 09:46:18,920 [Epoch: 2589 Step: 00023300] Batch Recognition Loss:   0.000952 => Gls Tokens per Sec:      601 || Batch Translation Loss:   0.018075 => Txt Tokens per Sec:     1664 || Lr: 0.000100
2024-02-10 09:46:19,395 Epoch 2589: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 09:46:19,395 EPOCH 2590
2024-02-10 09:46:35,158 Epoch 2590: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 09:46:35,158 EPOCH 2591
2024-02-10 09:46:51,490 Epoch 2591: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 09:46:51,490 EPOCH 2592
2024-02-10 09:47:07,710 Epoch 2592: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 09:47:07,710 EPOCH 2593
2024-02-10 09:47:23,964 Epoch 2593: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 09:47:23,964 EPOCH 2594
2024-02-10 09:47:40,351 Epoch 2594: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 09:47:40,351 EPOCH 2595
2024-02-10 09:47:56,253 Epoch 2595: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 09:47:56,254 EPOCH 2596
2024-02-10 09:48:12,027 Epoch 2596: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 09:48:12,028 EPOCH 2597
2024-02-10 09:48:28,009 Epoch 2597: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 09:48:28,009 EPOCH 2598
2024-02-10 09:48:44,242 Epoch 2598: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 09:48:44,243 EPOCH 2599
2024-02-10 09:49:00,021 Epoch 2599: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 09:49:00,021 EPOCH 2600
2024-02-10 09:49:15,869 [Epoch: 2600 Step: 00023400] Batch Recognition Loss:   0.000644 => Gls Tokens per Sec:      670 || Batch Translation Loss:   0.008138 => Txt Tokens per Sec:     1854 || Lr: 0.000100
2024-02-10 09:49:15,870 Epoch 2600: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 09:49:15,870 EPOCH 2601
2024-02-10 09:49:31,936 Epoch 2601: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 09:49:31,937 EPOCH 2602
2024-02-10 09:49:48,119 Epoch 2602: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 09:49:48,120 EPOCH 2603
2024-02-10 09:50:04,325 Epoch 2603: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 09:50:04,325 EPOCH 2604
2024-02-10 09:50:20,436 Epoch 2604: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 09:50:20,436 EPOCH 2605
2024-02-10 09:50:36,508 Epoch 2605: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 09:50:36,509 EPOCH 2606
2024-02-10 09:50:52,600 Epoch 2606: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 09:50:52,600 EPOCH 2607
2024-02-10 09:51:08,788 Epoch 2607: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 09:51:08,788 EPOCH 2608
2024-02-10 09:51:24,767 Epoch 2608: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 09:51:24,768 EPOCH 2609
2024-02-10 09:51:40,799 Epoch 2609: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 09:51:40,800 EPOCH 2610
2024-02-10 09:51:56,899 Epoch 2610: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 09:51:56,900 EPOCH 2611
2024-02-10 09:52:13,317 Epoch 2611: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 09:52:13,318 EPOCH 2612
2024-02-10 09:52:19,008 [Epoch: 2612 Step: 00023500] Batch Recognition Loss:   0.003028 => Gls Tokens per Sec:      225 || Batch Translation Loss:   0.029317 => Txt Tokens per Sec:      774 || Lr: 0.000100
2024-02-10 09:52:29,464 Epoch 2612: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 09:52:29,465 EPOCH 2613
2024-02-10 09:52:45,268 Epoch 2613: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 09:52:45,269 EPOCH 2614
2024-02-10 09:53:01,798 Epoch 2614: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 09:53:01,799 EPOCH 2615
2024-02-10 09:53:17,740 Epoch 2615: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 09:53:17,741 EPOCH 2616
2024-02-10 09:53:33,682 Epoch 2616: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 09:53:33,683 EPOCH 2617
2024-02-10 09:53:50,010 Epoch 2617: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 09:53:50,011 EPOCH 2618
2024-02-10 09:54:05,739 Epoch 2618: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 09:54:05,739 EPOCH 2619
2024-02-10 09:54:21,812 Epoch 2619: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 09:54:21,813 EPOCH 2620
2024-02-10 09:54:38,217 Epoch 2620: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 09:54:38,218 EPOCH 2621
2024-02-10 09:54:54,436 Epoch 2621: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 09:54:54,437 EPOCH 2622
2024-02-10 09:55:10,634 Epoch 2622: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 09:55:10,634 EPOCH 2623
2024-02-10 09:55:11,407 [Epoch: 2623 Step: 00023600] Batch Recognition Loss:   0.000382 => Gls Tokens per Sec:     3316 || Batch Translation Loss:   0.011849 => Txt Tokens per Sec:     8817 || Lr: 0.000100
2024-02-10 09:55:26,787 Epoch 2623: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 09:55:26,788 EPOCH 2624
2024-02-10 09:55:43,165 Epoch 2624: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 09:55:43,165 EPOCH 2625
2024-02-10 09:55:59,172 Epoch 2625: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 09:55:59,173 EPOCH 2626
2024-02-10 09:56:15,439 Epoch 2626: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 09:56:15,440 EPOCH 2627
2024-02-10 09:56:31,462 Epoch 2627: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 09:56:31,463 EPOCH 2628
2024-02-10 09:56:47,448 Epoch 2628: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-10 09:56:47,449 EPOCH 2629
2024-02-10 09:57:03,827 Epoch 2629: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.02 
2024-02-10 09:57:03,828 EPOCH 2630
2024-02-10 09:57:19,792 Epoch 2630: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.42 
2024-02-10 09:57:19,793 EPOCH 2631
2024-02-10 09:57:35,820 Epoch 2631: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.27 
2024-02-10 09:57:35,821 EPOCH 2632
2024-02-10 09:57:51,928 Epoch 2632: Total Training Recognition Loss 0.17  Total Training Translation Loss 2.28 
2024-02-10 09:57:51,929 EPOCH 2633
2024-02-10 09:58:08,198 Epoch 2633: Total Training Recognition Loss 0.45  Total Training Translation Loss 1.18 
2024-02-10 09:58:08,199 EPOCH 2634
2024-02-10 09:58:15,984 [Epoch: 2634 Step: 00023700] Batch Recognition Loss:   0.055434 => Gls Tokens per Sec:      378 || Batch Translation Loss:   0.107290 => Txt Tokens per Sec:      992 || Lr: 0.000100
2024-02-10 09:58:24,553 Epoch 2634: Total Training Recognition Loss 1.76  Total Training Translation Loss 0.87 
2024-02-10 09:58:24,554 EPOCH 2635
2024-02-10 09:58:40,730 Epoch 2635: Total Training Recognition Loss 0.48  Total Training Translation Loss 0.66 
2024-02-10 09:58:40,731 EPOCH 2636
2024-02-10 09:58:57,116 Epoch 2636: Total Training Recognition Loss 0.17  Total Training Translation Loss 0.46 
2024-02-10 09:58:57,117 EPOCH 2637
2024-02-10 09:59:13,022 Epoch 2637: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.40 
2024-02-10 09:59:13,023 EPOCH 2638
2024-02-10 09:59:28,774 Epoch 2638: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.35 
2024-02-10 09:59:28,775 EPOCH 2639
2024-02-10 09:59:44,828 Epoch 2639: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.36 
2024-02-10 09:59:44,829 EPOCH 2640
2024-02-10 10:00:01,141 Epoch 2640: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.30 
2024-02-10 10:00:01,142 EPOCH 2641
2024-02-10 10:00:17,070 Epoch 2641: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 10:00:17,071 EPOCH 2642
2024-02-10 10:00:33,113 Epoch 2642: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 10:00:33,113 EPOCH 2643
2024-02-10 10:00:49,511 Epoch 2643: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 10:00:49,511 EPOCH 2644
2024-02-10 10:01:05,645 Epoch 2644: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 10:01:05,645 EPOCH 2645
2024-02-10 10:01:07,519 [Epoch: 2645 Step: 00023800] Batch Recognition Loss:   0.000326 => Gls Tokens per Sec:     2734 || Batch Translation Loss:   0.015553 => Txt Tokens per Sec:     7114 || Lr: 0.000100
2024-02-10 10:01:22,217 Epoch 2645: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 10:01:22,218 EPOCH 2646
2024-02-10 10:01:38,183 Epoch 2646: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 10:01:38,184 EPOCH 2647
2024-02-10 10:01:54,226 Epoch 2647: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 10:01:54,227 EPOCH 2648
2024-02-10 10:02:10,364 Epoch 2648: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 10:02:10,364 EPOCH 2649
2024-02-10 10:02:26,573 Epoch 2649: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 10:02:26,574 EPOCH 2650
2024-02-10 10:02:42,535 Epoch 2650: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 10:02:42,536 EPOCH 2651
2024-02-10 10:02:58,615 Epoch 2651: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 10:02:58,616 EPOCH 2652
2024-02-10 10:03:14,822 Epoch 2652: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 10:03:14,822 EPOCH 2653
2024-02-10 10:03:31,192 Epoch 2653: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 10:03:31,193 EPOCH 2654
2024-02-10 10:03:47,311 Epoch 2654: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 10:03:47,311 EPOCH 2655
2024-02-10 10:04:03,224 Epoch 2655: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 10:04:03,225 EPOCH 2656
2024-02-10 10:04:13,806 [Epoch: 2656 Step: 00023900] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:      605 || Batch Translation Loss:   0.018763 => Txt Tokens per Sec:     1788 || Lr: 0.000100
2024-02-10 10:04:19,228 Epoch 2656: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 10:04:19,229 EPOCH 2657
2024-02-10 10:04:35,387 Epoch 2657: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 10:04:35,388 EPOCH 2658
2024-02-10 10:04:51,555 Epoch 2658: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 10:04:51,556 EPOCH 2659
2024-02-10 10:05:07,777 Epoch 2659: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 10:05:07,778 EPOCH 2660
2024-02-10 10:05:23,733 Epoch 2660: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 10:05:23,734 EPOCH 2661
2024-02-10 10:05:39,524 Epoch 2661: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 10:05:39,525 EPOCH 2662
2024-02-10 10:05:55,429 Epoch 2662: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 10:05:55,430 EPOCH 2663
2024-02-10 10:06:11,590 Epoch 2663: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 10:06:11,591 EPOCH 2664
2024-02-10 10:06:28,022 Epoch 2664: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:06:28,023 EPOCH 2665
2024-02-10 10:06:44,323 Epoch 2665: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:06:44,323 EPOCH 2666
2024-02-10 10:07:00,503 Epoch 2666: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:07:00,503 EPOCH 2667
2024-02-10 10:07:12,319 [Epoch: 2667 Step: 00024000] Batch Recognition Loss:   0.001381 => Gls Tokens per Sec:      574 || Batch Translation Loss:   0.022721 => Txt Tokens per Sec:     1647 || Lr: 0.000100
2024-02-10 10:08:24,114 Validation result at epoch 2667, step    24000: duration: 71.7948s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.44572	Translation Loss: 94133.10938	PPL: 12112.92188
	Eval Metric: BLEU
	WER 3.60	(DEL: 0.00,	INS: 0.00,	SUB: 3.60)
	BLEU-4 0.75	(BLEU-1: 10.74,	BLEU-2: 3.36,	BLEU-3: 1.42,	BLEU-4: 0.75)
	CHRF 17.23	ROUGE 9.04
2024-02-10 10:08:24,117 Logging Recognition and Translation Outputs
2024-02-10 10:08:24,117 ========================================================================================================================
2024-02-10 10:08:24,117 Logging Sequence: 171_2.00
2024-02-10 10:08:24,118 	Gloss Reference :	A B+C+D+E  
2024-02-10 10:08:24,118 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-10 10:08:24,119 	Gloss Alignment :	  S        
2024-02-10 10:08:24,119 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 10:08:24,121 	Text Reference  :	as you might all know that the ***** ipl is  about to   end      the finals are on  28th  may
2024-02-10 10:08:24,121 	Text Hypothesis :	** *** on    16  july 2021 the aiscd won the first time spending the ****** *** t20 world cup
2024-02-10 10:08:24,121 	Text Alignment  :	D  D   S     S   S    S        I     S   S   S     S    S            D      D   S   S     S  
2024-02-10 10:08:24,122 ========================================================================================================================
2024-02-10 10:08:24,122 Logging Sequence: 119_33.00
2024-02-10 10:08:24,122 	Gloss Reference :	A B+C+D+E
2024-02-10 10:08:24,122 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 10:08:24,122 	Gloss Alignment :	         
2024-02-10 10:08:24,122 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 10:08:24,123 	Text Reference  :	***** he  wanted  to gift *** ****** 35  people wow wonderful
2024-02-10 10:08:24,123 	Text Hypothesis :	messi got married to gift the team's and it     was injured  
2024-02-10 10:08:24,124 	Text Alignment  :	I     S   S               I   I      S   S      S   S        
2024-02-10 10:08:24,124 ========================================================================================================================
2024-02-10 10:08:24,124 Logging Sequence: 158_131.00
2024-02-10 10:08:24,124 	Gloss Reference :	A B+C+D+E
2024-02-10 10:08:24,124 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 10:08:24,124 	Gloss Alignment :	         
2024-02-10 10:08:24,125 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 10:08:24,126 	Text Reference  :	on 10th april 2023 there was   a   match   between rcb and      lsg     in      bengaluru
2024-02-10 10:08:24,126 	Text Hypothesis :	** you  know  that virat kohli and gambhir had     an  argument women's cricket team     
2024-02-10 10:08:24,126 	Text Alignment  :	D  S    S     S    S     S     S   S       S       S   S        S       S       S        
2024-02-10 10:08:24,126 ========================================================================================================================
2024-02-10 10:08:24,126 Logging Sequence: 164_412.00
2024-02-10 10:08:24,127 	Gloss Reference :	A B+C+D+E
2024-02-10 10:08:24,127 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 10:08:24,127 	Gloss Alignment :	         
2024-02-10 10:08:24,127 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 10:08:24,128 	Text Reference  :	if you divide these two figures you will be shocked to know     that       each ball's worth  is rs  50         lakhs  
2024-02-10 10:08:24,128 	Text Hypothesis :	** *** ****** ***** *** ******* *** **** ** ******* ** reliance industries owns 51     shares of the viacom18's company
2024-02-10 10:08:24,128 	Text Alignment  :	D  D   D      D     D   D       D   D    D  D       D  S        S          S    S      S      S  S   S          S      
2024-02-10 10:08:24,129 ========================================================================================================================
2024-02-10 10:08:24,129 Logging Sequence: 159_112.00
2024-02-10 10:08:24,129 	Gloss Reference :	A B+C+D+E
2024-02-10 10:08:24,129 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 10:08:24,129 	Gloss Alignment :	         
2024-02-10 10:08:24,129 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 10:08:24,131 	Text Reference  :	******** kohli had revealed that before the tournament he   did  not    touch his bat for a  month   yes 1   month   
2024-02-10 10:08:24,131 	Text Hypothesis :	mohammed shami has said     that ****** the ********** pani puri seller on    his *** *** 50 million on  his athletes
2024-02-10 10:08:24,131 	Text Alignment  :	I        S     S   S             D          D          S    S    S      S         D   D   S  S       S   S   S       
2024-02-10 10:08:24,132 ========================================================================================================================
2024-02-10 10:08:28,461 Epoch 2667: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:08:28,462 EPOCH 2668
2024-02-10 10:08:44,840 Epoch 2668: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 10:08:44,841 EPOCH 2669
2024-02-10 10:09:00,786 Epoch 2669: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 10:09:00,787 EPOCH 2670
2024-02-10 10:09:17,001 Epoch 2670: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 10:09:17,001 EPOCH 2671
2024-02-10 10:09:33,031 Epoch 2671: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 10:09:33,032 EPOCH 2672
2024-02-10 10:09:48,942 Epoch 2672: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 10:09:48,942 EPOCH 2673
2024-02-10 10:10:05,198 Epoch 2673: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 10:10:05,199 EPOCH 2674
2024-02-10 10:10:21,299 Epoch 2674: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:10:21,300 EPOCH 2675
2024-02-10 10:10:37,226 Epoch 2675: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:10:37,227 EPOCH 2676
2024-02-10 10:10:53,319 Epoch 2676: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 10:10:53,320 EPOCH 2677
2024-02-10 10:11:09,553 Epoch 2677: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:11:09,554 EPOCH 2678
2024-02-10 10:11:20,765 [Epoch: 2678 Step: 00024100] Batch Recognition Loss:   0.000207 => Gls Tokens per Sec:      799 || Batch Translation Loss:   0.014056 => Txt Tokens per Sec:     2186 || Lr: 0.000100
2024-02-10 10:11:25,580 Epoch 2678: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 10:11:25,581 EPOCH 2679
2024-02-10 10:11:41,840 Epoch 2679: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 10:11:41,841 EPOCH 2680
2024-02-10 10:11:57,694 Epoch 2680: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:11:57,694 EPOCH 2681
2024-02-10 10:12:13,710 Epoch 2681: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:12:13,710 EPOCH 2682
2024-02-10 10:12:29,799 Epoch 2682: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:12:29,799 EPOCH 2683
2024-02-10 10:12:45,882 Epoch 2683: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:12:45,883 EPOCH 2684
2024-02-10 10:13:02,095 Epoch 2684: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:13:02,095 EPOCH 2685
2024-02-10 10:13:18,243 Epoch 2685: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 10:13:18,244 EPOCH 2686
2024-02-10 10:13:34,410 Epoch 2686: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:13:34,411 EPOCH 2687
2024-02-10 10:13:50,628 Epoch 2687: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:13:50,628 EPOCH 2688
2024-02-10 10:14:06,828 Epoch 2688: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:14:06,828 EPOCH 2689
2024-02-10 10:14:16,863 [Epoch: 2689 Step: 00024200] Batch Recognition Loss:   0.000242 => Gls Tokens per Sec:      931 || Batch Translation Loss:   0.010855 => Txt Tokens per Sec:     2488 || Lr: 0.000100
2024-02-10 10:14:22,602 Epoch 2689: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 10:14:22,603 EPOCH 2690
2024-02-10 10:14:38,289 Epoch 2690: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 10:14:38,290 EPOCH 2691
2024-02-10 10:14:54,359 Epoch 2691: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:14:54,360 EPOCH 2692
2024-02-10 10:15:10,494 Epoch 2692: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:15:10,495 EPOCH 2693
2024-02-10 10:15:26,743 Epoch 2693: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 10:15:26,744 EPOCH 2694
2024-02-10 10:15:42,766 Epoch 2694: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:15:42,767 EPOCH 2695
2024-02-10 10:15:58,824 Epoch 2695: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 10:15:58,825 EPOCH 2696
2024-02-10 10:16:14,797 Epoch 2696: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:16:14,798 EPOCH 2697
2024-02-10 10:16:30,968 Epoch 2697: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 10:16:30,969 EPOCH 2698
2024-02-10 10:16:47,283 Epoch 2698: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:16:47,283 EPOCH 2699
2024-02-10 10:17:03,595 Epoch 2699: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 10:17:03,596 EPOCH 2700
2024-02-10 10:17:19,786 [Epoch: 2700 Step: 00024300] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:      656 || Batch Translation Loss:   0.006417 => Txt Tokens per Sec:     1815 || Lr: 0.000100
2024-02-10 10:17:19,787 Epoch 2700: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:17:19,787 EPOCH 2701
2024-02-10 10:17:36,036 Epoch 2701: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 10:17:36,036 EPOCH 2702
2024-02-10 10:17:51,911 Epoch 2702: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:17:51,912 EPOCH 2703
2024-02-10 10:18:07,796 Epoch 2703: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:18:07,796 EPOCH 2704
2024-02-10 10:18:24,136 Epoch 2704: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:18:24,136 EPOCH 2705
2024-02-10 10:18:40,144 Epoch 2705: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:18:40,144 EPOCH 2706
2024-02-10 10:18:56,054 Epoch 2706: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:18:56,055 EPOCH 2707
2024-02-10 10:19:11,823 Epoch 2707: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:19:11,824 EPOCH 2708
2024-02-10 10:19:28,195 Epoch 2708: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:19:28,196 EPOCH 2709
2024-02-10 10:19:44,370 Epoch 2709: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:19:44,370 EPOCH 2710
2024-02-10 10:20:00,484 Epoch 2710: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:20:00,485 EPOCH 2711
2024-02-10 10:20:16,023 Epoch 2711: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:20:16,023 EPOCH 2712
2024-02-10 10:20:16,411 [Epoch: 2712 Step: 00024400] Batch Recognition Loss:   0.000437 => Gls Tokens per Sec:     3307 || Batch Translation Loss:   0.014037 => Txt Tokens per Sec:     9085 || Lr: 0.000100
2024-02-10 10:20:31,860 Epoch 2712: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:20:31,860 EPOCH 2713
2024-02-10 10:20:48,251 Epoch 2713: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:20:48,252 EPOCH 2714
2024-02-10 10:21:04,153 Epoch 2714: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:21:04,153 EPOCH 2715
2024-02-10 10:21:20,393 Epoch 2715: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:21:20,393 EPOCH 2716
2024-02-10 10:21:36,585 Epoch 2716: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:21:36,585 EPOCH 2717
2024-02-10 10:21:52,555 Epoch 2717: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:21:52,556 EPOCH 2718
2024-02-10 10:22:08,563 Epoch 2718: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:22:08,564 EPOCH 2719
2024-02-10 10:22:24,592 Epoch 2719: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:22:24,592 EPOCH 2720
2024-02-10 10:22:40,667 Epoch 2720: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:22:40,667 EPOCH 2721
2024-02-10 10:22:56,580 Epoch 2721: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:22:56,581 EPOCH 2722
2024-02-10 10:23:12,575 Epoch 2722: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:23:12,575 EPOCH 2723
2024-02-10 10:23:16,300 [Epoch: 2723 Step: 00024500] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:      687 || Batch Translation Loss:   0.016640 => Txt Tokens per Sec:     2058 || Lr: 0.000100
2024-02-10 10:23:28,631 Epoch 2723: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:23:28,632 EPOCH 2724
2024-02-10 10:23:44,683 Epoch 2724: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:23:44,683 EPOCH 2725
2024-02-10 10:24:00,770 Epoch 2725: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:24:00,771 EPOCH 2726
2024-02-10 10:24:16,881 Epoch 2726: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:24:16,881 EPOCH 2727
2024-02-10 10:24:33,203 Epoch 2727: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:24:33,203 EPOCH 2728
2024-02-10 10:24:49,388 Epoch 2728: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:24:49,389 EPOCH 2729
2024-02-10 10:25:05,478 Epoch 2729: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:25:05,479 EPOCH 2730
2024-02-10 10:25:21,471 Epoch 2730: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:25:21,472 EPOCH 2731
2024-02-10 10:25:37,555 Epoch 2731: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 10:25:37,555 EPOCH 2732
2024-02-10 10:25:53,481 Epoch 2732: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 10:25:53,482 EPOCH 2733
2024-02-10 10:26:09,941 Epoch 2733: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 10:26:09,941 EPOCH 2734
2024-02-10 10:26:17,855 [Epoch: 2734 Step: 00024600] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:      372 || Batch Translation Loss:   0.013230 => Txt Tokens per Sec:     1098 || Lr: 0.000100
2024-02-10 10:26:25,856 Epoch 2734: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 10:26:25,856 EPOCH 2735
2024-02-10 10:26:42,327 Epoch 2735: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 10:26:42,327 EPOCH 2736
2024-02-10 10:26:58,413 Epoch 2736: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 10:26:58,414 EPOCH 2737
2024-02-10 10:27:14,588 Epoch 2737: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 10:27:14,588 EPOCH 2738
2024-02-10 10:27:30,425 Epoch 2738: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 10:27:30,426 EPOCH 2739
2024-02-10 10:27:46,537 Epoch 2739: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 10:27:46,538 EPOCH 2740
2024-02-10 10:28:02,521 Epoch 2740: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 10:28:02,521 EPOCH 2741
2024-02-10 10:28:18,829 Epoch 2741: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 10:28:18,830 EPOCH 2742
2024-02-10 10:28:34,998 Epoch 2742: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 10:28:34,999 EPOCH 2743
2024-02-10 10:28:51,126 Epoch 2743: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 10:28:51,127 EPOCH 2744
2024-02-10 10:29:07,377 Epoch 2744: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 10:29:07,378 EPOCH 2745
2024-02-10 10:29:17,217 [Epoch: 2745 Step: 00024700] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:      520 || Batch Translation Loss:   0.017216 => Txt Tokens per Sec:     1489 || Lr: 0.000100
2024-02-10 10:29:23,456 Epoch 2745: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 10:29:23,457 EPOCH 2746
2024-02-10 10:29:39,767 Epoch 2746: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 10:29:39,767 EPOCH 2747
2024-02-10 10:29:55,933 Epoch 2747: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-10 10:29:55,934 EPOCH 2748
2024-02-10 10:30:12,093 Epoch 2748: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.64 
2024-02-10 10:30:12,093 EPOCH 2749
2024-02-10 10:30:28,174 Epoch 2749: Total Training Recognition Loss 0.00  Total Training Translation Loss 5.13 
2024-02-10 10:30:28,175 EPOCH 2750
2024-02-10 10:30:44,428 Epoch 2750: Total Training Recognition Loss 0.09  Total Training Translation Loss 10.15 
2024-02-10 10:30:44,428 EPOCH 2751
2024-02-10 10:31:00,561 Epoch 2751: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.59 
2024-02-10 10:31:00,562 EPOCH 2752
2024-02-10 10:31:16,752 Epoch 2752: Total Training Recognition Loss 0.03  Total Training Translation Loss 5.39 
2024-02-10 10:31:16,753 EPOCH 2753
2024-02-10 10:31:33,630 Epoch 2753: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.58 
2024-02-10 10:31:33,630 EPOCH 2754
2024-02-10 10:31:49,891 Epoch 2754: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.55 
2024-02-10 10:31:49,891 EPOCH 2755
2024-02-10 10:32:05,878 Epoch 2755: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.88 
2024-02-10 10:32:05,879 EPOCH 2756
2024-02-10 10:32:16,999 [Epoch: 2756 Step: 00024800] Batch Recognition Loss:   0.001189 => Gls Tokens per Sec:      495 || Batch Translation Loss:   0.031921 => Txt Tokens per Sec:     1274 || Lr: 0.000100
2024-02-10 10:32:21,926 Epoch 2756: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.58 
2024-02-10 10:32:21,926 EPOCH 2757
2024-02-10 10:32:37,611 Epoch 2757: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-10 10:32:37,612 EPOCH 2758
2024-02-10 10:32:53,743 Epoch 2758: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-10 10:32:53,744 EPOCH 2759
2024-02-10 10:33:10,083 Epoch 2759: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 10:33:10,084 EPOCH 2760
2024-02-10 10:33:25,933 Epoch 2760: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 10:33:25,934 EPOCH 2761
2024-02-10 10:33:41,801 Epoch 2761: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 10:33:41,802 EPOCH 2762
2024-02-10 10:33:57,730 Epoch 2762: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 10:33:57,731 EPOCH 2763
2024-02-10 10:34:13,943 Epoch 2763: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 10:34:13,943 EPOCH 2764
2024-02-10 10:34:29,725 Epoch 2764: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 10:34:29,725 EPOCH 2765
2024-02-10 10:34:45,801 Epoch 2765: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 10:34:45,801 EPOCH 2766
2024-02-10 10:35:02,000 Epoch 2766: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 10:35:02,000 EPOCH 2767
2024-02-10 10:35:11,580 [Epoch: 2767 Step: 00024900] Batch Recognition Loss:   0.000351 => Gls Tokens per Sec:      708 || Batch Translation Loss:   0.020123 => Txt Tokens per Sec:     2028 || Lr: 0.000100
2024-02-10 10:35:18,236 Epoch 2767: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 10:35:18,237 EPOCH 2768
2024-02-10 10:35:34,427 Epoch 2768: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 10:35:34,427 EPOCH 2769
2024-02-10 10:35:50,584 Epoch 2769: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 10:35:50,585 EPOCH 2770
2024-02-10 10:36:06,531 Epoch 2770: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 10:36:06,532 EPOCH 2771
2024-02-10 10:36:22,679 Epoch 2771: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 10:36:22,679 EPOCH 2772
2024-02-10 10:36:38,900 Epoch 2772: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 10:36:38,900 EPOCH 2773
2024-02-10 10:36:55,170 Epoch 2773: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 10:36:55,171 EPOCH 2774
2024-02-10 10:37:11,253 Epoch 2774: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 10:37:11,253 EPOCH 2775
2024-02-10 10:37:27,568 Epoch 2775: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 10:37:27,569 EPOCH 2776
2024-02-10 10:37:43,806 Epoch 2776: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 10:37:43,806 EPOCH 2777
2024-02-10 10:37:59,823 Epoch 2777: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 10:37:59,823 EPOCH 2778
2024-02-10 10:38:11,829 [Epoch: 2778 Step: 00025000] Batch Recognition Loss:   0.000288 => Gls Tokens per Sec:      671 || Batch Translation Loss:   0.017448 => Txt Tokens per Sec:     1784 || Lr: 0.000100
2024-02-10 10:38:15,689 Epoch 2778: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 10:38:15,690 EPOCH 2779
2024-02-10 10:38:32,113 Epoch 2779: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 10:38:32,114 EPOCH 2780
2024-02-10 10:38:47,996 Epoch 2780: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 10:38:47,997 EPOCH 2781
2024-02-10 10:39:03,928 Epoch 2781: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 10:39:03,929 EPOCH 2782
2024-02-10 10:39:20,043 Epoch 2782: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 10:39:20,044 EPOCH 2783
2024-02-10 10:39:36,280 Epoch 2783: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 10:39:36,281 EPOCH 2784
2024-02-10 10:39:52,175 Epoch 2784: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 10:39:52,176 EPOCH 2785
2024-02-10 10:40:08,166 Epoch 2785: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 10:40:08,166 EPOCH 2786
2024-02-10 10:40:24,760 Epoch 2786: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 10:40:24,761 EPOCH 2787
2024-02-10 10:40:40,787 Epoch 2787: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 10:40:40,788 EPOCH 2788
2024-02-10 10:40:56,827 Epoch 2788: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 10:40:56,828 EPOCH 2789
2024-02-10 10:41:12,576 [Epoch: 2789 Step: 00025100] Batch Recognition Loss:   0.000282 => Gls Tokens per Sec:      593 || Batch Translation Loss:   0.014495 => Txt Tokens per Sec:     1684 || Lr: 0.000100
2024-02-10 10:41:12,960 Epoch 2789: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 10:41:12,960 EPOCH 2790
2024-02-10 10:41:29,166 Epoch 2790: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 10:41:29,167 EPOCH 2791
2024-02-10 10:41:45,179 Epoch 2791: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 10:41:45,180 EPOCH 2792
2024-02-10 10:42:01,354 Epoch 2792: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 10:42:01,355 EPOCH 2793
2024-02-10 10:42:17,863 Epoch 2793: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 10:42:17,863 EPOCH 2794
2024-02-10 10:42:33,894 Epoch 2794: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 10:42:33,895 EPOCH 2795
2024-02-10 10:42:49,661 Epoch 2795: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 10:42:49,662 EPOCH 2796
2024-02-10 10:43:06,016 Epoch 2796: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 10:43:06,016 EPOCH 2797
2024-02-10 10:43:21,800 Epoch 2797: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 10:43:21,801 EPOCH 2798
2024-02-10 10:43:38,099 Epoch 2798: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 10:43:38,100 EPOCH 2799
2024-02-10 10:43:54,037 Epoch 2799: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 10:43:54,038 EPOCH 2800
2024-02-10 10:44:10,235 [Epoch: 2800 Step: 00025200] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:      656 || Batch Translation Loss:   0.008946 => Txt Tokens per Sec:     1814 || Lr: 0.000100
2024-02-10 10:44:10,236 Epoch 2800: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 10:44:10,236 EPOCH 2801
2024-02-10 10:44:26,381 Epoch 2801: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:44:26,382 EPOCH 2802
2024-02-10 10:44:42,532 Epoch 2802: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 10:44:42,532 EPOCH 2803
2024-02-10 10:44:58,633 Epoch 2803: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 10:44:58,634 EPOCH 2804
2024-02-10 10:45:14,714 Epoch 2804: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 10:45:14,714 EPOCH 2805
2024-02-10 10:45:30,775 Epoch 2805: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 10:45:30,775 EPOCH 2806
2024-02-10 10:45:46,836 Epoch 2806: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 10:45:46,836 EPOCH 2807
2024-02-10 10:46:03,168 Epoch 2807: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 10:46:03,168 EPOCH 2808
2024-02-10 10:46:18,999 Epoch 2808: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:46:18,999 EPOCH 2809
2024-02-10 10:46:35,127 Epoch 2809: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 10:46:35,128 EPOCH 2810
2024-02-10 10:46:51,145 Epoch 2810: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 10:46:51,145 EPOCH 2811
2024-02-10 10:47:07,386 Epoch 2811: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:47:07,387 EPOCH 2812
2024-02-10 10:47:11,700 [Epoch: 2812 Step: 00025300] Batch Recognition Loss:   0.000820 => Gls Tokens per Sec:       88 || Batch Translation Loss:   0.007111 => Txt Tokens per Sec:      315 || Lr: 0.000100
2024-02-10 10:47:23,502 Epoch 2812: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:47:23,503 EPOCH 2813
2024-02-10 10:47:39,689 Epoch 2813: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:47:39,689 EPOCH 2814
2024-02-10 10:47:55,834 Epoch 2814: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 10:47:55,835 EPOCH 2815
2024-02-10 10:48:11,927 Epoch 2815: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:48:11,927 EPOCH 2816
2024-02-10 10:48:27,963 Epoch 2816: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 10:48:27,963 EPOCH 2817
2024-02-10 10:48:44,289 Epoch 2817: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:48:44,289 EPOCH 2818
2024-02-10 10:49:00,458 Epoch 2818: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:49:00,458 EPOCH 2819
2024-02-10 10:49:16,257 Epoch 2819: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:49:16,258 EPOCH 2820
2024-02-10 10:49:31,887 Epoch 2820: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 10:49:31,888 EPOCH 2821
2024-02-10 10:49:48,048 Epoch 2821: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 10:49:48,049 EPOCH 2822
2024-02-10 10:50:03,696 Epoch 2822: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 10:50:03,696 EPOCH 2823
2024-02-10 10:50:04,693 [Epoch: 2823 Step: 00025400] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     2573 || Batch Translation Loss:   0.012117 => Txt Tokens per Sec:     7139 || Lr: 0.000100
2024-02-10 10:50:19,668 Epoch 2823: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:50:19,669 EPOCH 2824
2024-02-10 10:50:35,790 Epoch 2824: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 10:50:35,790 EPOCH 2825
2024-02-10 10:50:52,251 Epoch 2825: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 10:50:52,251 EPOCH 2826
2024-02-10 10:51:08,488 Epoch 2826: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 10:51:08,489 EPOCH 2827
2024-02-10 10:51:24,425 Epoch 2827: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 10:51:24,426 EPOCH 2828
2024-02-10 10:51:40,553 Epoch 2828: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 10:51:40,554 EPOCH 2829
2024-02-10 10:51:56,343 Epoch 2829: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:51:56,343 EPOCH 2830
2024-02-10 10:52:12,048 Epoch 2830: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:52:12,048 EPOCH 2831
2024-02-10 10:52:28,016 Epoch 2831: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:52:28,017 EPOCH 2832
2024-02-10 10:52:44,167 Epoch 2832: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:52:44,167 EPOCH 2833
2024-02-10 10:53:00,574 Epoch 2833: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:53:00,574 EPOCH 2834
2024-02-10 10:53:07,505 [Epoch: 2834 Step: 00025500] Batch Recognition Loss:   0.000209 => Gls Tokens per Sec:      554 || Batch Translation Loss:   0.022052 => Txt Tokens per Sec:     1700 || Lr: 0.000100
2024-02-10 10:53:16,557 Epoch 2834: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:53:16,557 EPOCH 2835
2024-02-10 10:53:32,501 Epoch 2835: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:53:32,501 EPOCH 2836
2024-02-10 10:53:48,575 Epoch 2836: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:53:48,576 EPOCH 2837
2024-02-10 10:54:04,541 Epoch 2837: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:54:04,542 EPOCH 2838
2024-02-10 10:54:20,837 Epoch 2838: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:54:20,838 EPOCH 2839
2024-02-10 10:54:37,080 Epoch 2839: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:54:37,080 EPOCH 2840
2024-02-10 10:54:52,792 Epoch 2840: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:54:52,792 EPOCH 2841
2024-02-10 10:55:08,891 Epoch 2841: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:55:08,892 EPOCH 2842
2024-02-10 10:55:25,030 Epoch 2842: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 10:55:25,031 EPOCH 2843
2024-02-10 10:55:41,621 Epoch 2843: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:55:41,621 EPOCH 2844
2024-02-10 10:55:57,810 Epoch 2844: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 10:55:57,810 EPOCH 2845
2024-02-10 10:56:06,255 [Epoch: 2845 Step: 00025600] Batch Recognition Loss:   0.000191 => Gls Tokens per Sec:      500 || Batch Translation Loss:   0.018450 => Txt Tokens per Sec:     1448 || Lr: 0.000100
2024-02-10 10:56:13,999 Epoch 2845: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:56:14,000 EPOCH 2846
2024-02-10 10:56:30,129 Epoch 2846: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:56:30,130 EPOCH 2847
2024-02-10 10:56:46,203 Epoch 2847: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:56:46,204 EPOCH 2848
2024-02-10 10:57:02,304 Epoch 2848: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:57:02,305 EPOCH 2849
2024-02-10 10:57:18,446 Epoch 2849: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:57:18,447 EPOCH 2850
2024-02-10 10:57:34,304 Epoch 2850: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:57:34,305 EPOCH 2851
2024-02-10 10:57:50,833 Epoch 2851: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:57:50,834 EPOCH 2852
2024-02-10 10:58:06,932 Epoch 2852: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:58:06,932 EPOCH 2853
2024-02-10 10:58:23,113 Epoch 2853: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:58:23,113 EPOCH 2854
2024-02-10 10:58:39,260 Epoch 2854: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:58:39,261 EPOCH 2855
2024-02-10 10:58:55,252 Epoch 2855: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 10:58:55,252 EPOCH 2856
2024-02-10 10:59:03,982 [Epoch: 2856 Step: 00025700] Batch Recognition Loss:   0.000207 => Gls Tokens per Sec:      630 || Batch Translation Loss:   0.017868 => Txt Tokens per Sec:     1689 || Lr: 0.000100
2024-02-10 10:59:11,365 Epoch 2856: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:59:11,366 EPOCH 2857
2024-02-10 10:59:27,746 Epoch 2857: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:59:27,747 EPOCH 2858
2024-02-10 10:59:43,683 Epoch 2858: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:59:43,683 EPOCH 2859
2024-02-10 10:59:59,920 Epoch 2859: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 10:59:59,921 EPOCH 2860
2024-02-10 11:00:16,178 Epoch 2860: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 11:00:16,178 EPOCH 2861
2024-02-10 11:00:32,481 Epoch 2861: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 11:00:32,482 EPOCH 2862
2024-02-10 11:00:48,085 Epoch 2862: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 11:00:48,086 EPOCH 2863
2024-02-10 11:01:04,091 Epoch 2863: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 11:01:04,091 EPOCH 2864
2024-02-10 11:01:20,412 Epoch 2864: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 11:01:20,413 EPOCH 2865
2024-02-10 11:01:36,738 Epoch 2865: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 11:01:36,738 EPOCH 2866
2024-02-10 11:01:52,929 Epoch 2866: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 11:01:52,930 EPOCH 2867
2024-02-10 11:02:01,126 [Epoch: 2867 Step: 00025800] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:      937 || Batch Translation Loss:   0.017126 => Txt Tokens per Sec:     2639 || Lr: 0.000100
2024-02-10 11:02:09,007 Epoch 2867: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 11:02:09,008 EPOCH 2868
2024-02-10 11:02:25,170 Epoch 2868: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 11:02:25,171 EPOCH 2869
2024-02-10 11:02:41,690 Epoch 2869: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 11:02:41,691 EPOCH 2870
2024-02-10 11:02:57,641 Epoch 2870: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-10 11:02:57,641 EPOCH 2871
2024-02-10 11:03:13,617 Epoch 2871: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 11:03:13,618 EPOCH 2872
2024-02-10 11:03:30,003 Epoch 2872: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 11:03:30,003 EPOCH 2873
2024-02-10 11:03:46,004 Epoch 2873: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 11:03:46,005 EPOCH 2874
2024-02-10 11:04:01,844 Epoch 2874: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-10 11:04:01,844 EPOCH 2875
2024-02-10 11:04:18,182 Epoch 2875: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-10 11:04:18,183 EPOCH 2876
2024-02-10 11:04:34,327 Epoch 2876: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-10 11:04:34,328 EPOCH 2877
2024-02-10 11:04:50,296 Epoch 2877: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-10 11:04:50,297 EPOCH 2878
2024-02-10 11:04:59,811 [Epoch: 2878 Step: 00025900] Batch Recognition Loss:   0.002454 => Gls Tokens per Sec:      847 || Batch Translation Loss:   0.134756 => Txt Tokens per Sec:     2219 || Lr: 0.000100
2024-02-10 11:05:06,200 Epoch 2878: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.59 
2024-02-10 11:05:06,200 EPOCH 2879
2024-02-10 11:05:22,523 Epoch 2879: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.60 
2024-02-10 11:05:22,524 EPOCH 2880
2024-02-10 11:05:38,672 Epoch 2880: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.33 
2024-02-10 11:05:38,673 EPOCH 2881
2024-02-10 11:05:54,927 Epoch 2881: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.53 
2024-02-10 11:05:54,928 EPOCH 2882
2024-02-10 11:06:11,177 Epoch 2882: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.68 
2024-02-10 11:06:11,177 EPOCH 2883
2024-02-10 11:06:27,078 Epoch 2883: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.87 
2024-02-10 11:06:27,078 EPOCH 2884
2024-02-10 11:06:43,193 Epoch 2884: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.34 
2024-02-10 11:06:43,194 EPOCH 2885
2024-02-10 11:06:59,337 Epoch 2885: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.85 
2024-02-10 11:06:59,338 EPOCH 2886
2024-02-10 11:07:15,419 Epoch 2886: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-10 11:07:15,420 EPOCH 2887
2024-02-10 11:07:31,486 Epoch 2887: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-10 11:07:31,486 EPOCH 2888
2024-02-10 11:07:48,070 Epoch 2888: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 11:07:48,071 EPOCH 2889
2024-02-10 11:08:03,368 [Epoch: 2889 Step: 00026000] Batch Recognition Loss:   0.000641 => Gls Tokens per Sec:      611 || Batch Translation Loss:   0.032848 => Txt Tokens per Sec:     1665 || Lr: 0.000100
2024-02-10 11:09:15,137 Validation result at epoch 2889, step    26000: duration: 71.7668s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.33895	Translation Loss: 93180.61719	PPL: 11013.67969
	Eval Metric: BLEU
	WER 3.88	(DEL: 0.00,	INS: 0.00,	SUB: 3.88)
	BLEU-4 0.53	(BLEU-1: 11.02,	BLEU-2: 3.40,	BLEU-3: 1.24,	BLEU-4: 0.53)
	CHRF 16.98	ROUGE 9.17
2024-02-10 11:09:15,139 Logging Recognition and Translation Outputs
2024-02-10 11:09:15,139 ========================================================================================================================
2024-02-10 11:09:15,139 Logging Sequence: 166_243.00
2024-02-10 11:09:15,139 	Gloss Reference :	A B+C+D+E
2024-02-10 11:09:15,140 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 11:09:15,140 	Gloss Alignment :	         
2024-02-10 11:09:15,140 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 11:09:15,141 	Text Reference  :	*** ***** ********* *** ***** *** ** **** icc   worked with  members boards like bcci  pcb     cricket australia etc
2024-02-10 11:09:15,142 	Text Hypothesis :	the board organised the world cup rs 2000 crore the    board of      the    2    teams playing for     the       mlc
2024-02-10 11:09:15,142 	Text Alignment  :	I   I     I         I   I     I   I  I    S     S      S     S       S      S    S     S       S       S         S  
2024-02-10 11:09:15,142 ========================================================================================================================
2024-02-10 11:09:15,142 Logging Sequence: 59_152.00
2024-02-10 11:09:15,142 	Gloss Reference :	A B+C+D+E
2024-02-10 11:09:15,142 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 11:09:15,142 	Gloss Alignment :	         
2024-02-10 11:09:15,143 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 11:09:15,144 	Text Reference  :	** *** *** **** **** ****** ** *** the   organisers encouraged athletes to    use        the  condoms in   their home countries
2024-02-10 11:09:15,144 	Text Hypothesis :	he had not even worn gloves he was using his        bare       hands    these sandwiches were being   sold for   a    condom   
2024-02-10 11:09:15,144 	Text Alignment  :	I  I   I   I    I    I      I  I   S     S          S          S        S     S          S    S       S    S     S    S        
2024-02-10 11:09:15,144 ========================================================================================================================
2024-02-10 11:09:15,145 Logging Sequence: 145_52.00
2024-02-10 11:09:15,145 	Gloss Reference :	A B+C+D+E
2024-02-10 11:09:15,145 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 11:09:15,145 	Gloss Alignment :	         
2024-02-10 11:09:15,145 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 11:09:15,146 	Text Reference  :	her name was dropped despite having qualified as    she   was the only female athlete
2024-02-10 11:09:15,146 	Text Hypothesis :	*** **** *** the     finals  were   asked     their child to  be  held on     it     
2024-02-10 11:09:15,147 	Text Alignment  :	D   D    D   S       S       S      S         S     S     S   S   S    S      S      
2024-02-10 11:09:15,147 ========================================================================================================================
2024-02-10 11:09:15,147 Logging Sequence: 172_163.00
2024-02-10 11:09:15,147 	Gloss Reference :	A B+C+D+E
2024-02-10 11:09:15,147 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 11:09:15,147 	Gloss Alignment :	         
2024-02-10 11:09:15,148 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 11:09:15,149 	Text Reference  :	if the match starts anywhere between 730 pm   to **** 935 pm    a  full 20-over match *** **** can be  played
2024-02-10 11:09:15,150 	Text Hypothesis :	if *** ***** ****** ******** ******* you wish to know the match as per  the     match all know for the game  
2024-02-10 11:09:15,150 	Text Alignment  :	   D   D     D      D        D       S   S       I    S   S     S  S    S             I   I    S   S   S     
2024-02-10 11:09:15,150 ========================================================================================================================
2024-02-10 11:09:15,150 Logging Sequence: 150_20.00
2024-02-10 11:09:15,150 	Gloss Reference :	A B+C+D+E
2024-02-10 11:09:15,150 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 11:09:15,151 	Gloss Alignment :	         
2024-02-10 11:09:15,151 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 11:09:15,152 	Text Reference  :	after a tough match india won the **** *** ***** *** saff   championship 2023        title
2024-02-10 11:09:15,152 	Text Hypothesis :	***** * ***** he    has   won the toss and could not played very         interesting times
2024-02-10 11:09:15,152 	Text Alignment  :	D     D D     S     S             I    I   I     I   S      S            S           S    
2024-02-10 11:09:15,152 ========================================================================================================================
2024-02-10 11:09:16,131 Epoch 2889: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 11:09:16,132 EPOCH 2890
2024-02-10 11:09:32,705 Epoch 2890: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 11:09:32,706 EPOCH 2891
2024-02-10 11:09:48,849 Epoch 2891: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 11:09:48,850 EPOCH 2892
2024-02-10 11:10:04,977 Epoch 2892: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 11:10:04,978 EPOCH 2893
2024-02-10 11:10:20,967 Epoch 2893: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 11:10:20,968 EPOCH 2894
2024-02-10 11:10:36,912 Epoch 2894: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 11:10:36,912 EPOCH 2895
2024-02-10 11:10:52,650 Epoch 2895: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 11:10:52,650 EPOCH 2896
2024-02-10 11:11:08,861 Epoch 2896: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 11:11:08,862 EPOCH 2897
2024-02-10 11:11:25,101 Epoch 2897: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 11:11:25,101 EPOCH 2898
2024-02-10 11:11:40,831 Epoch 2898: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 11:11:40,831 EPOCH 2899
2024-02-10 11:11:57,088 Epoch 2899: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 11:11:57,089 EPOCH 2900
2024-02-10 11:12:13,185 [Epoch: 2900 Step: 00026100] Batch Recognition Loss:   0.001304 => Gls Tokens per Sec:      660 || Batch Translation Loss:   0.010487 => Txt Tokens per Sec:     1826 || Lr: 0.000100
2024-02-10 11:12:13,185 Epoch 2900: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 11:12:13,185 EPOCH 2901
2024-02-10 11:12:29,324 Epoch 2901: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 11:12:29,324 EPOCH 2902
2024-02-10 11:12:45,058 Epoch 2902: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 11:12:45,058 EPOCH 2903
2024-02-10 11:13:01,284 Epoch 2903: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 11:13:01,285 EPOCH 2904
2024-02-10 11:13:17,230 Epoch 2904: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 11:13:17,231 EPOCH 2905
2024-02-10 11:13:33,419 Epoch 2905: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 11:13:33,420 EPOCH 2906
2024-02-10 11:13:49,836 Epoch 2906: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 11:13:49,836 EPOCH 2907
2024-02-10 11:14:05,543 Epoch 2907: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 11:14:05,543 EPOCH 2908
2024-02-10 11:14:21,759 Epoch 2908: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 11:14:21,760 EPOCH 2909
2024-02-10 11:14:37,755 Epoch 2909: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 11:14:37,756 EPOCH 2910
2024-02-10 11:14:54,194 Epoch 2910: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 11:14:54,195 EPOCH 2911
2024-02-10 11:15:10,189 Epoch 2911: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 11:15:10,190 EPOCH 2912
2024-02-10 11:15:10,436 [Epoch: 2912 Step: 00026200] Batch Recognition Loss:   0.000445 => Gls Tokens per Sec:     5224 || Batch Translation Loss:   0.008585 => Txt Tokens per Sec:     9469 || Lr: 0.000100
2024-02-10 11:15:25,857 Epoch 2912: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 11:15:25,858 EPOCH 2913
2024-02-10 11:15:42,328 Epoch 2913: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 11:15:42,329 EPOCH 2914
2024-02-10 11:15:58,327 Epoch 2914: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 11:15:58,327 EPOCH 2915
2024-02-10 11:16:14,625 Epoch 2915: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 11:16:14,625 EPOCH 2916
2024-02-10 11:16:30,962 Epoch 2916: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 11:16:30,962 EPOCH 2917
2024-02-10 11:16:47,375 Epoch 2917: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 11:16:47,375 EPOCH 2918
2024-02-10 11:17:03,402 Epoch 2918: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 11:17:03,402 EPOCH 2919
2024-02-10 11:17:19,463 Epoch 2919: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 11:17:19,464 EPOCH 2920
2024-02-10 11:17:35,587 Epoch 2920: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 11:17:35,587 EPOCH 2921
2024-02-10 11:17:51,647 Epoch 2921: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 11:17:51,647 EPOCH 2922
2024-02-10 11:18:08,121 Epoch 2922: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 11:18:08,122 EPOCH 2923
2024-02-10 11:18:09,143 [Epoch: 2923 Step: 00026300] Batch Recognition Loss:   0.000314 => Gls Tokens per Sec:     2510 || Batch Translation Loss:   0.014893 => Txt Tokens per Sec:     7071 || Lr: 0.000100
2024-02-10 11:18:24,255 Epoch 2923: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 11:18:24,255 EPOCH 2924
2024-02-10 11:18:40,327 Epoch 2924: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 11:18:40,327 EPOCH 2925
2024-02-10 11:18:56,454 Epoch 2925: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 11:18:56,455 EPOCH 2926
2024-02-10 11:19:12,503 Epoch 2926: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 11:19:12,504 EPOCH 2927
2024-02-10 11:19:28,826 Epoch 2927: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 11:19:28,827 EPOCH 2928
2024-02-10 11:19:45,164 Epoch 2928: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 11:19:45,164 EPOCH 2929
2024-02-10 11:20:01,348 Epoch 2929: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 11:20:01,349 EPOCH 2930
2024-02-10 11:20:17,513 Epoch 2930: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 11:20:17,513 EPOCH 2931
2024-02-10 11:20:33,819 Epoch 2931: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 11:20:33,820 EPOCH 2932
2024-02-10 11:20:50,788 Epoch 2932: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 11:20:50,789 EPOCH 2933
2024-02-10 11:21:06,993 Epoch 2933: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 11:21:06,994 EPOCH 2934
2024-02-10 11:21:08,139 [Epoch: 2934 Step: 00026400] Batch Recognition Loss:   0.000458 => Gls Tokens per Sec:     3357 || Batch Translation Loss:   0.008938 => Txt Tokens per Sec:     7357 || Lr: 0.000100
2024-02-10 11:21:23,201 Epoch 2934: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 11:21:23,201 EPOCH 2935
2024-02-10 11:21:39,151 Epoch 2935: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 11:21:39,152 EPOCH 2936
2024-02-10 11:21:55,139 Epoch 2936: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 11:21:55,140 EPOCH 2937
2024-02-10 11:22:11,366 Epoch 2937: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 11:22:11,366 EPOCH 2938
2024-02-10 11:22:27,671 Epoch 2938: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 11:22:27,671 EPOCH 2939
2024-02-10 11:22:43,712 Epoch 2939: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 11:22:43,712 EPOCH 2940
2024-02-10 11:23:00,137 Epoch 2940: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 11:23:00,137 EPOCH 2941
2024-02-10 11:23:16,123 Epoch 2941: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 11:23:16,124 EPOCH 2942
2024-02-10 11:23:32,127 Epoch 2942: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 11:23:32,128 EPOCH 2943
2024-02-10 11:23:48,750 Epoch 2943: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 11:23:48,751 EPOCH 2944
2024-02-10 11:24:04,824 Epoch 2944: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 11:24:04,824 EPOCH 2945
2024-02-10 11:24:13,475 [Epoch: 2945 Step: 00026500] Batch Recognition Loss:   0.001049 => Gls Tokens per Sec:      488 || Batch Translation Loss:   0.016550 => Txt Tokens per Sec:     1508 || Lr: 0.000100
2024-02-10 11:24:20,753 Epoch 2945: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 11:24:20,753 EPOCH 2946
2024-02-10 11:24:37,228 Epoch 2946: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 11:24:37,228 EPOCH 2947
2024-02-10 11:24:53,452 Epoch 2947: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 11:24:53,453 EPOCH 2948
2024-02-10 11:25:09,501 Epoch 2948: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 11:25:09,502 EPOCH 2949
2024-02-10 11:25:25,816 Epoch 2949: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 11:25:25,817 EPOCH 2950
2024-02-10 11:25:41,815 Epoch 2950: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 11:25:41,816 EPOCH 2951
2024-02-10 11:25:57,879 Epoch 2951: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 11:25:57,880 EPOCH 2952
2024-02-10 11:26:14,156 Epoch 2952: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 11:26:14,157 EPOCH 2953
2024-02-10 11:26:30,284 Epoch 2953: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 11:26:30,285 EPOCH 2954
2024-02-10 11:26:46,378 Epoch 2954: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 11:26:46,379 EPOCH 2955
2024-02-10 11:27:02,378 Epoch 2955: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 11:27:02,378 EPOCH 2956
2024-02-10 11:27:11,465 [Epoch: 2956 Step: 00026600] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:      605 || Batch Translation Loss:   0.026280 => Txt Tokens per Sec:     1773 || Lr: 0.000100
2024-02-10 11:27:18,282 Epoch 2956: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 11:27:18,282 EPOCH 2957
2024-02-10 11:27:34,857 Epoch 2957: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 11:27:34,857 EPOCH 2958
2024-02-10 11:27:51,218 Epoch 2958: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 11:27:51,219 EPOCH 2959
2024-02-10 11:28:07,492 Epoch 2959: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 11:28:07,492 EPOCH 2960
2024-02-10 11:28:23,778 Epoch 2960: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 11:28:23,779 EPOCH 2961
2024-02-10 11:28:40,102 Epoch 2961: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 11:28:40,102 EPOCH 2962
2024-02-10 11:28:56,341 Epoch 2962: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-10 11:28:56,342 EPOCH 2963
2024-02-10 11:29:12,424 Epoch 2963: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-10 11:29:12,425 EPOCH 2964
2024-02-10 11:29:28,403 Epoch 2964: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-10 11:29:28,403 EPOCH 2965
2024-02-10 11:29:44,395 Epoch 2965: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-10 11:29:44,395 EPOCH 2966
2024-02-10 11:30:00,551 Epoch 2966: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-10 11:30:00,552 EPOCH 2967
2024-02-10 11:30:05,967 [Epoch: 2967 Step: 00026700] Batch Recognition Loss:   0.000385 => Gls Tokens per Sec:     1418 || Batch Translation Loss:   0.055553 => Txt Tokens per Sec:     3649 || Lr: 0.000100
2024-02-10 11:30:16,915 Epoch 2967: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-10 11:30:16,915 EPOCH 2968
2024-02-10 11:30:33,261 Epoch 2968: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-10 11:30:33,261 EPOCH 2969
2024-02-10 11:30:49,432 Epoch 2969: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-10 11:30:49,432 EPOCH 2970
2024-02-10 11:31:05,235 Epoch 2970: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 11:31:05,236 EPOCH 2971
2024-02-10 11:31:21,759 Epoch 2971: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 11:31:21,760 EPOCH 2972
2024-02-10 11:31:38,897 Epoch 2972: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 11:31:38,898 EPOCH 2973
2024-02-10 11:31:54,958 Epoch 2973: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 11:31:54,958 EPOCH 2974
2024-02-10 11:32:10,902 Epoch 2974: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 11:32:10,903 EPOCH 2975
2024-02-10 11:32:26,909 Epoch 2975: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-10 11:32:26,910 EPOCH 2976
2024-02-10 11:32:43,126 Epoch 2976: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 11:32:43,126 EPOCH 2977
2024-02-10 11:32:59,106 Epoch 2977: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 11:32:59,106 EPOCH 2978
2024-02-10 11:33:10,623 [Epoch: 2978 Step: 00026800] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:      778 || Batch Translation Loss:   0.017582 => Txt Tokens per Sec:     2128 || Lr: 0.000100
2024-02-10 11:33:15,296 Epoch 2978: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 11:33:15,296 EPOCH 2979
2024-02-10 11:33:31,321 Epoch 2979: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 11:33:31,322 EPOCH 2980
2024-02-10 11:33:47,310 Epoch 2980: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 11:33:47,311 EPOCH 2981
2024-02-10 11:34:03,244 Epoch 2981: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 11:34:03,244 EPOCH 2982
2024-02-10 11:34:19,552 Epoch 2982: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 11:34:19,552 EPOCH 2983
2024-02-10 11:34:35,629 Epoch 2983: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 11:34:35,630 EPOCH 2984
2024-02-10 11:34:51,976 Epoch 2984: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 11:34:51,977 EPOCH 2985
2024-02-10 11:35:08,272 Epoch 2985: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 11:35:08,272 EPOCH 2986
2024-02-10 11:35:24,406 Epoch 2986: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 11:35:24,407 EPOCH 2987
2024-02-10 11:35:40,688 Epoch 2987: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 11:35:40,689 EPOCH 2988
2024-02-10 11:35:56,618 Epoch 2988: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 11:35:56,618 EPOCH 2989
2024-02-10 11:36:12,125 [Epoch: 2989 Step: 00026900] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:      602 || Batch Translation Loss:   0.031176 => Txt Tokens per Sec:     1667 || Lr: 0.000100
2024-02-10 11:36:12,596 Epoch 2989: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 11:36:12,596 EPOCH 2990
2024-02-10 11:36:28,618 Epoch 2990: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.19 
2024-02-10 11:36:28,619 EPOCH 2991
2024-02-10 11:36:44,652 Epoch 2991: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.21 
2024-02-10 11:36:44,653 EPOCH 2992
2024-02-10 11:37:00,457 Epoch 2992: Total Training Recognition Loss 0.89  Total Training Translation Loss 0.24 
2024-02-10 11:37:00,458 EPOCH 2993
2024-02-10 11:37:16,338 Epoch 2993: Total Training Recognition Loss 2.56  Total Training Translation Loss 0.38 
2024-02-10 11:37:16,338 EPOCH 2994
2024-02-10 11:37:32,492 Epoch 2994: Total Training Recognition Loss 2.41  Total Training Translation Loss 0.59 
2024-02-10 11:37:32,493 EPOCH 2995
2024-02-10 11:37:48,369 Epoch 2995: Total Training Recognition Loss 1.15  Total Training Translation Loss 0.65 
2024-02-10 11:37:48,370 EPOCH 2996
2024-02-10 11:38:04,712 Epoch 2996: Total Training Recognition Loss 0.61  Total Training Translation Loss 0.72 
2024-02-10 11:38:04,713 EPOCH 2997
2024-02-10 11:38:21,101 Epoch 2997: Total Training Recognition Loss 0.45  Total Training Translation Loss 0.70 
2024-02-10 11:38:21,102 EPOCH 2998
2024-02-10 11:38:37,100 Epoch 2998: Total Training Recognition Loss 0.14  Total Training Translation Loss 0.81 
2024-02-10 11:38:37,100 EPOCH 2999
2024-02-10 11:38:52,943 Epoch 2999: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.79 
2024-02-10 11:38:52,943 EPOCH 3000
2024-02-10 11:39:09,296 [Epoch: 3000 Step: 00027000] Batch Recognition Loss:   0.004085 => Gls Tokens per Sec:      650 || Batch Translation Loss:   0.035981 => Txt Tokens per Sec:     1797 || Lr: 0.000100
2024-02-10 11:39:09,297 Epoch 3000: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-10 11:39:09,297 EPOCH 3001
2024-02-10 11:39:25,350 Epoch 3001: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-10 11:39:25,351 EPOCH 3002
2024-02-10 11:39:41,517 Epoch 3002: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-10 11:39:41,518 EPOCH 3003
2024-02-10 11:39:57,570 Epoch 3003: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.54 
2024-02-10 11:39:57,571 EPOCH 3004
2024-02-10 11:40:13,801 Epoch 3004: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.44 
2024-02-10 11:40:13,801 EPOCH 3005
2024-02-10 11:40:30,168 Epoch 3005: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.41 
2024-02-10 11:40:30,169 EPOCH 3006
2024-02-10 11:40:46,231 Epoch 3006: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.44 
2024-02-10 11:40:46,231 EPOCH 3007
2024-02-10 11:41:02,317 Epoch 3007: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-10 11:41:02,318 EPOCH 3008
2024-02-10 11:41:18,234 Epoch 3008: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 11:41:18,235 EPOCH 3009
2024-02-10 11:41:34,177 Epoch 3009: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-10 11:41:34,178 EPOCH 3010
2024-02-10 11:41:50,376 Epoch 3010: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 11:41:50,377 EPOCH 3011
2024-02-10 11:42:06,260 Epoch 3011: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 11:42:06,260 EPOCH 3012
2024-02-10 11:42:06,896 [Epoch: 3012 Step: 00027100] Batch Recognition Loss:   0.004070 => Gls Tokens per Sec:     2019 || Batch Translation Loss:   0.028085 => Txt Tokens per Sec:     5563 || Lr: 0.000100
2024-02-10 11:42:22,500 Epoch 3012: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 11:42:22,501 EPOCH 3013
2024-02-10 11:42:38,606 Epoch 3013: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 11:42:38,607 EPOCH 3014
2024-02-10 11:42:54,917 Epoch 3014: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 11:42:54,918 EPOCH 3015
2024-02-10 11:43:10,835 Epoch 3015: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 11:43:10,835 EPOCH 3016
2024-02-10 11:43:26,724 Epoch 3016: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 11:43:26,725 EPOCH 3017
2024-02-10 11:43:42,629 Epoch 3017: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 11:43:42,630 EPOCH 3018
2024-02-10 11:43:59,059 Epoch 3018: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 11:43:59,060 EPOCH 3019
2024-02-10 11:44:15,218 Epoch 3019: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 11:44:15,218 EPOCH 3020
2024-02-10 11:44:31,222 Epoch 3020: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 11:44:31,222 EPOCH 3021
2024-02-10 11:44:47,593 Epoch 3021: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 11:44:47,594 EPOCH 3022
2024-02-10 11:45:03,701 Epoch 3022: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 11:45:03,701 EPOCH 3023
2024-02-10 11:45:04,676 [Epoch: 3023 Step: 00027200] Batch Recognition Loss:   0.002523 => Gls Tokens per Sec:     2631 || Batch Translation Loss:   0.009233 => Txt Tokens per Sec:     6159 || Lr: 0.000100
2024-02-10 11:45:19,901 Epoch 3023: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 11:45:19,901 EPOCH 3024
2024-02-10 11:45:36,069 Epoch 3024: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 11:45:36,070 EPOCH 3025
2024-02-10 11:45:52,239 Epoch 3025: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 11:45:52,240 EPOCH 3026
2024-02-10 11:46:08,568 Epoch 3026: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 11:46:08,568 EPOCH 3027
2024-02-10 11:46:24,538 Epoch 3027: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 11:46:24,539 EPOCH 3028
2024-02-10 11:46:40,448 Epoch 3028: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 11:46:40,449 EPOCH 3029
2024-02-10 11:46:56,533 Epoch 3029: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 11:46:56,533 EPOCH 3030
2024-02-10 11:47:12,615 Epoch 3030: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 11:47:12,616 EPOCH 3031
2024-02-10 11:47:28,523 Epoch 3031: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 11:47:28,523 EPOCH 3032
2024-02-10 11:47:44,557 Epoch 3032: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 11:47:44,557 EPOCH 3033
2024-02-10 11:48:00,496 Epoch 3033: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 11:48:00,496 EPOCH 3034
2024-02-10 11:48:07,181 [Epoch: 3034 Step: 00027300] Batch Recognition Loss:   0.000966 => Gls Tokens per Sec:      574 || Batch Translation Loss:   0.018822 => Txt Tokens per Sec:     1659 || Lr: 0.000100
2024-02-10 11:48:16,550 Epoch 3034: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 11:48:16,551 EPOCH 3035
2024-02-10 11:48:32,653 Epoch 3035: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 11:48:32,654 EPOCH 3036
2024-02-10 11:48:48,669 Epoch 3036: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 11:48:48,670 EPOCH 3037
2024-02-10 11:49:04,570 Epoch 3037: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 11:49:04,571 EPOCH 3038
2024-02-10 11:49:20,665 Epoch 3038: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 11:49:20,665 EPOCH 3039
2024-02-10 11:49:37,096 Epoch 3039: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 11:49:37,096 EPOCH 3040
2024-02-10 11:49:53,179 Epoch 3040: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 11:49:53,179 EPOCH 3041
2024-02-10 11:50:08,832 Epoch 3041: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 11:50:08,833 EPOCH 3042
2024-02-10 11:50:25,015 Epoch 3042: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 11:50:25,015 EPOCH 3043
2024-02-10 11:50:41,090 Epoch 3043: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 11:50:41,090 EPOCH 3044
2024-02-10 11:50:57,076 Epoch 3044: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 11:50:57,077 EPOCH 3045
2024-02-10 11:51:04,618 [Epoch: 3045 Step: 00027400] Batch Recognition Loss:   0.000116 => Gls Tokens per Sec:      679 || Batch Translation Loss:   0.015071 => Txt Tokens per Sec:     2001 || Lr: 0.000100
2024-02-10 11:51:13,449 Epoch 3045: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 11:51:13,450 EPOCH 3046
2024-02-10 11:51:29,647 Epoch 3046: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 11:51:29,648 EPOCH 3047
2024-02-10 11:51:45,183 Epoch 3047: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 11:51:45,183 EPOCH 3048
2024-02-10 11:52:00,866 Epoch 3048: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 11:52:00,866 EPOCH 3049
2024-02-10 11:52:17,385 Epoch 3049: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 11:52:17,385 EPOCH 3050
2024-02-10 11:52:33,391 Epoch 3050: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 11:52:33,391 EPOCH 3051
2024-02-10 11:52:49,349 Epoch 3051: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 11:52:49,349 EPOCH 3052
2024-02-10 11:53:05,711 Epoch 3052: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 11:53:05,712 EPOCH 3053
2024-02-10 11:53:21,870 Epoch 3053: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 11:53:21,870 EPOCH 3054
2024-02-10 11:53:38,077 Epoch 3054: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 11:53:38,077 EPOCH 3055
2024-02-10 11:53:54,010 Epoch 3055: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 11:53:54,011 EPOCH 3056
2024-02-10 11:54:08,809 [Epoch: 3056 Step: 00027500] Batch Recognition Loss:   0.000232 => Gls Tokens per Sec:      372 || Batch Translation Loss:   0.019460 => Txt Tokens per Sec:     1178 || Lr: 0.000100
2024-02-10 11:54:10,246 Epoch 3056: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 11:54:10,246 EPOCH 3057
2024-02-10 11:54:26,158 Epoch 3057: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 11:54:26,159 EPOCH 3058
2024-02-10 11:54:42,250 Epoch 3058: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 11:54:42,250 EPOCH 3059
2024-02-10 11:54:58,582 Epoch 3059: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 11:54:58,583 EPOCH 3060
2024-02-10 11:55:14,557 Epoch 3060: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 11:55:14,557 EPOCH 3061
2024-02-10 11:55:30,400 Epoch 3061: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-10 11:55:30,400 EPOCH 3062
2024-02-10 11:55:46,735 Epoch 3062: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.90 
2024-02-10 11:55:46,736 EPOCH 3063
2024-02-10 11:56:03,051 Epoch 3063: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.55 
2024-02-10 11:56:03,052 EPOCH 3064
2024-02-10 11:56:18,878 Epoch 3064: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.36 
2024-02-10 11:56:18,879 EPOCH 3065
2024-02-10 11:56:34,871 Epoch 3065: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.88 
2024-02-10 11:56:34,872 EPOCH 3066
2024-02-10 11:56:51,031 Epoch 3066: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.84 
2024-02-10 11:56:51,031 EPOCH 3067
2024-02-10 11:57:05,913 [Epoch: 3067 Step: 00027600] Batch Recognition Loss:   0.000840 => Gls Tokens per Sec:      456 || Batch Translation Loss:   0.138486 => Txt Tokens per Sec:     1332 || Lr: 0.000100
2024-02-10 11:57:07,168 Epoch 3067: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.13 
2024-02-10 11:57:07,168 EPOCH 3068
2024-02-10 11:57:22,936 Epoch 3068: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.96 
2024-02-10 11:57:22,937 EPOCH 3069
2024-02-10 11:57:38,612 Epoch 3069: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-10 11:57:38,612 EPOCH 3070
2024-02-10 11:57:54,710 Epoch 3070: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-10 11:57:54,710 EPOCH 3071
2024-02-10 11:58:10,732 Epoch 3071: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-10 11:58:10,733 EPOCH 3072
2024-02-10 11:58:26,852 Epoch 3072: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-10 11:58:26,853 EPOCH 3073
2024-02-10 11:58:42,508 Epoch 3073: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 11:58:42,509 EPOCH 3074
2024-02-10 11:58:58,854 Epoch 3074: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-10 11:58:58,855 EPOCH 3075
2024-02-10 11:59:14,824 Epoch 3075: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 11:59:14,825 EPOCH 3076
2024-02-10 11:59:30,740 Epoch 3076: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 11:59:30,741 EPOCH 3077
2024-02-10 11:59:47,139 Epoch 3077: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-10 11:59:47,139 EPOCH 3078
2024-02-10 12:00:01,751 [Epoch: 3078 Step: 00027700] Batch Recognition Loss:   0.000792 => Gls Tokens per Sec:      552 || Batch Translation Loss:   0.037479 => Txt Tokens per Sec:     1551 || Lr: 0.000100
2024-02-10 12:00:03,034 Epoch 3078: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-10 12:00:03,035 EPOCH 3079
2024-02-10 12:00:19,238 Epoch 3079: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-10 12:00:19,238 EPOCH 3080
2024-02-10 12:00:35,208 Epoch 3080: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-10 12:00:35,208 EPOCH 3081
2024-02-10 12:00:51,460 Epoch 3081: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 12:00:51,460 EPOCH 3082
2024-02-10 12:01:07,327 Epoch 3082: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 12:01:07,327 EPOCH 3083
2024-02-10 12:01:23,492 Epoch 3083: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 12:01:23,493 EPOCH 3084
2024-02-10 12:01:39,420 Epoch 3084: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 12:01:39,420 EPOCH 3085
2024-02-10 12:01:55,693 Epoch 3085: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 12:01:55,693 EPOCH 3086
2024-02-10 12:02:11,671 Epoch 3086: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-10 12:02:11,672 EPOCH 3087
2024-02-10 12:02:27,480 Epoch 3087: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 12:02:27,481 EPOCH 3088
2024-02-10 12:02:43,625 Epoch 3088: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 12:02:43,626 EPOCH 3089
2024-02-10 12:02:56,714 [Epoch: 3089 Step: 00027800] Batch Recognition Loss:   0.000280 => Gls Tokens per Sec:      714 || Batch Translation Loss:   0.027234 => Txt Tokens per Sec:     1931 || Lr: 0.000100
2024-02-10 12:02:59,994 Epoch 3089: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 12:02:59,994 EPOCH 3090
2024-02-10 12:03:16,048 Epoch 3090: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 12:03:16,048 EPOCH 3091
2024-02-10 12:03:32,101 Epoch 3091: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-10 12:03:32,102 EPOCH 3092
2024-02-10 12:03:48,183 Epoch 3092: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 12:03:48,183 EPOCH 3093
2024-02-10 12:04:04,396 Epoch 3093: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 12:04:04,396 EPOCH 3094
2024-02-10 12:04:20,377 Epoch 3094: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 12:04:20,377 EPOCH 3095
2024-02-10 12:04:36,468 Epoch 3095: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 12:04:36,468 EPOCH 3096
2024-02-10 12:04:52,436 Epoch 3096: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 12:04:52,437 EPOCH 3097
2024-02-10 12:05:08,568 Epoch 3097: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 12:05:08,569 EPOCH 3098
2024-02-10 12:05:24,144 Epoch 3098: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 12:05:24,144 EPOCH 3099
2024-02-10 12:05:40,326 Epoch 3099: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:05:40,327 EPOCH 3100
2024-02-10 12:05:56,138 [Epoch: 3100 Step: 00027900] Batch Recognition Loss:   0.000759 => Gls Tokens per Sec:      672 || Batch Translation Loss:   0.018865 => Txt Tokens per Sec:     1858 || Lr: 0.000100
2024-02-10 12:05:56,138 Epoch 3100: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:05:56,138 EPOCH 3101
2024-02-10 12:06:11,999 Epoch 3101: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:06:12,000 EPOCH 3102
2024-02-10 12:06:28,378 Epoch 3102: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:06:28,379 EPOCH 3103
2024-02-10 12:06:44,864 Epoch 3103: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 12:06:44,864 EPOCH 3104
2024-02-10 12:07:01,212 Epoch 3104: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 12:07:01,212 EPOCH 3105
2024-02-10 12:07:17,357 Epoch 3105: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 12:07:17,357 EPOCH 3106
2024-02-10 12:07:33,518 Epoch 3106: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 12:07:33,518 EPOCH 3107
2024-02-10 12:07:49,614 Epoch 3107: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 12:07:49,614 EPOCH 3108
2024-02-10 12:08:05,525 Epoch 3108: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 12:08:05,526 EPOCH 3109
2024-02-10 12:08:21,469 Epoch 3109: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 12:08:21,470 EPOCH 3110
2024-02-10 12:08:37,714 Epoch 3110: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 12:08:37,715 EPOCH 3111
2024-02-10 12:08:53,471 Epoch 3111: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:08:53,471 EPOCH 3112
2024-02-10 12:08:54,060 [Epoch: 3112 Step: 00028000] Batch Recognition Loss:   0.000288 => Gls Tokens per Sec:     2177 || Batch Translation Loss:   0.013830 => Txt Tokens per Sec:     5937 || Lr: 0.000100
2024-02-10 12:10:05,933 Validation result at epoch 3112, step    28000: duration: 71.8723s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.29291	Translation Loss: 95479.87500	PPL: 13856.95898
	Eval Metric: BLEU
	WER 2.82	(DEL: 0.00,	INS: 0.00,	SUB: 2.82)
	BLEU-4 0.41	(BLEU-1: 11.30,	BLEU-2: 3.10,	BLEU-3: 0.99,	BLEU-4: 0.41)
	CHRF 17.28	ROUGE 9.21
2024-02-10 12:10:05,935 Logging Recognition and Translation Outputs
2024-02-10 12:10:05,935 ========================================================================================================================
2024-02-10 12:10:05,936 Logging Sequence: 156_288.00
2024-02-10 12:10:05,936 	Gloss Reference :	A B+C+D+E
2024-02-10 12:10:05,936 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 12:10:05,936 	Gloss Alignment :	         
2024-02-10 12:10:05,936 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 12:10:05,938 	Text Reference  :	pooran led  the ***** ******* team  to  victory miny became winners of the 1st        season
2024-02-10 12:10:05,938 	Text Hypothesis :	****** both the match between india and kuwait  at   the    end     of the regulation time  
2024-02-10 12:10:05,938 	Text Alignment  :	D      S        I     I       S     S   S       S    S      S              S          S     
2024-02-10 12:10:05,938 ========================================================================================================================
2024-02-10 12:10:05,938 Logging Sequence: 98_135.00
2024-02-10 12:10:05,938 	Gloss Reference :	A B+C+D+E
2024-02-10 12:10:05,939 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 12:10:05,939 	Gloss Alignment :	         
2024-02-10 12:10:05,939 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 12:10:05,940 	Text Reference  :	however due to the rise in   coronavirus cases the tournament ***** was shifted
2024-02-10 12:10:05,940 	Text Hypothesis :	******* *** ** the **** bcci can         not   the tournament after the series 
2024-02-10 12:10:05,940 	Text Alignment  :	D       D   D      D    S    S           S                    I     S   S      
2024-02-10 12:10:05,940 ========================================================================================================================
2024-02-10 12:10:05,940 Logging Sequence: 161_47.00
2024-02-10 12:10:05,940 	Gloss Reference :	A B+C+D+E
2024-02-10 12:10:05,941 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 12:10:05,941 	Gloss Alignment :	         
2024-02-10 12:10:05,941 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 12:10:05,942 	Text Reference  :	he requested confidentiality as      he was planning to    make  an official announcement
2024-02-10 12:10:05,942 	Text Hypothesis :	** ********* *************** because of the for      rana' death a  court    marriage    
2024-02-10 12:10:05,942 	Text Alignment  :	D  D         D               S       S  S   S        S     S     S  S        S           
2024-02-10 12:10:05,942 ========================================================================================================================
2024-02-10 12:10:05,942 Logging Sequence: 131_159.00
2024-02-10 12:10:05,943 	Gloss Reference :	A B+C+D+E
2024-02-10 12:10:05,943 	Gloss Hypothesis:	A B+C+D  
2024-02-10 12:10:05,943 	Gloss Alignment :	  S      
2024-02-10 12:10:05,943 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 12:10:05,944 	Text Reference  :	chanu also met biren singh following the     meeting  singh   described chanu       as    our nation' pride     
2024-02-10 12:10:05,944 	Text Hypothesis :	***** **** *** ***** the   taliban   swiftly regained control of        afghanistan after us  troops  withdrawal
2024-02-10 12:10:05,944 	Text Alignment  :	D     D    D   D     S     S         S       S        S       S         S           S     S   S       S         
2024-02-10 12:10:05,944 ========================================================================================================================
2024-02-10 12:10:05,945 Logging Sequence: 137_167.00
2024-02-10 12:10:05,945 	Gloss Reference :	A B+C+D+E
2024-02-10 12:10:05,945 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 12:10:05,945 	Gloss Alignment :	         
2024-02-10 12:10:05,945 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 12:10:05,946 	Text Reference  :	however after 630 pm there will be certain fan zones where beer   will be  available and nowhere else   
2024-02-10 12:10:05,946 	Text Hypothesis :	******* ***** *** ** ***** **** ** ******* *** ***** the   indian team was disgusted by  the     threats
2024-02-10 12:10:05,947 	Text Alignment  :	D       D     D   D  D     D    D  D       D   D     S     S      S    S   S         S   S       S      
2024-02-10 12:10:05,947 ========================================================================================================================
2024-02-10 12:10:22,306 Epoch 3112: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 12:10:22,306 EPOCH 3113
2024-02-10 12:10:38,168 Epoch 3113: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 12:10:38,169 EPOCH 3114
2024-02-10 12:10:54,391 Epoch 3114: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:10:54,392 EPOCH 3115
2024-02-10 12:11:10,714 Epoch 3115: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:11:10,714 EPOCH 3116
2024-02-10 12:11:26,737 Epoch 3116: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:11:26,738 EPOCH 3117
2024-02-10 12:11:43,081 Epoch 3117: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 12:11:43,081 EPOCH 3118
2024-02-10 12:11:59,106 Epoch 3118: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 12:11:59,106 EPOCH 3119
2024-02-10 12:12:14,902 Epoch 3119: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 12:12:14,902 EPOCH 3120
2024-02-10 12:12:30,798 Epoch 3120: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 12:12:30,798 EPOCH 3121
2024-02-10 12:12:46,871 Epoch 3121: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 12:12:46,872 EPOCH 3122
2024-02-10 12:13:03,048 Epoch 3122: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 12:13:03,049 EPOCH 3123
2024-02-10 12:13:04,063 [Epoch: 3123 Step: 00028100] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:     2525 || Batch Translation Loss:   0.017004 => Txt Tokens per Sec:     7040 || Lr: 0.000100
2024-02-10 12:13:19,074 Epoch 3123: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:13:19,074 EPOCH 3124
2024-02-10 12:13:35,370 Epoch 3124: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 12:13:35,370 EPOCH 3125
2024-02-10 12:13:51,383 Epoch 3125: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 12:13:51,384 EPOCH 3126
2024-02-10 12:14:07,660 Epoch 3126: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 12:14:07,660 EPOCH 3127
2024-02-10 12:14:23,796 Epoch 3127: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 12:14:23,796 EPOCH 3128
2024-02-10 12:14:40,026 Epoch 3128: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 12:14:40,027 EPOCH 3129
2024-02-10 12:14:56,422 Epoch 3129: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:14:56,422 EPOCH 3130
2024-02-10 12:15:12,588 Epoch 3130: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 12:15:12,588 EPOCH 3131
2024-02-10 12:15:28,953 Epoch 3131: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:15:28,954 EPOCH 3132
2024-02-10 12:15:45,263 Epoch 3132: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 12:15:45,264 EPOCH 3133
2024-02-10 12:16:01,481 Epoch 3133: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 12:16:01,481 EPOCH 3134
2024-02-10 12:16:08,520 [Epoch: 3134 Step: 00028200] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:      546 || Batch Translation Loss:   0.013940 => Txt Tokens per Sec:     1501 || Lr: 0.000100
2024-02-10 12:16:17,793 Epoch 3134: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 12:16:17,794 EPOCH 3135
2024-02-10 12:16:34,191 Epoch 3135: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 12:16:34,191 EPOCH 3136
2024-02-10 12:16:50,106 Epoch 3136: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 12:16:50,106 EPOCH 3137
2024-02-10 12:17:05,872 Epoch 3137: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:17:05,873 EPOCH 3138
2024-02-10 12:17:22,133 Epoch 3138: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 12:17:22,134 EPOCH 3139
2024-02-10 12:17:38,117 Epoch 3139: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 12:17:38,117 EPOCH 3140
2024-02-10 12:17:54,508 Epoch 3140: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 12:17:54,509 EPOCH 3141
2024-02-10 12:18:10,761 Epoch 3141: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 12:18:10,762 EPOCH 3142
2024-02-10 12:18:27,059 Epoch 3142: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 12:18:27,059 EPOCH 3143
2024-02-10 12:18:43,336 Epoch 3143: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 12:18:43,336 EPOCH 3144
2024-02-10 12:18:59,612 Epoch 3144: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:18:59,612 EPOCH 3145
2024-02-10 12:19:07,878 [Epoch: 3145 Step: 00028300] Batch Recognition Loss:   0.000818 => Gls Tokens per Sec:      511 || Batch Translation Loss:   0.010886 => Txt Tokens per Sec:     1401 || Lr: 0.000100
2024-02-10 12:19:15,805 Epoch 3145: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:19:15,806 EPOCH 3146
2024-02-10 12:19:31,676 Epoch 3146: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:19:31,677 EPOCH 3147
2024-02-10 12:19:47,743 Epoch 3147: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:19:47,743 EPOCH 3148
2024-02-10 12:20:03,980 Epoch 3148: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 12:20:03,981 EPOCH 3149
2024-02-10 12:20:20,194 Epoch 3149: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:20:20,195 EPOCH 3150
2024-02-10 12:20:35,844 Epoch 3150: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 12:20:35,844 EPOCH 3151
2024-02-10 12:20:51,448 Epoch 3151: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:20:51,449 EPOCH 3152
2024-02-10 12:21:08,729 Epoch 3152: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:21:08,730 EPOCH 3153
2024-02-10 12:21:24,830 Epoch 3153: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:21:24,831 EPOCH 3154
2024-02-10 12:21:40,827 Epoch 3154: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:21:40,828 EPOCH 3155
2024-02-10 12:21:57,213 Epoch 3155: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 12:21:57,214 EPOCH 3156
2024-02-10 12:22:05,968 [Epoch: 3156 Step: 00028400] Batch Recognition Loss:   0.000314 => Gls Tokens per Sec:      628 || Batch Translation Loss:   0.009724 => Txt Tokens per Sec:     1753 || Lr: 0.000100
2024-02-10 12:22:13,179 Epoch 3156: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:22:13,180 EPOCH 3157
2024-02-10 12:22:29,400 Epoch 3157: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 12:22:29,401 EPOCH 3158
2024-02-10 12:22:45,481 Epoch 3158: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 12:22:45,481 EPOCH 3159
2024-02-10 12:23:01,399 Epoch 3159: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 12:23:01,400 EPOCH 3160
2024-02-10 12:23:17,422 Epoch 3160: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 12:23:17,423 EPOCH 3161
2024-02-10 12:23:33,297 Epoch 3161: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-10 12:23:33,297 EPOCH 3162
2024-02-10 12:23:49,463 Epoch 3162: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-10 12:23:49,464 EPOCH 3163
2024-02-10 12:24:05,478 Epoch 3163: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-10 12:24:05,478 EPOCH 3164
2024-02-10 12:24:21,631 Epoch 3164: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-10 12:24:21,632 EPOCH 3165
2024-02-10 12:24:37,897 Epoch 3165: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.98 
2024-02-10 12:24:37,899 EPOCH 3166
2024-02-10 12:24:54,035 Epoch 3166: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.82 
2024-02-10 12:24:54,036 EPOCH 3167
2024-02-10 12:25:06,184 [Epoch: 3167 Step: 00028500] Batch Recognition Loss:   0.002808 => Gls Tokens per Sec:      558 || Batch Translation Loss:   0.706959 => Txt Tokens per Sec:     1627 || Lr: 0.000100
2024-02-10 12:25:10,024 Epoch 3167: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.33 
2024-02-10 12:25:10,024 EPOCH 3168
2024-02-10 12:25:26,114 Epoch 3168: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.59 
2024-02-10 12:25:26,115 EPOCH 3169
2024-02-10 12:25:42,137 Epoch 3169: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.61 
2024-02-10 12:25:42,138 EPOCH 3170
2024-02-10 12:25:58,551 Epoch 3170: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.33 
2024-02-10 12:25:58,552 EPOCH 3171
2024-02-10 12:26:14,538 Epoch 3171: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.22 
2024-02-10 12:26:14,539 EPOCH 3172
2024-02-10 12:26:30,611 Epoch 3172: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.84 
2024-02-10 12:26:30,611 EPOCH 3173
2024-02-10 12:26:46,417 Epoch 3173: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-10 12:26:46,418 EPOCH 3174
2024-02-10 12:27:02,772 Epoch 3174: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-10 12:27:02,773 EPOCH 3175
2024-02-10 12:27:18,546 Epoch 3175: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-10 12:27:18,546 EPOCH 3176
2024-02-10 12:27:34,676 Epoch 3176: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-10 12:27:34,676 EPOCH 3177
2024-02-10 12:27:50,574 Epoch 3177: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-10 12:27:50,575 EPOCH 3178
2024-02-10 12:28:00,458 [Epoch: 3178 Step: 00028600] Batch Recognition Loss:   0.000461 => Gls Tokens per Sec:      816 || Batch Translation Loss:   0.043048 => Txt Tokens per Sec:     2152 || Lr: 0.000100
2024-02-10 12:28:06,785 Epoch 3178: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 12:28:06,785 EPOCH 3179
2024-02-10 12:28:22,990 Epoch 3179: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 12:28:22,991 EPOCH 3180
2024-02-10 12:28:39,108 Epoch 3180: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 12:28:39,109 EPOCH 3181
2024-02-10 12:28:54,965 Epoch 3181: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 12:28:54,966 EPOCH 3182
2024-02-10 12:29:10,832 Epoch 3182: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 12:29:10,833 EPOCH 3183
2024-02-10 12:29:27,138 Epoch 3183: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 12:29:27,138 EPOCH 3184
2024-02-10 12:29:42,715 Epoch 3184: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 12:29:42,716 EPOCH 3185
2024-02-10 12:29:59,019 Epoch 3185: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 12:29:59,020 EPOCH 3186
2024-02-10 12:30:15,277 Epoch 3186: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 12:30:15,277 EPOCH 3187
2024-02-10 12:30:31,043 Epoch 3187: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-10 12:30:31,044 EPOCH 3188
2024-02-10 12:30:47,309 Epoch 3188: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 12:30:47,310 EPOCH 3189
2024-02-10 12:30:57,509 [Epoch: 3189 Step: 00028700] Batch Recognition Loss:   0.000248 => Gls Tokens per Sec:      916 || Batch Translation Loss:   0.017927 => Txt Tokens per Sec:     2450 || Lr: 0.000100
2024-02-10 12:31:03,238 Epoch 3189: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-10 12:31:03,238 EPOCH 3190
2024-02-10 12:31:19,334 Epoch 3190: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 12:31:19,334 EPOCH 3191
2024-02-10 12:31:35,668 Epoch 3191: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 12:31:35,669 EPOCH 3192
2024-02-10 12:31:52,552 Epoch 3192: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 12:31:52,552 EPOCH 3193
2024-02-10 12:32:08,437 Epoch 3193: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 12:32:08,438 EPOCH 3194
2024-02-10 12:32:24,398 Epoch 3194: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 12:32:24,399 EPOCH 3195
2024-02-10 12:32:40,699 Epoch 3195: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 12:32:40,699 EPOCH 3196
2024-02-10 12:32:56,545 Epoch 3196: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 12:32:56,545 EPOCH 3197
2024-02-10 12:33:12,841 Epoch 3197: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 12:33:12,842 EPOCH 3198
2024-02-10 12:33:28,824 Epoch 3198: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 12:33:28,824 EPOCH 3199
2024-02-10 12:33:45,075 Epoch 3199: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 12:33:45,075 EPOCH 3200
2024-02-10 12:34:01,177 [Epoch: 3200 Step: 00028800] Batch Recognition Loss:   0.000409 => Gls Tokens per Sec:      660 || Batch Translation Loss:   0.019621 => Txt Tokens per Sec:     1825 || Lr: 0.000100
2024-02-10 12:34:01,178 Epoch 3200: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 12:34:01,178 EPOCH 3201
2024-02-10 12:34:17,231 Epoch 3201: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:34:17,232 EPOCH 3202
2024-02-10 12:34:33,453 Epoch 3202: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:34:33,454 EPOCH 3203
2024-02-10 12:34:49,440 Epoch 3203: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:34:49,441 EPOCH 3204
2024-02-10 12:35:05,442 Epoch 3204: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:35:05,443 EPOCH 3205
2024-02-10 12:35:21,267 Epoch 3205: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:35:21,267 EPOCH 3206
2024-02-10 12:35:37,398 Epoch 3206: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:35:37,399 EPOCH 3207
2024-02-10 12:35:53,481 Epoch 3207: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:35:53,482 EPOCH 3208
2024-02-10 12:36:09,242 Epoch 3208: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:36:09,242 EPOCH 3209
2024-02-10 12:36:25,226 Epoch 3209: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 12:36:25,226 EPOCH 3210
2024-02-10 12:36:41,454 Epoch 3210: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:36:41,455 EPOCH 3211
2024-02-10 12:36:57,434 Epoch 3211: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 12:36:57,434 EPOCH 3212
2024-02-10 12:36:58,183 [Epoch: 3212 Step: 00028900] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:     1711 || Batch Translation Loss:   0.020647 => Txt Tokens per Sec:     5151 || Lr: 0.000100
2024-02-10 12:37:13,465 Epoch 3212: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 12:37:13,466 EPOCH 3213
2024-02-10 12:37:29,710 Epoch 3213: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 12:37:29,710 EPOCH 3214
2024-02-10 12:37:45,863 Epoch 3214: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:37:45,863 EPOCH 3215
2024-02-10 12:38:01,779 Epoch 3215: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:38:01,780 EPOCH 3216
2024-02-10 12:38:18,105 Epoch 3216: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 12:38:18,105 EPOCH 3217
2024-02-10 12:38:33,806 Epoch 3217: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:38:33,806 EPOCH 3218
2024-02-10 12:38:49,846 Epoch 3218: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:38:49,846 EPOCH 3219
2024-02-10 12:39:06,114 Epoch 3219: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 12:39:06,115 EPOCH 3220
2024-02-10 12:39:22,184 Epoch 3220: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:39:22,185 EPOCH 3221
2024-02-10 12:39:38,236 Epoch 3221: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:39:38,237 EPOCH 3222
2024-02-10 12:39:54,096 Epoch 3222: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 12:39:54,097 EPOCH 3223
2024-02-10 12:39:59,183 [Epoch: 3223 Step: 00029000] Batch Recognition Loss:   0.000163 => Gls Tokens per Sec:      326 || Batch Translation Loss:   0.022095 => Txt Tokens per Sec:     1035 || Lr: 0.000100
2024-02-10 12:40:10,370 Epoch 3223: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:40:10,370 EPOCH 3224
2024-02-10 12:40:26,242 Epoch 3224: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 12:40:26,243 EPOCH 3225
2024-02-10 12:40:42,586 Epoch 3225: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 12:40:42,586 EPOCH 3226
2024-02-10 12:40:58,729 Epoch 3226: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 12:40:58,729 EPOCH 3227
2024-02-10 12:41:14,447 Epoch 3227: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 12:41:14,448 EPOCH 3228
2024-02-10 12:41:30,486 Epoch 3228: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 12:41:30,487 EPOCH 3229
2024-02-10 12:41:46,435 Epoch 3229: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 12:41:46,436 EPOCH 3230
2024-02-10 12:42:02,379 Epoch 3230: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 12:42:02,380 EPOCH 3231
2024-02-10 12:42:18,286 Epoch 3231: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 12:42:18,286 EPOCH 3232
2024-02-10 12:42:34,410 Epoch 3232: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:42:34,410 EPOCH 3233
2024-02-10 12:42:50,469 Epoch 3233: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 12:42:50,470 EPOCH 3234
2024-02-10 12:42:55,102 [Epoch: 3234 Step: 00029100] Batch Recognition Loss:   0.000416 => Gls Tokens per Sec:      829 || Batch Translation Loss:   0.024073 => Txt Tokens per Sec:     2430 || Lr: 0.000100
2024-02-10 12:43:06,860 Epoch 3234: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 12:43:06,860 EPOCH 3235
2024-02-10 12:43:22,968 Epoch 3235: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 12:43:22,968 EPOCH 3236
2024-02-10 12:43:38,848 Epoch 3236: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 12:43:38,849 EPOCH 3237
2024-02-10 12:43:55,154 Epoch 3237: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-10 12:43:55,154 EPOCH 3238
2024-02-10 12:44:11,213 Epoch 3238: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-10 12:44:11,213 EPOCH 3239
2024-02-10 12:44:27,188 Epoch 3239: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-10 12:44:27,188 EPOCH 3240
2024-02-10 12:44:43,052 Epoch 3240: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-10 12:44:43,053 EPOCH 3241
2024-02-10 12:44:59,006 Epoch 3241: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-10 12:44:59,007 EPOCH 3242
2024-02-10 12:45:14,953 Epoch 3242: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-10 12:45:14,954 EPOCH 3243
2024-02-10 12:45:31,025 Epoch 3243: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-10 12:45:31,025 EPOCH 3244
2024-02-10 12:45:47,265 Epoch 3244: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 12:45:47,266 EPOCH 3245
2024-02-10 12:45:52,051 [Epoch: 3245 Step: 00029200] Batch Recognition Loss:   0.000284 => Gls Tokens per Sec:     1070 || Batch Translation Loss:   0.063920 => Txt Tokens per Sec:     3086 || Lr: 0.000100
2024-02-10 12:46:03,484 Epoch 3245: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-10 12:46:03,485 EPOCH 3246
2024-02-10 12:46:19,560 Epoch 3246: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-10 12:46:19,561 EPOCH 3247
2024-02-10 12:46:35,737 Epoch 3247: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-10 12:46:35,737 EPOCH 3248
2024-02-10 12:46:51,930 Epoch 3248: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-10 12:46:51,931 EPOCH 3249
2024-02-10 12:47:07,895 Epoch 3249: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-10 12:47:07,896 EPOCH 3250
2024-02-10 12:47:23,932 Epoch 3250: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-10 12:47:23,933 EPOCH 3251
2024-02-10 12:47:40,135 Epoch 3251: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-10 12:47:40,136 EPOCH 3252
2024-02-10 12:47:56,330 Epoch 3252: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-10 12:47:56,330 EPOCH 3253
2024-02-10 12:48:12,747 Epoch 3253: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-10 12:48:12,748 EPOCH 3254
2024-02-10 12:48:28,797 Epoch 3254: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-10 12:48:28,798 EPOCH 3255
2024-02-10 12:48:44,907 Epoch 3255: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.79 
2024-02-10 12:48:44,908 EPOCH 3256
2024-02-10 12:48:49,867 [Epoch: 3256 Step: 00029300] Batch Recognition Loss:   0.003087 => Gls Tokens per Sec:     1291 || Batch Translation Loss:   0.882488 => Txt Tokens per Sec:     3441 || Lr: 0.000100
2024-02-10 12:49:01,070 Epoch 3256: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.85 
2024-02-10 12:49:01,071 EPOCH 3257
2024-02-10 12:49:17,223 Epoch 3257: Total Training Recognition Loss 0.06  Total Training Translation Loss 5.87 
2024-02-10 12:49:17,223 EPOCH 3258
2024-02-10 12:49:33,478 Epoch 3258: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.00 
2024-02-10 12:49:33,478 EPOCH 3259
2024-02-10 12:49:49,532 Epoch 3259: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.14 
2024-02-10 12:49:49,533 EPOCH 3260
2024-02-10 12:50:05,651 Epoch 3260: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.24 
2024-02-10 12:50:05,651 EPOCH 3261
2024-02-10 12:50:21,744 Epoch 3261: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.75 
2024-02-10 12:50:21,745 EPOCH 3262
2024-02-10 12:50:37,676 Epoch 3262: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.51 
2024-02-10 12:50:37,677 EPOCH 3263
2024-02-10 12:50:53,661 Epoch 3263: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-10 12:50:53,662 EPOCH 3264
2024-02-10 12:51:09,769 Epoch 3264: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-10 12:51:09,770 EPOCH 3265
2024-02-10 12:51:26,273 Epoch 3265: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-10 12:51:26,274 EPOCH 3266
2024-02-10 12:51:42,526 Epoch 3266: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 12:51:42,526 EPOCH 3267
2024-02-10 12:51:48,245 [Epoch: 3267 Step: 00029400] Batch Recognition Loss:   0.000548 => Gls Tokens per Sec:     1343 || Batch Translation Loss:   0.028098 => Txt Tokens per Sec:     3560 || Lr: 0.000100
2024-02-10 12:51:58,833 Epoch 3267: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 12:51:58,834 EPOCH 3268
2024-02-10 12:52:15,286 Epoch 3268: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 12:52:15,286 EPOCH 3269
2024-02-10 12:52:31,659 Epoch 3269: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 12:52:31,659 EPOCH 3270
2024-02-10 12:52:47,840 Epoch 3270: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 12:52:47,841 EPOCH 3271
2024-02-10 12:53:03,993 Epoch 3271: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 12:53:03,994 EPOCH 3272
2024-02-10 12:53:20,271 Epoch 3272: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 12:53:20,272 EPOCH 3273
2024-02-10 12:53:36,359 Epoch 3273: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 12:53:36,360 EPOCH 3274
2024-02-10 12:53:52,269 Epoch 3274: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 12:53:52,269 EPOCH 3275
2024-02-10 12:54:08,340 Epoch 3275: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 12:54:08,341 EPOCH 3276
2024-02-10 12:54:24,122 Epoch 3276: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 12:54:24,123 EPOCH 3277
2024-02-10 12:54:40,431 Epoch 3277: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 12:54:40,432 EPOCH 3278
2024-02-10 12:54:47,399 [Epoch: 3278 Step: 00029500] Batch Recognition Loss:   0.000660 => Gls Tokens per Sec:     1157 || Batch Translation Loss:   0.010062 => Txt Tokens per Sec:     2997 || Lr: 0.000100
2024-02-10 12:54:56,330 Epoch 3278: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 12:54:56,330 EPOCH 3279
2024-02-10 12:55:12,329 Epoch 3279: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 12:55:12,329 EPOCH 3280
2024-02-10 12:55:28,942 Epoch 3280: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 12:55:28,942 EPOCH 3281
2024-02-10 12:55:44,860 Epoch 3281: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:55:44,860 EPOCH 3282
2024-02-10 12:56:00,858 Epoch 3282: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:56:00,858 EPOCH 3283
2024-02-10 12:56:17,193 Epoch 3283: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 12:56:17,194 EPOCH 3284
2024-02-10 12:56:33,436 Epoch 3284: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:56:33,436 EPOCH 3285
2024-02-10 12:56:49,342 Epoch 3285: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:56:49,342 EPOCH 3286
2024-02-10 12:57:05,610 Epoch 3286: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:57:05,611 EPOCH 3287
2024-02-10 12:57:21,515 Epoch 3287: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 12:57:21,516 EPOCH 3288
2024-02-10 12:57:37,638 Epoch 3288: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:57:37,639 EPOCH 3289
2024-02-10 12:57:52,871 [Epoch: 3289 Step: 00029600] Batch Recognition Loss:   0.000304 => Gls Tokens per Sec:      613 || Batch Translation Loss:   0.006838 => Txt Tokens per Sec:     1673 || Lr: 0.000100
2024-02-10 12:57:53,688 Epoch 3289: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:57:53,689 EPOCH 3290
2024-02-10 12:58:10,190 Epoch 3290: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 12:58:10,190 EPOCH 3291
2024-02-10 12:58:25,909 Epoch 3291: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 12:58:25,909 EPOCH 3292
2024-02-10 12:58:41,799 Epoch 3292: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 12:58:41,799 EPOCH 3293
2024-02-10 12:58:58,174 Epoch 3293: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 12:58:58,175 EPOCH 3294
2024-02-10 12:59:14,376 Epoch 3294: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 12:59:14,377 EPOCH 3295
2024-02-10 12:59:30,437 Epoch 3295: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 12:59:30,437 EPOCH 3296
2024-02-10 12:59:46,612 Epoch 3296: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 12:59:46,613 EPOCH 3297
2024-02-10 13:00:02,707 Epoch 3297: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 13:00:02,707 EPOCH 3298
2024-02-10 13:00:19,018 Epoch 3298: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 13:00:19,019 EPOCH 3299
2024-02-10 13:00:35,072 Epoch 3299: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:00:35,073 EPOCH 3300
2024-02-10 13:00:51,307 [Epoch: 3300 Step: 00029700] Batch Recognition Loss:   0.000342 => Gls Tokens per Sec:      654 || Batch Translation Loss:   0.009131 => Txt Tokens per Sec:     1810 || Lr: 0.000100
2024-02-10 13:00:51,308 Epoch 3300: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:00:51,308 EPOCH 3301
2024-02-10 13:01:07,448 Epoch 3301: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:01:07,449 EPOCH 3302
2024-02-10 13:01:23,510 Epoch 3302: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:01:23,510 EPOCH 3303
2024-02-10 13:01:39,711 Epoch 3303: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:01:39,712 EPOCH 3304
2024-02-10 13:01:55,910 Epoch 3304: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:01:55,911 EPOCH 3305
2024-02-10 13:02:12,087 Epoch 3305: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:02:12,087 EPOCH 3306
2024-02-10 13:02:28,245 Epoch 3306: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:02:28,246 EPOCH 3307
2024-02-10 13:02:44,104 Epoch 3307: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:02:44,105 EPOCH 3308
2024-02-10 13:03:00,603 Epoch 3308: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:03:00,603 EPOCH 3309
2024-02-10 13:03:16,642 Epoch 3309: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:03:16,643 EPOCH 3310
2024-02-10 13:03:32,608 Epoch 3310: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:03:32,608 EPOCH 3311
2024-02-10 13:03:48,536 Epoch 3311: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:03:48,536 EPOCH 3312
2024-02-10 13:03:54,379 [Epoch: 3312 Step: 00029800] Batch Recognition Loss:   0.000296 => Gls Tokens per Sec:      219 || Batch Translation Loss:   0.016558 => Txt Tokens per Sec:      756 || Lr: 0.000100
2024-02-10 13:04:04,661 Epoch 3312: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 13:04:04,662 EPOCH 3313
2024-02-10 13:04:20,461 Epoch 3313: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 13:04:20,462 EPOCH 3314
2024-02-10 13:04:36,903 Epoch 3314: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 13:04:36,903 EPOCH 3315
2024-02-10 13:04:53,073 Epoch 3315: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 13:04:53,073 EPOCH 3316
2024-02-10 13:05:09,145 Epoch 3316: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 13:05:09,146 EPOCH 3317
2024-02-10 13:05:25,469 Epoch 3317: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:05:25,470 EPOCH 3318
2024-02-10 13:05:41,302 Epoch 3318: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:05:41,303 EPOCH 3319
2024-02-10 13:05:57,289 Epoch 3319: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:05:57,290 EPOCH 3320
2024-02-10 13:06:13,166 Epoch 3320: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 13:06:13,167 EPOCH 3321
2024-02-10 13:06:29,446 Epoch 3321: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 13:06:29,447 EPOCH 3322
2024-02-10 13:06:45,700 Epoch 3322: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 13:06:45,700 EPOCH 3323
2024-02-10 13:06:49,243 [Epoch: 3323 Step: 00029900] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:      723 || Batch Translation Loss:   0.011828 => Txt Tokens per Sec:     2143 || Lr: 0.000100
2024-02-10 13:07:01,490 Epoch 3323: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 13:07:01,490 EPOCH 3324
2024-02-10 13:07:17,542 Epoch 3324: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 13:07:17,543 EPOCH 3325
2024-02-10 13:07:33,704 Epoch 3325: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:07:33,705 EPOCH 3326
2024-02-10 13:07:49,710 Epoch 3326: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:07:49,711 EPOCH 3327
2024-02-10 13:08:05,684 Epoch 3327: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:08:05,685 EPOCH 3328
2024-02-10 13:08:21,654 Epoch 3328: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:08:21,655 EPOCH 3329
2024-02-10 13:08:37,504 Epoch 3329: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 13:08:37,504 EPOCH 3330
2024-02-10 13:08:53,663 Epoch 3330: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:08:53,664 EPOCH 3331
2024-02-10 13:09:09,857 Epoch 3331: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:09:09,858 EPOCH 3332
2024-02-10 13:09:25,628 Epoch 3332: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:09:25,629 EPOCH 3333
2024-02-10 13:09:41,581 Epoch 3333: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 13:09:41,582 EPOCH 3334
2024-02-10 13:09:46,931 [Epoch: 3334 Step: 00030000] Batch Recognition Loss:   0.000377 => Gls Tokens per Sec:      550 || Batch Translation Loss:   0.010539 => Txt Tokens per Sec:     1438 || Lr: 0.000100
2024-02-10 13:10:58,770 Validation result at epoch 3334, step    30000: duration: 71.8378s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.28709	Translation Loss: 96162.85938	PPL: 14835.23242
	Eval Metric: BLEU
	WER 2.68	(DEL: 0.00,	INS: 0.00,	SUB: 2.68)
	BLEU-4 0.40	(BLEU-1: 10.99,	BLEU-2: 3.49,	BLEU-3: 1.23,	BLEU-4: 0.40)
	CHRF 17.19	ROUGE 9.23
2024-02-10 13:10:58,772 Logging Recognition and Translation Outputs
2024-02-10 13:10:58,772 ========================================================================================================================
2024-02-10 13:10:58,773 Logging Sequence: 146_102.00
2024-02-10 13:10:58,773 	Gloss Reference :	A B+C+D+E
2024-02-10 13:10:58,773 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 13:10:58,773 	Gloss Alignment :	         
2024-02-10 13:10:58,773 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 13:10:58,775 	Text Reference  :	famous indian champion players like kidambi  srikanth and ashwini ponappa have tested positive for  coronavirus
2024-02-10 13:10:58,775 	Text Hypothesis :	along  with   them     these   who  messaged kohli    and ******* ******* want to     win      many wickets    
2024-02-10 13:10:58,775 	Text Alignment  :	S      S      S        S       S    S        S            D       D       S    S      S        S    S          
2024-02-10 13:10:58,775 ========================================================================================================================
2024-02-10 13:10:58,775 Logging Sequence: 53_178.00
2024-02-10 13:10:58,776 	Gloss Reference :	A B+C+D+E
2024-02-10 13:10:58,776 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 13:10:58,776 	Gloss Alignment :	         
2024-02-10 13:10:58,776 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 13:10:58,777 	Text Reference  :	the   money would help all    those affected by  the   humanitarian crisis in    afghanistan
2024-02-10 13:10:58,777 	Text Hypothesis :	there are   a     huge number of    people   are still has          the    trent rockets    
2024-02-10 13:10:58,777 	Text Alignment  :	S     S     S     S    S      S     S        S   S     S            S      S     S          
2024-02-10 13:10:58,778 ========================================================================================================================
2024-02-10 13:10:58,778 Logging Sequence: 129_200.00
2024-02-10 13:10:58,778 	Gloss Reference :	A B+C+D+E      
2024-02-10 13:10:58,778 	Gloss Hypothesis:	A B+C+D+E+D+E+D
2024-02-10 13:10:58,778 	Gloss Alignment :	  S            
2024-02-10 13:10:58,778 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 13:10:58,780 	Text Reference  :	*** **** *** ********** ** the ioc      would lose  about  4   billion if  the olympics were to be cancelled
2024-02-10 13:10:58,780 	Text Hypothesis :	now that the organisers of the olympics in    tokyo handed out for     all the olympics **** ** ** *********
2024-02-10 13:10:58,780 	Text Alignment  :	I   I    I   I          I      S        S     S     S      S   S       S                D    D  D  D        
2024-02-10 13:10:58,780 ========================================================================================================================
2024-02-10 13:10:58,780 Logging Sequence: 77_2.00
2024-02-10 13:10:58,780 	Gloss Reference :	A B+C+D+E  
2024-02-10 13:10:58,780 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-10 13:10:58,781 	Gloss Alignment :	  S        
2024-02-10 13:10:58,781 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 13:10:58,782 	Text Reference  :	on 25th april the ipl match between sunrisers hyderabad and  delhi capitals ended in  a   tie  
2024-02-10 13:10:58,782 	Text Hypothesis :	on **** ***** *** *** ***** 24th    october   india     lost the   t20      world cup sri lanka
2024-02-10 13:10:58,782 	Text Alignment  :	   D    D     D   D   D     S       S         S         S    S     S        S     S   S   S    
2024-02-10 13:10:58,782 ========================================================================================================================
2024-02-10 13:10:58,782 Logging Sequence: 119_170.00
2024-02-10 13:10:58,783 	Gloss Reference :	A B+C+D+E
2024-02-10 13:10:58,783 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 13:10:58,783 	Gloss Alignment :	         
2024-02-10 13:10:58,783 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 13:10:58,784 	Text Reference  :	they said it was a proud moment messi  is     a    big  hearted man  
2024-02-10 13:10:58,784 	Text Hypothesis :	**** **** ** *** * after the    defeat indian team lost the     match
2024-02-10 13:10:58,784 	Text Alignment  :	D    D    D  D   D S     S      S      S      S    S    S       S    
2024-02-10 13:10:58,784 ========================================================================================================================
2024-02-10 13:11:10,488 Epoch 3334: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 13:11:10,489 EPOCH 3335
2024-02-10 13:11:26,671 Epoch 3335: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 13:11:26,672 EPOCH 3336
2024-02-10 13:11:42,590 Epoch 3336: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:11:42,591 EPOCH 3337
2024-02-10 13:11:58,655 Epoch 3337: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:11:58,655 EPOCH 3338
2024-02-10 13:12:14,727 Epoch 3338: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 13:12:14,728 EPOCH 3339
2024-02-10 13:12:30,754 Epoch 3339: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:12:30,754 EPOCH 3340
2024-02-10 13:12:46,585 Epoch 3340: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 13:12:46,586 EPOCH 3341
2024-02-10 13:13:03,072 Epoch 3341: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:13:03,072 EPOCH 3342
2024-02-10 13:13:19,406 Epoch 3342: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:13:19,406 EPOCH 3343
2024-02-10 13:13:35,660 Epoch 3343: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:13:35,661 EPOCH 3344
2024-02-10 13:13:52,036 Epoch 3344: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-10 13:13:52,037 EPOCH 3345
2024-02-10 13:13:56,813 [Epoch: 3345 Step: 00030100] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:     1072 || Batch Translation Loss:   0.007236 => Txt Tokens per Sec:     2896 || Lr: 0.000100
2024-02-10 13:14:08,477 Epoch 3345: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 13:14:08,477 EPOCH 3346
2024-02-10 13:14:24,697 Epoch 3346: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 13:14:24,697 EPOCH 3347
2024-02-10 13:14:41,153 Epoch 3347: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 13:14:41,153 EPOCH 3348
2024-02-10 13:14:57,162 Epoch 3348: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 13:14:57,163 EPOCH 3349
2024-02-10 13:15:13,303 Epoch 3349: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 13:15:13,304 EPOCH 3350
2024-02-10 13:15:29,566 Epoch 3350: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 13:15:29,566 EPOCH 3351
2024-02-10 13:15:45,753 Epoch 3351: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 13:15:45,753 EPOCH 3352
2024-02-10 13:16:01,874 Epoch 3352: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 13:16:01,875 EPOCH 3353
2024-02-10 13:16:18,259 Epoch 3353: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:16:18,260 EPOCH 3354
2024-02-10 13:16:34,168 Epoch 3354: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:16:34,169 EPOCH 3355
2024-02-10 13:16:50,308 Epoch 3355: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 13:16:50,308 EPOCH 3356
2024-02-10 13:17:00,669 [Epoch: 3356 Step: 00030200] Batch Recognition Loss:   0.000318 => Gls Tokens per Sec:      618 || Batch Translation Loss:   0.010309 => Txt Tokens per Sec:     1662 || Lr: 0.000100
2024-02-10 13:17:06,716 Epoch 3356: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 13:17:06,716 EPOCH 3357
2024-02-10 13:17:22,544 Epoch 3357: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:17:22,545 EPOCH 3358
2024-02-10 13:17:38,599 Epoch 3358: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 13:17:38,600 EPOCH 3359
2024-02-10 13:17:54,645 Epoch 3359: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 13:17:54,646 EPOCH 3360
2024-02-10 13:18:10,940 Epoch 3360: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 13:18:10,941 EPOCH 3361
2024-02-10 13:18:27,104 Epoch 3361: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 13:18:27,105 EPOCH 3362
2024-02-10 13:18:43,276 Epoch 3362: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 13:18:43,277 EPOCH 3363
2024-02-10 13:18:59,259 Epoch 3363: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 13:18:59,259 EPOCH 3364
2024-02-10 13:19:15,651 Epoch 3364: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 13:19:15,651 EPOCH 3365
2024-02-10 13:19:31,925 Epoch 3365: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 13:19:31,925 EPOCH 3366
2024-02-10 13:19:48,026 Epoch 3366: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 13:19:48,027 EPOCH 3367
2024-02-10 13:19:59,248 [Epoch: 3367 Step: 00030300] Batch Recognition Loss:   0.000373 => Gls Tokens per Sec:      684 || Batch Translation Loss:   0.024909 => Txt Tokens per Sec:     2040 || Lr: 0.000100
2024-02-10 13:20:04,169 Epoch 3367: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 13:20:04,169 EPOCH 3368
2024-02-10 13:20:20,373 Epoch 3368: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 13:20:20,373 EPOCH 3369
2024-02-10 13:20:36,411 Epoch 3369: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 13:20:36,411 EPOCH 3370
2024-02-10 13:20:52,503 Epoch 3370: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 13:20:52,504 EPOCH 3371
2024-02-10 13:21:09,633 Epoch 3371: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 13:21:09,634 EPOCH 3372
2024-02-10 13:21:25,836 Epoch 3372: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 13:21:25,837 EPOCH 3373
2024-02-10 13:21:42,572 Epoch 3373: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 13:21:42,573 EPOCH 3374
2024-02-10 13:21:58,802 Epoch 3374: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 13:21:58,803 EPOCH 3375
2024-02-10 13:22:15,011 Epoch 3375: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 13:22:15,012 EPOCH 3376
2024-02-10 13:22:30,757 Epoch 3376: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 13:22:30,758 EPOCH 3377
2024-02-10 13:22:46,744 Epoch 3377: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 13:22:46,745 EPOCH 3378
2024-02-10 13:22:59,107 [Epoch: 3378 Step: 00030400] Batch Recognition Loss:   0.000343 => Gls Tokens per Sec:      652 || Batch Translation Loss:   0.047525 => Txt Tokens per Sec:     1764 || Lr: 0.000100
2024-02-10 13:23:02,816 Epoch 3378: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 13:23:02,816 EPOCH 3379
2024-02-10 13:23:18,951 Epoch 3379: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-10 13:23:18,951 EPOCH 3380
2024-02-10 13:23:35,108 Epoch 3380: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-10 13:23:35,109 EPOCH 3381
2024-02-10 13:23:51,206 Epoch 3381: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-10 13:23:51,207 EPOCH 3382
2024-02-10 13:24:07,609 Epoch 3382: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.51 
2024-02-10 13:24:07,609 EPOCH 3383
2024-02-10 13:24:23,765 Epoch 3383: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-10 13:24:23,765 EPOCH 3384
2024-02-10 13:24:39,756 Epoch 3384: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-10 13:24:39,757 EPOCH 3385
2024-02-10 13:24:55,935 Epoch 3385: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.22 
2024-02-10 13:24:55,935 EPOCH 3386
2024-02-10 13:25:11,926 Epoch 3386: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.81 
2024-02-10 13:25:11,926 EPOCH 3387
2024-02-10 13:25:27,559 Epoch 3387: Total Training Recognition Loss 0.04  Total Training Translation Loss 3.25 
2024-02-10 13:25:27,560 EPOCH 3388
2024-02-10 13:25:43,603 Epoch 3388: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.10 
2024-02-10 13:25:43,604 EPOCH 3389
2024-02-10 13:25:59,000 [Epoch: 3389 Step: 00030500] Batch Recognition Loss:   0.002198 => Gls Tokens per Sec:      607 || Batch Translation Loss:   0.155450 => Txt Tokens per Sec:     1656 || Lr: 0.000100
2024-02-10 13:25:59,707 Epoch 3389: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.55 
2024-02-10 13:25:59,708 EPOCH 3390
2024-02-10 13:26:15,740 Epoch 3390: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.05 
2024-02-10 13:26:15,740 EPOCH 3391
2024-02-10 13:26:31,737 Epoch 3391: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.64 
2024-02-10 13:26:31,738 EPOCH 3392
2024-02-10 13:26:47,728 Epoch 3392: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-10 13:26:47,729 EPOCH 3393
2024-02-10 13:27:03,897 Epoch 3393: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-10 13:27:03,897 EPOCH 3394
2024-02-10 13:27:20,061 Epoch 3394: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 13:27:20,061 EPOCH 3395
2024-02-10 13:27:36,065 Epoch 3395: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 13:27:36,065 EPOCH 3396
2024-02-10 13:27:51,956 Epoch 3396: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 13:27:51,957 EPOCH 3397
2024-02-10 13:28:08,009 Epoch 3397: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 13:28:08,009 EPOCH 3398
2024-02-10 13:28:23,930 Epoch 3398: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 13:28:23,931 EPOCH 3399
2024-02-10 13:28:40,285 Epoch 3399: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 13:28:40,285 EPOCH 3400
2024-02-10 13:28:56,184 [Epoch: 3400 Step: 00030600] Batch Recognition Loss:   0.000427 => Gls Tokens per Sec:      668 || Batch Translation Loss:   0.033280 => Txt Tokens per Sec:     1848 || Lr: 0.000100
2024-02-10 13:28:56,185 Epoch 3400: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 13:28:56,185 EPOCH 3401
2024-02-10 13:29:12,223 Epoch 3401: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 13:29:12,223 EPOCH 3402
2024-02-10 13:29:28,592 Epoch 3402: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 13:29:28,593 EPOCH 3403
2024-02-10 13:29:44,458 Epoch 3403: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 13:29:44,459 EPOCH 3404
2024-02-10 13:30:00,648 Epoch 3404: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 13:30:00,648 EPOCH 3405
2024-02-10 13:30:16,807 Epoch 3405: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 13:30:16,807 EPOCH 3406
2024-02-10 13:30:32,804 Epoch 3406: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 13:30:32,804 EPOCH 3407
2024-02-10 13:30:48,646 Epoch 3407: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 13:30:48,646 EPOCH 3408
2024-02-10 13:31:04,691 Epoch 3408: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 13:31:04,691 EPOCH 3409
2024-02-10 13:31:20,892 Epoch 3409: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 13:31:20,892 EPOCH 3410
2024-02-10 13:31:37,242 Epoch 3410: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 13:31:37,243 EPOCH 3411
2024-02-10 13:31:54,065 Epoch 3411: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 13:31:54,065 EPOCH 3412
2024-02-10 13:31:54,587 [Epoch: 3412 Step: 00030700] Batch Recognition Loss:   0.000401 => Gls Tokens per Sec:     2457 || Batch Translation Loss:   0.017071 => Txt Tokens per Sec:     6722 || Lr: 0.000100
2024-02-10 13:32:10,412 Epoch 3412: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 13:32:10,412 EPOCH 3413
2024-02-10 13:32:26,410 Epoch 3413: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 13:32:26,410 EPOCH 3414
2024-02-10 13:32:42,951 Epoch 3414: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 13:32:42,952 EPOCH 3415
2024-02-10 13:32:59,337 Epoch 3415: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 13:32:59,338 EPOCH 3416
2024-02-10 13:33:15,450 Epoch 3416: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 13:33:15,451 EPOCH 3417
2024-02-10 13:33:31,790 Epoch 3417: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 13:33:31,791 EPOCH 3418
2024-02-10 13:33:47,729 Epoch 3418: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 13:33:47,730 EPOCH 3419
2024-02-10 13:34:03,626 Epoch 3419: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 13:34:03,627 EPOCH 3420
2024-02-10 13:34:20,076 Epoch 3420: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 13:34:20,077 EPOCH 3421
2024-02-10 13:34:35,904 Epoch 3421: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 13:34:35,905 EPOCH 3422
2024-02-10 13:34:52,074 Epoch 3422: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:34:52,075 EPOCH 3423
2024-02-10 13:34:57,055 [Epoch: 3423 Step: 00030800] Batch Recognition Loss:   0.000758 => Gls Tokens per Sec:      333 || Batch Translation Loss:   0.007279 => Txt Tokens per Sec:     1017 || Lr: 0.000100
2024-02-10 13:35:08,298 Epoch 3423: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 13:35:08,299 EPOCH 3424
2024-02-10 13:35:24,606 Epoch 3424: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 13:35:24,607 EPOCH 3425
2024-02-10 13:35:40,364 Epoch 3425: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 13:35:40,364 EPOCH 3426
2024-02-10 13:35:56,508 Epoch 3426: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 13:35:56,509 EPOCH 3427
2024-02-10 13:36:12,515 Epoch 3427: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 13:36:12,516 EPOCH 3428
2024-02-10 13:36:28,684 Epoch 3428: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 13:36:28,685 EPOCH 3429
2024-02-10 13:36:44,684 Epoch 3429: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 13:36:44,685 EPOCH 3430
2024-02-10 13:37:00,699 Epoch 3430: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 13:37:00,700 EPOCH 3431
2024-02-10 13:37:16,537 Epoch 3431: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:37:16,538 EPOCH 3432
2024-02-10 13:37:32,636 Epoch 3432: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 13:37:32,637 EPOCH 3433
2024-02-10 13:37:48,828 Epoch 3433: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 13:37:48,829 EPOCH 3434
2024-02-10 13:37:54,000 [Epoch: 3434 Step: 00030900] Batch Recognition Loss:   0.000267 => Gls Tokens per Sec:      569 || Batch Translation Loss:   0.006347 => Txt Tokens per Sec:     1612 || Lr: 0.000100
2024-02-10 13:38:04,653 Epoch 3434: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:38:04,653 EPOCH 3435
2024-02-10 13:38:20,696 Epoch 3435: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 13:38:20,696 EPOCH 3436
2024-02-10 13:38:36,478 Epoch 3436: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 13:38:36,478 EPOCH 3437
2024-02-10 13:38:52,752 Epoch 3437: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 13:38:52,753 EPOCH 3438
2024-02-10 13:39:08,594 Epoch 3438: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 13:39:08,594 EPOCH 3439
2024-02-10 13:39:24,692 Epoch 3439: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 13:39:24,692 EPOCH 3440
2024-02-10 13:39:40,776 Epoch 3440: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 13:39:40,777 EPOCH 3441
2024-02-10 13:39:56,957 Epoch 3441: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:39:56,958 EPOCH 3442
2024-02-10 13:40:12,953 Epoch 3442: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:40:12,954 EPOCH 3443
2024-02-10 13:40:28,936 Epoch 3443: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:40:28,937 EPOCH 3444
2024-02-10 13:40:45,287 Epoch 3444: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:40:45,287 EPOCH 3445
2024-02-10 13:40:52,612 [Epoch: 3445 Step: 00031000] Batch Recognition Loss:   0.000356 => Gls Tokens per Sec:      699 || Batch Translation Loss:   0.019139 => Txt Tokens per Sec:     2058 || Lr: 0.000100
2024-02-10 13:41:01,370 Epoch 3445: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:41:01,371 EPOCH 3446
2024-02-10 13:41:17,312 Epoch 3446: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:41:17,313 EPOCH 3447
2024-02-10 13:41:33,689 Epoch 3447: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 13:41:33,689 EPOCH 3448
2024-02-10 13:41:49,953 Epoch 3448: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 13:41:49,953 EPOCH 3449
2024-02-10 13:42:06,124 Epoch 3449: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 13:42:06,125 EPOCH 3450
2024-02-10 13:42:22,366 Epoch 3450: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 13:42:22,367 EPOCH 3451
2024-02-10 13:42:38,662 Epoch 3451: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 13:42:38,663 EPOCH 3452
2024-02-10 13:42:54,997 Epoch 3452: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 13:42:54,997 EPOCH 3453
2024-02-10 13:43:11,330 Epoch 3453: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 13:43:11,331 EPOCH 3454
2024-02-10 13:43:27,589 Epoch 3454: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 13:43:27,590 EPOCH 3455
2024-02-10 13:43:43,889 Epoch 3455: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 13:43:43,890 EPOCH 3456
2024-02-10 13:43:51,565 [Epoch: 3456 Step: 00031100] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:      834 || Batch Translation Loss:   0.010235 => Txt Tokens per Sec:     2209 || Lr: 0.000100
2024-02-10 13:44:00,127 Epoch 3456: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.13 
2024-02-10 13:44:00,127 EPOCH 3457
2024-02-10 13:44:16,154 Epoch 3457: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.13 
2024-02-10 13:44:16,154 EPOCH 3458
2024-02-10 13:44:32,185 Epoch 3458: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 13:44:32,186 EPOCH 3459
2024-02-10 13:44:48,165 Epoch 3459: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.12 
2024-02-10 13:44:48,165 EPOCH 3460
2024-02-10 13:45:04,142 Epoch 3460: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.12 
2024-02-10 13:45:04,143 EPOCH 3461
2024-02-10 13:45:19,868 Epoch 3461: Total Training Recognition Loss 0.14  Total Training Translation Loss 0.16 
2024-02-10 13:45:19,868 EPOCH 3462
2024-02-10 13:45:35,701 Epoch 3462: Total Training Recognition Loss 0.20  Total Training Translation Loss 0.14 
2024-02-10 13:45:35,702 EPOCH 3463
2024-02-10 13:45:51,800 Epoch 3463: Total Training Recognition Loss 0.62  Total Training Translation Loss 0.19 
2024-02-10 13:45:51,801 EPOCH 3464
2024-02-10 13:46:07,477 Epoch 3464: Total Training Recognition Loss 0.22  Total Training Translation Loss 0.23 
2024-02-10 13:46:07,478 EPOCH 3465
2024-02-10 13:46:23,751 Epoch 3465: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.25 
2024-02-10 13:46:23,751 EPOCH 3466
2024-02-10 13:46:39,980 Epoch 3466: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.26 
2024-02-10 13:46:39,981 EPOCH 3467
2024-02-10 13:46:48,003 [Epoch: 3467 Step: 00031200] Batch Recognition Loss:   0.011135 => Gls Tokens per Sec:      958 || Batch Translation Loss:   0.041902 => Txt Tokens per Sec:     2575 || Lr: 0.000100
2024-02-10 13:46:56,192 Epoch 3467: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.27 
2024-02-10 13:46:56,193 EPOCH 3468
2024-02-10 13:47:12,551 Epoch 3468: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.30 
2024-02-10 13:47:12,552 EPOCH 3469
2024-02-10 13:47:28,807 Epoch 3469: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.25 
2024-02-10 13:47:28,808 EPOCH 3470
2024-02-10 13:47:44,882 Epoch 3470: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 13:47:44,882 EPOCH 3471
2024-02-10 13:48:00,999 Epoch 3471: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.20 
2024-02-10 13:48:00,999 EPOCH 3472
2024-02-10 13:48:17,077 Epoch 3472: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 13:48:17,077 EPOCH 3473
2024-02-10 13:48:33,252 Epoch 3473: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 13:48:33,253 EPOCH 3474
2024-02-10 13:48:49,227 Epoch 3474: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 13:48:49,228 EPOCH 3475
2024-02-10 13:49:05,627 Epoch 3475: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 13:49:05,628 EPOCH 3476
2024-02-10 13:49:21,621 Epoch 3476: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 13:49:21,621 EPOCH 3477
2024-02-10 13:49:37,893 Epoch 3477: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 13:49:37,894 EPOCH 3478
2024-02-10 13:49:53,004 [Epoch: 3478 Step: 00031300] Batch Recognition Loss:   0.000309 => Gls Tokens per Sec:      533 || Batch Translation Loss:   0.035445 => Txt Tokens per Sec:     1471 || Lr: 0.000100
2024-02-10 13:49:54,101 Epoch 3478: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 13:49:54,101 EPOCH 3479
2024-02-10 13:50:10,275 Epoch 3479: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 13:50:10,276 EPOCH 3480
2024-02-10 13:50:26,449 Epoch 3480: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 13:50:26,449 EPOCH 3481
2024-02-10 13:50:42,613 Epoch 3481: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 13:50:42,614 EPOCH 3482
2024-02-10 13:50:58,746 Epoch 3482: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-10 13:50:58,746 EPOCH 3483
2024-02-10 13:51:14,760 Epoch 3483: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 13:51:14,760 EPOCH 3484
2024-02-10 13:51:30,856 Epoch 3484: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 13:51:30,856 EPOCH 3485
2024-02-10 13:51:47,218 Epoch 3485: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 13:51:47,219 EPOCH 3486
2024-02-10 13:52:03,482 Epoch 3486: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 13:52:03,482 EPOCH 3487
2024-02-10 13:52:19,505 Epoch 3487: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 13:52:19,506 EPOCH 3488
2024-02-10 13:52:35,363 Epoch 3488: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 13:52:35,364 EPOCH 3489
2024-02-10 13:52:47,343 [Epoch: 3489 Step: 00031400] Batch Recognition Loss:   0.000346 => Gls Tokens per Sec:      855 || Batch Translation Loss:   0.031955 => Txt Tokens per Sec:     2340 || Lr: 0.000100
2024-02-10 13:52:51,681 Epoch 3489: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 13:52:51,682 EPOCH 3490
2024-02-10 13:53:07,709 Epoch 3490: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 13:53:07,710 EPOCH 3491
2024-02-10 13:53:23,728 Epoch 3491: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 13:53:23,729 EPOCH 3492
2024-02-10 13:53:39,940 Epoch 3492: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 13:53:39,941 EPOCH 3493
2024-02-10 13:53:56,014 Epoch 3493: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 13:53:56,015 EPOCH 3494
2024-02-10 13:54:12,341 Epoch 3494: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 13:54:12,342 EPOCH 3495
2024-02-10 13:54:28,395 Epoch 3495: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-10 13:54:28,395 EPOCH 3496
2024-02-10 13:54:44,703 Epoch 3496: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-10 13:54:44,703 EPOCH 3497
2024-02-10 13:55:00,278 Epoch 3497: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-10 13:55:00,279 EPOCH 3498
2024-02-10 13:55:16,449 Epoch 3498: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.09 
2024-02-10 13:55:16,449 EPOCH 3499
2024-02-10 13:55:32,381 Epoch 3499: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.99 
2024-02-10 13:55:32,382 EPOCH 3500
2024-02-10 13:55:48,711 [Epoch: 3500 Step: 00031500] Batch Recognition Loss:   0.000539 => Gls Tokens per Sec:      650 || Batch Translation Loss:   0.131934 => Txt Tokens per Sec:     1799 || Lr: 0.000100
2024-02-10 13:55:48,712 Epoch 3500: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-10 13:55:48,712 EPOCH 3501
2024-02-10 13:56:04,893 Epoch 3501: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-10 13:56:04,894 EPOCH 3502
2024-02-10 13:56:21,220 Epoch 3502: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-10 13:56:21,221 EPOCH 3503
2024-02-10 13:56:37,246 Epoch 3503: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-10 13:56:37,246 EPOCH 3504
2024-02-10 13:56:53,705 Epoch 3504: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-10 13:56:53,706 EPOCH 3505
2024-02-10 13:57:09,809 Epoch 3505: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-10 13:57:09,810 EPOCH 3506
2024-02-10 13:57:26,036 Epoch 3506: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-10 13:57:26,037 EPOCH 3507
2024-02-10 13:57:41,950 Epoch 3507: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 13:57:41,951 EPOCH 3508
2024-02-10 13:57:58,087 Epoch 3508: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 13:57:58,087 EPOCH 3509
2024-02-10 13:58:14,187 Epoch 3509: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 13:58:14,187 EPOCH 3510
2024-02-10 13:58:29,951 Epoch 3510: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 13:58:29,952 EPOCH 3511
2024-02-10 13:58:46,273 Epoch 3511: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 13:58:46,274 EPOCH 3512
2024-02-10 13:58:46,590 [Epoch: 3512 Step: 00031600] Batch Recognition Loss:   0.000318 => Gls Tokens per Sec:     4051 || Batch Translation Loss:   0.014346 => Txt Tokens per Sec:     8975 || Lr: 0.000100
2024-02-10 13:59:02,202 Epoch 3512: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 13:59:02,203 EPOCH 3513
2024-02-10 13:59:18,518 Epoch 3513: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 13:59:18,518 EPOCH 3514
2024-02-10 13:59:34,922 Epoch 3514: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 13:59:34,923 EPOCH 3515
2024-02-10 13:59:51,067 Epoch 3515: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 13:59:51,068 EPOCH 3516
2024-02-10 14:00:06,846 Epoch 3516: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 14:00:06,846 EPOCH 3517
2024-02-10 14:00:22,860 Epoch 3517: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 14:00:22,861 EPOCH 3518
2024-02-10 14:00:39,128 Epoch 3518: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 14:00:39,128 EPOCH 3519
2024-02-10 14:00:55,577 Epoch 3519: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 14:00:55,578 EPOCH 3520
2024-02-10 14:01:11,526 Epoch 3520: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:01:11,527 EPOCH 3521
2024-02-10 14:01:27,603 Epoch 3521: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:01:27,603 EPOCH 3522
2024-02-10 14:01:43,765 Epoch 3522: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:01:43,766 EPOCH 3523
2024-02-10 14:01:48,320 [Epoch: 3523 Step: 00031700] Batch Recognition Loss:   0.000199 => Gls Tokens per Sec:      365 || Batch Translation Loss:   0.006169 => Txt Tokens per Sec:      800 || Lr: 0.000100
2024-02-10 14:01:59,489 Epoch 3523: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:01:59,489 EPOCH 3524
2024-02-10 14:02:15,839 Epoch 3524: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:02:15,839 EPOCH 3525
2024-02-10 14:02:31,849 Epoch 3525: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:02:31,850 EPOCH 3526
2024-02-10 14:02:47,936 Epoch 3526: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:02:47,936 EPOCH 3527
2024-02-10 14:03:04,183 Epoch 3527: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 14:03:04,184 EPOCH 3528
2024-02-10 14:03:20,309 Epoch 3528: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 14:03:20,309 EPOCH 3529
2024-02-10 14:03:36,439 Epoch 3529: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 14:03:36,439 EPOCH 3530
2024-02-10 14:03:52,667 Epoch 3530: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:03:52,667 EPOCH 3531
2024-02-10 14:04:08,982 Epoch 3531: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 14:04:08,982 EPOCH 3532
2024-02-10 14:04:25,276 Epoch 3532: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:04:25,276 EPOCH 3533
2024-02-10 14:04:41,367 Epoch 3533: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 14:04:41,368 EPOCH 3534
2024-02-10 14:04:42,244 [Epoch: 3534 Step: 00031800] Batch Recognition Loss:   0.000495 => Gls Tokens per Sec:     4389 || Batch Translation Loss:   0.008562 => Txt Tokens per Sec:     9617 || Lr: 0.000100
2024-02-10 14:04:57,361 Epoch 3534: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 14:04:57,362 EPOCH 3535
2024-02-10 14:05:13,129 Epoch 3535: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 14:05:13,129 EPOCH 3536
2024-02-10 14:05:29,053 Epoch 3536: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 14:05:29,053 EPOCH 3537
2024-02-10 14:05:45,045 Epoch 3537: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 14:05:45,046 EPOCH 3538
2024-02-10 14:06:01,057 Epoch 3538: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 14:06:01,058 EPOCH 3539
2024-02-10 14:06:17,059 Epoch 3539: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 14:06:17,060 EPOCH 3540
2024-02-10 14:06:33,175 Epoch 3540: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-10 14:06:33,176 EPOCH 3541
2024-02-10 14:06:49,368 Epoch 3541: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-10 14:06:49,368 EPOCH 3542
2024-02-10 14:07:05,423 Epoch 3542: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-10 14:07:05,424 EPOCH 3543
2024-02-10 14:07:21,469 Epoch 3543: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-10 14:07:21,469 EPOCH 3544
2024-02-10 14:07:37,433 Epoch 3544: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-10 14:07:37,433 EPOCH 3545
2024-02-10 14:07:48,878 [Epoch: 3545 Step: 00031900] Batch Recognition Loss:   0.000415 => Gls Tokens per Sec:      369 || Batch Translation Loss:   0.037965 => Txt Tokens per Sec:     1093 || Lr: 0.000100
2024-02-10 14:07:53,718 Epoch 3545: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-10 14:07:53,718 EPOCH 3546
2024-02-10 14:08:09,727 Epoch 3546: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-10 14:08:09,728 EPOCH 3547
2024-02-10 14:08:25,731 Epoch 3547: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.75 
2024-02-10 14:08:25,732 EPOCH 3548
2024-02-10 14:08:41,958 Epoch 3548: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.21 
2024-02-10 14:08:41,959 EPOCH 3549
2024-02-10 14:08:57,759 Epoch 3549: Total Training Recognition Loss 0.02  Total Training Translation Loss 3.88 
2024-02-10 14:08:57,760 EPOCH 3550
2024-02-10 14:09:13,656 Epoch 3550: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.00 
2024-02-10 14:09:13,657 EPOCH 3551
2024-02-10 14:09:29,864 Epoch 3551: Total Training Recognition Loss 0.05  Total Training Translation Loss 3.72 
2024-02-10 14:09:29,865 EPOCH 3552
2024-02-10 14:09:45,898 Epoch 3552: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.07 
2024-02-10 14:09:45,899 EPOCH 3553
2024-02-10 14:10:02,087 Epoch 3553: Total Training Recognition Loss 0.07  Total Training Translation Loss 6.51 
2024-02-10 14:10:02,088 EPOCH 3554
2024-02-10 14:10:18,067 Epoch 3554: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.38 
2024-02-10 14:10:18,067 EPOCH 3555
2024-02-10 14:10:34,252 Epoch 3555: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.43 
2024-02-10 14:10:34,253 EPOCH 3556
2024-02-10 14:10:40,371 [Epoch: 3556 Step: 00032000] Batch Recognition Loss:   0.001786 => Gls Tokens per Sec:      899 || Batch Translation Loss:   0.082868 => Txt Tokens per Sec:     2570 || Lr: 0.000100
2024-02-10 14:11:52,319 Validation result at epoch 3556, step    32000: duration: 71.9464s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.27637	Translation Loss: 95558.33594	PPL: 13965.98730
	Eval Metric: BLEU
	WER 2.82	(DEL: 0.00,	INS: 0.00,	SUB: 2.82)
	BLEU-4 0.50	(BLEU-1: 11.62,	BLEU-2: 3.68,	BLEU-3: 1.31,	BLEU-4: 0.50)
	CHRF 16.80	ROUGE 9.53
2024-02-10 14:11:52,321 Logging Recognition and Translation Outputs
2024-02-10 14:11:52,321 ========================================================================================================================
2024-02-10 14:11:52,321 Logging Sequence: 162_133.00
2024-02-10 14:11:52,322 	Gloss Reference :	A B+C+D+E
2024-02-10 14:11:52,322 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 14:11:52,322 	Gloss Alignment :	         
2024-02-10 14:11:52,322 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 14:11:52,323 	Text Reference  :	***** *** * **** they          also        sent rape threats to his 9-month old daughter
2024-02-10 14:11:52,323 	Text Hypothesis :	kohli has a such technological advancement in   ipl  which   is the right   pad first   
2024-02-10 14:11:52,324 	Text Alignment  :	I     I   I I    S             S           S    S    S       S  S   S       S   S       
2024-02-10 14:11:52,324 ========================================================================================================================
2024-02-10 14:11:52,324 Logging Sequence: 134_236.00
2024-02-10 14:11:52,324 	Gloss Reference :	A B+C+D+E
2024-02-10 14:11:52,324 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 14:11:52,324 	Gloss Alignment :	         
2024-02-10 14:11:52,325 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 14:11:52,326 	Text Reference  :	after   the  interaction modi  tweeted the ******** ** *** images and captioned it   saying
2024-02-10 14:11:52,326 	Text Hypothesis :	indians were very        happy of      the athletes at his goals  he  played    very well  
2024-02-10 14:11:52,326 	Text Alignment  :	S       S    S           S     S           I        I  I   S      S   S         S    S     
2024-02-10 14:11:52,326 ========================================================================================================================
2024-02-10 14:11:52,326 Logging Sequence: 145_52.00
2024-02-10 14:11:52,327 	Gloss Reference :	A B+C+D+E
2024-02-10 14:11:52,327 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 14:11:52,327 	Gloss Alignment :	         
2024-02-10 14:11:52,327 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 14:11:52,328 	Text Reference  :	her name was dropped despite having qualified as    she  was the      only female athlete
2024-02-10 14:11:52,328 	Text Hypothesis :	*** **** *** the     finals  were   made      india with a   talented with his    famous 
2024-02-10 14:11:52,328 	Text Alignment  :	D   D    D   S       S       S      S         S     S    S   S        S    S      S      
2024-02-10 14:11:52,328 ========================================================================================================================
2024-02-10 14:11:52,329 Logging Sequence: 175_40.00
2024-02-10 14:11:52,329 	Gloss Reference :	A B+C+D+E
2024-02-10 14:11:52,329 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 14:11:52,329 	Gloss Alignment :	         
2024-02-10 14:11:52,329 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 14:11:52,331 	Text Reference  :	*** ** **** ***** soumyadeep and     shreya bagged three medals each including a silver medal ** *** ***** each
2024-02-10 14:11:52,331 	Text Hypothesis :	now we have taken it's       revenge by     this   is    why    he   won       a gold   medal in the world cup 
2024-02-10 14:11:52,331 	Text Alignment  :	I   I  I    I     S          S       S      S      S     S      S    S           S            I  I   I     S   
2024-02-10 14:11:52,331 ========================================================================================================================
2024-02-10 14:11:52,331 Logging Sequence: 156_51.00
2024-02-10 14:11:52,332 	Gloss Reference :	A B+C+D+E
2024-02-10 14:11:52,332 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 14:11:52,332 	Gloss Alignment :	         
2024-02-10 14:11:52,332 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 14:11:52,333 	Text Reference  :	** *** ** the  selection of      the players was similar to that of  ipl 
2024-02-10 14:11:52,333 	Text Hypothesis :	in ipl we have been      friends and i       am  sure    to see  the post
2024-02-10 14:11:52,333 	Text Alignment  :	I  I   I  S    S         S       S   S       S   S          S    S   S   
2024-02-10 14:11:52,334 ========================================================================================================================
2024-02-10 14:12:02,458 Epoch 3556: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.83 
2024-02-10 14:12:02,459 EPOCH 3557
2024-02-10 14:12:18,635 Epoch 3557: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-10 14:12:18,636 EPOCH 3558
2024-02-10 14:12:34,763 Epoch 3558: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-10 14:12:34,763 EPOCH 3559
2024-02-10 14:12:51,061 Epoch 3559: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 14:12:51,062 EPOCH 3560
2024-02-10 14:13:07,609 Epoch 3560: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 14:13:07,610 EPOCH 3561
2024-02-10 14:13:23,682 Epoch 3561: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 14:13:23,683 EPOCH 3562
2024-02-10 14:13:39,838 Epoch 3562: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 14:13:39,838 EPOCH 3563
2024-02-10 14:13:55,993 Epoch 3563: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 14:13:55,993 EPOCH 3564
2024-02-10 14:14:12,300 Epoch 3564: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 14:14:12,300 EPOCH 3565
2024-02-10 14:14:28,336 Epoch 3565: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 14:14:28,337 EPOCH 3566
2024-02-10 14:14:44,043 Epoch 3566: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 14:14:44,043 EPOCH 3567
2024-02-10 14:14:58,891 [Epoch: 3567 Step: 00032100] Batch Recognition Loss:   0.000562 => Gls Tokens per Sec:      457 || Batch Translation Loss:   0.020102 => Txt Tokens per Sec:     1305 || Lr: 0.000100
2024-02-10 14:15:00,369 Epoch 3567: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 14:15:00,369 EPOCH 3568
2024-02-10 14:15:16,461 Epoch 3568: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 14:15:16,462 EPOCH 3569
2024-02-10 14:15:32,363 Epoch 3569: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 14:15:32,364 EPOCH 3570
2024-02-10 14:15:48,737 Epoch 3570: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 14:15:48,737 EPOCH 3571
2024-02-10 14:16:04,976 Epoch 3571: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 14:16:04,976 EPOCH 3572
2024-02-10 14:16:21,053 Epoch 3572: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 14:16:21,054 EPOCH 3573
2024-02-10 14:16:37,539 Epoch 3573: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 14:16:37,539 EPOCH 3574
2024-02-10 14:16:53,820 Epoch 3574: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 14:16:53,820 EPOCH 3575
2024-02-10 14:17:09,941 Epoch 3575: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 14:17:09,942 EPOCH 3576
2024-02-10 14:17:26,048 Epoch 3576: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 14:17:26,048 EPOCH 3577
2024-02-10 14:17:42,074 Epoch 3577: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 14:17:42,074 EPOCH 3578
2024-02-10 14:17:51,836 [Epoch: 3578 Step: 00032200] Batch Recognition Loss:   0.000311 => Gls Tokens per Sec:      826 || Batch Translation Loss:   0.015719 => Txt Tokens per Sec:     2322 || Lr: 0.000100
2024-02-10 14:17:58,031 Epoch 3578: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 14:17:58,032 EPOCH 3579
2024-02-10 14:18:13,936 Epoch 3579: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 14:18:13,936 EPOCH 3580
2024-02-10 14:18:30,055 Epoch 3580: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 14:18:30,056 EPOCH 3581
2024-02-10 14:18:45,952 Epoch 3581: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 14:18:45,952 EPOCH 3582
2024-02-10 14:19:01,862 Epoch 3582: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 14:19:01,863 EPOCH 3583
2024-02-10 14:19:18,235 Epoch 3583: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 14:19:18,236 EPOCH 3584
2024-02-10 14:19:34,233 Epoch 3584: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 14:19:34,234 EPOCH 3585
2024-02-10 14:19:50,289 Epoch 3585: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 14:19:50,290 EPOCH 3586
2024-02-10 14:20:06,492 Epoch 3586: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:20:06,493 EPOCH 3587
2024-02-10 14:20:22,606 Epoch 3587: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:20:22,606 EPOCH 3588
2024-02-10 14:20:38,737 Epoch 3588: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:20:38,738 EPOCH 3589
2024-02-10 14:20:48,975 [Epoch: 3589 Step: 00032300] Batch Recognition Loss:   0.000249 => Gls Tokens per Sec:      912 || Batch Translation Loss:   0.012563 => Txt Tokens per Sec:     2440 || Lr: 0.000100
2024-02-10 14:20:54,635 Epoch 3589: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:20:54,635 EPOCH 3590
2024-02-10 14:21:10,686 Epoch 3590: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:21:10,687 EPOCH 3591
2024-02-10 14:21:27,731 Epoch 3591: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:21:27,731 EPOCH 3592
2024-02-10 14:21:43,282 Epoch 3592: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:21:43,283 EPOCH 3593
2024-02-10 14:21:59,814 Epoch 3593: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:21:59,814 EPOCH 3594
2024-02-10 14:22:15,937 Epoch 3594: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:22:15,937 EPOCH 3595
2024-02-10 14:22:31,965 Epoch 3595: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:22:31,965 EPOCH 3596
2024-02-10 14:22:48,401 Epoch 3596: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:22:48,402 EPOCH 3597
2024-02-10 14:23:04,496 Epoch 3597: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:23:04,497 EPOCH 3598
2024-02-10 14:23:20,682 Epoch 3598: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:23:20,683 EPOCH 3599
2024-02-10 14:23:36,634 Epoch 3599: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:23:36,635 EPOCH 3600
2024-02-10 14:23:52,834 [Epoch: 3600 Step: 00032400] Batch Recognition Loss:   0.000668 => Gls Tokens per Sec:      656 || Batch Translation Loss:   0.007463 => Txt Tokens per Sec:     1814 || Lr: 0.000100
2024-02-10 14:23:52,834 Epoch 3600: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:23:52,834 EPOCH 3601
2024-02-10 14:24:08,921 Epoch 3601: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:24:08,921 EPOCH 3602
2024-02-10 14:24:24,900 Epoch 3602: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:24:24,901 EPOCH 3603
2024-02-10 14:24:40,761 Epoch 3603: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:24:40,762 EPOCH 3604
2024-02-10 14:24:57,021 Epoch 3604: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:24:57,022 EPOCH 3605
2024-02-10 14:25:13,547 Epoch 3605: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:25:13,547 EPOCH 3606
2024-02-10 14:25:29,600 Epoch 3606: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:25:29,601 EPOCH 3607
2024-02-10 14:25:45,657 Epoch 3607: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:25:45,658 EPOCH 3608
2024-02-10 14:26:01,507 Epoch 3608: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:26:01,508 EPOCH 3609
2024-02-10 14:26:17,509 Epoch 3609: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:26:17,510 EPOCH 3610
2024-02-10 14:26:33,883 Epoch 3610: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:26:33,883 EPOCH 3611
2024-02-10 14:26:49,854 Epoch 3611: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:26:49,854 EPOCH 3612
2024-02-10 14:26:50,244 [Epoch: 3612 Step: 00032500] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:     3290 || Batch Translation Loss:   0.010945 => Txt Tokens per Sec:     9015 || Lr: 0.000100
2024-02-10 14:27:05,893 Epoch 3612: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:27:05,893 EPOCH 3613
2024-02-10 14:27:22,363 Epoch 3613: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:27:22,364 EPOCH 3614
2024-02-10 14:27:38,402 Epoch 3614: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:27:38,402 EPOCH 3615
2024-02-10 14:27:54,347 Epoch 3615: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-10 14:27:54,347 EPOCH 3616
2024-02-10 14:28:10,371 Epoch 3616: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-10 14:28:10,371 EPOCH 3617
2024-02-10 14:28:26,492 Epoch 3617: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 14:28:26,493 EPOCH 3618
2024-02-10 14:28:42,425 Epoch 3618: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-10 14:28:42,426 EPOCH 3619
2024-02-10 14:28:58,386 Epoch 3619: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 14:28:58,387 EPOCH 3620
2024-02-10 14:29:14,787 Epoch 3620: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 14:29:14,788 EPOCH 3621
2024-02-10 14:29:30,626 Epoch 3621: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 14:29:30,627 EPOCH 3622
2024-02-10 14:29:46,959 Epoch 3622: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 14:29:46,960 EPOCH 3623
2024-02-10 14:29:51,868 [Epoch: 3623 Step: 00032600] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:      338 || Batch Translation Loss:   0.006599 => Txt Tokens per Sec:     1069 || Lr: 0.000100
2024-02-10 14:30:03,136 Epoch 3623: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:30:03,136 EPOCH 3624
2024-02-10 14:30:19,261 Epoch 3624: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:30:19,262 EPOCH 3625
2024-02-10 14:30:35,716 Epoch 3625: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:30:35,716 EPOCH 3626
2024-02-10 14:30:52,224 Epoch 3626: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:30:52,224 EPOCH 3627
2024-02-10 14:31:08,491 Epoch 3627: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:31:08,492 EPOCH 3628
2024-02-10 14:31:24,460 Epoch 3628: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:31:24,460 EPOCH 3629
2024-02-10 14:31:40,091 Epoch 3629: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:31:40,092 EPOCH 3630
2024-02-10 14:31:56,577 Epoch 3630: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:31:56,578 EPOCH 3631
2024-02-10 14:32:13,558 Epoch 3631: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:32:13,559 EPOCH 3632
2024-02-10 14:32:29,601 Epoch 3632: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:32:29,602 EPOCH 3633
2024-02-10 14:32:45,583 Epoch 3633: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:32:45,584 EPOCH 3634
2024-02-10 14:32:50,570 [Epoch: 3634 Step: 00032700] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:      590 || Batch Translation Loss:   0.005890 => Txt Tokens per Sec:     1633 || Lr: 0.000100
2024-02-10 14:33:01,501 Epoch 3634: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:33:01,502 EPOCH 3635
2024-02-10 14:33:17,665 Epoch 3635: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:33:17,666 EPOCH 3636
2024-02-10 14:33:33,757 Epoch 3636: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:33:33,758 EPOCH 3637
2024-02-10 14:33:49,873 Epoch 3637: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 14:33:49,873 EPOCH 3638
2024-02-10 14:34:05,815 Epoch 3638: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-10 14:34:05,816 EPOCH 3639
2024-02-10 14:34:21,990 Epoch 3639: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-10 14:34:21,990 EPOCH 3640
2024-02-10 14:34:37,897 Epoch 3640: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-10 14:34:37,897 EPOCH 3641
2024-02-10 14:34:54,261 Epoch 3641: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-10 14:34:54,262 EPOCH 3642
2024-02-10 14:35:10,049 Epoch 3642: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.56 
2024-02-10 14:35:10,050 EPOCH 3643
2024-02-10 14:35:26,129 Epoch 3643: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-10 14:35:26,130 EPOCH 3644
2024-02-10 14:35:42,405 Epoch 3644: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-10 14:35:42,405 EPOCH 3645
2024-02-10 14:35:47,917 [Epoch: 3645 Step: 00032800] Batch Recognition Loss:   0.000522 => Gls Tokens per Sec:      766 || Batch Translation Loss:   0.051031 => Txt Tokens per Sec:     2071 || Lr: 0.000100
2024-02-10 14:35:58,570 Epoch 3645: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-10 14:35:58,570 EPOCH 3646
2024-02-10 14:36:14,866 Epoch 3646: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-10 14:36:14,867 EPOCH 3647
2024-02-10 14:36:30,957 Epoch 3647: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 14:36:30,958 EPOCH 3648
2024-02-10 14:36:47,119 Epoch 3648: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 14:36:47,120 EPOCH 3649
2024-02-10 14:37:03,124 Epoch 3649: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 14:37:03,124 EPOCH 3650
2024-02-10 14:37:18,949 Epoch 3650: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 14:37:18,950 EPOCH 3651
2024-02-10 14:37:35,191 Epoch 3651: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 14:37:35,191 EPOCH 3652
2024-02-10 14:37:51,340 Epoch 3652: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 14:37:51,340 EPOCH 3653
2024-02-10 14:38:07,273 Epoch 3653: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 14:38:07,274 EPOCH 3654
2024-02-10 14:38:23,228 Epoch 3654: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 14:38:23,228 EPOCH 3655
2024-02-10 14:38:39,124 Epoch 3655: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 14:38:39,125 EPOCH 3656
2024-02-10 14:38:45,303 [Epoch: 3656 Step: 00032900] Batch Recognition Loss:   0.000201 => Gls Tokens per Sec:      890 || Batch Translation Loss:   0.009371 => Txt Tokens per Sec:     2207 || Lr: 0.000100
2024-02-10 14:38:55,411 Epoch 3656: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 14:38:55,412 EPOCH 3657
2024-02-10 14:39:11,777 Epoch 3657: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 14:39:11,778 EPOCH 3658
2024-02-10 14:39:27,667 Epoch 3658: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 14:39:27,667 EPOCH 3659
2024-02-10 14:39:44,042 Epoch 3659: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 14:39:44,042 EPOCH 3660
2024-02-10 14:40:00,195 Epoch 3660: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 14:40:00,196 EPOCH 3661
2024-02-10 14:40:16,586 Epoch 3661: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 14:40:16,586 EPOCH 3662
2024-02-10 14:40:32,903 Epoch 3662: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 14:40:32,903 EPOCH 3663
2024-02-10 14:40:49,221 Epoch 3663: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 14:40:49,221 EPOCH 3664
2024-02-10 14:41:05,626 Epoch 3664: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 14:41:05,627 EPOCH 3665
2024-02-10 14:41:21,878 Epoch 3665: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 14:41:21,878 EPOCH 3666
2024-02-10 14:41:38,005 Epoch 3666: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 14:41:38,005 EPOCH 3667
2024-02-10 14:41:49,280 [Epoch: 3667 Step: 00033000] Batch Recognition Loss:   0.000309 => Gls Tokens per Sec:      681 || Batch Translation Loss:   0.017737 => Txt Tokens per Sec:     2028 || Lr: 0.000100
2024-02-10 14:41:54,210 Epoch 3667: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 14:41:54,211 EPOCH 3668
2024-02-10 14:42:10,158 Epoch 3668: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 14:42:10,159 EPOCH 3669
2024-02-10 14:42:26,545 Epoch 3669: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 14:42:26,546 EPOCH 3670
2024-02-10 14:42:42,608 Epoch 3670: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 14:42:42,609 EPOCH 3671
2024-02-10 14:42:58,691 Epoch 3671: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:42:58,692 EPOCH 3672
2024-02-10 14:43:15,123 Epoch 3672: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 14:43:15,124 EPOCH 3673
2024-02-10 14:43:31,180 Epoch 3673: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 14:43:31,180 EPOCH 3674
2024-02-10 14:43:46,983 Epoch 3674: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-10 14:43:46,984 EPOCH 3675
2024-02-10 14:44:03,031 Epoch 3675: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.02 
2024-02-10 14:44:03,032 EPOCH 3676
2024-02-10 14:44:19,079 Epoch 3676: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.63 
2024-02-10 14:44:19,079 EPOCH 3677
2024-02-10 14:44:35,433 Epoch 3677: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.33 
2024-02-10 14:44:35,434 EPOCH 3678
2024-02-10 14:44:45,029 [Epoch: 3678 Step: 00033100] Batch Recognition Loss:   0.007266 => Gls Tokens per Sec:      840 || Batch Translation Loss:   0.383648 => Txt Tokens per Sec:     2200 || Lr: 0.000100
2024-02-10 14:44:51,470 Epoch 3678: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.29 
2024-02-10 14:44:51,471 EPOCH 3679
2024-02-10 14:45:07,664 Epoch 3679: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.21 
2024-02-10 14:45:07,664 EPOCH 3680
2024-02-10 14:45:23,750 Epoch 3680: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.79 
2024-02-10 14:45:23,751 EPOCH 3681
2024-02-10 14:45:39,847 Epoch 3681: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-10 14:45:39,848 EPOCH 3682
2024-02-10 14:45:55,845 Epoch 3682: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-10 14:45:55,845 EPOCH 3683
2024-02-10 14:46:11,889 Epoch 3683: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-10 14:46:11,890 EPOCH 3684
2024-02-10 14:46:28,096 Epoch 3684: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 14:46:28,097 EPOCH 3685
2024-02-10 14:46:44,329 Epoch 3685: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 14:46:44,330 EPOCH 3686
2024-02-10 14:47:00,351 Epoch 3686: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 14:47:00,351 EPOCH 3687
2024-02-10 14:47:16,338 Epoch 3687: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 14:47:16,338 EPOCH 3688
2024-02-10 14:47:33,016 Epoch 3688: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 14:47:33,017 EPOCH 3689
2024-02-10 14:47:48,193 [Epoch: 3689 Step: 00033200] Batch Recognition Loss:   0.000502 => Gls Tokens per Sec:      615 || Batch Translation Loss:   0.022555 => Txt Tokens per Sec:     1679 || Lr: 0.000100
2024-02-10 14:47:48,896 Epoch 3689: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 14:47:48,896 EPOCH 3690
2024-02-10 14:48:05,277 Epoch 3690: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 14:48:05,278 EPOCH 3691
2024-02-10 14:48:21,129 Epoch 3691: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 14:48:21,130 EPOCH 3692
2024-02-10 14:48:37,180 Epoch 3692: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 14:48:37,180 EPOCH 3693
2024-02-10 14:48:53,090 Epoch 3693: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 14:48:53,091 EPOCH 3694
2024-02-10 14:49:09,280 Epoch 3694: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 14:49:09,281 EPOCH 3695
2024-02-10 14:49:25,321 Epoch 3695: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 14:49:25,321 EPOCH 3696
2024-02-10 14:49:41,065 Epoch 3696: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 14:49:41,065 EPOCH 3697
2024-02-10 14:49:57,110 Epoch 3697: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 14:49:57,111 EPOCH 3698
2024-02-10 14:50:13,269 Epoch 3698: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 14:50:13,270 EPOCH 3699
2024-02-10 14:50:29,079 Epoch 3699: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 14:50:29,080 EPOCH 3700
2024-02-10 14:50:44,974 [Epoch: 3700 Step: 00033300] Batch Recognition Loss:   0.000346 => Gls Tokens per Sec:      668 || Batch Translation Loss:   0.006982 => Txt Tokens per Sec:     1849 || Lr: 0.000100
2024-02-10 14:50:44,975 Epoch 3700: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 14:50:44,975 EPOCH 3701
2024-02-10 14:51:00,861 Epoch 3701: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 14:51:00,862 EPOCH 3702
2024-02-10 14:51:17,081 Epoch 3702: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 14:51:17,082 EPOCH 3703
2024-02-10 14:51:33,032 Epoch 3703: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:51:33,033 EPOCH 3704
2024-02-10 14:51:49,102 Epoch 3704: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:51:49,103 EPOCH 3705
2024-02-10 14:52:04,770 Epoch 3705: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 14:52:04,770 EPOCH 3706
2024-02-10 14:52:21,073 Epoch 3706: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 14:52:21,074 EPOCH 3707
2024-02-10 14:52:37,475 Epoch 3707: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 14:52:37,476 EPOCH 3708
2024-02-10 14:52:53,486 Epoch 3708: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 14:52:53,486 EPOCH 3709
2024-02-10 14:53:09,602 Epoch 3709: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:53:09,602 EPOCH 3710
2024-02-10 14:53:25,880 Epoch 3710: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:53:25,880 EPOCH 3711
2024-02-10 14:53:42,118 Epoch 3711: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:53:42,118 EPOCH 3712
2024-02-10 14:53:42,569 [Epoch: 3712 Step: 00033400] Batch Recognition Loss:   0.000186 => Gls Tokens per Sec:     2844 || Batch Translation Loss:   0.015199 => Txt Tokens per Sec:     7796 || Lr: 0.000100
2024-02-10 14:53:58,096 Epoch 3712: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:53:58,096 EPOCH 3713
2024-02-10 14:54:14,263 Epoch 3713: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 14:54:14,263 EPOCH 3714
2024-02-10 14:54:30,773 Epoch 3714: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 14:54:30,774 EPOCH 3715
2024-02-10 14:54:46,941 Epoch 3715: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:54:46,941 EPOCH 3716
2024-02-10 14:55:02,939 Epoch 3716: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:55:02,939 EPOCH 3717
2024-02-10 14:55:18,896 Epoch 3717: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:55:18,897 EPOCH 3718
2024-02-10 14:55:34,827 Epoch 3718: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:55:34,827 EPOCH 3719
2024-02-10 14:55:50,686 Epoch 3719: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:55:50,687 EPOCH 3720
2024-02-10 14:56:06,774 Epoch 3720: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:56:06,775 EPOCH 3721
2024-02-10 14:56:23,086 Epoch 3721: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:56:23,087 EPOCH 3722
2024-02-10 14:56:39,025 Epoch 3722: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:56:39,025 EPOCH 3723
2024-02-10 14:56:45,258 [Epoch: 3723 Step: 00033500] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:      411 || Batch Translation Loss:   0.010745 => Txt Tokens per Sec:     1234 || Lr: 0.000100
2024-02-10 14:56:55,164 Epoch 3723: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 14:56:55,164 EPOCH 3724
2024-02-10 14:57:11,027 Epoch 3724: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:57:11,027 EPOCH 3725
2024-02-10 14:57:27,267 Epoch 3725: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:57:27,267 EPOCH 3726
2024-02-10 14:57:42,998 Epoch 3726: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:57:42,999 EPOCH 3727
2024-02-10 14:57:58,958 Epoch 3727: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:57:58,959 EPOCH 3728
2024-02-10 14:58:14,927 Epoch 3728: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:58:14,927 EPOCH 3729
2024-02-10 14:58:30,794 Epoch 3729: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:58:30,795 EPOCH 3730
2024-02-10 14:58:46,782 Epoch 3730: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:58:46,783 EPOCH 3731
2024-02-10 14:59:02,896 Epoch 3731: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:59:02,897 EPOCH 3732
2024-02-10 14:59:18,854 Epoch 3732: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 14:59:18,854 EPOCH 3733
2024-02-10 14:59:35,116 Epoch 3733: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:59:35,117 EPOCH 3734
2024-02-10 14:59:40,116 [Epoch: 3734 Step: 00033600] Batch Recognition Loss:   0.000257 => Gls Tokens per Sec:      588 || Batch Translation Loss:   0.011217 => Txt Tokens per Sec:     1629 || Lr: 0.000100
2024-02-10 14:59:51,134 Epoch 3734: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 14:59:51,135 EPOCH 3735
2024-02-10 15:00:06,988 Epoch 3735: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:00:06,988 EPOCH 3736
2024-02-10 15:00:22,756 Epoch 3736: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:00:22,757 EPOCH 3737
2024-02-10 15:00:38,766 Epoch 3737: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:00:38,767 EPOCH 3738
2024-02-10 15:00:54,992 Epoch 3738: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:00:54,993 EPOCH 3739
2024-02-10 15:01:10,870 Epoch 3739: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 15:01:10,870 EPOCH 3740
2024-02-10 15:01:26,936 Epoch 3740: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 15:01:26,936 EPOCH 3741
2024-02-10 15:01:43,056 Epoch 3741: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 15:01:43,056 EPOCH 3742
2024-02-10 15:01:59,083 Epoch 3742: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 15:01:59,084 EPOCH 3743
2024-02-10 15:02:15,027 Epoch 3743: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 15:02:15,028 EPOCH 3744
2024-02-10 15:02:30,943 Epoch 3744: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 15:02:30,943 EPOCH 3745
2024-02-10 15:02:35,400 [Epoch: 3745 Step: 00033700] Batch Recognition Loss:   0.001144 => Gls Tokens per Sec:     1149 || Batch Translation Loss:   0.014543 => Txt Tokens per Sec:     2896 || Lr: 0.000100
2024-02-10 15:02:47,112 Epoch 3745: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 15:02:47,112 EPOCH 3746
2024-02-10 15:03:03,143 Epoch 3746: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 15:03:03,143 EPOCH 3747
2024-02-10 15:03:19,066 Epoch 3747: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 15:03:19,066 EPOCH 3748
2024-02-10 15:03:35,009 Epoch 3748: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-10 15:03:35,010 EPOCH 3749
2024-02-10 15:03:50,877 Epoch 3749: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.59 
2024-02-10 15:03:50,878 EPOCH 3750
2024-02-10 15:04:07,287 Epoch 3750: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-10 15:04:07,288 EPOCH 3751
2024-02-10 15:04:23,690 Epoch 3751: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-10 15:04:23,691 EPOCH 3752
2024-02-10 15:04:39,679 Epoch 3752: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.20 
2024-02-10 15:04:39,680 EPOCH 3753
2024-02-10 15:04:56,030 Epoch 3753: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.07 
2024-02-10 15:04:56,031 EPOCH 3754
2024-02-10 15:05:11,952 Epoch 3754: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-10 15:05:11,952 EPOCH 3755
2024-02-10 15:05:27,899 Epoch 3755: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-10 15:05:27,900 EPOCH 3756
2024-02-10 15:05:33,101 [Epoch: 3756 Step: 00033800] Batch Recognition Loss:   0.000647 => Gls Tokens per Sec:     1231 || Batch Translation Loss:   0.075605 => Txt Tokens per Sec:     3370 || Lr: 0.000100
2024-02-10 15:05:43,724 Epoch 3756: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.69 
2024-02-10 15:05:43,725 EPOCH 3757
2024-02-10 15:06:00,106 Epoch 3757: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-10 15:06:00,107 EPOCH 3758
2024-02-10 15:06:16,334 Epoch 3758: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-10 15:06:16,334 EPOCH 3759
2024-02-10 15:06:32,320 Epoch 3759: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-10 15:06:32,320 EPOCH 3760
2024-02-10 15:06:48,579 Epoch 3760: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-10 15:06:48,579 EPOCH 3761
2024-02-10 15:07:04,988 Epoch 3761: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 15:07:04,988 EPOCH 3762
2024-02-10 15:07:21,183 Epoch 3762: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-10 15:07:21,184 EPOCH 3763
2024-02-10 15:07:37,061 Epoch 3763: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-10 15:07:37,062 EPOCH 3764
2024-02-10 15:07:53,194 Epoch 3764: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-10 15:07:53,195 EPOCH 3765
2024-02-10 15:08:09,208 Epoch 3765: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-10 15:08:09,209 EPOCH 3766
2024-02-10 15:08:25,468 Epoch 3766: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-10 15:08:25,468 EPOCH 3767
2024-02-10 15:08:37,601 [Epoch: 3767 Step: 00033900] Batch Recognition Loss:   0.001692 => Gls Tokens per Sec:      559 || Batch Translation Loss:   0.053683 => Txt Tokens per Sec:     1585 || Lr: 0.000100
2024-02-10 15:08:41,839 Epoch 3767: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 15:08:41,840 EPOCH 3768
2024-02-10 15:08:57,936 Epoch 3768: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-10 15:08:57,936 EPOCH 3769
2024-02-10 15:09:14,070 Epoch 3769: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 15:09:14,071 EPOCH 3770
2024-02-10 15:09:30,116 Epoch 3770: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-10 15:09:30,116 EPOCH 3771
2024-02-10 15:09:46,197 Epoch 3771: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 15:09:46,198 EPOCH 3772
2024-02-10 15:10:02,060 Epoch 3772: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 15:10:02,060 EPOCH 3773
2024-02-10 15:10:18,159 Epoch 3773: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 15:10:18,159 EPOCH 3774
2024-02-10 15:10:34,269 Epoch 3774: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 15:10:34,270 EPOCH 3775
2024-02-10 15:10:50,359 Epoch 3775: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 15:10:50,360 EPOCH 3776
2024-02-10 15:11:06,231 Epoch 3776: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 15:11:06,231 EPOCH 3777
2024-02-10 15:11:22,077 Epoch 3777: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 15:11:22,077 EPOCH 3778
2024-02-10 15:11:37,472 [Epoch: 3778 Step: 00034000] Batch Recognition Loss:   0.000606 => Gls Tokens per Sec:      524 || Batch Translation Loss:   0.013680 => Txt Tokens per Sec:     1548 || Lr: 0.000100
2024-02-10 15:12:48,922 Validation result at epoch 3778, step    34000: duration: 71.4493s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.31221	Translation Loss: 97719.20312	PPL: 17330.23828
	Eval Metric: BLEU
	WER 2.97	(DEL: 0.00,	INS: 0.00,	SUB: 2.97)
	BLEU-4 0.39	(BLEU-1: 10.11,	BLEU-2: 3.03,	BLEU-3: 0.94,	BLEU-4: 0.39)
	CHRF 16.44	ROUGE 8.53
2024-02-10 15:12:48,925 Logging Recognition and Translation Outputs
2024-02-10 15:12:48,925 ========================================================================================================================
2024-02-10 15:12:48,925 Logging Sequence: 171_158.00
2024-02-10 15:12:48,925 	Gloss Reference :	A B+C+D+E
2024-02-10 15:12:48,925 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 15:12:48,926 	Gloss Alignment :	         
2024-02-10 15:12:48,926 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 15:12:48,927 	Text Reference  :	with speculations of dhoni being banned are spreading many   say  that   it is unlikely to  happen
2024-02-10 15:12:48,927 	Text Hypothesis :	**** ************ ** ***** ***** ****** *** shocking  people were hoping it ** does     not rain  
2024-02-10 15:12:48,927 	Text Alignment  :	D    D            D  D     D     D      D   S         S      S    S         D  S        S   S     
2024-02-10 15:12:48,927 ========================================================================================================================
2024-02-10 15:12:48,927 Logging Sequence: 108_235.00
2024-02-10 15:12:48,928 	Gloss Reference :	A B+C+D+E
2024-02-10 15:12:48,928 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 15:12:48,928 	Gloss Alignment :	         
2024-02-10 15:12:48,928 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 15:12:48,929 	Text Reference  :	he was taken to the hospital and    it was reported that he   is  not       in any danger
2024-02-10 15:12:48,929 	Text Hypothesis :	** *** ***** ** *** even     though he was ******** a    huge fan following in the 2020  
2024-02-10 15:12:48,929 	Text Alignment  :	D  D   D     D  D   S        S      S      D        S    S    S   S            S   S     
2024-02-10 15:12:48,930 ========================================================================================================================
2024-02-10 15:12:48,930 Logging Sequence: 153_206.00
2024-02-10 15:12:48,930 	Gloss Reference :	A B+C+D+E
2024-02-10 15:12:48,930 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 15:12:48,930 	Gloss Alignment :	         
2024-02-10 15:12:48,930 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 15:12:48,931 	Text Reference  :	*** ****** *** ** now       on   13th november everyone is     hoping pakistan rewrites history
2024-02-10 15:12:48,932 	Text Hypothesis :	the couple met in australia both men  and      south    africa during the      visuals  love   
2024-02-10 15:12:48,932 	Text Alignment  :	I   I      I   I  S         S    S    S        S        S      S      S        S        S      
2024-02-10 15:12:48,932 ========================================================================================================================
2024-02-10 15:12:48,932 Logging Sequence: 87_202.00
2024-02-10 15:12:48,932 	Gloss Reference :	A B+C+D+E
2024-02-10 15:12:48,932 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 15:12:48,932 	Gloss Alignment :	         
2024-02-10 15:12:48,933 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 15:12:48,934 	Text Reference  :	** *** **** **** ********* ******** i   love  our     players and   i    love my country
2024-02-10 15:12:48,934 	Text Hypothesis :	do you know that wikipedia provides was being jealous of      india with some of india  
2024-02-10 15:12:48,934 	Text Alignment  :	I  I   I    I    I         I        S   S     S       S       S     S    S    S  S      
2024-02-10 15:12:48,934 ========================================================================================================================
2024-02-10 15:12:48,934 Logging Sequence: 84_2.00
2024-02-10 15:12:48,934 	Gloss Reference :	A B+C+D+E
2024-02-10 15:12:48,935 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 15:12:48,935 	Gloss Alignment :	         
2024-02-10 15:12:48,935 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 15:12:48,937 	Text Reference  :	the 2022   fifa         football world  cup is    going on    in  qatar from    20th   november 2022    to    18th december 2022   
2024-02-10 15:12:48,937 	Text Hypothesis :	*** police instructions even     subway and viral have  taken for the   highest number of       omicron cases at   the      auction
2024-02-10 15:12:48,937 	Text Alignment  :	D   S      S            S        S      S   S     S     S     S   S     S       S      S        S       S     S    S        S      
2024-02-10 15:12:48,937 ========================================================================================================================
2024-02-10 15:12:49,815 Epoch 3778: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 15:12:49,815 EPOCH 3779
2024-02-10 15:13:06,542 Epoch 3779: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 15:13:06,543 EPOCH 3780
2024-02-10 15:13:22,966 Epoch 3780: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 15:13:22,966 EPOCH 3781
2024-02-10 15:13:39,006 Epoch 3781: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 15:13:39,006 EPOCH 3782
2024-02-10 15:13:54,963 Epoch 3782: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 15:13:54,963 EPOCH 3783
2024-02-10 15:14:11,100 Epoch 3783: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 15:14:11,101 EPOCH 3784
2024-02-10 15:14:27,337 Epoch 3784: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 15:14:27,338 EPOCH 3785
2024-02-10 15:14:43,428 Epoch 3785: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 15:14:43,428 EPOCH 3786
2024-02-10 15:14:59,371 Epoch 3786: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 15:14:59,371 EPOCH 3787
2024-02-10 15:15:15,805 Epoch 3787: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 15:15:15,805 EPOCH 3788
2024-02-10 15:15:31,527 Epoch 3788: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 15:15:31,527 EPOCH 3789
2024-02-10 15:15:43,267 [Epoch: 3789 Step: 00034100] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:      872 || Batch Translation Loss:   0.012240 => Txt Tokens per Sec:     2388 || Lr: 0.000100
2024-02-10 15:15:47,561 Epoch 3789: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 15:15:47,562 EPOCH 3790
2024-02-10 15:16:03,755 Epoch 3790: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:16:03,756 EPOCH 3791
2024-02-10 15:16:19,884 Epoch 3791: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:16:19,884 EPOCH 3792
2024-02-10 15:16:36,037 Epoch 3792: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:16:36,037 EPOCH 3793
2024-02-10 15:16:52,090 Epoch 3793: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:16:52,090 EPOCH 3794
2024-02-10 15:17:08,193 Epoch 3794: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:17:08,193 EPOCH 3795
2024-02-10 15:17:24,033 Epoch 3795: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 15:17:24,034 EPOCH 3796
2024-02-10 15:17:40,345 Epoch 3796: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:17:40,346 EPOCH 3797
2024-02-10 15:17:56,153 Epoch 3797: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:17:56,154 EPOCH 3798
2024-02-10 15:18:12,253 Epoch 3798: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:18:12,254 EPOCH 3799
2024-02-10 15:18:28,222 Epoch 3799: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-10 15:18:28,223 EPOCH 3800
2024-02-10 15:18:44,407 [Epoch: 3800 Step: 00034200] Batch Recognition Loss:   0.000292 => Gls Tokens per Sec:      656 || Batch Translation Loss:   0.007005 => Txt Tokens per Sec:     1816 || Lr: 0.000100
2024-02-10 15:18:44,408 Epoch 3800: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 15:18:44,408 EPOCH 3801
2024-02-10 15:19:00,407 Epoch 3801: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 15:19:00,407 EPOCH 3802
2024-02-10 15:19:16,461 Epoch 3802: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 15:19:16,461 EPOCH 3803
2024-02-10 15:19:32,032 Epoch 3803: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 15:19:32,033 EPOCH 3804
2024-02-10 15:19:48,577 Epoch 3804: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:19:48,577 EPOCH 3805
2024-02-10 15:20:04,349 Epoch 3805: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:20:04,350 EPOCH 3806
2024-02-10 15:20:20,432 Epoch 3806: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:20:20,433 EPOCH 3807
2024-02-10 15:20:36,588 Epoch 3807: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:20:36,588 EPOCH 3808
2024-02-10 15:20:52,590 Epoch 3808: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:20:52,590 EPOCH 3809
2024-02-10 15:21:09,215 Epoch 3809: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-10 15:21:09,216 EPOCH 3810
2024-02-10 15:21:26,192 Epoch 3810: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:21:26,193 EPOCH 3811
2024-02-10 15:21:42,577 Epoch 3811: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:21:42,578 EPOCH 3812
2024-02-10 15:21:48,248 [Epoch: 3812 Step: 00034300] Batch Recognition Loss:   0.000571 => Gls Tokens per Sec:      226 || Batch Translation Loss:   0.017530 => Txt Tokens per Sec:      779 || Lr: 0.000100
2024-02-10 15:21:58,700 Epoch 3812: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:21:58,701 EPOCH 3813
2024-02-10 15:22:14,959 Epoch 3813: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:22:14,959 EPOCH 3814
2024-02-10 15:22:31,241 Epoch 3814: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:22:31,242 EPOCH 3815
2024-02-10 15:22:47,143 Epoch 3815: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:22:47,144 EPOCH 3816
2024-02-10 15:23:03,085 Epoch 3816: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:23:03,086 EPOCH 3817
2024-02-10 15:23:19,132 Epoch 3817: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:23:19,133 EPOCH 3818
2024-02-10 15:23:35,075 Epoch 3818: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:23:35,076 EPOCH 3819
2024-02-10 15:23:51,138 Epoch 3819: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:23:51,138 EPOCH 3820
2024-02-10 15:24:07,310 Epoch 3820: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:24:07,310 EPOCH 3821
2024-02-10 15:24:23,455 Epoch 3821: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:24:23,455 EPOCH 3822
2024-02-10 15:24:39,518 Epoch 3822: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:24:39,519 EPOCH 3823
2024-02-10 15:24:43,134 [Epoch: 3823 Step: 00034400] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:      708 || Batch Translation Loss:   0.010366 => Txt Tokens per Sec:     2048 || Lr: 0.000100
2024-02-10 15:24:55,634 Epoch 3823: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 15:24:55,635 EPOCH 3824
2024-02-10 15:25:11,667 Epoch 3824: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 15:25:11,667 EPOCH 3825
2024-02-10 15:25:27,901 Epoch 3825: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 15:25:27,902 EPOCH 3826
2024-02-10 15:25:43,928 Epoch 3826: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 15:25:43,929 EPOCH 3827
2024-02-10 15:26:00,023 Epoch 3827: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 15:26:00,023 EPOCH 3828
2024-02-10 15:26:15,841 Epoch 3828: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-10 15:26:15,842 EPOCH 3829
2024-02-10 15:26:32,050 Epoch 3829: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-10 15:26:32,051 EPOCH 3830
2024-02-10 15:26:48,115 Epoch 3830: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.38 
2024-02-10 15:26:48,115 EPOCH 3831
2024-02-10 15:27:04,294 Epoch 3831: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-10 15:27:04,295 EPOCH 3832
2024-02-10 15:27:20,613 Epoch 3832: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-10 15:27:20,614 EPOCH 3833
2024-02-10 15:27:36,681 Epoch 3833: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-10 15:27:36,682 EPOCH 3834
2024-02-10 15:27:40,447 [Epoch: 3834 Step: 00034500] Batch Recognition Loss:   0.000611 => Gls Tokens per Sec:     1020 || Batch Translation Loss:   0.126494 => Txt Tokens per Sec:     2464 || Lr: 0.000100
2024-02-10 15:27:52,788 Epoch 3834: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-10 15:27:52,788 EPOCH 3835
2024-02-10 15:28:08,855 Epoch 3835: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.73 
2024-02-10 15:28:08,856 EPOCH 3836
2024-02-10 15:28:25,228 Epoch 3836: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.62 
2024-02-10 15:28:25,228 EPOCH 3837
2024-02-10 15:28:41,324 Epoch 3837: Total Training Recognition Loss 0.05  Total Training Translation Loss 14.33 
2024-02-10 15:28:41,324 EPOCH 3838
2024-02-10 15:28:57,327 Epoch 3838: Total Training Recognition Loss 0.06  Total Training Translation Loss 8.17 
2024-02-10 15:28:57,328 EPOCH 3839
2024-02-10 15:29:13,425 Epoch 3839: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.83 
2024-02-10 15:29:13,426 EPOCH 3840
2024-02-10 15:29:29,415 Epoch 3840: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.41 
2024-02-10 15:29:29,416 EPOCH 3841
2024-02-10 15:29:45,451 Epoch 3841: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.85 
2024-02-10 15:29:45,451 EPOCH 3842
2024-02-10 15:30:01,224 Epoch 3842: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.50 
2024-02-10 15:30:01,225 EPOCH 3843
2024-02-10 15:30:17,174 Epoch 3843: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-10 15:30:17,174 EPOCH 3844
2024-02-10 15:30:33,034 Epoch 3844: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-10 15:30:33,034 EPOCH 3845
2024-02-10 15:30:35,183 [Epoch: 3845 Step: 00034600] Batch Recognition Loss:   0.000975 => Gls Tokens per Sec:     2384 || Batch Translation Loss:   0.039296 => Txt Tokens per Sec:     6248 || Lr: 0.000100
2024-02-10 15:30:49,261 Epoch 3845: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 15:30:49,262 EPOCH 3846
2024-02-10 15:31:05,624 Epoch 3846: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 15:31:05,625 EPOCH 3847
2024-02-10 15:31:21,474 Epoch 3847: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 15:31:21,475 EPOCH 3848
2024-02-10 15:31:37,489 Epoch 3848: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 15:31:37,489 EPOCH 3849
2024-02-10 15:31:53,775 Epoch 3849: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 15:31:53,776 EPOCH 3850
2024-02-10 15:32:09,849 Epoch 3850: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 15:32:09,850 EPOCH 3851
2024-02-10 15:32:26,630 Epoch 3851: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 15:32:26,631 EPOCH 3852
2024-02-10 15:32:42,814 Epoch 3852: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 15:32:42,815 EPOCH 3853
2024-02-10 15:32:58,736 Epoch 3853: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 15:32:58,737 EPOCH 3854
2024-02-10 15:33:14,969 Epoch 3854: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 15:33:14,970 EPOCH 3855
2024-02-10 15:33:30,853 Epoch 3855: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 15:33:30,854 EPOCH 3856
2024-02-10 15:33:45,338 [Epoch: 3856 Step: 00034700] Batch Recognition Loss:   0.000378 => Gls Tokens per Sec:      380 || Batch Translation Loss:   0.015286 => Txt Tokens per Sec:     1151 || Lr: 0.000100
2024-02-10 15:33:47,392 Epoch 3856: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 15:33:47,393 EPOCH 3857
2024-02-10 15:34:03,328 Epoch 3857: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 15:34:03,329 EPOCH 3858
2024-02-10 15:34:19,259 Epoch 3858: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 15:34:19,259 EPOCH 3859
2024-02-10 15:34:35,163 Epoch 3859: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 15:34:35,163 EPOCH 3860
2024-02-10 15:34:51,352 Epoch 3860: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 15:34:51,353 EPOCH 3861
2024-02-10 15:35:07,226 Epoch 3861: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 15:35:07,227 EPOCH 3862
2024-02-10 15:35:23,394 Epoch 3862: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 15:35:23,394 EPOCH 3863
2024-02-10 15:35:39,510 Epoch 3863: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 15:35:39,510 EPOCH 3864
2024-02-10 15:35:55,663 Epoch 3864: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 15:35:55,663 EPOCH 3865
2024-02-10 15:36:11,521 Epoch 3865: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 15:36:11,522 EPOCH 3866
2024-02-10 15:36:27,927 Epoch 3866: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 15:36:27,927 EPOCH 3867
2024-02-10 15:36:38,731 [Epoch: 3867 Step: 00034800] Batch Recognition Loss:   0.001493 => Gls Tokens per Sec:      711 || Batch Translation Loss:   0.018784 => Txt Tokens per Sec:     2118 || Lr: 0.000100
2024-02-10 15:36:43,892 Epoch 3867: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 15:36:43,893 EPOCH 3868
2024-02-10 15:36:59,917 Epoch 3868: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 15:36:59,918 EPOCH 3869
2024-02-10 15:37:16,218 Epoch 3869: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 15:37:16,218 EPOCH 3870
2024-02-10 15:37:32,011 Epoch 3870: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 15:37:32,012 EPOCH 3871
2024-02-10 15:37:47,722 Epoch 3871: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 15:37:47,723 EPOCH 3872
2024-02-10 15:38:04,074 Epoch 3872: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 15:38:04,074 EPOCH 3873
2024-02-10 15:38:20,445 Epoch 3873: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 15:38:20,445 EPOCH 3874
2024-02-10 15:38:36,313 Epoch 3874: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 15:38:36,313 EPOCH 3875
2024-02-10 15:38:52,617 Epoch 3875: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 15:38:52,618 EPOCH 3876
2024-02-10 15:39:08,626 Epoch 3876: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 15:39:08,627 EPOCH 3877
2024-02-10 15:39:24,413 Epoch 3877: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 15:39:24,414 EPOCH 3878
2024-02-10 15:39:39,284 [Epoch: 3878 Step: 00034900] Batch Recognition Loss:   0.000222 => Gls Tokens per Sec:      542 || Batch Translation Loss:   0.008314 => Txt Tokens per Sec:     1477 || Lr: 0.000100
2024-02-10 15:39:40,447 Epoch 3878: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 15:39:40,447 EPOCH 3879
2024-02-10 15:39:56,473 Epoch 3879: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 15:39:56,474 EPOCH 3880
2024-02-10 15:40:12,759 Epoch 3880: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 15:40:12,760 EPOCH 3881
2024-02-10 15:40:28,870 Epoch 3881: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 15:40:28,870 EPOCH 3882
2024-02-10 15:40:45,086 Epoch 3882: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 15:40:45,086 EPOCH 3883
2024-02-10 15:41:01,145 Epoch 3883: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 15:41:01,145 EPOCH 3884
2024-02-10 15:41:17,292 Epoch 3884: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 15:41:17,293 EPOCH 3885
2024-02-10 15:41:33,531 Epoch 3885: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:41:33,531 EPOCH 3886
2024-02-10 15:41:49,496 Epoch 3886: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 15:41:49,497 EPOCH 3887
2024-02-10 15:42:05,760 Epoch 3887: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 15:42:05,760 EPOCH 3888
2024-02-10 15:42:21,738 Epoch 3888: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:42:21,738 EPOCH 3889
2024-02-10 15:42:37,113 [Epoch: 3889 Step: 00035000] Batch Recognition Loss:   0.002163 => Gls Tokens per Sec:      608 || Batch Translation Loss:   0.017508 => Txt Tokens per Sec:     1679 || Lr: 0.000100
2024-02-10 15:42:37,631 Epoch 3889: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-10 15:42:37,631 EPOCH 3890
2024-02-10 15:42:53,567 Epoch 3890: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:42:53,568 EPOCH 3891
2024-02-10 15:43:09,607 Epoch 3891: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:43:09,607 EPOCH 3892
2024-02-10 15:43:25,885 Epoch 3892: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:43:25,885 EPOCH 3893
2024-02-10 15:43:41,922 Epoch 3893: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:43:41,923 EPOCH 3894
2024-02-10 15:43:57,943 Epoch 3894: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:43:57,944 EPOCH 3895
2024-02-10 15:44:14,272 Epoch 3895: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:44:14,272 EPOCH 3896
2024-02-10 15:44:30,200 Epoch 3896: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:44:30,201 EPOCH 3897
2024-02-10 15:44:46,313 Epoch 3897: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:44:46,314 EPOCH 3898
2024-02-10 15:45:02,439 Epoch 3898: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 15:45:02,439 EPOCH 3899
2024-02-10 15:45:18,333 Epoch 3899: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:45:18,334 EPOCH 3900
2024-02-10 15:45:34,555 [Epoch: 3900 Step: 00035100] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:      655 || Batch Translation Loss:   0.010458 => Txt Tokens per Sec:     1811 || Lr: 0.000100
2024-02-10 15:45:34,556 Epoch 3900: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:45:34,556 EPOCH 3901
2024-02-10 15:45:50,806 Epoch 3901: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:45:50,807 EPOCH 3902
2024-02-10 15:46:06,855 Epoch 3902: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:46:06,855 EPOCH 3903
2024-02-10 15:46:22,934 Epoch 3903: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:46:22,935 EPOCH 3904
2024-02-10 15:46:38,719 Epoch 3904: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:46:38,720 EPOCH 3905
2024-02-10 15:46:55,104 Epoch 3905: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:46:55,104 EPOCH 3906
2024-02-10 15:47:11,071 Epoch 3906: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-10 15:47:11,072 EPOCH 3907
2024-02-10 15:47:27,104 Epoch 3907: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:47:27,104 EPOCH 3908
2024-02-10 15:47:43,018 Epoch 3908: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:47:43,019 EPOCH 3909
2024-02-10 15:47:59,131 Epoch 3909: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:47:59,133 EPOCH 3910
2024-02-10 15:48:15,265 Epoch 3910: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:48:15,266 EPOCH 3911
2024-02-10 15:48:31,151 Epoch 3911: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:48:31,152 EPOCH 3912
2024-02-10 15:48:35,291 [Epoch: 3912 Step: 00035200] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:       92 || Batch Translation Loss:   0.006269 => Txt Tokens per Sec:      326 || Lr: 0.000100
2024-02-10 15:48:47,012 Epoch 3912: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:48:47,013 EPOCH 3913
2024-02-10 15:49:03,001 Epoch 3913: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:49:03,001 EPOCH 3914
2024-02-10 15:49:19,184 Epoch 3914: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 15:49:19,185 EPOCH 3915
2024-02-10 15:49:35,599 Epoch 3915: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-10 15:49:35,599 EPOCH 3916
2024-02-10 15:49:51,527 Epoch 3916: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:49:51,527 EPOCH 3917
2024-02-10 15:50:07,781 Epoch 3917: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:50:07,782 EPOCH 3918
2024-02-10 15:50:23,608 Epoch 3918: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:50:23,608 EPOCH 3919
2024-02-10 15:50:39,608 Epoch 3919: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:50:39,609 EPOCH 3920
2024-02-10 15:50:55,863 Epoch 3920: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 15:50:55,864 EPOCH 3921
2024-02-10 15:51:11,809 Epoch 3921: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 15:51:11,810 EPOCH 3922
2024-02-10 15:51:27,669 Epoch 3922: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 15:51:27,670 EPOCH 3923
2024-02-10 15:51:32,499 [Epoch: 3923 Step: 00035300] Batch Recognition Loss:   0.001255 => Gls Tokens per Sec:      344 || Batch Translation Loss:   0.006612 => Txt Tokens per Sec:      867 || Lr: 0.000100
2024-02-10 15:51:43,714 Epoch 3923: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 15:51:43,714 EPOCH 3924
2024-02-10 15:51:58,904 Epoch 3924: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 15:51:58,904 EPOCH 3925
2024-02-10 15:52:14,979 Epoch 3925: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 15:52:14,980 EPOCH 3926
2024-02-10 15:52:31,537 Epoch 3926: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 15:52:31,538 EPOCH 3927
2024-02-10 15:52:47,560 Epoch 3927: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 15:52:47,560 EPOCH 3928
2024-02-10 15:53:03,731 Epoch 3928: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 15:53:03,732 EPOCH 3929
2024-02-10 15:53:19,961 Epoch 3929: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 15:53:19,962 EPOCH 3930
2024-02-10 15:53:36,007 Epoch 3930: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 15:53:36,007 EPOCH 3931
2024-02-10 15:53:52,524 Epoch 3931: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 15:53:52,524 EPOCH 3932
2024-02-10 15:54:08,798 Epoch 3932: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 15:54:08,798 EPOCH 3933
2024-02-10 15:54:24,934 Epoch 3933: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 15:54:24,935 EPOCH 3934
2024-02-10 15:54:31,187 [Epoch: 3934 Step: 00035400] Batch Recognition Loss:   0.000627 => Gls Tokens per Sec:      614 || Batch Translation Loss:   0.018617 => Txt Tokens per Sec:     1593 || Lr: 0.000100
2024-02-10 15:54:40,783 Epoch 3934: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 15:54:40,783 EPOCH 3935
2024-02-10 15:54:57,057 Epoch 3935: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 15:54:57,057 EPOCH 3936
2024-02-10 15:55:13,224 Epoch 3936: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 15:55:13,225 EPOCH 3937
2024-02-10 15:55:29,520 Epoch 3937: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 15:55:29,520 EPOCH 3938
2024-02-10 15:55:45,866 Epoch 3938: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 15:55:45,867 EPOCH 3939
2024-02-10 15:56:02,203 Epoch 3939: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 15:56:02,205 EPOCH 3940
2024-02-10 15:56:18,057 Epoch 3940: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-10 15:56:18,058 EPOCH 3941
2024-02-10 15:56:34,506 Epoch 3941: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-10 15:56:34,506 EPOCH 3942
2024-02-10 15:56:50,425 Epoch 3942: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-10 15:56:50,426 EPOCH 3943
2024-02-10 15:57:07,112 Epoch 3943: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-10 15:57:07,113 EPOCH 3944
2024-02-10 15:57:23,194 Epoch 3944: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.95 
2024-02-10 15:57:23,195 EPOCH 3945
2024-02-10 15:57:37,107 [Epoch: 3945 Step: 00035500] Batch Recognition Loss:   0.001181 => Gls Tokens per Sec:      303 || Batch Translation Loss:   0.098354 => Txt Tokens per Sec:      963 || Lr: 0.000100
2024-02-10 15:57:39,413 Epoch 3945: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.89 
2024-02-10 15:57:39,414 EPOCH 3946
2024-02-10 15:57:55,224 Epoch 3946: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-10 15:57:55,225 EPOCH 3947
2024-02-10 15:58:11,327 Epoch 3947: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-10 15:58:11,328 EPOCH 3948
2024-02-10 15:58:27,558 Epoch 3948: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-10 15:58:27,559 EPOCH 3949
2024-02-10 15:58:43,795 Epoch 3949: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-10 15:58:43,795 EPOCH 3950
2024-02-10 15:58:59,687 Epoch 3950: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 15:58:59,688 EPOCH 3951
2024-02-10 15:59:15,781 Epoch 3951: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-10 15:59:15,782 EPOCH 3952
2024-02-10 15:59:32,011 Epoch 3952: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 15:59:32,012 EPOCH 3953
2024-02-10 15:59:47,995 Epoch 3953: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 15:59:47,996 EPOCH 3954
2024-02-10 16:00:04,232 Epoch 3954: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 16:00:04,233 EPOCH 3955
2024-02-10 16:00:20,066 Epoch 3955: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-10 16:00:20,066 EPOCH 3956
2024-02-10 16:00:27,629 [Epoch: 3956 Step: 00035600] Batch Recognition Loss:   0.000210 => Gls Tokens per Sec:      846 || Batch Translation Loss:   0.016158 => Txt Tokens per Sec:     2222 || Lr: 0.000100
2024-02-10 16:00:36,387 Epoch 3956: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 16:00:36,387 EPOCH 3957
2024-02-10 16:00:52,279 Epoch 3957: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-10 16:00:52,280 EPOCH 3958
2024-02-10 16:01:08,602 Epoch 3958: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 16:01:08,602 EPOCH 3959
2024-02-10 16:01:24,602 Epoch 3959: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-10 16:01:24,603 EPOCH 3960
2024-02-10 16:01:40,166 Epoch 3960: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 16:01:40,166 EPOCH 3961
2024-02-10 16:01:56,419 Epoch 3961: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-10 16:01:56,419 EPOCH 3962
2024-02-10 16:02:12,480 Epoch 3962: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-10 16:02:12,480 EPOCH 3963
2024-02-10 16:02:28,351 Epoch 3963: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-10 16:02:28,352 EPOCH 3964
2024-02-10 16:02:44,275 Epoch 3964: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-10 16:02:44,276 EPOCH 3965
2024-02-10 16:03:00,631 Epoch 3965: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-10 16:03:00,631 EPOCH 3966
2024-02-10 16:03:16,869 Epoch 3966: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-10 16:03:16,870 EPOCH 3967
2024-02-10 16:03:27,831 [Epoch: 3967 Step: 00035700] Batch Recognition Loss:   0.000829 => Gls Tokens per Sec:      701 || Batch Translation Loss:   0.038895 => Txt Tokens per Sec:     1977 || Lr: 0.000100
2024-02-10 16:03:33,121 Epoch 3967: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 16:03:33,122 EPOCH 3968
2024-02-10 16:03:49,105 Epoch 3968: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 16:03:49,106 EPOCH 3969
2024-02-10 16:04:05,106 Epoch 3969: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 16:04:05,107 EPOCH 3970
2024-02-10 16:04:21,305 Epoch 3970: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 16:04:21,305 EPOCH 3971
2024-02-10 16:04:37,172 Epoch 3971: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 16:04:37,172 EPOCH 3972
2024-02-10 16:04:53,206 Epoch 3972: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 16:04:53,207 EPOCH 3973
2024-02-10 16:05:09,268 Epoch 3973: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 16:05:09,269 EPOCH 3974
2024-02-10 16:05:25,618 Epoch 3974: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 16:05:25,619 EPOCH 3975
2024-02-10 16:05:41,730 Epoch 3975: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 16:05:41,731 EPOCH 3976
2024-02-10 16:05:57,423 Epoch 3976: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 16:05:57,423 EPOCH 3977
2024-02-10 16:06:13,353 Epoch 3977: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 16:06:13,353 EPOCH 3978
2024-02-10 16:06:28,617 [Epoch: 3978 Step: 00035800] Batch Recognition Loss:   0.000331 => Gls Tokens per Sec:      528 || Batch Translation Loss:   0.012391 => Txt Tokens per Sec:     1456 || Lr: 0.000100
2024-02-10 16:06:29,710 Epoch 3978: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 16:06:29,710 EPOCH 3979
2024-02-10 16:06:45,628 Epoch 3979: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 16:06:45,629 EPOCH 3980
2024-02-10 16:07:01,624 Epoch 3980: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 16:07:01,624 EPOCH 3981
2024-02-10 16:07:17,595 Epoch 3981: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 16:07:17,596 EPOCH 3982
2024-02-10 16:07:33,854 Epoch 3982: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 16:07:33,854 EPOCH 3983
2024-02-10 16:07:50,080 Epoch 3983: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 16:07:50,081 EPOCH 3984
2024-02-10 16:08:06,338 Epoch 3984: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 16:08:06,339 EPOCH 3985
2024-02-10 16:08:22,369 Epoch 3985: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 16:08:22,370 EPOCH 3986
2024-02-10 16:08:38,293 Epoch 3986: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 16:08:38,293 EPOCH 3987
2024-02-10 16:08:54,310 Epoch 3987: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 16:08:54,310 EPOCH 3988
2024-02-10 16:09:10,576 Epoch 3988: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 16:09:10,577 EPOCH 3989
2024-02-10 16:09:25,882 [Epoch: 3989 Step: 00035900] Batch Recognition Loss:   0.000774 => Gls Tokens per Sec:      610 || Batch Translation Loss:   0.012350 => Txt Tokens per Sec:     1665 || Lr: 0.000100
2024-02-10 16:09:26,593 Epoch 3989: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 16:09:26,594 EPOCH 3990
2024-02-10 16:09:42,651 Epoch 3990: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 16:09:42,651 EPOCH 3991
2024-02-10 16:09:59,071 Epoch 3991: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 16:09:59,072 EPOCH 3992
2024-02-10 16:10:15,241 Epoch 3992: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 16:10:15,241 EPOCH 3993
2024-02-10 16:10:31,370 Epoch 3993: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 16:10:31,371 EPOCH 3994
2024-02-10 16:10:47,491 Epoch 3994: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 16:10:47,493 EPOCH 3995
2024-02-10 16:11:03,653 Epoch 3995: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 16:11:03,654 EPOCH 3996
2024-02-10 16:11:19,681 Epoch 3996: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 16:11:19,681 EPOCH 3997
2024-02-10 16:11:35,749 Epoch 3997: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 16:11:35,749 EPOCH 3998
2024-02-10 16:11:51,607 Epoch 3998: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 16:11:51,608 EPOCH 3999
2024-02-10 16:12:08,035 Epoch 3999: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 16:12:08,035 EPOCH 4000
2024-02-10 16:12:24,177 [Epoch: 4000 Step: 00036000] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:      658 || Batch Translation Loss:   0.014431 => Txt Tokens per Sec:     1820 || Lr: 0.000100
2024-02-10 16:13:36,041 Validation result at epoch 4000, step    36000: duration: 71.8636s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.30003	Translation Loss: 98467.32812	PPL: 18674.80273
	Eval Metric: BLEU
	WER 2.90	(DEL: 0.00,	INS: 0.00,	SUB: 2.90)
	BLEU-4 0.48	(BLEU-1: 10.60,	BLEU-2: 3.35,	BLEU-3: 1.23,	BLEU-4: 0.48)
	CHRF 17.10	ROUGE 8.68
2024-02-10 16:13:36,043 Logging Recognition and Translation Outputs
2024-02-10 16:13:36,043 ========================================================================================================================
2024-02-10 16:13:36,043 Logging Sequence: 153_36.00
2024-02-10 16:13:36,044 	Gloss Reference :	A B+C+D+E
2024-02-10 16:13:36,044 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 16:13:36,044 	Gloss Alignment :	         
2024-02-10 16:13:36,044 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 16:13:36,045 	Text Reference  :	********* ** **** ******* **** ***** ** india made   a   good score of 1686 in  20    overs
2024-02-10 16:13:36,046 	Text Hypothesis :	yesterday on 16th october 2022 kohli is only  scored 356 and  loss  of **** the world cup  
2024-02-10 16:13:36,046 	Text Alignment  :	I         I  I    I       I    I     I  S     S      S   S    S        D    S   S     S    
2024-02-10 16:13:36,046 ========================================================================================================================
2024-02-10 16:13:36,046 Logging Sequence: 163_30.00
2024-02-10 16:13:36,046 	Gloss Reference :	A B+C+D+E
2024-02-10 16:13:36,046 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 16:13:36,046 	Gloss Alignment :	         
2024-02-10 16:13:36,047 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 16:13:36,047 	Text Reference  :	***** *** they  never permitted anyone to ****** reveal her face   
2024-02-10 16:13:36,048 	Text Hypothesis :	after the video of    fans      want   to change their  own matches
2024-02-10 16:13:36,048 	Text Alignment  :	I     I   S     S     S         S         I      S      S   S      
2024-02-10 16:13:36,048 ========================================================================================================================
2024-02-10 16:13:36,048 Logging Sequence: 167_60.00
2024-02-10 16:13:36,048 	Gloss Reference :	A B+C+D+E
2024-02-10 16:13:36,048 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 16:13:36,049 	Gloss Alignment :	         
2024-02-10 16:13:36,049 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 16:13:36,050 	Text Reference  :	***** **** *** camel flu  spreads rapidly when   one comes in          close contact with the infected
2024-02-10 16:13:36,050 	Text Hypothesis :	being made out of    them for     all     thanks to  the   coronavirus for   no      run  was slow    
2024-02-10 16:13:36,050 	Text Alignment  :	I     I    I   S     S    S       S       S      S   S     S           S     S       S    S   S       
2024-02-10 16:13:36,051 ========================================================================================================================
2024-02-10 16:13:36,051 Logging Sequence: 84_35.00
2024-02-10 16:13:36,051 	Gloss Reference :	A B+C+D+E
2024-02-10 16:13:36,051 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 16:13:36,051 	Gloss Alignment :	         
2024-02-10 16:13:36,051 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 16:13:36,052 	Text Reference  :	here is  the reason why     they covered their mouth
2024-02-10 16:13:36,052 	Text Hypothesis :	it   was a   we     decided to   keep    the   match
2024-02-10 16:13:36,052 	Text Alignment  :	S    S   S   S      S       S    S       S     S    
2024-02-10 16:13:36,053 ========================================================================================================================
2024-02-10 16:13:36,053 Logging Sequence: 96_2.00
2024-02-10 16:13:36,053 	Gloss Reference :	A B+C+D+E
2024-02-10 16:13:36,053 	Gloss Hypothesis:	A B+C+D  
2024-02-10 16:13:36,053 	Gloss Alignment :	  S      
2024-02-10 16:13:36,053 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 16:13:36,055 	Text Reference  :	the world is preparing for the      t20     world cup scheduled to start from 16th  october this year
2024-02-10 16:13:36,055 	Text Hypothesis :	the ***** ** ********* icc under-19 cricket world cup ********* ** ***** was  first played  in   1988
2024-02-10 16:13:36,055 	Text Alignment  :	    D     D  D         S   S        S                 D         D  D     S    S     S       S    S   
2024-02-10 16:13:36,055 ========================================================================================================================
2024-02-10 16:13:36,060 Epoch 4000: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 16:13:36,060 EPOCH 4001
2024-02-10 16:13:53,300 Epoch 4001: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 16:13:53,300 EPOCH 4002
2024-02-10 16:14:09,285 Epoch 4002: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 16:14:09,286 EPOCH 4003
2024-02-10 16:14:25,674 Epoch 4003: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-10 16:14:25,675 EPOCH 4004
2024-02-10 16:14:41,939 Epoch 4004: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 16:14:41,940 EPOCH 4005
2024-02-10 16:14:57,804 Epoch 4005: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 16:14:57,805 EPOCH 4006
2024-02-10 16:15:13,683 Epoch 4006: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-10 16:15:13,683 EPOCH 4007
2024-02-10 16:15:29,971 Epoch 4007: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-10 16:15:29,972 EPOCH 4008
2024-02-10 16:15:45,708 Epoch 4008: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 16:15:45,708 EPOCH 4009
2024-02-10 16:16:01,618 Epoch 4009: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 16:16:01,618 EPOCH 4010
2024-02-10 16:16:17,740 Epoch 4010: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 16:16:17,740 EPOCH 4011
2024-02-10 16:16:33,629 Epoch 4011: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.31 
2024-02-10 16:16:33,630 EPOCH 4012
2024-02-10 16:16:37,866 [Epoch: 4012 Step: 00036100] Batch Recognition Loss:   0.000607 => Gls Tokens per Sec:       90 || Batch Translation Loss:   0.149898 => Txt Tokens per Sec:      321 || Lr: 0.000100
2024-02-10 16:16:49,534 Epoch 4012: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.82 
2024-02-10 16:16:49,534 EPOCH 4013
2024-02-10 16:17:05,695 Epoch 4013: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.05 
2024-02-10 16:17:05,695 EPOCH 4014
2024-02-10 16:17:21,412 Epoch 4014: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.88 
2024-02-10 16:17:21,412 EPOCH 4015
2024-02-10 16:17:37,604 Epoch 4015: Total Training Recognition Loss 0.14  Total Training Translation Loss 0.57 
2024-02-10 16:17:37,604 EPOCH 4016
2024-02-10 16:17:53,985 Epoch 4016: Total Training Recognition Loss 0.24  Total Training Translation Loss 0.44 
2024-02-10 16:17:53,986 EPOCH 4017
2024-02-10 16:18:10,350 Epoch 4017: Total Training Recognition Loss 0.35  Total Training Translation Loss 0.43 
2024-02-10 16:18:10,351 EPOCH 4018
2024-02-10 16:18:26,546 Epoch 4018: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.45 
2024-02-10 16:18:26,546 EPOCH 4019
2024-02-10 16:18:42,540 Epoch 4019: Total Training Recognition Loss 0.14  Total Training Translation Loss 0.37 
2024-02-10 16:18:42,541 EPOCH 4020
2024-02-10 16:18:58,673 Epoch 4020: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.36 
2024-02-10 16:18:58,674 EPOCH 4021
2024-02-10 16:19:15,145 Epoch 4021: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.25 
2024-02-10 16:19:15,145 EPOCH 4022
2024-02-10 16:19:31,052 Epoch 4022: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.20 
2024-02-10 16:19:31,052 EPOCH 4023
2024-02-10 16:19:31,688 [Epoch: 4023 Step: 00036200] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:     4038 || Batch Translation Loss:   0.016271 => Txt Tokens per Sec:     9603 || Lr: 0.000100
2024-02-10 16:19:47,131 Epoch 4023: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.22 
2024-02-10 16:19:47,132 EPOCH 4024
2024-02-10 16:20:02,940 Epoch 4024: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.34 
2024-02-10 16:20:02,940 EPOCH 4025
2024-02-10 16:20:18,842 Epoch 4025: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 16:20:18,842 EPOCH 4026
2024-02-10 16:20:35,037 Epoch 4026: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 16:20:35,037 EPOCH 4027
2024-02-10 16:20:50,992 Epoch 4027: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 16:20:50,992 EPOCH 4028
2024-02-10 16:21:06,902 Epoch 4028: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 16:21:06,903 EPOCH 4029
2024-02-10 16:21:23,083 Epoch 4029: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 16:21:23,084 EPOCH 4030
2024-02-10 16:21:39,950 Epoch 4030: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 16:21:39,951 EPOCH 4031
2024-02-10 16:21:56,189 Epoch 4031: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 16:21:56,190 EPOCH 4032
2024-02-10 16:22:12,335 Epoch 4032: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 16:22:12,335 EPOCH 4033
2024-02-10 16:22:28,715 Epoch 4033: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 16:22:28,715 EPOCH 4034
2024-02-10 16:22:35,387 [Epoch: 4034 Step: 00036300] Batch Recognition Loss:   0.001352 => Gls Tokens per Sec:      576 || Batch Translation Loss:   0.025501 => Txt Tokens per Sec:     1734 || Lr: 0.000100
2024-02-10 16:22:44,846 Epoch 4034: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 16:22:44,847 EPOCH 4035
2024-02-10 16:23:01,131 Epoch 4035: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 16:23:01,131 EPOCH 4036
2024-02-10 16:23:17,384 Epoch 4036: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 16:23:17,384 EPOCH 4037
2024-02-10 16:23:33,331 Epoch 4037: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-10 16:23:33,332 EPOCH 4038
2024-02-10 16:23:49,349 Epoch 4038: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 16:23:49,349 EPOCH 4039
2024-02-10 16:24:05,384 Epoch 4039: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 16:24:05,384 EPOCH 4040
2024-02-10 16:24:21,653 Epoch 4040: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 16:24:21,653 EPOCH 4041
2024-02-10 16:24:37,363 Epoch 4041: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 16:24:37,363 EPOCH 4042
2024-02-10 16:24:53,185 Epoch 4042: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 16:24:53,186 EPOCH 4043
2024-02-10 16:25:09,704 Epoch 4043: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 16:25:09,704 EPOCH 4044
2024-02-10 16:25:25,930 Epoch 4044: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 16:25:25,930 EPOCH 4045
2024-02-10 16:25:30,690 [Epoch: 4045 Step: 00036400] Batch Recognition Loss:   0.000530 => Gls Tokens per Sec:     1076 || Batch Translation Loss:   0.016908 => Txt Tokens per Sec:     3142 || Lr: 0.000100
2024-02-10 16:25:41,870 Epoch 4045: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 16:25:41,870 EPOCH 4046
2024-02-10 16:25:58,110 Epoch 4046: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 16:25:58,110 EPOCH 4047
2024-02-10 16:26:14,514 Epoch 4047: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 16:26:14,514 EPOCH 4048
2024-02-10 16:26:30,386 Epoch 4048: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 16:26:30,388 EPOCH 4049
2024-02-10 16:26:46,617 Epoch 4049: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 16:26:46,617 EPOCH 4050
2024-02-10 16:27:02,402 Epoch 4050: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 16:27:02,403 EPOCH 4051
2024-02-10 16:27:18,380 Epoch 4051: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 16:27:18,381 EPOCH 4052
2024-02-10 16:27:34,468 Epoch 4052: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 16:27:34,469 EPOCH 4053
2024-02-10 16:27:50,696 Epoch 4053: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 16:27:50,697 EPOCH 4054
2024-02-10 16:28:06,707 Epoch 4054: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 16:28:06,708 EPOCH 4055
2024-02-10 16:28:22,818 Epoch 4055: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 16:28:22,819 EPOCH 4056
2024-02-10 16:28:31,225 [Epoch: 4056 Step: 00036500] Batch Recognition Loss:   0.000462 => Gls Tokens per Sec:      654 || Batch Translation Loss:   0.015618 => Txt Tokens per Sec:     1838 || Lr: 0.000100
2024-02-10 16:28:38,347 Epoch 4056: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 16:28:38,347 EPOCH 4057
2024-02-10 16:28:54,648 Epoch 4057: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 16:28:54,649 EPOCH 4058
2024-02-10 16:29:10,743 Epoch 4058: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 16:29:10,743 EPOCH 4059
2024-02-10 16:29:26,767 Epoch 4059: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-10 16:29:26,767 EPOCH 4060
2024-02-10 16:29:42,566 Epoch 4060: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 16:29:42,567 EPOCH 4061
2024-02-10 16:29:59,007 Epoch 4061: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 16:29:59,008 EPOCH 4062
2024-02-10 16:30:15,017 Epoch 4062: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 16:30:15,018 EPOCH 4063
2024-02-10 16:30:31,057 Epoch 4063: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 16:30:31,057 EPOCH 4064
2024-02-10 16:30:46,953 Epoch 4064: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 16:30:46,954 EPOCH 4065
2024-02-10 16:31:03,258 Epoch 4065: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 16:31:03,259 EPOCH 4066
2024-02-10 16:31:19,343 Epoch 4066: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 16:31:19,344 EPOCH 4067
2024-02-10 16:31:28,601 [Epoch: 4067 Step: 00036600] Batch Recognition Loss:   0.000161 => Gls Tokens per Sec:      732 || Batch Translation Loss:   0.012797 => Txt Tokens per Sec:     2034 || Lr: 0.000100
2024-02-10 16:31:35,286 Epoch 4067: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 16:31:35,287 EPOCH 4068
2024-02-10 16:31:51,663 Epoch 4068: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 16:31:51,664 EPOCH 4069
2024-02-10 16:32:07,590 Epoch 4069: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 16:32:07,590 EPOCH 4070
2024-02-10 16:32:24,271 Epoch 4070: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 16:32:24,271 EPOCH 4071
2024-02-10 16:32:40,437 Epoch 4071: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 16:32:40,438 EPOCH 4072
2024-02-10 16:32:56,694 Epoch 4072: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 16:32:56,694 EPOCH 4073
2024-02-10 16:33:12,729 Epoch 4073: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 16:33:12,730 EPOCH 4074
2024-02-10 16:33:28,815 Epoch 4074: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-10 16:33:28,816 EPOCH 4075
2024-02-10 16:33:44,894 Epoch 4075: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-10 16:33:44,894 EPOCH 4076
2024-02-10 16:34:00,927 Epoch 4076: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-10 16:34:00,928 EPOCH 4077
2024-02-10 16:34:17,013 Epoch 4077: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.56 
2024-02-10 16:34:17,013 EPOCH 4078
2024-02-10 16:34:31,992 [Epoch: 4078 Step: 00036700] Batch Recognition Loss:   0.000461 => Gls Tokens per Sec:      538 || Batch Translation Loss:   0.045353 => Txt Tokens per Sec:     1471 || Lr: 0.000100
2024-02-10 16:34:33,140 Epoch 4078: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-10 16:34:33,141 EPOCH 4079
2024-02-10 16:34:49,484 Epoch 4079: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-10 16:34:49,484 EPOCH 4080
2024-02-10 16:35:05,521 Epoch 4080: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.39 
2024-02-10 16:35:05,521 EPOCH 4081
2024-02-10 16:35:21,540 Epoch 4081: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.23 
2024-02-10 16:35:21,540 EPOCH 4082
2024-02-10 16:35:37,540 Epoch 4082: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.65 
2024-02-10 16:35:37,541 EPOCH 4083
2024-02-10 16:35:53,680 Epoch 4083: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.95 
2024-02-10 16:35:53,681 EPOCH 4084
2024-02-10 16:36:09,896 Epoch 4084: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.12 
2024-02-10 16:36:09,897 EPOCH 4085
2024-02-10 16:36:25,783 Epoch 4085: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.64 
2024-02-10 16:36:25,784 EPOCH 4086
2024-02-10 16:36:41,741 Epoch 4086: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-10 16:36:41,742 EPOCH 4087
2024-02-10 16:36:57,552 Epoch 4087: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-10 16:36:57,553 EPOCH 4088
2024-02-10 16:37:12,955 Epoch 4088: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-10 16:37:12,955 EPOCH 4089
2024-02-10 16:37:24,647 [Epoch: 4089 Step: 00036800] Batch Recognition Loss:   0.000990 => Gls Tokens per Sec:      876 || Batch Translation Loss:   0.057685 => Txt Tokens per Sec:     2397 || Lr: 0.000100
2024-02-10 16:37:28,921 Epoch 4089: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-10 16:37:28,922 EPOCH 4090
2024-02-10 16:37:45,037 Epoch 4090: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-10 16:37:45,038 EPOCH 4091
2024-02-10 16:38:00,906 Epoch 4091: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-10 16:38:00,907 EPOCH 4092
2024-02-10 16:38:17,036 Epoch 4092: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 16:38:17,037 EPOCH 4093
2024-02-10 16:38:33,110 Epoch 4093: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 16:38:33,111 EPOCH 4094
2024-02-10 16:38:48,715 Epoch 4094: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 16:38:48,716 EPOCH 4095
2024-02-10 16:39:04,952 Epoch 4095: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 16:39:04,953 EPOCH 4096
2024-02-10 16:39:21,013 Epoch 4096: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 16:39:21,014 EPOCH 4097
2024-02-10 16:39:37,292 Epoch 4097: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 16:39:37,293 EPOCH 4098
2024-02-10 16:39:53,450 Epoch 4098: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 16:39:53,450 EPOCH 4099
2024-02-10 16:40:09,443 Epoch 4099: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 16:40:09,444 EPOCH 4100
2024-02-10 16:40:25,319 [Epoch: 4100 Step: 00036900] Batch Recognition Loss:   0.000836 => Gls Tokens per Sec:      669 || Batch Translation Loss:   0.018050 => Txt Tokens per Sec:     1851 || Lr: 0.000100
2024-02-10 16:40:25,319 Epoch 4100: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 16:40:25,320 EPOCH 4101
2024-02-10 16:40:41,472 Epoch 4101: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 16:40:41,472 EPOCH 4102
2024-02-10 16:40:57,858 Epoch 4102: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 16:40:57,858 EPOCH 4103
2024-02-10 16:41:14,136 Epoch 4103: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 16:41:14,137 EPOCH 4104
2024-02-10 16:41:30,141 Epoch 4104: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 16:41:30,141 EPOCH 4105
2024-02-10 16:41:46,574 Epoch 4105: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 16:41:46,575 EPOCH 4106
2024-02-10 16:42:02,610 Epoch 4106: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 16:42:02,611 EPOCH 4107
2024-02-10 16:42:18,872 Epoch 4107: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 16:42:18,873 EPOCH 4108
2024-02-10 16:42:35,132 Epoch 4108: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 16:42:35,132 EPOCH 4109
2024-02-10 16:42:51,187 Epoch 4109: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 16:42:51,188 EPOCH 4110
2024-02-10 16:43:07,496 Epoch 4110: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 16:43:07,497 EPOCH 4111
2024-02-10 16:43:23,285 Epoch 4111: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 16:43:23,286 EPOCH 4112
2024-02-10 16:43:23,607 [Epoch: 4112 Step: 00037000] Batch Recognition Loss:   0.000206 => Gls Tokens per Sec:     4013 || Batch Translation Loss:   0.011609 => Txt Tokens per Sec:    10166 || Lr: 0.000100
2024-02-10 16:43:39,455 Epoch 4112: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 16:43:39,456 EPOCH 4113
2024-02-10 16:43:55,525 Epoch 4113: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 16:43:55,525 EPOCH 4114
2024-02-10 16:44:11,634 Epoch 4114: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 16:44:11,635 EPOCH 4115
2024-02-10 16:44:27,892 Epoch 4115: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 16:44:27,893 EPOCH 4116
2024-02-10 16:44:44,029 Epoch 4116: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-10 16:44:44,030 EPOCH 4117
2024-02-10 16:45:00,118 Epoch 4117: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 16:45:00,118 EPOCH 4118
2024-02-10 16:45:16,380 Epoch 4118: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 16:45:16,381 EPOCH 4119
2024-02-10 16:45:32,320 Epoch 4119: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 16:45:32,321 EPOCH 4120
2024-02-10 16:45:48,242 Epoch 4120: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 16:45:48,243 EPOCH 4121
2024-02-10 16:46:04,349 Epoch 4121: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 16:46:04,350 EPOCH 4122
2024-02-10 16:46:20,254 Epoch 4122: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 16:46:20,255 EPOCH 4123
2024-02-10 16:46:21,286 [Epoch: 4123 Step: 00037100] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:     2485 || Batch Translation Loss:   0.014801 => Txt Tokens per Sec:     7187 || Lr: 0.000100
2024-02-10 16:46:36,194 Epoch 4123: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 16:46:36,194 EPOCH 4124
2024-02-10 16:46:52,462 Epoch 4124: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 16:46:52,463 EPOCH 4125
2024-02-10 16:47:08,560 Epoch 4125: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 16:47:08,560 EPOCH 4126
2024-02-10 16:47:24,589 Epoch 4126: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 16:47:24,590 EPOCH 4127
2024-02-10 16:47:40,938 Epoch 4127: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 16:47:40,939 EPOCH 4128
2024-02-10 16:47:56,959 Epoch 4128: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 16:47:56,960 EPOCH 4129
2024-02-10 16:48:12,925 Epoch 4129: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 16:48:12,925 EPOCH 4130
2024-02-10 16:48:29,115 Epoch 4130: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 16:48:29,116 EPOCH 4131
2024-02-10 16:48:45,185 Epoch 4131: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 16:48:45,185 EPOCH 4132
2024-02-10 16:49:01,170 Epoch 4132: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 16:49:01,170 EPOCH 4133
2024-02-10 16:49:17,113 Epoch 4133: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 16:49:17,114 EPOCH 4134
2024-02-10 16:49:26,698 [Epoch: 4134 Step: 00037200] Batch Recognition Loss:   0.000308 => Gls Tokens per Sec:      401 || Batch Translation Loss:   0.007866 => Txt Tokens per Sec:     1128 || Lr: 0.000100
2024-02-10 16:49:33,524 Epoch 4134: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 16:49:33,525 EPOCH 4135
2024-02-10 16:49:49,561 Epoch 4135: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 16:49:49,561 EPOCH 4136
2024-02-10 16:50:05,520 Epoch 4136: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 16:50:05,520 EPOCH 4137
2024-02-10 16:50:21,683 Epoch 4137: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 16:50:21,684 EPOCH 4138
2024-02-10 16:50:38,353 Epoch 4138: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 16:50:38,353 EPOCH 4139
2024-02-10 16:50:54,287 Epoch 4139: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 16:50:54,287 EPOCH 4140
2024-02-10 16:51:10,417 Epoch 4140: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 16:51:10,418 EPOCH 4141
2024-02-10 16:51:26,764 Epoch 4141: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 16:51:26,764 EPOCH 4142
2024-02-10 16:51:42,945 Epoch 4142: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 16:51:42,946 EPOCH 4143
2024-02-10 16:51:58,820 Epoch 4143: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-10 16:51:58,821 EPOCH 4144
2024-02-10 16:52:14,772 Epoch 4144: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 16:52:14,772 EPOCH 4145
2024-02-10 16:52:19,221 [Epoch: 4145 Step: 00037300] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:     1151 || Batch Translation Loss:   0.008500 => Txt Tokens per Sec:     3134 || Lr: 0.000100
2024-02-10 16:52:30,885 Epoch 4145: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 16:52:30,885 EPOCH 4146
2024-02-10 16:52:47,501 Epoch 4146: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 16:52:47,502 EPOCH 4147
2024-02-10 16:53:03,692 Epoch 4147: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 16:53:03,692 EPOCH 4148
2024-02-10 16:53:19,784 Epoch 4148: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 16:53:19,785 EPOCH 4149
2024-02-10 16:53:35,849 Epoch 4149: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 16:53:35,849 EPOCH 4150
2024-02-10 16:53:51,748 Epoch 4150: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 16:53:51,749 EPOCH 4151
2024-02-10 16:54:08,032 Epoch 4151: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 16:54:08,032 EPOCH 4152
2024-02-10 16:54:24,178 Epoch 4152: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-10 16:54:24,178 EPOCH 4153
2024-02-10 16:54:40,726 Epoch 4153: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 16:54:40,727 EPOCH 4154
2024-02-10 16:54:56,697 Epoch 4154: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 16:54:56,698 EPOCH 4155
2024-02-10 16:55:12,685 Epoch 4155: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 16:55:12,685 EPOCH 4156
2024-02-10 16:55:21,917 [Epoch: 4156 Step: 00037400] Batch Recognition Loss:   0.000152 => Gls Tokens per Sec:      596 || Batch Translation Loss:   0.017100 => Txt Tokens per Sec:     1721 || Lr: 0.000100
2024-02-10 16:55:28,928 Epoch 4156: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 16:55:28,929 EPOCH 4157
2024-02-10 16:55:44,897 Epoch 4157: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 16:55:44,897 EPOCH 4158
2024-02-10 16:56:00,814 Epoch 4158: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 16:56:00,815 EPOCH 4159
2024-02-10 16:56:16,999 Epoch 4159: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 16:56:16,999 EPOCH 4160
2024-02-10 16:56:33,138 Epoch 4160: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 16:56:33,139 EPOCH 4161
2024-02-10 16:56:49,459 Epoch 4161: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 16:56:49,460 EPOCH 4162
2024-02-10 16:57:05,378 Epoch 4162: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 16:57:05,379 EPOCH 4163
2024-02-10 16:57:21,504 Epoch 4163: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 16:57:21,504 EPOCH 4164
2024-02-10 16:57:37,665 Epoch 4164: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-10 16:57:37,666 EPOCH 4165
2024-02-10 16:57:53,817 Epoch 4165: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.86 
2024-02-10 16:57:53,818 EPOCH 4166
2024-02-10 16:58:09,783 Epoch 4166: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.50 
2024-02-10 16:58:09,783 EPOCH 4167
2024-02-10 16:58:21,588 [Epoch: 4167 Step: 00037500] Batch Recognition Loss:   0.001752 => Gls Tokens per Sec:      574 || Batch Translation Loss:   0.261504 => Txt Tokens per Sec:     1539 || Lr: 0.000100
2024-02-10 16:58:25,893 Epoch 4167: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.39 
2024-02-10 16:58:25,894 EPOCH 4168
2024-02-10 16:58:41,997 Epoch 4168: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.54 
2024-02-10 16:58:41,998 EPOCH 4169
2024-02-10 16:58:58,262 Epoch 4169: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.48 
2024-02-10 16:58:58,263 EPOCH 4170
2024-02-10 16:59:14,222 Epoch 4170: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.61 
2024-02-10 16:59:14,223 EPOCH 4171
2024-02-10 16:59:30,469 Epoch 4171: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.52 
2024-02-10 16:59:30,470 EPOCH 4172
2024-02-10 16:59:46,277 Epoch 4172: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.47 
2024-02-10 16:59:46,278 EPOCH 4173
2024-02-10 17:00:02,415 Epoch 4173: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.37 
2024-02-10 17:00:02,416 EPOCH 4174
2024-02-10 17:00:18,453 Epoch 4174: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.04 
2024-02-10 17:00:18,453 EPOCH 4175
2024-02-10 17:00:34,429 Epoch 4175: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.76 
2024-02-10 17:00:34,429 EPOCH 4176
2024-02-10 17:00:50,631 Epoch 4176: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-10 17:00:50,631 EPOCH 4177
2024-02-10 17:01:06,862 Epoch 4177: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.91 
2024-02-10 17:01:06,863 EPOCH 4178
2024-02-10 17:01:12,640 [Epoch: 4178 Step: 00037600] Batch Recognition Loss:   0.000392 => Gls Tokens per Sec:     1551 || Batch Translation Loss:   0.051885 => Txt Tokens per Sec:     4092 || Lr: 0.000100
2024-02-10 17:01:22,911 Epoch 4178: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-10 17:01:22,911 EPOCH 4179
2024-02-10 17:01:39,025 Epoch 4179: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-10 17:01:39,025 EPOCH 4180
2024-02-10 17:01:55,383 Epoch 4180: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 17:01:55,384 EPOCH 4181
2024-02-10 17:02:11,465 Epoch 4181: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 17:02:11,466 EPOCH 4182
2024-02-10 17:02:27,368 Epoch 4182: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 17:02:27,368 EPOCH 4183
2024-02-10 17:02:43,420 Epoch 4183: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 17:02:43,420 EPOCH 4184
2024-02-10 17:02:59,874 Epoch 4184: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 17:02:59,874 EPOCH 4185
2024-02-10 17:03:15,643 Epoch 4185: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 17:03:15,643 EPOCH 4186
2024-02-10 17:03:31,696 Epoch 4186: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 17:03:31,697 EPOCH 4187
2024-02-10 17:03:47,962 Epoch 4187: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 17:03:47,962 EPOCH 4188
2024-02-10 17:04:04,023 Epoch 4188: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 17:04:04,024 EPOCH 4189
2024-02-10 17:04:19,480 [Epoch: 4189 Step: 00037700] Batch Recognition Loss:   0.000465 => Gls Tokens per Sec:      604 || Batch Translation Loss:   0.011279 => Txt Tokens per Sec:     1673 || Lr: 0.000100
2024-02-10 17:04:19,958 Epoch 4189: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 17:04:19,958 EPOCH 4190
2024-02-10 17:04:35,989 Epoch 4190: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 17:04:35,990 EPOCH 4191
2024-02-10 17:04:51,735 Epoch 4191: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 17:04:51,736 EPOCH 4192
2024-02-10 17:05:08,042 Epoch 4192: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 17:05:08,043 EPOCH 4193
2024-02-10 17:05:24,147 Epoch 4193: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:05:24,148 EPOCH 4194
2024-02-10 17:05:40,187 Epoch 4194: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 17:05:40,188 EPOCH 4195
2024-02-10 17:05:55,848 Epoch 4195: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 17:05:55,849 EPOCH 4196
2024-02-10 17:06:12,408 Epoch 4196: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 17:06:12,409 EPOCH 4197
2024-02-10 17:06:28,380 Epoch 4197: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:06:28,381 EPOCH 4198
2024-02-10 17:06:44,338 Epoch 4198: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:06:44,339 EPOCH 4199
2024-02-10 17:07:00,673 Epoch 4199: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:07:00,674 EPOCH 4200
2024-02-10 17:07:16,773 [Epoch: 4200 Step: 00037800] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:      660 || Batch Translation Loss:   0.013037 => Txt Tokens per Sec:     1825 || Lr: 0.000100
2024-02-10 17:07:16,773 Epoch 4200: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 17:07:16,773 EPOCH 4201
2024-02-10 17:07:32,979 Epoch 4201: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:07:32,979 EPOCH 4202
2024-02-10 17:07:48,858 Epoch 4202: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:07:48,859 EPOCH 4203
2024-02-10 17:08:05,132 Epoch 4203: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:08:05,132 EPOCH 4204
2024-02-10 17:08:21,326 Epoch 4204: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:08:21,327 EPOCH 4205
2024-02-10 17:08:37,157 Epoch 4205: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:08:37,158 EPOCH 4206
2024-02-10 17:08:53,213 Epoch 4206: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 17:08:53,214 EPOCH 4207
2024-02-10 17:09:08,912 Epoch 4207: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 17:09:08,912 EPOCH 4208
2024-02-10 17:09:25,205 Epoch 4208: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 17:09:25,206 EPOCH 4209
2024-02-10 17:09:41,278 Epoch 4209: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 17:09:41,278 EPOCH 4210
2024-02-10 17:09:57,455 Epoch 4210: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 17:09:57,456 EPOCH 4211
2024-02-10 17:10:13,517 Epoch 4211: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 17:10:13,517 EPOCH 4212
2024-02-10 17:10:13,818 [Epoch: 4212 Step: 00037900] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:     4267 || Batch Translation Loss:   0.012087 => Txt Tokens per Sec:     9437 || Lr: 0.000100
2024-02-10 17:10:29,695 Epoch 4212: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:10:29,696 EPOCH 4213
2024-02-10 17:10:46,000 Epoch 4213: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:10:46,000 EPOCH 4214
2024-02-10 17:11:01,904 Epoch 4214: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:11:01,905 EPOCH 4215
2024-02-10 17:11:18,072 Epoch 4215: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:11:18,072 EPOCH 4216
2024-02-10 17:11:34,010 Epoch 4216: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:11:34,010 EPOCH 4217
2024-02-10 17:11:49,829 Epoch 4217: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-10 17:11:49,830 EPOCH 4218
2024-02-10 17:12:05,876 Epoch 4218: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 17:12:05,877 EPOCH 4219
2024-02-10 17:12:21,873 Epoch 4219: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-10 17:12:21,873 EPOCH 4220
2024-02-10 17:12:37,758 Epoch 4220: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 17:12:37,759 EPOCH 4221
2024-02-10 17:12:53,507 Epoch 4221: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:12:53,508 EPOCH 4222
2024-02-10 17:13:09,828 Epoch 4222: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:13:09,828 EPOCH 4223
2024-02-10 17:13:13,480 [Epoch: 4223 Step: 00038000] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:      701 || Batch Translation Loss:   0.008606 => Txt Tokens per Sec:     1895 || Lr: 0.000100
2024-02-10 17:14:25,467 Validation result at epoch 4223, step    38000: duration: 71.9863s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.30881	Translation Loss: 99128.32031	PPL: 19949.32227
	Eval Metric: BLEU
	WER 2.40	(DEL: 0.00,	INS: 0.00,	SUB: 2.40)
	BLEU-4 0.35	(BLEU-1: 10.39,	BLEU-2: 2.87,	BLEU-3: 1.02,	BLEU-4: 0.35)
	CHRF 16.75	ROUGE 8.62
2024-02-10 17:14:25,469 Logging Recognition and Translation Outputs
2024-02-10 17:14:25,470 ========================================================================================================================
2024-02-10 17:14:25,470 Logging Sequence: 59_152.00
2024-02-10 17:14:25,470 	Gloss Reference :	A B+C+D+E
2024-02-10 17:14:25,470 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 17:14:25,473 	Gloss Alignment :	         
2024-02-10 17:14:25,474 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 17:14:25,475 	Text Reference  :	**** the organisers encouraged athletes to  use    the     condoms in  their  home countries
2024-02-10 17:14:25,475 	Text Hypothesis :	well and all        formats    of       odi series against delhi's ace bowler axar etc      
2024-02-10 17:14:25,475 	Text Alignment  :	I    S   S          S          S        S   S      S       S       S   S      S    S        
2024-02-10 17:14:25,475 ========================================================================================================================
2024-02-10 17:14:25,475 Logging Sequence: 155_78.00
2024-02-10 17:14:25,475 	Gloss Reference :	A B+C+D+E
2024-02-10 17:14:25,476 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 17:14:25,476 	Gloss Alignment :	         
2024-02-10 17:14:25,476 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 17:14:25,478 	Text Reference  :	it was difficult for icc to disqualify the   afghan team at    the last minute so   they included them as    per the schedule     
2024-02-10 17:14:25,478 	Text Hypothesis :	** *** ********* *** *** ** until      there is     a    known as  they will   have been excited  and  watch and are championships
2024-02-10 17:14:25,478 	Text Alignment  :	D  D   D         D   D   D  S          S     S      S    S     S   S    S      S    S    S        S    S     S   S   S            
2024-02-10 17:14:25,478 ========================================================================================================================
2024-02-10 17:14:25,479 Logging Sequence: 102_147.00
2024-02-10 17:14:25,479 	Gloss Reference :	A B+C+D+E
2024-02-10 17:14:25,479 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 17:14:25,479 	Gloss Alignment :	         
2024-02-10 17:14:25,479 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 17:14:25,481 	Text Reference  :	despite the muscle cramps this young boy    lifted such a    huge weight and    made   the country proud by securing a gold    medal   
2024-02-10 17:14:25,481 	Text Hypothesis :	******* the ****** ****** **** match starts at     the  same time medal  sheuli lifted the ******* ***** ** ******** * penalty shootout
2024-02-10 17:14:25,481 	Text Alignment  :	D           D      D      D    S     S      S      S    S    S    S      S      S          D       D     D  D        D S       S       
2024-02-10 17:14:25,482 ========================================================================================================================
2024-02-10 17:14:25,482 Logging Sequence: 105_2.00
2024-02-10 17:14:25,482 	Gloss Reference :	A B+C+D+E  
2024-02-10 17:14:25,482 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-10 17:14:25,482 	Gloss Alignment :	  S        
2024-02-10 17:14:25,482 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 17:14:25,483 	Text Reference  :	***** the ******* **** *** airthings masters tournament is     an       online  chess tournament
2024-02-10 17:14:25,484 	Text Hypothesis :	after the intense game was held      at      the        iconic wankhede stadium in    mumbai    
2024-02-10 17:14:25,484 	Text Alignment  :	I         I       I    I   S         S       S          S      S        S       S     S         
2024-02-10 17:14:25,484 ========================================================================================================================
2024-02-10 17:14:25,484 Logging Sequence: 96_31.00
2024-02-10 17:14:25,484 	Gloss Reference :	A B+C+D+E
2024-02-10 17:14:25,484 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 17:14:25,485 	Gloss Alignment :	         
2024-02-10 17:14:25,485 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 17:14:25,486 	Text Reference  :	******* and     then   2 teams will go on to     play the **** final 
2024-02-10 17:14:25,486 	Text Hypothesis :	however india's scored 2 ***** **** ** ** groups -    the next wicket
2024-02-10 17:14:25,486 	Text Alignment  :	I       S       S        D     D    D  D  S      S        I    S     
2024-02-10 17:14:25,486 ========================================================================================================================
2024-02-10 17:14:38,597 Epoch 4223: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 17:14:38,597 EPOCH 4224
2024-02-10 17:14:54,739 Epoch 4224: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 17:14:54,740 EPOCH 4225
2024-02-10 17:15:10,601 Epoch 4225: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:15:10,602 EPOCH 4226
2024-02-10 17:15:27,198 Epoch 4226: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 17:15:27,198 EPOCH 4227
2024-02-10 17:15:43,155 Epoch 4227: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 17:15:43,156 EPOCH 4228
2024-02-10 17:15:59,282 Epoch 4228: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:15:59,283 EPOCH 4229
2024-02-10 17:16:15,621 Epoch 4229: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:16:15,621 EPOCH 4230
2024-02-10 17:16:31,696 Epoch 4230: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 17:16:31,696 EPOCH 4231
2024-02-10 17:16:47,845 Epoch 4231: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:16:47,846 EPOCH 4232
2024-02-10 17:17:03,917 Epoch 4232: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 17:17:03,918 EPOCH 4233
2024-02-10 17:17:19,836 Epoch 4233: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:17:19,837 EPOCH 4234
2024-02-10 17:17:24,036 [Epoch: 4234 Step: 00038100] Batch Recognition Loss:   0.000188 => Gls Tokens per Sec:      915 || Batch Translation Loss:   0.014837 => Txt Tokens per Sec:     2638 || Lr: 0.000100
2024-02-10 17:17:36,010 Epoch 4234: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:17:36,011 EPOCH 4235
2024-02-10 17:17:51,995 Epoch 4235: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:17:51,995 EPOCH 4236
2024-02-10 17:18:08,383 Epoch 4236: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:18:08,384 EPOCH 4237
2024-02-10 17:18:24,586 Epoch 4237: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:18:24,586 EPOCH 4238
2024-02-10 17:18:40,222 Epoch 4238: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:18:40,223 EPOCH 4239
2024-02-10 17:18:56,242 Epoch 4239: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 17:18:56,242 EPOCH 4240
2024-02-10 17:19:12,562 Epoch 4240: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 17:19:12,563 EPOCH 4241
2024-02-10 17:19:28,890 Epoch 4241: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:19:28,891 EPOCH 4242
2024-02-10 17:19:45,063 Epoch 4242: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:19:45,063 EPOCH 4243
2024-02-10 17:20:01,355 Epoch 4243: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:20:01,356 EPOCH 4244
2024-02-10 17:20:17,878 Epoch 4244: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:20:17,879 EPOCH 4245
2024-02-10 17:20:24,890 [Epoch: 4245 Step: 00038200] Batch Recognition Loss:   0.000413 => Gls Tokens per Sec:      730 || Batch Translation Loss:   0.017428 => Txt Tokens per Sec:     2078 || Lr: 0.000100
2024-02-10 17:20:34,061 Epoch 4245: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:20:34,062 EPOCH 4246
2024-02-10 17:20:50,384 Epoch 4246: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:20:50,385 EPOCH 4247
2024-02-10 17:21:06,565 Epoch 4247: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:21:06,565 EPOCH 4248
2024-02-10 17:21:22,463 Epoch 4248: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-10 17:21:22,464 EPOCH 4249
2024-02-10 17:21:38,739 Epoch 4249: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 17:21:38,739 EPOCH 4250
2024-02-10 17:21:55,720 Epoch 4250: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:21:55,721 EPOCH 4251
2024-02-10 17:22:12,089 Epoch 4251: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:22:12,090 EPOCH 4252
2024-02-10 17:22:28,532 Epoch 4252: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:22:28,533 EPOCH 4253
2024-02-10 17:22:44,546 Epoch 4253: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-10 17:22:44,547 EPOCH 4254
2024-02-10 17:23:00,819 Epoch 4254: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:23:00,819 EPOCH 4255
2024-02-10 17:23:16,545 Epoch 4255: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 17:23:16,545 EPOCH 4256
2024-02-10 17:23:21,691 [Epoch: 4256 Step: 00038300] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:     1244 || Batch Translation Loss:   0.019041 => Txt Tokens per Sec:     3389 || Lr: 0.000100
2024-02-10 17:23:32,937 Epoch 4256: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 17:23:32,938 EPOCH 4257
2024-02-10 17:23:49,212 Epoch 4257: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:23:49,213 EPOCH 4258
2024-02-10 17:24:05,162 Epoch 4258: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:24:05,162 EPOCH 4259
2024-02-10 17:24:21,456 Epoch 4259: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 17:24:21,456 EPOCH 4260
2024-02-10 17:24:37,684 Epoch 4260: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 17:24:37,685 EPOCH 4261
2024-02-10 17:24:53,830 Epoch 4261: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 17:24:53,830 EPOCH 4262
2024-02-10 17:25:09,732 Epoch 4262: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 17:25:09,732 EPOCH 4263
2024-02-10 17:25:26,102 Epoch 4263: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-10 17:25:26,103 EPOCH 4264
2024-02-10 17:25:42,037 Epoch 4264: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.65 
2024-02-10 17:25:42,038 EPOCH 4265
2024-02-10 17:25:58,086 Epoch 4265: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-10 17:25:58,087 EPOCH 4266
2024-02-10 17:26:14,027 Epoch 4266: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-10 17:26:14,027 EPOCH 4267
2024-02-10 17:26:25,252 [Epoch: 4267 Step: 00038400] Batch Recognition Loss:   0.000421 => Gls Tokens per Sec:      684 || Batch Translation Loss:   0.054524 => Txt Tokens per Sec:     2004 || Lr: 0.000100
2024-02-10 17:26:30,371 Epoch 4267: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-10 17:26:30,371 EPOCH 4268
2024-02-10 17:26:46,308 Epoch 4268: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-10 17:26:46,309 EPOCH 4269
2024-02-10 17:27:02,593 Epoch 4269: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-10 17:27:02,594 EPOCH 4270
2024-02-10 17:27:18,582 Epoch 4270: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-10 17:27:18,583 EPOCH 4271
2024-02-10 17:27:34,498 Epoch 4271: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.72 
2024-02-10 17:27:34,499 EPOCH 4272
2024-02-10 17:27:50,907 Epoch 4272: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.35 
2024-02-10 17:27:50,907 EPOCH 4273
2024-02-10 17:28:07,132 Epoch 4273: Total Training Recognition Loss 0.05  Total Training Translation Loss 5.11 
2024-02-10 17:28:07,133 EPOCH 4274
2024-02-10 17:28:23,236 Epoch 4274: Total Training Recognition Loss 0.06  Total Training Translation Loss 4.10 
2024-02-10 17:28:23,236 EPOCH 4275
2024-02-10 17:28:39,164 Epoch 4275: Total Training Recognition Loss 0.05  Total Training Translation Loss 6.18 
2024-02-10 17:28:39,165 EPOCH 4276
2024-02-10 17:28:55,111 Epoch 4276: Total Training Recognition Loss 0.04  Total Training Translation Loss 8.48 
2024-02-10 17:28:55,112 EPOCH 4277
2024-02-10 17:29:11,282 Epoch 4277: Total Training Recognition Loss 0.07  Total Training Translation Loss 3.45 
2024-02-10 17:29:11,283 EPOCH 4278
2024-02-10 17:29:22,741 [Epoch: 4278 Step: 00038500] Batch Recognition Loss:   0.002747 => Gls Tokens per Sec:      782 || Batch Translation Loss:   0.090710 => Txt Tokens per Sec:     2199 || Lr: 0.000100
2024-02-10 17:29:27,436 Epoch 4278: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.64 
2024-02-10 17:29:27,436 EPOCH 4279
2024-02-10 17:29:43,489 Epoch 4279: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.78 
2024-02-10 17:29:43,490 EPOCH 4280
2024-02-10 17:29:59,560 Epoch 4280: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.53 
2024-02-10 17:29:59,561 EPOCH 4281
2024-02-10 17:30:15,595 Epoch 4281: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-10 17:30:15,595 EPOCH 4282
2024-02-10 17:30:31,737 Epoch 4282: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 17:30:31,738 EPOCH 4283
2024-02-10 17:30:47,906 Epoch 4283: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 17:30:47,907 EPOCH 4284
2024-02-10 17:31:04,172 Epoch 4284: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 17:31:04,173 EPOCH 4285
2024-02-10 17:31:20,357 Epoch 4285: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 17:31:20,357 EPOCH 4286
2024-02-10 17:31:36,720 Epoch 4286: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 17:31:36,721 EPOCH 4287
2024-02-10 17:31:53,115 Epoch 4287: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 17:31:53,115 EPOCH 4288
2024-02-10 17:32:09,217 Epoch 4288: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 17:32:09,218 EPOCH 4289
2024-02-10 17:32:24,702 [Epoch: 4289 Step: 00038600] Batch Recognition Loss:   0.000286 => Gls Tokens per Sec:      603 || Batch Translation Loss:   0.015681 => Txt Tokens per Sec:     1647 || Lr: 0.000100
2024-02-10 17:32:25,431 Epoch 4289: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 17:32:25,432 EPOCH 4290
2024-02-10 17:32:42,646 Epoch 4290: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 17:32:42,647 EPOCH 4291
2024-02-10 17:32:58,876 Epoch 4291: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 17:32:58,877 EPOCH 4292
2024-02-10 17:33:15,073 Epoch 4292: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 17:33:15,073 EPOCH 4293
2024-02-10 17:33:31,185 Epoch 4293: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 17:33:31,185 EPOCH 4294
2024-02-10 17:33:47,554 Epoch 4294: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 17:33:47,555 EPOCH 4295
2024-02-10 17:34:03,741 Epoch 4295: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 17:34:03,741 EPOCH 4296
2024-02-10 17:34:19,887 Epoch 4296: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 17:34:19,888 EPOCH 4297
2024-02-10 17:34:36,106 Epoch 4297: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 17:34:36,106 EPOCH 4298
2024-02-10 17:34:52,292 Epoch 4298: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 17:34:52,293 EPOCH 4299
2024-02-10 17:35:08,310 Epoch 4299: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 17:35:08,311 EPOCH 4300
2024-02-10 17:35:24,336 [Epoch: 4300 Step: 00038700] Batch Recognition Loss:   0.000250 => Gls Tokens per Sec:      663 || Batch Translation Loss:   0.016908 => Txt Tokens per Sec:     1834 || Lr: 0.000100
2024-02-10 17:35:24,337 Epoch 4300: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:35:24,337 EPOCH 4301
2024-02-10 17:35:40,573 Epoch 4301: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:35:40,573 EPOCH 4302
2024-02-10 17:35:56,466 Epoch 4302: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 17:35:56,467 EPOCH 4303
2024-02-10 17:36:12,307 Epoch 4303: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:36:12,307 EPOCH 4304
2024-02-10 17:36:28,568 Epoch 4304: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:36:28,569 EPOCH 4305
2024-02-10 17:36:44,664 Epoch 4305: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:36:44,665 EPOCH 4306
2024-02-10 17:37:00,655 Epoch 4306: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:37:00,655 EPOCH 4307
2024-02-10 17:37:16,685 Epoch 4307: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:37:16,685 EPOCH 4308
2024-02-10 17:37:32,712 Epoch 4308: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:37:32,713 EPOCH 4309
2024-02-10 17:37:48,629 Epoch 4309: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:37:48,630 EPOCH 4310
2024-02-10 17:38:04,456 Epoch 4310: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:38:04,456 EPOCH 4311
2024-02-10 17:38:20,615 Epoch 4311: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:38:20,616 EPOCH 4312
2024-02-10 17:38:25,021 [Epoch: 4312 Step: 00038800] Batch Recognition Loss:   0.000211 => Gls Tokens per Sec:       86 || Batch Translation Loss:   0.005919 => Txt Tokens per Sec:      308 || Lr: 0.000100
2024-02-10 17:38:36,930 Epoch 4312: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:38:36,931 EPOCH 4313
2024-02-10 17:38:53,459 Epoch 4313: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:38:53,460 EPOCH 4314
2024-02-10 17:39:09,771 Epoch 4314: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:39:09,771 EPOCH 4315
2024-02-10 17:39:25,986 Epoch 4315: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:39:25,987 EPOCH 4316
2024-02-10 17:39:42,039 Epoch 4316: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-10 17:39:42,039 EPOCH 4317
2024-02-10 17:39:58,020 Epoch 4317: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:39:58,021 EPOCH 4318
2024-02-10 17:40:14,251 Epoch 4318: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:40:14,251 EPOCH 4319
2024-02-10 17:40:30,195 Epoch 4319: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:40:30,196 EPOCH 4320
2024-02-10 17:40:46,088 Epoch 4320: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:40:46,088 EPOCH 4321
2024-02-10 17:41:02,414 Epoch 4321: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:41:02,414 EPOCH 4322
2024-02-10 17:41:18,494 Epoch 4322: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:41:18,495 EPOCH 4323
2024-02-10 17:41:19,432 [Epoch: 4323 Step: 00038900] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:     2732 || Batch Translation Loss:   0.014657 => Txt Tokens per Sec:     7618 || Lr: 0.000100
2024-02-10 17:41:34,539 Epoch 4323: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 17:41:34,539 EPOCH 4324
2024-02-10 17:41:50,647 Epoch 4324: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:41:50,648 EPOCH 4325
2024-02-10 17:42:06,942 Epoch 4325: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:42:06,943 EPOCH 4326
2024-02-10 17:42:22,654 Epoch 4326: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:42:22,654 EPOCH 4327
2024-02-10 17:42:38,432 Epoch 4327: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-10 17:42:38,433 EPOCH 4328
2024-02-10 17:42:54,537 Epoch 4328: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:42:54,538 EPOCH 4329
2024-02-10 17:43:10,787 Epoch 4329: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:43:10,788 EPOCH 4330
2024-02-10 17:43:26,603 Epoch 4330: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:43:26,604 EPOCH 4331
2024-02-10 17:43:42,753 Epoch 4331: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:43:42,754 EPOCH 4332
2024-02-10 17:43:58,845 Epoch 4332: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:43:58,846 EPOCH 4333
2024-02-10 17:44:14,707 Epoch 4333: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:44:14,708 EPOCH 4334
2024-02-10 17:44:18,786 [Epoch: 4334 Step: 00039000] Batch Recognition Loss:   0.000205 => Gls Tokens per Sec:      942 || Batch Translation Loss:   0.039682 => Txt Tokens per Sec:     2816 || Lr: 0.000100
2024-02-10 17:44:30,765 Epoch 4334: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 17:44:30,766 EPOCH 4335
2024-02-10 17:44:46,902 Epoch 4335: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:44:46,902 EPOCH 4336
2024-02-10 17:45:03,294 Epoch 4336: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:45:03,294 EPOCH 4337
2024-02-10 17:45:19,270 Epoch 4337: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:45:19,271 EPOCH 4338
2024-02-10 17:45:34,950 Epoch 4338: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:45:34,951 EPOCH 4339
2024-02-10 17:45:51,527 Epoch 4339: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:45:51,528 EPOCH 4340
2024-02-10 17:46:07,586 Epoch 4340: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:46:07,587 EPOCH 4341
2024-02-10 17:46:23,651 Epoch 4341: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:46:23,652 EPOCH 4342
2024-02-10 17:46:39,410 Epoch 4342: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 17:46:39,410 EPOCH 4343
2024-02-10 17:46:55,533 Epoch 4343: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 17:46:55,534 EPOCH 4344
2024-02-10 17:47:11,657 Epoch 4344: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:47:11,657 EPOCH 4345
2024-02-10 17:47:23,033 [Epoch: 4345 Step: 00039100] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:      371 || Batch Translation Loss:   0.011895 => Txt Tokens per Sec:     1138 || Lr: 0.000100
2024-02-10 17:47:27,780 Epoch 4345: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:47:27,780 EPOCH 4346
2024-02-10 17:47:43,524 Epoch 4346: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:47:43,525 EPOCH 4347
2024-02-10 17:47:59,611 Epoch 4347: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:47:59,612 EPOCH 4348
2024-02-10 17:48:15,832 Epoch 4348: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:48:15,832 EPOCH 4349
2024-02-10 17:48:31,846 Epoch 4349: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:48:31,847 EPOCH 4350
2024-02-10 17:48:47,851 Epoch 4350: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:48:47,852 EPOCH 4351
2024-02-10 17:49:03,989 Epoch 4351: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:49:03,989 EPOCH 4352
2024-02-10 17:49:20,048 Epoch 4352: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:49:20,048 EPOCH 4353
2024-02-10 17:49:35,999 Epoch 4353: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:49:36,000 EPOCH 4354
2024-02-10 17:49:52,133 Epoch 4354: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 17:49:52,133 EPOCH 4355
2024-02-10 17:50:07,978 Epoch 4355: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 17:50:07,979 EPOCH 4356
2024-02-10 17:50:19,481 [Epoch: 4356 Step: 00039200] Batch Recognition Loss:   0.000291 => Gls Tokens per Sec:      478 || Batch Translation Loss:   0.017480 => Txt Tokens per Sec:     1389 || Lr: 0.000100
2024-02-10 17:50:24,061 Epoch 4356: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:50:24,062 EPOCH 4357
2024-02-10 17:50:40,061 Epoch 4357: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 17:50:40,062 EPOCH 4358
2024-02-10 17:50:56,077 Epoch 4358: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:50:56,078 EPOCH 4359
2024-02-10 17:51:12,477 Epoch 4359: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 17:51:12,478 EPOCH 4360
2024-02-10 17:51:28,673 Epoch 4360: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 17:51:28,673 EPOCH 4361
2024-02-10 17:51:44,732 Epoch 4361: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 17:51:44,732 EPOCH 4362
2024-02-10 17:52:00,897 Epoch 4362: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 17:52:00,898 EPOCH 4363
2024-02-10 17:52:16,996 Epoch 4363: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 17:52:16,996 EPOCH 4364
2024-02-10 17:52:33,278 Epoch 4364: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 17:52:33,279 EPOCH 4365
2024-02-10 17:52:49,501 Epoch 4365: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 17:52:49,501 EPOCH 4366
2024-02-10 17:53:05,279 Epoch 4366: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 17:53:05,279 EPOCH 4367
2024-02-10 17:53:14,672 [Epoch: 4367 Step: 00039300] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:      722 || Batch Translation Loss:   0.018610 => Txt Tokens per Sec:     1915 || Lr: 0.000100
2024-02-10 17:53:21,696 Epoch 4367: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 17:53:21,697 EPOCH 4368
2024-02-10 17:53:37,815 Epoch 4368: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 17:53:37,816 EPOCH 4369
2024-02-10 17:53:53,706 Epoch 4369: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 17:53:53,706 EPOCH 4370
2024-02-10 17:54:09,805 Epoch 4370: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 17:54:09,805 EPOCH 4371
2024-02-10 17:54:25,823 Epoch 4371: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 17:54:25,824 EPOCH 4372
2024-02-10 17:54:41,685 Epoch 4372: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 17:54:41,686 EPOCH 4373
2024-02-10 17:54:57,520 Epoch 4373: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 17:54:57,521 EPOCH 4374
2024-02-10 17:55:13,623 Epoch 4374: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 17:55:13,624 EPOCH 4375
2024-02-10 17:55:29,819 Epoch 4375: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:55:29,819 EPOCH 4376
2024-02-10 17:55:46,206 Epoch 4376: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-10 17:55:46,207 EPOCH 4377
2024-02-10 17:56:02,147 Epoch 4377: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:56:02,148 EPOCH 4378
2024-02-10 17:56:17,168 [Epoch: 4378 Step: 00039400] Batch Recognition Loss:   0.000423 => Gls Tokens per Sec:      537 || Batch Translation Loss:   0.017361 => Txt Tokens per Sec:     1480 || Lr: 0.000100
2024-02-10 17:56:18,442 Epoch 4378: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:56:18,442 EPOCH 4379
2024-02-10 17:56:34,495 Epoch 4379: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-10 17:56:34,495 EPOCH 4380
2024-02-10 17:56:50,526 Epoch 4380: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:56:50,526 EPOCH 4381
2024-02-10 17:57:06,522 Epoch 4381: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-10 17:57:06,522 EPOCH 4382
2024-02-10 17:57:22,671 Epoch 4382: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:57:22,671 EPOCH 4383
2024-02-10 17:57:38,500 Epoch 4383: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:57:38,500 EPOCH 4384
2024-02-10 17:57:54,360 Epoch 4384: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:57:54,360 EPOCH 4385
2024-02-10 17:58:10,901 Epoch 4385: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 17:58:10,901 EPOCH 4386
2024-02-10 17:58:27,132 Epoch 4386: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:58:27,133 EPOCH 4387
2024-02-10 17:58:43,266 Epoch 4387: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 17:58:43,266 EPOCH 4388
2024-02-10 17:58:59,187 Epoch 4388: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 17:58:59,188 EPOCH 4389
2024-02-10 17:59:14,429 [Epoch: 4389 Step: 00039500] Batch Recognition Loss:   0.000316 => Gls Tokens per Sec:      613 || Batch Translation Loss:   0.014490 => Txt Tokens per Sec:     1673 || Lr: 0.000100
2024-02-10 17:59:15,268 Epoch 4389: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 17:59:15,269 EPOCH 4390
2024-02-10 17:59:31,474 Epoch 4390: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:59:31,475 EPOCH 4391
2024-02-10 17:59:47,560 Epoch 4391: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 17:59:47,560 EPOCH 4392
2024-02-10 18:00:03,424 Epoch 4392: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 18:00:03,424 EPOCH 4393
2024-02-10 18:00:19,782 Epoch 4393: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 18:00:19,782 EPOCH 4394
2024-02-10 18:00:36,178 Epoch 4394: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 18:00:36,179 EPOCH 4395
2024-02-10 18:00:52,342 Epoch 4395: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 18:00:52,343 EPOCH 4396
2024-02-10 18:01:08,361 Epoch 4396: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 18:01:08,362 EPOCH 4397
2024-02-10 18:01:24,322 Epoch 4397: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 18:01:24,323 EPOCH 4398
2024-02-10 18:01:40,497 Epoch 4398: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 18:01:40,498 EPOCH 4399
2024-02-10 18:01:56,472 Epoch 4399: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 18:01:56,473 EPOCH 4400
2024-02-10 18:02:12,461 [Epoch: 4400 Step: 00039600] Batch Recognition Loss:   0.000241 => Gls Tokens per Sec:      664 || Batch Translation Loss:   0.022238 => Txt Tokens per Sec:     1838 || Lr: 0.000100
2024-02-10 18:02:12,462 Epoch 4400: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 18:02:12,462 EPOCH 4401
2024-02-10 18:02:28,742 Epoch 4401: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 18:02:28,742 EPOCH 4402
2024-02-10 18:02:44,584 Epoch 4402: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-10 18:02:44,585 EPOCH 4403
2024-02-10 18:03:00,442 Epoch 4403: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 18:03:00,442 EPOCH 4404
2024-02-10 18:03:16,786 Epoch 4404: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 18:03:16,786 EPOCH 4405
2024-02-10 18:03:33,124 Epoch 4405: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 18:03:33,124 EPOCH 4406
2024-02-10 18:03:49,208 Epoch 4406: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 18:03:49,208 EPOCH 4407
2024-02-10 18:04:05,092 Epoch 4407: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 18:04:05,093 EPOCH 4408
2024-02-10 18:04:21,336 Epoch 4408: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 18:04:21,336 EPOCH 4409
2024-02-10 18:04:37,344 Epoch 4409: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-10 18:04:37,345 EPOCH 4410
2024-02-10 18:04:53,389 Epoch 4410: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 18:04:53,390 EPOCH 4411
2024-02-10 18:05:09,709 Epoch 4411: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-10 18:05:09,710 EPOCH 4412
2024-02-10 18:05:10,432 [Epoch: 4412 Step: 00039700] Batch Recognition Loss:   0.000233 => Gls Tokens per Sec:     1778 || Batch Translation Loss:   0.057701 => Txt Tokens per Sec:     5396 || Lr: 0.000100
2024-02-10 18:05:25,663 Epoch 4412: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-10 18:05:25,664 EPOCH 4413
2024-02-10 18:05:41,491 Epoch 4413: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-10 18:05:41,492 EPOCH 4414
2024-02-10 18:05:57,529 Epoch 4414: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 18:05:57,530 EPOCH 4415
2024-02-10 18:06:13,934 Epoch 4415: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 18:06:13,934 EPOCH 4416
2024-02-10 18:06:29,910 Epoch 4416: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 18:06:29,911 EPOCH 4417
2024-02-10 18:06:46,076 Epoch 4417: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 18:06:46,077 EPOCH 4418
2024-02-10 18:07:02,045 Epoch 4418: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 18:07:02,045 EPOCH 4419
2024-02-10 18:07:17,747 Epoch 4419: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 18:07:17,747 EPOCH 4420
2024-02-10 18:07:34,127 Epoch 4420: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 18:07:34,127 EPOCH 4421
2024-02-10 18:07:50,319 Epoch 4421: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 18:07:50,320 EPOCH 4422
2024-02-10 18:08:06,526 Epoch 4422: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 18:08:06,526 EPOCH 4423
2024-02-10 18:08:15,628 [Epoch: 4423 Step: 00039800] Batch Recognition Loss:   0.000328 => Gls Tokens per Sec:      281 || Batch Translation Loss:   0.022248 => Txt Tokens per Sec:      935 || Lr: 0.000100
2024-02-10 18:08:22,516 Epoch 4423: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 18:08:22,516 EPOCH 4424
2024-02-10 18:08:38,468 Epoch 4424: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 18:08:38,468 EPOCH 4425
2024-02-10 18:08:54,413 Epoch 4425: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 18:08:54,413 EPOCH 4426
2024-02-10 18:09:10,447 Epoch 4426: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 18:09:10,447 EPOCH 4427
2024-02-10 18:09:26,621 Epoch 4427: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 18:09:26,621 EPOCH 4428
2024-02-10 18:09:42,536 Epoch 4428: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 18:09:42,537 EPOCH 4429
2024-02-10 18:09:58,902 Epoch 4429: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 18:09:58,902 EPOCH 4430
2024-02-10 18:10:14,997 Epoch 4430: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 18:10:14,998 EPOCH 4431
2024-02-10 18:10:31,366 Epoch 4431: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 18:10:31,367 EPOCH 4432
2024-02-10 18:10:47,449 Epoch 4432: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 18:10:47,450 EPOCH 4433
2024-02-10 18:11:03,619 Epoch 4433: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 18:11:03,620 EPOCH 4434
2024-02-10 18:11:11,623 [Epoch: 4434 Step: 00039900] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:      367 || Batch Translation Loss:   0.015212 => Txt Tokens per Sec:     1038 || Lr: 0.000100
2024-02-10 18:11:19,947 Epoch 4434: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-10 18:11:19,947 EPOCH 4435
2024-02-10 18:11:35,897 Epoch 4435: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-10 18:11:35,897 EPOCH 4436
2024-02-10 18:11:52,110 Epoch 4436: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 18:11:52,111 EPOCH 4437
2024-02-10 18:12:08,102 Epoch 4437: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-10 18:12:08,103 EPOCH 4438
2024-02-10 18:12:24,443 Epoch 4438: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-10 18:12:24,444 EPOCH 4439
2024-02-10 18:12:40,710 Epoch 4439: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-10 18:12:40,710 EPOCH 4440
2024-02-10 18:12:56,481 Epoch 4440: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 18:12:56,481 EPOCH 4441
2024-02-10 18:13:12,489 Epoch 4441: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-10 18:13:12,490 EPOCH 4442
2024-02-10 18:13:28,465 Epoch 4442: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-10 18:13:28,466 EPOCH 4443
2024-02-10 18:13:44,696 Epoch 4443: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-10 18:13:44,697 EPOCH 4444
2024-02-10 18:14:01,437 Epoch 4444: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-10 18:14:01,438 EPOCH 4445
2024-02-10 18:14:05,520 [Epoch: 4445 Step: 00040000] Batch Recognition Loss:   0.000271 => Gls Tokens per Sec:     1255 || Batch Translation Loss:   0.050951 => Txt Tokens per Sec:     3136 || Lr: 0.000100
2024-02-10 18:15:17,251 Validation result at epoch 4445, step    40000: duration: 71.7295s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.25351	Translation Loss: 98860.15625	PPL: 19422.09180
	Eval Metric: BLEU
	WER 2.33	(DEL: 0.00,	INS: 0.00,	SUB: 2.33)
	BLEU-4 0.52	(BLEU-1: 9.69,	BLEU-2: 2.73,	BLEU-3: 1.02,	BLEU-4: 0.52)
	CHRF 16.40	ROUGE 8.21
2024-02-10 18:15:17,253 Logging Recognition and Translation Outputs
2024-02-10 18:15:17,253 ========================================================================================================================
2024-02-10 18:15:17,253 Logging Sequence: 86_84.00
2024-02-10 18:15:17,253 	Gloss Reference :	A B+C+D+E
2024-02-10 18:15:17,254 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 18:15:17,254 	Gloss Alignment :	         
2024-02-10 18:15:17,254 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 18:15:17,256 	Text Reference  :	amassing 8933 runs    which included 21 centuries with   a  highest score of  201     not out 
2024-02-10 18:15:17,256 	Text Hypothesis :	******** **** yashpal was   one      of the       heroes in india'  world cup triumph in  1983
2024-02-10 18:15:17,256 	Text Alignment  :	D        D    S       S     S        S  S         S      S  S       S     S   S       S   S   
2024-02-10 18:15:17,256 ========================================================================================================================
2024-02-10 18:15:17,256 Logging Sequence: 179_110.00
2024-02-10 18:15:17,256 	Gloss Reference :	A B+C+D+E
2024-02-10 18:15:17,257 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 18:15:17,257 	Gloss Alignment :	         
2024-02-10 18:15:17,257 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 18:15:17,259 	Text Reference  :	******* **** *** *** ********* ****** *** ******* phogat refused    to  stay  in       the same room with    other indian female wrestlers
2024-02-10 18:15:17,259 	Text Hypothesis :	however soon old old coca-cola advert had respect their  daughter's and sonam received it  at   the  airport on    the    july   2023     
2024-02-10 18:15:17,259 	Text Alignment  :	I       I    I   I   I         I      I   I       S      S          S   S     S        S   S    S    S       S     S      S      S        
2024-02-10 18:15:17,259 ========================================================================================================================
2024-02-10 18:15:17,260 Logging Sequence: 102_2.00
2024-02-10 18:15:17,260 	Gloss Reference :	A B+C+D+E    
2024-02-10 18:15:17,260 	Gloss Hypothesis:	A B+C+D+E+D+E
2024-02-10 18:15:17,260 	Gloss Alignment :	  S          
2024-02-10 18:15:17,260 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 18:15:17,262 	Text Reference  :	commonwealth games are ********* ***** among the world's most recognised gaming championships after   the olympics
2024-02-10 18:15:17,262 	Text Hypothesis :	the          games are currently being held  in  pune    were supposed   to     play          against his place   
2024-02-10 18:15:17,262 	Text Alignment  :	S                      I         I     S     S   S       S    S          S      S             S       S   S       
2024-02-10 18:15:17,262 ========================================================================================================================
2024-02-10 18:15:17,262 Logging Sequence: 60_195.00
2024-02-10 18:15:17,262 	Gloss Reference :	A B+C+D+E
2024-02-10 18:15:17,263 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 18:15:17,263 	Gloss Alignment :	         
2024-02-10 18:15:17,263 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 18:15:17,264 	Text Reference  :	people loved to  watch his   aggressive expressions and   his   bowling
2024-02-10 18:15:17,264 	Text Hypothesis :	as     bcci  has an    image on         the         insta story may    
2024-02-10 18:15:17,264 	Text Alignment  :	S      S     S   S     S     S          S           S     S     S      
2024-02-10 18:15:17,264 ========================================================================================================================
2024-02-10 18:15:17,264 Logging Sequence: 70_200.00
2024-02-10 18:15:17,264 	Gloss Reference :	A B+C+D+E
2024-02-10 18:15:17,264 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 18:15:17,265 	Gloss Alignment :	         
2024-02-10 18:15:17,265 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 18:15:17,265 	Text Reference  :	******* ***** ***** ** showing ronaldo whole-heartedly endorsing the brand 
2024-02-10 18:15:17,266 	Text Hypothesis :	however being proud of her     beer    bottle          against   a   condom
2024-02-10 18:15:17,266 	Text Alignment  :	I       I     I     I  S       S       S               S         S   S     
2024-02-10 18:15:17,266 ========================================================================================================================
2024-02-10 18:15:29,996 Epoch 4445: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-10 18:15:29,997 EPOCH 4446
2024-02-10 18:15:46,376 Epoch 4446: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-10 18:15:46,377 EPOCH 4447
2024-02-10 18:16:02,037 Epoch 4447: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.78 
2024-02-10 18:16:02,037 EPOCH 4448
2024-02-10 18:16:17,870 Epoch 4448: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.87 
2024-02-10 18:16:17,871 EPOCH 4449
2024-02-10 18:16:34,204 Epoch 4449: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.15 
2024-02-10 18:16:34,205 EPOCH 4450
2024-02-10 18:16:50,326 Epoch 4450: Total Training Recognition Loss 0.04  Total Training Translation Loss 1.62 
2024-02-10 18:16:50,326 EPOCH 4451
2024-02-10 18:17:06,204 Epoch 4451: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.05 
2024-02-10 18:17:06,204 EPOCH 4452
2024-02-10 18:17:22,169 Epoch 4452: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.76 
2024-02-10 18:17:22,170 EPOCH 4453
2024-02-10 18:17:38,144 Epoch 4453: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.55 
2024-02-10 18:17:38,144 EPOCH 4454
2024-02-10 18:17:54,418 Epoch 4454: Total Training Recognition Loss 0.30  Total Training Translation Loss 0.40 
2024-02-10 18:17:54,419 EPOCH 4455
2024-02-10 18:18:10,627 Epoch 4455: Total Training Recognition Loss 0.17  Total Training Translation Loss 0.35 
2024-02-10 18:18:10,628 EPOCH 4456
2024-02-10 18:18:15,238 [Epoch: 4456 Step: 00040100] Batch Recognition Loss:   0.003942 => Gls Tokens per Sec:     1389 || Batch Translation Loss:   0.027617 => Txt Tokens per Sec:     3521 || Lr: 0.000100
2024-02-10 18:18:26,536 Epoch 4456: Total Training Recognition Loss 1.06  Total Training Translation Loss 0.30 
2024-02-10 18:18:26,537 EPOCH 4457
2024-02-10 18:18:42,312 Epoch 4457: Total Training Recognition Loss 0.13  Total Training Translation Loss 0.29 
2024-02-10 18:18:42,312 EPOCH 4458
2024-02-10 18:18:58,226 Epoch 4458: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.28 
2024-02-10 18:18:58,227 EPOCH 4459
2024-02-10 18:19:14,426 Epoch 4459: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.26 
2024-02-10 18:19:14,427 EPOCH 4460
2024-02-10 18:19:30,566 Epoch 4460: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 18:19:30,566 EPOCH 4461
2024-02-10 18:19:46,386 Epoch 4461: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.20 
2024-02-10 18:19:46,386 EPOCH 4462
2024-02-10 18:20:02,600 Epoch 4462: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 18:20:02,601 EPOCH 4463
2024-02-10 18:20:18,865 Epoch 4463: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 18:20:18,866 EPOCH 4464
2024-02-10 18:20:35,262 Epoch 4464: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 18:20:35,263 EPOCH 4465
2024-02-10 18:20:51,572 Epoch 4465: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 18:20:51,573 EPOCH 4466
2024-02-10 18:21:07,687 Epoch 4466: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 18:21:07,688 EPOCH 4467
2024-02-10 18:21:19,990 [Epoch: 4467 Step: 00040200] Batch Recognition Loss:   0.001796 => Gls Tokens per Sec:      551 || Batch Translation Loss:   0.020229 => Txt Tokens per Sec:     1523 || Lr: 0.000100
2024-02-10 18:21:24,099 Epoch 4467: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 18:21:24,100 EPOCH 4468
2024-02-10 18:21:40,260 Epoch 4468: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 18:21:40,261 EPOCH 4469
2024-02-10 18:21:57,102 Epoch 4469: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 18:21:57,102 EPOCH 4470
2024-02-10 18:22:13,706 Epoch 4470: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 18:22:13,706 EPOCH 4471
2024-02-10 18:22:29,797 Epoch 4471: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 18:22:29,798 EPOCH 4472
2024-02-10 18:22:45,857 Epoch 4472: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 18:22:45,858 EPOCH 4473
2024-02-10 18:23:01,991 Epoch 4473: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 18:23:01,992 EPOCH 4474
2024-02-10 18:23:18,387 Epoch 4474: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 18:23:18,388 EPOCH 4475
2024-02-10 18:23:34,674 Epoch 4475: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 18:23:34,674 EPOCH 4476
2024-02-10 18:23:50,829 Epoch 4476: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 18:23:50,829 EPOCH 4477
2024-02-10 18:24:06,990 Epoch 4477: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 18:24:06,991 EPOCH 4478
2024-02-10 18:24:22,251 [Epoch: 4478 Step: 00040300] Batch Recognition Loss:   0.000219 => Gls Tokens per Sec:      528 || Batch Translation Loss:   0.008062 => Txt Tokens per Sec:     1452 || Lr: 0.000100
2024-02-10 18:24:23,273 Epoch 4478: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 18:24:23,274 EPOCH 4479
2024-02-10 18:24:39,433 Epoch 4479: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 18:24:39,434 EPOCH 4480
2024-02-10 18:24:55,678 Epoch 4480: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 18:24:55,678 EPOCH 4481
2024-02-10 18:25:11,794 Epoch 4481: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 18:25:11,795 EPOCH 4482
2024-02-10 18:25:28,019 Epoch 4482: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-10 18:25:28,019 EPOCH 4483
2024-02-10 18:25:44,373 Epoch 4483: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 18:25:44,374 EPOCH 4484
2024-02-10 18:26:00,485 Epoch 4484: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 18:26:00,486 EPOCH 4485
2024-02-10 18:26:16,652 Epoch 4485: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 18:26:16,653 EPOCH 4486
2024-02-10 18:26:32,501 Epoch 4486: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-10 18:26:32,502 EPOCH 4487
2024-02-10 18:26:48,799 Epoch 4487: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 18:26:48,799 EPOCH 4488
2024-02-10 18:27:05,026 Epoch 4488: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 18:27:05,027 EPOCH 4489
2024-02-10 18:27:16,620 [Epoch: 4489 Step: 00040400] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:      883 || Batch Translation Loss:   0.016539 => Txt Tokens per Sec:     2417 || Lr: 0.000100
2024-02-10 18:27:20,908 Epoch 4489: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-10 18:27:20,908 EPOCH 4490
2024-02-10 18:27:36,956 Epoch 4490: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-10 18:27:36,957 EPOCH 4491
2024-02-10 18:27:52,662 Epoch 4491: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 18:27:52,663 EPOCH 4492
2024-02-10 18:28:08,669 Epoch 4492: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 18:28:08,669 EPOCH 4493
2024-02-10 18:28:24,764 Epoch 4493: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 18:28:24,765 EPOCH 4494
2024-02-10 18:28:40,909 Epoch 4494: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 18:28:40,909 EPOCH 4495
2024-02-10 18:28:56,856 Epoch 4495: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 18:28:56,857 EPOCH 4496
2024-02-10 18:29:12,963 Epoch 4496: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 18:29:12,963 EPOCH 4497
2024-02-10 18:29:29,046 Epoch 4497: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 18:29:29,046 EPOCH 4498
2024-02-10 18:29:45,231 Epoch 4498: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 18:29:45,232 EPOCH 4499
2024-02-10 18:30:01,267 Epoch 4499: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 18:30:01,267 EPOCH 4500
2024-02-10 18:30:17,699 [Epoch: 4500 Step: 00040500] Batch Recognition Loss:   0.000235 => Gls Tokens per Sec:      646 || Batch Translation Loss:   0.018613 => Txt Tokens per Sec:     1788 || Lr: 0.000100
2024-02-10 18:30:17,699 Epoch 4500: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 18:30:17,700 EPOCH 4501
2024-02-10 18:30:33,689 Epoch 4501: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 18:30:33,690 EPOCH 4502
2024-02-10 18:30:49,467 Epoch 4502: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 18:30:49,467 EPOCH 4503
2024-02-10 18:31:05,789 Epoch 4503: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 18:31:05,790 EPOCH 4504
2024-02-10 18:31:21,855 Epoch 4504: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 18:31:21,856 EPOCH 4505
2024-02-10 18:31:37,943 Epoch 4505: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 18:31:37,944 EPOCH 4506
2024-02-10 18:31:53,868 Epoch 4506: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 18:31:53,869 EPOCH 4507
2024-02-10 18:32:09,976 Epoch 4507: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 18:32:09,976 EPOCH 4508
2024-02-10 18:32:25,870 Epoch 4508: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 18:32:25,870 EPOCH 4509
2024-02-10 18:32:42,192 Epoch 4509: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 18:32:42,192 EPOCH 4510
2024-02-10 18:32:58,922 Epoch 4510: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 18:32:58,922 EPOCH 4511
2024-02-10 18:33:15,284 Epoch 4511: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 18:33:15,285 EPOCH 4512
2024-02-10 18:33:15,755 [Epoch: 4512 Step: 00040600] Batch Recognition Loss:   0.000208 => Gls Tokens per Sec:     2729 || Batch Translation Loss:   0.023818 => Txt Tokens per Sec:     6949 || Lr: 0.000100
2024-02-10 18:33:31,429 Epoch 4512: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 18:33:31,429 EPOCH 4513
2024-02-10 18:33:47,308 Epoch 4513: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 18:33:47,309 EPOCH 4514
2024-02-10 18:34:03,467 Epoch 4514: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 18:34:03,467 EPOCH 4515
2024-02-10 18:34:19,271 Epoch 4515: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 18:34:19,272 EPOCH 4516
2024-02-10 18:34:35,356 Epoch 4516: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 18:34:35,356 EPOCH 4517
2024-02-10 18:34:51,559 Epoch 4517: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 18:34:51,559 EPOCH 4518
2024-02-10 18:35:07,670 Epoch 4518: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 18:35:07,671 EPOCH 4519
2024-02-10 18:35:23,757 Epoch 4519: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 18:35:23,758 EPOCH 4520
2024-02-10 18:35:40,009 Epoch 4520: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 18:35:40,010 EPOCH 4521
2024-02-10 18:35:55,945 Epoch 4521: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 18:35:55,945 EPOCH 4522
2024-02-10 18:36:11,972 Epoch 4522: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 18:36:11,973 EPOCH 4523
2024-02-10 18:36:15,425 [Epoch: 4523 Step: 00040700] Batch Recognition Loss:   0.000983 => Gls Tokens per Sec:      742 || Batch Translation Loss:   0.016888 => Txt Tokens per Sec:     2221 || Lr: 0.000100
2024-02-10 18:36:27,982 Epoch 4523: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 18:36:27,983 EPOCH 4524
2024-02-10 18:36:44,247 Epoch 4524: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 18:36:44,248 EPOCH 4525
2024-02-10 18:37:00,520 Epoch 4525: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 18:37:00,521 EPOCH 4526
2024-02-10 18:37:16,620 Epoch 4526: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 18:37:16,621 EPOCH 4527
2024-02-10 18:37:32,834 Epoch 4527: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-10 18:37:32,835 EPOCH 4528
2024-02-10 18:37:48,852 Epoch 4528: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-10 18:37:48,853 EPOCH 4529
2024-02-10 18:38:04,738 Epoch 4529: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-10 18:38:04,738 EPOCH 4530
2024-02-10 18:38:20,832 Epoch 4530: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-10 18:38:20,833 EPOCH 4531
2024-02-10 18:38:37,230 Epoch 4531: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-10 18:38:37,231 EPOCH 4532
2024-02-10 18:38:53,331 Epoch 4532: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 18:38:53,332 EPOCH 4533
2024-02-10 18:39:09,199 Epoch 4533: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-10 18:39:09,200 EPOCH 4534
2024-02-10 18:39:13,378 [Epoch: 4534 Step: 00040800] Batch Recognition Loss:   0.000319 => Gls Tokens per Sec:      919 || Batch Translation Loss:   0.028551 => Txt Tokens per Sec:     2799 || Lr: 0.000100
2024-02-10 18:39:25,096 Epoch 4534: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 18:39:25,097 EPOCH 4535
2024-02-10 18:39:41,166 Epoch 4535: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 18:39:41,166 EPOCH 4536
2024-02-10 18:39:57,138 Epoch 4536: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 18:39:57,139 EPOCH 4537
2024-02-10 18:40:12,947 Epoch 4537: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-10 18:40:12,947 EPOCH 4538
2024-02-10 18:40:29,220 Epoch 4538: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.01 
2024-02-10 18:40:29,220 EPOCH 4539
2024-02-10 18:40:45,135 Epoch 4539: Total Training Recognition Loss 0.02  Total Training Translation Loss 5.37 
2024-02-10 18:40:45,136 EPOCH 4540
2024-02-10 18:41:01,072 Epoch 4540: Total Training Recognition Loss 0.04  Total Training Translation Loss 2.53 
2024-02-10 18:41:01,072 EPOCH 4541
2024-02-10 18:41:17,278 Epoch 4541: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.62 
2024-02-10 18:41:17,278 EPOCH 4542
2024-02-10 18:41:33,114 Epoch 4542: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.00 
2024-02-10 18:41:33,114 EPOCH 4543
2024-02-10 18:41:49,185 Epoch 4543: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.68 
2024-02-10 18:41:49,185 EPOCH 4544
2024-02-10 18:42:05,293 Epoch 4544: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-10 18:42:05,293 EPOCH 4545
2024-02-10 18:42:13,911 [Epoch: 4545 Step: 00040900] Batch Recognition Loss:   0.000642 => Gls Tokens per Sec:      490 || Batch Translation Loss:   0.053295 => Txt Tokens per Sec:     1513 || Lr: 0.000100
2024-02-10 18:42:21,218 Epoch 4545: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-10 18:42:21,219 EPOCH 4546
2024-02-10 18:42:37,316 Epoch 4546: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-10 18:42:37,317 EPOCH 4547
2024-02-10 18:42:53,493 Epoch 4547: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-10 18:42:53,494 EPOCH 4548
2024-02-10 18:43:09,363 Epoch 4548: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 18:43:09,364 EPOCH 4549
2024-02-10 18:43:25,109 Epoch 4549: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 18:43:25,110 EPOCH 4550
2024-02-10 18:43:41,061 Epoch 4550: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 18:43:41,061 EPOCH 4551
2024-02-10 18:43:57,473 Epoch 4551: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 18:43:57,474 EPOCH 4552
2024-02-10 18:44:13,390 Epoch 4552: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 18:44:13,390 EPOCH 4553
2024-02-10 18:44:29,407 Epoch 4553: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 18:44:29,407 EPOCH 4554
2024-02-10 18:44:45,286 Epoch 4554: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 18:44:45,286 EPOCH 4555
2024-02-10 18:45:01,248 Epoch 4555: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 18:45:01,248 EPOCH 4556
2024-02-10 18:45:11,378 [Epoch: 4556 Step: 00041000] Batch Recognition Loss:   0.000305 => Gls Tokens per Sec:      632 || Batch Translation Loss:   0.009144 => Txt Tokens per Sec:     1698 || Lr: 0.000100
2024-02-10 18:45:17,293 Epoch 4556: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 18:45:17,294 EPOCH 4557
2024-02-10 18:45:33,549 Epoch 4557: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 18:45:33,550 EPOCH 4558
2024-02-10 18:45:49,752 Epoch 4558: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 18:45:49,753 EPOCH 4559
2024-02-10 18:46:05,578 Epoch 4559: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 18:46:05,579 EPOCH 4560
2024-02-10 18:46:21,525 Epoch 4560: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 18:46:21,526 EPOCH 4561
2024-02-10 18:46:37,647 Epoch 4561: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 18:46:37,647 EPOCH 4562
2024-02-10 18:46:53,749 Epoch 4562: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 18:46:53,750 EPOCH 4563
2024-02-10 18:47:09,713 Epoch 4563: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 18:47:09,713 EPOCH 4564
2024-02-10 18:47:26,005 Epoch 4564: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 18:47:26,006 EPOCH 4565
2024-02-10 18:47:41,953 Epoch 4565: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 18:47:41,954 EPOCH 4566
2024-02-10 18:47:58,178 Epoch 4566: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 18:47:58,178 EPOCH 4567
2024-02-10 18:48:09,246 [Epoch: 4567 Step: 00041100] Batch Recognition Loss:   0.000261 => Gls Tokens per Sec:      694 || Batch Translation Loss:   0.015499 => Txt Tokens per Sec:     2027 || Lr: 0.000100
2024-02-10 18:48:14,144 Epoch 4567: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 18:48:14,145 EPOCH 4568
2024-02-10 18:48:30,134 Epoch 4568: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 18:48:30,134 EPOCH 4569
2024-02-10 18:48:46,428 Epoch 4569: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 18:48:46,428 EPOCH 4570
2024-02-10 18:49:02,460 Epoch 4570: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 18:49:02,460 EPOCH 4571
2024-02-10 18:49:18,539 Epoch 4571: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 18:49:18,539 EPOCH 4572
2024-02-10 18:49:34,555 Epoch 4572: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 18:49:34,555 EPOCH 4573
2024-02-10 18:49:50,328 Epoch 4573: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 18:49:50,330 EPOCH 4574
2024-02-10 18:50:06,463 Epoch 4574: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 18:50:06,463 EPOCH 4575
2024-02-10 18:50:22,626 Epoch 4575: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 18:50:22,627 EPOCH 4576
2024-02-10 18:50:38,515 Epoch 4576: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-10 18:50:38,515 EPOCH 4577
2024-02-10 18:50:54,879 Epoch 4577: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 18:50:54,880 EPOCH 4578
2024-02-10 18:51:07,195 [Epoch: 4578 Step: 00041200] Batch Recognition Loss:   0.000661 => Gls Tokens per Sec:      655 || Batch Translation Loss:   0.016845 => Txt Tokens per Sec:     1739 || Lr: 0.000100
2024-02-10 18:51:11,286 Epoch 4578: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 18:51:11,287 EPOCH 4579
2024-02-10 18:51:27,292 Epoch 4579: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 18:51:27,292 EPOCH 4580
2024-02-10 18:51:43,591 Epoch 4580: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 18:51:43,591 EPOCH 4581
2024-02-10 18:52:00,359 Epoch 4581: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 18:52:00,359 EPOCH 4582
2024-02-10 18:52:16,295 Epoch 4582: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 18:52:16,295 EPOCH 4583
2024-02-10 18:52:32,321 Epoch 4583: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 18:52:32,321 EPOCH 4584
2024-02-10 18:52:48,065 Epoch 4584: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 18:52:48,065 EPOCH 4585
2024-02-10 18:53:04,199 Epoch 4585: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 18:53:04,200 EPOCH 4586
2024-02-10 18:53:20,198 Epoch 4586: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 18:53:20,199 EPOCH 4587
2024-02-10 18:53:36,451 Epoch 4587: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 18:53:36,452 EPOCH 4588
2024-02-10 18:53:52,608 Epoch 4588: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 18:53:52,608 EPOCH 4589
2024-02-10 18:54:08,388 [Epoch: 4589 Step: 00041300] Batch Recognition Loss:   0.001262 => Gls Tokens per Sec:      592 || Batch Translation Loss:   0.012154 => Txt Tokens per Sec:     1656 || Lr: 0.000100
2024-02-10 18:54:08,766 Epoch 4589: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-10 18:54:08,767 EPOCH 4590
2024-02-10 18:54:24,717 Epoch 4590: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 18:54:24,718 EPOCH 4591
2024-02-10 18:54:40,867 Epoch 4591: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 18:54:40,867 EPOCH 4592
2024-02-10 18:54:56,862 Epoch 4592: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 18:54:56,863 EPOCH 4593
2024-02-10 18:55:12,707 Epoch 4593: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 18:55:12,708 EPOCH 4594
2024-02-10 18:55:29,048 Epoch 4594: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 18:55:29,048 EPOCH 4595
2024-02-10 18:55:45,191 Epoch 4595: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 18:55:45,192 EPOCH 4596
2024-02-10 18:56:01,180 Epoch 4596: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 18:56:01,181 EPOCH 4597
2024-02-10 18:56:17,266 Epoch 4597: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 18:56:17,267 EPOCH 4598
2024-02-10 18:56:33,145 Epoch 4598: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 18:56:33,146 EPOCH 4599
2024-02-10 18:56:49,306 Epoch 4599: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 18:56:49,307 EPOCH 4600
2024-02-10 18:57:05,308 [Epoch: 4600 Step: 00041400] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:      664 || Batch Translation Loss:   0.017527 => Txt Tokens per Sec:     1836 || Lr: 0.000100
2024-02-10 18:57:05,309 Epoch 4600: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 18:57:05,309 EPOCH 4601
2024-02-10 18:57:21,476 Epoch 4601: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 18:57:21,477 EPOCH 4602
2024-02-10 18:57:37,331 Epoch 4602: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 18:57:37,331 EPOCH 4603
2024-02-10 18:57:53,416 Epoch 4603: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 18:57:53,416 EPOCH 4604
2024-02-10 18:58:09,342 Epoch 4604: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 18:58:09,342 EPOCH 4605
2024-02-10 18:58:25,508 Epoch 4605: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 18:58:25,509 EPOCH 4606
2024-02-10 18:58:41,592 Epoch 4606: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 18:58:41,592 EPOCH 4607
2024-02-10 18:58:57,504 Epoch 4607: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 18:58:57,504 EPOCH 4608
2024-02-10 18:59:13,627 Epoch 4608: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 18:59:13,628 EPOCH 4609
2024-02-10 18:59:29,466 Epoch 4609: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 18:59:29,467 EPOCH 4610
2024-02-10 18:59:45,682 Epoch 4610: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 18:59:45,683 EPOCH 4611
2024-02-10 19:00:01,687 Epoch 4611: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:00:01,688 EPOCH 4612
2024-02-10 19:00:02,060 [Epoch: 4612 Step: 00041500] Batch Recognition Loss:   0.000194 => Gls Tokens per Sec:     3450 || Batch Translation Loss:   0.009939 => Txt Tokens per Sec:     8806 || Lr: 0.000100
2024-02-10 19:00:17,927 Epoch 4612: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:00:17,928 EPOCH 4613
2024-02-10 19:00:33,946 Epoch 4613: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:00:33,946 EPOCH 4614
2024-02-10 19:00:50,078 Epoch 4614: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:00:50,079 EPOCH 4615
2024-02-10 19:01:06,221 Epoch 4615: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:01:06,221 EPOCH 4616
2024-02-10 19:01:22,227 Epoch 4616: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:01:22,228 EPOCH 4617
2024-02-10 19:01:38,529 Epoch 4617: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:01:38,529 EPOCH 4618
2024-02-10 19:01:54,497 Epoch 4618: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:01:54,497 EPOCH 4619
2024-02-10 19:02:10,274 Epoch 4619: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:02:10,275 EPOCH 4620
2024-02-10 19:02:26,172 Epoch 4620: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 19:02:26,173 EPOCH 4621
2024-02-10 19:02:42,181 Epoch 4621: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:02:42,181 EPOCH 4622
2024-02-10 19:02:58,434 Epoch 4622: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 19:02:58,435 EPOCH 4623
2024-02-10 19:02:59,141 [Epoch: 4623 Step: 00041600] Batch Recognition Loss:   0.000113 => Gls Tokens per Sec:     3631 || Batch Translation Loss:   0.015064 => Txt Tokens per Sec:     8207 || Lr: 0.000100
2024-02-10 19:03:14,432 Epoch 4623: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:03:14,432 EPOCH 4624
2024-02-10 19:03:30,784 Epoch 4624: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 19:03:30,784 EPOCH 4625
2024-02-10 19:03:46,773 Epoch 4625: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 19:03:46,773 EPOCH 4626
2024-02-10 19:04:02,807 Epoch 4626: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:04:02,807 EPOCH 4627
2024-02-10 19:04:18,750 Epoch 4627: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:04:18,750 EPOCH 4628
2024-02-10 19:04:34,704 Epoch 4628: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:04:34,705 EPOCH 4629
2024-02-10 19:04:50,698 Epoch 4629: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:04:50,699 EPOCH 4630
2024-02-10 19:05:06,773 Epoch 4630: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:05:06,774 EPOCH 4631
2024-02-10 19:05:22,831 Epoch 4631: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:05:22,831 EPOCH 4632
2024-02-10 19:05:39,093 Epoch 4632: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:05:39,093 EPOCH 4633
2024-02-10 19:05:55,439 Epoch 4633: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 19:05:55,440 EPOCH 4634
2024-02-10 19:05:59,091 [Epoch: 4634 Step: 00041700] Batch Recognition Loss:   0.000871 => Gls Tokens per Sec:     1052 || Batch Translation Loss:   0.006912 => Txt Tokens per Sec:     2523 || Lr: 0.000100
2024-02-10 19:06:11,419 Epoch 4634: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 19:06:11,419 EPOCH 4635
2024-02-10 19:06:27,059 Epoch 4635: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 19:06:27,060 EPOCH 4636
2024-02-10 19:06:43,272 Epoch 4636: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 19:06:43,272 EPOCH 4637
2024-02-10 19:06:59,651 Epoch 4637: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-10 19:06:59,652 EPOCH 4638
2024-02-10 19:07:15,713 Epoch 4638: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-10 19:07:15,714 EPOCH 4639
2024-02-10 19:07:31,950 Epoch 4639: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-10 19:07:31,950 EPOCH 4640
2024-02-10 19:07:48,041 Epoch 4640: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-10 19:07:48,042 EPOCH 4641
2024-02-10 19:08:03,940 Epoch 4641: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-10 19:08:03,941 EPOCH 4642
2024-02-10 19:08:19,968 Epoch 4642: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-10 19:08:19,969 EPOCH 4643
2024-02-10 19:08:36,267 Epoch 4643: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-10 19:08:36,267 EPOCH 4644
2024-02-10 19:08:52,614 Epoch 4644: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 19:08:52,615 EPOCH 4645
2024-02-10 19:08:54,124 [Epoch: 4645 Step: 00041800] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:     3395 || Batch Translation Loss:   0.018758 => Txt Tokens per Sec:     7926 || Lr: 0.000100
2024-02-10 19:09:08,470 Epoch 4645: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 19:09:08,470 EPOCH 4646
2024-02-10 19:09:24,402 Epoch 4646: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 19:09:24,402 EPOCH 4647
2024-02-10 19:09:40,624 Epoch 4647: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-10 19:09:40,625 EPOCH 4648
2024-02-10 19:09:56,683 Epoch 4648: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 19:09:56,684 EPOCH 4649
2024-02-10 19:10:12,725 Epoch 4649: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 19:10:12,726 EPOCH 4650
2024-02-10 19:10:29,474 Epoch 4650: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 19:10:29,475 EPOCH 4651
2024-02-10 19:10:45,782 Epoch 4651: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 19:10:45,782 EPOCH 4652
2024-02-10 19:11:01,669 Epoch 4652: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 19:11:01,670 EPOCH 4653
2024-02-10 19:11:17,732 Epoch 4653: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 19:11:17,733 EPOCH 4654
2024-02-10 19:11:33,848 Epoch 4654: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 19:11:33,848 EPOCH 4655
2024-02-10 19:11:49,864 Epoch 4655: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 19:11:49,864 EPOCH 4656
2024-02-10 19:12:04,054 [Epoch: 4656 Step: 00041900] Batch Recognition Loss:   0.000982 => Gls Tokens per Sec:      388 || Batch Translation Loss:   0.007096 => Txt Tokens per Sec:     1134 || Lr: 0.000100
2024-02-10 19:12:05,714 Epoch 4656: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 19:12:05,714 EPOCH 4657
2024-02-10 19:12:21,502 Epoch 4657: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 19:12:21,503 EPOCH 4658
2024-02-10 19:12:37,759 Epoch 4658: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 19:12:37,759 EPOCH 4659
2024-02-10 19:12:54,101 Epoch 4659: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 19:12:54,102 EPOCH 4660
2024-02-10 19:13:09,930 Epoch 4660: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 19:13:09,931 EPOCH 4661
2024-02-10 19:13:26,337 Epoch 4661: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 19:13:26,338 EPOCH 4662
2024-02-10 19:13:42,645 Epoch 4662: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 19:13:42,646 EPOCH 4663
2024-02-10 19:13:59,039 Epoch 4663: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 19:13:59,040 EPOCH 4664
2024-02-10 19:14:15,345 Epoch 4664: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 19:14:15,345 EPOCH 4665
2024-02-10 19:14:31,634 Epoch 4665: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 19:14:31,634 EPOCH 4666
2024-02-10 19:14:48,079 Epoch 4666: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 19:14:48,079 EPOCH 4667
2024-02-10 19:15:02,853 [Epoch: 4667 Step: 00042000] Batch Recognition Loss:   0.000442 => Gls Tokens per Sec:      459 || Batch Translation Loss:   0.027617 => Txt Tokens per Sec:     1364 || Lr: 0.000100
2024-02-10 19:16:14,353 Validation result at epoch 4667, step    42000: duration: 71.4985s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.29070	Translation Loss: 101333.43750	PPL: 24864.53125
	Eval Metric: BLEU
	WER 2.75	(DEL: 0.00,	INS: 0.00,	SUB: 2.75)
	BLEU-4 0.49	(BLEU-1: 10.64,	BLEU-2: 3.09,	BLEU-3: 1.10,	BLEU-4: 0.49)
	CHRF 17.13	ROUGE 8.86
2024-02-10 19:16:14,355 Logging Recognition and Translation Outputs
2024-02-10 19:16:14,355 ========================================================================================================================
2024-02-10 19:16:14,355 Logging Sequence: 154_94.00
2024-02-10 19:16:14,355 	Gloss Reference :	A B+C+D+E
2024-02-10 19:16:14,356 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 19:16:14,356 	Gloss Alignment :	         
2024-02-10 19:16:14,356 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 19:16:14,357 	Text Reference  :	the ipl will also be  held in     uae    from september 19  to october 15         
2024-02-10 19:16:14,357 	Text Hypothesis :	*** *** **** **** was then deepak chahar have could     not at the     coronavirus
2024-02-10 19:16:14,357 	Text Alignment  :	D   D   D    D    S   S    S      S      S    S         S   S  S       S          
2024-02-10 19:16:14,357 ========================================================================================================================
2024-02-10 19:16:14,357 Logging Sequence: 118_2.00
2024-02-10 19:16:14,358 	Gloss Reference :	A B+C+D+E
2024-02-10 19:16:14,358 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 19:16:14,358 	Gloss Alignment :	         
2024-02-10 19:16:14,358 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 19:16:14,359 	Text Reference  :	yesterday was a very exciting day people across the world were watching
2024-02-10 19:16:14,359 	Text Hypothesis :	********* *** * **** ******** *** ****** ****** the video went viral   
2024-02-10 19:16:14,359 	Text Alignment  :	D         D   D D    D        D   D      D          S     S    S       
2024-02-10 19:16:14,359 ========================================================================================================================
2024-02-10 19:16:14,359 Logging Sequence: 165_453.00
2024-02-10 19:16:14,359 	Gloss Reference :	A B+C+D+E
2024-02-10 19:16:14,360 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 19:16:14,360 	Gloss Alignment :	         
2024-02-10 19:16:14,360 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 19:16:14,361 	Text Reference  :	****** icc  did      not agree to    sehwag' decision of  wearing a  numberless jersey 
2024-02-10 19:16:14,361 	Text Hypothesis :	police have qualifed for the   match on      28th     may now     be all        matches
2024-02-10 19:16:14,361 	Text Alignment  :	I      S    S        S   S     S     S       S        S   S       S  S          S      
2024-02-10 19:16:14,361 ========================================================================================================================
2024-02-10 19:16:14,361 Logging Sequence: 126_163.00
2024-02-10 19:16:14,362 	Gloss Reference :	A B+C+D+E
2024-02-10 19:16:14,362 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 19:16:14,362 	Gloss Alignment :	         
2024-02-10 19:16:14,362 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 19:16:14,363 	Text Reference  :	* **** ****** your hard  work has  helped secure a  medal at    the tokyo olympics
2024-02-10 19:16:14,363 	Text Hypothesis :	i were unable to   train and  this is     i      am very  grate to  my    country 
2024-02-10 19:16:14,364 	Text Alignment  :	I I    I      S    S     S    S    S      S      S  S     S     S   S     S       
2024-02-10 19:16:14,364 ========================================================================================================================
2024-02-10 19:16:14,364 Logging Sequence: 84_2.00
2024-02-10 19:16:14,364 	Gloss Reference :	A B+C+D+E
2024-02-10 19:16:14,364 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 19:16:14,364 	Gloss Alignment :	         
2024-02-10 19:16:14,365 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 19:16:14,367 	Text Reference  :	the 2022 fifa football world cup is going on in qatar  from 20th  november 2022 to  18th december 2022
2024-02-10 19:16:14,367 	Text Hypothesis :	*** **** **** police   said  'i  am going ** to retire from virat kohli    with 211 away 8        days
2024-02-10 19:16:14,367 	Text Alignment  :	D   D    D    S        S     S   S        D  S  S           S     S        S    S   S    S        S   
2024-02-10 19:16:14,367 ========================================================================================================================
2024-02-10 19:16:15,868 Epoch 4667: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 19:16:15,869 EPOCH 4668
2024-02-10 19:16:32,029 Epoch 4668: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 19:16:32,029 EPOCH 4669
2024-02-10 19:16:47,739 Epoch 4669: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 19:16:47,739 EPOCH 4670
2024-02-10 19:17:03,647 Epoch 4670: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 19:17:03,647 EPOCH 4671
2024-02-10 19:17:20,202 Epoch 4671: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 19:17:20,203 EPOCH 4672
2024-02-10 19:17:36,104 Epoch 4672: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 19:17:36,104 EPOCH 4673
2024-02-10 19:17:52,113 Epoch 4673: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 19:17:52,113 EPOCH 4674
2024-02-10 19:18:08,287 Epoch 4674: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 19:18:08,288 EPOCH 4675
2024-02-10 19:18:24,347 Epoch 4675: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-10 19:18:24,347 EPOCH 4676
2024-02-10 19:18:40,573 Epoch 4676: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 19:18:40,574 EPOCH 4677
2024-02-10 19:18:56,590 Epoch 4677: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-10 19:18:56,591 EPOCH 4678
2024-02-10 19:19:09,010 [Epoch: 4678 Step: 00042100] Batch Recognition Loss:   0.000345 => Gls Tokens per Sec:      649 || Batch Translation Loss:   0.039865 => Txt Tokens per Sec:     1753 || Lr: 0.000100
2024-02-10 19:19:12,795 Epoch 4678: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-10 19:19:12,796 EPOCH 4679
2024-02-10 19:19:28,645 Epoch 4679: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 19:19:28,646 EPOCH 4680
2024-02-10 19:19:44,991 Epoch 4680: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-10 19:19:44,992 EPOCH 4681
2024-02-10 19:20:00,863 Epoch 4681: Total Training Recognition Loss 0.00  Total Training Translation Loss 2.01 
2024-02-10 19:20:00,863 EPOCH 4682
2024-02-10 19:20:17,176 Epoch 4682: Total Training Recognition Loss 0.02  Total Training Translation Loss 4.49 
2024-02-10 19:20:17,176 EPOCH 4683
2024-02-10 19:20:33,300 Epoch 4683: Total Training Recognition Loss 0.05  Total Training Translation Loss 7.45 
2024-02-10 19:20:33,300 EPOCH 4684
2024-02-10 19:20:49,390 Epoch 4684: Total Training Recognition Loss 0.09  Total Training Translation Loss 7.31 
2024-02-10 19:20:49,390 EPOCH 4685
2024-02-10 19:21:05,333 Epoch 4685: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.92 
2024-02-10 19:21:05,333 EPOCH 4686
2024-02-10 19:21:21,237 Epoch 4686: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.18 
2024-02-10 19:21:21,237 EPOCH 4687
2024-02-10 19:21:37,048 Epoch 4687: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.24 
2024-02-10 19:21:37,049 EPOCH 4688
2024-02-10 19:21:52,926 Epoch 4688: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.91 
2024-02-10 19:21:52,926 EPOCH 4689
2024-02-10 19:22:09,521 [Epoch: 4689 Step: 00042200] Batch Recognition Loss:   0.001497 => Gls Tokens per Sec:      563 || Batch Translation Loss:   0.046246 => Txt Tokens per Sec:     1548 || Lr: 0.000100
2024-02-10 19:22:10,170 Epoch 4689: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-10 19:22:10,171 EPOCH 4690
2024-02-10 19:22:26,241 Epoch 4690: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-10 19:22:26,242 EPOCH 4691
2024-02-10 19:22:42,301 Epoch 4691: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-10 19:22:42,302 EPOCH 4692
2024-02-10 19:22:58,183 Epoch 4692: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 19:22:58,183 EPOCH 4693
2024-02-10 19:23:14,583 Epoch 4693: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 19:23:14,584 EPOCH 4694
2024-02-10 19:23:30,779 Epoch 4694: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 19:23:30,780 EPOCH 4695
2024-02-10 19:23:46,704 Epoch 4695: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 19:23:46,705 EPOCH 4696
2024-02-10 19:24:03,195 Epoch 4696: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 19:24:03,195 EPOCH 4697
2024-02-10 19:24:18,861 Epoch 4697: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 19:24:18,862 EPOCH 4698
2024-02-10 19:24:34,885 Epoch 4698: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 19:24:34,886 EPOCH 4699
2024-02-10 19:24:51,141 Epoch 4699: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 19:24:51,142 EPOCH 4700
2024-02-10 19:25:07,584 [Epoch: 4700 Step: 00042300] Batch Recognition Loss:   0.000719 => Gls Tokens per Sec:      646 || Batch Translation Loss:   0.051660 => Txt Tokens per Sec:     1787 || Lr: 0.000100
2024-02-10 19:25:07,585 Epoch 4700: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 19:25:07,585 EPOCH 4701
2024-02-10 19:25:23,969 Epoch 4701: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 19:25:23,970 EPOCH 4702
2024-02-10 19:25:40,269 Epoch 4702: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 19:25:40,270 EPOCH 4703
2024-02-10 19:25:56,521 Epoch 4703: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 19:25:56,522 EPOCH 4704
2024-02-10 19:26:12,592 Epoch 4704: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 19:26:12,592 EPOCH 4705
2024-02-10 19:26:28,677 Epoch 4705: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 19:26:28,678 EPOCH 4706
2024-02-10 19:26:44,434 Epoch 4706: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 19:26:44,435 EPOCH 4707
2024-02-10 19:27:00,901 Epoch 4707: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 19:27:00,901 EPOCH 4708
2024-02-10 19:27:17,136 Epoch 4708: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 19:27:17,137 EPOCH 4709
2024-02-10 19:27:33,402 Epoch 4709: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 19:27:33,403 EPOCH 4710
2024-02-10 19:27:49,454 Epoch 4710: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 19:27:49,454 EPOCH 4711
2024-02-10 19:28:05,809 Epoch 4711: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 19:28:05,810 EPOCH 4712
2024-02-10 19:28:06,550 [Epoch: 4712 Step: 00042400] Batch Recognition Loss:   0.000496 => Gls Tokens per Sec:     1734 || Batch Translation Loss:   0.019317 => Txt Tokens per Sec:     5228 || Lr: 0.000100
2024-02-10 19:28:21,751 Epoch 4712: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 19:28:21,752 EPOCH 4713
2024-02-10 19:28:37,994 Epoch 4713: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 19:28:37,995 EPOCH 4714
2024-02-10 19:28:53,881 Epoch 4714: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 19:28:53,881 EPOCH 4715
2024-02-10 19:29:09,887 Epoch 4715: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:29:09,887 EPOCH 4716
2024-02-10 19:29:26,361 Epoch 4716: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:29:26,362 EPOCH 4717
2024-02-10 19:29:42,697 Epoch 4717: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:29:42,697 EPOCH 4718
2024-02-10 19:29:58,859 Epoch 4718: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:29:58,860 EPOCH 4719
2024-02-10 19:30:15,280 Epoch 4719: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:30:15,280 EPOCH 4720
2024-02-10 19:30:31,345 Epoch 4720: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:30:31,346 EPOCH 4721
2024-02-10 19:30:47,293 Epoch 4721: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:30:47,293 EPOCH 4722
2024-02-10 19:31:03,366 Epoch 4722: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:31:03,366 EPOCH 4723
2024-02-10 19:31:06,956 [Epoch: 4723 Step: 00042500] Batch Recognition Loss:   0.000261 => Gls Tokens per Sec:      713 || Batch Translation Loss:   0.011027 => Txt Tokens per Sec:     2123 || Lr: 0.000100
2024-02-10 19:31:19,395 Epoch 4723: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:31:19,395 EPOCH 4724
2024-02-10 19:31:35,493 Epoch 4724: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:31:35,494 EPOCH 4725
2024-02-10 19:31:51,565 Epoch 4725: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:31:51,566 EPOCH 4726
2024-02-10 19:32:07,667 Epoch 4726: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:32:07,667 EPOCH 4727
2024-02-10 19:32:23,555 Epoch 4727: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:32:23,556 EPOCH 4728
2024-02-10 19:32:39,706 Epoch 4728: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:32:39,706 EPOCH 4729
2024-02-10 19:32:56,629 Epoch 4729: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:32:56,629 EPOCH 4730
2024-02-10 19:33:12,832 Epoch 4730: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:33:12,832 EPOCH 4731
2024-02-10 19:33:28,942 Epoch 4731: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:33:28,943 EPOCH 4732
2024-02-10 19:33:45,227 Epoch 4732: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:33:45,227 EPOCH 4733
2024-02-10 19:34:01,391 Epoch 4733: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:34:01,392 EPOCH 4734
2024-02-10 19:34:12,458 [Epoch: 4734 Step: 00042600] Batch Recognition Loss:   0.000352 => Gls Tokens per Sec:      266 || Batch Translation Loss:   0.019627 => Txt Tokens per Sec:      869 || Lr: 0.000100
2024-02-10 19:34:17,594 Epoch 4734: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:34:17,594 EPOCH 4735
2024-02-10 19:34:33,932 Epoch 4735: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:34:33,932 EPOCH 4736
2024-02-10 19:34:50,146 Epoch 4736: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:34:50,147 EPOCH 4737
2024-02-10 19:35:06,277 Epoch 4737: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:35:06,278 EPOCH 4738
2024-02-10 19:35:22,581 Epoch 4738: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:35:22,582 EPOCH 4739
2024-02-10 19:35:38,847 Epoch 4739: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:35:38,848 EPOCH 4740
2024-02-10 19:35:54,937 Epoch 4740: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:35:54,938 EPOCH 4741
2024-02-10 19:36:10,928 Epoch 4741: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:36:10,929 EPOCH 4742
2024-02-10 19:36:26,934 Epoch 4742: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:36:26,934 EPOCH 4743
2024-02-10 19:36:43,110 Epoch 4743: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:36:43,110 EPOCH 4744
2024-02-10 19:36:59,039 Epoch 4744: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:36:59,040 EPOCH 4745
2024-02-10 19:37:09,965 [Epoch: 4745 Step: 00042700] Batch Recognition Loss:   0.000296 => Gls Tokens per Sec:      386 || Batch Translation Loss:   0.016267 => Txt Tokens per Sec:     1037 || Lr: 0.000100
2024-02-10 19:37:15,099 Epoch 4745: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-10 19:37:15,100 EPOCH 4746
2024-02-10 19:37:31,476 Epoch 4746: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-10 19:37:31,477 EPOCH 4747
2024-02-10 19:37:47,921 Epoch 4747: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-10 19:37:47,922 EPOCH 4748
2024-02-10 19:38:04,040 Epoch 4748: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 19:38:04,041 EPOCH 4749
2024-02-10 19:38:20,000 Epoch 4749: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 19:38:20,001 EPOCH 4750
2024-02-10 19:38:36,204 Epoch 4750: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 19:38:36,204 EPOCH 4751
2024-02-10 19:38:52,479 Epoch 4751: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 19:38:52,480 EPOCH 4752
2024-02-10 19:39:08,440 Epoch 4752: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 19:39:08,440 EPOCH 4753
2024-02-10 19:39:24,055 Epoch 4753: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 19:39:24,055 EPOCH 4754
2024-02-10 19:39:40,308 Epoch 4754: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 19:39:40,309 EPOCH 4755
2024-02-10 19:39:56,723 Epoch 4755: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 19:39:56,724 EPOCH 4756
2024-02-10 19:40:04,178 [Epoch: 4756 Step: 00042800] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:      859 || Batch Translation Loss:   0.015843 => Txt Tokens per Sec:     2310 || Lr: 0.000100
2024-02-10 19:40:12,542 Epoch 4756: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 19:40:12,543 EPOCH 4757
2024-02-10 19:40:29,056 Epoch 4757: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 19:40:29,056 EPOCH 4758
2024-02-10 19:40:44,883 Epoch 4758: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 19:40:44,884 EPOCH 4759
2024-02-10 19:41:01,083 Epoch 4759: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 19:41:01,084 EPOCH 4760
2024-02-10 19:41:17,371 Epoch 4760: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 19:41:17,372 EPOCH 4761
2024-02-10 19:41:33,391 Epoch 4761: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 19:41:33,392 EPOCH 4762
2024-02-10 19:41:49,616 Epoch 4762: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:41:49,617 EPOCH 4763
2024-02-10 19:42:05,694 Epoch 4763: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:42:05,695 EPOCH 4764
2024-02-10 19:42:21,859 Epoch 4764: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 19:42:21,860 EPOCH 4765
2024-02-10 19:42:38,208 Epoch 4765: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 19:42:38,208 EPOCH 4766
2024-02-10 19:42:54,276 Epoch 4766: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 19:42:54,277 EPOCH 4767
2024-02-10 19:43:04,873 [Epoch: 4767 Step: 00042900] Batch Recognition Loss:   0.000328 => Gls Tokens per Sec:      725 || Batch Translation Loss:   0.031539 => Txt Tokens per Sec:     1948 || Lr: 0.000100
2024-02-10 19:43:10,337 Epoch 4767: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 19:43:10,337 EPOCH 4768
2024-02-10 19:43:26,509 Epoch 4768: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 19:43:26,510 EPOCH 4769
2024-02-10 19:43:42,632 Epoch 4769: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 19:43:42,632 EPOCH 4770
2024-02-10 19:43:58,644 Epoch 4770: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 19:43:58,645 EPOCH 4771
2024-02-10 19:44:14,522 Epoch 4771: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 19:44:14,522 EPOCH 4772
2024-02-10 19:44:30,903 Epoch 4772: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 19:44:30,904 EPOCH 4773
2024-02-10 19:44:46,746 Epoch 4773: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:44:46,747 EPOCH 4774
2024-02-10 19:45:02,814 Epoch 4774: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:45:02,814 EPOCH 4775
2024-02-10 19:45:19,316 Epoch 4775: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 19:45:19,316 EPOCH 4776
2024-02-10 19:45:35,472 Epoch 4776: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:45:35,472 EPOCH 4777
2024-02-10 19:45:51,637 Epoch 4777: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-10 19:45:51,638 EPOCH 4778
2024-02-10 19:46:04,097 [Epoch: 4778 Step: 00043000] Batch Recognition Loss:   0.000432 => Gls Tokens per Sec:      647 || Batch Translation Loss:   0.019707 => Txt Tokens per Sec:     1845 || Lr: 0.000100
2024-02-10 19:46:07,570 Epoch 4778: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:46:07,570 EPOCH 4779
2024-02-10 19:46:23,754 Epoch 4779: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:46:23,754 EPOCH 4780
2024-02-10 19:46:40,218 Epoch 4780: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:46:40,218 EPOCH 4781
2024-02-10 19:46:56,247 Epoch 4781: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:46:56,248 EPOCH 4782
2024-02-10 19:47:12,177 Epoch 4782: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 19:47:12,178 EPOCH 4783
2024-02-10 19:47:28,280 Epoch 4783: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:47:28,281 EPOCH 4784
2024-02-10 19:47:44,294 Epoch 4784: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:47:44,295 EPOCH 4785
2024-02-10 19:48:00,149 Epoch 4785: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:48:00,150 EPOCH 4786
2024-02-10 19:48:16,435 Epoch 4786: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:48:16,435 EPOCH 4787
2024-02-10 19:48:32,374 Epoch 4787: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:48:32,375 EPOCH 4788
2024-02-10 19:48:48,154 Epoch 4788: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 19:48:48,155 EPOCH 4789
2024-02-10 19:49:00,173 [Epoch: 4789 Step: 00043100] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:      852 || Batch Translation Loss:   0.012457 => Txt Tokens per Sec:     2332 || Lr: 0.000100
2024-02-10 19:49:04,559 Epoch 4789: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:49:04,560 EPOCH 4790
2024-02-10 19:49:20,722 Epoch 4790: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:49:20,723 EPOCH 4791
2024-02-10 19:49:36,911 Epoch 4791: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:49:36,912 EPOCH 4792
2024-02-10 19:49:52,775 Epoch 4792: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:49:52,776 EPOCH 4793
2024-02-10 19:50:09,048 Epoch 4793: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:50:09,049 EPOCH 4794
2024-02-10 19:50:25,245 Epoch 4794: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:50:25,245 EPOCH 4795
2024-02-10 19:50:41,215 Epoch 4795: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:50:41,215 EPOCH 4796
2024-02-10 19:50:57,217 Epoch 4796: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:50:57,218 EPOCH 4797
2024-02-10 19:51:13,460 Epoch 4797: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:51:13,461 EPOCH 4798
2024-02-10 19:51:29,854 Epoch 4798: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:51:29,855 EPOCH 4799
2024-02-10 19:51:46,140 Epoch 4799: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 19:51:46,140 EPOCH 4800
2024-02-10 19:52:02,440 [Epoch: 4800 Step: 00043200] Batch Recognition Loss:   0.000091 => Gls Tokens per Sec:      652 || Batch Translation Loss:   0.008484 => Txt Tokens per Sec:     1803 || Lr: 0.000100
2024-02-10 19:52:02,440 Epoch 4800: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 19:52:02,441 EPOCH 4801
2024-02-10 19:52:18,775 Epoch 4801: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 19:52:18,775 EPOCH 4802
2024-02-10 19:52:34,662 Epoch 4802: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 19:52:34,663 EPOCH 4803
2024-02-10 19:52:50,543 Epoch 4803: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 19:52:50,543 EPOCH 4804
2024-02-10 19:53:06,814 Epoch 4804: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 19:53:06,815 EPOCH 4805
2024-02-10 19:53:22,877 Epoch 4805: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 19:53:22,878 EPOCH 4806
2024-02-10 19:53:39,109 Epoch 4806: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 19:53:39,110 EPOCH 4807
2024-02-10 19:53:55,272 Epoch 4807: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-10 19:53:55,273 EPOCH 4808
2024-02-10 19:54:11,417 Epoch 4808: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-10 19:54:11,418 EPOCH 4809
2024-02-10 19:54:27,537 Epoch 4809: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-10 19:54:27,538 EPOCH 4810
2024-02-10 19:54:43,539 Epoch 4810: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-10 19:54:43,540 EPOCH 4811
2024-02-10 19:54:59,653 Epoch 4811: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-10 19:54:59,654 EPOCH 4812
2024-02-10 19:55:00,019 [Epoch: 4812 Step: 00043300] Batch Recognition Loss:   0.000657 => Gls Tokens per Sec:     3516 || Batch Translation Loss:   0.064325 => Txt Tokens per Sec:     9610 || Lr: 0.000100
2024-02-10 19:55:15,738 Epoch 4812: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.53 
2024-02-10 19:55:15,738 EPOCH 4813
2024-02-10 19:55:31,763 Epoch 4813: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.61 
2024-02-10 19:55:31,763 EPOCH 4814
2024-02-10 19:55:47,877 Epoch 4814: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.21 
2024-02-10 19:55:47,878 EPOCH 4815
2024-02-10 19:56:03,995 Epoch 4815: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.44 
2024-02-10 19:56:03,995 EPOCH 4816
2024-02-10 19:56:19,690 Epoch 4816: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.11 
2024-02-10 19:56:19,691 EPOCH 4817
2024-02-10 19:56:35,646 Epoch 4817: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.81 
2024-02-10 19:56:35,647 EPOCH 4818
2024-02-10 19:56:51,506 Epoch 4818: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.67 
2024-02-10 19:56:51,507 EPOCH 4819
2024-02-10 19:57:07,760 Epoch 4819: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.52 
2024-02-10 19:57:07,761 EPOCH 4820
2024-02-10 19:57:23,769 Epoch 4820: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.88 
2024-02-10 19:57:23,770 EPOCH 4821
2024-02-10 19:57:39,691 Epoch 4821: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-10 19:57:39,691 EPOCH 4822
2024-02-10 19:57:55,881 Epoch 4822: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-10 19:57:55,882 EPOCH 4823
2024-02-10 19:58:02,137 [Epoch: 4823 Step: 00043400] Batch Recognition Loss:   0.002054 => Gls Tokens per Sec:      409 || Batch Translation Loss:   0.044137 => Txt Tokens per Sec:     1268 || Lr: 0.000100
2024-02-10 19:58:12,027 Epoch 4823: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-10 19:58:12,027 EPOCH 4824
2024-02-10 19:58:28,124 Epoch 4824: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-10 19:58:28,125 EPOCH 4825
2024-02-10 19:58:44,063 Epoch 4825: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-10 19:58:44,064 EPOCH 4826
2024-02-10 19:59:00,303 Epoch 4826: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-10 19:59:00,303 EPOCH 4827
2024-02-10 19:59:16,183 Epoch 4827: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 19:59:16,184 EPOCH 4828
2024-02-10 19:59:32,225 Epoch 4828: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 19:59:32,226 EPOCH 4829
2024-02-10 19:59:48,079 Epoch 4829: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 19:59:48,079 EPOCH 4830
2024-02-10 20:00:04,071 Epoch 4830: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 20:00:04,072 EPOCH 4831
2024-02-10 20:00:19,893 Epoch 4831: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 20:00:19,894 EPOCH 4832
2024-02-10 20:00:36,150 Epoch 4832: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 20:00:36,150 EPOCH 4833
2024-02-10 20:00:52,201 Epoch 4833: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 20:00:52,202 EPOCH 4834
2024-02-10 20:01:02,886 [Epoch: 4834 Step: 00043500] Batch Recognition Loss:   0.000469 => Gls Tokens per Sec:      275 || Batch Translation Loss:   0.026932 => Txt Tokens per Sec:      884 || Lr: 0.000100
2024-02-10 20:01:08,302 Epoch 4834: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 20:01:08,302 EPOCH 4835
2024-02-10 20:01:24,383 Epoch 4835: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 20:01:24,384 EPOCH 4836
2024-02-10 20:01:40,512 Epoch 4836: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 20:01:40,513 EPOCH 4837
2024-02-10 20:01:56,558 Epoch 4837: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 20:01:56,559 EPOCH 4838
2024-02-10 20:02:12,547 Epoch 4838: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 20:02:12,548 EPOCH 4839
2024-02-10 20:02:28,506 Epoch 4839: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 20:02:28,506 EPOCH 4840
2024-02-10 20:02:44,900 Epoch 4840: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-10 20:02:44,900 EPOCH 4841
2024-02-10 20:03:00,843 Epoch 4841: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 20:03:00,844 EPOCH 4842
2024-02-10 20:03:16,807 Epoch 4842: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 20:03:16,808 EPOCH 4843
2024-02-10 20:03:32,790 Epoch 4843: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 20:03:32,790 EPOCH 4844
2024-02-10 20:03:49,084 Epoch 4844: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 20:03:49,084 EPOCH 4845
2024-02-10 20:03:53,519 [Epoch: 4845 Step: 00043600] Batch Recognition Loss:   0.000269 => Gls Tokens per Sec:     1155 || Batch Translation Loss:   0.014995 => Txt Tokens per Sec:     3190 || Lr: 0.000100
2024-02-10 20:04:04,906 Epoch 4845: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 20:04:04,907 EPOCH 4846
2024-02-10 20:04:21,037 Epoch 4846: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 20:04:21,038 EPOCH 4847
2024-02-10 20:04:37,143 Epoch 4847: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 20:04:37,144 EPOCH 4848
2024-02-10 20:04:52,817 Epoch 4848: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 20:04:52,818 EPOCH 4849
2024-02-10 20:05:08,947 Epoch 4849: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.17 
2024-02-10 20:05:08,948 EPOCH 4850
2024-02-10 20:05:24,978 Epoch 4850: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.14 
2024-02-10 20:05:24,979 EPOCH 4851
2024-02-10 20:05:41,244 Epoch 4851: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 20:05:41,245 EPOCH 4852
2024-02-10 20:05:57,227 Epoch 4852: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 20:05:57,228 EPOCH 4853
2024-02-10 20:06:13,473 Epoch 4853: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 20:06:13,474 EPOCH 4854
2024-02-10 20:06:29,690 Epoch 4854: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 20:06:29,690 EPOCH 4855
2024-02-10 20:06:45,916 Epoch 4855: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 20:06:45,916 EPOCH 4856
2024-02-10 20:07:00,323 [Epoch: 4856 Step: 00043700] Batch Recognition Loss:   0.005636 => Gls Tokens per Sec:      382 || Batch Translation Loss:   0.005891 => Txt Tokens per Sec:     1191 || Lr: 0.000100
2024-02-10 20:07:02,022 Epoch 4856: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 20:07:02,022 EPOCH 4857
2024-02-10 20:07:17,932 Epoch 4857: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 20:07:17,933 EPOCH 4858
2024-02-10 20:07:34,560 Epoch 4858: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 20:07:34,561 EPOCH 4859
2024-02-10 20:07:50,578 Epoch 4859: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 20:07:50,579 EPOCH 4860
2024-02-10 20:08:06,628 Epoch 4860: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 20:08:06,628 EPOCH 4861
2024-02-10 20:08:22,942 Epoch 4861: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 20:08:22,942 EPOCH 4862
2024-02-10 20:08:39,029 Epoch 4862: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 20:08:39,029 EPOCH 4863
2024-02-10 20:08:55,111 Epoch 4863: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 20:08:55,112 EPOCH 4864
2024-02-10 20:09:10,993 Epoch 4864: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 20:09:10,993 EPOCH 4865
2024-02-10 20:09:27,189 Epoch 4865: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 20:09:27,189 EPOCH 4866
2024-02-10 20:09:43,139 Epoch 4866: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 20:09:43,139 EPOCH 4867
2024-02-10 20:09:53,982 [Epoch: 4867 Step: 00043800] Batch Recognition Loss:   0.000504 => Gls Tokens per Sec:      708 || Batch Translation Loss:   0.031015 => Txt Tokens per Sec:     2030 || Lr: 0.000100
2024-02-10 20:09:59,281 Epoch 4867: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 20:09:59,282 EPOCH 4868
2024-02-10 20:10:15,542 Epoch 4868: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-10 20:10:15,542 EPOCH 4869
2024-02-10 20:10:31,366 Epoch 4869: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 20:10:31,367 EPOCH 4870
2024-02-10 20:10:47,557 Epoch 4870: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 20:10:47,558 EPOCH 4871
2024-02-10 20:11:04,085 Epoch 4871: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 20:11:04,086 EPOCH 4872
2024-02-10 20:11:20,177 Epoch 4872: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 20:11:20,178 EPOCH 4873
2024-02-10 20:11:36,500 Epoch 4873: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 20:11:36,501 EPOCH 4874
2024-02-10 20:11:52,537 Epoch 4874: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 20:11:52,538 EPOCH 4875
2024-02-10 20:12:08,376 Epoch 4875: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 20:12:08,377 EPOCH 4876
2024-02-10 20:12:24,620 Epoch 4876: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 20:12:24,621 EPOCH 4877
2024-02-10 20:12:40,655 Epoch 4877: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 20:12:40,655 EPOCH 4878
2024-02-10 20:12:52,001 [Epoch: 4878 Step: 00043900] Batch Recognition Loss:   0.000588 => Gls Tokens per Sec:      790 || Batch Translation Loss:   0.019886 => Txt Tokens per Sec:     2180 || Lr: 0.000100
2024-02-10 20:12:57,000 Epoch 4878: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 20:12:57,001 EPOCH 4879
2024-02-10 20:13:13,210 Epoch 4879: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 20:13:13,210 EPOCH 4880
2024-02-10 20:13:29,219 Epoch 4880: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 20:13:29,220 EPOCH 4881
2024-02-10 20:13:45,513 Epoch 4881: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 20:13:45,514 EPOCH 4882
2024-02-10 20:14:01,655 Epoch 4882: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 20:14:01,655 EPOCH 4883
2024-02-10 20:14:17,602 Epoch 4883: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 20:14:17,602 EPOCH 4884
2024-02-10 20:14:33,642 Epoch 4884: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 20:14:33,643 EPOCH 4885
2024-02-10 20:14:49,710 Epoch 4885: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 20:14:49,711 EPOCH 4886
2024-02-10 20:15:05,744 Epoch 4886: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 20:15:05,745 EPOCH 4887
2024-02-10 20:15:21,845 Epoch 4887: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 20:15:21,845 EPOCH 4888
2024-02-10 20:15:37,684 Epoch 4888: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 20:15:37,684 EPOCH 4889
2024-02-10 20:15:47,824 [Epoch: 4889 Step: 00044000] Batch Recognition Loss:   0.000383 => Gls Tokens per Sec:      921 || Batch Translation Loss:   0.006563 => Txt Tokens per Sec:     2462 || Lr: 0.000100
2024-02-10 20:16:59,619 Validation result at epoch 4889, step    44000: duration: 71.7934s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.30501	Translation Loss: 101155.42969	PPL: 24426.36523
	Eval Metric: BLEU
	WER 2.33	(DEL: 0.00,	INS: 0.00,	SUB: 2.33)
	BLEU-4 0.00	(BLEU-1: 9.53,	BLEU-2: 2.63,	BLEU-3: 0.81,	BLEU-4: 0.00)
	CHRF 16.40	ROUGE 8.13
2024-02-10 20:16:59,621 Logging Recognition and Translation Outputs
2024-02-10 20:16:59,621 ========================================================================================================================
2024-02-10 20:16:59,621 Logging Sequence: 57_104.00
2024-02-10 20:16:59,621 	Gloss Reference :	A B+C+D+E
2024-02-10 20:16:59,622 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 20:16:59,622 	Gloss Alignment :	         
2024-02-10 20:16:59,622 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 20:16:59,623 	Text Reference  :	the next day kohli and kl rahul continued from    where    they had left and displayed amazing batting performance without losing their wickets
2024-02-10 20:16:59,623 	Text Hypothesis :	*** **** *** ***** *** ** ***** ********* against pakistan won  the toss and ********* ******* ******* *********** ******* chose  to    bowl   
2024-02-10 20:16:59,623 	Text Alignment  :	D   D    D   D     D   D  D     D         S       S        S    S   S        D         D       D       D           D       S      S     S      
2024-02-10 20:16:59,624 ========================================================================================================================
2024-02-10 20:16:59,624 Logging Sequence: 136_64.00
2024-02-10 20:16:59,624 	Gloss Reference :	A B+C+D+E
2024-02-10 20:16:59,624 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 20:16:59,624 	Gloss Alignment :	         
2024-02-10 20:16:59,624 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 20:16:59,625 	Text Reference  :	in all  she has       won 2    medals
2024-02-10 20:16:59,625 	Text Hypothesis :	** this is  currently i   have proud 
2024-02-10 20:16:59,625 	Text Alignment  :	D  S    S   S         S   S    S     
2024-02-10 20:16:59,625 ========================================================================================================================
2024-02-10 20:16:59,626 Logging Sequence: 54_123.00
2024-02-10 20:16:59,626 	Gloss Reference :	A B+C+D+E
2024-02-10 20:16:59,626 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 20:16:59,626 	Gloss Alignment :	         
2024-02-10 20:16:59,626 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 20:16:59,627 	Text Reference  :	vips sponsors international cricket groups have    already booked their hotel rooms     
2024-02-10 20:16:59,627 	Text Hypothesis :	**** ******** people        are     not    believe in      the    same  my    surprising
2024-02-10 20:16:59,627 	Text Alignment  :	D    D        S             S       S      S       S       S      S     S     S         
2024-02-10 20:16:59,627 ========================================================================================================================
2024-02-10 20:16:59,628 Logging Sequence: 168_115.00
2024-02-10 20:16:59,628 	Gloss Reference :	A B+C+D+E
2024-02-10 20:16:59,628 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 20:16:59,628 	Gloss Alignment :	         
2024-02-10 20:16:59,628 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 20:16:59,629 	Text Reference  :	this has  sparked a   major discussion on  social media
2024-02-10 20:16:59,629 	Text Hypothesis :	**** were shocked the world cup        for the    team 
2024-02-10 20:16:59,629 	Text Alignment  :	D    S    S       S   S     S          S   S      S    
2024-02-10 20:16:59,630 ========================================================================================================================
2024-02-10 20:16:59,630 Logging Sequence: 121_132.00
2024-02-10 20:16:59,630 	Gloss Reference :	A B+C+D+E
2024-02-10 20:16:59,630 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 20:16:59,630 	Gloss Alignment :	         
2024-02-10 20:16:59,630 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 20:16:59,632 	Text Reference  :	which is why they will be  retesting her to   check if she consumed any  stamina enhancing drugs
2024-02-10 20:16:59,632 	Text Hypothesis :	***** ** *** **** **** the team      was very proud by hou has      been found   sushil    kumar
2024-02-10 20:16:59,632 	Text Alignment  :	D     D  D   D    D    S   S         S   S    S     S  S   S        S    S       S         S    
2024-02-10 20:16:59,632 ========================================================================================================================
2024-02-10 20:17:05,582 Epoch 4889: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 20:17:05,583 EPOCH 4890
2024-02-10 20:17:21,794 Epoch 4890: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 20:17:21,794 EPOCH 4891
2024-02-10 20:17:38,079 Epoch 4891: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 20:17:38,080 EPOCH 4892
2024-02-10 20:17:54,165 Epoch 4892: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 20:17:54,166 EPOCH 4893
2024-02-10 20:18:09,983 Epoch 4893: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 20:18:09,984 EPOCH 4894
2024-02-10 20:18:25,941 Epoch 4894: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 20:18:25,941 EPOCH 4895
2024-02-10 20:18:41,609 Epoch 4895: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 20:18:41,609 EPOCH 4896
2024-02-10 20:18:57,525 Epoch 4896: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 20:18:57,526 EPOCH 4897
2024-02-10 20:19:13,934 Epoch 4897: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 20:19:13,934 EPOCH 4898
2024-02-10 20:19:30,049 Epoch 4898: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 20:19:30,049 EPOCH 4899
2024-02-10 20:19:46,361 Epoch 4899: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 20:19:46,362 EPOCH 4900
2024-02-10 20:20:02,495 [Epoch: 4900 Step: 00044100] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:      658 || Batch Translation Loss:   0.070526 => Txt Tokens per Sec:     1821 || Lr: 0.000100
2024-02-10 20:20:02,496 Epoch 4900: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 20:20:02,496 EPOCH 4901
2024-02-10 20:20:18,634 Epoch 4901: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 20:20:18,635 EPOCH 4902
2024-02-10 20:20:34,663 Epoch 4902: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 20:20:34,664 EPOCH 4903
2024-02-10 20:20:50,842 Epoch 4903: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 20:20:50,842 EPOCH 4904
2024-02-10 20:21:07,255 Epoch 4904: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 20:21:07,255 EPOCH 4905
2024-02-10 20:21:23,464 Epoch 4905: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-10 20:21:23,465 EPOCH 4906
2024-02-10 20:21:39,918 Epoch 4906: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 20:21:39,918 EPOCH 4907
2024-02-10 20:21:55,867 Epoch 4907: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-10 20:21:55,867 EPOCH 4908
2024-02-10 20:22:11,977 Epoch 4908: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-10 20:22:11,978 EPOCH 4909
2024-02-10 20:22:28,491 Epoch 4909: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.46 
2024-02-10 20:22:28,491 EPOCH 4910
2024-02-10 20:22:44,593 Epoch 4910: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.97 
2024-02-10 20:22:44,594 EPOCH 4911
2024-02-10 20:23:00,940 Epoch 4911: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.28 
2024-02-10 20:23:00,940 EPOCH 4912
2024-02-10 20:23:01,525 [Epoch: 4912 Step: 00044200] Batch Recognition Loss:   0.001770 => Gls Tokens per Sec:     2196 || Batch Translation Loss:   0.165894 => Txt Tokens per Sec:     6395 || Lr: 0.000100
2024-02-10 20:23:16,908 Epoch 4912: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.53 
2024-02-10 20:23:16,909 EPOCH 4913
2024-02-10 20:23:32,776 Epoch 4913: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.24 
2024-02-10 20:23:32,777 EPOCH 4914
2024-02-10 20:23:48,670 Epoch 4914: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-10 20:23:48,670 EPOCH 4915
2024-02-10 20:24:05,287 Epoch 4915: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.57 
2024-02-10 20:24:05,287 EPOCH 4916
2024-02-10 20:24:21,590 Epoch 4916: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-10 20:24:21,590 EPOCH 4917
2024-02-10 20:24:37,628 Epoch 4917: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 20:24:37,629 EPOCH 4918
2024-02-10 20:24:53,871 Epoch 4918: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 20:24:53,872 EPOCH 4919
2024-02-10 20:25:10,085 Epoch 4919: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 20:25:10,086 EPOCH 4920
2024-02-10 20:25:25,953 Epoch 4920: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 20:25:25,953 EPOCH 4921
2024-02-10 20:25:42,002 Epoch 4921: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 20:25:42,002 EPOCH 4922
2024-02-10 20:25:58,281 Epoch 4922: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 20:25:58,281 EPOCH 4923
2024-02-10 20:26:04,507 [Epoch: 4923 Step: 00044300] Batch Recognition Loss:   0.000817 => Gls Tokens per Sec:      411 || Batch Translation Loss:   0.024053 => Txt Tokens per Sec:     1308 || Lr: 0.000100
2024-02-10 20:26:14,029 Epoch 4923: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 20:26:14,030 EPOCH 4924
2024-02-10 20:26:29,770 Epoch 4924: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 20:26:29,771 EPOCH 4925
2024-02-10 20:26:46,082 Epoch 4925: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 20:26:46,083 EPOCH 4926
2024-02-10 20:27:01,880 Epoch 4926: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 20:27:01,880 EPOCH 4927
2024-02-10 20:27:17,920 Epoch 4927: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 20:27:17,920 EPOCH 4928
2024-02-10 20:27:34,103 Epoch 4928: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 20:27:34,103 EPOCH 4929
2024-02-10 20:27:50,319 Epoch 4929: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 20:27:50,320 EPOCH 4930
2024-02-10 20:28:06,199 Epoch 4930: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 20:28:06,200 EPOCH 4931
2024-02-10 20:28:22,736 Epoch 4931: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 20:28:22,737 EPOCH 4932
2024-02-10 20:28:38,802 Epoch 4932: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 20:28:38,803 EPOCH 4933
2024-02-10 20:28:54,936 Epoch 4933: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 20:28:54,937 EPOCH 4934
2024-02-10 20:28:59,152 [Epoch: 4934 Step: 00044400] Batch Recognition Loss:   0.000198 => Gls Tokens per Sec:      911 || Batch Translation Loss:   0.009932 => Txt Tokens per Sec:     2528 || Lr: 0.000100
2024-02-10 20:29:11,306 Epoch 4934: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 20:29:11,306 EPOCH 4935
2024-02-10 20:29:27,470 Epoch 4935: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 20:29:27,471 EPOCH 4936
2024-02-10 20:29:43,610 Epoch 4936: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 20:29:43,610 EPOCH 4937
2024-02-10 20:29:59,700 Epoch 4937: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 20:29:59,700 EPOCH 4938
2024-02-10 20:30:15,816 Epoch 4938: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 20:30:15,817 EPOCH 4939
2024-02-10 20:30:31,868 Epoch 4939: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 20:30:31,868 EPOCH 4940
2024-02-10 20:30:47,796 Epoch 4940: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 20:30:47,797 EPOCH 4941
2024-02-10 20:31:03,977 Epoch 4941: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 20:31:03,977 EPOCH 4942
2024-02-10 20:31:19,788 Epoch 4942: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 20:31:19,788 EPOCH 4943
2024-02-10 20:31:35,918 Epoch 4943: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 20:31:35,918 EPOCH 4944
2024-02-10 20:31:51,986 Epoch 4944: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 20:31:51,987 EPOCH 4945
2024-02-10 20:31:57,514 [Epoch: 4945 Step: 00044500] Batch Recognition Loss:   0.000465 => Gls Tokens per Sec:      764 || Batch Translation Loss:   0.012957 => Txt Tokens per Sec:     2072 || Lr: 0.000100
2024-02-10 20:32:07,927 Epoch 4945: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 20:32:07,928 EPOCH 4946
2024-02-10 20:32:23,948 Epoch 4946: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 20:32:23,949 EPOCH 4947
2024-02-10 20:32:40,070 Epoch 4947: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 20:32:40,070 EPOCH 4948
2024-02-10 20:32:56,443 Epoch 4948: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 20:32:56,443 EPOCH 4949
2024-02-10 20:33:13,254 Epoch 4949: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 20:33:13,254 EPOCH 4950
2024-02-10 20:33:29,277 Epoch 4950: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 20:33:29,277 EPOCH 4951
2024-02-10 20:33:45,561 Epoch 4951: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 20:33:45,562 EPOCH 4952
2024-02-10 20:34:01,562 Epoch 4952: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 20:34:01,563 EPOCH 4953
2024-02-10 20:34:17,654 Epoch 4953: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 20:34:17,655 EPOCH 4954
2024-02-10 20:34:33,650 Epoch 4954: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 20:34:33,651 EPOCH 4955
2024-02-10 20:34:49,681 Epoch 4955: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 20:34:49,681 EPOCH 4956
2024-02-10 20:34:59,744 [Epoch: 4956 Step: 00044600] Batch Recognition Loss:   0.000192 => Gls Tokens per Sec:      636 || Batch Translation Loss:   0.012469 => Txt Tokens per Sec:     1765 || Lr: 0.000100
2024-02-10 20:35:05,833 Epoch 4956: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 20:35:05,833 EPOCH 4957
2024-02-10 20:35:21,913 Epoch 4957: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 20:35:21,913 EPOCH 4958
2024-02-10 20:35:37,921 Epoch 4958: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 20:35:37,922 EPOCH 4959
2024-02-10 20:35:54,227 Epoch 4959: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 20:35:54,227 EPOCH 4960
2024-02-10 20:36:10,170 Epoch 4960: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 20:36:10,171 EPOCH 4961
2024-02-10 20:36:26,035 Epoch 4961: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 20:36:26,035 EPOCH 4962
2024-02-10 20:36:41,761 Epoch 4962: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 20:36:41,762 EPOCH 4963
2024-02-10 20:36:57,767 Epoch 4963: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-10 20:36:57,768 EPOCH 4964
2024-02-10 20:37:14,180 Epoch 4964: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-10 20:37:14,181 EPOCH 4965
2024-02-10 20:37:30,265 Epoch 4965: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 20:37:30,265 EPOCH 4966
2024-02-10 20:37:46,381 Epoch 4966: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 20:37:46,381 EPOCH 4967
2024-02-10 20:37:54,563 [Epoch: 4967 Step: 00044700] Batch Recognition Loss:   0.000351 => Gls Tokens per Sec:      939 || Batch Translation Loss:   0.011829 => Txt Tokens per Sec:     2576 || Lr: 0.000100
2024-02-10 20:38:02,643 Epoch 4967: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 20:38:02,643 EPOCH 4968
2024-02-10 20:38:18,739 Epoch 4968: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 20:38:18,740 EPOCH 4969
2024-02-10 20:38:34,875 Epoch 4969: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 20:38:34,876 EPOCH 4970
2024-02-10 20:38:50,631 Epoch 4970: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 20:38:50,632 EPOCH 4971
2024-02-10 20:39:06,562 Epoch 4971: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 20:39:06,562 EPOCH 4972
2024-02-10 20:39:22,908 Epoch 4972: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 20:39:22,908 EPOCH 4973
2024-02-10 20:39:38,857 Epoch 4973: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 20:39:38,857 EPOCH 4974
2024-02-10 20:39:54,963 Epoch 4974: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 20:39:54,964 EPOCH 4975
2024-02-10 20:40:10,845 Epoch 4975: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 20:40:10,845 EPOCH 4976
2024-02-10 20:40:27,035 Epoch 4976: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 20:40:27,036 EPOCH 4977
2024-02-10 20:40:42,919 Epoch 4977: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 20:40:42,920 EPOCH 4978
2024-02-10 20:40:53,920 [Epoch: 4978 Step: 00044800] Batch Recognition Loss:   0.000180 => Gls Tokens per Sec:      815 || Batch Translation Loss:   0.007790 => Txt Tokens per Sec:     2196 || Lr: 0.000100
2024-02-10 20:40:58,854 Epoch 4978: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 20:40:58,854 EPOCH 4979
2024-02-10 20:41:15,193 Epoch 4979: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 20:41:15,194 EPOCH 4980
2024-02-10 20:41:31,339 Epoch 4980: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 20:41:31,340 EPOCH 4981
2024-02-10 20:41:47,298 Epoch 4981: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 20:41:47,298 EPOCH 4982
2024-02-10 20:42:03,040 Epoch 4982: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 20:42:03,041 EPOCH 4983
2024-02-10 20:42:18,990 Epoch 4983: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 20:42:18,990 EPOCH 4984
2024-02-10 20:42:35,020 Epoch 4984: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 20:42:35,020 EPOCH 4985
2024-02-10 20:42:50,832 Epoch 4985: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 20:42:50,833 EPOCH 4986
2024-02-10 20:43:07,116 Epoch 4986: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 20:43:07,117 EPOCH 4987
2024-02-10 20:43:23,314 Epoch 4987: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 20:43:23,315 EPOCH 4988
2024-02-10 20:43:39,319 Epoch 4988: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 20:43:39,319 EPOCH 4989
2024-02-10 20:43:54,901 [Epoch: 4989 Step: 00044900] Batch Recognition Loss:   0.000320 => Gls Tokens per Sec:      599 || Batch Translation Loss:   0.013771 => Txt Tokens per Sec:     1637 || Lr: 0.000100
2024-02-10 20:43:55,710 Epoch 4989: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 20:43:55,711 EPOCH 4990
2024-02-10 20:44:12,159 Epoch 4990: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 20:44:12,159 EPOCH 4991
2024-02-10 20:44:28,309 Epoch 4991: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 20:44:28,310 EPOCH 4992
2024-02-10 20:44:44,629 Epoch 4992: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 20:44:44,629 EPOCH 4993
2024-02-10 20:45:00,893 Epoch 4993: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 20:45:00,894 EPOCH 4994
2024-02-10 20:45:16,889 Epoch 4994: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 20:45:16,890 EPOCH 4995
2024-02-10 20:45:33,305 Epoch 4995: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 20:45:33,305 EPOCH 4996
2024-02-10 20:45:49,526 Epoch 4996: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 20:45:49,527 EPOCH 4997
2024-02-10 20:46:05,581 Epoch 4997: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 20:46:05,582 EPOCH 4998
2024-02-10 20:46:21,463 Epoch 4998: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 20:46:21,464 EPOCH 4999
2024-02-10 20:46:38,084 Epoch 4999: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 20:46:38,084 EPOCH 5000
2024-02-10 20:46:54,269 [Epoch: 5000 Step: 00045000] Batch Recognition Loss:   0.000748 => Gls Tokens per Sec:      656 || Batch Translation Loss:   0.007712 => Txt Tokens per Sec:     1816 || Lr: 0.000100
2024-02-10 20:46:54,270 Epoch 5000: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 20:46:54,270 EPOCH 5001
2024-02-10 20:47:10,224 Epoch 5001: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 20:47:10,225 EPOCH 5002
2024-02-10 20:47:26,336 Epoch 5002: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 20:47:26,337 EPOCH 5003
2024-02-10 20:47:42,682 Epoch 5003: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 20:47:42,683 EPOCH 5004
2024-02-10 20:47:58,841 Epoch 5004: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 20:47:58,842 EPOCH 5005
2024-02-10 20:48:15,108 Epoch 5005: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 20:48:15,108 EPOCH 5006
2024-02-10 20:48:31,302 Epoch 5006: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 20:48:31,303 EPOCH 5007
2024-02-10 20:48:47,716 Epoch 5007: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 20:48:47,717 EPOCH 5008
2024-02-10 20:49:03,607 Epoch 5008: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-10 20:49:03,607 EPOCH 5009
2024-02-10 20:49:19,528 Epoch 5009: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.74 
2024-02-10 20:49:19,528 EPOCH 5010
2024-02-10 20:49:35,992 Epoch 5010: Total Training Recognition Loss 0.05  Total Training Translation Loss 4.91 
2024-02-10 20:49:35,993 EPOCH 5011
2024-02-10 20:49:52,014 Epoch 5011: Total Training Recognition Loss 0.07  Total Training Translation Loss 1.85 
2024-02-10 20:49:52,014 EPOCH 5012
2024-02-10 20:49:52,238 [Epoch: 5012 Step: 00045100] Batch Recognition Loss:   0.001052 => Gls Tokens per Sec:     5740 || Batch Translation Loss:   0.061502 => Txt Tokens per Sec:    10377 || Lr: 0.000100
2024-02-10 20:50:07,749 Epoch 5012: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.71 
2024-02-10 20:50:07,749 EPOCH 5013
2024-02-10 20:50:23,797 Epoch 5013: Total Training Recognition Loss 0.08  Total Training Translation Loss 1.38 
2024-02-10 20:50:23,798 EPOCH 5014
2024-02-10 20:50:40,161 Epoch 5014: Total Training Recognition Loss 0.06  Total Training Translation Loss 2.19 
2024-02-10 20:50:40,161 EPOCH 5015
2024-02-10 20:50:56,198 Epoch 5015: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.06 
2024-02-10 20:50:56,198 EPOCH 5016
2024-02-10 20:51:12,538 Epoch 5016: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.72 
2024-02-10 20:51:12,539 EPOCH 5017
2024-02-10 20:51:28,615 Epoch 5017: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.74 
2024-02-10 20:51:28,615 EPOCH 5018
2024-02-10 20:51:44,823 Epoch 5018: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.14 
2024-02-10 20:51:44,824 EPOCH 5019
2024-02-10 20:52:00,663 Epoch 5019: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.11 
2024-02-10 20:52:00,664 EPOCH 5020
2024-02-10 20:52:16,748 Epoch 5020: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.80 
2024-02-10 20:52:16,749 EPOCH 5021
2024-02-10 20:52:33,169 Epoch 5021: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.46 
2024-02-10 20:52:33,169 EPOCH 5022
2024-02-10 20:52:49,373 Epoch 5022: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 20:52:49,373 EPOCH 5023
2024-02-10 20:52:50,207 [Epoch: 5023 Step: 00045200] Batch Recognition Loss:   0.001075 => Gls Tokens per Sec:     3077 || Batch Translation Loss:   0.032560 => Txt Tokens per Sec:     7826 || Lr: 0.000100
2024-02-10 20:53:05,324 Epoch 5023: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 20:53:05,325 EPOCH 5024
2024-02-10 20:53:21,268 Epoch 5024: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 20:53:21,269 EPOCH 5025
2024-02-10 20:53:37,322 Epoch 5025: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 20:53:37,322 EPOCH 5026
2024-02-10 20:53:53,400 Epoch 5026: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 20:53:53,400 EPOCH 5027
2024-02-10 20:54:09,515 Epoch 5027: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-10 20:54:09,516 EPOCH 5028
2024-02-10 20:54:25,478 Epoch 5028: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 20:54:25,478 EPOCH 5029
2024-02-10 20:54:41,821 Epoch 5029: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 20:54:41,821 EPOCH 5030
2024-02-10 20:54:57,763 Epoch 5030: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 20:54:57,764 EPOCH 5031
2024-02-10 20:55:13,244 Epoch 5031: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 20:55:13,244 EPOCH 5032
2024-02-10 20:55:29,736 Epoch 5032: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 20:55:29,737 EPOCH 5033
2024-02-10 20:55:46,010 Epoch 5033: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 20:55:46,010 EPOCH 5034
2024-02-10 20:55:47,292 [Epoch: 5034 Step: 00045300] Batch Recognition Loss:   0.000238 => Gls Tokens per Sec:     3002 || Batch Translation Loss:   0.016425 => Txt Tokens per Sec:     7294 || Lr: 0.000100
2024-02-10 20:56:02,320 Epoch 5034: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 20:56:02,321 EPOCH 5035
2024-02-10 20:56:18,287 Epoch 5035: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 20:56:18,288 EPOCH 5036
2024-02-10 20:56:34,064 Epoch 5036: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 20:56:34,065 EPOCH 5037
2024-02-10 20:56:50,073 Epoch 5037: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 20:56:50,073 EPOCH 5038
2024-02-10 20:57:06,184 Epoch 5038: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 20:57:06,185 EPOCH 5039
2024-02-10 20:57:22,276 Epoch 5039: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 20:57:22,276 EPOCH 5040
2024-02-10 20:57:38,397 Epoch 5040: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 20:57:38,397 EPOCH 5041
2024-02-10 20:57:54,189 Epoch 5041: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 20:57:54,190 EPOCH 5042
2024-02-10 20:58:10,654 Epoch 5042: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 20:58:10,654 EPOCH 5043
2024-02-10 20:58:27,260 Epoch 5043: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 20:58:27,261 EPOCH 5044
2024-02-10 20:58:43,280 Epoch 5044: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 20:58:43,281 EPOCH 5045
2024-02-10 20:58:53,267 [Epoch: 5045 Step: 00045400] Batch Recognition Loss:   0.000230 => Gls Tokens per Sec:      513 || Batch Translation Loss:   0.009965 => Txt Tokens per Sec:     1508 || Lr: 0.000100
2024-02-10 20:58:59,484 Epoch 5045: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 20:58:59,484 EPOCH 5046
2024-02-10 20:59:15,309 Epoch 5046: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 20:59:15,310 EPOCH 5047
2024-02-10 20:59:31,231 Epoch 5047: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 20:59:31,232 EPOCH 5048
2024-02-10 20:59:47,381 Epoch 5048: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 20:59:47,382 EPOCH 5049
2024-02-10 21:00:03,573 Epoch 5049: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:00:03,574 EPOCH 5050
2024-02-10 21:00:19,714 Epoch 5050: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:00:19,714 EPOCH 5051
2024-02-10 21:00:35,944 Epoch 5051: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 21:00:35,945 EPOCH 5052
2024-02-10 21:00:51,814 Epoch 5052: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:00:51,815 EPOCH 5053
2024-02-10 21:01:08,087 Epoch 5053: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:01:08,088 EPOCH 5054
2024-02-10 21:01:24,438 Epoch 5054: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:01:24,439 EPOCH 5055
2024-02-10 21:01:40,659 Epoch 5055: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:01:40,660 EPOCH 5056
2024-02-10 21:01:49,898 [Epoch: 5056 Step: 00045500] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:      595 || Batch Translation Loss:   0.013280 => Txt Tokens per Sec:     1746 || Lr: 0.000100
2024-02-10 21:01:56,784 Epoch 5056: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:01:56,784 EPOCH 5057
2024-02-10 21:02:13,162 Epoch 5057: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:02:13,162 EPOCH 5058
2024-02-10 21:02:29,709 Epoch 5058: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:02:29,710 EPOCH 5059
2024-02-10 21:02:45,720 Epoch 5059: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:02:45,720 EPOCH 5060
2024-02-10 21:03:01,918 Epoch 5060: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:03:01,918 EPOCH 5061
2024-02-10 21:03:18,011 Epoch 5061: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:03:18,012 EPOCH 5062
2024-02-10 21:03:34,086 Epoch 5062: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:03:34,087 EPOCH 5063
2024-02-10 21:03:50,117 Epoch 5063: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:03:50,117 EPOCH 5064
2024-02-10 21:04:06,291 Epoch 5064: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:04:06,292 EPOCH 5065
2024-02-10 21:04:22,330 Epoch 5065: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 21:04:22,331 EPOCH 5066
2024-02-10 21:04:38,420 Epoch 5066: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 21:04:38,420 EPOCH 5067
2024-02-10 21:04:48,982 [Epoch: 5067 Step: 00045600] Batch Recognition Loss:   0.000149 => Gls Tokens per Sec:      727 || Batch Translation Loss:   0.012780 => Txt Tokens per Sec:     1989 || Lr: 0.000100
2024-02-10 21:04:54,261 Epoch 5067: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:04:54,261 EPOCH 5068
2024-02-10 21:05:10,517 Epoch 5068: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 21:05:10,518 EPOCH 5069
2024-02-10 21:05:26,512 Epoch 5069: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 21:05:26,512 EPOCH 5070
2024-02-10 21:05:42,258 Epoch 5070: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 21:05:42,258 EPOCH 5071
2024-02-10 21:05:58,466 Epoch 5071: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 21:05:58,467 EPOCH 5072
2024-02-10 21:06:14,976 Epoch 5072: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 21:06:14,976 EPOCH 5073
2024-02-10 21:06:31,045 Epoch 5073: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 21:06:31,045 EPOCH 5074
2024-02-10 21:06:47,113 Epoch 5074: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:06:47,114 EPOCH 5075
2024-02-10 21:07:03,022 Epoch 5075: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 21:07:03,022 EPOCH 5076
2024-02-10 21:07:19,077 Epoch 5076: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:07:19,077 EPOCH 5077
2024-02-10 21:07:34,698 Epoch 5077: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:07:34,699 EPOCH 5078
2024-02-10 21:07:45,979 [Epoch: 5078 Step: 00045700] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:      794 || Batch Translation Loss:   0.016796 => Txt Tokens per Sec:     2230 || Lr: 0.000100
2024-02-10 21:07:50,575 Epoch 5078: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:07:50,576 EPOCH 5079
2024-02-10 21:08:06,368 Epoch 5079: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:08:06,369 EPOCH 5080
2024-02-10 21:08:22,710 Epoch 5080: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:08:22,711 EPOCH 5081
2024-02-10 21:08:38,894 Epoch 5081: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:08:38,894 EPOCH 5082
2024-02-10 21:08:55,217 Epoch 5082: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:08:55,217 EPOCH 5083
2024-02-10 21:09:11,383 Epoch 5083: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:09:11,384 EPOCH 5084
2024-02-10 21:09:27,187 Epoch 5084: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:09:27,187 EPOCH 5085
2024-02-10 21:09:43,206 Epoch 5085: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 21:09:43,206 EPOCH 5086
2024-02-10 21:09:59,508 Epoch 5086: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:09:59,508 EPOCH 5087
2024-02-10 21:10:15,806 Epoch 5087: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 21:10:15,807 EPOCH 5088
2024-02-10 21:10:31,726 Epoch 5088: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:10:31,727 EPOCH 5089
2024-02-10 21:10:47,395 [Epoch: 5089 Step: 00045800] Batch Recognition Loss:   0.000187 => Gls Tokens per Sec:      596 || Batch Translation Loss:   0.007874 => Txt Tokens per Sec:     1649 || Lr: 0.000100
2024-02-10 21:10:47,851 Epoch 5089: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 21:10:47,851 EPOCH 5090
2024-02-10 21:11:03,963 Epoch 5090: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:11:03,964 EPOCH 5091
2024-02-10 21:11:20,060 Epoch 5091: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:11:20,061 EPOCH 5092
2024-02-10 21:11:36,081 Epoch 5092: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:11:36,081 EPOCH 5093
2024-02-10 21:11:52,115 Epoch 5093: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 21:11:52,115 EPOCH 5094
2024-02-10 21:12:08,199 Epoch 5094: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 21:12:08,200 EPOCH 5095
2024-02-10 21:12:24,095 Epoch 5095: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 21:12:24,096 EPOCH 5096
2024-02-10 21:12:40,386 Epoch 5096: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 21:12:40,387 EPOCH 5097
2024-02-10 21:12:56,076 Epoch 5097: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-10 21:12:56,076 EPOCH 5098
2024-02-10 21:13:12,176 Epoch 5098: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 21:13:12,177 EPOCH 5099
2024-02-10 21:13:28,394 Epoch 5099: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 21:13:28,395 EPOCH 5100
2024-02-10 21:13:44,174 [Epoch: 5100 Step: 00045900] Batch Recognition Loss:   0.000573 => Gls Tokens per Sec:      673 || Batch Translation Loss:   0.026981 => Txt Tokens per Sec:     1862 || Lr: 0.000100
2024-02-10 21:13:44,174 Epoch 5100: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 21:13:44,175 EPOCH 5101
2024-02-10 21:14:00,576 Epoch 5101: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 21:14:00,576 EPOCH 5102
2024-02-10 21:14:16,468 Epoch 5102: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 21:14:16,468 EPOCH 5103
2024-02-10 21:14:32,659 Epoch 5103: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-10 21:14:32,659 EPOCH 5104
2024-02-10 21:14:48,654 Epoch 5104: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-10 21:14:48,655 EPOCH 5105
2024-02-10 21:15:04,684 Epoch 5105: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.07 
2024-02-10 21:15:04,685 EPOCH 5106
2024-02-10 21:15:20,582 Epoch 5106: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-10 21:15:20,583 EPOCH 5107
2024-02-10 21:15:36,781 Epoch 5107: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.87 
2024-02-10 21:15:36,782 EPOCH 5108
2024-02-10 21:15:53,132 Epoch 5108: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-10 21:15:53,132 EPOCH 5109
2024-02-10 21:16:09,304 Epoch 5109: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-10 21:16:09,305 EPOCH 5110
2024-02-10 21:16:25,569 Epoch 5110: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-10 21:16:25,570 EPOCH 5111
2024-02-10 21:16:41,976 Epoch 5111: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-10 21:16:41,977 EPOCH 5112
2024-02-10 21:16:42,645 [Epoch: 5112 Step: 00046000] Batch Recognition Loss:   0.000793 => Gls Tokens per Sec:     1919 || Batch Translation Loss:   0.025205 => Txt Tokens per Sec:     5579 || Lr: 0.000100
2024-02-10 21:17:54,953 Validation result at epoch 5112, step    46000: duration: 72.3069s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.25114	Translation Loss: 101445.79688	PPL: 25145.15625
	Eval Metric: BLEU
	WER 2.47	(DEL: 0.00,	INS: 0.00,	SUB: 2.47)
	BLEU-4 0.28	(BLEU-1: 10.20,	BLEU-2: 2.58,	BLEU-3: 0.79,	BLEU-4: 0.28)
	CHRF 17.20	ROUGE 8.61
2024-02-10 21:17:54,955 Logging Recognition and Translation Outputs
2024-02-10 21:17:54,955 ========================================================================================================================
2024-02-10 21:17:54,955 Logging Sequence: 87_207.00
2024-02-10 21:17:54,955 	Gloss Reference :	A B+C+D+E
2024-02-10 21:17:54,955 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 21:17:54,956 	Gloss Alignment :	         
2024-02-10 21:17:54,956 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 21:17:54,957 	Text Reference  :	**** ******** ***** **** ** **** ***** *** there     were 2-3 pakistanis who were speaking anti-india things and things on  kashmir
2024-02-10 21:17:54,958 	Text Hypothesis :	this amrapali group paid rs 3570 crore the remaining rs   652 crore      was paid a        close      to     do  with   the video  
2024-02-10 21:17:54,958 	Text Alignment  :	I    I        I     I    I  I    I     I   S         S    S   S          S   S    S        S          S      S   S      S   S      
2024-02-10 21:17:54,958 ========================================================================================================================
2024-02-10 21:17:54,958 Logging Sequence: 67_73.00
2024-02-10 21:17:54,958 	Gloss Reference :	A B+C+D+E
2024-02-10 21:17:54,958 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 21:17:54,958 	Gloss Alignment :	         
2024-02-10 21:17:54,959 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 21:17:54,959 	Text Reference  :	**** ******* ******* in    his     tweet he  also  said       
2024-02-10 21:17:54,959 	Text Hypothesis :	with india's amazing moves carlsen won   the first tie-breaker
2024-02-10 21:17:54,959 	Text Alignment  :	I    I       I       S     S       S     S   S     S          
2024-02-10 21:17:54,960 ========================================================================================================================
2024-02-10 21:17:54,960 Logging Sequence: 172_267.00
2024-02-10 21:17:54,960 	Gloss Reference :	A B+C+D+E
2024-02-10 21:17:54,960 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 21:17:54,960 	Gloss Alignment :	         
2024-02-10 21:17:54,960 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 21:17:54,961 	Text Reference  :	*** such provisions have been made
2024-02-10 21:17:54,961 	Text Hypothesis :	let me   tell       you  all  it  
2024-02-10 21:17:54,961 	Text Alignment  :	I   S    S          S    S    S   
2024-02-10 21:17:54,961 ========================================================================================================================
2024-02-10 21:17:54,961 Logging Sequence: 144_23.00
2024-02-10 21:17:54,962 	Gloss Reference :	A B+C+D+E
2024-02-10 21:17:54,962 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 21:17:54,962 	Gloss Alignment :	         
2024-02-10 21:17:54,962 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 21:17:54,963 	Text Reference  :	the   girl  is   14-year-old mumal mehar and    she   is      from    kanasar village of barmer in rajasthan  
2024-02-10 21:17:54,963 	Text Hypothesis :	since women this was         known as    sushil kumar urvashi rautela the     target  of ****** ** recognition
2024-02-10 21:17:54,964 	Text Alignment  :	S     S     S    S           S     S     S      S     S       S       S       S          D      D  S          
2024-02-10 21:17:54,964 ========================================================================================================================
2024-02-10 21:17:54,964 Logging Sequence: 133_202.00
2024-02-10 21:17:54,964 	Gloss Reference :	A B+C+D+E
2024-02-10 21:17:54,964 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 21:17:54,964 	Gloss Alignment :	         
2024-02-10 21:17:54,964 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 21:17:54,965 	Text Reference  :	********* australia has  already qualified for the final if india wins  it  will face australia
2024-02-10 21:17:54,966 	Text Hypothesis :	pakistani messi     kept scoring run       for *** ***** ** ***** angry out they felt oppressed
2024-02-10 21:17:54,966 	Text Alignment  :	I         S         S    S       S             D   D     D  D     S     S   S    S    S        
2024-02-10 21:17:54,966 ========================================================================================================================
2024-02-10 21:18:11,197 Epoch 5112: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 21:18:11,198 EPOCH 5113
2024-02-10 21:18:27,525 Epoch 5113: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 21:18:27,526 EPOCH 5114
2024-02-10 21:18:43,458 Epoch 5114: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 21:18:43,459 EPOCH 5115
2024-02-10 21:18:59,146 Epoch 5115: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 21:18:59,147 EPOCH 5116
2024-02-10 21:19:15,491 Epoch 5116: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 21:19:15,491 EPOCH 5117
2024-02-10 21:19:31,617 Epoch 5117: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 21:19:31,617 EPOCH 5118
2024-02-10 21:19:48,084 Epoch 5118: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 21:19:48,084 EPOCH 5119
2024-02-10 21:20:04,123 Epoch 5119: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 21:20:04,123 EPOCH 5120
2024-02-10 21:20:20,185 Epoch 5120: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 21:20:20,185 EPOCH 5121
2024-02-10 21:20:36,474 Epoch 5121: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 21:20:36,475 EPOCH 5122
2024-02-10 21:20:52,904 Epoch 5122: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 21:20:52,904 EPOCH 5123
2024-02-10 21:20:56,404 [Epoch: 5123 Step: 00046100] Batch Recognition Loss:   0.000312 => Gls Tokens per Sec:      732 || Batch Translation Loss:   0.016822 => Txt Tokens per Sec:     2181 || Lr: 0.000100
2024-02-10 21:21:08,834 Epoch 5123: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 21:21:08,835 EPOCH 5124
2024-02-10 21:21:24,899 Epoch 5124: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 21:21:24,900 EPOCH 5125
2024-02-10 21:21:41,243 Epoch 5125: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 21:21:41,244 EPOCH 5126
2024-02-10 21:21:57,461 Epoch 5126: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 21:21:57,461 EPOCH 5127
2024-02-10 21:22:13,646 Epoch 5127: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 21:22:13,646 EPOCH 5128
2024-02-10 21:22:30,542 Epoch 5128: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 21:22:30,542 EPOCH 5129
2024-02-10 21:22:46,681 Epoch 5129: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 21:22:46,682 EPOCH 5130
2024-02-10 21:23:02,763 Epoch 5130: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 21:23:02,763 EPOCH 5131
2024-02-10 21:23:18,837 Epoch 5131: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 21:23:18,838 EPOCH 5132
2024-02-10 21:23:34,924 Epoch 5132: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 21:23:34,925 EPOCH 5133
2024-02-10 21:23:51,152 Epoch 5133: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 21:23:51,152 EPOCH 5134
2024-02-10 21:23:56,340 [Epoch: 5134 Step: 00046200] Batch Recognition Loss:   0.000214 => Gls Tokens per Sec:      567 || Batch Translation Loss:   0.017224 => Txt Tokens per Sec:     1596 || Lr: 0.000100
2024-02-10 21:24:07,080 Epoch 5134: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 21:24:07,081 EPOCH 5135
2024-02-10 21:24:23,710 Epoch 5135: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 21:24:23,710 EPOCH 5136
2024-02-10 21:24:40,033 Epoch 5136: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 21:24:40,034 EPOCH 5137
2024-02-10 21:24:56,192 Epoch 5137: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 21:24:56,192 EPOCH 5138
2024-02-10 21:25:12,295 Epoch 5138: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:25:12,296 EPOCH 5139
2024-02-10 21:25:28,508 Epoch 5139: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 21:25:28,508 EPOCH 5140
2024-02-10 21:25:44,589 Epoch 5140: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 21:25:44,590 EPOCH 5141
2024-02-10 21:26:00,648 Epoch 5141: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 21:26:00,648 EPOCH 5142
2024-02-10 21:26:16,730 Epoch 5142: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 21:26:16,730 EPOCH 5143
2024-02-10 21:26:32,968 Epoch 5143: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 21:26:32,969 EPOCH 5144
2024-02-10 21:26:48,900 Epoch 5144: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:26:48,901 EPOCH 5145
2024-02-10 21:26:54,110 [Epoch: 5145 Step: 00046300] Batch Recognition Loss:   0.000125 => Gls Tokens per Sec:      810 || Batch Translation Loss:   0.007682 => Txt Tokens per Sec:     1878 || Lr: 0.000100
2024-02-10 21:27:05,167 Epoch 5145: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:27:05,167 EPOCH 5146
2024-02-10 21:27:21,275 Epoch 5146: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 21:27:21,276 EPOCH 5147
2024-02-10 21:27:37,318 Epoch 5147: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 21:27:37,318 EPOCH 5148
2024-02-10 21:27:53,628 Epoch 5148: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 21:27:53,629 EPOCH 5149
2024-02-10 21:28:09,871 Epoch 5149: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 21:28:09,871 EPOCH 5150
2024-02-10 21:28:25,986 Epoch 5150: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 21:28:25,987 EPOCH 5151
2024-02-10 21:28:41,990 Epoch 5151: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 21:28:41,991 EPOCH 5152
2024-02-10 21:28:58,175 Epoch 5152: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.50 
2024-02-10 21:28:58,176 EPOCH 5153
2024-02-10 21:29:13,941 Epoch 5153: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 21:29:13,942 EPOCH 5154
2024-02-10 21:29:30,032 Epoch 5154: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 21:29:30,032 EPOCH 5155
2024-02-10 21:29:46,281 Epoch 5155: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 21:29:46,282 EPOCH 5156
2024-02-10 21:29:57,008 [Epoch: 5156 Step: 00046400] Batch Recognition Loss:   0.000167 => Gls Tokens per Sec:      597 || Batch Translation Loss:   0.015633 => Txt Tokens per Sec:     1767 || Lr: 0.000100
2024-02-10 21:30:02,615 Epoch 5156: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 21:30:02,615 EPOCH 5157
2024-02-10 21:30:18,372 Epoch 5157: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 21:30:18,372 EPOCH 5158
2024-02-10 21:30:34,670 Epoch 5158: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 21:30:34,671 EPOCH 5159
2024-02-10 21:30:50,909 Epoch 5159: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 21:30:50,909 EPOCH 5160
2024-02-10 21:31:06,929 Epoch 5160: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 21:31:06,930 EPOCH 5161
2024-02-10 21:31:22,738 Epoch 5161: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 21:31:22,739 EPOCH 5162
2024-02-10 21:31:38,763 Epoch 5162: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 21:31:38,764 EPOCH 5163
2024-02-10 21:31:54,907 Epoch 5163: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 21:31:54,908 EPOCH 5164
2024-02-10 21:32:10,985 Epoch 5164: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 21:32:10,985 EPOCH 5165
2024-02-10 21:32:27,042 Epoch 5165: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-10 21:32:27,043 EPOCH 5166
2024-02-10 21:32:43,177 Epoch 5166: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-10 21:32:43,177 EPOCH 5167
2024-02-10 21:32:58,053 [Epoch: 5167 Step: 00046500] Batch Recognition Loss:   0.000514 => Gls Tokens per Sec:      456 || Batch Translation Loss:   0.054580 => Txt Tokens per Sec:     1394 || Lr: 0.000100
2024-02-10 21:32:59,125 Epoch 5167: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.61 
2024-02-10 21:32:59,126 EPOCH 5168
2024-02-10 21:33:16,356 Epoch 5168: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.03 
2024-02-10 21:33:16,356 EPOCH 5169
2024-02-10 21:33:32,431 Epoch 5169: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-10 21:33:32,432 EPOCH 5170
2024-02-10 21:33:48,149 Epoch 5170: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.65 
2024-02-10 21:33:48,150 EPOCH 5171
2024-02-10 21:34:04,358 Epoch 5171: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.58 
2024-02-10 21:34:04,359 EPOCH 5172
2024-02-10 21:34:20,521 Epoch 5172: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.47 
2024-02-10 21:34:20,521 EPOCH 5173
2024-02-10 21:34:36,806 Epoch 5173: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-10 21:34:36,807 EPOCH 5174
2024-02-10 21:34:53,026 Epoch 5174: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-10 21:34:53,027 EPOCH 5175
2024-02-10 21:35:09,057 Epoch 5175: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.90 
2024-02-10 21:35:09,058 EPOCH 5176
2024-02-10 21:35:24,805 Epoch 5176: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.86 
2024-02-10 21:35:24,806 EPOCH 5177
2024-02-10 21:35:40,705 Epoch 5177: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.63 
2024-02-10 21:35:40,706 EPOCH 5178
2024-02-10 21:35:52,240 [Epoch: 5178 Step: 00046600] Batch Recognition Loss:   0.000486 => Gls Tokens per Sec:      777 || Batch Translation Loss:   0.073188 => Txt Tokens per Sec:     2231 || Lr: 0.000100
2024-02-10 21:35:56,805 Epoch 5178: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.70 
2024-02-10 21:35:56,805 EPOCH 5179
2024-02-10 21:36:13,084 Epoch 5179: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.68 
2024-02-10 21:36:13,084 EPOCH 5180
2024-02-10 21:36:29,032 Epoch 5180: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-10 21:36:29,032 EPOCH 5181
2024-02-10 21:36:44,816 Epoch 5181: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-10 21:36:44,816 EPOCH 5182
2024-02-10 21:37:00,503 Epoch 5182: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-10 21:37:00,504 EPOCH 5183
2024-02-10 21:37:16,636 Epoch 5183: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 21:37:16,637 EPOCH 5184
2024-02-10 21:37:32,598 Epoch 5184: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 21:37:32,599 EPOCH 5185
2024-02-10 21:37:48,581 Epoch 5185: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 21:37:48,582 EPOCH 5186
2024-02-10 21:38:04,579 Epoch 5186: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 21:38:04,580 EPOCH 5187
2024-02-10 21:38:20,590 Epoch 5187: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 21:38:20,590 EPOCH 5188
2024-02-10 21:38:37,017 Epoch 5188: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 21:38:37,017 EPOCH 5189
2024-02-10 21:38:49,597 [Epoch: 5189 Step: 00046700] Batch Recognition Loss:   0.000578 => Gls Tokens per Sec:      743 || Batch Translation Loss:   0.006829 => Txt Tokens per Sec:     2010 || Lr: 0.000100
2024-02-10 21:38:52,881 Epoch 5189: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 21:38:52,882 EPOCH 5190
2024-02-10 21:39:08,660 Epoch 5190: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 21:39:08,661 EPOCH 5191
2024-02-10 21:39:24,887 Epoch 5191: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 21:39:24,888 EPOCH 5192
2024-02-10 21:39:40,836 Epoch 5192: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 21:39:40,837 EPOCH 5193
2024-02-10 21:39:56,803 Epoch 5193: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 21:39:56,804 EPOCH 5194
2024-02-10 21:40:12,783 Epoch 5194: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 21:40:12,783 EPOCH 5195
2024-02-10 21:40:28,991 Epoch 5195: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 21:40:28,992 EPOCH 5196
2024-02-10 21:40:45,081 Epoch 5196: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 21:40:45,082 EPOCH 5197
2024-02-10 21:41:01,359 Epoch 5197: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 21:41:01,360 EPOCH 5198
2024-02-10 21:41:17,397 Epoch 5198: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:41:17,398 EPOCH 5199
2024-02-10 21:41:33,309 Epoch 5199: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 21:41:33,309 EPOCH 5200
2024-02-10 21:41:49,170 [Epoch: 5200 Step: 00046800] Batch Recognition Loss:   0.000437 => Gls Tokens per Sec:      670 || Batch Translation Loss:   0.010468 => Txt Tokens per Sec:     1853 || Lr: 0.000100
2024-02-10 21:41:49,170 Epoch 5200: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 21:41:49,170 EPOCH 5201
2024-02-10 21:42:05,286 Epoch 5201: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 21:42:05,286 EPOCH 5202
2024-02-10 21:42:21,428 Epoch 5202: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:42:21,429 EPOCH 5203
2024-02-10 21:42:37,669 Epoch 5203: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:42:37,670 EPOCH 5204
2024-02-10 21:42:53,953 Epoch 5204: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:42:53,954 EPOCH 5205
2024-02-10 21:43:09,974 Epoch 5205: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 21:43:09,975 EPOCH 5206
2024-02-10 21:43:26,234 Epoch 5206: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:43:26,235 EPOCH 5207
2024-02-10 21:43:42,099 Epoch 5207: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-10 21:43:42,100 EPOCH 5208
2024-02-10 21:43:58,214 Epoch 5208: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:43:58,214 EPOCH 5209
2024-02-10 21:44:14,482 Epoch 5209: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:44:14,483 EPOCH 5210
2024-02-10 21:44:30,711 Epoch 5210: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:44:30,711 EPOCH 5211
2024-02-10 21:44:46,856 Epoch 5211: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:44:46,856 EPOCH 5212
2024-02-10 21:44:47,101 [Epoch: 5212 Step: 00046900] Batch Recognition Loss:   0.000176 => Gls Tokens per Sec:     5246 || Batch Translation Loss:   0.006124 => Txt Tokens per Sec:     9356 || Lr: 0.000100
2024-02-10 21:45:02,929 Epoch 5212: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 21:45:02,930 EPOCH 5213
2024-02-10 21:45:19,033 Epoch 5213: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.09 
2024-02-10 21:45:19,034 EPOCH 5214
2024-02-10 21:45:35,081 Epoch 5214: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 21:45:35,081 EPOCH 5215
2024-02-10 21:45:51,016 Epoch 5215: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:45:51,017 EPOCH 5216
2024-02-10 21:46:07,525 Epoch 5216: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 21:46:07,525 EPOCH 5217
2024-02-10 21:46:23,831 Epoch 5217: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:46:23,831 EPOCH 5218
2024-02-10 21:46:39,861 Epoch 5218: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:46:39,862 EPOCH 5219
2024-02-10 21:46:55,742 Epoch 5219: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:46:55,742 EPOCH 5220
2024-02-10 21:47:11,952 Epoch 5220: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:47:11,953 EPOCH 5221
2024-02-10 21:47:27,920 Epoch 5221: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:47:27,921 EPOCH 5222
2024-02-10 21:47:44,020 Epoch 5222: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:47:44,020 EPOCH 5223
2024-02-10 21:47:47,686 [Epoch: 5223 Step: 00047000] Batch Recognition Loss:   0.000262 => Gls Tokens per Sec:      698 || Batch Translation Loss:   0.006042 => Txt Tokens per Sec:     1747 || Lr: 0.000100
2024-02-10 21:48:00,505 Epoch 5223: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 21:48:00,505 EPOCH 5224
2024-02-10 21:48:16,820 Epoch 5224: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:48:16,821 EPOCH 5225
2024-02-10 21:48:33,246 Epoch 5225: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:48:33,247 EPOCH 5226
2024-02-10 21:48:49,546 Epoch 5226: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 21:48:49,547 EPOCH 5227
2024-02-10 21:49:05,814 Epoch 5227: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:49:05,815 EPOCH 5228
2024-02-10 21:49:21,924 Epoch 5228: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 21:49:21,925 EPOCH 5229
2024-02-10 21:49:38,257 Epoch 5229: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:49:38,257 EPOCH 5230
2024-02-10 21:49:54,579 Epoch 5230: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:49:54,580 EPOCH 5231
2024-02-10 21:50:10,664 Epoch 5231: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:50:10,665 EPOCH 5232
2024-02-10 21:50:26,742 Epoch 5232: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:50:26,742 EPOCH 5233
2024-02-10 21:50:42,772 Epoch 5233: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 21:50:42,772 EPOCH 5234
2024-02-10 21:50:43,900 [Epoch: 5234 Step: 00047100] Batch Recognition Loss:   0.000367 => Gls Tokens per Sec:     3410 || Batch Translation Loss:   0.009457 => Txt Tokens per Sec:     7452 || Lr: 0.000100
2024-02-10 21:50:59,019 Epoch 5234: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:50:59,019 EPOCH 5235
2024-02-10 21:51:14,990 Epoch 5235: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:51:14,990 EPOCH 5236
2024-02-10 21:51:30,925 Epoch 5236: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:51:30,926 EPOCH 5237
2024-02-10 21:51:47,214 Epoch 5237: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:51:47,214 EPOCH 5238
2024-02-10 21:52:03,458 Epoch 5238: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 21:52:03,459 EPOCH 5239
2024-02-10 21:52:19,842 Epoch 5239: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 21:52:19,842 EPOCH 5240
2024-02-10 21:52:35,922 Epoch 5240: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 21:52:35,922 EPOCH 5241
2024-02-10 21:52:51,910 Epoch 5241: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-10 21:52:51,910 EPOCH 5242
2024-02-10 21:53:08,323 Epoch 5242: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 21:53:08,324 EPOCH 5243
2024-02-10 21:53:24,177 Epoch 5243: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.10 
2024-02-10 21:53:24,177 EPOCH 5244
2024-02-10 21:53:40,052 Epoch 5244: Total Training Recognition Loss 0.22  Total Training Translation Loss 0.10 
2024-02-10 21:53:40,053 EPOCH 5245
2024-02-10 21:53:49,604 [Epoch: 5245 Step: 00047200] Batch Recognition Loss:   0.020506 => Gls Tokens per Sec:      536 || Batch Translation Loss:   0.018971 => Txt Tokens per Sec:     1534 || Lr: 0.000100
2024-02-10 21:53:55,814 Epoch 5245: Total Training Recognition Loss 0.60  Total Training Translation Loss 0.11 
2024-02-10 21:53:55,814 EPOCH 5246
2024-02-10 21:54:11,923 Epoch 5246: Total Training Recognition Loss 0.39  Total Training Translation Loss 0.23 
2024-02-10 21:54:11,924 EPOCH 5247
2024-02-10 21:54:28,185 Epoch 5247: Total Training Recognition Loss 0.17  Total Training Translation Loss 0.22 
2024-02-10 21:54:28,185 EPOCH 5248
2024-02-10 21:54:44,228 Epoch 5248: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.22 
2024-02-10 21:54:44,229 EPOCH 5249
2024-02-10 21:54:59,894 Epoch 5249: Total Training Recognition Loss 0.12  Total Training Translation Loss 0.24 
2024-02-10 21:54:59,895 EPOCH 5250
2024-02-10 21:55:16,128 Epoch 5250: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.55 
2024-02-10 21:55:16,129 EPOCH 5251
2024-02-10 21:55:32,139 Epoch 5251: Total Training Recognition Loss 0.10  Total Training Translation Loss 0.58 
2024-02-10 21:55:32,140 EPOCH 5252
2024-02-10 21:55:48,403 Epoch 5252: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.99 
2024-02-10 21:55:48,404 EPOCH 5253
2024-02-10 21:56:04,486 Epoch 5253: Total Training Recognition Loss 0.03  Total Training Translation Loss 2.58 
2024-02-10 21:56:04,486 EPOCH 5254
2024-02-10 21:56:20,638 Epoch 5254: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.52 
2024-02-10 21:56:20,639 EPOCH 5255
2024-02-10 21:56:36,748 Epoch 5255: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.04 
2024-02-10 21:56:36,748 EPOCH 5256
2024-02-10 21:56:43,039 [Epoch: 5256 Step: 00047300] Batch Recognition Loss:   0.000948 => Gls Tokens per Sec:      874 || Batch Translation Loss:   0.068701 => Txt Tokens per Sec:     2397 || Lr: 0.000100
2024-02-10 21:56:52,650 Epoch 5256: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.62 
2024-02-10 21:56:52,650 EPOCH 5257
2024-02-10 21:57:08,762 Epoch 5257: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-10 21:57:08,763 EPOCH 5258
2024-02-10 21:57:24,849 Epoch 5258: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-10 21:57:24,850 EPOCH 5259
2024-02-10 21:57:40,882 Epoch 5259: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-10 21:57:40,883 EPOCH 5260
2024-02-10 21:57:56,909 Epoch 5260: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-10 21:57:56,910 EPOCH 5261
2024-02-10 21:58:12,930 Epoch 5261: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.34 
2024-02-10 21:58:12,931 EPOCH 5262
2024-02-10 21:58:29,198 Epoch 5262: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-10 21:58:29,199 EPOCH 5263
2024-02-10 21:58:45,168 Epoch 5263: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 21:58:45,168 EPOCH 5264
2024-02-10 21:59:01,071 Epoch 5264: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-10 21:59:01,071 EPOCH 5265
2024-02-10 21:59:17,362 Epoch 5265: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-10 21:59:17,363 EPOCH 5266
2024-02-10 21:59:33,491 Epoch 5266: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 21:59:33,491 EPOCH 5267
2024-02-10 21:59:48,126 [Epoch: 5267 Step: 00047400] Batch Recognition Loss:   0.000667 => Gls Tokens per Sec:      463 || Batch Translation Loss:   0.028489 => Txt Tokens per Sec:     1290 || Lr: 0.000100
2024-02-10 21:59:49,626 Epoch 5267: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 21:59:49,626 EPOCH 5268
2024-02-10 22:00:05,465 Epoch 5268: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 22:00:05,466 EPOCH 5269
2024-02-10 22:00:21,278 Epoch 5269: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 22:00:21,278 EPOCH 5270
2024-02-10 22:00:37,281 Epoch 5270: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 22:00:37,281 EPOCH 5271
2024-02-10 22:00:53,261 Epoch 5271: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 22:00:53,262 EPOCH 5272
2024-02-10 22:01:09,072 Epoch 5272: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 22:01:09,073 EPOCH 5273
2024-02-10 22:01:25,384 Epoch 5273: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 22:01:25,384 EPOCH 5274
2024-02-10 22:01:41,389 Epoch 5274: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 22:01:41,390 EPOCH 5275
2024-02-10 22:01:57,293 Epoch 5275: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 22:01:57,294 EPOCH 5276
2024-02-10 22:02:13,743 Epoch 5276: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:02:13,743 EPOCH 5277
2024-02-10 22:02:30,079 Epoch 5277: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:02:30,080 EPOCH 5278
2024-02-10 22:02:39,715 [Epoch: 5278 Step: 00047500] Batch Recognition Loss:   0.000737 => Gls Tokens per Sec:      836 || Batch Translation Loss:   0.008908 => Txt Tokens per Sec:     2206 || Lr: 0.000100
2024-02-10 22:02:45,928 Epoch 5278: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 22:02:45,928 EPOCH 5279
2024-02-10 22:03:02,355 Epoch 5279: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:03:02,356 EPOCH 5280
2024-02-10 22:03:18,348 Epoch 5280: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:03:18,349 EPOCH 5281
2024-02-10 22:03:34,555 Epoch 5281: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:03:34,556 EPOCH 5282
2024-02-10 22:03:50,733 Epoch 5282: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:03:50,734 EPOCH 5283
2024-02-10 22:04:06,908 Epoch 5283: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 22:04:06,908 EPOCH 5284
2024-02-10 22:04:23,117 Epoch 5284: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 22:04:23,117 EPOCH 5285
2024-02-10 22:04:39,136 Epoch 5285: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 22:04:39,137 EPOCH 5286
2024-02-10 22:04:54,650 Epoch 5286: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 22:04:54,651 EPOCH 5287
2024-02-10 22:05:10,828 Epoch 5287: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 22:05:10,829 EPOCH 5288
2024-02-10 22:05:27,143 Epoch 5288: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 22:05:27,144 EPOCH 5289
2024-02-10 22:05:42,317 [Epoch: 5289 Step: 00047600] Batch Recognition Loss:   0.000342 => Gls Tokens per Sec:      616 || Batch Translation Loss:   0.017120 => Txt Tokens per Sec:     1681 || Lr: 0.000100
2024-02-10 22:05:43,054 Epoch 5289: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:05:43,054 EPOCH 5290
2024-02-10 22:05:59,065 Epoch 5290: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:05:59,065 EPOCH 5291
2024-02-10 22:06:15,193 Epoch 5291: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:06:15,193 EPOCH 5292
2024-02-10 22:06:31,205 Epoch 5292: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:06:31,206 EPOCH 5293
2024-02-10 22:06:47,426 Epoch 5293: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-10 22:06:47,427 EPOCH 5294
2024-02-10 22:07:03,404 Epoch 5294: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:07:03,405 EPOCH 5295
2024-02-10 22:07:19,432 Epoch 5295: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:07:19,432 EPOCH 5296
2024-02-10 22:07:35,821 Epoch 5296: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 22:07:35,822 EPOCH 5297
2024-02-10 22:07:52,148 Epoch 5297: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:07:52,148 EPOCH 5298
2024-02-10 22:08:08,159 Epoch 5298: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:08:08,159 EPOCH 5299
2024-02-10 22:08:24,151 Epoch 5299: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:08:24,152 EPOCH 5300
2024-02-10 22:08:40,038 [Epoch: 5300 Step: 00047700] Batch Recognition Loss:   0.000213 => Gls Tokens per Sec:      669 || Batch Translation Loss:   0.013817 => Txt Tokens per Sec:     1850 || Lr: 0.000100
2024-02-10 22:08:40,038 Epoch 5300: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:08:40,038 EPOCH 5301
2024-02-10 22:08:56,284 Epoch 5301: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:08:56,285 EPOCH 5302
2024-02-10 22:09:12,633 Epoch 5302: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:09:12,634 EPOCH 5303
2024-02-10 22:09:28,724 Epoch 5303: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-10 22:09:28,725 EPOCH 5304
2024-02-10 22:09:44,733 Epoch 5304: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 22:09:44,734 EPOCH 5305
2024-02-10 22:10:00,685 Epoch 5305: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 22:10:00,685 EPOCH 5306
2024-02-10 22:10:16,947 Epoch 5306: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 22:10:16,948 EPOCH 5307
2024-02-10 22:10:33,066 Epoch 5307: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 22:10:33,066 EPOCH 5308
2024-02-10 22:10:49,013 Epoch 5308: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:10:49,014 EPOCH 5309
2024-02-10 22:11:04,820 Epoch 5309: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:11:04,820 EPOCH 5310
2024-02-10 22:11:20,809 Epoch 5310: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:11:20,810 EPOCH 5311
2024-02-10 22:11:37,056 Epoch 5311: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:11:37,056 EPOCH 5312
2024-02-10 22:11:37,440 [Epoch: 5312 Step: 00047800] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:     3351 || Batch Translation Loss:   0.008251 => Txt Tokens per Sec:     7445 || Lr: 0.000100
2024-02-10 22:11:52,867 Epoch 5312: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:11:52,868 EPOCH 5313
2024-02-10 22:12:09,254 Epoch 5313: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:12:09,254 EPOCH 5314
2024-02-10 22:12:25,647 Epoch 5314: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:12:25,648 EPOCH 5315
2024-02-10 22:12:41,814 Epoch 5315: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:12:41,815 EPOCH 5316
2024-02-10 22:12:57,840 Epoch 5316: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:12:57,840 EPOCH 5317
2024-02-10 22:13:13,975 Epoch 5317: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 22:13:13,976 EPOCH 5318
2024-02-10 22:13:30,574 Epoch 5318: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 22:13:30,575 EPOCH 5319
2024-02-10 22:13:46,580 Epoch 5319: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 22:13:46,581 EPOCH 5320
2024-02-10 22:14:02,630 Epoch 5320: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 22:14:02,630 EPOCH 5321
2024-02-10 22:14:19,112 Epoch 5321: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 22:14:19,113 EPOCH 5322
2024-02-10 22:14:34,987 Epoch 5322: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 22:14:34,988 EPOCH 5323
2024-02-10 22:14:42,496 [Epoch: 5323 Step: 00047900] Batch Recognition Loss:   0.000378 => Gls Tokens per Sec:      221 || Batch Translation Loss:   0.005911 => Txt Tokens per Sec:      725 || Lr: 0.000100
2024-02-10 22:14:51,308 Epoch 5323: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 22:14:51,309 EPOCH 5324
2024-02-10 22:15:07,698 Epoch 5324: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-10 22:15:07,699 EPOCH 5325
2024-02-10 22:15:23,742 Epoch 5325: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 22:15:23,743 EPOCH 5326
2024-02-10 22:15:39,866 Epoch 5326: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 22:15:39,866 EPOCH 5327
2024-02-10 22:15:55,853 Epoch 5327: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 22:15:55,853 EPOCH 5328
2024-02-10 22:16:12,053 Epoch 5328: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 22:16:12,054 EPOCH 5329
2024-02-10 22:16:28,115 Epoch 5329: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 22:16:28,116 EPOCH 5330
2024-02-10 22:16:44,200 Epoch 5330: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:16:44,200 EPOCH 5331
2024-02-10 22:16:59,959 Epoch 5331: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:16:59,959 EPOCH 5332
2024-02-10 22:17:16,346 Epoch 5332: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 22:17:16,346 EPOCH 5333
2024-02-10 22:17:32,423 Epoch 5333: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:17:32,424 EPOCH 5334
2024-02-10 22:17:40,346 [Epoch: 5334 Step: 00048000] Batch Recognition Loss:   0.000242 => Gls Tokens per Sec:      371 || Batch Translation Loss:   0.014467 => Txt Tokens per Sec:     1153 || Lr: 0.000100
2024-02-10 22:18:51,902 Validation result at epoch 5334, step    48000: duration: 71.5547s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.27862	Translation Loss: 103640.67188	PPL: 31308.48828
	Eval Metric: BLEU
	WER 2.12	(DEL: 0.00,	INS: 0.00,	SUB: 2.12)
	BLEU-4 0.44	(BLEU-1: 10.75,	BLEU-2: 3.32,	BLEU-3: 1.12,	BLEU-4: 0.44)
	CHRF 16.98	ROUGE 8.95
2024-02-10 22:18:51,903 Logging Recognition and Translation Outputs
2024-02-10 22:18:51,903 ========================================================================================================================
2024-02-10 22:18:51,904 Logging Sequence: 96_93.00
2024-02-10 22:18:51,904 	Gloss Reference :	A B+C+D+E
2024-02-10 22:18:51,904 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 22:18:51,904 	Gloss Alignment :	         
2024-02-10 22:18:51,905 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 22:18:51,906 	Text Reference  :	*** **** **** ** bhuvneshwar kumar took 4  wickets and  hardik pandya took 3   wickets wonderful
2024-02-10 22:18:51,906 	Text Hypothesis :	the 19th over in the         dhoni on   10 teams   have a      rest   of   win was     win      
2024-02-10 22:18:51,906 	Text Alignment  :	I   I    I    I  S           S     S    S  S       S    S      S      S    S   S       S        
2024-02-10 22:18:51,906 ========================================================================================================================
2024-02-10 22:18:51,906 Logging Sequence: 144_2.00
2024-02-10 22:18:51,907 	Gloss Reference :	A B+C+D+E  
2024-02-10 22:18:51,907 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-10 22:18:51,907 	Gloss Alignment :	  S        
2024-02-10 22:18:51,907 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 22:18:51,909 	Text Reference  :	a      girl     posted a ***** video  of herself  playing cricket on   a  village farm   on  social media   the video has gone viral   
2024-02-10 22:18:51,910 	Text Hypothesis :	police detained over   a dozen people in brussels and     eight   more at the     bottom now people started the ***** *** **** district
2024-02-10 22:18:51,910 	Text Alignment  :	S      S        S        I     S      S  S        S       S       S    S  S       S      S   S      S           D     D   D    S       
2024-02-10 22:18:51,910 ========================================================================================================================
2024-02-10 22:18:51,910 Logging Sequence: 178_83.00
2024-02-10 22:18:51,910 	Gloss Reference :	A B+C+D+E
2024-02-10 22:18:51,910 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 22:18:51,911 	Gloss Alignment :	         
2024-02-10 22:18:51,911 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 22:18:51,911 	Text Reference  :	and the     police still haven't apprehended the wrestler
2024-02-10 22:18:51,911 	Text Hypothesis :	*** against sushil kumar was     involved    in  home    
2024-02-10 22:18:51,912 	Text Alignment  :	D   S       S      S     S       S           S   S       
2024-02-10 22:18:51,912 ========================================================================================================================
2024-02-10 22:18:51,912 Logging Sequence: 169_214.00
2024-02-10 22:18:51,912 	Gloss Reference :	A B+C+D+E
2024-02-10 22:18:51,912 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 22:18:51,912 	Gloss Alignment :	         
2024-02-10 22:18:51,913 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 22:18:51,914 	Text Reference  :	virat kohli said that though arshdeep dropped    the   catch he           is   still a    strong part       of     the indian team       
2024-02-10 22:18:51,914 	Text Hypothesis :	***** ***** **** **** ****** the      government found this  unacceptable this could have put    arshdeep's family in  danger harrassment
2024-02-10 22:18:51,915 	Text Alignment  :	D     D     D    D    D      S        S          S     S     S            S    S     S    S      S          S      S   S      S          
2024-02-10 22:18:51,915 ========================================================================================================================
2024-02-10 22:18:51,915 Logging Sequence: 147_202.00
2024-02-10 22:18:51,915 	Gloss Reference :	A B+C+D+E
2024-02-10 22:18:51,915 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 22:18:51,915 	Gloss Alignment :	         
2024-02-10 22:18:51,916 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 22:18:51,918 	Text Reference  :	were impressed that she took the   difficult decision to   withdraw from the olympics and focus  on *** her    mental health
2024-02-10 22:18:51,918 	Text Hypothesis :	**** this      is   why he   loves playing   games    like pubg     call of  duty     and others on his mobile or     ipad  
2024-02-10 22:18:51,918 	Text Alignment  :	D    S         S    S   S    S     S         S        S    S        S    S   S            S         I   S      S      S     
2024-02-10 22:18:51,918 ========================================================================================================================
2024-02-10 22:19:00,525 Epoch 5334: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:19:00,526 EPOCH 5335
2024-02-10 22:19:16,858 Epoch 5335: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:19:16,858 EPOCH 5336
2024-02-10 22:19:33,385 Epoch 5336: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:19:33,385 EPOCH 5337
2024-02-10 22:19:49,247 Epoch 5337: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:19:49,247 EPOCH 5338
2024-02-10 22:20:05,306 Epoch 5338: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:20:05,307 EPOCH 5339
2024-02-10 22:20:21,295 Epoch 5339: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:20:21,295 EPOCH 5340
2024-02-10 22:20:37,360 Epoch 5340: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:20:37,361 EPOCH 5341
2024-02-10 22:20:53,628 Epoch 5341: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:20:53,629 EPOCH 5342
2024-02-10 22:21:10,078 Epoch 5342: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:21:10,079 EPOCH 5343
2024-02-10 22:21:26,116 Epoch 5343: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:21:26,117 EPOCH 5344
2024-02-10 22:21:42,286 Epoch 5344: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:21:42,287 EPOCH 5345
2024-02-10 22:21:51,090 [Epoch: 5345 Step: 00048100] Batch Recognition Loss:   0.000173 => Gls Tokens per Sec:      479 || Batch Translation Loss:   0.012805 => Txt Tokens per Sec:     1483 || Lr: 0.000050
2024-02-10 22:21:58,301 Epoch 5345: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:21:58,302 EPOCH 5346
2024-02-10 22:22:14,499 Epoch 5346: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:22:14,499 EPOCH 5347
2024-02-10 22:22:30,552 Epoch 5347: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:22:30,553 EPOCH 5348
2024-02-10 22:22:47,315 Epoch 5348: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:22:47,315 EPOCH 5349
2024-02-10 22:23:03,663 Epoch 5349: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:23:03,663 EPOCH 5350
2024-02-10 22:23:19,669 Epoch 5350: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:23:19,669 EPOCH 5351
2024-02-10 22:23:35,701 Epoch 5351: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:23:35,701 EPOCH 5352
2024-02-10 22:23:51,374 Epoch 5352: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:23:51,375 EPOCH 5353
2024-02-10 22:24:07,987 Epoch 5353: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:24:07,987 EPOCH 5354
2024-02-10 22:24:24,055 Epoch 5354: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:24:24,055 EPOCH 5355
2024-02-10 22:24:40,150 Epoch 5355: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:24:40,151 EPOCH 5356
2024-02-10 22:24:50,973 [Epoch: 5356 Step: 00048200] Batch Recognition Loss:   0.000210 => Gls Tokens per Sec:      591 || Batch Translation Loss:   0.014947 => Txt Tokens per Sec:     1748 || Lr: 0.000050
2024-02-10 22:24:56,720 Epoch 5356: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:24:56,721 EPOCH 5357
2024-02-10 22:25:12,790 Epoch 5357: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:25:12,790 EPOCH 5358
2024-02-10 22:25:28,862 Epoch 5358: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:25:28,862 EPOCH 5359
2024-02-10 22:25:44,976 Epoch 5359: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:25:44,977 EPOCH 5360
2024-02-10 22:26:01,275 Epoch 5360: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:26:01,275 EPOCH 5361
2024-02-10 22:26:17,488 Epoch 5361: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:26:17,489 EPOCH 5362
2024-02-10 22:26:33,441 Epoch 5362: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:26:33,442 EPOCH 5363
2024-02-10 22:26:49,510 Epoch 5363: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:26:49,511 EPOCH 5364
2024-02-10 22:27:05,970 Epoch 5364: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:27:05,971 EPOCH 5365
2024-02-10 22:27:21,934 Epoch 5365: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:27:21,935 EPOCH 5366
2024-02-10 22:27:37,612 Epoch 5366: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:27:37,613 EPOCH 5367
2024-02-10 22:27:49,010 [Epoch: 5367 Step: 00048300] Batch Recognition Loss:   0.000125 => Gls Tokens per Sec:      674 || Batch Translation Loss:   0.015745 => Txt Tokens per Sec:     1971 || Lr: 0.000050
2024-02-10 22:27:54,033 Epoch 5367: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:27:54,034 EPOCH 5368
2024-02-10 22:28:10,083 Epoch 5368: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:28:10,084 EPOCH 5369
2024-02-10 22:28:26,303 Epoch 5369: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:28:26,303 EPOCH 5370
2024-02-10 22:28:42,742 Epoch 5370: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:28:42,742 EPOCH 5371
2024-02-10 22:28:58,871 Epoch 5371: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:28:58,872 EPOCH 5372
2024-02-10 22:29:14,652 Epoch 5372: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:29:14,652 EPOCH 5373
2024-02-10 22:29:31,127 Epoch 5373: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:29:31,128 EPOCH 5374
2024-02-10 22:29:47,381 Epoch 5374: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:29:47,382 EPOCH 5375
2024-02-10 22:30:03,587 Epoch 5375: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:30:03,588 EPOCH 5376
2024-02-10 22:30:19,776 Epoch 5376: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:30:19,777 EPOCH 5377
2024-02-10 22:30:35,619 Epoch 5377: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:30:35,620 EPOCH 5378
2024-02-10 22:30:50,665 [Epoch: 5378 Step: 00048400] Batch Recognition Loss:   0.000270 => Gls Tokens per Sec:      536 || Batch Translation Loss:   0.008185 => Txt Tokens per Sec:     1458 || Lr: 0.000050
2024-02-10 22:30:51,907 Epoch 5378: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:30:51,907 EPOCH 5379
2024-02-10 22:31:08,060 Epoch 5379: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:31:08,061 EPOCH 5380
2024-02-10 22:31:24,112 Epoch 5380: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:31:24,112 EPOCH 5381
2024-02-10 22:31:40,133 Epoch 5381: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:31:40,134 EPOCH 5382
2024-02-10 22:31:56,323 Epoch 5382: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:31:56,324 EPOCH 5383
2024-02-10 22:32:12,739 Epoch 5383: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:32:12,740 EPOCH 5384
2024-02-10 22:32:28,210 Epoch 5384: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:32:28,210 EPOCH 5385
2024-02-10 22:32:44,463 Epoch 5385: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:32:44,464 EPOCH 5386
2024-02-10 22:33:00,745 Epoch 5386: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:33:00,746 EPOCH 5387
2024-02-10 22:33:16,572 Epoch 5387: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:33:16,572 EPOCH 5388
2024-02-10 22:33:33,763 Epoch 5388: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:33:33,764 EPOCH 5389
2024-02-10 22:33:45,506 [Epoch: 5389 Step: 00048500] Batch Recognition Loss:   0.000142 => Gls Tokens per Sec:      872 || Batch Translation Loss:   0.014906 => Txt Tokens per Sec:     2386 || Lr: 0.000050
2024-02-10 22:33:49,791 Epoch 5389: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:33:49,791 EPOCH 5390
2024-02-10 22:34:05,992 Epoch 5390: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:34:05,993 EPOCH 5391
2024-02-10 22:34:22,146 Epoch 5391: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:34:22,147 EPOCH 5392
2024-02-10 22:34:37,945 Epoch 5392: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:34:37,946 EPOCH 5393
2024-02-10 22:34:53,850 Epoch 5393: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:34:53,850 EPOCH 5394
2024-02-10 22:35:10,369 Epoch 5394: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:35:10,370 EPOCH 5395
2024-02-10 22:35:26,439 Epoch 5395: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:35:26,439 EPOCH 5396
2024-02-10 22:35:42,379 Epoch 5396: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:35:42,380 EPOCH 5397
2024-02-10 22:35:58,230 Epoch 5397: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:35:58,231 EPOCH 5398
2024-02-10 22:36:14,763 Epoch 5398: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:36:14,763 EPOCH 5399
2024-02-10 22:36:30,658 Epoch 5399: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:36:30,659 EPOCH 5400
2024-02-10 22:36:46,800 [Epoch: 5400 Step: 00048600] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:      658 || Batch Translation Loss:   0.009527 => Txt Tokens per Sec:     1820 || Lr: 0.000050
2024-02-10 22:36:46,801 Epoch 5400: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:36:46,801 EPOCH 5401
2024-02-10 22:37:02,706 Epoch 5401: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:37:02,707 EPOCH 5402
2024-02-10 22:37:19,067 Epoch 5402: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:37:19,067 EPOCH 5403
2024-02-10 22:37:35,169 Epoch 5403: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:37:35,170 EPOCH 5404
2024-02-10 22:37:51,524 Epoch 5404: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:37:51,524 EPOCH 5405
2024-02-10 22:38:07,724 Epoch 5405: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-10 22:38:07,725 EPOCH 5406
2024-02-10 22:38:23,857 Epoch 5406: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:38:23,858 EPOCH 5407
2024-02-10 22:38:39,954 Epoch 5407: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:38:39,955 EPOCH 5408
2024-02-10 22:38:56,154 Epoch 5408: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:38:56,155 EPOCH 5409
2024-02-10 22:39:12,384 Epoch 5409: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:39:12,385 EPOCH 5410
2024-02-10 22:39:28,242 Epoch 5410: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:39:28,243 EPOCH 5411
2024-02-10 22:39:44,115 Epoch 5411: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-10 22:39:44,115 EPOCH 5412
2024-02-10 22:39:44,860 [Epoch: 5412 Step: 00048700] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:     1723 || Batch Translation Loss:   0.014356 => Txt Tokens per Sec:     5205 || Lr: 0.000050
2024-02-10 22:39:59,968 Epoch 5412: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:39:59,968 EPOCH 5413
2024-02-10 22:40:15,954 Epoch 5413: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:40:15,955 EPOCH 5414
2024-02-10 22:40:32,223 Epoch 5414: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:40:32,223 EPOCH 5415
2024-02-10 22:40:48,328 Epoch 5415: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:40:48,329 EPOCH 5416
2024-02-10 22:41:04,468 Epoch 5416: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 22:41:04,468 EPOCH 5417
2024-02-10 22:41:20,380 Epoch 5417: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 22:41:20,380 EPOCH 5418
2024-02-10 22:41:36,327 Epoch 5418: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 22:41:36,328 EPOCH 5419
2024-02-10 22:41:52,654 Epoch 5419: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 22:41:52,654 EPOCH 5420
2024-02-10 22:42:08,940 Epoch 5420: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 22:42:08,940 EPOCH 5421
2024-02-10 22:42:25,189 Epoch 5421: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 22:42:25,189 EPOCH 5422
2024-02-10 22:42:40,979 Epoch 5422: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 22:42:40,979 EPOCH 5423
2024-02-10 22:42:41,963 [Epoch: 5423 Step: 00048800] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:     2607 || Batch Translation Loss:   0.021840 => Txt Tokens per Sec:     7321 || Lr: 0.000050
2024-02-10 22:42:56,691 Epoch 5423: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 22:42:56,692 EPOCH 5424
2024-02-10 22:43:12,661 Epoch 5424: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 22:43:12,661 EPOCH 5425
2024-02-10 22:43:29,097 Epoch 5425: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 22:43:29,098 EPOCH 5426
2024-02-10 22:43:45,095 Epoch 5426: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-10 22:43:45,096 EPOCH 5427
2024-02-10 22:44:01,561 Epoch 5427: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.76 
2024-02-10 22:44:01,562 EPOCH 5428
2024-02-10 22:44:17,561 Epoch 5428: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-10 22:44:17,562 EPOCH 5429
2024-02-10 22:44:33,955 Epoch 5429: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-10 22:44:33,956 EPOCH 5430
2024-02-10 22:44:49,861 Epoch 5430: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-10 22:44:49,862 EPOCH 5431
2024-02-10 22:45:06,008 Epoch 5431: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 22:45:06,008 EPOCH 5432
2024-02-10 22:45:21,963 Epoch 5432: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 22:45:21,964 EPOCH 5433
2024-02-10 22:45:38,140 Epoch 5433: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 22:45:38,140 EPOCH 5434
2024-02-10 22:45:39,294 [Epoch: 5434 Step: 00048900] Batch Recognition Loss:   0.000391 => Gls Tokens per Sec:     3330 || Batch Translation Loss:   0.007493 => Txt Tokens per Sec:     8028 || Lr: 0.000050
2024-02-10 22:45:54,299 Epoch 5434: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 22:45:54,299 EPOCH 5435
2024-02-10 22:46:10,127 Epoch 5435: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 22:46:10,128 EPOCH 5436
2024-02-10 22:46:26,499 Epoch 5436: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 22:46:26,500 EPOCH 5437
2024-02-10 22:46:42,479 Epoch 5437: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 22:46:42,479 EPOCH 5438
2024-02-10 22:46:58,947 Epoch 5438: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:46:58,947 EPOCH 5439
2024-02-10 22:47:15,117 Epoch 5439: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:47:15,117 EPOCH 5440
2024-02-10 22:47:31,033 Epoch 5440: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:47:31,034 EPOCH 5441
2024-02-10 22:47:47,049 Epoch 5441: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:47:47,050 EPOCH 5442
2024-02-10 22:48:03,124 Epoch 5442: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:48:03,124 EPOCH 5443
2024-02-10 22:48:18,927 Epoch 5443: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:48:18,927 EPOCH 5444
2024-02-10 22:48:35,091 Epoch 5444: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 22:48:35,092 EPOCH 5445
2024-02-10 22:48:39,485 [Epoch: 5445 Step: 00049000] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:     1166 || Batch Translation Loss:   0.012863 => Txt Tokens per Sec:     3100 || Lr: 0.000050
2024-02-10 22:48:51,275 Epoch 5445: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:48:51,275 EPOCH 5446
2024-02-10 22:49:07,321 Epoch 5446: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 22:49:07,322 EPOCH 5447
2024-02-10 22:49:23,619 Epoch 5447: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 22:49:23,620 EPOCH 5448
2024-02-10 22:49:39,722 Epoch 5448: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 22:49:39,722 EPOCH 5449
2024-02-10 22:49:55,682 Epoch 5449: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 22:49:55,683 EPOCH 5450
2024-02-10 22:50:11,627 Epoch 5450: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 22:50:11,627 EPOCH 5451
2024-02-10 22:50:27,774 Epoch 5451: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:50:27,775 EPOCH 5452
2024-02-10 22:50:44,007 Epoch 5452: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:50:44,008 EPOCH 5453
2024-02-10 22:51:00,398 Epoch 5453: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:51:00,399 EPOCH 5454
2024-02-10 22:51:16,116 Epoch 5454: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 22:51:16,116 EPOCH 5455
2024-02-10 22:51:32,368 Epoch 5455: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:51:32,369 EPOCH 5456
2024-02-10 22:51:34,675 [Epoch: 5456 Step: 00049100] Batch Recognition Loss:   0.000118 => Gls Tokens per Sec:     2778 || Batch Translation Loss:   0.010411 => Txt Tokens per Sec:     7478 || Lr: 0.000050
2024-02-10 22:51:48,356 Epoch 5456: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 22:51:48,357 EPOCH 5457
2024-02-10 22:52:04,456 Epoch 5457: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:52:04,456 EPOCH 5458
2024-02-10 22:52:20,366 Epoch 5458: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 22:52:20,367 EPOCH 5459
2024-02-10 22:52:37,004 Epoch 5459: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 22:52:37,005 EPOCH 5460
2024-02-10 22:52:53,329 Epoch 5460: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 22:52:53,329 EPOCH 5461
2024-02-10 22:53:09,272 Epoch 5461: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 22:53:09,272 EPOCH 5462
2024-02-10 22:53:25,285 Epoch 5462: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:53:25,286 EPOCH 5463
2024-02-10 22:53:41,500 Epoch 5463: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 22:53:41,500 EPOCH 5464
2024-02-10 22:53:57,781 Epoch 5464: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 22:53:57,782 EPOCH 5465
2024-02-10 22:54:13,785 Epoch 5465: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:54:13,786 EPOCH 5466
2024-02-10 22:54:29,583 Epoch 5466: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 22:54:29,584 EPOCH 5467
2024-02-10 22:54:44,491 [Epoch: 5467 Step: 00049200] Batch Recognition Loss:   0.000113 => Gls Tokens per Sec:      455 || Batch Translation Loss:   0.011277 => Txt Tokens per Sec:     1375 || Lr: 0.000050
2024-02-10 22:54:45,705 Epoch 5467: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 22:54:45,705 EPOCH 5468
2024-02-10 22:55:01,910 Epoch 5468: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 22:55:01,911 EPOCH 5469
2024-02-10 22:55:17,871 Epoch 5469: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 22:55:17,871 EPOCH 5470
2024-02-10 22:55:34,125 Epoch 5470: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:55:34,126 EPOCH 5471
2024-02-10 22:55:50,146 Epoch 5471: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:55:50,147 EPOCH 5472
2024-02-10 22:56:06,152 Epoch 5472: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 22:56:06,152 EPOCH 5473
2024-02-10 22:56:21,848 Epoch 5473: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 22:56:21,848 EPOCH 5474
2024-02-10 22:56:37,756 Epoch 5474: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:56:37,757 EPOCH 5475
2024-02-10 22:56:53,534 Epoch 5475: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 22:56:53,534 EPOCH 5476
2024-02-10 22:57:09,427 Epoch 5476: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 22:57:09,428 EPOCH 5477
2024-02-10 22:57:25,760 Epoch 5477: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 22:57:25,760 EPOCH 5478
2024-02-10 22:57:35,320 [Epoch: 5478 Step: 00049300] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:      843 || Batch Translation Loss:   0.006786 => Txt Tokens per Sec:     2371 || Lr: 0.000050
2024-02-10 22:57:41,500 Epoch 5478: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 22:57:41,500 EPOCH 5479
2024-02-10 22:57:57,392 Epoch 5479: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 22:57:57,392 EPOCH 5480
2024-02-10 22:58:13,083 Epoch 5480: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 22:58:13,084 EPOCH 5481
2024-02-10 22:58:28,912 Epoch 5481: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 22:58:28,912 EPOCH 5482
2024-02-10 22:58:45,170 Epoch 5482: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 22:58:45,171 EPOCH 5483
2024-02-10 22:59:01,244 Epoch 5483: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 22:59:01,244 EPOCH 5484
2024-02-10 22:59:17,352 Epoch 5484: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 22:59:17,352 EPOCH 5485
2024-02-10 22:59:33,291 Epoch 5485: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 22:59:33,292 EPOCH 5486
2024-02-10 22:59:49,358 Epoch 5486: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 22:59:49,359 EPOCH 5487
2024-02-10 23:00:05,423 Epoch 5487: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 23:00:05,424 EPOCH 5488
2024-02-10 23:00:21,551 Epoch 5488: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 23:00:21,551 EPOCH 5489
2024-02-10 23:00:33,148 [Epoch: 5489 Step: 00049400] Batch Recognition Loss:   0.000330 => Gls Tokens per Sec:      883 || Batch Translation Loss:   0.008214 => Txt Tokens per Sec:     2417 || Lr: 0.000050
2024-02-10 23:00:37,429 Epoch 5489: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 23:00:37,429 EPOCH 5490
2024-02-10 23:00:53,718 Epoch 5490: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 23:00:53,718 EPOCH 5491
2024-02-10 23:01:09,794 Epoch 5491: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 23:01:09,794 EPOCH 5492
2024-02-10 23:01:25,775 Epoch 5492: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 23:01:25,776 EPOCH 5493
2024-02-10 23:01:41,952 Epoch 5493: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 23:01:41,952 EPOCH 5494
2024-02-10 23:01:57,751 Epoch 5494: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 23:01:57,752 EPOCH 5495
2024-02-10 23:02:14,073 Epoch 5495: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 23:02:14,074 EPOCH 5496
2024-02-10 23:02:29,948 Epoch 5496: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 23:02:29,949 EPOCH 5497
2024-02-10 23:02:46,244 Epoch 5497: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 23:02:46,245 EPOCH 5498
2024-02-10 23:03:02,263 Epoch 5498: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 23:03:02,263 EPOCH 5499
2024-02-10 23:03:18,441 Epoch 5499: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 23:03:18,442 EPOCH 5500
2024-02-10 23:03:34,237 [Epoch: 5500 Step: 00049500] Batch Recognition Loss:   0.000259 => Gls Tokens per Sec:      672 || Batch Translation Loss:   0.020997 => Txt Tokens per Sec:     1860 || Lr: 0.000050
2024-02-10 23:03:34,238 Epoch 5500: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 23:03:34,238 EPOCH 5501
2024-02-10 23:03:50,359 Epoch 5501: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 23:03:50,360 EPOCH 5502
2024-02-10 23:04:06,500 Epoch 5502: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 23:04:06,501 EPOCH 5503
2024-02-10 23:04:22,877 Epoch 5503: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 23:04:22,877 EPOCH 5504
2024-02-10 23:04:38,626 Epoch 5504: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 23:04:38,627 EPOCH 5505
2024-02-10 23:04:54,727 Epoch 5505: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:04:54,728 EPOCH 5506
2024-02-10 23:05:10,334 Epoch 5506: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 23:05:10,335 EPOCH 5507
2024-02-10 23:05:26,471 Epoch 5507: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 23:05:26,472 EPOCH 5508
2024-02-10 23:05:42,436 Epoch 5508: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 23:05:42,436 EPOCH 5509
2024-02-10 23:05:58,486 Epoch 5509: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-10 23:05:58,487 EPOCH 5510
2024-02-10 23:06:14,555 Epoch 5510: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-10 23:06:14,555 EPOCH 5511
2024-02-10 23:06:30,542 Epoch 5511: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-10 23:06:30,543 EPOCH 5512
2024-02-10 23:06:30,762 [Epoch: 5512 Step: 00049600] Batch Recognition Loss:   0.000334 => Gls Tokens per Sec:     5845 || Batch Translation Loss:   0.011353 => Txt Tokens per Sec:    10548 || Lr: 0.000050
2024-02-10 23:06:46,464 Epoch 5512: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-10 23:06:46,464 EPOCH 5513
2024-02-10 23:07:02,296 Epoch 5513: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-10 23:07:02,296 EPOCH 5514
2024-02-10 23:07:18,874 Epoch 5514: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-10 23:07:18,874 EPOCH 5515
2024-02-10 23:07:35,004 Epoch 5515: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 23:07:35,005 EPOCH 5516
2024-02-10 23:07:51,161 Epoch 5516: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 23:07:51,161 EPOCH 5517
2024-02-10 23:08:07,113 Epoch 5517: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 23:08:07,113 EPOCH 5518
2024-02-10 23:08:23,399 Epoch 5518: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-10 23:08:23,399 EPOCH 5519
2024-02-10 23:08:39,770 Epoch 5519: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 23:08:39,771 EPOCH 5520
2024-02-10 23:08:56,020 Epoch 5520: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-10 23:08:56,021 EPOCH 5521
2024-02-10 23:09:12,373 Epoch 5521: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 23:09:12,374 EPOCH 5522
2024-02-10 23:09:28,647 Epoch 5522: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 23:09:28,647 EPOCH 5523
2024-02-10 23:09:32,588 [Epoch: 5523 Step: 00049700] Batch Recognition Loss:   0.000212 => Gls Tokens per Sec:      650 || Batch Translation Loss:   0.017534 => Txt Tokens per Sec:     1979 || Lr: 0.000050
2024-02-10 23:09:45,065 Epoch 5523: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 23:09:45,065 EPOCH 5524
2024-02-10 23:10:01,355 Epoch 5524: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 23:10:01,355 EPOCH 5525
2024-02-10 23:10:17,911 Epoch 5525: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 23:10:17,912 EPOCH 5526
2024-02-10 23:10:34,257 Epoch 5526: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 23:10:34,258 EPOCH 5527
2024-02-10 23:10:50,453 Epoch 5527: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 23:10:50,454 EPOCH 5528
2024-02-10 23:11:06,804 Epoch 5528: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 23:11:06,804 EPOCH 5529
2024-02-10 23:11:22,953 Epoch 5529: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 23:11:22,954 EPOCH 5530
2024-02-10 23:11:38,849 Epoch 5530: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 23:11:38,850 EPOCH 5531
2024-02-10 23:11:54,752 Epoch 5531: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 23:11:54,752 EPOCH 5532
2024-02-10 23:12:10,878 Epoch 5532: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:12:10,878 EPOCH 5533
2024-02-10 23:12:27,173 Epoch 5533: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:12:27,173 EPOCH 5534
2024-02-10 23:12:31,280 [Epoch: 5534 Step: 00049800] Batch Recognition Loss:   0.000292 => Gls Tokens per Sec:      935 || Batch Translation Loss:   0.016686 => Txt Tokens per Sec:     2475 || Lr: 0.000050
2024-02-10 23:12:43,375 Epoch 5534: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 23:12:43,376 EPOCH 5535
2024-02-10 23:12:59,428 Epoch 5535: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:12:59,429 EPOCH 5536
2024-02-10 23:13:15,014 Epoch 5536: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 23:13:15,015 EPOCH 5537
2024-02-10 23:13:31,084 Epoch 5537: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 23:13:31,085 EPOCH 5538
2024-02-10 23:13:47,109 Epoch 5538: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:13:47,109 EPOCH 5539
2024-02-10 23:14:03,246 Epoch 5539: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:14:03,247 EPOCH 5540
2024-02-10 23:14:19,472 Epoch 5540: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:14:19,472 EPOCH 5541
2024-02-10 23:14:35,689 Epoch 5541: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:14:35,689 EPOCH 5542
2024-02-10 23:14:51,699 Epoch 5542: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:14:51,700 EPOCH 5543
2024-02-10 23:15:07,587 Epoch 5543: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:15:07,587 EPOCH 5544
2024-02-10 23:15:23,742 Epoch 5544: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:15:23,742 EPOCH 5545
2024-02-10 23:15:25,250 [Epoch: 5545 Step: 00049900] Batch Recognition Loss:   0.000169 => Gls Tokens per Sec:     3397 || Batch Translation Loss:   0.016332 => Txt Tokens per Sec:     8218 || Lr: 0.000050
2024-02-10 23:15:39,786 Epoch 5545: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:15:39,786 EPOCH 5546
2024-02-10 23:15:56,089 Epoch 5546: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:15:56,090 EPOCH 5547
2024-02-10 23:16:12,009 Epoch 5547: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:16:12,009 EPOCH 5548
2024-02-10 23:16:28,104 Epoch 5548: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:16:28,105 EPOCH 5549
2024-02-10 23:16:43,989 Epoch 5549: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:16:43,990 EPOCH 5550
2024-02-10 23:17:00,194 Epoch 5550: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 23:17:00,194 EPOCH 5551
2024-02-10 23:17:16,490 Epoch 5551: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 23:17:16,491 EPOCH 5552
2024-02-10 23:17:32,787 Epoch 5552: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:17:32,788 EPOCH 5553
2024-02-10 23:17:48,728 Epoch 5553: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:17:48,728 EPOCH 5554
2024-02-10 23:18:04,702 Epoch 5554: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:18:04,703 EPOCH 5555
2024-02-10 23:18:20,677 Epoch 5555: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:18:20,677 EPOCH 5556
2024-02-10 23:18:32,146 [Epoch: 5556 Step: 00050000] Batch Recognition Loss:   0.000151 => Gls Tokens per Sec:      480 || Batch Translation Loss:   0.009683 => Txt Tokens per Sec:     1296 || Lr: 0.000050
2024-02-10 23:19:44,708 Validation result at epoch 5556, step    50000: duration: 72.5613s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.26571	Translation Loss: 104185.42188	PPL: 33059.17969
	Eval Metric: BLEU
	WER 2.05	(DEL: 0.00,	INS: 0.00,	SUB: 2.05)
	BLEU-4 0.59	(BLEU-1: 10.19,	BLEU-2: 2.84,	BLEU-3: 1.13,	BLEU-4: 0.59)
	CHRF 16.84	ROUGE 8.50
2024-02-10 23:19:44,710 Logging Recognition and Translation Outputs
2024-02-10 23:19:44,710 ========================================================================================================================
2024-02-10 23:19:44,711 Logging Sequence: 178_157.00
2024-02-10 23:19:44,711 	Gloss Reference :	A B+C+D+E
2024-02-10 23:19:44,711 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 23:19:44,711 	Gloss Alignment :	         
2024-02-10 23:19:44,711 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 23:19:44,712 	Text Reference  :	this is why sushil kumar will   have  to  be      arrested
2024-02-10 23:19:44,712 	Text Hypothesis :	**** ** *** ****** the   couple could not believe that    
2024-02-10 23:19:44,712 	Text Alignment  :	D    D  D   D      S     S      S     S   S       S       
2024-02-10 23:19:44,712 ========================================================================================================================
2024-02-10 23:19:44,713 Logging Sequence: 118_111.00
2024-02-10 23:19:44,713 	Gloss Reference :	A B+C+D+E
2024-02-10 23:19:44,713 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 23:19:44,713 	Gloss Alignment :	         
2024-02-10 23:19:44,713 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 23:19:44,714 	Text Reference  :	and people encourage him have      hope      for         the   next    world cup    
2024-02-10 23:19:44,714 	Text Hypothesis :	*** ****** but       the pakistani batsmen's belligerant shots secured their victory
2024-02-10 23:19:44,714 	Text Alignment  :	D   D      S         S   S         S         S           S     S       S     S      
2024-02-10 23:19:44,714 ========================================================================================================================
2024-02-10 23:19:44,715 Logging Sequence: 148_2.00
2024-02-10 23:19:44,715 	Gloss Reference :	A B+C+D+E
2024-02-10 23:19:44,715 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 23:19:44,715 	Gloss Alignment :	         
2024-02-10 23:19:44,715 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 23:19:44,717 	Text Reference  :	the final of the asia cup 2023   cricket tournament was     played between india and sri lanka *** on     17th september 2023
2024-02-10 23:19:44,717 	Text Hypothesis :	*** ***** ** *** **** *** scored 1       day        earlier played between india and sri lanka had scored 6    balls     it  
2024-02-10 23:19:44,717 	Text Alignment  :	D   D     D  D   D    D   S      S       S          S                                          I   S      S    S         S   
2024-02-10 23:19:44,718 ========================================================================================================================
2024-02-10 23:19:44,718 Logging Sequence: 83_129.00
2024-02-10 23:19:44,718 	Gloss Reference :	A B+C+D+E
2024-02-10 23:19:44,718 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 23:19:44,718 	Gloss Alignment :	         
2024-02-10 23:19:44,718 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 23:19:44,719 	Text Reference  :	later the *** **** ***** **** ** ** * denmark football association tweeted
2024-02-10 23:19:44,719 	Text Hypothesis :	and   the too were being held in no 2 more    than     its         head   
2024-02-10 23:19:44,719 	Text Alignment  :	S         I   I    I     I    I  I  I S       S        S           S      
2024-02-10 23:19:44,720 ========================================================================================================================
2024-02-10 23:19:44,720 Logging Sequence: 99_158.00
2024-02-10 23:19:44,720 	Gloss Reference :	A B+C+D+E
2024-02-10 23:19:44,720 	Gloss Hypothesis:	A B+C+D+E
2024-02-10 23:19:44,720 	Gloss Alignment :	         
2024-02-10 23:19:44,720 	--------------------------------------------------------------------------------------------------------------------
2024-02-10 23:19:44,721 	Text Reference  :	**** ***** *** the     incident occured in  dubai   and it was extremely shameful
2024-02-10 23:19:44,721 	Text Hypothesis :	then kohli and gambhir do       with    the protest and ** did not       athletes
2024-02-10 23:19:44,722 	Text Alignment  :	I    I     I   S       S        S       S   S           D  S   S         S       
2024-02-10 23:19:44,722 ========================================================================================================================
2024-02-10 23:19:50,115 Epoch 5556: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:19:50,116 EPOCH 5557
2024-02-10 23:20:06,522 Epoch 5557: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:20:06,522 EPOCH 5558
2024-02-10 23:20:22,781 Epoch 5558: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:20:22,781 EPOCH 5559
2024-02-10 23:20:38,895 Epoch 5559: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:20:38,895 EPOCH 5560
2024-02-10 23:20:54,804 Epoch 5560: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:20:54,805 EPOCH 5561
2024-02-10 23:21:10,514 Epoch 5561: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:21:10,515 EPOCH 5562
2024-02-10 23:21:26,997 Epoch 5562: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:21:26,998 EPOCH 5563
2024-02-10 23:21:42,806 Epoch 5563: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:21:42,807 EPOCH 5564
2024-02-10 23:21:58,992 Epoch 5564: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:21:58,993 EPOCH 5565
2024-02-10 23:22:15,106 Epoch 5565: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:22:15,107 EPOCH 5566
2024-02-10 23:22:30,995 Epoch 5566: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:22:30,995 EPOCH 5567
2024-02-10 23:22:36,513 [Epoch: 5567 Step: 00050100] Batch Recognition Loss:   0.000121 => Gls Tokens per Sec:     1392 || Batch Translation Loss:   0.011094 => Txt Tokens per Sec:     3687 || Lr: 0.000050
2024-02-10 23:22:47,530 Epoch 5567: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:22:47,531 EPOCH 5568
2024-02-10 23:23:03,744 Epoch 5568: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:23:03,744 EPOCH 5569
2024-02-10 23:23:19,751 Epoch 5569: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:23:19,751 EPOCH 5570
2024-02-10 23:23:35,606 Epoch 5570: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:23:35,607 EPOCH 5571
2024-02-10 23:23:51,557 Epoch 5571: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:23:51,557 EPOCH 5572
2024-02-10 23:24:07,463 Epoch 5572: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 23:24:07,464 EPOCH 5573
2024-02-10 23:24:23,463 Epoch 5573: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 23:24:23,464 EPOCH 5574
2024-02-10 23:24:39,416 Epoch 5574: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 23:24:39,417 EPOCH 5575
2024-02-10 23:24:55,514 Epoch 5575: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 23:24:55,514 EPOCH 5576
2024-02-10 23:25:12,039 Epoch 5576: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 23:25:12,040 EPOCH 5577
2024-02-10 23:25:28,354 Epoch 5577: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 23:25:28,354 EPOCH 5578
2024-02-10 23:25:43,597 [Epoch: 5578 Step: 00050200] Batch Recognition Loss:   0.000200 => Gls Tokens per Sec:      529 || Batch Translation Loss:   0.016857 => Txt Tokens per Sec:     1487 || Lr: 0.000050
2024-02-10 23:25:44,623 Epoch 5578: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 23:25:44,623 EPOCH 5579
2024-02-10 23:26:00,640 Epoch 5579: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 23:26:00,640 EPOCH 5580
2024-02-10 23:26:16,772 Epoch 5580: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 23:26:16,773 EPOCH 5581
2024-02-10 23:26:32,801 Epoch 5581: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 23:26:32,802 EPOCH 5582
2024-02-10 23:26:48,743 Epoch 5582: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 23:26:48,743 EPOCH 5583
2024-02-10 23:27:04,737 Epoch 5583: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 23:27:04,738 EPOCH 5584
2024-02-10 23:27:20,853 Epoch 5584: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 23:27:20,854 EPOCH 5585
2024-02-10 23:27:36,744 Epoch 5585: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 23:27:36,744 EPOCH 5586
2024-02-10 23:27:52,812 Epoch 5586: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 23:27:52,813 EPOCH 5587
2024-02-10 23:28:09,169 Epoch 5587: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 23:28:09,170 EPOCH 5588
2024-02-10 23:28:25,510 Epoch 5588: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 23:28:25,510 EPOCH 5589
2024-02-10 23:28:37,286 [Epoch: 5589 Step: 00050300] Batch Recognition Loss:   0.000376 => Gls Tokens per Sec:      870 || Batch Translation Loss:   0.039975 => Txt Tokens per Sec:     2380 || Lr: 0.000050
2024-02-10 23:28:41,580 Epoch 5589: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 23:28:41,580 EPOCH 5590
2024-02-10 23:28:57,950 Epoch 5590: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-10 23:28:57,951 EPOCH 5591
2024-02-10 23:29:13,915 Epoch 5591: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 23:29:13,916 EPOCH 5592
2024-02-10 23:29:29,985 Epoch 5592: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 23:29:29,986 EPOCH 5593
2024-02-10 23:29:45,916 Epoch 5593: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 23:29:45,916 EPOCH 5594
2024-02-10 23:30:01,897 Epoch 5594: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 23:30:01,898 EPOCH 5595
2024-02-10 23:30:17,810 Epoch 5595: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 23:30:17,811 EPOCH 5596
2024-02-10 23:30:33,970 Epoch 5596: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 23:30:33,970 EPOCH 5597
2024-02-10 23:30:49,966 Epoch 5597: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 23:30:49,967 EPOCH 5598
2024-02-10 23:31:05,988 Epoch 5598: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 23:31:05,988 EPOCH 5599
2024-02-10 23:31:22,513 Epoch 5599: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 23:31:22,513 EPOCH 5600
2024-02-10 23:31:38,487 [Epoch: 5600 Step: 00050400] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:      665 || Batch Translation Loss:   0.017080 => Txt Tokens per Sec:     1840 || Lr: 0.000050
2024-02-10 23:31:38,488 Epoch 5600: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:31:38,488 EPOCH 5601
2024-02-10 23:31:54,499 Epoch 5601: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:31:54,500 EPOCH 5602
2024-02-10 23:32:10,866 Epoch 5602: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:32:10,867 EPOCH 5603
2024-02-10 23:32:26,773 Epoch 5603: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:32:26,774 EPOCH 5604
2024-02-10 23:32:42,615 Epoch 5604: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:32:42,616 EPOCH 5605
2024-02-10 23:32:58,832 Epoch 5605: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:32:58,833 EPOCH 5606
2024-02-10 23:33:14,728 Epoch 5606: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:33:14,729 EPOCH 5607
2024-02-10 23:33:30,928 Epoch 5607: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:33:30,929 EPOCH 5608
2024-02-10 23:33:47,792 Epoch 5608: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:33:47,793 EPOCH 5609
2024-02-10 23:34:04,013 Epoch 5609: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 23:34:04,014 EPOCH 5610
2024-02-10 23:34:20,246 Epoch 5610: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-10 23:34:20,246 EPOCH 5611
2024-02-10 23:34:36,237 Epoch 5611: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 23:34:36,237 EPOCH 5612
2024-02-10 23:34:36,626 [Epoch: 5612 Step: 00050500] Batch Recognition Loss:   0.000218 => Gls Tokens per Sec:     3299 || Batch Translation Loss:   0.093468 => Txt Tokens per Sec:     9155 || Lr: 0.000050
2024-02-10 23:34:52,149 Epoch 5612: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-10 23:34:52,150 EPOCH 5613
2024-02-10 23:35:08,186 Epoch 5613: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-10 23:35:08,186 EPOCH 5614
2024-02-10 23:35:24,606 Epoch 5614: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-10 23:35:24,607 EPOCH 5615
2024-02-10 23:35:40,782 Epoch 5615: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-10 23:35:40,782 EPOCH 5616
2024-02-10 23:35:57,194 Epoch 5616: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-10 23:35:57,195 EPOCH 5617
2024-02-10 23:36:13,334 Epoch 5617: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-10 23:36:13,335 EPOCH 5618
2024-02-10 23:36:29,669 Epoch 5618: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-10 23:36:29,670 EPOCH 5619
2024-02-10 23:36:46,068 Epoch 5619: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-10 23:36:46,069 EPOCH 5620
2024-02-10 23:37:02,210 Epoch 5620: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.78 
2024-02-10 23:37:02,211 EPOCH 5621
2024-02-10 23:37:18,395 Epoch 5621: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-10 23:37:18,396 EPOCH 5622
2024-02-10 23:37:34,563 Epoch 5622: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-10 23:37:34,564 EPOCH 5623
2024-02-10 23:37:35,512 [Epoch: 5623 Step: 00050600] Batch Recognition Loss:   0.000350 => Gls Tokens per Sec:     2703 || Batch Translation Loss:   0.038463 => Txt Tokens per Sec:     7372 || Lr: 0.000050
2024-02-10 23:37:50,882 Epoch 5623: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-10 23:37:50,882 EPOCH 5624
2024-02-10 23:38:07,246 Epoch 5624: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-10 23:38:07,246 EPOCH 5625
2024-02-10 23:38:23,416 Epoch 5625: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-10 23:38:23,416 EPOCH 5626
2024-02-10 23:38:39,406 Epoch 5626: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-10 23:38:39,407 EPOCH 5627
2024-02-10 23:38:55,260 Epoch 5627: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-10 23:38:55,261 EPOCH 5628
2024-02-10 23:39:11,387 Epoch 5628: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-10 23:39:11,387 EPOCH 5629
2024-02-10 23:39:27,672 Epoch 5629: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-10 23:39:27,672 EPOCH 5630
2024-02-10 23:39:43,868 Epoch 5630: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 23:39:43,868 EPOCH 5631
2024-02-10 23:40:00,004 Epoch 5631: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 23:40:00,005 EPOCH 5632
2024-02-10 23:40:16,118 Epoch 5632: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 23:40:16,119 EPOCH 5633
2024-02-10 23:40:32,271 Epoch 5633: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 23:40:32,272 EPOCH 5634
2024-02-10 23:40:37,544 [Epoch: 5634 Step: 00050700] Batch Recognition Loss:   0.000208 => Gls Tokens per Sec:      558 || Batch Translation Loss:   0.022828 => Txt Tokens per Sec:     1665 || Lr: 0.000050
2024-02-10 23:40:48,113 Epoch 5634: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 23:40:48,113 EPOCH 5635
2024-02-10 23:41:04,030 Epoch 5635: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 23:41:04,030 EPOCH 5636
2024-02-10 23:41:19,867 Epoch 5636: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:41:19,867 EPOCH 5637
2024-02-10 23:41:36,019 Epoch 5637: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:41:36,020 EPOCH 5638
2024-02-10 23:41:52,147 Epoch 5638: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:41:52,148 EPOCH 5639
2024-02-10 23:42:08,081 Epoch 5639: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:42:08,082 EPOCH 5640
2024-02-10 23:42:24,148 Epoch 5640: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:42:24,149 EPOCH 5641
2024-02-10 23:42:40,574 Epoch 5641: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:42:40,574 EPOCH 5642
2024-02-10 23:42:56,692 Epoch 5642: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:42:56,693 EPOCH 5643
2024-02-10 23:43:12,863 Epoch 5643: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:43:12,863 EPOCH 5644
2024-02-10 23:43:28,931 Epoch 5644: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:43:28,931 EPOCH 5645
2024-02-10 23:43:34,942 [Epoch: 5645 Step: 00050800] Batch Recognition Loss:   0.000330 => Gls Tokens per Sec:      702 || Batch Translation Loss:   0.006248 => Txt Tokens per Sec:     2069 || Lr: 0.000050
2024-02-10 23:43:45,053 Epoch 5645: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:43:45,053 EPOCH 5646
2024-02-10 23:44:01,345 Epoch 5646: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:44:01,346 EPOCH 5647
2024-02-10 23:44:17,406 Epoch 5647: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:44:17,407 EPOCH 5648
2024-02-10 23:44:33,413 Epoch 5648: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:44:33,414 EPOCH 5649
2024-02-10 23:44:49,528 Epoch 5649: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:44:49,529 EPOCH 5650
2024-02-10 23:45:05,911 Epoch 5650: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:45:05,912 EPOCH 5651
2024-02-10 23:45:21,908 Epoch 5651: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-10 23:45:21,908 EPOCH 5652
2024-02-10 23:45:37,818 Epoch 5652: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 23:45:37,818 EPOCH 5653
2024-02-10 23:45:54,191 Epoch 5653: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 23:45:54,192 EPOCH 5654
2024-02-10 23:46:10,334 Epoch 5654: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:46:10,335 EPOCH 5655
2024-02-10 23:46:26,640 Epoch 5655: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 23:46:26,641 EPOCH 5656
2024-02-10 23:46:34,182 [Epoch: 5656 Step: 00050900] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:      849 || Batch Translation Loss:   0.009938 => Txt Tokens per Sec:     2195 || Lr: 0.000050
2024-02-10 23:46:42,688 Epoch 5656: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:46:42,688 EPOCH 5657
2024-02-10 23:46:59,142 Epoch 5657: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:46:59,143 EPOCH 5658
2024-02-10 23:47:15,157 Epoch 5658: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:47:15,157 EPOCH 5659
2024-02-10 23:47:31,424 Epoch 5659: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:47:31,425 EPOCH 5660
2024-02-10 23:47:47,521 Epoch 5660: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:47:47,521 EPOCH 5661
2024-02-10 23:48:03,284 Epoch 5661: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:48:03,285 EPOCH 5662
2024-02-10 23:48:19,411 Epoch 5662: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 23:48:19,411 EPOCH 5663
2024-02-10 23:48:35,660 Epoch 5663: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 23:48:35,661 EPOCH 5664
2024-02-10 23:48:51,648 Epoch 5664: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:48:51,648 EPOCH 5665
2024-02-10 23:49:07,559 Epoch 5665: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:49:07,560 EPOCH 5666
2024-02-10 23:49:23,582 Epoch 5666: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:49:23,582 EPOCH 5667
2024-02-10 23:49:34,040 [Epoch: 5667 Step: 00051000] Batch Recognition Loss:   0.000484 => Gls Tokens per Sec:      734 || Batch Translation Loss:   0.014601 => Txt Tokens per Sec:     1971 || Lr: 0.000050
2024-02-10 23:49:39,688 Epoch 5667: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:49:39,689 EPOCH 5668
2024-02-10 23:49:55,592 Epoch 5668: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:49:55,592 EPOCH 5669
2024-02-10 23:50:11,739 Epoch 5669: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 23:50:11,739 EPOCH 5670
2024-02-10 23:50:27,831 Epoch 5670: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 23:50:27,831 EPOCH 5671
2024-02-10 23:50:43,904 Epoch 5671: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 23:50:43,905 EPOCH 5672
2024-02-10 23:50:59,980 Epoch 5672: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-10 23:50:59,981 EPOCH 5673
2024-02-10 23:51:16,065 Epoch 5673: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 23:51:16,066 EPOCH 5674
2024-02-10 23:51:32,000 Epoch 5674: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:51:32,001 EPOCH 5675
2024-02-10 23:51:48,054 Epoch 5675: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:51:48,055 EPOCH 5676
2024-02-10 23:52:04,107 Epoch 5676: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:52:04,107 EPOCH 5677
2024-02-10 23:52:20,171 Epoch 5677: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:52:20,172 EPOCH 5678
2024-02-10 23:52:31,308 [Epoch: 5678 Step: 00051100] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:      805 || Batch Translation Loss:   0.011549 => Txt Tokens per Sec:     2223 || Lr: 0.000050
2024-02-10 23:52:36,165 Epoch 5678: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:52:36,166 EPOCH 5679
2024-02-10 23:52:52,332 Epoch 5679: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:52:52,333 EPOCH 5680
2024-02-10 23:53:08,323 Epoch 5680: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:53:08,323 EPOCH 5681
2024-02-10 23:53:24,524 Epoch 5681: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:53:24,524 EPOCH 5682
2024-02-10 23:53:40,298 Epoch 5682: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:53:40,298 EPOCH 5683
2024-02-10 23:53:56,337 Epoch 5683: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:53:56,338 EPOCH 5684
2024-02-10 23:54:12,502 Epoch 5684: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:54:12,503 EPOCH 5685
2024-02-10 23:54:28,433 Epoch 5685: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:54:28,434 EPOCH 5686
2024-02-10 23:54:44,279 Epoch 5686: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:54:44,280 EPOCH 5687
2024-02-10 23:55:00,592 Epoch 5687: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:55:00,592 EPOCH 5688
2024-02-10 23:55:16,595 Epoch 5688: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-10 23:55:16,596 EPOCH 5689
2024-02-10 23:55:28,554 [Epoch: 5689 Step: 00051200] Batch Recognition Loss:   0.000627 => Gls Tokens per Sec:      856 || Batch Translation Loss:   0.017549 => Txt Tokens per Sec:     2344 || Lr: 0.000050
2024-02-10 23:55:32,859 Epoch 5689: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:55:32,859 EPOCH 5690
2024-02-10 23:55:49,096 Epoch 5690: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:55:49,097 EPOCH 5691
2024-02-10 23:56:05,502 Epoch 5691: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 23:56:05,503 EPOCH 5692
2024-02-10 23:56:21,646 Epoch 5692: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 23:56:21,647 EPOCH 5693
2024-02-10 23:56:37,704 Epoch 5693: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-10 23:56:37,705 EPOCH 5694
2024-02-10 23:56:53,665 Epoch 5694: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-10 23:56:53,666 EPOCH 5695
2024-02-10 23:57:09,706 Epoch 5695: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 23:57:09,706 EPOCH 5696
2024-02-10 23:57:26,037 Epoch 5696: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 23:57:26,037 EPOCH 5697
2024-02-10 23:57:42,559 Epoch 5697: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:57:42,559 EPOCH 5698
2024-02-10 23:57:58,853 Epoch 5698: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:57:58,854 EPOCH 5699
2024-02-10 23:58:14,879 Epoch 5699: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-10 23:58:14,879 EPOCH 5700
2024-02-10 23:58:30,756 [Epoch: 5700 Step: 00051300] Batch Recognition Loss:   0.001603 => Gls Tokens per Sec:      669 || Batch Translation Loss:   0.011686 => Txt Tokens per Sec:     1851 || Lr: 0.000050
2024-02-10 23:58:30,757 Epoch 5700: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-10 23:58:30,757 EPOCH 5701
2024-02-10 23:58:47,025 Epoch 5701: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-10 23:58:47,026 EPOCH 5702
2024-02-10 23:59:03,219 Epoch 5702: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-10 23:59:03,220 EPOCH 5703
2024-02-10 23:59:18,672 Epoch 5703: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 23:59:18,673 EPOCH 5704
2024-02-10 23:59:35,067 Epoch 5704: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-10 23:59:35,067 EPOCH 5705
2024-02-10 23:59:50,991 Epoch 5705: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-10 23:59:50,992 EPOCH 5706
2024-02-11 00:00:06,932 Epoch 5706: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-11 00:00:06,933 EPOCH 5707
2024-02-11 00:00:23,092 Epoch 5707: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-11 00:00:23,093 EPOCH 5708
2024-02-11 00:00:39,248 Epoch 5708: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-11 00:00:39,249 EPOCH 5709
2024-02-11 00:00:55,390 Epoch 5709: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-11 00:00:55,391 EPOCH 5710
2024-02-11 00:01:11,385 Epoch 5710: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-11 00:01:11,386 EPOCH 5711
2024-02-11 00:01:27,484 Epoch 5711: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-11 00:01:27,485 EPOCH 5712
2024-02-11 00:01:32,021 [Epoch: 5712 Step: 00051400] Batch Recognition Loss:   0.000370 => Gls Tokens per Sec:       84 || Batch Translation Loss:   0.008985 => Txt Tokens per Sec:      298 || Lr: 0.000050
2024-02-11 00:01:43,802 Epoch 5712: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-11 00:01:43,803 EPOCH 5713
2024-02-11 00:02:00,088 Epoch 5713: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-11 00:02:00,089 EPOCH 5714
2024-02-11 00:02:16,032 Epoch 5714: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-11 00:02:16,032 EPOCH 5715
2024-02-11 00:02:31,815 Epoch 5715: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 00:02:31,816 EPOCH 5716
2024-02-11 00:02:47,918 Epoch 5716: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 00:02:47,919 EPOCH 5717
2024-02-11 00:03:04,018 Epoch 5717: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 00:03:04,018 EPOCH 5718
2024-02-11 00:03:20,134 Epoch 5718: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 00:03:20,134 EPOCH 5719
2024-02-11 00:03:36,316 Epoch 5719: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 00:03:36,317 EPOCH 5720
2024-02-11 00:03:52,209 Epoch 5720: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 00:03:52,209 EPOCH 5721
2024-02-11 00:04:08,607 Epoch 5721: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 00:04:08,607 EPOCH 5722
2024-02-11 00:04:24,238 Epoch 5722: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 00:04:24,239 EPOCH 5723
2024-02-11 00:04:31,720 [Epoch: 5723 Step: 00051500] Batch Recognition Loss:   0.000262 => Gls Tokens per Sec:      222 || Batch Translation Loss:   0.018039 => Txt Tokens per Sec:      727 || Lr: 0.000050
2024-02-11 00:04:40,516 Epoch 5723: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-11 00:04:40,516 EPOCH 5724
2024-02-11 00:04:56,626 Epoch 5724: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 00:04:56,626 EPOCH 5725
2024-02-11 00:05:12,692 Epoch 5725: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 00:05:12,692 EPOCH 5726
2024-02-11 00:05:28,779 Epoch 5726: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 00:05:28,779 EPOCH 5727
2024-02-11 00:05:44,708 Epoch 5727: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 00:05:44,708 EPOCH 5728
2024-02-11 00:06:00,752 Epoch 5728: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-11 00:06:00,752 EPOCH 5729
2024-02-11 00:06:16,385 Epoch 5729: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 00:06:16,385 EPOCH 5730
2024-02-11 00:06:32,639 Epoch 5730: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 00:06:32,639 EPOCH 5731
2024-02-11 00:06:48,659 Epoch 5731: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 00:06:48,659 EPOCH 5732
2024-02-11 00:07:04,830 Epoch 5732: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 00:07:04,831 EPOCH 5733
2024-02-11 00:07:20,756 Epoch 5733: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:07:20,757 EPOCH 5734
2024-02-11 00:07:31,368 [Epoch: 5734 Step: 00051600] Batch Recognition Loss:   0.000534 => Gls Tokens per Sec:      277 || Batch Translation Loss:   0.018890 => Txt Tokens per Sec:      871 || Lr: 0.000050
2024-02-11 00:07:36,875 Epoch 5734: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 00:07:36,875 EPOCH 5735
2024-02-11 00:07:52,769 Epoch 5735: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:07:52,770 EPOCH 5736
2024-02-11 00:08:09,191 Epoch 5736: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:08:09,192 EPOCH 5737
2024-02-11 00:08:25,324 Epoch 5737: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 00:08:25,325 EPOCH 5738
2024-02-11 00:08:41,373 Epoch 5738: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 00:08:41,373 EPOCH 5739
2024-02-11 00:08:57,203 Epoch 5739: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 00:08:57,204 EPOCH 5740
2024-02-11 00:09:13,409 Epoch 5740: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:09:13,409 EPOCH 5741
2024-02-11 00:09:29,447 Epoch 5741: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:09:29,448 EPOCH 5742
2024-02-11 00:09:45,544 Epoch 5742: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:09:45,544 EPOCH 5743
2024-02-11 00:10:01,608 Epoch 5743: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:10:01,608 EPOCH 5744
2024-02-11 00:10:17,811 Epoch 5744: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 00:10:17,812 EPOCH 5745
2024-02-11 00:10:25,909 [Epoch: 5745 Step: 00051700] Batch Recognition Loss:   0.000276 => Gls Tokens per Sec:      521 || Batch Translation Loss:   0.008828 => Txt Tokens per Sec:     1425 || Lr: 0.000050
2024-02-11 00:10:33,634 Epoch 5745: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-11 00:10:33,635 EPOCH 5746
2024-02-11 00:10:49,822 Epoch 5746: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 00:10:49,822 EPOCH 5747
2024-02-11 00:11:05,758 Epoch 5747: Total Training Recognition Loss 0.00  Total Training Translation Loss 2.43 
2024-02-11 00:11:05,759 EPOCH 5748
2024-02-11 00:11:21,787 Epoch 5748: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.52 
2024-02-11 00:11:21,788 EPOCH 5749
2024-02-11 00:11:37,965 Epoch 5749: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.01 
2024-02-11 00:11:37,966 EPOCH 5750
2024-02-11 00:11:53,929 Epoch 5750: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.55 
2024-02-11 00:11:53,930 EPOCH 5751
2024-02-11 00:12:10,166 Epoch 5751: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.41 
2024-02-11 00:12:10,167 EPOCH 5752
2024-02-11 00:12:26,258 Epoch 5752: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-11 00:12:26,258 EPOCH 5753
2024-02-11 00:12:42,101 Epoch 5753: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-11 00:12:42,102 EPOCH 5754
2024-02-11 00:12:57,969 Epoch 5754: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-11 00:12:57,970 EPOCH 5755
2024-02-11 00:13:14,450 Epoch 5755: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-11 00:13:14,451 EPOCH 5756
2024-02-11 00:13:28,810 [Epoch: 5756 Step: 00051800] Batch Recognition Loss:   0.000504 => Gls Tokens per Sec:      383 || Batch Translation Loss:   0.021036 => Txt Tokens per Sec:     1173 || Lr: 0.000050
2024-02-11 00:13:30,569 Epoch 5756: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-11 00:13:30,569 EPOCH 5757
2024-02-11 00:13:46,199 Epoch 5757: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 00:13:46,199 EPOCH 5758
2024-02-11 00:14:02,301 Epoch 5758: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 00:14:02,301 EPOCH 5759
2024-02-11 00:14:18,175 Epoch 5759: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 00:14:18,175 EPOCH 5760
2024-02-11 00:14:34,007 Epoch 5760: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 00:14:34,008 EPOCH 5761
2024-02-11 00:14:50,142 Epoch 5761: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 00:14:50,142 EPOCH 5762
2024-02-11 00:15:06,089 Epoch 5762: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 00:15:06,089 EPOCH 5763
2024-02-11 00:15:22,107 Epoch 5763: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 00:15:22,107 EPOCH 5764
2024-02-11 00:15:38,087 Epoch 5764: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 00:15:38,087 EPOCH 5765
2024-02-11 00:15:54,352 Epoch 5765: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 00:15:54,353 EPOCH 5766
2024-02-11 00:16:10,352 Epoch 5766: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:16:10,353 EPOCH 5767
2024-02-11 00:16:19,495 [Epoch: 5767 Step: 00051900] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:      742 || Batch Translation Loss:   0.008759 => Txt Tokens per Sec:     1944 || Lr: 0.000050
2024-02-11 00:16:26,402 Epoch 5767: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 00:16:26,402 EPOCH 5768
2024-02-11 00:16:42,680 Epoch 5768: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 00:16:42,680 EPOCH 5769
2024-02-11 00:16:58,460 Epoch 5769: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 00:16:58,461 EPOCH 5770
2024-02-11 00:17:14,620 Epoch 5770: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 00:17:14,620 EPOCH 5771
2024-02-11 00:17:30,768 Epoch 5771: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 00:17:30,768 EPOCH 5772
2024-02-11 00:17:46,769 Epoch 5772: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-11 00:17:46,769 EPOCH 5773
2024-02-11 00:18:02,786 Epoch 5773: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 00:18:02,786 EPOCH 5774
2024-02-11 00:18:18,744 Epoch 5774: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 00:18:18,745 EPOCH 5775
2024-02-11 00:18:34,949 Epoch 5775: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 00:18:34,950 EPOCH 5776
2024-02-11 00:18:50,971 Epoch 5776: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:18:50,972 EPOCH 5777
2024-02-11 00:19:07,156 Epoch 5777: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:19:07,156 EPOCH 5778
2024-02-11 00:19:22,276 [Epoch: 5778 Step: 00052000] Batch Recognition Loss:   0.000422 => Gls Tokens per Sec:      533 || Batch Translation Loss:   0.017792 => Txt Tokens per Sec:     1538 || Lr: 0.000050
2024-02-11 00:20:34,109 Validation result at epoch 5778, step    52000: duration: 71.8335s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.25643	Translation Loss: 105872.86719	PPL: 39128.10938
	Eval Metric: BLEU
	WER 2.12	(DEL: 0.00,	INS: 0.00,	SUB: 2.12)
	BLEU-4 0.33	(BLEU-1: 10.07,	BLEU-2: 2.83,	BLEU-3: 0.93,	BLEU-4: 0.33)
	CHRF 16.40	ROUGE 8.54
2024-02-11 00:20:34,111 Logging Recognition and Translation Outputs
2024-02-11 00:20:34,111 ========================================================================================================================
2024-02-11 00:20:34,111 Logging Sequence: 59_101.00
2024-02-11 00:20:34,112 	Gloss Reference :	A B+C+D+E
2024-02-11 00:20:34,112 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 00:20:34,112 	Gloss Alignment :	         
2024-02-11 00:20:34,112 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 00:20:34,113 	Text Reference  :	did you see the video fox said she won her medals    because    of a       condom  and   is     very happy
2024-02-11 00:20:34,114 	Text Hypothesis :	*** *** *** *** ***** *** **** *** *** a   wrestling federation of india's skipper rohit sharma 14   overs
2024-02-11 00:20:34,114 	Text Alignment  :	D   D   D   D   D     D   D    D   D   S   S         S             S       S       S     S      S    S    
2024-02-11 00:20:34,114 ========================================================================================================================
2024-02-11 00:20:34,114 Logging Sequence: 103_112.00
2024-02-11 00:20:34,114 	Gloss Reference :	A B+C+D+E
2024-02-11 00:20:34,115 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 00:20:34,115 	Gloss Alignment :	         
2024-02-11 00:20:34,115 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 00:20:34,116 	Text Reference  :	you  are aware  that earlier the      britishers had  colonized a   lot   of countries in the world
2024-02-11 00:20:34,116 	Text Hypothesis :	only 50  guests were called  arshdeep singh      from around    the video of ********* ** *** *****
2024-02-11 00:20:34,116 	Text Alignment  :	S    S   S      S    S       S        S          S    S         S   S        D         D  D   D    
2024-02-11 00:20:34,116 ========================================================================================================================
2024-02-11 00:20:34,117 Logging Sequence: 143_11.00
2024-02-11 00:20:34,117 	Gloss Reference :	A B+C+D+E
2024-02-11 00:20:34,117 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 00:20:34,117 	Gloss Alignment :	         
2024-02-11 00:20:34,117 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 00:20:34,119 	Text Reference  :	ronaldo has also become the first person to have 500 million followers on    instagram he is   the most loved footballer
2024-02-11 00:20:34,119 	Text Hypothesis :	******* *** **** ****** *** ***** ****** ** **** *** in      ipl       there is        a  such a   huge fan   following 
2024-02-11 00:20:34,119 	Text Alignment  :	D       D   D    D      D   D     D      D  D    D   S       S         S     S         S  S    S   S    S     S         
2024-02-11 00:20:34,119 ========================================================================================================================
2024-02-11 00:20:34,119 Logging Sequence: 183_23.00
2024-02-11 00:20:34,119 	Gloss Reference :	A B+C+D+E
2024-02-11 00:20:34,120 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 00:20:34,120 	Gloss Alignment :	         
2024-02-11 00:20:34,120 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 00:20:34,121 	Text Reference  :	however everybody has          been waiting for them   to announce the *********** ******* ***** *** name of        the child
2024-02-11 00:20:34,121 	Text Hypothesis :	singh   also      participated in   the     all thanks to ******** the partnership between india has bcci secretary jay shah 
2024-02-11 00:20:34,122 	Text Alignment  :	S       S         S            S    S       S   S         D            I           I       I     I   S    S         S   S    
2024-02-11 00:20:34,122 ========================================================================================================================
2024-02-11 00:20:34,122 Logging Sequence: 169_165.00
2024-02-11 00:20:34,122 	Gloss Reference :	A B+C+D+E
2024-02-11 00:20:34,122 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 00:20:34,122 	Gloss Alignment :	         
2024-02-11 00:20:34,123 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 00:20:34,124 	Text Reference  :	** the indian government was   outraged  by the  incident and     these changes were      undone by     wikipedia  
2024-02-11 00:20:34,124 	Text Hypothesis :	is the ****** ********** first cricketer in such a        violent brawl that    cricketer in     danger harrassment
2024-02-11 00:20:34,124 	Text Alignment  :	I      D      D          S     S         S  S    S        S       S     S       S         S      S      S          
2024-02-11 00:20:34,124 ========================================================================================================================
2024-02-11 00:20:35,123 Epoch 5778: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:20:35,123 EPOCH 5779
2024-02-11 00:20:51,990 Epoch 5779: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:20:51,990 EPOCH 5780
2024-02-11 00:21:08,259 Epoch 5780: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:21:08,259 EPOCH 5781
2024-02-11 00:21:24,133 Epoch 5781: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:21:24,134 EPOCH 5782
2024-02-11 00:21:40,245 Epoch 5782: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 00:21:40,245 EPOCH 5783
2024-02-11 00:21:56,675 Epoch 5783: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 00:21:56,675 EPOCH 5784
2024-02-11 00:22:12,879 Epoch 5784: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:22:12,879 EPOCH 5785
2024-02-11 00:22:29,120 Epoch 5785: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:22:29,120 EPOCH 5786
2024-02-11 00:22:45,005 Epoch 5786: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:22:45,006 EPOCH 5787
2024-02-11 00:23:01,811 Epoch 5787: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:23:01,812 EPOCH 5788
2024-02-11 00:23:18,173 Epoch 5788: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:23:18,174 EPOCH 5789
2024-02-11 00:23:28,579 [Epoch: 5789 Step: 00052100] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:      898 || Batch Translation Loss:   0.010810 => Txt Tokens per Sec:     2398 || Lr: 0.000050
2024-02-11 00:23:34,298 Epoch 5789: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:23:34,298 EPOCH 5790
2024-02-11 00:23:50,705 Epoch 5790: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-11 00:23:50,705 EPOCH 5791
2024-02-11 00:24:07,178 Epoch 5791: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:24:07,178 EPOCH 5792
2024-02-11 00:24:23,253 Epoch 5792: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:24:23,254 EPOCH 5793
2024-02-11 00:24:39,338 Epoch 5793: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:24:39,339 EPOCH 5794
2024-02-11 00:24:55,340 Epoch 5794: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:24:55,341 EPOCH 5795
2024-02-11 00:25:11,676 Epoch 5795: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:25:11,677 EPOCH 5796
2024-02-11 00:25:27,952 Epoch 5796: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:25:27,953 EPOCH 5797
2024-02-11 00:25:44,221 Epoch 5797: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:25:44,222 EPOCH 5798
2024-02-11 00:26:00,598 Epoch 5798: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:26:00,599 EPOCH 5799
2024-02-11 00:26:16,796 Epoch 5799: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:26:16,797 EPOCH 5800
2024-02-11 00:26:32,820 [Epoch: 5800 Step: 00052200] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:      663 || Batch Translation Loss:   0.021105 => Txt Tokens per Sec:     1834 || Lr: 0.000050
2024-02-11 00:26:32,820 Epoch 5800: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:26:32,821 EPOCH 5801
2024-02-11 00:26:48,866 Epoch 5801: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:26:48,867 EPOCH 5802
2024-02-11 00:27:04,898 Epoch 5802: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:27:04,899 EPOCH 5803
2024-02-11 00:27:21,209 Epoch 5803: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:27:21,210 EPOCH 5804
2024-02-11 00:27:37,230 Epoch 5804: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:27:37,231 EPOCH 5805
2024-02-11 00:27:53,047 Epoch 5805: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 00:27:53,047 EPOCH 5806
2024-02-11 00:28:09,431 Epoch 5806: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:28:09,432 EPOCH 5807
2024-02-11 00:28:25,627 Epoch 5807: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 00:28:25,628 EPOCH 5808
2024-02-11 00:28:41,777 Epoch 5808: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:28:41,778 EPOCH 5809
2024-02-11 00:28:58,080 Epoch 5809: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:28:58,081 EPOCH 5810
2024-02-11 00:29:13,353 Epoch 5810: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:29:13,354 EPOCH 5811
2024-02-11 00:29:29,773 Epoch 5811: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:29:29,773 EPOCH 5812
2024-02-11 00:29:35,405 [Epoch: 5812 Step: 00052300] Batch Recognition Loss:   0.000399 => Gls Tokens per Sec:      227 || Batch Translation Loss:   0.015817 => Txt Tokens per Sec:      782 || Lr: 0.000050
2024-02-11 00:29:45,591 Epoch 5812: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:29:45,591 EPOCH 5813
2024-02-11 00:30:01,720 Epoch 5813: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:30:01,720 EPOCH 5814
2024-02-11 00:30:17,864 Epoch 5814: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:30:17,864 EPOCH 5815
2024-02-11 00:30:33,835 Epoch 5815: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:30:33,836 EPOCH 5816
2024-02-11 00:30:49,681 Epoch 5816: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:30:49,682 EPOCH 5817
2024-02-11 00:31:05,877 Epoch 5817: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:31:05,878 EPOCH 5818
2024-02-11 00:31:22,102 Epoch 5818: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:31:22,102 EPOCH 5819
2024-02-11 00:31:38,088 Epoch 5819: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:31:38,088 EPOCH 5820
2024-02-11 00:31:54,284 Epoch 5820: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:31:54,285 EPOCH 5821
2024-02-11 00:32:10,492 Epoch 5821: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:32:10,493 EPOCH 5822
2024-02-11 00:32:26,479 Epoch 5822: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 00:32:26,479 EPOCH 5823
2024-02-11 00:32:27,483 [Epoch: 5823 Step: 00052400] Batch Recognition Loss:   0.000193 => Gls Tokens per Sec:     2555 || Batch Translation Loss:   0.013332 => Txt Tokens per Sec:     6701 || Lr: 0.000050
2024-02-11 00:32:42,496 Epoch 5823: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-11 00:32:42,496 EPOCH 5824
2024-02-11 00:32:58,580 Epoch 5824: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.33 
2024-02-11 00:32:58,581 EPOCH 5825
2024-02-11 00:33:14,507 Epoch 5825: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-11 00:33:14,508 EPOCH 5826
2024-02-11 00:33:30,904 Epoch 5826: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 00:33:30,904 EPOCH 5827
2024-02-11 00:33:47,672 Epoch 5827: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 00:33:47,673 EPOCH 5828
2024-02-11 00:34:03,854 Epoch 5828: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 00:34:03,854 EPOCH 5829
2024-02-11 00:34:19,901 Epoch 5829: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 00:34:19,901 EPOCH 5830
2024-02-11 00:34:36,012 Epoch 5830: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 00:34:36,013 EPOCH 5831
2024-02-11 00:34:51,999 Epoch 5831: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 00:34:52,000 EPOCH 5832
2024-02-11 00:35:07,999 Epoch 5832: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-11 00:35:07,999 EPOCH 5833
2024-02-11 00:35:24,132 Epoch 5833: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:35:24,132 EPOCH 5834
2024-02-11 00:35:29,662 [Epoch: 5834 Step: 00052500] Batch Recognition Loss:   0.000240 => Gls Tokens per Sec:      532 || Batch Translation Loss:   0.016294 => Txt Tokens per Sec:     1540 || Lr: 0.000050
2024-02-11 00:35:40,508 Epoch 5834: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:35:40,508 EPOCH 5835
2024-02-11 00:35:56,658 Epoch 5835: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:35:56,659 EPOCH 5836
2024-02-11 00:36:12,793 Epoch 5836: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:36:12,794 EPOCH 5837
2024-02-11 00:36:28,871 Epoch 5837: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:36:28,871 EPOCH 5838
2024-02-11 00:36:44,970 Epoch 5838: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:36:44,971 EPOCH 5839
2024-02-11 00:37:00,842 Epoch 5839: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:37:00,842 EPOCH 5840
2024-02-11 00:37:16,804 Epoch 5840: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:37:16,805 EPOCH 5841
2024-02-11 00:37:32,792 Epoch 5841: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:37:32,792 EPOCH 5842
2024-02-11 00:37:48,955 Epoch 5842: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:37:48,956 EPOCH 5843
2024-02-11 00:38:05,139 Epoch 5843: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:38:05,139 EPOCH 5844
2024-02-11 00:38:21,441 Epoch 5844: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:38:21,442 EPOCH 5845
2024-02-11 00:38:28,290 [Epoch: 5845 Step: 00052600] Batch Recognition Loss:   0.000164 => Gls Tokens per Sec:      748 || Batch Translation Loss:   0.016980 => Txt Tokens per Sec:     1943 || Lr: 0.000050
2024-02-11 00:38:37,412 Epoch 5845: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:38:37,412 EPOCH 5846
2024-02-11 00:38:53,515 Epoch 5846: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:38:53,516 EPOCH 5847
2024-02-11 00:39:09,837 Epoch 5847: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:39:09,838 EPOCH 5848
2024-02-11 00:39:26,188 Epoch 5848: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:39:26,189 EPOCH 5849
2024-02-11 00:39:42,055 Epoch 5849: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:39:42,056 EPOCH 5850
2024-02-11 00:39:58,214 Epoch 5850: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:39:58,215 EPOCH 5851
2024-02-11 00:40:13,886 Epoch 5851: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:40:13,887 EPOCH 5852
2024-02-11 00:40:30,365 Epoch 5852: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:40:30,365 EPOCH 5853
2024-02-11 00:40:46,341 Epoch 5853: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:40:46,341 EPOCH 5854
2024-02-11 00:41:02,429 Epoch 5854: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:41:02,430 EPOCH 5855
2024-02-11 00:41:18,565 Epoch 5855: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:41:18,566 EPOCH 5856
2024-02-11 00:41:23,358 [Epoch: 5856 Step: 00052700] Batch Recognition Loss:   0.000207 => Gls Tokens per Sec:     1336 || Batch Translation Loss:   0.014776 => Txt Tokens per Sec:     3561 || Lr: 0.000050
2024-02-11 00:41:34,384 Epoch 5856: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:41:34,385 EPOCH 5857
2024-02-11 00:41:50,780 Epoch 5857: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:41:50,781 EPOCH 5858
2024-02-11 00:42:07,198 Epoch 5858: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:42:07,199 EPOCH 5859
2024-02-11 00:42:23,095 Epoch 5859: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:42:23,096 EPOCH 5860
2024-02-11 00:42:39,113 Epoch 5860: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:42:39,113 EPOCH 5861
2024-02-11 00:42:55,584 Epoch 5861: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:42:55,585 EPOCH 5862
2024-02-11 00:43:11,408 Epoch 5862: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:43:11,409 EPOCH 5863
2024-02-11 00:43:27,493 Epoch 5863: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:43:27,494 EPOCH 5864
2024-02-11 00:43:43,634 Epoch 5864: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:43:43,634 EPOCH 5865
2024-02-11 00:43:59,483 Epoch 5865: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:43:59,483 EPOCH 5866
2024-02-11 00:44:15,788 Epoch 5866: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:44:15,788 EPOCH 5867
2024-02-11 00:44:25,289 [Epoch: 5867 Step: 00052800] Batch Recognition Loss:   0.000432 => Gls Tokens per Sec:      714 || Batch Translation Loss:   0.014826 => Txt Tokens per Sec:     2002 || Lr: 0.000050
2024-02-11 00:44:32,139 Epoch 5867: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:44:32,140 EPOCH 5868
2024-02-11 00:44:48,251 Epoch 5868: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:44:48,252 EPOCH 5869
2024-02-11 00:45:03,834 Epoch 5869: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 00:45:03,835 EPOCH 5870
2024-02-11 00:45:20,105 Epoch 5870: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 00:45:20,106 EPOCH 5871
2024-02-11 00:45:36,267 Epoch 5871: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 00:45:36,267 EPOCH 5872
2024-02-11 00:45:52,371 Epoch 5872: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:45:52,372 EPOCH 5873
2024-02-11 00:46:08,349 Epoch 5873: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 00:46:08,350 EPOCH 5874
2024-02-11 00:46:24,368 Epoch 5874: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:46:24,368 EPOCH 5875
2024-02-11 00:46:40,539 Epoch 5875: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 00:46:40,540 EPOCH 5876
2024-02-11 00:46:56,547 Epoch 5876: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:46:56,548 EPOCH 5877
2024-02-11 00:47:12,684 Epoch 5877: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:47:12,684 EPOCH 5878
2024-02-11 00:47:24,179 [Epoch: 5878 Step: 00052900] Batch Recognition Loss:   0.000098 => Gls Tokens per Sec:      779 || Batch Translation Loss:   0.010142 => Txt Tokens per Sec:     2189 || Lr: 0.000050
2024-02-11 00:47:28,877 Epoch 5878: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:47:28,878 EPOCH 5879
2024-02-11 00:47:44,894 Epoch 5879: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:47:44,895 EPOCH 5880
2024-02-11 00:48:01,167 Epoch 5880: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:48:01,168 EPOCH 5881
2024-02-11 00:48:16,972 Epoch 5881: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:48:16,972 EPOCH 5882
2024-02-11 00:48:33,096 Epoch 5882: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:48:33,097 EPOCH 5883
2024-02-11 00:48:48,954 Epoch 5883: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:48:48,954 EPOCH 5884
2024-02-11 00:49:04,960 Epoch 5884: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:49:04,961 EPOCH 5885
2024-02-11 00:49:21,050 Epoch 5885: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 00:49:21,051 EPOCH 5886
2024-02-11 00:49:37,317 Epoch 5886: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-11 00:49:37,317 EPOCH 5887
2024-02-11 00:49:53,647 Epoch 5887: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-11 00:49:53,648 EPOCH 5888
2024-02-11 00:50:09,604 Epoch 5888: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.14 
2024-02-11 00:50:09,605 EPOCH 5889
2024-02-11 00:50:25,077 [Epoch: 5889 Step: 00053000] Batch Recognition Loss:   0.001022 => Gls Tokens per Sec:      604 || Batch Translation Loss:   0.010916 => Txt Tokens per Sec:     1651 || Lr: 0.000050
2024-02-11 00:50:25,782 Epoch 5889: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 00:50:25,783 EPOCH 5890
2024-02-11 00:50:41,870 Epoch 5890: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.14 
2024-02-11 00:50:41,871 EPOCH 5891
2024-02-11 00:50:58,042 Epoch 5891: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-11 00:50:58,043 EPOCH 5892
2024-02-11 00:51:14,205 Epoch 5892: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-11 00:51:14,206 EPOCH 5893
2024-02-11 00:51:29,907 Epoch 5893: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-11 00:51:29,908 EPOCH 5894
2024-02-11 00:51:46,218 Epoch 5894: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 00:51:46,218 EPOCH 5895
2024-02-11 00:52:02,297 Epoch 5895: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 00:52:02,297 EPOCH 5896
2024-02-11 00:52:18,286 Epoch 5896: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-11 00:52:18,286 EPOCH 5897
2024-02-11 00:52:34,301 Epoch 5897: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:52:34,302 EPOCH 5898
2024-02-11 00:52:50,453 Epoch 5898: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 00:52:50,453 EPOCH 5899
2024-02-11 00:53:06,447 Epoch 5899: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 00:53:06,448 EPOCH 5900
2024-02-11 00:53:22,586 [Epoch: 5900 Step: 00053100] Batch Recognition Loss:   0.000316 => Gls Tokens per Sec:      658 || Batch Translation Loss:   0.014919 => Txt Tokens per Sec:     1821 || Lr: 0.000050
2024-02-11 00:53:22,587 Epoch 5900: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:53:22,587 EPOCH 5901
2024-02-11 00:53:38,868 Epoch 5901: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 00:53:38,869 EPOCH 5902
2024-02-11 00:53:55,015 Epoch 5902: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 00:53:55,016 EPOCH 5903
2024-02-11 00:54:11,003 Epoch 5903: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 00:54:11,003 EPOCH 5904
2024-02-11 00:54:26,932 Epoch 5904: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 00:54:26,932 EPOCH 5905
2024-02-11 00:54:42,891 Epoch 5905: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 00:54:42,892 EPOCH 5906
2024-02-11 00:54:59,189 Epoch 5906: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 00:54:59,190 EPOCH 5907
2024-02-11 00:55:15,427 Epoch 5907: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-11 00:55:15,428 EPOCH 5908
2024-02-11 00:55:31,342 Epoch 5908: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 00:55:31,343 EPOCH 5909
2024-02-11 00:55:47,686 Epoch 5909: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-11 00:55:47,687 EPOCH 5910
2024-02-11 00:56:03,554 Epoch 5910: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 00:56:03,554 EPOCH 5911
2024-02-11 00:56:19,579 Epoch 5911: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-11 00:56:19,579 EPOCH 5912
2024-02-11 00:56:23,989 [Epoch: 5912 Step: 00053200] Batch Recognition Loss:   0.000666 => Gls Tokens per Sec:       86 || Batch Translation Loss:   0.008616 => Txt Tokens per Sec:      308 || Lr: 0.000050
2024-02-11 00:56:35,935 Epoch 5912: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-11 00:56:35,936 EPOCH 5913
2024-02-11 00:56:52,376 Epoch 5913: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-11 00:56:52,377 EPOCH 5914
2024-02-11 00:57:08,320 Epoch 5914: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-11 00:57:08,320 EPOCH 5915
2024-02-11 00:57:24,353 Epoch 5915: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-11 00:57:24,353 EPOCH 5916
2024-02-11 00:57:40,430 Epoch 5916: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 00:57:40,431 EPOCH 5917
2024-02-11 00:57:56,602 Epoch 5917: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 00:57:56,603 EPOCH 5918
2024-02-11 00:58:12,602 Epoch 5918: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 00:58:12,603 EPOCH 5919
2024-02-11 00:58:28,719 Epoch 5919: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 00:58:28,720 EPOCH 5920
2024-02-11 00:58:44,940 Epoch 5920: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 00:58:44,940 EPOCH 5921
2024-02-11 00:59:00,922 Epoch 5921: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 00:59:00,923 EPOCH 5922
2024-02-11 00:59:16,814 Epoch 5922: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 00:59:16,815 EPOCH 5923
2024-02-11 00:59:17,472 [Epoch: 5923 Step: 00053300] Batch Recognition Loss:   0.000170 => Gls Tokens per Sec:     3902 || Batch Translation Loss:   0.016696 => Txt Tokens per Sec:     9742 || Lr: 0.000050
2024-02-11 00:59:32,483 Epoch 5923: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 00:59:32,483 EPOCH 5924
2024-02-11 00:59:48,209 Epoch 5924: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-11 00:59:48,209 EPOCH 5925
2024-02-11 01:00:04,054 Epoch 5925: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.40 
2024-02-11 01:00:04,055 EPOCH 5926
2024-02-11 01:00:20,443 Epoch 5926: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 01:00:20,444 EPOCH 5927
2024-02-11 01:00:36,411 Epoch 5927: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 01:00:36,412 EPOCH 5928
2024-02-11 01:00:52,462 Epoch 5928: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 01:00:52,462 EPOCH 5929
2024-02-11 01:01:08,675 Epoch 5929: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-11 01:01:08,675 EPOCH 5930
2024-02-11 01:01:24,643 Epoch 5930: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 01:01:24,644 EPOCH 5931
2024-02-11 01:01:40,841 Epoch 5931: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 01:01:40,842 EPOCH 5932
2024-02-11 01:01:57,084 Epoch 5932: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 01:01:57,085 EPOCH 5933
2024-02-11 01:02:13,262 Epoch 5933: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 01:02:13,263 EPOCH 5934
2024-02-11 01:02:17,597 [Epoch: 5934 Step: 00053400] Batch Recognition Loss:   0.000486 => Gls Tokens per Sec:      886 || Batch Translation Loss:   0.013567 => Txt Tokens per Sec:     2618 || Lr: 0.000050
2024-02-11 01:02:29,400 Epoch 5934: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 01:02:29,400 EPOCH 5935
2024-02-11 01:02:45,257 Epoch 5935: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 01:02:45,257 EPOCH 5936
2024-02-11 01:03:01,132 Epoch 5936: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 01:03:01,133 EPOCH 5937
2024-02-11 01:03:17,229 Epoch 5937: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 01:03:17,229 EPOCH 5938
2024-02-11 01:03:33,204 Epoch 5938: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 01:03:33,205 EPOCH 5939
2024-02-11 01:03:49,381 Epoch 5939: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 01:03:49,381 EPOCH 5940
2024-02-11 01:04:05,082 Epoch 5940: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 01:04:05,082 EPOCH 5941
2024-02-11 01:04:21,168 Epoch 5941: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 01:04:21,169 EPOCH 5942
2024-02-11 01:04:37,588 Epoch 5942: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 01:04:37,589 EPOCH 5943
2024-02-11 01:04:53,615 Epoch 5943: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 01:04:53,615 EPOCH 5944
2024-02-11 01:05:09,715 Epoch 5944: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 01:05:09,716 EPOCH 5945
2024-02-11 01:05:11,008 [Epoch: 5945 Step: 00053500] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:     3966 || Batch Translation Loss:   0.011258 => Txt Tokens per Sec:     9248 || Lr: 0.000050
2024-02-11 01:05:25,713 Epoch 5945: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 01:05:25,713 EPOCH 5946
2024-02-11 01:05:41,731 Epoch 5946: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 01:05:41,732 EPOCH 5947
2024-02-11 01:05:57,784 Epoch 5947: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 01:05:57,785 EPOCH 5948
2024-02-11 01:06:14,158 Epoch 5948: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 01:06:14,158 EPOCH 5949
2024-02-11 01:06:30,077 Epoch 5949: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 01:06:30,078 EPOCH 5950
2024-02-11 01:06:45,987 Epoch 5950: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 01:06:45,988 EPOCH 5951
2024-02-11 01:07:02,053 Epoch 5951: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 01:07:02,053 EPOCH 5952
2024-02-11 01:07:18,510 Epoch 5952: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 01:07:18,511 EPOCH 5953
2024-02-11 01:07:34,746 Epoch 5953: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 01:07:34,746 EPOCH 5954
2024-02-11 01:07:50,871 Epoch 5954: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 01:07:50,871 EPOCH 5955
2024-02-11 01:08:07,350 Epoch 5955: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-11 01:08:07,351 EPOCH 5956
2024-02-11 01:08:18,254 [Epoch: 5956 Step: 00053600] Batch Recognition Loss:   0.000244 => Gls Tokens per Sec:      587 || Batch Translation Loss:   0.015029 => Txt Tokens per Sec:     1716 || Lr: 0.000050
2024-02-11 01:08:24,030 Epoch 5956: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 01:08:24,030 EPOCH 5957
2024-02-11 01:08:40,089 Epoch 5957: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 01:08:40,090 EPOCH 5958
2024-02-11 01:08:56,546 Epoch 5958: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 01:08:56,546 EPOCH 5959
2024-02-11 01:09:12,796 Epoch 5959: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 01:09:12,797 EPOCH 5960
2024-02-11 01:09:28,781 Epoch 5960: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 01:09:28,782 EPOCH 5961
2024-02-11 01:09:45,396 Epoch 5961: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 01:09:45,396 EPOCH 5962
2024-02-11 01:10:01,374 Epoch 5962: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 01:10:01,374 EPOCH 5963
2024-02-11 01:10:17,299 Epoch 5963: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 01:10:17,300 EPOCH 5964
2024-02-11 01:10:33,545 Epoch 5964: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 01:10:33,546 EPOCH 5965
2024-02-11 01:10:49,820 Epoch 5965: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 01:10:49,821 EPOCH 5966
2024-02-11 01:11:05,863 Epoch 5966: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 01:11:05,863 EPOCH 5967
2024-02-11 01:11:15,319 [Epoch: 5967 Step: 00053700] Batch Recognition Loss:   0.000281 => Gls Tokens per Sec:      717 || Batch Translation Loss:   0.033849 => Txt Tokens per Sec:     1909 || Lr: 0.000050
2024-02-11 01:11:21,983 Epoch 5967: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 01:11:21,983 EPOCH 5968
2024-02-11 01:11:38,047 Epoch 5968: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 01:11:38,048 EPOCH 5969
2024-02-11 01:11:54,021 Epoch 5969: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 01:11:54,021 EPOCH 5970
2024-02-11 01:12:09,859 Epoch 5970: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 01:12:09,859 EPOCH 5971
2024-02-11 01:12:25,978 Epoch 5971: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 01:12:25,978 EPOCH 5972
2024-02-11 01:12:42,291 Epoch 5972: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 01:12:42,292 EPOCH 5973
2024-02-11 01:12:58,488 Epoch 5973: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 01:12:58,489 EPOCH 5974
2024-02-11 01:13:14,851 Epoch 5974: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 01:13:14,851 EPOCH 5975
2024-02-11 01:13:31,057 Epoch 5975: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 01:13:31,058 EPOCH 5976
2024-02-11 01:13:47,278 Epoch 5976: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 01:13:47,278 EPOCH 5977
2024-02-11 01:14:03,342 Epoch 5977: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 01:14:03,343 EPOCH 5978
2024-02-11 01:14:13,042 [Epoch: 5978 Step: 00053800] Batch Recognition Loss:   0.000445 => Gls Tokens per Sec:      831 || Batch Translation Loss:   0.019672 => Txt Tokens per Sec:     2190 || Lr: 0.000050
2024-02-11 01:14:19,532 Epoch 5978: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 01:14:19,533 EPOCH 5979
2024-02-11 01:14:35,505 Epoch 5979: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 01:14:35,506 EPOCH 5980
2024-02-11 01:14:51,906 Epoch 5980: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 01:14:51,907 EPOCH 5981
2024-02-11 01:15:07,877 Epoch 5981: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-11 01:15:07,878 EPOCH 5982
2024-02-11 01:15:24,187 Epoch 5982: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-11 01:15:24,188 EPOCH 5983
2024-02-11 01:15:40,298 Epoch 5983: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 01:15:40,299 EPOCH 5984
2024-02-11 01:15:56,362 Epoch 5984: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 01:15:56,362 EPOCH 5985
2024-02-11 01:16:12,545 Epoch 5985: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 01:16:12,545 EPOCH 5986
2024-02-11 01:16:28,650 Epoch 5986: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-11 01:16:28,651 EPOCH 5987
2024-02-11 01:16:45,029 Epoch 5987: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.54 
2024-02-11 01:16:45,030 EPOCH 5988
2024-02-11 01:17:01,043 Epoch 5988: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-11 01:17:01,044 EPOCH 5989
2024-02-11 01:17:16,385 [Epoch: 5989 Step: 00053900] Batch Recognition Loss:   0.001124 => Gls Tokens per Sec:      609 || Batch Translation Loss:   0.040754 => Txt Tokens per Sec:     1667 || Lr: 0.000050
2024-02-11 01:17:17,104 Epoch 5989: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-11 01:17:17,104 EPOCH 5990
2024-02-11 01:17:33,136 Epoch 5990: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-11 01:17:33,137 EPOCH 5991
2024-02-11 01:17:49,309 Epoch 5991: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-11 01:17:49,310 EPOCH 5992
2024-02-11 01:18:05,648 Epoch 5992: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-11 01:18:05,649 EPOCH 5993
2024-02-11 01:18:21,967 Epoch 5993: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-11 01:18:21,967 EPOCH 5994
2024-02-11 01:18:38,004 Epoch 5994: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-11 01:18:38,005 EPOCH 5995
2024-02-11 01:18:53,912 Epoch 5995: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-11 01:18:53,913 EPOCH 5996
2024-02-11 01:19:09,735 Epoch 5996: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 01:19:09,735 EPOCH 5997
2024-02-11 01:19:25,769 Epoch 5997: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-11 01:19:25,769 EPOCH 5998
2024-02-11 01:19:41,910 Epoch 5998: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 01:19:41,910 EPOCH 5999
2024-02-11 01:19:58,126 Epoch 5999: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 01:19:58,127 EPOCH 6000
2024-02-11 01:20:14,136 [Epoch: 6000 Step: 00054000] Batch Recognition Loss:   0.001143 => Gls Tokens per Sec:      663 || Batch Translation Loss:   0.019885 => Txt Tokens per Sec:     1835 || Lr: 0.000050
2024-02-11 01:21:26,238 Validation result at epoch 6000, step    54000: duration: 72.1011s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.22280	Translation Loss: 107011.29688	PPL: 43840.04688
	Eval Metric: BLEU
	WER 1.98	(DEL: 0.00,	INS: 0.00,	SUB: 1.98)
	BLEU-4 0.29	(BLEU-1: 9.58,	BLEU-2: 2.50,	BLEU-3: 0.78,	BLEU-4: 0.29)
	CHRF 16.50	ROUGE 7.96
2024-02-11 01:21:26,240 Logging Recognition and Translation Outputs
2024-02-11 01:21:26,240 ========================================================================================================================
2024-02-11 01:21:26,240 Logging Sequence: 166_243.00
2024-02-11 01:21:26,240 	Gloss Reference :	A B+C+D+E
2024-02-11 01:21:26,240 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 01:21:26,241 	Gloss Alignment :	         
2024-02-11 01:21:26,241 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 01:21:26,242 	Text Reference  :	*** ***** icc       worked with  members boards  like bcci   pcb  cricket australia etc 
2024-02-11 01:21:26,242 	Text Hypothesis :	the board organised the    world cup     matches in   mumbai navi mumbai  and       pune
2024-02-11 01:21:26,242 	Text Alignment  :	I   I     S         S      S     S       S       S    S      S    S       S         S   
2024-02-11 01:21:26,243 ========================================================================================================================
2024-02-11 01:21:26,243 Logging Sequence: 179_409.00
2024-02-11 01:21:26,243 	Gloss Reference :	A B+C+D+E
2024-02-11 01:21:26,243 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 01:21:26,243 	Gloss Alignment :	         
2024-02-11 01:21:26,244 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 01:21:26,245 	Text Reference  :	*********** the passport was at the ****** **** wfi   office  in delhi
2024-02-11 01:21:26,245 	Text Hypothesis :	afghanistan and then     or  up the finals this would against 23 years
2024-02-11 01:21:26,245 	Text Alignment  :	I           S   S        S   S      I      I    S     S       S  S    
2024-02-11 01:21:26,245 ========================================================================================================================
2024-02-11 01:21:26,245 Logging Sequence: 81_407.00
2024-02-11 01:21:26,246 	Gloss Reference :	A B+C+D+E
2024-02-11 01:21:26,246 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 01:21:26,246 	Gloss Alignment :	         
2024-02-11 01:21:26,246 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 01:21:26,248 	Text Reference  :	******* the ***** ******* government company   -  national buildings construction corporation and they  will complete them in    a   time-bound manner
2024-02-11 01:21:26,248 	Text Hypothesis :	however the start getting deducted   depending on the      fine      of           rs          150 crore so   we       to   dhoni for rs         250   
2024-02-11 01:21:26,248 	Text Alignment  :	I           I     I       S          S         S  S        S         S            S           S   S     S    S        S    S     S   S          S     
2024-02-11 01:21:26,249 ========================================================================================================================
2024-02-11 01:21:26,249 Logging Sequence: 96_31.00
2024-02-11 01:21:26,249 	Gloss Reference :	A B+C+D+E
2024-02-11 01:21:26,249 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 01:21:26,249 	Gloss Alignment :	         
2024-02-11 01:21:26,249 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 01:21:26,250 	Text Reference  :	and then 2 teams will go on to    play the  final  
2024-02-11 01:21:26,250 	Text Hypothesis :	*** **** * ***** **** ** ** kohli also felt relaxed
2024-02-11 01:21:26,250 	Text Alignment  :	D   D    D D     D    D  D  S     S    S    S      
2024-02-11 01:21:26,250 ========================================================================================================================
2024-02-11 01:21:26,251 Logging Sequence: 160_87.00
2024-02-11 01:21:26,251 	Gloss Reference :	A B+C+D+E
2024-02-11 01:21:26,251 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 01:21:26,251 	Gloss Alignment :	         
2024-02-11 01:21:26,251 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 01:21:26,252 	Text Reference  :	*** **** kohli held a    press conference and  said
2024-02-11 01:21:26,252 	Text Hypothesis :	and they have  been done in    the        next day 
2024-02-11 01:21:26,252 	Text Alignment  :	I   I    S     S    S    S     S          S    S   
2024-02-11 01:21:26,252 ========================================================================================================================
2024-02-11 01:21:26,257 Epoch 6000: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 01:21:26,257 EPOCH 6001
2024-02-11 01:21:43,241 Epoch 6001: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 01:21:43,242 EPOCH 6002
2024-02-11 01:21:59,427 Epoch 6002: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 01:21:59,428 EPOCH 6003
2024-02-11 01:22:15,783 Epoch 6003: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 01:22:15,784 EPOCH 6004
2024-02-11 01:22:31,857 Epoch 6004: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 01:22:31,857 EPOCH 6005
2024-02-11 01:22:48,054 Epoch 6005: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 01:22:48,055 EPOCH 6006
2024-02-11 01:23:04,232 Epoch 6006: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 01:23:04,233 EPOCH 6007
2024-02-11 01:23:21,241 Epoch 6007: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 01:23:21,242 EPOCH 6008
2024-02-11 01:23:37,465 Epoch 6008: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 01:23:37,465 EPOCH 6009
2024-02-11 01:23:53,508 Epoch 6009: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 01:23:53,509 EPOCH 6010
2024-02-11 01:24:09,817 Epoch 6010: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 01:24:09,817 EPOCH 6011
2024-02-11 01:24:26,087 Epoch 6011: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:24:26,087 EPOCH 6012
2024-02-11 01:24:30,342 [Epoch: 6012 Step: 00054100] Batch Recognition Loss:   0.000246 => Gls Tokens per Sec:       89 || Batch Translation Loss:   0.005404 => Txt Tokens per Sec:      319 || Lr: 0.000050
2024-02-11 01:24:42,047 Epoch 6012: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 01:24:42,048 EPOCH 6013
2024-02-11 01:24:57,965 Epoch 6013: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 01:24:57,965 EPOCH 6014
2024-02-11 01:25:14,183 Epoch 6014: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-11 01:25:14,184 EPOCH 6015
2024-02-11 01:25:29,933 Epoch 6015: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-11 01:25:29,933 EPOCH 6016
2024-02-11 01:25:45,774 Epoch 6016: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-11 01:25:45,775 EPOCH 6017
2024-02-11 01:26:01,753 Epoch 6017: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 01:26:01,754 EPOCH 6018
2024-02-11 01:26:17,976 Epoch 6018: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 01:26:17,976 EPOCH 6019
2024-02-11 01:26:34,046 Epoch 6019: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 01:26:34,047 EPOCH 6020
2024-02-11 01:26:50,173 Epoch 6020: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 01:26:50,173 EPOCH 6021
2024-02-11 01:27:06,279 Epoch 6021: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 01:27:06,280 EPOCH 6022
2024-02-11 01:27:22,185 Epoch 6022: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 01:27:22,186 EPOCH 6023
2024-02-11 01:27:28,558 [Epoch: 6023 Step: 00054200] Batch Recognition Loss:   0.000162 => Gls Tokens per Sec:      402 || Batch Translation Loss:   0.012477 => Txt Tokens per Sec:     1244 || Lr: 0.000050
2024-02-11 01:27:38,504 Epoch 6023: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 01:27:38,505 EPOCH 6024
2024-02-11 01:27:54,216 Epoch 6024: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 01:27:54,217 EPOCH 6025
2024-02-11 01:28:10,561 Epoch 6025: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 01:28:10,562 EPOCH 6026
2024-02-11 01:28:26,476 Epoch 6026: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 01:28:26,476 EPOCH 6027
2024-02-11 01:28:42,494 Epoch 6027: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-11 01:28:42,495 EPOCH 6028
2024-02-11 01:28:58,473 Epoch 6028: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-11 01:28:58,473 EPOCH 6029
2024-02-11 01:29:14,595 Epoch 6029: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.60 
2024-02-11 01:29:14,595 EPOCH 6030
2024-02-11 01:29:30,765 Epoch 6030: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.48 
2024-02-11 01:29:30,766 EPOCH 6031
2024-02-11 01:29:47,084 Epoch 6031: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-11 01:29:47,085 EPOCH 6032
2024-02-11 01:30:03,002 Epoch 6032: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-11 01:30:03,003 EPOCH 6033
2024-02-11 01:30:19,170 Epoch 6033: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-11 01:30:19,170 EPOCH 6034
2024-02-11 01:30:20,364 [Epoch: 6034 Step: 00054300] Batch Recognition Loss:   0.000435 => Gls Tokens per Sec:     3219 || Batch Translation Loss:   0.043204 => Txt Tokens per Sec:     8226 || Lr: 0.000050
2024-02-11 01:30:35,016 Epoch 6034: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-11 01:30:35,017 EPOCH 6035
2024-02-11 01:30:51,424 Epoch 6035: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-11 01:30:51,424 EPOCH 6036
2024-02-11 01:31:07,443 Epoch 6036: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 01:31:07,443 EPOCH 6037
2024-02-11 01:31:23,393 Epoch 6037: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 01:31:23,394 EPOCH 6038
2024-02-11 01:31:39,333 Epoch 6038: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 01:31:39,334 EPOCH 6039
2024-02-11 01:31:54,768 Epoch 6039: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 01:31:54,769 EPOCH 6040
2024-02-11 01:32:10,843 Epoch 6040: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 01:32:10,843 EPOCH 6041
2024-02-11 01:32:26,988 Epoch 6041: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 01:32:26,988 EPOCH 6042
2024-02-11 01:32:43,107 Epoch 6042: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 01:32:43,107 EPOCH 6043
2024-02-11 01:32:59,231 Epoch 6043: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 01:32:59,232 EPOCH 6044
2024-02-11 01:33:14,996 Epoch 6044: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-11 01:33:14,997 EPOCH 6045
2024-02-11 01:33:19,341 [Epoch: 6045 Step: 00054400] Batch Recognition Loss:   0.000317 => Gls Tokens per Sec:     1179 || Batch Translation Loss:   0.017359 => Txt Tokens per Sec:     3202 || Lr: 0.000050
2024-02-11 01:33:30,985 Epoch 6045: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 01:33:30,985 EPOCH 6046
2024-02-11 01:33:47,158 Epoch 6046: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 01:33:47,158 EPOCH 6047
2024-02-11 01:34:03,276 Epoch 6047: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 01:34:03,276 EPOCH 6048
2024-02-11 01:34:19,536 Epoch 6048: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 01:34:19,537 EPOCH 6049
2024-02-11 01:34:35,577 Epoch 6049: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 01:34:35,577 EPOCH 6050
2024-02-11 01:34:51,795 Epoch 6050: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 01:34:51,796 EPOCH 6051
2024-02-11 01:35:08,135 Epoch 6051: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:35:08,135 EPOCH 6052
2024-02-11 01:35:24,392 Epoch 6052: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:35:24,392 EPOCH 6053
2024-02-11 01:35:40,827 Epoch 6053: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:35:40,828 EPOCH 6054
2024-02-11 01:35:56,690 Epoch 6054: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:35:56,690 EPOCH 6055
2024-02-11 01:36:12,969 Epoch 6055: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 01:36:12,970 EPOCH 6056
2024-02-11 01:36:26,938 [Epoch: 6056 Step: 00054500] Batch Recognition Loss:   0.000224 => Gls Tokens per Sec:      394 || Batch Translation Loss:   0.013014 => Txt Tokens per Sec:     1074 || Lr: 0.000050
2024-02-11 01:36:29,043 Epoch 6056: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:36:29,044 EPOCH 6057
2024-02-11 01:36:45,441 Epoch 6057: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 01:36:45,442 EPOCH 6058
2024-02-11 01:37:01,246 Epoch 6058: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 01:37:01,247 EPOCH 6059
2024-02-11 01:37:17,559 Epoch 6059: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 01:37:17,560 EPOCH 6060
2024-02-11 01:37:33,864 Epoch 6060: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 01:37:33,865 EPOCH 6061
2024-02-11 01:37:50,097 Epoch 6061: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 01:37:50,097 EPOCH 6062
2024-02-11 01:38:06,305 Epoch 6062: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:38:06,306 EPOCH 6063
2024-02-11 01:38:22,435 Epoch 6063: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 01:38:22,436 EPOCH 6064
2024-02-11 01:38:38,339 Epoch 6064: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 01:38:38,340 EPOCH 6065
2024-02-11 01:38:54,273 Epoch 6065: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:38:54,273 EPOCH 6066
2024-02-11 01:39:10,524 Epoch 6066: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 01:39:10,525 EPOCH 6067
2024-02-11 01:39:15,879 [Epoch: 6067 Step: 00054600] Batch Recognition Loss:   0.000154 => Gls Tokens per Sec:     1435 || Batch Translation Loss:   0.006128 => Txt Tokens per Sec:     3797 || Lr: 0.000050
2024-02-11 01:39:26,592 Epoch 6067: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:39:26,592 EPOCH 6068
2024-02-11 01:39:42,923 Epoch 6068: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:39:42,924 EPOCH 6069
2024-02-11 01:39:58,901 Epoch 6069: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:39:58,901 EPOCH 6070
2024-02-11 01:40:14,958 Epoch 6070: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:40:14,958 EPOCH 6071
2024-02-11 01:40:31,730 Epoch 6071: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 01:40:31,731 EPOCH 6072
2024-02-11 01:40:47,747 Epoch 6072: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:40:47,748 EPOCH 6073
2024-02-11 01:41:03,802 Epoch 6073: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:41:03,803 EPOCH 6074
2024-02-11 01:41:19,751 Epoch 6074: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:41:19,751 EPOCH 6075
2024-02-11 01:41:35,620 Epoch 6075: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:41:35,621 EPOCH 6076
2024-02-11 01:41:52,011 Epoch 6076: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:41:52,011 EPOCH 6077
2024-02-11 01:42:08,164 Epoch 6077: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:42:08,164 EPOCH 6078
2024-02-11 01:42:18,148 [Epoch: 6078 Step: 00054700] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:      807 || Batch Translation Loss:   0.009190 => Txt Tokens per Sec:     2270 || Lr: 0.000050
2024-02-11 01:42:24,123 Epoch 6078: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 01:42:24,123 EPOCH 6079
2024-02-11 01:42:40,455 Epoch 6079: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 01:42:40,456 EPOCH 6080
2024-02-11 01:42:56,884 Epoch 6080: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:42:56,885 EPOCH 6081
2024-02-11 01:43:12,988 Epoch 6081: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 01:43:12,988 EPOCH 6082
2024-02-11 01:43:29,359 Epoch 6082: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 01:43:29,360 EPOCH 6083
2024-02-11 01:43:45,273 Epoch 6083: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:43:45,273 EPOCH 6084
2024-02-11 01:44:01,862 Epoch 6084: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:44:01,863 EPOCH 6085
2024-02-11 01:44:17,786 Epoch 6085: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 01:44:17,787 EPOCH 6086
2024-02-11 01:44:33,829 Epoch 6086: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:44:33,830 EPOCH 6087
2024-02-11 01:44:49,389 Epoch 6087: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 01:44:49,389 EPOCH 6088
2024-02-11 01:45:05,290 Epoch 6088: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:45:05,291 EPOCH 6089
2024-02-11 01:45:17,294 [Epoch: 6089 Step: 00054800] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:      853 || Batch Translation Loss:   0.016660 => Txt Tokens per Sec:     2335 || Lr: 0.000050
2024-02-11 01:45:21,542 Epoch 6089: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:45:21,542 EPOCH 6090
2024-02-11 01:45:37,253 Epoch 6090: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:45:37,254 EPOCH 6091
2024-02-11 01:45:53,595 Epoch 6091: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:45:53,596 EPOCH 6092
2024-02-11 01:46:09,725 Epoch 6092: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:46:09,725 EPOCH 6093
2024-02-11 01:46:25,645 Epoch 6093: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:46:25,646 EPOCH 6094
2024-02-11 01:46:41,688 Epoch 6094: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:46:41,689 EPOCH 6095
2024-02-11 01:46:57,825 Epoch 6095: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 01:46:57,825 EPOCH 6096
2024-02-11 01:47:14,088 Epoch 6096: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:47:14,088 EPOCH 6097
2024-02-11 01:47:30,585 Epoch 6097: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:47:30,586 EPOCH 6098
2024-02-11 01:47:46,723 Epoch 6098: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 01:47:46,724 EPOCH 6099
2024-02-11 01:48:02,878 Epoch 6099: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 01:48:02,878 EPOCH 6100
2024-02-11 01:48:19,211 [Epoch: 6100 Step: 00054900] Batch Recognition Loss:   0.000139 => Gls Tokens per Sec:      650 || Batch Translation Loss:   0.018795 => Txt Tokens per Sec:     1799 || Lr: 0.000050
2024-02-11 01:48:19,212 Epoch 6100: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:48:19,212 EPOCH 6101
2024-02-11 01:48:35,353 Epoch 6101: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:48:35,354 EPOCH 6102
2024-02-11 01:48:51,371 Epoch 6102: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.09 
2024-02-11 01:48:51,371 EPOCH 6103
2024-02-11 01:49:07,669 Epoch 6103: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:49:07,669 EPOCH 6104
2024-02-11 01:49:23,743 Epoch 6104: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:49:23,743 EPOCH 6105
2024-02-11 01:49:39,908 Epoch 6105: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:49:39,908 EPOCH 6106
2024-02-11 01:49:55,963 Epoch 6106: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:49:55,964 EPOCH 6107
2024-02-11 01:50:12,093 Epoch 6107: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:50:12,094 EPOCH 6108
2024-02-11 01:50:28,134 Epoch 6108: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 01:50:28,134 EPOCH 6109
2024-02-11 01:50:44,145 Epoch 6109: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:50:44,145 EPOCH 6110
2024-02-11 01:51:00,310 Epoch 6110: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 01:51:00,310 EPOCH 6111
2024-02-11 01:51:16,323 Epoch 6111: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 01:51:16,324 EPOCH 6112
2024-02-11 01:51:16,941 [Epoch: 6112 Step: 00055000] Batch Recognition Loss:   0.000160 => Gls Tokens per Sec:     2077 || Batch Translation Loss:   0.018001 => Txt Tokens per Sec:     6327 || Lr: 0.000050
2024-02-11 01:51:32,243 Epoch 6112: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 01:51:32,244 EPOCH 6113
2024-02-11 01:51:48,081 Epoch 6113: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 01:51:48,082 EPOCH 6114
2024-02-11 01:52:04,454 Epoch 6114: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 01:52:04,455 EPOCH 6115
2024-02-11 01:52:20,450 Epoch 6115: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 01:52:20,451 EPOCH 6116
2024-02-11 01:52:36,305 Epoch 6116: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 01:52:36,305 EPOCH 6117
2024-02-11 01:52:52,615 Epoch 6117: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 01:52:52,616 EPOCH 6118
2024-02-11 01:53:08,588 Epoch 6118: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 01:53:08,588 EPOCH 6119
2024-02-11 01:53:24,494 Epoch 6119: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 01:53:24,494 EPOCH 6120
2024-02-11 01:53:40,421 Epoch 6120: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 01:53:40,422 EPOCH 6121
2024-02-11 01:53:56,712 Epoch 6121: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 01:53:56,712 EPOCH 6122
2024-02-11 01:54:12,842 Epoch 6122: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 01:54:12,842 EPOCH 6123
2024-02-11 01:54:21,741 [Epoch: 6123 Step: 00055100] Batch Recognition Loss:   0.000327 => Gls Tokens per Sec:      288 || Batch Translation Loss:   0.015114 => Txt Tokens per Sec:      958 || Lr: 0.000050
2024-02-11 01:54:28,892 Epoch 6123: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 01:54:28,893 EPOCH 6124
2024-02-11 01:54:44,950 Epoch 6124: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 01:54:44,951 EPOCH 6125
2024-02-11 01:55:01,203 Epoch 6125: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 01:55:01,203 EPOCH 6126
2024-02-11 01:55:17,509 Epoch 6126: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 01:55:17,509 EPOCH 6127
2024-02-11 01:55:33,327 Epoch 6127: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 01:55:33,327 EPOCH 6128
2024-02-11 01:55:49,693 Epoch 6128: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-11 01:55:49,694 EPOCH 6129
2024-02-11 01:56:06,131 Epoch 6129: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 01:56:06,131 EPOCH 6130
2024-02-11 01:56:22,230 Epoch 6130: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 01:56:22,230 EPOCH 6131
2024-02-11 01:56:38,404 Epoch 6131: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 01:56:38,405 EPOCH 6132
2024-02-11 01:56:54,807 Epoch 6132: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-11 01:56:54,808 EPOCH 6133
2024-02-11 01:57:10,937 Epoch 6133: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-11 01:57:10,938 EPOCH 6134
2024-02-11 01:57:16,252 [Epoch: 6134 Step: 00055200] Batch Recognition Loss:   0.000313 => Gls Tokens per Sec:      553 || Batch Translation Loss:   0.028489 => Txt Tokens per Sec:     1494 || Lr: 0.000050
2024-02-11 01:57:27,053 Epoch 6134: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-11 01:57:27,054 EPOCH 6135
2024-02-11 01:57:43,112 Epoch 6135: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-11 01:57:43,113 EPOCH 6136
2024-02-11 01:57:59,242 Epoch 6136: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 01:57:59,243 EPOCH 6137
2024-02-11 01:58:15,542 Epoch 6137: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-11 01:58:15,543 EPOCH 6138
2024-02-11 01:58:31,813 Epoch 6138: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 01:58:31,814 EPOCH 6139
2024-02-11 01:58:47,891 Epoch 6139: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 01:58:47,891 EPOCH 6140
2024-02-11 01:59:03,698 Epoch 6140: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 01:59:03,699 EPOCH 6141
2024-02-11 01:59:19,921 Epoch 6141: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 01:59:19,921 EPOCH 6142
2024-02-11 01:59:35,986 Epoch 6142: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 01:59:35,987 EPOCH 6143
2024-02-11 01:59:52,268 Epoch 6143: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-11 01:59:52,269 EPOCH 6144
2024-02-11 02:00:07,778 Epoch 6144: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 02:00:07,778 EPOCH 6145
2024-02-11 02:00:16,200 [Epoch: 6145 Step: 00055300] Batch Recognition Loss:   0.000339 => Gls Tokens per Sec:      501 || Batch Translation Loss:   0.005809 => Txt Tokens per Sec:     1366 || Lr: 0.000050
2024-02-11 02:00:23,904 Epoch 6145: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 02:00:23,905 EPOCH 6146
2024-02-11 02:00:39,630 Epoch 6146: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 02:00:39,630 EPOCH 6147
2024-02-11 02:00:55,566 Epoch 6147: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 02:00:55,567 EPOCH 6148
2024-02-11 02:01:11,583 Epoch 6148: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:01:11,584 EPOCH 6149
2024-02-11 02:01:27,751 Epoch 6149: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 02:01:27,752 EPOCH 6150
2024-02-11 02:01:43,770 Epoch 6150: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-11 02:01:43,771 EPOCH 6151
2024-02-11 02:02:00,047 Epoch 6151: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 02:02:00,048 EPOCH 6152
2024-02-11 02:02:16,323 Epoch 6152: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 02:02:16,323 EPOCH 6153
2024-02-11 02:02:32,292 Epoch 6153: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-11 02:02:32,293 EPOCH 6154
2024-02-11 02:02:49,169 Epoch 6154: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 02:02:49,169 EPOCH 6155
2024-02-11 02:03:05,783 Epoch 6155: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 02:03:05,784 EPOCH 6156
2024-02-11 02:03:17,482 [Epoch: 6156 Step: 00055400] Batch Recognition Loss:   0.000145 => Gls Tokens per Sec:      470 || Batch Translation Loss:   0.012589 => Txt Tokens per Sec:     1346 || Lr: 0.000050
2024-02-11 02:03:21,924 Epoch 6156: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-11 02:03:21,924 EPOCH 6157
2024-02-11 02:03:38,052 Epoch 6157: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.60 
2024-02-11 02:03:38,052 EPOCH 6158
2024-02-11 02:03:53,983 Epoch 6158: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-11 02:03:53,984 EPOCH 6159
2024-02-11 02:04:10,120 Epoch 6159: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-11 02:04:10,120 EPOCH 6160
2024-02-11 02:04:26,162 Epoch 6160: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-11 02:04:26,163 EPOCH 6161
2024-02-11 02:04:42,579 Epoch 6161: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-11 02:04:42,580 EPOCH 6162
2024-02-11 02:04:59,046 Epoch 6162: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-11 02:04:59,046 EPOCH 6163
2024-02-11 02:05:15,290 Epoch 6163: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-11 02:05:15,291 EPOCH 6164
2024-02-11 02:05:31,701 Epoch 6164: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-11 02:05:31,702 EPOCH 6165
2024-02-11 02:05:47,874 Epoch 6165: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-11 02:05:47,875 EPOCH 6166
2024-02-11 02:06:04,129 Epoch 6166: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-11 02:06:04,129 EPOCH 6167
2024-02-11 02:06:15,849 [Epoch: 6167 Step: 00055500] Batch Recognition Loss:   0.000741 => Gls Tokens per Sec:      579 || Batch Translation Loss:   0.043220 => Txt Tokens per Sec:     1579 || Lr: 0.000050
2024-02-11 02:06:19,972 Epoch 6167: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-11 02:06:19,972 EPOCH 6168
2024-02-11 02:06:36,323 Epoch 6168: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-11 02:06:36,324 EPOCH 6169
2024-02-11 02:06:52,529 Epoch 6169: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-11 02:06:52,529 EPOCH 6170
2024-02-11 02:07:08,748 Epoch 6170: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-11 02:07:08,749 EPOCH 6171
2024-02-11 02:07:24,840 Epoch 6171: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-11 02:07:24,841 EPOCH 6172
2024-02-11 02:07:40,756 Epoch 6172: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 02:07:40,757 EPOCH 6173
2024-02-11 02:07:56,890 Epoch 6173: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 02:07:56,891 EPOCH 6174
2024-02-11 02:08:12,839 Epoch 6174: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-11 02:08:12,840 EPOCH 6175
2024-02-11 02:08:28,878 Epoch 6175: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-11 02:08:28,879 EPOCH 6176
2024-02-11 02:08:44,763 Epoch 6176: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 02:08:44,764 EPOCH 6177
2024-02-11 02:09:00,930 Epoch 6177: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-11 02:09:00,930 EPOCH 6178
2024-02-11 02:09:10,889 [Epoch: 6178 Step: 00055600] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:      809 || Batch Translation Loss:   0.011340 => Txt Tokens per Sec:     2278 || Lr: 0.000050
2024-02-11 02:09:17,203 Epoch 6178: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 02:09:17,204 EPOCH 6179
2024-02-11 02:09:33,626 Epoch 6179: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-11 02:09:33,626 EPOCH 6180
2024-02-11 02:09:49,792 Epoch 6180: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 02:09:49,792 EPOCH 6181
2024-02-11 02:10:06,005 Epoch 6181: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 02:10:06,005 EPOCH 6182
2024-02-11 02:10:22,233 Epoch 6182: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 02:10:22,234 EPOCH 6183
2024-02-11 02:10:38,214 Epoch 6183: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 02:10:38,215 EPOCH 6184
2024-02-11 02:10:54,357 Epoch 6184: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 02:10:54,358 EPOCH 6185
2024-02-11 02:11:10,324 Epoch 6185: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:11:10,324 EPOCH 6186
2024-02-11 02:11:26,679 Epoch 6186: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:11:26,680 EPOCH 6187
2024-02-11 02:11:42,783 Epoch 6187: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:11:42,783 EPOCH 6188
2024-02-11 02:11:58,737 Epoch 6188: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:11:58,737 EPOCH 6189
2024-02-11 02:12:14,636 [Epoch: 6189 Step: 00055700] Batch Recognition Loss:   0.000412 => Gls Tokens per Sec:      588 || Batch Translation Loss:   0.016416 => Txt Tokens per Sec:     1704 || Lr: 0.000050
2024-02-11 02:12:15,058 Epoch 6189: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:12:15,058 EPOCH 6190
2024-02-11 02:12:31,172 Epoch 6190: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:12:31,173 EPOCH 6191
2024-02-11 02:12:47,211 Epoch 6191: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:12:47,212 EPOCH 6192
2024-02-11 02:13:02,876 Epoch 6192: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:13:02,877 EPOCH 6193
2024-02-11 02:13:18,880 Epoch 6193: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:13:18,880 EPOCH 6194
2024-02-11 02:13:34,877 Epoch 6194: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 02:13:34,878 EPOCH 6195
2024-02-11 02:13:51,075 Epoch 6195: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:13:51,076 EPOCH 6196
2024-02-11 02:14:07,122 Epoch 6196: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:14:07,122 EPOCH 6197
2024-02-11 02:14:23,184 Epoch 6197: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:14:23,185 EPOCH 6198
2024-02-11 02:14:39,296 Epoch 6198: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:14:39,297 EPOCH 6199
2024-02-11 02:14:55,697 Epoch 6199: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:14:55,697 EPOCH 6200
2024-02-11 02:15:11,913 [Epoch: 6200 Step: 00055800] Batch Recognition Loss:   0.000147 => Gls Tokens per Sec:      655 || Batch Translation Loss:   0.012284 => Txt Tokens per Sec:     1812 || Lr: 0.000050
2024-02-11 02:15:11,914 Epoch 6200: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:15:11,914 EPOCH 6201
2024-02-11 02:15:28,238 Epoch 6201: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:15:28,238 EPOCH 6202
2024-02-11 02:15:44,395 Epoch 6202: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:15:44,395 EPOCH 6203
2024-02-11 02:16:00,438 Epoch 6203: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:16:00,439 EPOCH 6204
2024-02-11 02:16:16,381 Epoch 6204: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:16:16,381 EPOCH 6205
2024-02-11 02:16:32,604 Epoch 6205: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:16:32,604 EPOCH 6206
2024-02-11 02:16:48,509 Epoch 6206: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:16:48,510 EPOCH 6207
2024-02-11 02:17:04,819 Epoch 6207: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:17:04,819 EPOCH 6208
2024-02-11 02:17:20,715 Epoch 6208: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 02:17:20,715 EPOCH 6209
2024-02-11 02:17:36,662 Epoch 6209: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 02:17:36,662 EPOCH 6210
2024-02-11 02:17:53,088 Epoch 6210: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 02:17:53,088 EPOCH 6211
2024-02-11 02:18:09,023 Epoch 6211: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 02:18:09,024 EPOCH 6212
2024-02-11 02:18:09,488 [Epoch: 6212 Step: 00055900] Batch Recognition Loss:   0.000505 => Gls Tokens per Sec:     2765 || Batch Translation Loss:   0.014194 => Txt Tokens per Sec:     7687 || Lr: 0.000050
2024-02-11 02:18:24,888 Epoch 6212: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 02:18:24,889 EPOCH 6213
2024-02-11 02:18:40,983 Epoch 6213: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 02:18:40,983 EPOCH 6214
2024-02-11 02:18:56,831 Epoch 6214: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 02:18:56,831 EPOCH 6215
2024-02-11 02:19:13,207 Epoch 6215: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 02:19:13,208 EPOCH 6216
2024-02-11 02:19:29,177 Epoch 6216: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 02:19:29,177 EPOCH 6217
2024-02-11 02:19:45,304 Epoch 6217: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 02:19:45,305 EPOCH 6218
2024-02-11 02:20:01,048 Epoch 6218: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 02:20:01,048 EPOCH 6219
2024-02-11 02:20:17,094 Epoch 6219: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:20:17,094 EPOCH 6220
2024-02-11 02:20:33,249 Epoch 6220: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:20:33,249 EPOCH 6221
2024-02-11 02:20:49,308 Epoch 6221: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:20:49,309 EPOCH 6222
2024-02-11 02:21:05,611 Epoch 6222: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:21:05,611 EPOCH 6223
2024-02-11 02:21:09,234 [Epoch: 6223 Step: 00056000] Batch Recognition Loss:   0.000395 => Gls Tokens per Sec:      707 || Batch Translation Loss:   0.014239 => Txt Tokens per Sec:     2115 || Lr: 0.000050
2024-02-11 02:22:20,900 Validation result at epoch 6223, step    56000: duration: 71.6658s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.24148	Translation Loss: 107654.69531	PPL: 46749.85156
	Eval Metric: BLEU
	WER 2.33	(DEL: 0.00,	INS: 0.00,	SUB: 2.33)
	BLEU-4 0.40	(BLEU-1: 9.46,	BLEU-2: 2.64,	BLEU-3: 0.96,	BLEU-4: 0.40)
	CHRF 16.64	ROUGE 7.97
2024-02-11 02:22:20,903 Logging Recognition and Translation Outputs
2024-02-11 02:22:20,903 ========================================================================================================================
2024-02-11 02:22:20,903 Logging Sequence: 177_167.00
2024-02-11 02:22:20,903 	Gloss Reference :	A B+C+D+E
2024-02-11 02:22:20,903 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 02:22:20,904 	Gloss Alignment :	         
2024-02-11 02:22:20,904 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 02:22:20,905 	Text Reference  :	this is because sushil wanted   to   establish his   fear   to ensure   no  one    would oppose him        
2024-02-11 02:22:20,905 	Text Hypothesis :	**** ** and     police detained over a         dozen people in brussels and others have  been   quarantined
2024-02-11 02:22:20,906 	Text Alignment  :	D    D  S       S      S        S    S         S     S      S  S        S   S      S     S      S          
2024-02-11 02:22:20,906 ========================================================================================================================
2024-02-11 02:22:20,906 Logging Sequence: 127_140.00
2024-02-11 02:22:20,906 	Gloss Reference :	A B+C+D+E
2024-02-11 02:22:20,906 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 02:22:20,906 	Gloss Alignment :	         
2024-02-11 02:22:20,906 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 02:22:20,908 	Text Reference  :	this is india' 3rd   medal in  the world athletics championships he is very  talented and his     performance is   highly impressive 
2024-02-11 02:22:20,908 	Text Hypothesis :	**** ** ****** after india won the ***** ********* ************* ** ** match by       kkr batters could       have been   quarantined
2024-02-11 02:22:20,908 	Text Alignment  :	D    D  D      S     S     S       D     D         D             D  D  S     S        S   S       S           S    S      S          
2024-02-11 02:22:20,908 ========================================================================================================================
2024-02-11 02:22:20,908 Logging Sequence: 126_200.00
2024-02-11 02:22:20,909 	Gloss Reference :	A B+C+D+E
2024-02-11 02:22:20,909 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 02:22:20,909 	Gloss Alignment :	         
2024-02-11 02:22:20,909 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 02:22:20,910 	Text Reference  :	** let me  tell  you   about them        
2024-02-11 02:22:20,910 	Text Hypothesis :	he was her first count of    indiscipline
2024-02-11 02:22:20,910 	Text Alignment  :	I  S   S   S     S     S     S           
2024-02-11 02:22:20,910 ========================================================================================================================
2024-02-11 02:22:20,910 Logging Sequence: 104_119.00
2024-02-11 02:22:20,910 	Gloss Reference :	A B+C+D+E
2024-02-11 02:22:20,911 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 02:22:20,911 	Gloss Alignment :	         
2024-02-11 02:22:20,911 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 02:22:20,913 	Text Reference  :	famous chess players like   viswanathan anand   and   praggnanandhaa's coach  r    b     ramesh congratulated him    for        his   impressive performance
2024-02-11 02:22:20,913 	Text Hypothesis :	****** the   cameras caught indian      skipper rohit sharma           wiping away tears as     he            looked devastated after the        loss       
2024-02-11 02:22:20,913 	Text Alignment  :	D      S     S       S      S           S       S     S                S      S    S     S      S             S      S          S     S          S          
2024-02-11 02:22:20,913 ========================================================================================================================
2024-02-11 02:22:20,913 Logging Sequence: 172_267.00
2024-02-11 02:22:20,913 	Gloss Reference :	A B+C+D+E
2024-02-11 02:22:20,914 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 02:22:20,914 	Gloss Alignment :	         
2024-02-11 02:22:20,914 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 02:22:20,914 	Text Reference  :	*** ** **** such provisions have been made
2024-02-11 02:22:20,914 	Text Hypothesis :	let me tell you  about      it   be   yet 
2024-02-11 02:22:20,915 	Text Alignment  :	I   I  I    S    S          S    S    S   
2024-02-11 02:22:20,915 ========================================================================================================================
2024-02-11 02:22:33,727 Epoch 6223: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:22:33,727 EPOCH 6224
2024-02-11 02:22:49,784 Epoch 6224: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:22:49,784 EPOCH 6225
2024-02-11 02:23:05,967 Epoch 6225: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:23:05,968 EPOCH 6226
2024-02-11 02:23:22,924 Epoch 6226: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:23:22,925 EPOCH 6227
2024-02-11 02:23:39,274 Epoch 6227: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:23:39,275 EPOCH 6228
2024-02-11 02:23:55,366 Epoch 6228: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:23:55,367 EPOCH 6229
2024-02-11 02:24:11,571 Epoch 6229: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:24:11,572 EPOCH 6230
2024-02-11 02:24:27,777 Epoch 6230: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:24:27,778 EPOCH 6231
2024-02-11 02:24:43,745 Epoch 6231: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:24:43,745 EPOCH 6232
2024-02-11 02:24:59,786 Epoch 6232: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:24:59,786 EPOCH 6233
2024-02-11 02:25:16,183 Epoch 6233: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:25:16,183 EPOCH 6234
2024-02-11 02:25:22,505 [Epoch: 6234 Step: 00056100] Batch Recognition Loss:   0.000398 => Gls Tokens per Sec:      607 || Batch Translation Loss:   0.015349 => Txt Tokens per Sec:     1663 || Lr: 0.000050
2024-02-11 02:25:32,055 Epoch 6234: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:25:32,056 EPOCH 6235
2024-02-11 02:25:48,097 Epoch 6235: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:25:48,097 EPOCH 6236
2024-02-11 02:26:04,245 Epoch 6236: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:26:04,245 EPOCH 6237
2024-02-11 02:26:20,467 Epoch 6237: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:26:20,467 EPOCH 6238
2024-02-11 02:26:36,568 Epoch 6238: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 02:26:36,569 EPOCH 6239
2024-02-11 02:26:52,687 Epoch 6239: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 02:26:52,688 EPOCH 6240
2024-02-11 02:27:08,734 Epoch 6240: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 02:27:08,735 EPOCH 6241
2024-02-11 02:27:24,973 Epoch 6241: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:27:24,974 EPOCH 6242
2024-02-11 02:27:41,123 Epoch 6242: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:27:41,124 EPOCH 6243
2024-02-11 02:27:57,119 Epoch 6243: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:27:57,120 EPOCH 6244
2024-02-11 02:28:13,718 Epoch 6244: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:28:13,719 EPOCH 6245
2024-02-11 02:28:25,135 [Epoch: 6245 Step: 00056200] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:      370 || Batch Translation Loss:   0.015199 => Txt Tokens per Sec:     1134 || Lr: 0.000050
2024-02-11 02:28:29,974 Epoch 6245: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:28:29,974 EPOCH 6246
2024-02-11 02:28:46,191 Epoch 6246: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:28:46,191 EPOCH 6247
2024-02-11 02:29:02,351 Epoch 6247: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:29:02,352 EPOCH 6248
2024-02-11 02:29:17,954 Epoch 6248: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:29:17,955 EPOCH 6249
2024-02-11 02:29:34,329 Epoch 6249: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 02:29:34,329 EPOCH 6250
2024-02-11 02:29:50,350 Epoch 6250: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:29:50,350 EPOCH 6251
2024-02-11 02:30:06,519 Epoch 6251: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:30:06,520 EPOCH 6252
2024-02-11 02:30:22,556 Epoch 6252: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:30:22,557 EPOCH 6253
2024-02-11 02:30:38,779 Epoch 6253: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 02:30:38,780 EPOCH 6254
2024-02-11 02:30:54,631 Epoch 6254: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 02:30:54,631 EPOCH 6255
2024-02-11 02:31:10,586 Epoch 6255: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 02:31:10,587 EPOCH 6256
2024-02-11 02:31:12,700 [Epoch: 6256 Step: 00056300] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:     3029 || Batch Translation Loss:   0.025865 => Txt Tokens per Sec:     7563 || Lr: 0.000050
2024-02-11 02:31:26,436 Epoch 6256: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-11 02:31:26,436 EPOCH 6257
2024-02-11 02:31:42,735 Epoch 6257: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 02:31:42,735 EPOCH 6258
2024-02-11 02:31:58,856 Epoch 6258: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 02:31:58,857 EPOCH 6259
2024-02-11 02:32:14,834 Epoch 6259: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 02:32:14,835 EPOCH 6260
2024-02-11 02:32:30,803 Epoch 6260: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 02:32:30,804 EPOCH 6261
2024-02-11 02:32:47,048 Epoch 6261: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 02:32:47,049 EPOCH 6262
2024-02-11 02:33:03,381 Epoch 6262: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 02:33:03,381 EPOCH 6263
2024-02-11 02:33:19,489 Epoch 6263: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:33:19,490 EPOCH 6264
2024-02-11 02:33:35,404 Epoch 6264: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:33:35,404 EPOCH 6265
2024-02-11 02:33:51,683 Epoch 6265: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 02:33:51,683 EPOCH 6266
2024-02-11 02:34:07,880 Epoch 6266: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 02:34:07,880 EPOCH 6267
2024-02-11 02:34:12,982 [Epoch: 6267 Step: 00056400] Batch Recognition Loss:   0.000177 => Gls Tokens per Sec:     1506 || Batch Translation Loss:   0.016904 => Txt Tokens per Sec:     3867 || Lr: 0.000050
2024-02-11 02:34:23,655 Epoch 6267: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 02:34:23,656 EPOCH 6268
2024-02-11 02:34:39,760 Epoch 6268: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.34 
2024-02-11 02:34:39,761 EPOCH 6269
2024-02-11 02:34:56,121 Epoch 6269: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.45 
2024-02-11 02:34:56,121 EPOCH 6270
2024-02-11 02:35:12,245 Epoch 6270: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.77 
2024-02-11 02:35:12,245 EPOCH 6271
2024-02-11 02:35:27,967 Epoch 6271: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-11 02:35:27,968 EPOCH 6272
2024-02-11 02:35:43,870 Epoch 6272: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.44 
2024-02-11 02:35:43,871 EPOCH 6273
2024-02-11 02:36:00,170 Epoch 6273: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-11 02:36:00,170 EPOCH 6274
2024-02-11 02:36:17,288 Epoch 6274: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.37 
2024-02-11 02:36:17,288 EPOCH 6275
2024-02-11 02:36:33,471 Epoch 6275: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.31 
2024-02-11 02:36:33,472 EPOCH 6276
2024-02-11 02:36:49,496 Epoch 6276: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-11 02:36:49,496 EPOCH 6277
2024-02-11 02:37:05,796 Epoch 6277: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-11 02:37:05,797 EPOCH 6278
2024-02-11 02:37:16,914 [Epoch: 6278 Step: 00056500] Batch Recognition Loss:   0.000575 => Gls Tokens per Sec:      806 || Batch Translation Loss:   0.024943 => Txt Tokens per Sec:     2315 || Lr: 0.000050
2024-02-11 02:37:21,611 Epoch 6278: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-11 02:37:21,611 EPOCH 6279
2024-02-11 02:37:37,773 Epoch 6279: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 02:37:37,774 EPOCH 6280
2024-02-11 02:37:53,817 Epoch 6280: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 02:37:53,818 EPOCH 6281
2024-02-11 02:38:10,145 Epoch 6281: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 02:38:10,145 EPOCH 6282
2024-02-11 02:38:25,955 Epoch 6282: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 02:38:25,955 EPOCH 6283
2024-02-11 02:38:42,150 Epoch 6283: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 02:38:42,152 EPOCH 6284
2024-02-11 02:38:58,368 Epoch 6284: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-11 02:38:58,368 EPOCH 6285
2024-02-11 02:39:14,884 Epoch 6285: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 02:39:14,884 EPOCH 6286
2024-02-11 02:39:31,000 Epoch 6286: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 02:39:31,000 EPOCH 6287
2024-02-11 02:39:47,093 Epoch 6287: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 02:39:47,093 EPOCH 6288
2024-02-11 02:40:03,457 Epoch 6288: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 02:40:03,458 EPOCH 6289
2024-02-11 02:40:13,805 [Epoch: 6289 Step: 00056600] Batch Recognition Loss:   0.000514 => Gls Tokens per Sec:      903 || Batch Translation Loss:   0.006524 => Txt Tokens per Sec:     2414 || Lr: 0.000050
2024-02-11 02:40:19,569 Epoch 6289: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:40:19,569 EPOCH 6290
2024-02-11 02:40:36,252 Epoch 6290: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:40:36,252 EPOCH 6291
2024-02-11 02:40:52,440 Epoch 6291: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:40:52,440 EPOCH 6292
2024-02-11 02:41:08,286 Epoch 6292: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:41:08,287 EPOCH 6293
2024-02-11 02:41:24,468 Epoch 6293: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:41:24,468 EPOCH 6294
2024-02-11 02:41:40,556 Epoch 6294: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:41:40,556 EPOCH 6295
2024-02-11 02:41:56,668 Epoch 6295: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:41:56,669 EPOCH 6296
2024-02-11 02:42:12,826 Epoch 6296: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:42:12,827 EPOCH 6297
2024-02-11 02:42:28,877 Epoch 6297: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:42:28,878 EPOCH 6298
2024-02-11 02:42:44,867 Epoch 6298: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:42:44,869 EPOCH 6299
2024-02-11 02:43:01,077 Epoch 6299: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:43:01,077 EPOCH 6300
2024-02-11 02:43:17,101 [Epoch: 6300 Step: 00056700] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:      663 || Batch Translation Loss:   0.010588 => Txt Tokens per Sec:     1834 || Lr: 0.000050
2024-02-11 02:43:17,101 Epoch 6300: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:43:17,101 EPOCH 6301
2024-02-11 02:43:33,047 Epoch 6301: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:43:33,047 EPOCH 6302
2024-02-11 02:43:48,898 Epoch 6302: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:43:48,899 EPOCH 6303
2024-02-11 02:44:04,614 Epoch 6303: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:44:04,615 EPOCH 6304
2024-02-11 02:44:21,000 Epoch 6304: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:44:21,000 EPOCH 6305
2024-02-11 02:44:37,011 Epoch 6305: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:44:37,012 EPOCH 6306
2024-02-11 02:44:53,060 Epoch 6306: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:44:53,062 EPOCH 6307
2024-02-11 02:45:09,039 Epoch 6307: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:45:09,040 EPOCH 6308
2024-02-11 02:45:25,204 Epoch 6308: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:45:25,204 EPOCH 6309
2024-02-11 02:45:41,170 Epoch 6309: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:45:41,170 EPOCH 6310
2024-02-11 02:45:57,481 Epoch 6310: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:45:57,481 EPOCH 6311
2024-02-11 02:46:13,280 Epoch 6311: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:46:13,282 EPOCH 6312
2024-02-11 02:46:17,597 [Epoch: 6312 Step: 00056800] Batch Recognition Loss:   0.000466 => Gls Tokens per Sec:       88 || Batch Translation Loss:   0.005242 => Txt Tokens per Sec:      313 || Lr: 0.000050
2024-02-11 02:46:29,532 Epoch 6312: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:46:29,532 EPOCH 6313
2024-02-11 02:46:45,592 Epoch 6313: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:46:45,593 EPOCH 6314
2024-02-11 02:47:01,428 Epoch 6314: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:47:01,429 EPOCH 6315
2024-02-11 02:47:17,581 Epoch 6315: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 02:47:17,582 EPOCH 6316
2024-02-11 02:47:33,753 Epoch 6316: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:47:33,753 EPOCH 6317
2024-02-11 02:47:49,844 Epoch 6317: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:47:49,845 EPOCH 6318
2024-02-11 02:48:05,948 Epoch 6318: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 02:48:05,949 EPOCH 6319
2024-02-11 02:48:22,209 Epoch 6319: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:48:22,210 EPOCH 6320
2024-02-11 02:48:38,245 Epoch 6320: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:48:38,246 EPOCH 6321
2024-02-11 02:48:54,149 Epoch 6321: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:48:54,150 EPOCH 6322
2024-02-11 02:49:09,948 Epoch 6322: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 02:49:09,949 EPOCH 6323
2024-02-11 02:49:14,816 [Epoch: 6323 Step: 00056900] Batch Recognition Loss:   0.000216 => Gls Tokens per Sec:      341 || Batch Translation Loss:   0.013087 => Txt Tokens per Sec:     1036 || Lr: 0.000050
2024-02-11 02:49:26,036 Epoch 6323: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:49:26,037 EPOCH 6324
2024-02-11 02:49:42,301 Epoch 6324: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 02:49:42,301 EPOCH 6325
2024-02-11 02:49:58,214 Epoch 6325: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 02:49:58,214 EPOCH 6326
2024-02-11 02:50:14,421 Epoch 6326: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 02:50:14,422 EPOCH 6327
2024-02-11 02:50:30,813 Epoch 6327: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:50:30,814 EPOCH 6328
2024-02-11 02:50:46,902 Epoch 6328: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:50:46,903 EPOCH 6329
2024-02-11 02:51:02,795 Epoch 6329: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:51:02,796 EPOCH 6330
2024-02-11 02:51:19,315 Epoch 6330: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:51:19,316 EPOCH 6331
2024-02-11 02:51:35,331 Epoch 6331: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:51:35,332 EPOCH 6332
2024-02-11 02:51:51,327 Epoch 6332: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:51:51,327 EPOCH 6333
2024-02-11 02:52:07,289 Epoch 6333: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:52:07,290 EPOCH 6334
2024-02-11 02:52:11,469 [Epoch: 6334 Step: 00057000] Batch Recognition Loss:   0.000172 => Gls Tokens per Sec:      919 || Batch Translation Loss:   0.010473 => Txt Tokens per Sec:     2716 || Lr: 0.000050
2024-02-11 02:52:23,224 Epoch 6334: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 02:52:23,224 EPOCH 6335
2024-02-11 02:52:39,542 Epoch 6335: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:52:39,543 EPOCH 6336
2024-02-11 02:52:55,765 Epoch 6336: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:52:55,766 EPOCH 6337
2024-02-11 02:53:12,264 Epoch 6337: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:53:12,264 EPOCH 6338
2024-02-11 02:53:28,331 Epoch 6338: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:53:28,331 EPOCH 6339
2024-02-11 02:53:44,258 Epoch 6339: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:53:44,258 EPOCH 6340
2024-02-11 02:54:00,757 Epoch 6340: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:54:00,758 EPOCH 6341
2024-02-11 02:54:17,182 Epoch 6341: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 02:54:17,183 EPOCH 6342
2024-02-11 02:54:33,207 Epoch 6342: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:54:33,207 EPOCH 6343
2024-02-11 02:54:49,059 Epoch 6343: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 02:54:49,060 EPOCH 6344
2024-02-11 02:55:05,190 Epoch 6344: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 02:55:05,190 EPOCH 6345
2024-02-11 02:55:14,805 [Epoch: 6345 Step: 00057100] Batch Recognition Loss:   0.000470 => Gls Tokens per Sec:      533 || Batch Translation Loss:   0.007022 => Txt Tokens per Sec:     1423 || Lr: 0.000050
2024-02-11 02:55:21,311 Epoch 6345: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 02:55:21,311 EPOCH 6346
2024-02-11 02:55:36,866 Epoch 6346: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 02:55:36,867 EPOCH 6347
2024-02-11 02:55:53,251 Epoch 6347: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 02:55:53,251 EPOCH 6348
2024-02-11 02:56:08,982 Epoch 6348: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:56:08,983 EPOCH 6349
2024-02-11 02:56:25,058 Epoch 6349: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 02:56:25,059 EPOCH 6350
2024-02-11 02:56:41,334 Epoch 6350: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 02:56:41,335 EPOCH 6351
2024-02-11 02:56:57,316 Epoch 6351: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 02:56:57,317 EPOCH 6352
2024-02-11 02:57:14,040 Epoch 6352: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-11 02:57:14,040 EPOCH 6353
2024-02-11 02:57:30,431 Epoch 6353: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 02:57:30,431 EPOCH 6354
2024-02-11 02:57:46,417 Epoch 6354: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 02:57:46,418 EPOCH 6355
2024-02-11 02:58:02,476 Epoch 6355: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 02:58:02,476 EPOCH 6356
2024-02-11 02:58:08,776 [Epoch: 6356 Step: 00057200] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:      873 || Batch Translation Loss:   0.012842 => Txt Tokens per Sec:     2424 || Lr: 0.000050
2024-02-11 02:58:18,367 Epoch 6356: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 02:58:18,368 EPOCH 6357
2024-02-11 02:58:34,818 Epoch 6357: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 02:58:34,819 EPOCH 6358
2024-02-11 02:58:50,978 Epoch 6358: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 02:58:50,979 EPOCH 6359
2024-02-11 02:59:07,224 Epoch 6359: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 02:59:07,224 EPOCH 6360
2024-02-11 02:59:23,173 Epoch 6360: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.41 
2024-02-11 02:59:23,174 EPOCH 6361
2024-02-11 02:59:39,209 Epoch 6361: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.10 
2024-02-11 02:59:39,209 EPOCH 6362
2024-02-11 02:59:55,629 Epoch 6362: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.78 
2024-02-11 02:59:55,630 EPOCH 6363
2024-02-11 03:00:11,671 Epoch 6363: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.04 
2024-02-11 03:00:11,672 EPOCH 6364
2024-02-11 03:00:27,532 Epoch 6364: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.67 
2024-02-11 03:00:27,532 EPOCH 6365
2024-02-11 03:00:43,435 Epoch 6365: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-11 03:00:43,435 EPOCH 6366
2024-02-11 03:00:59,531 Epoch 6366: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.20 
2024-02-11 03:00:59,532 EPOCH 6367
2024-02-11 03:01:08,876 [Epoch: 6367 Step: 00057300] Batch Recognition Loss:   0.001107 => Gls Tokens per Sec:      726 || Batch Translation Loss:   0.101593 => Txt Tokens per Sec:     1969 || Lr: 0.000050
2024-02-11 03:01:15,725 Epoch 6367: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.65 
2024-02-11 03:01:15,726 EPOCH 6368
2024-02-11 03:01:31,878 Epoch 6368: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.26 
2024-02-11 03:01:31,879 EPOCH 6369
2024-02-11 03:01:47,937 Epoch 6369: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.72 
2024-02-11 03:01:47,939 EPOCH 6370
2024-02-11 03:02:04,456 Epoch 6370: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.73 
2024-02-11 03:02:04,457 EPOCH 6371
2024-02-11 03:02:20,696 Epoch 6371: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-11 03:02:20,697 EPOCH 6372
2024-02-11 03:02:36,842 Epoch 6372: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-11 03:02:36,843 EPOCH 6373
2024-02-11 03:02:53,271 Epoch 6373: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-11 03:02:53,271 EPOCH 6374
2024-02-11 03:03:09,206 Epoch 6374: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-11 03:03:09,207 EPOCH 6375
2024-02-11 03:03:25,360 Epoch 6375: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-11 03:03:25,361 EPOCH 6376
2024-02-11 03:03:41,118 Epoch 6376: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-11 03:03:41,119 EPOCH 6377
2024-02-11 03:03:57,434 Epoch 6377: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-11 03:03:57,434 EPOCH 6378
2024-02-11 03:04:09,646 [Epoch: 6378 Step: 00057400] Batch Recognition Loss:   0.001004 => Gls Tokens per Sec:      660 || Batch Translation Loss:   0.019518 => Txt Tokens per Sec:     1804 || Lr: 0.000050
2024-02-11 03:04:13,395 Epoch 6378: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 03:04:13,395 EPOCH 6379
2024-02-11 03:04:29,532 Epoch 6379: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-11 03:04:29,533 EPOCH 6380
2024-02-11 03:04:45,518 Epoch 6380: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-11 03:04:45,518 EPOCH 6381
2024-02-11 03:05:01,945 Epoch 6381: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 03:05:01,946 EPOCH 6382
2024-02-11 03:05:17,924 Epoch 6382: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 03:05:17,924 EPOCH 6383
2024-02-11 03:05:34,000 Epoch 6383: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-11 03:05:34,001 EPOCH 6384
2024-02-11 03:05:49,920 Epoch 6384: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-11 03:05:49,921 EPOCH 6385
2024-02-11 03:06:06,314 Epoch 6385: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-11 03:06:06,315 EPOCH 6386
2024-02-11 03:06:22,374 Epoch 6386: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 03:06:22,375 EPOCH 6387
2024-02-11 03:06:38,273 Epoch 6387: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 03:06:38,273 EPOCH 6388
2024-02-11 03:06:54,578 Epoch 6388: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:06:54,578 EPOCH 6389
2024-02-11 03:07:10,780 [Epoch: 6389 Step: 00057500] Batch Recognition Loss:   0.000288 => Gls Tokens per Sec:      577 || Batch Translation Loss:   0.019367 => Txt Tokens per Sec:     1671 || Lr: 0.000050
2024-02-11 03:07:11,030 Epoch 6389: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 03:07:11,030 EPOCH 6390
2024-02-11 03:07:27,320 Epoch 6390: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 03:07:27,322 EPOCH 6391
2024-02-11 03:07:43,548 Epoch 6391: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-11 03:07:43,550 EPOCH 6392
2024-02-11 03:07:59,418 Epoch 6392: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:07:59,418 EPOCH 6393
2024-02-11 03:08:15,469 Epoch 6393: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:08:15,469 EPOCH 6394
2024-02-11 03:08:31,585 Epoch 6394: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:08:31,586 EPOCH 6395
2024-02-11 03:08:47,704 Epoch 6395: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:08:47,706 EPOCH 6396
2024-02-11 03:09:03,884 Epoch 6396: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:09:03,886 EPOCH 6397
2024-02-11 03:09:20,288 Epoch 6397: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 03:09:20,288 EPOCH 6398
2024-02-11 03:09:36,540 Epoch 6398: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:09:36,540 EPOCH 6399
2024-02-11 03:09:52,416 Epoch 6399: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:09:52,416 EPOCH 6400
2024-02-11 03:10:08,373 [Epoch: 6400 Step: 00057600] Batch Recognition Loss:   0.000373 => Gls Tokens per Sec:      666 || Batch Translation Loss:   0.011757 => Txt Tokens per Sec:     1841 || Lr: 0.000050
2024-02-11 03:10:08,375 Epoch 6400: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:10:08,375 EPOCH 6401
2024-02-11 03:10:24,516 Epoch 6401: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 03:10:24,517 EPOCH 6402
2024-02-11 03:10:40,540 Epoch 6402: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:10:40,540 EPOCH 6403
2024-02-11 03:10:56,436 Epoch 6403: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:10:56,437 EPOCH 6404
2024-02-11 03:11:12,231 Epoch 6404: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:11:12,233 EPOCH 6405
2024-02-11 03:11:28,387 Epoch 6405: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:11:28,388 EPOCH 6406
2024-02-11 03:11:44,296 Epoch 6406: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:11:44,297 EPOCH 6407
2024-02-11 03:12:00,762 Epoch 6407: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:12:00,762 EPOCH 6408
2024-02-11 03:12:16,797 Epoch 6408: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:12:16,797 EPOCH 6409
2024-02-11 03:12:32,719 Epoch 6409: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:12:32,721 EPOCH 6410
2024-02-11 03:12:49,011 Epoch 6410: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:12:49,012 EPOCH 6411
2024-02-11 03:13:05,183 Epoch 6411: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:13:05,184 EPOCH 6412
2024-02-11 03:13:05,889 [Epoch: 6412 Step: 00057700] Batch Recognition Loss:   0.000298 => Gls Tokens per Sec:     1816 || Batch Translation Loss:   0.014719 => Txt Tokens per Sec:     5275 || Lr: 0.000050
2024-02-11 03:13:21,722 Epoch 6412: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:13:21,723 EPOCH 6413
2024-02-11 03:13:37,855 Epoch 6413: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:13:37,857 EPOCH 6414
2024-02-11 03:13:54,249 Epoch 6414: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:13:54,249 EPOCH 6415
2024-02-11 03:14:10,706 Epoch 6415: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:14:10,707 EPOCH 6416
2024-02-11 03:14:26,574 Epoch 6416: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:14:26,574 EPOCH 6417
2024-02-11 03:14:42,758 Epoch 6417: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:14:42,759 EPOCH 6418
2024-02-11 03:14:58,826 Epoch 6418: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:14:58,827 EPOCH 6419
2024-02-11 03:15:15,105 Epoch 6419: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:15:15,106 EPOCH 6420
2024-02-11 03:15:31,233 Epoch 6420: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:15:31,233 EPOCH 6421
2024-02-11 03:15:47,514 Epoch 6421: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:15:47,515 EPOCH 6422
2024-02-11 03:16:03,472 Epoch 6422: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:16:03,473 EPOCH 6423
2024-02-11 03:16:07,199 [Epoch: 6423 Step: 00057800] Batch Recognition Loss:   0.000319 => Gls Tokens per Sec:      687 || Batch Translation Loss:   0.012324 => Txt Tokens per Sec:     1719 || Lr: 0.000050
2024-02-11 03:16:19,989 Epoch 6423: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:16:19,989 EPOCH 6424
2024-02-11 03:16:36,077 Epoch 6424: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:16:36,078 EPOCH 6425
2024-02-11 03:16:52,184 Epoch 6425: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:16:52,185 EPOCH 6426
2024-02-11 03:17:08,097 Epoch 6426: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:17:08,098 EPOCH 6427
2024-02-11 03:17:24,514 Epoch 6427: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:17:24,515 EPOCH 6428
2024-02-11 03:17:40,741 Epoch 6428: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:17:40,741 EPOCH 6429
2024-02-11 03:17:56,849 Epoch 6429: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:17:56,850 EPOCH 6430
2024-02-11 03:18:13,065 Epoch 6430: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:18:13,066 EPOCH 6431
2024-02-11 03:18:29,353 Epoch 6431: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:18:29,354 EPOCH 6432
2024-02-11 03:18:45,602 Epoch 6432: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:18:45,603 EPOCH 6433
2024-02-11 03:19:01,726 Epoch 6433: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:19:01,728 EPOCH 6434
2024-02-11 03:19:02,677 [Epoch: 6434 Step: 00057900] Batch Recognition Loss:   0.000136 => Gls Tokens per Sec:     4051 || Batch Translation Loss:   0.010361 => Txt Tokens per Sec:     9171 || Lr: 0.000050
2024-02-11 03:19:17,635 Epoch 6434: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:19:17,636 EPOCH 6435
2024-02-11 03:19:33,737 Epoch 6435: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:19:33,738 EPOCH 6436
2024-02-11 03:19:49,829 Epoch 6436: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:19:49,829 EPOCH 6437
2024-02-11 03:20:06,207 Epoch 6437: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:20:06,208 EPOCH 6438
2024-02-11 03:20:22,641 Epoch 6438: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:20:22,641 EPOCH 6439
2024-02-11 03:20:38,597 Epoch 6439: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:20:38,599 EPOCH 6440
2024-02-11 03:20:54,598 Epoch 6440: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:20:54,600 EPOCH 6441
2024-02-11 03:21:11,024 Epoch 6441: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:21:11,024 EPOCH 6442
2024-02-11 03:21:26,747 Epoch 6442: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:21:26,748 EPOCH 6443
2024-02-11 03:21:42,680 Epoch 6443: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:21:42,681 EPOCH 6444
2024-02-11 03:21:58,548 Epoch 6444: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:21:58,549 EPOCH 6445
2024-02-11 03:22:04,612 [Epoch: 6445 Step: 00058000] Batch Recognition Loss:   0.000141 => Gls Tokens per Sec:      696 || Batch Translation Loss:   0.014358 => Txt Tokens per Sec:     2005 || Lr: 0.000050
2024-02-11 03:23:16,050 Validation result at epoch 6445, step    58000: duration: 71.4370s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.24362	Translation Loss: 107972.07812	PPL: 48255.56641
	Eval Metric: BLEU
	WER 2.33	(DEL: 0.00,	INS: 0.00,	SUB: 2.33)
	BLEU-4 0.00	(BLEU-1: 10.46,	BLEU-2: 2.81,	BLEU-3: 1.00,	BLEU-4: 0.00)
	CHRF 16.70	ROUGE 8.48
2024-02-11 03:23:16,052 Logging Recognition and Translation Outputs
2024-02-11 03:23:16,053 ========================================================================================================================
2024-02-11 03:23:16,053 Logging Sequence: 60_264.00
2024-02-11 03:23:16,053 	Gloss Reference :	A B+C+D+E
2024-02-11 03:23:16,053 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 03:23:16,053 	Gloss Alignment :	         
2024-02-11 03:23:16,053 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 03:23:16,055 	Text Reference  :	plus    do you      know  that a  sex tape of his with two   women had gone viral
2024-02-11 03:23:16,055 	Text Hypothesis :	however at amrapali group paid in the end  of you the  world cup   and talk more 
2024-02-11 03:23:16,055 	Text Alignment  :	S       S  S        S     S    S  S   S       S   S    S     S     S   S    S    
2024-02-11 03:23:16,056 ========================================================================================================================
2024-02-11 03:23:16,056 Logging Sequence: 100_50.00
2024-02-11 03:23:16,056 	Gloss Reference :	A B+C+D+E
2024-02-11 03:23:16,056 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 03:23:16,056 	Gloss Alignment :	         
2024-02-11 03:23:16,056 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 03:23:16,057 	Text Reference  :	with virat kohli as  the   captain
2024-02-11 03:23:16,057 	Text Hypothesis :	let  me    tell  you about it     
2024-02-11 03:23:16,057 	Text Alignment  :	S    S     S     S   S     S      
2024-02-11 03:23:16,057 ========================================================================================================================
2024-02-11 03:23:16,057 Logging Sequence: 137_44.00
2024-02-11 03:23:16,058 	Gloss Reference :	A B+C+D+E
2024-02-11 03:23:16,058 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 03:23:16,058 	Gloss Alignment :	         
2024-02-11 03:23:16,058 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 03:23:16,059 	Text Reference  :	let me tell you the rules that qatar has announced for the       fans  travelling for the   world cup  
2024-02-11 03:23:16,059 	Text Hypothesis :	*** ** **** *** *** ***** **** ***** *** ********* *** meanwhile there was        sad faced each  other
2024-02-11 03:23:16,060 	Text Alignment  :	D   D  D    D   D   D     D    D     D   D         D   S         S     S          S   S     S     S    
2024-02-11 03:23:16,060 ========================================================================================================================
2024-02-11 03:23:16,060 Logging Sequence: 58_27.00
2024-02-11 03:23:16,060 	Gloss Reference :	A B+C+D+E
2024-02-11 03:23:16,060 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 03:23:16,060 	Gloss Alignment :	         
2024-02-11 03:23:16,060 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 03:23:16,062 	Text Reference  :	***** the 19th asian games 2022 were to     be held in **** ** *** ** ** *** hangzhou china 
2024-02-11 03:23:16,062 	Text Hypothesis :	india has won  the   first time in   should be held in them by due to on his 50       medals
2024-02-11 03:23:16,062 	Text Alignment  :	I     S   S    S     S     S    S    S                 I    I  I   I  I  I   S        S     
2024-02-11 03:23:16,062 ========================================================================================================================
2024-02-11 03:23:16,062 Logging Sequence: 75_255.00
2024-02-11 03:23:16,063 	Gloss Reference :	A B+C+D+E  
2024-02-11 03:23:16,063 	Gloss Hypothesis:	A B+C+D+E+D
2024-02-11 03:23:16,063 	Gloss Alignment :	  S        
2024-02-11 03:23:16,063 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 03:23:16,064 	Text Reference  :	we miss our baby boy with this ronaldo' total baby   count has     reached 5 with  2  boys 3   girls
2024-02-11 03:23:16,064 	Text Hypothesis :	** **** *** **** *** **** **** ******** later rooney then  invited the     3 girls to his  vip booth
2024-02-11 03:23:16,065 	Text Alignment  :	D  D    D   D    D   D    D    D        S     S      S     S       S       S S     S  S    S   S    
2024-02-11 03:23:16,065 ========================================================================================================================
2024-02-11 03:23:27,794 Epoch 6445: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:23:27,795 EPOCH 6446
2024-02-11 03:23:44,366 Epoch 6446: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:23:44,366 EPOCH 6447
2024-02-11 03:24:00,467 Epoch 6447: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:24:00,468 EPOCH 6448
2024-02-11 03:24:16,579 Epoch 6448: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:24:16,580 EPOCH 6449
2024-02-11 03:24:32,948 Epoch 6449: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:24:32,949 EPOCH 6450
2024-02-11 03:24:49,238 Epoch 6450: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:24:49,239 EPOCH 6451
2024-02-11 03:25:05,150 Epoch 6451: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:25:05,151 EPOCH 6452
2024-02-11 03:25:20,906 Epoch 6452: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:25:20,907 EPOCH 6453
2024-02-11 03:25:36,861 Epoch 6453: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:25:36,862 EPOCH 6454
2024-02-11 03:25:52,984 Epoch 6454: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:25:52,984 EPOCH 6455
2024-02-11 03:26:08,838 Epoch 6455: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:26:08,838 EPOCH 6456
2024-02-11 03:26:20,337 [Epoch: 6456 Step: 00058100] Batch Recognition Loss:   0.000446 => Gls Tokens per Sec:      478 || Batch Translation Loss:   0.013390 => Txt Tokens per Sec:     1361 || Lr: 0.000050
2024-02-11 03:26:24,959 Epoch 6456: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:26:24,960 EPOCH 6457
2024-02-11 03:26:41,050 Epoch 6457: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:26:41,051 EPOCH 6458
2024-02-11 03:26:57,116 Epoch 6458: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:26:57,117 EPOCH 6459
2024-02-11 03:27:13,111 Epoch 6459: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:27:13,112 EPOCH 6460
2024-02-11 03:27:29,068 Epoch 6460: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:27:29,068 EPOCH 6461
2024-02-11 03:27:45,105 Epoch 6461: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:27:45,106 EPOCH 6462
2024-02-11 03:28:01,534 Epoch 6462: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:28:01,535 EPOCH 6463
2024-02-11 03:28:17,509 Epoch 6463: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:28:17,509 EPOCH 6464
2024-02-11 03:28:33,618 Epoch 6464: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:28:33,618 EPOCH 6465
2024-02-11 03:28:49,567 Epoch 6465: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 03:28:49,568 EPOCH 6466
2024-02-11 03:29:06,016 Epoch 6466: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:29:06,016 EPOCH 6467
2024-02-11 03:29:16,783 [Epoch: 6467 Step: 00058200] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:      713 || Batch Translation Loss:   0.009550 => Txt Tokens per Sec:     2031 || Lr: 0.000050
2024-02-11 03:29:22,319 Epoch 6467: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:29:22,320 EPOCH 6468
2024-02-11 03:29:38,254 Epoch 6468: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 03:29:38,255 EPOCH 6469
2024-02-11 03:29:54,493 Epoch 6469: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:29:54,493 EPOCH 6470
2024-02-11 03:30:10,690 Epoch 6470: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:30:10,690 EPOCH 6471
2024-02-11 03:30:26,550 Epoch 6471: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:30:26,551 EPOCH 6472
2024-02-11 03:30:42,564 Epoch 6472: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:30:42,564 EPOCH 6473
2024-02-11 03:30:58,784 Epoch 6473: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:30:58,784 EPOCH 6474
2024-02-11 03:31:15,000 Epoch 6474: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:31:15,001 EPOCH 6475
2024-02-11 03:31:30,733 Epoch 6475: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:31:30,734 EPOCH 6476
2024-02-11 03:31:46,984 Epoch 6476: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:31:46,984 EPOCH 6477
2024-02-11 03:32:03,142 Epoch 6477: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:32:03,143 EPOCH 6478
2024-02-11 03:32:13,878 [Epoch: 6478 Step: 00058300] Batch Recognition Loss:   0.000163 => Gls Tokens per Sec:      835 || Batch Translation Loss:   0.005644 => Txt Tokens per Sec:     2252 || Lr: 0.000050
2024-02-11 03:32:18,916 Epoch 6478: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:32:18,916 EPOCH 6479
2024-02-11 03:32:34,956 Epoch 6479: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 03:32:34,956 EPOCH 6480
2024-02-11 03:32:51,436 Epoch 6480: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 03:32:51,437 EPOCH 6481
2024-02-11 03:33:07,382 Epoch 6481: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:33:07,382 EPOCH 6482
2024-02-11 03:33:23,460 Epoch 6482: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:33:23,460 EPOCH 6483
2024-02-11 03:33:39,605 Epoch 6483: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:33:39,606 EPOCH 6484
2024-02-11 03:33:56,129 Epoch 6484: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:33:56,129 EPOCH 6485
2024-02-11 03:34:12,805 Epoch 6485: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:34:12,805 EPOCH 6486
2024-02-11 03:34:29,029 Epoch 6486: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:34:29,029 EPOCH 6487
2024-02-11 03:34:45,331 Epoch 6487: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:34:45,332 EPOCH 6488
2024-02-11 03:35:01,178 Epoch 6488: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:35:01,179 EPOCH 6489
2024-02-11 03:35:16,494 [Epoch: 6489 Step: 00058400] Batch Recognition Loss:   0.000355 => Gls Tokens per Sec:      610 || Batch Translation Loss:   0.005582 => Txt Tokens per Sec:     1667 || Lr: 0.000050
2024-02-11 03:35:17,161 Epoch 6489: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:35:17,162 EPOCH 6490
2024-02-11 03:35:33,137 Epoch 6490: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:35:33,137 EPOCH 6491
2024-02-11 03:35:49,052 Epoch 6491: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:35:49,052 EPOCH 6492
2024-02-11 03:36:04,991 Epoch 6492: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:36:04,991 EPOCH 6493
2024-02-11 03:36:20,647 Epoch 6493: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:36:20,647 EPOCH 6494
2024-02-11 03:36:36,861 Epoch 6494: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 03:36:36,861 EPOCH 6495
2024-02-11 03:36:52,324 Epoch 6495: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 03:36:52,325 EPOCH 6496
2024-02-11 03:37:08,349 Epoch 6496: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 03:37:08,349 EPOCH 6497
2024-02-11 03:37:24,414 Epoch 6497: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 03:37:24,415 EPOCH 6498
2024-02-11 03:37:40,461 Epoch 6498: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 03:37:40,462 EPOCH 6499
2024-02-11 03:37:56,519 Epoch 6499: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 03:37:56,520 EPOCH 6500
2024-02-11 03:38:12,376 [Epoch: 6500 Step: 00058500] Batch Recognition Loss:   0.001081 => Gls Tokens per Sec:      670 || Batch Translation Loss:   0.016626 => Txt Tokens per Sec:     1853 || Lr: 0.000050
2024-02-11 03:38:12,377 Epoch 6500: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 03:38:12,377 EPOCH 6501
2024-02-11 03:38:28,276 Epoch 6501: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 03:38:28,277 EPOCH 6502
2024-02-11 03:38:44,526 Epoch 6502: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:38:44,527 EPOCH 6503
2024-02-11 03:39:00,544 Epoch 6503: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:39:00,545 EPOCH 6504
2024-02-11 03:39:16,422 Epoch 6504: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:39:16,423 EPOCH 6505
2024-02-11 03:39:32,310 Epoch 6505: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:39:32,310 EPOCH 6506
2024-02-11 03:39:48,371 Epoch 6506: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:39:48,372 EPOCH 6507
2024-02-11 03:40:04,583 Epoch 6507: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:40:04,583 EPOCH 6508
2024-02-11 03:40:20,632 Epoch 6508: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:40:20,632 EPOCH 6509
2024-02-11 03:40:36,798 Epoch 6509: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:40:36,799 EPOCH 6510
2024-02-11 03:40:52,582 Epoch 6510: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:40:52,583 EPOCH 6511
2024-02-11 03:41:08,648 Epoch 6511: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:41:08,649 EPOCH 6512
2024-02-11 03:41:09,536 [Epoch: 6512 Step: 00058600] Batch Recognition Loss:   0.000260 => Gls Tokens per Sec:     1446 || Batch Translation Loss:   0.014850 => Txt Tokens per Sec:     4328 || Lr: 0.000050
2024-02-11 03:41:24,789 Epoch 6512: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:41:24,789 EPOCH 6513
2024-02-11 03:41:40,935 Epoch 6513: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 03:41:40,935 EPOCH 6514
2024-02-11 03:41:56,969 Epoch 6514: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:41:56,970 EPOCH 6515
2024-02-11 03:42:13,407 Epoch 6515: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 03:42:13,408 EPOCH 6516
2024-02-11 03:42:29,476 Epoch 6516: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 03:42:29,476 EPOCH 6517
2024-02-11 03:42:45,475 Epoch 6517: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:42:45,476 EPOCH 6518
2024-02-11 03:43:01,626 Epoch 6518: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 03:43:01,626 EPOCH 6519
2024-02-11 03:43:17,676 Epoch 6519: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 03:43:17,677 EPOCH 6520
2024-02-11 03:43:33,623 Epoch 6520: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 03:43:33,624 EPOCH 6521
2024-02-11 03:43:49,587 Epoch 6521: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 03:43:49,587 EPOCH 6522
2024-02-11 03:44:05,804 Epoch 6522: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 03:44:05,804 EPOCH 6523
2024-02-11 03:44:12,153 [Epoch: 6523 Step: 00058700] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:      403 || Batch Translation Loss:   0.013984 => Txt Tokens per Sec:     1251 || Lr: 0.000050
2024-02-11 03:44:21,724 Epoch 6523: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 03:44:21,724 EPOCH 6524
2024-02-11 03:44:37,642 Epoch 6524: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 03:44:37,643 EPOCH 6525
2024-02-11 03:44:53,723 Epoch 6525: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 03:44:53,723 EPOCH 6526
2024-02-11 03:45:09,796 Epoch 6526: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 03:45:09,797 EPOCH 6527
2024-02-11 03:45:25,998 Epoch 6527: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-11 03:45:25,999 EPOCH 6528
2024-02-11 03:45:42,002 Epoch 6528: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-11 03:45:42,003 EPOCH 6529
2024-02-11 03:45:57,952 Epoch 6529: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.71 
2024-02-11 03:45:57,953 EPOCH 6530
2024-02-11 03:46:14,104 Epoch 6530: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-11 03:46:14,104 EPOCH 6531
2024-02-11 03:46:30,040 Epoch 6531: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.40 
2024-02-11 03:46:30,041 EPOCH 6532
2024-02-11 03:46:45,819 Epoch 6532: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-11 03:46:45,819 EPOCH 6533
2024-02-11 03:47:02,116 Epoch 6533: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-11 03:47:02,116 EPOCH 6534
2024-02-11 03:47:05,716 [Epoch: 6534 Step: 00058800] Batch Recognition Loss:   0.000609 => Gls Tokens per Sec:     1067 || Batch Translation Loss:   0.026888 => Txt Tokens per Sec:     2580 || Lr: 0.000050
2024-02-11 03:47:18,113 Epoch 6534: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-11 03:47:18,113 EPOCH 6535
2024-02-11 03:47:34,198 Epoch 6535: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 03:47:34,198 EPOCH 6536
2024-02-11 03:47:50,037 Epoch 6536: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-11 03:47:50,038 EPOCH 6537
2024-02-11 03:48:06,132 Epoch 6537: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 03:48:06,133 EPOCH 6538
2024-02-11 03:48:22,386 Epoch 6538: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 03:48:22,386 EPOCH 6539
2024-02-11 03:48:38,237 Epoch 6539: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 03:48:38,237 EPOCH 6540
2024-02-11 03:48:54,103 Epoch 6540: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 03:48:54,104 EPOCH 6541
2024-02-11 03:49:10,371 Epoch 6541: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 03:49:10,371 EPOCH 6542
2024-02-11 03:49:26,424 Epoch 6542: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 03:49:26,425 EPOCH 6543
2024-02-11 03:49:42,304 Epoch 6543: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-11 03:49:42,304 EPOCH 6544
2024-02-11 03:49:58,242 Epoch 6544: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 03:49:58,242 EPOCH 6545
2024-02-11 03:50:03,891 [Epoch: 6545 Step: 00058900] Batch Recognition Loss:   0.000335 => Gls Tokens per Sec:      747 || Batch Translation Loss:   0.027856 => Txt Tokens per Sec:     1909 || Lr: 0.000050
2024-02-11 03:50:14,095 Epoch 6545: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 03:50:14,096 EPOCH 6546
2024-02-11 03:50:30,141 Epoch 6546: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 03:50:30,142 EPOCH 6547
2024-02-11 03:50:46,141 Epoch 6547: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 03:50:46,141 EPOCH 6548
2024-02-11 03:51:02,080 Epoch 6548: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 03:51:02,081 EPOCH 6549
2024-02-11 03:51:18,167 Epoch 6549: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 03:51:18,168 EPOCH 6550
2024-02-11 03:51:34,605 Epoch 6550: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 03:51:34,606 EPOCH 6551
2024-02-11 03:51:50,642 Epoch 6551: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 03:51:50,642 EPOCH 6552
2024-02-11 03:52:06,712 Epoch 6552: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 03:52:06,712 EPOCH 6553
2024-02-11 03:52:22,767 Epoch 6553: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:52:22,768 EPOCH 6554
2024-02-11 03:52:38,592 Epoch 6554: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:52:38,592 EPOCH 6555
2024-02-11 03:52:54,662 Epoch 6555: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 03:52:54,662 EPOCH 6556
2024-02-11 03:52:59,841 [Epoch: 6556 Step: 00059000] Batch Recognition Loss:   0.000553 => Gls Tokens per Sec:     1236 || Batch Translation Loss:   0.012934 => Txt Tokens per Sec:     3404 || Lr: 0.000050
2024-02-11 03:53:10,734 Epoch 6556: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:53:10,734 EPOCH 6557
2024-02-11 03:53:26,850 Epoch 6557: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:53:26,850 EPOCH 6558
2024-02-11 03:53:42,686 Epoch 6558: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 03:53:42,686 EPOCH 6559
2024-02-11 03:53:58,823 Epoch 6559: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-11 03:53:58,824 EPOCH 6560
2024-02-11 03:54:15,159 Epoch 6560: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:54:15,160 EPOCH 6561
2024-02-11 03:54:31,384 Epoch 6561: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 03:54:31,385 EPOCH 6562
2024-02-11 03:54:47,555 Epoch 6562: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 03:54:47,555 EPOCH 6563
2024-02-11 03:55:03,667 Epoch 6563: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:55:03,668 EPOCH 6564
2024-02-11 03:55:19,912 Epoch 6564: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:55:19,913 EPOCH 6565
2024-02-11 03:55:36,101 Epoch 6565: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:55:36,102 EPOCH 6566
2024-02-11 03:55:52,342 Epoch 6566: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:55:52,343 EPOCH 6567
2024-02-11 03:56:02,824 [Epoch: 6567 Step: 00059100] Batch Recognition Loss:   0.000181 => Gls Tokens per Sec:      733 || Batch Translation Loss:   0.006935 => Txt Tokens per Sec:     1973 || Lr: 0.000050
2024-02-11 03:56:08,263 Epoch 6567: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:56:08,264 EPOCH 6568
2024-02-11 03:56:24,499 Epoch 6568: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 03:56:24,500 EPOCH 6569
2024-02-11 03:56:40,777 Epoch 6569: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 03:56:40,778 EPOCH 6570
2024-02-11 03:56:57,000 Epoch 6570: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 03:56:57,000 EPOCH 6571
2024-02-11 03:57:13,188 Epoch 6571: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 03:57:13,189 EPOCH 6572
2024-02-11 03:57:29,106 Epoch 6572: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 03:57:29,106 EPOCH 6573
2024-02-11 03:57:45,097 Epoch 6573: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 03:57:45,097 EPOCH 6574
2024-02-11 03:58:01,269 Epoch 6574: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 03:58:01,269 EPOCH 6575
2024-02-11 03:58:17,383 Epoch 6575: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:58:17,383 EPOCH 6576
2024-02-11 03:58:33,378 Epoch 6576: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:58:33,379 EPOCH 6577
2024-02-11 03:58:49,209 Epoch 6577: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:58:49,209 EPOCH 6578
2024-02-11 03:59:04,407 [Epoch: 6578 Step: 00059200] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:      530 || Batch Translation Loss:   0.016584 => Txt Tokens per Sec:     1467 || Lr: 0.000050
2024-02-11 03:59:05,587 Epoch 6578: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-11 03:59:05,587 EPOCH 6579
2024-02-11 03:59:21,817 Epoch 6579: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 03:59:21,818 EPOCH 6580
2024-02-11 03:59:37,568 Epoch 6580: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 03:59:37,568 EPOCH 6581
2024-02-11 03:59:53,752 Epoch 6581: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 03:59:53,752 EPOCH 6582
2024-02-11 04:00:09,925 Epoch 6582: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:00:09,926 EPOCH 6583
2024-02-11 04:00:25,938 Epoch 6583: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:00:25,939 EPOCH 6584
2024-02-11 04:00:41,862 Epoch 6584: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:00:41,862 EPOCH 6585
2024-02-11 04:00:58,083 Epoch 6585: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:00:58,084 EPOCH 6586
2024-02-11 04:01:14,033 Epoch 6586: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:01:14,033 EPOCH 6587
2024-02-11 04:01:30,270 Epoch 6587: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:01:30,270 EPOCH 6588
2024-02-11 04:01:46,303 Epoch 6588: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:01:46,303 EPOCH 6589
2024-02-11 04:02:01,909 [Epoch: 6589 Step: 00059300] Batch Recognition Loss:   0.000615 => Gls Tokens per Sec:      599 || Batch Translation Loss:   0.005581 => Txt Tokens per Sec:     1736 || Lr: 0.000050
2024-02-11 04:02:02,251 Epoch 6589: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:02:02,251 EPOCH 6590
2024-02-11 04:02:18,221 Epoch 6590: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:02:18,222 EPOCH 6591
2024-02-11 04:02:34,253 Epoch 6591: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:02:34,254 EPOCH 6592
2024-02-11 04:02:50,555 Epoch 6592: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-11 04:02:50,556 EPOCH 6593
2024-02-11 04:03:06,297 Epoch 6593: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:03:06,298 EPOCH 6594
2024-02-11 04:03:22,203 Epoch 6594: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:03:22,204 EPOCH 6595
2024-02-11 04:03:38,402 Epoch 6595: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:03:38,403 EPOCH 6596
2024-02-11 04:03:54,451 Epoch 6596: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:03:54,452 EPOCH 6597
2024-02-11 04:04:10,589 Epoch 6597: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 04:04:10,590 EPOCH 6598
2024-02-11 04:04:26,952 Epoch 6598: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 04:04:26,953 EPOCH 6599
2024-02-11 04:04:42,680 Epoch 6599: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 04:04:42,681 EPOCH 6600
2024-02-11 04:04:58,656 [Epoch: 6600 Step: 00059400] Batch Recognition Loss:   0.001174 => Gls Tokens per Sec:      665 || Batch Translation Loss:   0.005836 => Txt Tokens per Sec:     1839 || Lr: 0.000050
2024-02-11 04:04:58,657 Epoch 6600: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 04:04:58,657 EPOCH 6601
2024-02-11 04:05:14,934 Epoch 6601: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 04:05:14,934 EPOCH 6602
2024-02-11 04:05:31,009 Epoch 6602: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 04:05:31,009 EPOCH 6603
2024-02-11 04:05:47,462 Epoch 6603: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 04:05:47,463 EPOCH 6604
2024-02-11 04:06:03,750 Epoch 6604: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 04:06:03,751 EPOCH 6605
2024-02-11 04:06:19,608 Epoch 6605: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 04:06:19,608 EPOCH 6606
2024-02-11 04:06:35,502 Epoch 6606: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 04:06:35,502 EPOCH 6607
2024-02-11 04:06:51,439 Epoch 6607: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:06:51,440 EPOCH 6608
2024-02-11 04:07:07,698 Epoch 6608: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:07:07,698 EPOCH 6609
2024-02-11 04:07:23,787 Epoch 6609: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:07:23,787 EPOCH 6610
2024-02-11 04:07:39,766 Epoch 6610: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:07:39,766 EPOCH 6611
2024-02-11 04:07:55,695 Epoch 6611: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:07:55,696 EPOCH 6612
2024-02-11 04:07:56,066 [Epoch: 6612 Step: 00059500] Batch Recognition Loss:   0.000121 => Gls Tokens per Sec:     3469 || Batch Translation Loss:   0.009290 => Txt Tokens per Sec:     8913 || Lr: 0.000050
2024-02-11 04:08:11,805 Epoch 6612: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:08:11,805 EPOCH 6613
2024-02-11 04:08:27,779 Epoch 6613: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:08:27,779 EPOCH 6614
2024-02-11 04:08:43,796 Epoch 6614: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:08:43,797 EPOCH 6615
2024-02-11 04:09:00,151 Epoch 6615: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:09:00,152 EPOCH 6616
2024-02-11 04:09:16,265 Epoch 6616: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 04:09:16,266 EPOCH 6617
2024-02-11 04:09:32,273 Epoch 6617: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 04:09:32,273 EPOCH 6618
2024-02-11 04:09:48,240 Epoch 6618: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 04:09:48,241 EPOCH 6619
2024-02-11 04:10:04,551 Epoch 6619: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 04:10:04,552 EPOCH 6620
2024-02-11 04:10:20,490 Epoch 6620: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-11 04:10:20,490 EPOCH 6621
2024-02-11 04:10:36,447 Epoch 6621: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-11 04:10:36,448 EPOCH 6622
2024-02-11 04:10:52,337 Epoch 6622: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 04:10:52,338 EPOCH 6623
2024-02-11 04:10:59,918 [Epoch: 6623 Step: 00059600] Batch Recognition Loss:   0.000378 => Gls Tokens per Sec:      219 || Batch Translation Loss:   0.009322 => Txt Tokens per Sec:      720 || Lr: 0.000050
2024-02-11 04:11:08,638 Epoch 6623: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-11 04:11:08,638 EPOCH 6624
2024-02-11 04:11:24,681 Epoch 6624: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 04:11:24,682 EPOCH 6625
2024-02-11 04:11:40,652 Epoch 6625: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 04:11:40,652 EPOCH 6626
2024-02-11 04:11:56,639 Epoch 6626: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 04:11:56,639 EPOCH 6627
2024-02-11 04:12:12,809 Epoch 6627: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-11 04:12:12,810 EPOCH 6628
2024-02-11 04:12:28,796 Epoch 6628: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-11 04:12:28,797 EPOCH 6629
2024-02-11 04:12:44,986 Epoch 6629: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 04:12:44,987 EPOCH 6630
2024-02-11 04:13:00,913 Epoch 6630: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-11 04:13:00,913 EPOCH 6631
2024-02-11 04:13:16,830 Epoch 6631: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 04:13:16,831 EPOCH 6632
2024-02-11 04:13:32,794 Epoch 6632: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 04:13:32,795 EPOCH 6633
2024-02-11 04:13:48,650 Epoch 6633: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 04:13:48,651 EPOCH 6634
2024-02-11 04:13:55,192 [Epoch: 6634 Step: 00059700] Batch Recognition Loss:   0.001221 => Gls Tokens per Sec:      587 || Batch Translation Loss:   0.023353 => Txt Tokens per Sec:     1597 || Lr: 0.000050
2024-02-11 04:14:04,730 Epoch 6634: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-11 04:14:04,731 EPOCH 6635
2024-02-11 04:14:20,554 Epoch 6635: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 04:14:20,554 EPOCH 6636
2024-02-11 04:14:36,966 Epoch 6636: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-11 04:14:36,966 EPOCH 6637
2024-02-11 04:14:53,137 Epoch 6637: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-11 04:14:53,138 EPOCH 6638
2024-02-11 04:15:09,083 Epoch 6638: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-11 04:15:09,083 EPOCH 6639
2024-02-11 04:15:25,057 Epoch 6639: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.06 
2024-02-11 04:15:25,057 EPOCH 6640
2024-02-11 04:15:41,073 Epoch 6640: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.52 
2024-02-11 04:15:41,073 EPOCH 6641
2024-02-11 04:15:57,367 Epoch 6641: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-11 04:15:57,368 EPOCH 6642
2024-02-11 04:16:13,561 Epoch 6642: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.23 
2024-02-11 04:16:13,561 EPOCH 6643
2024-02-11 04:16:29,616 Epoch 6643: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.33 
2024-02-11 04:16:29,617 EPOCH 6644
2024-02-11 04:16:45,782 Epoch 6644: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.57 
2024-02-11 04:16:45,782 EPOCH 6645
2024-02-11 04:16:53,089 [Epoch: 6645 Step: 00059800] Batch Recognition Loss:   0.002077 => Gls Tokens per Sec:      701 || Batch Translation Loss:   0.061749 => Txt Tokens per Sec:     2124 || Lr: 0.000050
2024-02-11 04:17:01,690 Epoch 6645: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-11 04:17:01,691 EPOCH 6646
2024-02-11 04:17:17,779 Epoch 6646: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-11 04:17:17,780 EPOCH 6647
2024-02-11 04:17:34,107 Epoch 6647: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-11 04:17:34,108 EPOCH 6648
2024-02-11 04:17:50,131 Epoch 6648: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-11 04:17:50,132 EPOCH 6649
2024-02-11 04:18:06,634 Epoch 6649: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-11 04:18:06,635 EPOCH 6650
2024-02-11 04:18:22,861 Epoch 6650: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-11 04:18:22,862 EPOCH 6651
2024-02-11 04:18:38,891 Epoch 6651: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 04:18:38,892 EPOCH 6652
2024-02-11 04:18:55,119 Epoch 6652: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-11 04:18:55,120 EPOCH 6653
2024-02-11 04:19:11,207 Epoch 6653: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-11 04:19:11,207 EPOCH 6654
2024-02-11 04:19:27,551 Epoch 6654: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 04:19:27,552 EPOCH 6655
2024-02-11 04:19:43,717 Epoch 6655: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 04:19:43,718 EPOCH 6656
2024-02-11 04:19:58,037 [Epoch: 6656 Step: 00059900] Batch Recognition Loss:   0.000715 => Gls Tokens per Sec:      384 || Batch Translation Loss:   0.014322 => Txt Tokens per Sec:     1096 || Lr: 0.000050
2024-02-11 04:20:00,156 Epoch 6656: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 04:20:00,156 EPOCH 6657
2024-02-11 04:20:16,245 Epoch 6657: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-11 04:20:16,246 EPOCH 6658
2024-02-11 04:20:32,527 Epoch 6658: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:20:32,528 EPOCH 6659
2024-02-11 04:20:48,421 Epoch 6659: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 04:20:48,422 EPOCH 6660
2024-02-11 04:21:04,491 Epoch 6660: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-11 04:21:04,491 EPOCH 6661
2024-02-11 04:21:20,454 Epoch 6661: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:21:20,455 EPOCH 6662
2024-02-11 04:21:36,760 Epoch 6662: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:21:36,761 EPOCH 6663
2024-02-11 04:21:52,768 Epoch 6663: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:21:52,769 EPOCH 6664
2024-02-11 04:22:08,512 Epoch 6664: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:22:08,513 EPOCH 6665
2024-02-11 04:22:24,513 Epoch 6665: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:22:24,513 EPOCH 6666
2024-02-11 04:22:40,600 Epoch 6666: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:22:40,600 EPOCH 6667
2024-02-11 04:22:48,285 [Epoch: 6667 Step: 00060000] Batch Recognition Loss:   0.000245 => Gls Tokens per Sec:     1000 || Batch Translation Loss:   0.018150 => Txt Tokens per Sec:     2612 || Lr: 0.000050
2024-02-11 04:24:00,774 Validation result at epoch 6667, step    60000: duration: 72.4885s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.24266	Translation Loss: 108957.35938	PPL: 53245.90234
	Eval Metric: BLEU
	WER 2.26	(DEL: 0.00,	INS: 0.00,	SUB: 2.26)
	BLEU-4 0.44	(BLEU-1: 9.40,	BLEU-2: 2.73,	BLEU-3: 0.95,	BLEU-4: 0.44)
	CHRF 16.82	ROUGE 8.29
2024-02-11 04:24:00,777 Logging Recognition and Translation Outputs
2024-02-11 04:24:00,777 ========================================================================================================================
2024-02-11 04:24:00,777 Logging Sequence: 75_58.00
2024-02-11 04:24:00,777 	Gloss Reference :	A B+C+D+E
2024-02-11 04:24:00,777 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 04:24:00,778 	Gloss Alignment :	         
2024-02-11 04:24:00,778 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 04:24:00,779 	Text Reference  :	it seems like he like to date women and does not want to get married people seem     to      respect his   choices      
2024-02-11 04:24:00,779 	Text Hypothesis :	it ***** **** ** **** ** **** ***** *** **** *** **** ** was a       sudden decision without any     prior communication
2024-02-11 04:24:00,779 	Text Alignment  :	   D     D    D  D    D  D    D     D   D    D   D    D  S   S       S      S        S       S       S     S            
2024-02-11 04:24:00,779 ========================================================================================================================
2024-02-11 04:24:00,779 Logging Sequence: 152_113.00
2024-02-11 04:24:00,780 	Gloss Reference :	A B+C+D+E
2024-02-11 04:24:00,780 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 04:24:00,780 	Gloss Alignment :	         
2024-02-11 04:24:00,780 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 04:24:00,781 	Text Reference  :	******* ****** indians hoping  for   a  victory were distraught at  the defeat 
2024-02-11 04:24:00,781 	Text Hypothesis :	another indian deaf    cricket match kl rahul   was  bowled     for the england
2024-02-11 04:24:00,781 	Text Alignment  :	I       I      S       S       S     S  S       S    S          S       S      
2024-02-11 04:24:00,781 ========================================================================================================================
2024-02-11 04:24:00,782 Logging Sequence: 176_41.00
2024-02-11 04:24:00,782 	Gloss Reference :	A B+C+D+E
2024-02-11 04:24:00,782 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 04:24:00,782 	Gloss Alignment :	         
2024-02-11 04:24:00,782 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 04:24:00,783 	Text Reference  :	****** **** dahiya did not  loose   hope   and put up  a    strong fight
2024-02-11 04:24:00,783 	Text Hypothesis :	people were then   on  this morning attack no  one was very sad    news 
2024-02-11 04:24:00,784 	Text Alignment  :	I      I    S      S   S    S       S      S   S   S   S    S      S    
2024-02-11 04:24:00,784 ========================================================================================================================
2024-02-11 04:24:00,784 Logging Sequence: 77_190.00
2024-02-11 04:24:00,784 	Gloss Reference :	A B+C+D+E
2024-02-11 04:24:00,784 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 04:24:00,784 	Gloss Alignment :	         
2024-02-11 04:24:00,784 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 04:24:00,786 	Text Reference  :	*** *** ***** *** **** there are   many batsmen who   have scrored 36     runs     in  6   balls      
2024-02-11 04:24:00,786 	Text Hypothesis :	she now taken the 2020 was   taken and  sri     lanka has  been    tested positive for the coronavirus
2024-02-11 04:24:00,786 	Text Alignment  :	I   I   I     I   I    S     S     S    S       S     S    S       S      S        S   S   S          
2024-02-11 04:24:00,786 ========================================================================================================================
2024-02-11 04:24:00,786 Logging Sequence: 155_170.00
2024-02-11 04:24:00,787 	Gloss Reference :	A B+C+D+E
2024-02-11 04:24:00,787 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 04:24:00,787 	Gloss Alignment :	         
2024-02-11 04:24:00,787 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 04:24:00,788 	Text Reference  :	india lost the matches   and could not secure a  place in the semi final     
2024-02-11 04:24:00,788 	Text Hypothesis :	one   day  is  cancelled and ***** *** wanted to lead  to my  t20  tournament
2024-02-11 04:24:00,788 	Text Alignment  :	S     S    S   S             D     D   S      S  S     S  S   S    S         
2024-02-11 04:24:00,789 ========================================================================================================================
2024-02-11 04:24:09,107 Epoch 6667: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:24:09,107 EPOCH 6668
2024-02-11 04:24:25,984 Epoch 6668: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:24:25,985 EPOCH 6669
2024-02-11 04:24:42,249 Epoch 6669: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:24:42,249 EPOCH 6670
2024-02-11 04:24:58,205 Epoch 6670: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.09 
2024-02-11 04:24:58,205 EPOCH 6671
2024-02-11 04:25:14,292 Epoch 6671: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:25:14,293 EPOCH 6672
2024-02-11 04:25:30,349 Epoch 6672: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:25:30,350 EPOCH 6673
2024-02-11 04:25:46,663 Epoch 6673: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:25:46,663 EPOCH 6674
2024-02-11 04:26:02,841 Epoch 6674: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:26:02,841 EPOCH 6675
2024-02-11 04:26:18,772 Epoch 6675: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:26:18,773 EPOCH 6676
2024-02-11 04:26:35,158 Epoch 6676: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:26:35,159 EPOCH 6677
2024-02-11 04:26:51,193 Epoch 6677: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:26:51,193 EPOCH 6678
2024-02-11 04:27:00,843 [Epoch: 6678 Step: 00060100] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:      835 || Batch Translation Loss:   0.005710 => Txt Tokens per Sec:     2191 || Lr: 0.000050
2024-02-11 04:27:07,262 Epoch 6678: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:27:07,262 EPOCH 6679
2024-02-11 04:27:23,386 Epoch 6679: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:27:23,386 EPOCH 6680
2024-02-11 04:27:39,189 Epoch 6680: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:27:39,190 EPOCH 6681
2024-02-11 04:27:55,012 Epoch 6681: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:27:55,013 EPOCH 6682
2024-02-11 04:28:11,120 Epoch 6682: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:28:11,121 EPOCH 6683
2024-02-11 04:28:27,342 Epoch 6683: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:28:27,343 EPOCH 6684
2024-02-11 04:28:43,334 Epoch 6684: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:28:43,334 EPOCH 6685
2024-02-11 04:28:59,500 Epoch 6685: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:28:59,500 EPOCH 6686
2024-02-11 04:29:15,703 Epoch 6686: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:29:15,704 EPOCH 6687
2024-02-11 04:29:31,829 Epoch 6687: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:29:31,829 EPOCH 6688
2024-02-11 04:29:48,009 Epoch 6688: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:29:48,010 EPOCH 6689
2024-02-11 04:30:03,609 [Epoch: 6689 Step: 00060200] Batch Recognition Loss:   0.000150 => Gls Tokens per Sec:      599 || Batch Translation Loss:   0.013675 => Txt Tokens per Sec:     1737 || Lr: 0.000050
2024-02-11 04:30:03,856 Epoch 6689: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:30:03,856 EPOCH 6690
2024-02-11 04:30:19,704 Epoch 6690: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:30:19,705 EPOCH 6691
2024-02-11 04:30:36,247 Epoch 6691: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:30:36,248 EPOCH 6692
2024-02-11 04:30:52,782 Epoch 6692: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:30:52,782 EPOCH 6693
2024-02-11 04:31:08,974 Epoch 6693: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:31:08,975 EPOCH 6694
2024-02-11 04:31:25,408 Epoch 6694: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:31:25,408 EPOCH 6695
2024-02-11 04:31:41,472 Epoch 6695: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:31:41,473 EPOCH 6696
2024-02-11 04:31:57,669 Epoch 6696: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:31:57,670 EPOCH 6697
2024-02-11 04:32:13,738 Epoch 6697: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:32:13,739 EPOCH 6698
2024-02-11 04:32:30,283 Epoch 6698: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:32:30,284 EPOCH 6699
2024-02-11 04:32:46,253 Epoch 6699: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:32:46,253 EPOCH 6700
2024-02-11 04:33:02,049 [Epoch: 6700 Step: 00060300] Batch Recognition Loss:   0.000097 => Gls Tokens per Sec:      672 || Batch Translation Loss:   0.007979 => Txt Tokens per Sec:     1860 || Lr: 0.000050
2024-02-11 04:33:02,050 Epoch 6700: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:33:02,050 EPOCH 6701
2024-02-11 04:33:18,262 Epoch 6701: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:33:18,263 EPOCH 6702
2024-02-11 04:33:34,413 Epoch 6702: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:33:34,414 EPOCH 6703
2024-02-11 04:33:50,546 Epoch 6703: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:33:50,588 EPOCH 6704
2024-02-11 04:34:07,198 Epoch 6704: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:34:07,199 EPOCH 6705
2024-02-11 04:34:23,610 Epoch 6705: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:34:23,611 EPOCH 6706
2024-02-11 04:34:39,415 Epoch 6706: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:34:39,416 EPOCH 6707
2024-02-11 04:34:55,180 Epoch 6707: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:34:55,181 EPOCH 6708
2024-02-11 04:35:10,802 Epoch 6708: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:35:10,803 EPOCH 6709
2024-02-11 04:35:26,765 Epoch 6709: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:35:26,766 EPOCH 6710
2024-02-11 04:35:42,954 Epoch 6710: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:35:42,955 EPOCH 6711
2024-02-11 04:35:59,150 Epoch 6711: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:35:59,151 EPOCH 6712
2024-02-11 04:35:59,377 [Epoch: 6712 Step: 00060400] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     5689 || Batch Translation Loss:   0.006169 => Txt Tokens per Sec:    10147 || Lr: 0.000050
2024-02-11 04:36:15,152 Epoch 6712: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:36:15,153 EPOCH 6713
2024-02-11 04:36:31,225 Epoch 6713: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:36:31,225 EPOCH 6714
2024-02-11 04:36:47,197 Epoch 6714: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:36:47,198 EPOCH 6715
2024-02-11 04:37:03,052 Epoch 6715: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:37:03,053 EPOCH 6716
2024-02-11 04:37:19,120 Epoch 6716: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:37:19,120 EPOCH 6717
2024-02-11 04:37:34,980 Epoch 6717: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:37:34,980 EPOCH 6718
2024-02-11 04:37:50,899 Epoch 6718: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:37:50,899 EPOCH 6719
2024-02-11 04:38:06,799 Epoch 6719: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:38:06,799 EPOCH 6720
2024-02-11 04:38:22,956 Epoch 6720: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:38:22,957 EPOCH 6721
2024-02-11 04:38:39,195 Epoch 6721: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:38:39,195 EPOCH 6722
2024-02-11 04:38:55,320 Epoch 6722: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:38:55,321 EPOCH 6723
2024-02-11 04:39:01,795 [Epoch: 6723 Step: 00060500] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:      395 || Batch Translation Loss:   0.013414 => Txt Tokens per Sec:     1246 || Lr: 0.000050
2024-02-11 04:39:11,503 Epoch 6723: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:39:11,503 EPOCH 6724
2024-02-11 04:39:27,270 Epoch 6724: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:39:27,270 EPOCH 6725
2024-02-11 04:39:43,398 Epoch 6725: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:39:43,398 EPOCH 6726
2024-02-11 04:39:59,389 Epoch 6726: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:39:59,390 EPOCH 6727
2024-02-11 04:40:15,425 Epoch 6727: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:40:15,426 EPOCH 6728
2024-02-11 04:40:31,682 Epoch 6728: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:40:31,682 EPOCH 6729
2024-02-11 04:40:47,800 Epoch 6729: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:40:47,801 EPOCH 6730
2024-02-11 04:41:03,695 Epoch 6730: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:41:03,696 EPOCH 6731
2024-02-11 04:41:19,652 Epoch 6731: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:41:19,652 EPOCH 6732
2024-02-11 04:41:35,959 Epoch 6732: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:41:35,959 EPOCH 6733
2024-02-11 04:41:52,195 Epoch 6733: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 04:41:52,196 EPOCH 6734
2024-02-11 04:42:01,900 [Epoch: 6734 Step: 00060600] Batch Recognition Loss:   0.000330 => Gls Tokens per Sec:      396 || Batch Translation Loss:   0.018937 => Txt Tokens per Sec:     1276 || Lr: 0.000050
2024-02-11 04:42:08,240 Epoch 6734: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 04:42:08,240 EPOCH 6735
2024-02-11 04:42:24,230 Epoch 6735: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:42:24,231 EPOCH 6736
2024-02-11 04:42:40,266 Epoch 6736: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:42:40,267 EPOCH 6737
2024-02-11 04:42:56,124 Epoch 6737: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:42:56,125 EPOCH 6738
2024-02-11 04:43:12,270 Epoch 6738: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:43:12,271 EPOCH 6739
2024-02-11 04:43:28,349 Epoch 6739: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:43:28,350 EPOCH 6740
2024-02-11 04:43:44,768 Epoch 6740: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:43:44,768 EPOCH 6741
2024-02-11 04:44:00,833 Epoch 6741: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:44:00,834 EPOCH 6742
2024-02-11 04:44:16,738 Epoch 6742: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:44:16,739 EPOCH 6743
2024-02-11 04:44:32,721 Epoch 6743: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 04:44:32,722 EPOCH 6744
2024-02-11 04:44:48,621 Epoch 6744: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:44:48,621 EPOCH 6745
2024-02-11 04:44:54,549 [Epoch: 6745 Step: 00060700] Batch Recognition Loss:   0.000530 => Gls Tokens per Sec:      712 || Batch Translation Loss:   0.027115 => Txt Tokens per Sec:     1891 || Lr: 0.000050
2024-02-11 04:45:04,934 Epoch 6745: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 04:45:04,935 EPOCH 6746
2024-02-11 04:45:20,811 Epoch 6746: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 04:45:20,812 EPOCH 6747
2024-02-11 04:45:36,907 Epoch 6747: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 04:45:36,908 EPOCH 6748
2024-02-11 04:45:52,827 Epoch 6748: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 04:45:52,827 EPOCH 6749
2024-02-11 04:46:09,105 Epoch 6749: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-11 04:46:09,106 EPOCH 6750
2024-02-11 04:46:25,328 Epoch 6750: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 04:46:25,329 EPOCH 6751
2024-02-11 04:46:41,473 Epoch 6751: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-11 04:46:41,474 EPOCH 6752
2024-02-11 04:46:57,617 Epoch 6752: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-11 04:46:57,618 EPOCH 6753
2024-02-11 04:47:13,685 Epoch 6753: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-11 04:47:13,685 EPOCH 6754
2024-02-11 04:47:29,607 Epoch 6754: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 04:47:29,607 EPOCH 6755
2024-02-11 04:47:45,561 Epoch 6755: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 04:47:45,561 EPOCH 6756
2024-02-11 04:47:55,730 [Epoch: 6756 Step: 00060800] Batch Recognition Loss:   0.000467 => Gls Tokens per Sec:      629 || Batch Translation Loss:   0.024902 => Txt Tokens per Sec:     1805 || Lr: 0.000050
2024-02-11 04:48:01,804 Epoch 6756: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 04:48:01,805 EPOCH 6757
2024-02-11 04:48:18,295 Epoch 6757: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 04:48:18,295 EPOCH 6758
2024-02-11 04:48:34,398 Epoch 6758: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 04:48:34,398 EPOCH 6759
2024-02-11 04:48:50,452 Epoch 6759: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 04:48:50,452 EPOCH 6760
2024-02-11 04:49:06,285 Epoch 6760: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 04:49:06,285 EPOCH 6761
2024-02-11 04:49:22,505 Epoch 6761: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 04:49:22,505 EPOCH 6762
2024-02-11 04:49:38,621 Epoch 6762: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 04:49:38,622 EPOCH 6763
2024-02-11 04:49:54,963 Epoch 6763: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 04:49:54,964 EPOCH 6764
2024-02-11 04:50:10,667 Epoch 6764: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 04:50:10,668 EPOCH 6765
2024-02-11 04:50:26,798 Epoch 6765: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-11 04:50:26,799 EPOCH 6766
2024-02-11 04:50:43,010 Epoch 6766: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 04:50:43,010 EPOCH 6767
2024-02-11 04:50:50,762 [Epoch: 6767 Step: 00060900] Batch Recognition Loss:   0.000330 => Gls Tokens per Sec:      991 || Batch Translation Loss:   0.018730 => Txt Tokens per Sec:     2634 || Lr: 0.000050
2024-02-11 04:50:58,806 Epoch 6767: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 04:50:58,806 EPOCH 6768
2024-02-11 04:51:14,756 Epoch 6768: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 04:51:14,757 EPOCH 6769
2024-02-11 04:51:30,590 Epoch 6769: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 04:51:30,591 EPOCH 6770
2024-02-11 04:51:47,042 Epoch 6770: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 04:51:47,042 EPOCH 6771
2024-02-11 04:52:03,095 Epoch 6771: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:52:03,095 EPOCH 6772
2024-02-11 04:52:19,265 Epoch 6772: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 04:52:19,265 EPOCH 6773
2024-02-11 04:52:34,951 Epoch 6773: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 04:52:34,952 EPOCH 6774
2024-02-11 04:52:51,005 Epoch 6774: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 04:52:51,006 EPOCH 6775
2024-02-11 04:53:06,976 Epoch 6775: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 04:53:06,977 EPOCH 6776
2024-02-11 04:53:23,281 Epoch 6776: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 04:53:23,281 EPOCH 6777
2024-02-11 04:53:39,290 Epoch 6777: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 04:53:39,290 EPOCH 6778
2024-02-11 04:53:54,648 [Epoch: 6778 Step: 00061000] Batch Recognition Loss:   0.000323 => Gls Tokens per Sec:      525 || Batch Translation Loss:   0.025920 => Txt Tokens per Sec:     1550 || Lr: 0.000050
2024-02-11 04:53:55,368 Epoch 6778: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 04:53:55,368 EPOCH 6779
2024-02-11 04:54:11,612 Epoch 6779: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 04:54:11,612 EPOCH 6780
2024-02-11 04:54:27,595 Epoch 6780: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 04:54:27,595 EPOCH 6781
2024-02-11 04:54:43,841 Epoch 6781: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 04:54:43,842 EPOCH 6782
2024-02-11 04:55:00,017 Epoch 6782: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:55:00,018 EPOCH 6783
2024-02-11 04:55:16,086 Epoch 6783: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:55:16,086 EPOCH 6784
2024-02-11 04:55:32,174 Epoch 6784: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:55:32,175 EPOCH 6785
2024-02-11 04:55:48,274 Epoch 6785: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:55:48,275 EPOCH 6786
2024-02-11 04:56:04,290 Epoch 6786: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:56:04,291 EPOCH 6787
2024-02-11 04:56:20,575 Epoch 6787: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:56:20,575 EPOCH 6788
2024-02-11 04:56:36,636 Epoch 6788: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:56:36,637 EPOCH 6789
2024-02-11 04:56:49,585 [Epoch: 6789 Step: 00061100] Batch Recognition Loss:   0.000381 => Gls Tokens per Sec:      721 || Batch Translation Loss:   0.005127 => Txt Tokens per Sec:     1953 || Lr: 0.000050
2024-02-11 04:56:52,870 Epoch 6789: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:56:52,870 EPOCH 6790
2024-02-11 04:57:09,221 Epoch 6790: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:57:09,222 EPOCH 6791
2024-02-11 04:57:25,192 Epoch 6791: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:57:25,192 EPOCH 6792
2024-02-11 04:57:41,274 Epoch 6792: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 04:57:41,275 EPOCH 6793
2024-02-11 04:57:57,434 Epoch 6793: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 04:57:57,435 EPOCH 6794
2024-02-11 04:58:13,690 Epoch 6794: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 04:58:13,691 EPOCH 6795
2024-02-11 04:58:29,465 Epoch 6795: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 04:58:29,465 EPOCH 6796
2024-02-11 04:58:45,302 Epoch 6796: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 04:58:45,303 EPOCH 6797
2024-02-11 04:59:01,432 Epoch 6797: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 04:59:01,433 EPOCH 6798
2024-02-11 04:59:17,491 Epoch 6798: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 04:59:17,491 EPOCH 6799
2024-02-11 04:59:33,614 Epoch 6799: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 04:59:33,615 EPOCH 6800
2024-02-11 04:59:49,704 [Epoch: 6800 Step: 00061200] Batch Recognition Loss:   0.000297 => Gls Tokens per Sec:      660 || Batch Translation Loss:   0.012735 => Txt Tokens per Sec:     1826 || Lr: 0.000050
2024-02-11 04:59:49,705 Epoch 6800: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 04:59:49,705 EPOCH 6801
2024-02-11 05:00:05,991 Epoch 6801: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 05:00:05,991 EPOCH 6802
2024-02-11 05:00:21,901 Epoch 6802: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 05:00:21,901 EPOCH 6803
2024-02-11 05:00:37,907 Epoch 6803: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 05:00:37,907 EPOCH 6804
2024-02-11 05:00:53,843 Epoch 6804: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 05:00:53,843 EPOCH 6805
2024-02-11 05:01:09,930 Epoch 6805: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 05:01:09,931 EPOCH 6806
2024-02-11 05:01:26,137 Epoch 6806: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 05:01:26,138 EPOCH 6807
2024-02-11 05:01:42,156 Epoch 6807: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 05:01:42,156 EPOCH 6808
2024-02-11 05:01:58,227 Epoch 6808: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-11 05:01:58,227 EPOCH 6809
2024-02-11 05:02:14,373 Epoch 6809: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 05:02:14,374 EPOCH 6810
2024-02-11 05:02:30,317 Epoch 6810: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 05:02:30,317 EPOCH 6811
2024-02-11 05:02:46,699 Epoch 6811: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 05:02:46,700 EPOCH 6812
2024-02-11 05:02:49,835 [Epoch: 6812 Step: 00061300] Batch Recognition Loss:   0.000422 => Gls Tokens per Sec:      408 || Batch Translation Loss:   0.017979 => Txt Tokens per Sec:     1304 || Lr: 0.000050
2024-02-11 05:03:02,582 Epoch 6812: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 05:03:02,583 EPOCH 6813
2024-02-11 05:03:18,787 Epoch 6813: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-11 05:03:18,788 EPOCH 6814
2024-02-11 05:03:34,855 Epoch 6814: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-11 05:03:34,856 EPOCH 6815
2024-02-11 05:03:50,816 Epoch 6815: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-11 05:03:50,817 EPOCH 6816
2024-02-11 05:04:06,973 Epoch 6816: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 05:04:06,974 EPOCH 6817
2024-02-11 05:04:22,869 Epoch 6817: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 05:04:22,870 EPOCH 6818
2024-02-11 05:04:38,982 Epoch 6818: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 05:04:38,982 EPOCH 6819
2024-02-11 05:04:54,871 Epoch 6819: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 05:04:54,872 EPOCH 6820
2024-02-11 05:05:11,157 Epoch 6820: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 05:05:11,157 EPOCH 6821
2024-02-11 05:05:27,190 Epoch 6821: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 05:05:27,191 EPOCH 6822
2024-02-11 05:05:43,098 Epoch 6822: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 05:05:43,098 EPOCH 6823
2024-02-11 05:05:44,048 [Epoch: 6823 Step: 00061400] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:     2698 || Batch Translation Loss:   0.011057 => Txt Tokens per Sec:     7161 || Lr: 0.000050
2024-02-11 05:05:59,260 Epoch 6823: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 05:05:59,261 EPOCH 6824
2024-02-11 05:06:15,231 Epoch 6824: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 05:06:15,231 EPOCH 6825
2024-02-11 05:06:31,449 Epoch 6825: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 05:06:31,450 EPOCH 6826
2024-02-11 05:06:47,632 Epoch 6826: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 05:06:47,633 EPOCH 6827
2024-02-11 05:07:03,963 Epoch 6827: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:07:03,963 EPOCH 6828
2024-02-11 05:07:20,092 Epoch 6828: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-11 05:07:20,093 EPOCH 6829
2024-02-11 05:07:36,349 Epoch 6829: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:07:36,350 EPOCH 6830
2024-02-11 05:07:52,632 Epoch 6830: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 05:07:52,632 EPOCH 6831
2024-02-11 05:08:08,936 Epoch 6831: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 05:08:08,937 EPOCH 6832
2024-02-11 05:08:25,066 Epoch 6832: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 05:08:25,067 EPOCH 6833
2024-02-11 05:08:41,422 Epoch 6833: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-11 05:08:41,423 EPOCH 6834
2024-02-11 05:08:52,128 [Epoch: 6834 Step: 00061500] Batch Recognition Loss:   0.000323 => Gls Tokens per Sec:      275 || Batch Translation Loss:   0.016020 => Txt Tokens per Sec:      843 || Lr: 0.000050
2024-02-11 05:08:57,604 Epoch 6834: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 05:08:57,604 EPOCH 6835
2024-02-11 05:09:13,377 Epoch 6835: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 05:09:13,377 EPOCH 6836
2024-02-11 05:09:29,527 Epoch 6836: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 05:09:29,527 EPOCH 6837
2024-02-11 05:09:45,589 Epoch 6837: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-11 05:09:45,590 EPOCH 6838
2024-02-11 05:10:01,545 Epoch 6838: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-11 05:10:01,545 EPOCH 6839
2024-02-11 05:10:17,808 Epoch 6839: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-11 05:10:17,808 EPOCH 6840
2024-02-11 05:10:33,916 Epoch 6840: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-11 05:10:33,917 EPOCH 6841
2024-02-11 05:10:49,913 Epoch 6841: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.49 
2024-02-11 05:10:49,914 EPOCH 6842
2024-02-11 05:11:05,955 Epoch 6842: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.36 
2024-02-11 05:11:05,955 EPOCH 6843
2024-02-11 05:11:22,445 Epoch 6843: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-11 05:11:22,446 EPOCH 6844
2024-02-11 05:11:38,473 Epoch 6844: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-11 05:11:38,474 EPOCH 6845
2024-02-11 05:11:40,461 [Epoch: 6845 Step: 00061600] Batch Recognition Loss:   0.000558 => Gls Tokens per Sec:     2578 || Batch Translation Loss:   0.023686 => Txt Tokens per Sec:     7224 || Lr: 0.000050
2024-02-11 05:11:54,470 Epoch 6845: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-11 05:11:54,470 EPOCH 6846
2024-02-11 05:12:10,639 Epoch 6846: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-11 05:12:10,640 EPOCH 6847
2024-02-11 05:12:26,759 Epoch 6847: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-11 05:12:26,760 EPOCH 6848
2024-02-11 05:12:42,927 Epoch 6848: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 05:12:42,928 EPOCH 6849
2024-02-11 05:12:58,718 Epoch 6849: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 05:12:58,719 EPOCH 6850
2024-02-11 05:13:14,725 Epoch 6850: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:13:14,726 EPOCH 6851
2024-02-11 05:13:30,899 Epoch 6851: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:13:30,900 EPOCH 6852
2024-02-11 05:13:46,846 Epoch 6852: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:13:46,847 EPOCH 6853
2024-02-11 05:14:02,916 Epoch 6853: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:14:02,916 EPOCH 6854
2024-02-11 05:14:18,753 Epoch 6854: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:14:18,754 EPOCH 6855
2024-02-11 05:14:34,856 Epoch 6855: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:14:34,857 EPOCH 6856
2024-02-11 05:14:39,587 [Epoch: 6856 Step: 00061700] Batch Recognition Loss:   0.000135 => Gls Tokens per Sec:     1354 || Batch Translation Loss:   0.008679 => Txt Tokens per Sec:     3578 || Lr: 0.000050
2024-02-11 05:14:50,723 Epoch 6856: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:14:50,723 EPOCH 6857
2024-02-11 05:15:06,994 Epoch 6857: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:15:06,994 EPOCH 6858
2024-02-11 05:15:22,945 Epoch 6858: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:15:22,946 EPOCH 6859
2024-02-11 05:15:39,158 Epoch 6859: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:15:39,158 EPOCH 6860
2024-02-11 05:15:55,322 Epoch 6860: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:15:55,322 EPOCH 6861
2024-02-11 05:16:11,119 Epoch 6861: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:16:11,120 EPOCH 6862
2024-02-11 05:16:27,186 Epoch 6862: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:16:27,187 EPOCH 6863
2024-02-11 05:16:43,401 Epoch 6863: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:16:43,402 EPOCH 6864
2024-02-11 05:16:59,316 Epoch 6864: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:16:59,317 EPOCH 6865
2024-02-11 05:17:14,825 Epoch 6865: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-11 05:17:14,825 EPOCH 6866
2024-02-11 05:17:31,049 Epoch 6866: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:17:31,050 EPOCH 6867
2024-02-11 05:17:40,125 [Epoch: 6867 Step: 00061800] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:      747 || Batch Translation Loss:   0.009377 => Txt Tokens per Sec:     1957 || Lr: 0.000050
2024-02-11 05:17:47,049 Epoch 6867: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:17:47,049 EPOCH 6868
2024-02-11 05:18:03,227 Epoch 6868: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-11 05:18:03,228 EPOCH 6869
2024-02-11 05:18:19,378 Epoch 6869: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:18:19,378 EPOCH 6870
2024-02-11 05:18:35,596 Epoch 6870: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:18:35,597 EPOCH 6871
2024-02-11 05:18:51,546 Epoch 6871: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:18:51,546 EPOCH 6872
2024-02-11 05:19:07,875 Epoch 6872: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:19:07,875 EPOCH 6873
2024-02-11 05:19:24,205 Epoch 6873: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 05:19:24,205 EPOCH 6874
2024-02-11 05:19:40,356 Epoch 6874: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 05:19:40,356 EPOCH 6875
2024-02-11 05:19:56,497 Epoch 6875: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:19:56,497 EPOCH 6876
2024-02-11 05:20:12,413 Epoch 6876: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:20:12,413 EPOCH 6877
2024-02-11 05:20:28,672 Epoch 6877: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:20:28,673 EPOCH 6878
2024-02-11 05:20:38,242 [Epoch: 6878 Step: 00061900] Batch Recognition Loss:   0.000215 => Gls Tokens per Sec:      842 || Batch Translation Loss:   0.012133 => Txt Tokens per Sec:     2208 || Lr: 0.000050
2024-02-11 05:20:44,636 Epoch 6878: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:20:44,636 EPOCH 6879
2024-02-11 05:21:00,657 Epoch 6879: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:21:00,658 EPOCH 6880
2024-02-11 05:21:17,036 Epoch 6880: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:21:17,037 EPOCH 6881
2024-02-11 05:21:33,228 Epoch 6881: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:21:33,229 EPOCH 6882
2024-02-11 05:21:48,954 Epoch 6882: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:21:48,955 EPOCH 6883
2024-02-11 05:22:05,201 Epoch 6883: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 05:22:05,201 EPOCH 6884
2024-02-11 05:22:21,284 Epoch 6884: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 05:22:21,285 EPOCH 6885
2024-02-11 05:22:37,426 Epoch 6885: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 05:22:37,426 EPOCH 6886
2024-02-11 05:22:53,476 Epoch 6886: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 05:22:53,477 EPOCH 6887
2024-02-11 05:23:09,517 Epoch 6887: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 05:23:09,518 EPOCH 6888
2024-02-11 05:23:25,380 Epoch 6888: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:23:25,381 EPOCH 6889
2024-02-11 05:23:41,097 [Epoch: 6889 Step: 00062000] Batch Recognition Loss:   0.001888 => Gls Tokens per Sec:      594 || Batch Translation Loss:   0.005515 => Txt Tokens per Sec:     1632 || Lr: 0.000050
2024-02-11 05:24:53,074 Validation result at epoch 6889, step    62000: duration: 71.9754s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.24138	Translation Loss: 109490.98438	PPL: 56160.84375
	Eval Metric: BLEU
	WER 2.47	(DEL: 0.00,	INS: 0.00,	SUB: 2.47)
	BLEU-4 0.39	(BLEU-1: 10.02,	BLEU-2: 2.78,	BLEU-3: 0.95,	BLEU-4: 0.39)
	CHRF 16.95	ROUGE 8.09
2024-02-11 05:24:53,076 Logging Recognition and Translation Outputs
2024-02-11 05:24:53,076 ========================================================================================================================
2024-02-11 05:24:53,076 Logging Sequence: 165_523.00
2024-02-11 05:24:53,077 	Gloss Reference :	A B+C+D+E
2024-02-11 05:24:53,077 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 05:24:53,077 	Gloss Alignment :	         
2024-02-11 05:24:53,077 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 05:24:53,078 	Text Reference  :	as he believed that his team might lose if     he   takes off     his batting pads
2024-02-11 05:24:53,078 	Text Hypothesis :	** ** ******** **** *** **** ***** **** suhana khan was   shocked to  see     this
2024-02-11 05:24:53,078 	Text Alignment  :	D  D  D        D    D   D    D     D    S      S    S     S       S   S       S   
2024-02-11 05:24:53,078 ========================================================================================================================
2024-02-11 05:24:53,078 Logging Sequence: 165_233.00
2024-02-11 05:24:53,079 	Gloss Reference :	A B+C+D+E
2024-02-11 05:24:53,079 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 05:24:53,079 	Gloss Alignment :	         
2024-02-11 05:24:53,079 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 05:24:53,081 	Text Reference  :	irrespective of whether he was playing the match or not he always sat  with his bag  he  was   happy   when   the  team won     
2024-02-11 05:24:53,081 	Text Hypothesis :	************ ** ******* ** *** ******* *** ***** ** *** ** ****** many had  may tell you about cricket nobody knew what happened
2024-02-11 05:24:53,081 	Text Alignment  :	D            D  D       D  D   D       D   D     D  D   D  D      S    S    S   S    S   S     S       S      S    S    S       
2024-02-11 05:24:53,081 ========================================================================================================================
2024-02-11 05:24:53,081 Logging Sequence: 169_214.00
2024-02-11 05:24:53,081 	Gloss Reference :	A B+C+D+E
2024-02-11 05:24:53,082 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 05:24:53,082 	Gloss Alignment :	         
2024-02-11 05:24:53,082 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 05:24:53,084 	Text Reference  :	virat kohli said that though arshdeep dropped the   catch he       is    still  a   strong part      of the ***** ********** indian  team  
2024-02-11 05:24:53,084 	Text Hypothesis :	***** ***** the  that ****** ******** ******* group c     included south africa and 46     countries of the state government against amount
2024-02-11 05:24:53,084 	Text Alignment  :	D     D     S         D      D        D       S     S     S        S     S      S   S      S                I     I          S       S     
2024-02-11 05:24:53,084 ========================================================================================================================
2024-02-11 05:24:53,085 Logging Sequence: 88_67.00
2024-02-11 05:24:53,085 	Gloss Reference :	A B+C+D+E
2024-02-11 05:24:53,085 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 05:24:53,085 	Gloss Alignment :	         
2024-02-11 05:24:53,085 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 05:24:53,087 	Text Reference  :	pablo javkin the **** mayor  of  rosario is        also      a   drug trafficker so    he  won't take care of       you     
2024-02-11 05:24:53,087 	Text Hypothesis :	***** ****** the cops traced him to      hyderabad ramnagesh was only 23         years old and   was  a    software engineer
2024-02-11 05:24:53,087 	Text Alignment  :	D     D          I    S      S   S       S         S         S   S    S          S     S   S     S    S    S        S       
2024-02-11 05:24:53,087 ========================================================================================================================
2024-02-11 05:24:53,088 Logging Sequence: 69_95.00
2024-02-11 05:24:53,088 	Gloss Reference :	A B+C+D+E
2024-02-11 05:24:53,088 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 05:24:53,088 	Gloss Alignment :	         
2024-02-11 05:24:53,088 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 05:24:53,090 	Text Reference  :	**** *** a     six and a     four sealed csk's victory  and the team   won     the     match   
2024-02-11 05:24:53,090 	Text Hypothesis :	when the final was the world cup  trophy was   unvieled by  an  indian actress deepika padukone
2024-02-11 05:24:53,090 	Text Alignment  :	I    I   S     S   S   S     S    S      S     S        S   S   S      S       S       S       
2024-02-11 05:24:53,090 ========================================================================================================================
2024-02-11 05:24:53,910 Epoch 6889: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:24:53,910 EPOCH 6890
2024-02-11 05:25:10,708 Epoch 6890: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:25:10,708 EPOCH 6891
2024-02-11 05:25:28,300 Epoch 6891: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:25:28,300 EPOCH 6892
2024-02-11 05:25:44,549 Epoch 6892: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:25:44,550 EPOCH 6893
2024-02-11 05:26:00,730 Epoch 6893: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:26:00,731 EPOCH 6894
2024-02-11 05:26:16,831 Epoch 6894: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:26:16,832 EPOCH 6895
2024-02-11 05:26:32,785 Epoch 6895: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:26:32,786 EPOCH 6896
2024-02-11 05:26:48,907 Epoch 6896: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:26:48,908 EPOCH 6897
2024-02-11 05:27:04,862 Epoch 6897: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:27:04,862 EPOCH 6898
2024-02-11 05:27:21,701 Epoch 6898: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:27:21,703 EPOCH 6899
2024-02-11 05:27:37,688 Epoch 6899: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:27:37,689 EPOCH 6900
2024-02-11 05:27:53,424 [Epoch: 6900 Step: 00062100] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:      675 || Batch Translation Loss:   0.011088 => Txt Tokens per Sec:     1867 || Lr: 0.000050
2024-02-11 05:27:53,424 Epoch 6900: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 05:27:53,424 EPOCH 6901
2024-02-11 05:28:09,702 Epoch 6901: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:28:09,703 EPOCH 6902
2024-02-11 05:28:25,806 Epoch 6902: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:28:25,806 EPOCH 6903
2024-02-11 05:28:41,744 Epoch 6903: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:28:41,744 EPOCH 6904
2024-02-11 05:28:57,646 Epoch 6904: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 05:28:57,647 EPOCH 6905
2024-02-11 05:29:13,755 Epoch 6905: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:29:13,755 EPOCH 6906
2024-02-11 05:29:29,978 Epoch 6906: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:29:29,979 EPOCH 6907
2024-02-11 05:29:45,973 Epoch 6907: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:29:45,974 EPOCH 6908
2024-02-11 05:30:02,007 Epoch 6908: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 05:30:02,008 EPOCH 6909
2024-02-11 05:30:18,335 Epoch 6909: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-11 05:30:18,336 EPOCH 6910
2024-02-11 05:30:34,385 Epoch 6910: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 05:30:34,385 EPOCH 6911
2024-02-11 05:30:50,851 Epoch 6911: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 05:30:50,851 EPOCH 6912
2024-02-11 05:30:55,156 [Epoch: 6912 Step: 00062200] Batch Recognition Loss:   0.000539 => Gls Tokens per Sec:       88 || Batch Translation Loss:   0.009021 => Txt Tokens per Sec:      315 || Lr: 0.000050
2024-02-11 05:31:06,632 Epoch 6912: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 05:31:06,632 EPOCH 6913
2024-02-11 05:31:22,849 Epoch 6913: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-11 05:31:22,849 EPOCH 6914
2024-02-11 05:31:38,872 Epoch 6914: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 05:31:38,872 EPOCH 6915
2024-02-11 05:31:55,142 Epoch 6915: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 05:31:55,143 EPOCH 6916
2024-02-11 05:32:11,341 Epoch 6916: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 05:32:11,342 EPOCH 6917
2024-02-11 05:32:27,259 Epoch 6917: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 05:32:27,260 EPOCH 6918
2024-02-11 05:32:43,378 Epoch 6918: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 05:32:43,379 EPOCH 6919
2024-02-11 05:32:59,285 Epoch 6919: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 05:32:59,286 EPOCH 6920
2024-02-11 05:33:15,440 Epoch 6920: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 05:33:15,441 EPOCH 6921
2024-02-11 05:33:31,743 Epoch 6921: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.77 
2024-02-11 05:33:31,743 EPOCH 6922
2024-02-11 05:33:47,888 Epoch 6922: Total Training Recognition Loss 0.03  Total Training Translation Loss 8.79 
2024-02-11 05:33:47,888 EPOCH 6923
2024-02-11 05:33:51,409 [Epoch: 6923 Step: 00062300] Batch Recognition Loss:   0.000576 => Gls Tokens per Sec:      727 || Batch Translation Loss:   0.477586 => Txt Tokens per Sec:     1984 || Lr: 0.000050
2024-02-11 05:34:03,951 Epoch 6923: Total Training Recognition Loss 0.03  Total Training Translation Loss 3.76 
2024-02-11 05:34:03,952 EPOCH 6924
2024-02-11 05:34:20,538 Epoch 6924: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.27 
2024-02-11 05:34:20,538 EPOCH 6925
2024-02-11 05:34:36,678 Epoch 6925: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.70 
2024-02-11 05:34:36,678 EPOCH 6926
2024-02-11 05:34:53,157 Epoch 6926: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.47 
2024-02-11 05:34:53,157 EPOCH 6927
2024-02-11 05:35:09,519 Epoch 6927: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-11 05:35:09,520 EPOCH 6928
2024-02-11 05:35:25,505 Epoch 6928: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.25 
2024-02-11 05:35:25,506 EPOCH 6929
2024-02-11 05:35:41,672 Epoch 6929: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-11 05:35:41,673 EPOCH 6930
2024-02-11 05:35:58,029 Epoch 6930: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-11 05:35:58,030 EPOCH 6931
2024-02-11 05:36:14,107 Epoch 6931: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-11 05:36:14,108 EPOCH 6932
2024-02-11 05:36:29,892 Epoch 6932: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-11 05:36:29,893 EPOCH 6933
2024-02-11 05:36:45,657 Epoch 6933: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-11 05:36:45,657 EPOCH 6934
2024-02-11 05:36:50,907 [Epoch: 6934 Step: 00062400] Batch Recognition Loss:   0.000340 => Gls Tokens per Sec:      560 || Batch Translation Loss:   0.011727 => Txt Tokens per Sec:     1476 || Lr: 0.000050
2024-02-11 05:37:01,795 Epoch 6934: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 05:37:01,795 EPOCH 6935
2024-02-11 05:37:18,175 Epoch 6935: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-11 05:37:18,176 EPOCH 6936
2024-02-11 05:37:34,221 Epoch 6936: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-11 05:37:34,222 EPOCH 6937
2024-02-11 05:37:50,522 Epoch 6937: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 05:37:50,522 EPOCH 6938
2024-02-11 05:38:06,471 Epoch 6938: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-11 05:38:06,472 EPOCH 6939
2024-02-11 05:38:22,214 Epoch 6939: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 05:38:22,215 EPOCH 6940
2024-02-11 05:38:38,303 Epoch 6940: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 05:38:38,303 EPOCH 6941
2024-02-11 05:38:54,547 Epoch 6941: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-11 05:38:54,548 EPOCH 6942
2024-02-11 05:39:10,549 Epoch 6942: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 05:39:10,550 EPOCH 6943
2024-02-11 05:39:26,476 Epoch 6943: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 05:39:26,477 EPOCH 6944
2024-02-11 05:39:42,407 Epoch 6944: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 05:39:42,408 EPOCH 6945
2024-02-11 05:39:44,292 [Epoch: 6945 Step: 00062500] Batch Recognition Loss:   0.000295 => Gls Tokens per Sec:     2719 || Batch Translation Loss:   0.017636 => Txt Tokens per Sec:     7627 || Lr: 0.000050
2024-02-11 05:39:58,178 Epoch 6945: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 05:39:58,178 EPOCH 6946
2024-02-11 05:40:13,952 Epoch 6946: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 05:40:13,953 EPOCH 6947
2024-02-11 05:40:30,255 Epoch 6947: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 05:40:30,255 EPOCH 6948
2024-02-11 05:40:46,278 Epoch 6948: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:40:46,279 EPOCH 6949
2024-02-11 05:41:02,254 Epoch 6949: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:41:02,255 EPOCH 6950
2024-02-11 05:41:17,763 Epoch 6950: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 05:41:17,763 EPOCH 6951
2024-02-11 05:41:33,563 Epoch 6951: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:41:33,563 EPOCH 6952
2024-02-11 05:41:49,792 Epoch 6952: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-11 05:41:49,793 EPOCH 6953
2024-02-11 05:42:06,067 Epoch 6953: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:42:06,068 EPOCH 6954
2024-02-11 05:42:22,743 Epoch 6954: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:42:22,743 EPOCH 6955
2024-02-11 05:42:38,650 Epoch 6955: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:42:38,650 EPOCH 6956
2024-02-11 05:42:41,226 [Epoch: 6956 Step: 00062600] Batch Recognition Loss:   0.000332 => Gls Tokens per Sec:     2486 || Batch Translation Loss:   0.024100 => Txt Tokens per Sec:     6298 || Lr: 0.000050
2024-02-11 05:42:55,017 Epoch 6956: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 05:42:55,018 EPOCH 6957
2024-02-11 05:43:11,426 Epoch 6957: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:43:11,426 EPOCH 6958
2024-02-11 05:43:27,365 Epoch 6958: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:43:27,365 EPOCH 6959
2024-02-11 05:43:43,191 Epoch 6959: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:43:43,192 EPOCH 6960
2024-02-11 05:43:59,109 Epoch 6960: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:43:59,110 EPOCH 6961
2024-02-11 05:44:15,386 Epoch 6961: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:44:15,387 EPOCH 6962
2024-02-11 05:44:31,716 Epoch 6962: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:44:31,717 EPOCH 6963
2024-02-11 05:44:47,943 Epoch 6963: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:44:47,943 EPOCH 6964
2024-02-11 05:45:03,826 Epoch 6964: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:45:03,826 EPOCH 6965
2024-02-11 05:45:20,068 Epoch 6965: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:45:20,069 EPOCH 6966
2024-02-11 05:45:36,474 Epoch 6966: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.09 
2024-02-11 05:45:36,474 EPOCH 6967
2024-02-11 05:45:43,011 [Epoch: 6967 Step: 00062700] Batch Recognition Loss:   0.000270 => Gls Tokens per Sec:     1037 || Batch Translation Loss:   0.005300 => Txt Tokens per Sec:     2697 || Lr: 0.000050
2024-02-11 05:45:52,470 Epoch 6967: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:45:52,471 EPOCH 6968
2024-02-11 05:46:08,787 Epoch 6968: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:46:08,787 EPOCH 6969
2024-02-11 05:46:25,120 Epoch 6969: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:46:25,121 EPOCH 6970
2024-02-11 05:46:41,418 Epoch 6970: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 05:46:41,419 EPOCH 6971
2024-02-11 05:46:57,353 Epoch 6971: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 05:46:57,353 EPOCH 6972
2024-02-11 05:47:13,192 Epoch 6972: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:47:13,193 EPOCH 6973
2024-02-11 05:47:29,316 Epoch 6973: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 05:47:29,316 EPOCH 6974
2024-02-11 05:47:45,497 Epoch 6974: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:47:45,499 EPOCH 6975
2024-02-11 05:48:01,495 Epoch 6975: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:48:01,495 EPOCH 6976
2024-02-11 05:48:17,748 Epoch 6976: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:48:17,748 EPOCH 6977
2024-02-11 05:48:33,794 Epoch 6977: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 05:48:33,795 EPOCH 6978
2024-02-11 05:48:48,748 [Epoch: 6978 Step: 00062800] Batch Recognition Loss:   0.000907 => Gls Tokens per Sec:      539 || Batch Translation Loss:   0.010558 => Txt Tokens per Sec:     1487 || Lr: 0.000050
2024-02-11 05:48:49,838 Epoch 6978: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 05:48:49,838 EPOCH 6979
2024-02-11 05:49:05,676 Epoch 6979: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:49:05,676 EPOCH 6980
2024-02-11 05:49:21,499 Epoch 6980: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 05:49:21,499 EPOCH 6981
2024-02-11 05:49:37,288 Epoch 6981: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:49:37,289 EPOCH 6982
2024-02-11 05:49:53,376 Epoch 6982: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:49:53,376 EPOCH 6983
2024-02-11 05:50:09,515 Epoch 6983: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:50:09,516 EPOCH 6984
2024-02-11 05:50:25,483 Epoch 6984: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 05:50:25,483 EPOCH 6985
2024-02-11 05:50:41,478 Epoch 6985: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 05:50:41,479 EPOCH 6986
2024-02-11 05:50:57,676 Epoch 6986: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 05:50:57,677 EPOCH 6987
2024-02-11 05:51:13,616 Epoch 6987: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 05:51:13,617 EPOCH 6988
2024-02-11 05:51:29,920 Epoch 6988: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 05:51:29,920 EPOCH 6989
2024-02-11 05:51:45,501 [Epoch: 6989 Step: 00062900] Batch Recognition Loss:   0.000479 => Gls Tokens per Sec:      599 || Batch Translation Loss:   0.005390 => Txt Tokens per Sec:     1738 || Lr: 0.000050
2024-02-11 05:51:45,793 Epoch 6989: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:51:45,793 EPOCH 6990
2024-02-11 05:52:01,825 Epoch 6990: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 05:52:01,826 EPOCH 6991
2024-02-11 05:52:17,930 Epoch 6991: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 05:52:17,931 EPOCH 6992
2024-02-11 05:52:34,058 Epoch 6992: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 05:52:34,058 EPOCH 6993
2024-02-11 05:52:50,123 Epoch 6993: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:52:50,124 EPOCH 6994
2024-02-11 05:53:06,167 Epoch 6994: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 05:53:06,167 EPOCH 6995
2024-02-11 05:53:22,079 Epoch 6995: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:53:22,079 EPOCH 6996
2024-02-11 05:53:38,309 Epoch 6996: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 05:53:38,309 EPOCH 6997
2024-02-11 05:53:54,404 Epoch 6997: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 05:53:54,404 EPOCH 6998
2024-02-11 05:54:10,335 Epoch 6998: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 05:54:10,336 EPOCH 6999
2024-02-11 05:54:26,383 Epoch 6999: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 05:54:26,383 EPOCH 7000
2024-02-11 05:54:42,506 [Epoch: 7000 Step: 00063000] Batch Recognition Loss:   0.000479 => Gls Tokens per Sec:      659 || Batch Translation Loss:   0.004962 => Txt Tokens per Sec:     1822 || Lr: 0.000050
2024-02-11 05:54:42,507 Epoch 7000: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 05:54:42,507 EPOCH 7001
2024-02-11 05:54:58,417 Epoch 7001: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:54:58,418 EPOCH 7002
2024-02-11 05:55:14,947 Epoch 7002: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 05:55:14,947 EPOCH 7003
2024-02-11 05:55:30,948 Epoch 7003: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 05:55:30,949 EPOCH 7004
2024-02-11 05:55:46,979 Epoch 7004: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 05:55:46,980 EPOCH 7005
2024-02-11 05:56:03,052 Epoch 7005: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 05:56:03,052 EPOCH 7006
2024-02-11 05:56:19,297 Epoch 7006: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 05:56:19,297 EPOCH 7007
2024-02-11 05:56:35,122 Epoch 7007: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 05:56:35,123 EPOCH 7008
2024-02-11 05:56:51,430 Epoch 7008: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 05:56:51,431 EPOCH 7009
2024-02-11 05:57:08,070 Epoch 7009: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:57:08,070 EPOCH 7010
2024-02-11 05:57:24,406 Epoch 7010: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 05:57:24,407 EPOCH 7011
2024-02-11 05:57:40,566 Epoch 7011: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:57:40,567 EPOCH 7012
2024-02-11 05:57:41,199 [Epoch: 7012 Step: 00063100] Batch Recognition Loss:   0.000183 => Gls Tokens per Sec:     2025 || Batch Translation Loss:   0.012155 => Txt Tokens per Sec:     6144 || Lr: 0.000050
2024-02-11 05:57:56,375 Epoch 7012: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 05:57:56,375 EPOCH 7013
2024-02-11 05:58:12,291 Epoch 7013: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.09 
2024-02-11 05:58:12,292 EPOCH 7014
2024-02-11 05:58:28,693 Epoch 7014: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:58:28,694 EPOCH 7015
2024-02-11 05:58:44,960 Epoch 7015: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 05:58:44,961 EPOCH 7016
2024-02-11 05:59:01,173 Epoch 7016: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 05:59:01,173 EPOCH 7017
2024-02-11 05:59:17,387 Epoch 7017: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 05:59:17,387 EPOCH 7018
2024-02-11 05:59:33,788 Epoch 7018: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 05:59:33,788 EPOCH 7019
2024-02-11 05:59:50,002 Epoch 7019: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 05:59:50,003 EPOCH 7020
2024-02-11 06:00:06,058 Epoch 7020: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:00:06,058 EPOCH 7021
2024-02-11 06:00:22,194 Epoch 7021: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:00:22,195 EPOCH 7022
2024-02-11 06:00:38,185 Epoch 7022: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:00:38,185 EPOCH 7023
2024-02-11 06:00:44,429 [Epoch: 7023 Step: 00063200] Batch Recognition Loss:   0.000484 => Gls Tokens per Sec:      410 || Batch Translation Loss:   0.014716 => Txt Tokens per Sec:     1273 || Lr: 0.000050
2024-02-11 06:00:54,366 Epoch 7023: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:00:54,367 EPOCH 7024
2024-02-11 06:01:10,639 Epoch 7024: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:01:10,639 EPOCH 7025
2024-02-11 06:01:26,596 Epoch 7025: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:01:26,597 EPOCH 7026
2024-02-11 06:01:42,889 Epoch 7026: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:01:42,890 EPOCH 7027
2024-02-11 06:01:59,112 Epoch 7027: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 06:01:59,113 EPOCH 7028
2024-02-11 06:02:15,221 Epoch 7028: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:02:15,222 EPOCH 7029
2024-02-11 06:02:31,315 Epoch 7029: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:02:31,315 EPOCH 7030
2024-02-11 06:02:47,430 Epoch 7030: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:02:47,430 EPOCH 7031
2024-02-11 06:03:03,532 Epoch 7031: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:03:03,533 EPOCH 7032
2024-02-11 06:03:19,755 Epoch 7032: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 06:03:19,755 EPOCH 7033
2024-02-11 06:03:35,811 Epoch 7033: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:03:35,811 EPOCH 7034
2024-02-11 06:03:49,133 [Epoch: 7034 Step: 00063300] Batch Recognition Loss:   0.000423 => Gls Tokens per Sec:      221 || Batch Translation Loss:   0.013427 => Txt Tokens per Sec:      740 || Lr: 0.000050
2024-02-11 06:03:51,925 Epoch 7034: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.09 
2024-02-11 06:03:51,925 EPOCH 7035
2024-02-11 06:04:08,191 Epoch 7035: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:04:08,191 EPOCH 7036
2024-02-11 06:04:24,074 Epoch 7036: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:04:24,075 EPOCH 7037
2024-02-11 06:04:40,038 Epoch 7037: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:04:40,039 EPOCH 7038
2024-02-11 06:04:56,225 Epoch 7038: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:04:56,226 EPOCH 7039
2024-02-11 06:05:12,256 Epoch 7039: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:05:12,257 EPOCH 7040
2024-02-11 06:05:28,197 Epoch 7040: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:05:28,198 EPOCH 7041
2024-02-11 06:05:44,299 Epoch 7041: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 06:05:44,300 EPOCH 7042
2024-02-11 06:06:00,361 Epoch 7042: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:06:00,361 EPOCH 7043
2024-02-11 06:06:16,708 Epoch 7043: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:06:16,708 EPOCH 7044
2024-02-11 06:06:32,899 Epoch 7044: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:06:32,900 EPOCH 7045
2024-02-11 06:06:40,190 [Epoch: 7045 Step: 00063400] Batch Recognition Loss:   0.000171 => Gls Tokens per Sec:      702 || Batch Translation Loss:   0.014273 => Txt Tokens per Sec:     2097 || Lr: 0.000050
2024-02-11 06:06:48,856 Epoch 7045: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:06:48,857 EPOCH 7046
2024-02-11 06:07:04,900 Epoch 7046: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:07:04,900 EPOCH 7047
2024-02-11 06:07:20,632 Epoch 7047: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:07:20,633 EPOCH 7048
2024-02-11 06:07:36,811 Epoch 7048: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:07:36,811 EPOCH 7049
2024-02-11 06:07:52,668 Epoch 7049: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:07:52,669 EPOCH 7050
2024-02-11 06:08:08,923 Epoch 7050: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:08:08,924 EPOCH 7051
2024-02-11 06:08:24,926 Epoch 7051: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:08:24,926 EPOCH 7052
2024-02-11 06:08:41,098 Epoch 7052: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:08:41,099 EPOCH 7053
2024-02-11 06:08:56,912 Epoch 7053: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:08:56,912 EPOCH 7054
2024-02-11 06:09:13,534 Epoch 7054: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:09:13,535 EPOCH 7055
2024-02-11 06:09:29,953 Epoch 7055: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:09:29,954 EPOCH 7056
2024-02-11 06:09:44,687 [Epoch: 7056 Step: 00063500] Batch Recognition Loss:   0.000168 => Gls Tokens per Sec:      373 || Batch Translation Loss:   0.010296 => Txt Tokens per Sec:     1172 || Lr: 0.000050
2024-02-11 06:09:46,164 Epoch 7056: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:09:46,165 EPOCH 7057
2024-02-11 06:10:02,042 Epoch 7057: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:10:02,042 EPOCH 7058
2024-02-11 06:10:18,107 Epoch 7058: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:10:18,107 EPOCH 7059
2024-02-11 06:10:34,354 Epoch 7059: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 06:10:34,355 EPOCH 7060
2024-02-11 06:10:50,729 Epoch 7060: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 06:10:50,730 EPOCH 7061
2024-02-11 06:11:07,075 Epoch 7061: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 06:11:07,076 EPOCH 7062
2024-02-11 06:11:23,390 Epoch 7062: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 06:11:23,390 EPOCH 7063
2024-02-11 06:11:39,560 Epoch 7063: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 06:11:39,561 EPOCH 7064
2024-02-11 06:11:55,755 Epoch 7064: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 06:11:55,755 EPOCH 7065
2024-02-11 06:12:11,587 Epoch 7065: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 06:12:11,587 EPOCH 7066
2024-02-11 06:12:27,990 Epoch 7066: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 06:12:27,990 EPOCH 7067
2024-02-11 06:12:38,573 [Epoch: 7067 Step: 00063600] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:      726 || Batch Translation Loss:   0.006373 => Txt Tokens per Sec:     1965 || Lr: 0.000050
2024-02-11 06:12:43,911 Epoch 7067: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 06:12:43,911 EPOCH 7068
2024-02-11 06:13:00,111 Epoch 7068: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:13:00,112 EPOCH 7069
2024-02-11 06:13:16,373 Epoch 7069: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 06:13:16,374 EPOCH 7070
2024-02-11 06:13:32,701 Epoch 7070: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 06:13:32,702 EPOCH 7071
2024-02-11 06:13:48,939 Epoch 7071: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:13:48,939 EPOCH 7072
2024-02-11 06:14:04,825 Epoch 7072: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:14:04,825 EPOCH 7073
2024-02-11 06:14:21,164 Epoch 7073: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:14:21,164 EPOCH 7074
2024-02-11 06:14:37,110 Epoch 7074: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 06:14:37,111 EPOCH 7075
2024-02-11 06:14:53,119 Epoch 7075: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:14:53,120 EPOCH 7076
2024-02-11 06:15:08,918 Epoch 7076: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 06:15:08,919 EPOCH 7077
2024-02-11 06:15:24,933 Epoch 7077: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 06:15:24,933 EPOCH 7078
2024-02-11 06:15:35,985 [Epoch: 7078 Step: 00063700] Batch Recognition Loss:   0.000288 => Gls Tokens per Sec:      811 || Batch Translation Loss:   0.013115 => Txt Tokens per Sec:     2218 || Lr: 0.000050
2024-02-11 06:15:40,746 Epoch 7078: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:15:40,746 EPOCH 7079
2024-02-11 06:15:57,146 Epoch 7079: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:15:57,147 EPOCH 7080
2024-02-11 06:16:13,271 Epoch 7080: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:16:13,271 EPOCH 7081
2024-02-11 06:16:29,031 Epoch 7081: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:16:29,032 EPOCH 7082
2024-02-11 06:16:45,030 Epoch 7082: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-11 06:16:45,030 EPOCH 7083
2024-02-11 06:17:01,199 Epoch 7083: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:17:01,200 EPOCH 7084
2024-02-11 06:17:17,470 Epoch 7084: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:17:17,470 EPOCH 7085
2024-02-11 06:17:33,606 Epoch 7085: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:17:33,607 EPOCH 7086
2024-02-11 06:17:49,833 Epoch 7086: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 06:17:49,834 EPOCH 7087
2024-02-11 06:18:05,981 Epoch 7087: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 06:18:05,982 EPOCH 7088
2024-02-11 06:18:22,141 Epoch 7088: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 06:18:22,141 EPOCH 7089
2024-02-11 06:18:37,399 [Epoch: 7089 Step: 00063800] Batch Recognition Loss:   0.000127 => Gls Tokens per Sec:      612 || Batch Translation Loss:   0.010691 => Txt Tokens per Sec:     1674 || Lr: 0.000050
2024-02-11 06:18:38,107 Epoch 7089: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 06:18:38,107 EPOCH 7090
2024-02-11 06:18:54,045 Epoch 7090: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 06:18:54,046 EPOCH 7091
2024-02-11 06:19:10,089 Epoch 7091: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 06:19:10,090 EPOCH 7092
2024-02-11 06:19:26,106 Epoch 7092: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 06:19:26,106 EPOCH 7093
2024-02-11 06:19:42,090 Epoch 7093: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 06:19:42,091 EPOCH 7094
2024-02-11 06:19:58,070 Epoch 7094: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 06:19:58,071 EPOCH 7095
2024-02-11 06:20:14,182 Epoch 7095: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 06:20:14,182 EPOCH 7096
2024-02-11 06:20:30,464 Epoch 7096: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 06:20:30,464 EPOCH 7097
2024-02-11 06:20:46,575 Epoch 7097: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 06:20:46,576 EPOCH 7098
2024-02-11 06:21:02,528 Epoch 7098: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 06:21:02,529 EPOCH 7099
2024-02-11 06:21:18,435 Epoch 7099: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 06:21:18,435 EPOCH 7100
2024-02-11 06:21:34,752 [Epoch: 7100 Step: 00063900] Batch Recognition Loss:   0.000123 => Gls Tokens per Sec:      651 || Batch Translation Loss:   0.021495 => Txt Tokens per Sec:     1801 || Lr: 0.000050
2024-02-11 06:21:34,753 Epoch 7100: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 06:21:34,753 EPOCH 7101
2024-02-11 06:21:50,642 Epoch 7101: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 06:21:50,643 EPOCH 7102
2024-02-11 06:22:07,081 Epoch 7102: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 06:22:07,082 EPOCH 7103
2024-02-11 06:22:23,080 Epoch 7103: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 06:22:23,080 EPOCH 7104
2024-02-11 06:22:38,983 Epoch 7104: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-11 06:22:38,983 EPOCH 7105
2024-02-11 06:22:55,274 Epoch 7105: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-11 06:22:55,274 EPOCH 7106
2024-02-11 06:23:11,179 Epoch 7106: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 06:23:11,180 EPOCH 7107
2024-02-11 06:23:27,470 Epoch 7107: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 06:23:27,471 EPOCH 7108
2024-02-11 06:23:43,832 Epoch 7108: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 06:23:43,832 EPOCH 7109
2024-02-11 06:24:00,070 Epoch 7109: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 06:24:00,070 EPOCH 7110
2024-02-11 06:24:16,410 Epoch 7110: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 06:24:16,410 EPOCH 7111
2024-02-11 06:24:32,391 Epoch 7111: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 06:24:32,391 EPOCH 7112
2024-02-11 06:24:36,692 [Epoch: 7112 Step: 00064000] Batch Recognition Loss:   0.000333 => Gls Tokens per Sec:       88 || Batch Translation Loss:   0.005656 => Txt Tokens per Sec:      314 || Lr: 0.000050
2024-02-11 06:25:48,849 Validation result at epoch 7112, step    64000: duration: 72.1566s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.24701	Translation Loss: 110488.64062	PPL: 62045.34375
	Eval Metric: BLEU
	WER 2.47	(DEL: 0.00,	INS: 0.00,	SUB: 2.47)
	BLEU-4 0.41	(BLEU-1: 9.60,	BLEU-2: 2.55,	BLEU-3: 0.87,	BLEU-4: 0.41)
	CHRF 16.33	ROUGE 8.15
2024-02-11 06:25:48,851 Logging Recognition and Translation Outputs
2024-02-11 06:25:48,851 ========================================================================================================================
2024-02-11 06:25:48,851 Logging Sequence: 122_86.00
2024-02-11 06:25:48,852 	Gloss Reference :	A B+C+D+E
2024-02-11 06:25:48,852 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 06:25:48,852 	Gloss Alignment :	         
2024-02-11 06:25:48,852 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 06:25:48,853 	Text Reference  :	after winning chanu spoke to    the media  and said   
2024-02-11 06:25:48,853 	Text Hypothesis :	***** he      won   the   world cup before her innings
2024-02-11 06:25:48,853 	Text Alignment  :	D     S       S     S     S     S   S      S   S      
2024-02-11 06:25:48,853 ========================================================================================================================
2024-02-11 06:25:48,853 Logging Sequence: 82_81.00
2024-02-11 06:25:48,854 	Gloss Reference :	A B+C+D+E
2024-02-11 06:25:48,854 	Gloss Hypothesis:	A B+C+D  
2024-02-11 06:25:48,854 	Gloss Alignment :	  S      
2024-02-11 06:25:48,854 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 06:25:48,855 	Text Reference  :	since the couple were residents of       mumbai the mumbai police  cyber cell began investigating the matter
2024-02-11 06:25:48,855 	Text Hypothesis :	***** *** ****** on   13th      february 2023   the ****** auction was   tv   and   kozhikode     in  2019  
2024-02-11 06:25:48,855 	Text Alignment  :	D     D   D      S    S         S        S          D      S       S     S    S     S             S   S     
2024-02-11 06:25:48,856 ========================================================================================================================
2024-02-11 06:25:48,856 Logging Sequence: 61_65.00
2024-02-11 06:25:48,856 	Gloss Reference :	A B+C+D+E
2024-02-11 06:25:48,856 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 06:25:48,856 	Gloss Alignment :	         
2024-02-11 06:25:48,856 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 06:25:48,857 	Text Reference  :	the name seems indian but whether it has been     made    by     an  indian
2024-02-11 06:25:48,857 	Text Hypothesis :	*** **** ***** ****** *** gambhir is an  argument between season and kohli 
2024-02-11 06:25:48,857 	Text Alignment  :	D   D    D     D      D   S       S  S   S        S       S      S   S     
2024-02-11 06:25:48,858 ========================================================================================================================
2024-02-11 06:25:48,858 Logging Sequence: 179_126.00
2024-02-11 06:25:48,858 	Gloss Reference :	A B+C+D+E
2024-02-11 06:25:48,858 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 06:25:48,858 	Gloss Alignment :	         
2024-02-11 06:25:48,858 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 06:25:48,860 	Text Reference  :	vinesh argued that she might contract coronavirus since these wrestlers travelled    from india where there are      many infections
2024-02-11 06:25:48,860 	Text Hypothesis :	****** ****** **** *** ***** ******** *********** ***** he    also      participated in   the   2020  tokyo olympics very 2021      
2024-02-11 06:25:48,860 	Text Alignment  :	D      D      D    D   D     D        D           D     S     S         S            S    S     S     S     S        S    S         
2024-02-11 06:25:48,860 ========================================================================================================================
2024-02-11 06:25:48,860 Logging Sequence: 62_24.00
2024-02-11 06:25:48,860 	Gloss Reference :	A B+C+D+E
2024-02-11 06:25:48,861 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 06:25:48,861 	Gloss Alignment :	         
2024-02-11 06:25:48,861 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 06:25:48,863 	Text Reference  :	now the women's cricket team  too  is  giving splendid performances which are at  par   with the    men's team 
2024-02-11 06:25:48,863 	Text Hypothesis :	we  is  have    to      thank bcci was you    about    me           tell  you the woman an   number of    india
2024-02-11 06:25:48,863 	Text Alignment  :	S   S   S       S       S     S    S   S      S        S            S     S   S   S     S    S      S     S    
2024-02-11 06:25:48,863 ========================================================================================================================
2024-02-11 06:26:01,393 Epoch 7112: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 06:26:01,394 EPOCH 7113
2024-02-11 06:26:17,238 Epoch 7113: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 06:26:17,238 EPOCH 7114
2024-02-11 06:26:33,133 Epoch 7114: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-11 06:26:33,133 EPOCH 7115
2024-02-11 06:26:49,191 Epoch 7115: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-11 06:26:49,192 EPOCH 7116
2024-02-11 06:27:05,488 Epoch 7116: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.32 
2024-02-11 06:27:05,489 EPOCH 7117
2024-02-11 06:27:21,704 Epoch 7117: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-11 06:27:21,705 EPOCH 7118
2024-02-11 06:27:37,974 Epoch 7118: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-11 06:27:37,975 EPOCH 7119
2024-02-11 06:27:54,053 Epoch 7119: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-11 06:27:54,053 EPOCH 7120
2024-02-11 06:28:09,738 Epoch 7120: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.38 
2024-02-11 06:28:09,738 EPOCH 7121
2024-02-11 06:28:25,698 Epoch 7121: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-11 06:28:25,698 EPOCH 7122
2024-02-11 06:28:41,747 Epoch 7122: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-11 06:28:41,747 EPOCH 7123
2024-02-11 06:28:46,449 [Epoch: 7123 Step: 00064100] Batch Recognition Loss:   0.000303 => Gls Tokens per Sec:      353 || Batch Translation Loss:   0.046800 => Txt Tokens per Sec:     1034 || Lr: 0.000050
2024-02-11 06:28:57,628 Epoch 7123: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.35 
2024-02-11 06:28:57,628 EPOCH 7124
2024-02-11 06:29:13,607 Epoch 7124: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-11 06:29:13,607 EPOCH 7125
2024-02-11 06:29:29,475 Epoch 7125: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-11 06:29:29,476 EPOCH 7126
2024-02-11 06:29:45,613 Epoch 7126: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-11 06:29:45,614 EPOCH 7127
2024-02-11 06:30:01,721 Epoch 7127: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-11 06:30:01,721 EPOCH 7128
2024-02-11 06:30:17,520 Epoch 7128: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-11 06:30:17,521 EPOCH 7129
2024-02-11 06:30:33,769 Epoch 7129: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 06:30:33,769 EPOCH 7130
2024-02-11 06:30:50,272 Epoch 7130: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 06:30:50,273 EPOCH 7131
2024-02-11 06:31:06,350 Epoch 7131: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 06:31:06,350 EPOCH 7132
2024-02-11 06:31:22,605 Epoch 7132: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-11 06:31:22,606 EPOCH 7133
2024-02-11 06:31:38,917 Epoch 7133: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-11 06:31:38,917 EPOCH 7134
2024-02-11 06:31:42,649 [Epoch: 7134 Step: 00064200] Batch Recognition Loss:   0.000351 => Gls Tokens per Sec:     1029 || Batch Translation Loss:   0.006254 => Txt Tokens per Sec:     2488 || Lr: 0.000050
2024-02-11 06:31:55,145 Epoch 7134: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-11 06:31:55,145 EPOCH 7135
2024-02-11 06:32:11,140 Epoch 7135: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:32:11,141 EPOCH 7136
2024-02-11 06:32:27,133 Epoch 7136: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-11 06:32:27,134 EPOCH 7137
2024-02-11 06:32:42,994 Epoch 7137: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:32:42,994 EPOCH 7138
2024-02-11 06:32:58,757 Epoch 7138: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:32:58,757 EPOCH 7139
2024-02-11 06:33:15,117 Epoch 7139: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:33:15,118 EPOCH 7140
2024-02-11 06:33:31,072 Epoch 7140: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:33:31,072 EPOCH 7141
2024-02-11 06:33:47,475 Epoch 7141: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:33:47,475 EPOCH 7142
2024-02-11 06:34:03,764 Epoch 7142: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:34:03,764 EPOCH 7143
2024-02-11 06:34:20,197 Epoch 7143: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:34:20,198 EPOCH 7144
2024-02-11 06:34:35,761 Epoch 7144: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:34:35,761 EPOCH 7145
2024-02-11 06:34:40,081 [Epoch: 7145 Step: 00064300] Batch Recognition Loss:   0.000274 => Gls Tokens per Sec:     1185 || Batch Translation Loss:   0.011830 => Txt Tokens per Sec:     3140 || Lr: 0.000050
2024-02-11 06:34:51,613 Epoch 7145: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:34:51,613 EPOCH 7146
2024-02-11 06:35:07,772 Epoch 7146: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:35:07,772 EPOCH 7147
2024-02-11 06:35:23,744 Epoch 7147: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:35:23,745 EPOCH 7148
2024-02-11 06:35:40,143 Epoch 7148: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:35:40,144 EPOCH 7149
2024-02-11 06:35:56,182 Epoch 7149: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:35:56,183 EPOCH 7150
2024-02-11 06:36:11,896 Epoch 7150: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:36:11,897 EPOCH 7151
2024-02-11 06:36:28,248 Epoch 7151: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:36:28,249 EPOCH 7152
2024-02-11 06:36:44,435 Epoch 7152: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:36:44,435 EPOCH 7153
2024-02-11 06:37:00,218 Epoch 7153: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:37:00,218 EPOCH 7154
2024-02-11 06:37:16,354 Epoch 7154: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:37:16,354 EPOCH 7155
2024-02-11 06:37:32,327 Epoch 7155: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:37:32,328 EPOCH 7156
2024-02-11 06:37:38,391 [Epoch: 7156 Step: 00064400] Batch Recognition Loss:   0.000288 => Gls Tokens per Sec:      907 || Batch Translation Loss:   0.007314 => Txt Tokens per Sec:     2248 || Lr: 0.000050
2024-02-11 06:37:48,469 Epoch 7156: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:37:48,469 EPOCH 7157
2024-02-11 06:38:04,469 Epoch 7157: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:38:04,470 EPOCH 7158
2024-02-11 06:38:20,613 Epoch 7158: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:38:20,614 EPOCH 7159
2024-02-11 06:38:36,547 Epoch 7159: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:38:36,547 EPOCH 7160
2024-02-11 06:38:52,715 Epoch 7160: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 06:38:52,715 EPOCH 7161
2024-02-11 06:39:08,727 Epoch 7161: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:39:08,727 EPOCH 7162
2024-02-11 06:39:24,805 Epoch 7162: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:39:24,805 EPOCH 7163
2024-02-11 06:39:40,786 Epoch 7163: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 06:39:40,787 EPOCH 7164
2024-02-11 06:39:56,915 Epoch 7164: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 06:39:56,915 EPOCH 7165
2024-02-11 06:40:13,202 Epoch 7165: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 06:40:13,203 EPOCH 7166
2024-02-11 06:40:29,366 Epoch 7166: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:40:29,367 EPOCH 7167
2024-02-11 06:40:41,754 [Epoch: 7167 Step: 00064500] Batch Recognition Loss:   0.000319 => Gls Tokens per Sec:      547 || Batch Translation Loss:   0.013898 => Txt Tokens per Sec:     1572 || Lr: 0.000050
2024-02-11 06:40:45,818 Epoch 7167: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 06:40:45,818 EPOCH 7168
2024-02-11 06:41:02,000 Epoch 7168: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 06:41:02,000 EPOCH 7169
2024-02-11 06:41:18,308 Epoch 7169: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 06:41:18,308 EPOCH 7170
2024-02-11 06:41:34,751 Epoch 7170: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 06:41:34,752 EPOCH 7171
2024-02-11 06:41:51,159 Epoch 7171: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:41:51,160 EPOCH 7172
2024-02-11 06:42:07,558 Epoch 7172: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:42:07,558 EPOCH 7173
2024-02-11 06:42:23,762 Epoch 7173: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 06:42:23,763 EPOCH 7174
2024-02-11 06:42:40,092 Epoch 7174: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 06:42:40,093 EPOCH 7175
2024-02-11 06:42:56,266 Epoch 7175: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 06:42:56,267 EPOCH 7176
2024-02-11 06:43:12,366 Epoch 7176: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 06:43:12,367 EPOCH 7177
2024-02-11 06:43:28,660 Epoch 7177: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 06:43:28,661 EPOCH 7178
2024-02-11 06:43:40,325 [Epoch: 7178 Step: 00064600] Batch Recognition Loss:   0.000155 => Gls Tokens per Sec:      768 || Batch Translation Loss:   0.018130 => Txt Tokens per Sec:     2157 || Lr: 0.000050
2024-02-11 06:43:44,974 Epoch 7178: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 06:43:44,974 EPOCH 7179
2024-02-11 06:44:01,458 Epoch 7179: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 06:44:01,459 EPOCH 7180
2024-02-11 06:44:17,480 Epoch 7180: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:44:17,480 EPOCH 7181
2024-02-11 06:44:33,632 Epoch 7181: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:44:33,632 EPOCH 7182
2024-02-11 06:44:49,954 Epoch 7182: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:44:49,955 EPOCH 7183
2024-02-11 06:45:05,716 Epoch 7183: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:45:05,717 EPOCH 7184
2024-02-11 06:45:22,015 Epoch 7184: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:45:22,015 EPOCH 7185
2024-02-11 06:45:38,295 Epoch 7185: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:45:38,296 EPOCH 7186
2024-02-11 06:45:54,040 Epoch 7186: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:45:54,041 EPOCH 7187
2024-02-11 06:46:10,465 Epoch 7187: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:46:10,465 EPOCH 7188
2024-02-11 06:46:26,546 Epoch 7188: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:46:26,547 EPOCH 7189
2024-02-11 06:46:42,492 [Epoch: 7189 Step: 00064700] Batch Recognition Loss:   0.000158 => Gls Tokens per Sec:      586 || Batch Translation Loss:   0.009891 => Txt Tokens per Sec:     1664 || Lr: 0.000050
2024-02-11 06:46:42,825 Epoch 7189: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:46:42,825 EPOCH 7190
2024-02-11 06:46:58,761 Epoch 7190: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:46:58,762 EPOCH 7191
2024-02-11 06:47:15,229 Epoch 7191: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 06:47:15,230 EPOCH 7192
2024-02-11 06:47:31,209 Epoch 7192: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 06:47:31,209 EPOCH 7193
2024-02-11 06:47:47,204 Epoch 7193: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 06:47:47,205 EPOCH 7194
2024-02-11 06:48:03,483 Epoch 7194: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-11 06:48:03,484 EPOCH 7195
2024-02-11 06:48:19,628 Epoch 7195: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 06:48:19,629 EPOCH 7196
2024-02-11 06:48:35,842 Epoch 7196: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 06:48:35,842 EPOCH 7197
2024-02-11 06:48:51,892 Epoch 7197: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-11 06:48:51,892 EPOCH 7198
2024-02-11 06:49:07,790 Epoch 7198: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-11 06:49:07,790 EPOCH 7199
2024-02-11 06:49:23,974 Epoch 7199: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-11 06:49:23,974 EPOCH 7200
2024-02-11 06:49:39,601 [Epoch: 7200 Step: 00064800] Batch Recognition Loss:   0.000662 => Gls Tokens per Sec:      680 || Batch Translation Loss:   0.024583 => Txt Tokens per Sec:     1880 || Lr: 0.000050
2024-02-11 06:49:39,602 Epoch 7200: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 06:49:39,602 EPOCH 7201
2024-02-11 06:49:56,025 Epoch 7201: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-11 06:49:56,026 EPOCH 7202
2024-02-11 06:50:12,029 Epoch 7202: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 06:50:12,030 EPOCH 7203
2024-02-11 06:50:28,129 Epoch 7203: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 06:50:28,130 EPOCH 7204
2024-02-11 06:50:44,056 Epoch 7204: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 06:50:44,056 EPOCH 7205
2024-02-11 06:51:00,128 Epoch 7205: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 06:51:00,129 EPOCH 7206
2024-02-11 06:51:16,095 Epoch 7206: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 06:51:16,095 EPOCH 7207
2024-02-11 06:51:32,312 Epoch 7207: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 06:51:32,313 EPOCH 7208
2024-02-11 06:51:48,323 Epoch 7208: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 06:51:48,323 EPOCH 7209
2024-02-11 06:52:04,401 Epoch 7209: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 06:52:04,401 EPOCH 7210
2024-02-11 06:52:20,366 Epoch 7210: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 06:52:20,366 EPOCH 7211
2024-02-11 06:52:36,428 Epoch 7211: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 06:52:36,428 EPOCH 7212
2024-02-11 06:52:36,725 [Epoch: 7212 Step: 00064900] Batch Recognition Loss:   0.000192 => Gls Tokens per Sec:     4339 || Batch Translation Loss:   0.010974 => Txt Tokens per Sec:     9753 || Lr: 0.000050
2024-02-11 06:52:52,639 Epoch 7212: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-11 06:52:52,640 EPOCH 7213
2024-02-11 06:53:08,538 Epoch 7213: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-11 06:53:08,539 EPOCH 7214
2024-02-11 06:53:24,566 Epoch 7214: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 06:53:24,567 EPOCH 7215
2024-02-11 06:53:40,600 Epoch 7215: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 06:53:40,601 EPOCH 7216
2024-02-11 06:53:56,739 Epoch 7216: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 06:53:56,740 EPOCH 7217
2024-02-11 06:54:12,770 Epoch 7217: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 06:54:12,771 EPOCH 7218
2024-02-11 06:54:28,781 Epoch 7218: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 06:54:28,781 EPOCH 7219
2024-02-11 06:54:44,557 Epoch 7219: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 06:54:44,557 EPOCH 7220
2024-02-11 06:55:00,455 Epoch 7220: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 06:55:00,456 EPOCH 7221
2024-02-11 06:55:16,311 Epoch 7221: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 06:55:16,311 EPOCH 7222
2024-02-11 06:55:32,242 Epoch 7222: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:55:32,242 EPOCH 7223
2024-02-11 06:55:33,293 [Epoch: 7223 Step: 00065000] Batch Recognition Loss:   0.000190 => Gls Tokens per Sec:     2440 || Batch Translation Loss:   0.022171 => Txt Tokens per Sec:     6437 || Lr: 0.000050
2024-02-11 06:55:48,319 Epoch 7223: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 06:55:48,320 EPOCH 7224
2024-02-11 06:56:04,044 Epoch 7224: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:56:04,045 EPOCH 7225
2024-02-11 06:56:20,163 Epoch 7225: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:56:20,163 EPOCH 7226
2024-02-11 06:56:36,049 Epoch 7226: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:56:36,049 EPOCH 7227
2024-02-11 06:56:52,226 Epoch 7227: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:56:52,226 EPOCH 7228
2024-02-11 06:57:08,158 Epoch 7228: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:57:08,159 EPOCH 7229
2024-02-11 06:57:24,121 Epoch 7229: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 06:57:24,121 EPOCH 7230
2024-02-11 06:57:40,034 Epoch 7230: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 06:57:40,035 EPOCH 7231
2024-02-11 06:57:56,098 Epoch 7231: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 06:57:56,098 EPOCH 7232
2024-02-11 06:58:12,171 Epoch 7232: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 06:58:12,172 EPOCH 7233
2024-02-11 06:58:28,191 Epoch 7233: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-11 06:58:28,191 EPOCH 7234
2024-02-11 06:58:38,935 [Epoch: 7234 Step: 00065100] Batch Recognition Loss:   0.000114 => Gls Tokens per Sec:      274 || Batch Translation Loss:   0.009862 => Txt Tokens per Sec:      843 || Lr: 0.000050
2024-02-11 06:58:44,415 Epoch 7234: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 06:58:44,415 EPOCH 7235
2024-02-11 06:59:00,519 Epoch 7235: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 06:59:00,519 EPOCH 7236
2024-02-11 06:59:16,505 Epoch 7236: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 06:59:16,506 EPOCH 7237
2024-02-11 06:59:32,955 Epoch 7237: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 06:59:32,955 EPOCH 7238
2024-02-11 06:59:49,176 Epoch 7238: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 06:59:49,176 EPOCH 7239
2024-02-11 07:00:05,160 Epoch 7239: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 07:00:05,161 EPOCH 7240
2024-02-11 07:00:20,999 Epoch 7240: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 07:00:21,000 EPOCH 7241
2024-02-11 07:00:36,968 Epoch 7241: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 07:00:36,968 EPOCH 7242
2024-02-11 07:00:52,677 Epoch 7242: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 07:00:52,678 EPOCH 7243
2024-02-11 07:01:08,483 Epoch 7243: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 07:01:08,483 EPOCH 7244
2024-02-11 07:01:24,506 Epoch 7244: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 07:01:24,507 EPOCH 7245
2024-02-11 07:01:33,168 [Epoch: 7245 Step: 00065200] Batch Recognition Loss:   0.000488 => Gls Tokens per Sec:      487 || Batch Translation Loss:   0.004962 => Txt Tokens per Sec:     1477 || Lr: 0.000050
2024-02-11 07:01:40,459 Epoch 7245: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 07:01:40,460 EPOCH 7246
2024-02-11 07:01:57,033 Epoch 7246: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 07:01:57,033 EPOCH 7247
2024-02-11 07:02:13,334 Epoch 7247: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 07:02:13,334 EPOCH 7248
2024-02-11 07:02:29,317 Epoch 7248: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 07:02:29,318 EPOCH 7249
2024-02-11 07:02:45,599 Epoch 7249: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 07:02:45,600 EPOCH 7250
2024-02-11 07:03:01,489 Epoch 7250: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 07:03:01,489 EPOCH 7251
2024-02-11 07:03:17,574 Epoch 7251: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 07:03:17,575 EPOCH 7252
2024-02-11 07:03:33,364 Epoch 7252: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 07:03:33,365 EPOCH 7253
2024-02-11 07:03:49,262 Epoch 7253: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 07:03:49,262 EPOCH 7254
2024-02-11 07:04:05,335 Epoch 7254: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 07:04:05,336 EPOCH 7255
2024-02-11 07:04:21,234 Epoch 7255: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 07:04:21,235 EPOCH 7256
2024-02-11 07:04:29,954 [Epoch: 7256 Step: 00065300] Batch Recognition Loss:   0.000352 => Gls Tokens per Sec:      631 || Batch Translation Loss:   0.024238 => Txt Tokens per Sec:     1642 || Lr: 0.000050
2024-02-11 07:04:37,438 Epoch 7256: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 07:04:37,438 EPOCH 7257
2024-02-11 07:04:53,409 Epoch 7257: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 07:04:53,410 EPOCH 7258
2024-02-11 07:05:09,442 Epoch 7258: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-11 07:05:09,443 EPOCH 7259
2024-02-11 07:05:25,905 Epoch 7259: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-11 07:05:25,905 EPOCH 7260
2024-02-11 07:05:41,708 Epoch 7260: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-11 07:05:41,709 EPOCH 7261
2024-02-11 07:05:57,552 Epoch 7261: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-11 07:05:57,553 EPOCH 7262
2024-02-11 07:06:13,808 Epoch 7262: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-11 07:06:13,809 EPOCH 7263
2024-02-11 07:06:30,143 Epoch 7263: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-11 07:06:30,144 EPOCH 7264
2024-02-11 07:06:46,308 Epoch 7264: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-11 07:06:46,309 EPOCH 7265
2024-02-11 07:07:02,463 Epoch 7265: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-11 07:07:02,463 EPOCH 7266
2024-02-11 07:07:18,603 Epoch 7266: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 07:07:18,604 EPOCH 7267
2024-02-11 07:07:25,265 [Epoch: 7267 Step: 00065400] Batch Recognition Loss:   0.000140 => Gls Tokens per Sec:     1018 || Batch Translation Loss:   0.014098 => Txt Tokens per Sec:     2638 || Lr: 0.000050
2024-02-11 07:07:34,777 Epoch 7267: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 07:07:34,777 EPOCH 7268
2024-02-11 07:07:50,886 Epoch 7268: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-11 07:07:50,887 EPOCH 7269
2024-02-11 07:08:07,031 Epoch 7269: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-11 07:08:07,031 EPOCH 7270
2024-02-11 07:08:22,781 Epoch 7270: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 07:08:22,782 EPOCH 7271
2024-02-11 07:08:38,679 Epoch 7271: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 07:08:38,679 EPOCH 7272
2024-02-11 07:08:54,613 Epoch 7272: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 07:08:54,613 EPOCH 7273
2024-02-11 07:09:10,828 Epoch 7273: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 07:09:10,828 EPOCH 7274
2024-02-11 07:09:26,714 Epoch 7274: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 07:09:26,714 EPOCH 7275
2024-02-11 07:09:42,926 Epoch 7275: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 07:09:42,927 EPOCH 7276
2024-02-11 07:09:59,114 Epoch 7276: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-11 07:09:59,115 EPOCH 7277
2024-02-11 07:10:15,295 Epoch 7277: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 07:10:15,295 EPOCH 7278
2024-02-11 07:10:30,528 [Epoch: 7278 Step: 00065500] Batch Recognition Loss:   0.000373 => Gls Tokens per Sec:      529 || Batch Translation Loss:   0.008071 => Txt Tokens per Sec:     1484 || Lr: 0.000050
2024-02-11 07:10:31,462 Epoch 7278: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 07:10:31,463 EPOCH 7279
2024-02-11 07:10:47,526 Epoch 7279: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 07:10:47,527 EPOCH 7280
2024-02-11 07:11:03,617 Epoch 7280: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 07:11:03,617 EPOCH 7281
2024-02-11 07:11:19,511 Epoch 7281: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 07:11:19,512 EPOCH 7282
2024-02-11 07:11:35,518 Epoch 7282: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 07:11:35,518 EPOCH 7283
2024-02-11 07:11:51,666 Epoch 7283: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 07:11:51,666 EPOCH 7284
2024-02-11 07:12:07,732 Epoch 7284: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 07:12:07,733 EPOCH 7285
2024-02-11 07:12:23,868 Epoch 7285: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:12:23,868 EPOCH 7286
2024-02-11 07:12:39,976 Epoch 7286: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:12:39,977 EPOCH 7287
2024-02-11 07:12:55,782 Epoch 7287: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:12:55,783 EPOCH 7288
2024-02-11 07:13:11,778 Epoch 7288: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 07:13:11,778 EPOCH 7289
2024-02-11 07:13:24,620 [Epoch: 7289 Step: 00065600] Batch Recognition Loss:   0.000534 => Gls Tokens per Sec:      727 || Batch Translation Loss:   0.013132 => Txt Tokens per Sec:     1972 || Lr: 0.000050
2024-02-11 07:13:28,045 Epoch 7289: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:13:28,045 EPOCH 7290
2024-02-11 07:13:44,147 Epoch 7290: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:13:44,147 EPOCH 7291
2024-02-11 07:14:00,131 Epoch 7291: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:14:00,132 EPOCH 7292
2024-02-11 07:14:16,146 Epoch 7292: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:14:16,147 EPOCH 7293
2024-02-11 07:14:32,558 Epoch 7293: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 07:14:32,559 EPOCH 7294
2024-02-11 07:14:48,546 Epoch 7294: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:14:48,546 EPOCH 7295
2024-02-11 07:15:04,613 Epoch 7295: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 07:15:04,613 EPOCH 7296
2024-02-11 07:15:20,757 Epoch 7296: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 07:15:20,758 EPOCH 7297
2024-02-11 07:15:37,303 Epoch 7297: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 07:15:37,304 EPOCH 7298
2024-02-11 07:15:53,561 Epoch 7298: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 07:15:53,562 EPOCH 7299
2024-02-11 07:16:09,696 Epoch 7299: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 07:16:09,697 EPOCH 7300
2024-02-11 07:16:25,655 [Epoch: 7300 Step: 00065700] Batch Recognition Loss:   0.000223 => Gls Tokens per Sec:      666 || Batch Translation Loss:   0.009957 => Txt Tokens per Sec:     1841 || Lr: 0.000050
2024-02-11 07:16:25,656 Epoch 7300: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:16:25,656 EPOCH 7301
2024-02-11 07:16:41,576 Epoch 7301: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 07:16:41,576 EPOCH 7302
2024-02-11 07:16:57,428 Epoch 7302: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:16:57,428 EPOCH 7303
2024-02-11 07:17:13,429 Epoch 7303: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 07:17:13,429 EPOCH 7304
2024-02-11 07:17:29,702 Epoch 7304: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 07:17:29,703 EPOCH 7305
2024-02-11 07:17:46,018 Epoch 7305: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 07:17:46,018 EPOCH 7306
2024-02-11 07:18:02,136 Epoch 7306: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 07:18:02,137 EPOCH 7307
2024-02-11 07:18:18,181 Epoch 7307: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 07:18:18,182 EPOCH 7308
2024-02-11 07:18:34,187 Epoch 7308: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 07:18:34,187 EPOCH 7309
2024-02-11 07:18:50,325 Epoch 7309: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 07:18:50,326 EPOCH 7310
2024-02-11 07:19:06,267 Epoch 7310: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 07:19:06,268 EPOCH 7311
2024-02-11 07:19:22,394 Epoch 7311: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.58 
2024-02-11 07:19:22,395 EPOCH 7312
2024-02-11 07:19:22,887 [Epoch: 7312 Step: 00065800] Batch Recognition Loss:   0.000263 => Gls Tokens per Sec:     2607 || Batch Translation Loss:   0.096678 => Txt Tokens per Sec:     7507 || Lr: 0.000050
2024-02-11 07:19:38,565 Epoch 7312: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.69 
2024-02-11 07:19:38,566 EPOCH 7313
2024-02-11 07:19:54,387 Epoch 7313: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-11 07:19:54,388 EPOCH 7314
2024-02-11 07:20:10,412 Epoch 7314: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-11 07:20:10,413 EPOCH 7315
2024-02-11 07:20:26,980 Epoch 7315: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.45 
2024-02-11 07:20:26,981 EPOCH 7316
2024-02-11 07:20:42,939 Epoch 7316: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-11 07:20:42,939 EPOCH 7317
2024-02-11 07:20:58,772 Epoch 7317: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-11 07:20:58,773 EPOCH 7318
2024-02-11 07:21:14,880 Epoch 7318: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-11 07:21:14,881 EPOCH 7319
2024-02-11 07:21:31,065 Epoch 7319: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-11 07:21:31,066 EPOCH 7320
2024-02-11 07:21:46,787 Epoch 7320: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 07:21:46,788 EPOCH 7321
2024-02-11 07:22:02,950 Epoch 7321: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 07:22:02,951 EPOCH 7322
2024-02-11 07:22:18,940 Epoch 7322: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 07:22:18,940 EPOCH 7323
2024-02-11 07:22:25,014 [Epoch: 7323 Step: 00065900] Batch Recognition Loss:   0.000436 => Gls Tokens per Sec:      422 || Batch Translation Loss:   0.019100 => Txt Tokens per Sec:     1315 || Lr: 0.000050
2024-02-11 07:22:34,878 Epoch 7323: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 07:22:34,878 EPOCH 7324
2024-02-11 07:22:50,503 Epoch 7324: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 07:22:50,503 EPOCH 7325
2024-02-11 07:23:06,235 Epoch 7325: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 07:23:06,236 EPOCH 7326
2024-02-11 07:23:22,274 Epoch 7326: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 07:23:22,275 EPOCH 7327
2024-02-11 07:23:38,492 Epoch 7327: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 07:23:38,492 EPOCH 7328
2024-02-11 07:23:55,643 Epoch 7328: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 07:23:55,643 EPOCH 7329
2024-02-11 07:24:11,702 Epoch 7329: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 07:24:11,703 EPOCH 7330
2024-02-11 07:24:27,657 Epoch 7330: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 07:24:27,657 EPOCH 7331
2024-02-11 07:24:43,587 Epoch 7331: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 07:24:43,588 EPOCH 7332
2024-02-11 07:25:00,059 Epoch 7332: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 07:25:00,059 EPOCH 7333
2024-02-11 07:25:16,233 Epoch 7333: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 07:25:16,234 EPOCH 7334
2024-02-11 07:25:20,293 [Epoch: 7334 Step: 00066000] Batch Recognition Loss:   0.000286 => Gls Tokens per Sec:      946 || Batch Translation Loss:   0.006849 => Txt Tokens per Sec:     2539 || Lr: 0.000050
2024-02-11 07:26:32,551 Validation result at epoch 7334, step    66000: duration: 72.2565s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.25011	Translation Loss: 112036.29688	PPL: 72417.32031
	Eval Metric: BLEU
	WER 2.33	(DEL: 0.00,	INS: 0.00,	SUB: 2.33)
	BLEU-4 0.40	(BLEU-1: 9.68,	BLEU-2: 2.69,	BLEU-3: 0.86,	BLEU-4: 0.40)
	CHRF 16.53	ROUGE 8.18
2024-02-11 07:26:32,553 Logging Recognition and Translation Outputs
2024-02-11 07:26:32,553 ========================================================================================================================
2024-02-11 07:26:32,553 Logging Sequence: 85_97.00
2024-02-11 07:26:32,553 	Gloss Reference :	A B+C+D+E
2024-02-11 07:26:32,554 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 07:26:32,554 	Gloss Alignment :	         
2024-02-11 07:26:32,554 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 07:26:32,555 	Text Reference  :	** *** like india's bcci australia has *** ******* *** *** ********* *** ** cricket australia
2024-02-11 07:26:32,555 	Text Hypothesis :	it was very sad     by   he        has now retired for his centuries and he has     scored   
2024-02-11 07:26:32,555 	Text Alignment  :	I  I   S    S       S    S             I   I       I   I   I         I   I  S       S        
2024-02-11 07:26:32,555 ========================================================================================================================
2024-02-11 07:26:32,555 Logging Sequence: 53_161.00
2024-02-11 07:26:32,556 	Gloss Reference :	A B+C+D+E
2024-02-11 07:26:32,556 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 07:26:32,556 	Gloss Alignment :	         
2024-02-11 07:26:32,556 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 07:26:32,557 	Text Reference  :	rashid has also been urging people to   donate to  his      rashid khan  foundation and afghanistan cricket association
2024-02-11 07:26:32,558 	Text Hypothesis :	****** *** **** well let    me     tell you    the olympics in     tokyo olympics   in  currently   his     2021       
2024-02-11 07:26:32,558 	Text Alignment  :	D      D   D    S    S      S      S    S      S   S        S      S     S          S   S           S       S          
2024-02-11 07:26:32,558 ========================================================================================================================
2024-02-11 07:26:32,558 Logging Sequence: 101_97.00
2024-02-11 07:26:32,558 	Gloss Reference :	A B+C+D+E
2024-02-11 07:26:32,558 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 07:26:32,558 	Gloss Alignment :	         
2024-02-11 07:26:32,559 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 07:26:32,561 	Text Reference  :	2 of the best indian bowlers who bowled very well    were    raj bawa who took    5  wickets and ******* ravi kumar who   took 4       wickets
2024-02-11 07:26:32,561 	Text Hypothesis :	* ** *** **** ****** ******* *** ****** in   india's amazing to  see  the auction in england and england lost the   score of   india's 0      
2024-02-11 07:26:32,561 	Text Alignment  :	D D  D   D    D      D       D   D      S    S       S       S   S    S   S       S  S           I       S    S     S     S    S       S      
2024-02-11 07:26:32,561 ========================================================================================================================
2024-02-11 07:26:32,562 Logging Sequence: 118_130.00
2024-02-11 07:26:32,562 	Gloss Reference :	A B+C+D+E
2024-02-11 07:26:32,562 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 07:26:32,562 	Gloss Alignment :	         
2024-02-11 07:26:32,562 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 07:26:32,564 	Text Reference  :	**** messi's fans   the     entire team       were      in  tears everyone was   overwhelmed by the victory
2024-02-11 07:26:32,564 	Text Hypothesis :	many former  indian captain dilip  vengsarkar expressed her 2023  to       their way         to 4   teams  
2024-02-11 07:26:32,564 	Text Alignment  :	I    S       S      S       S      S          S         S   S     S        S     S           S  S   S      
2024-02-11 07:26:32,564 ========================================================================================================================
2024-02-11 07:26:32,564 Logging Sequence: 170_195.00
2024-02-11 07:26:32,565 	Gloss Reference :	A B+C+D+E
2024-02-11 07:26:32,565 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 07:26:32,565 	Gloss Alignment :	         
2024-02-11 07:26:32,565 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 07:26:32,566 	Text Reference  :	********* ** i    moved    to   rajasthan royals team  as      they    paid me   8       crores
2024-02-11 07:26:32,566 	Text Hypothesis :	yesterday on 23rd november 2022 there     was    match between germany of   14th october 2023  
2024-02-11 07:26:32,566 	Text Alignment  :	I         I  S    S        S    S         S      S     S       S       S    S    S       S     
2024-02-11 07:26:32,567 ========================================================================================================================
2024-02-11 07:26:44,998 Epoch 7334: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:26:44,999 EPOCH 7335
2024-02-11 07:27:01,234 Epoch 7335: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:27:01,235 EPOCH 7336
2024-02-11 07:27:17,595 Epoch 7336: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:27:17,596 EPOCH 7337
2024-02-11 07:27:33,384 Epoch 7337: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:27:33,384 EPOCH 7338
2024-02-11 07:27:49,671 Epoch 7338: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 07:27:49,671 EPOCH 7339
2024-02-11 07:28:06,080 Epoch 7339: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:28:06,081 EPOCH 7340
2024-02-11 07:28:21,804 Epoch 7340: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 07:28:21,804 EPOCH 7341
2024-02-11 07:28:38,034 Epoch 7341: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:28:38,035 EPOCH 7342
2024-02-11 07:28:54,157 Epoch 7342: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:28:54,158 EPOCH 7343
2024-02-11 07:29:10,536 Epoch 7343: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 07:29:10,536 EPOCH 7344
2024-02-11 07:29:26,743 Epoch 7344: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 07:29:26,744 EPOCH 7345
2024-02-11 07:29:33,972 [Epoch: 7345 Step: 00066100] Batch Recognition Loss:   0.000087 => Gls Tokens per Sec:      709 || Batch Translation Loss:   0.009501 => Txt Tokens per Sec:     1966 || Lr: 0.000050
2024-02-11 07:29:43,077 Epoch 7345: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 07:29:43,078 EPOCH 7346
2024-02-11 07:29:58,967 Epoch 7346: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 07:29:58,968 EPOCH 7347
2024-02-11 07:30:15,065 Epoch 7347: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 07:30:15,066 EPOCH 7348
2024-02-11 07:30:31,483 Epoch 7348: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 07:30:31,484 EPOCH 7349
2024-02-11 07:30:47,506 Epoch 7349: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-11 07:30:47,507 EPOCH 7350
2024-02-11 07:31:03,484 Epoch 7350: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 07:31:03,485 EPOCH 7351
2024-02-11 07:31:19,565 Epoch 7351: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-11 07:31:19,566 EPOCH 7352
2024-02-11 07:31:35,713 Epoch 7352: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 07:31:35,714 EPOCH 7353
2024-02-11 07:31:51,758 Epoch 7353: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 07:31:51,758 EPOCH 7354
2024-02-11 07:32:08,297 Epoch 7354: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 07:32:08,297 EPOCH 7355
2024-02-11 07:32:24,518 Epoch 7355: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 07:32:24,518 EPOCH 7356
2024-02-11 07:32:28,951 [Epoch: 7356 Step: 00066200] Batch Recognition Loss:   0.000475 => Gls Tokens per Sec:     1444 || Batch Translation Loss:   0.012984 => Txt Tokens per Sec:     3617 || Lr: 0.000050
2024-02-11 07:32:40,389 Epoch 7356: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:32:40,390 EPOCH 7357
2024-02-11 07:32:56,486 Epoch 7357: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 07:32:56,487 EPOCH 7358
2024-02-11 07:33:12,719 Epoch 7358: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 07:33:12,719 EPOCH 7359
2024-02-11 07:33:28,937 Epoch 7359: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 07:33:28,937 EPOCH 7360
2024-02-11 07:33:44,686 Epoch 7360: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 07:33:44,686 EPOCH 7361
2024-02-11 07:34:00,806 Epoch 7361: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 07:34:00,807 EPOCH 7362
2024-02-11 07:34:17,070 Epoch 7362: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 07:34:17,070 EPOCH 7363
2024-02-11 07:34:34,086 Epoch 7363: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-11 07:34:34,087 EPOCH 7364
2024-02-11 07:34:50,180 Epoch 7364: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 07:34:50,181 EPOCH 7365
2024-02-11 07:35:06,097 Epoch 7365: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 07:35:06,098 EPOCH 7366
2024-02-11 07:35:22,360 Epoch 7366: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 07:35:22,360 EPOCH 7367
2024-02-11 07:35:34,560 [Epoch: 7367 Step: 00066300] Batch Recognition Loss:   0.001704 => Gls Tokens per Sec:      556 || Batch Translation Loss:   0.012579 => Txt Tokens per Sec:     1519 || Lr: 0.000050
2024-02-11 07:35:38,564 Epoch 7367: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 07:35:38,564 EPOCH 7368
2024-02-11 07:35:54,551 Epoch 7368: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 07:35:54,551 EPOCH 7369
2024-02-11 07:36:10,885 Epoch 7369: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 07:36:10,885 EPOCH 7370
2024-02-11 07:36:27,148 Epoch 7370: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 07:36:27,148 EPOCH 7371
2024-02-11 07:36:43,328 Epoch 7371: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 07:36:43,329 EPOCH 7372
2024-02-11 07:36:59,102 Epoch 7372: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 07:36:59,103 EPOCH 7373
2024-02-11 07:37:14,960 Epoch 7373: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 07:37:14,961 EPOCH 7374
2024-02-11 07:37:31,006 Epoch 7374: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-11 07:37:31,006 EPOCH 7375
2024-02-11 07:37:47,213 Epoch 7375: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 07:37:47,214 EPOCH 7376
2024-02-11 07:38:03,041 Epoch 7376: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 07:38:03,042 EPOCH 7377
2024-02-11 07:38:18,866 Epoch 7377: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 07:38:18,866 EPOCH 7378
2024-02-11 07:38:33,970 [Epoch: 7378 Step: 00066400] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:      534 || Batch Translation Loss:   0.011534 => Txt Tokens per Sec:     1483 || Lr: 0.000050
2024-02-11 07:38:35,151 Epoch 7378: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:38:35,151 EPOCH 7379
2024-02-11 07:38:51,209 Epoch 7379: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:38:51,210 EPOCH 7380
2024-02-11 07:39:07,249 Epoch 7380: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 07:39:07,249 EPOCH 7381
2024-02-11 07:39:23,108 Epoch 7381: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:39:23,109 EPOCH 7382
2024-02-11 07:39:39,333 Epoch 7382: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 07:39:39,333 EPOCH 7383
2024-02-11 07:39:55,518 Epoch 7383: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 07:39:55,518 EPOCH 7384
2024-02-11 07:40:11,799 Epoch 7384: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:40:11,800 EPOCH 7385
2024-02-11 07:40:27,997 Epoch 7385: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:40:27,998 EPOCH 7386
2024-02-11 07:40:43,701 Epoch 7386: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 07:40:43,701 EPOCH 7387
2024-02-11 07:40:59,829 Epoch 7387: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 07:40:59,830 EPOCH 7388
2024-02-11 07:41:16,317 Epoch 7388: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 07:41:16,318 EPOCH 7389
2024-02-11 07:41:31,999 [Epoch: 7389 Step: 00066500] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:      596 || Batch Translation Loss:   0.023067 => Txt Tokens per Sec:     1664 || Lr: 0.000050
2024-02-11 07:41:32,372 Epoch 7389: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:41:32,373 EPOCH 7390
2024-02-11 07:41:48,147 Epoch 7390: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:41:48,148 EPOCH 7391
2024-02-11 07:42:04,404 Epoch 7391: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:42:04,405 EPOCH 7392
2024-02-11 07:42:20,665 Epoch 7392: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:42:20,666 EPOCH 7393
2024-02-11 07:42:36,599 Epoch 7393: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 07:42:36,600 EPOCH 7394
2024-02-11 07:42:52,366 Epoch 7394: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 07:42:52,367 EPOCH 7395
2024-02-11 07:43:08,202 Epoch 7395: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 07:43:08,203 EPOCH 7396
2024-02-11 07:43:24,327 Epoch 7396: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:43:24,327 EPOCH 7397
2024-02-11 07:43:40,367 Epoch 7397: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-11 07:43:40,368 EPOCH 7398
2024-02-11 07:43:56,500 Epoch 7398: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 07:43:56,501 EPOCH 7399
2024-02-11 07:44:13,152 Epoch 7399: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 07:44:13,153 EPOCH 7400
2024-02-11 07:44:29,138 [Epoch: 7400 Step: 00066600] Batch Recognition Loss:   0.000174 => Gls Tokens per Sec:      664 || Batch Translation Loss:   0.006330 => Txt Tokens per Sec:     1838 || Lr: 0.000050
2024-02-11 07:44:29,139 Epoch 7400: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 07:44:29,139 EPOCH 7401
2024-02-11 07:44:45,239 Epoch 7401: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 07:44:45,239 EPOCH 7402
2024-02-11 07:45:01,344 Epoch 7402: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 07:45:01,345 EPOCH 7403
2024-02-11 07:45:17,459 Epoch 7403: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:45:17,460 EPOCH 7404
2024-02-11 07:45:33,611 Epoch 7404: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:45:33,612 EPOCH 7405
2024-02-11 07:45:49,414 Epoch 7405: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:45:49,415 EPOCH 7406
2024-02-11 07:46:05,412 Epoch 7406: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 07:46:05,413 EPOCH 7407
2024-02-11 07:46:21,584 Epoch 7407: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:46:21,585 EPOCH 7408
2024-02-11 07:46:37,782 Epoch 7408: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:46:37,783 EPOCH 7409
2024-02-11 07:46:53,845 Epoch 7409: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 07:46:53,845 EPOCH 7410
2024-02-11 07:47:09,842 Epoch 7410: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 07:47:09,843 EPOCH 7411
2024-02-11 07:47:26,150 Epoch 7411: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 07:47:26,151 EPOCH 7412
2024-02-11 07:47:29,286 [Epoch: 7412 Step: 00066700] Batch Recognition Loss:   0.000350 => Gls Tokens per Sec:      408 || Batch Translation Loss:   0.018583 => Txt Tokens per Sec:     1308 || Lr: 0.000050
2024-02-11 07:47:42,313 Epoch 7412: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 07:47:42,313 EPOCH 7413
2024-02-11 07:47:58,278 Epoch 7413: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 07:47:58,279 EPOCH 7414
2024-02-11 07:48:14,531 Epoch 7414: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-11 07:48:14,532 EPOCH 7415
2024-02-11 07:48:30,687 Epoch 7415: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 07:48:30,688 EPOCH 7416
2024-02-11 07:48:46,846 Epoch 7416: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.26 
2024-02-11 07:48:46,847 EPOCH 7417
2024-02-11 07:49:03,272 Epoch 7417: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-11 07:49:03,273 EPOCH 7418
2024-02-11 07:49:19,147 Epoch 7418: Total Training Recognition Loss 0.01  Total Training Translation Loss 4.53 
2024-02-11 07:49:19,148 EPOCH 7419
2024-02-11 07:49:35,319 Epoch 7419: Total Training Recognition Loss 0.03  Total Training Translation Loss 4.90 
2024-02-11 07:49:35,320 EPOCH 7420
2024-02-11 07:49:51,492 Epoch 7420: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.60 
2024-02-11 07:49:51,492 EPOCH 7421
2024-02-11 07:50:07,553 Epoch 7421: Total Training Recognition Loss 0.03  Total Training Translation Loss 1.06 
2024-02-11 07:50:07,554 EPOCH 7422
2024-02-11 07:50:23,859 Epoch 7422: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.54 
2024-02-11 07:50:23,860 EPOCH 7423
2024-02-11 07:50:24,761 [Epoch: 7423 Step: 00066800] Batch Recognition Loss:   0.000668 => Gls Tokens per Sec:     2848 || Batch Translation Loss:   0.045996 => Txt Tokens per Sec:     7039 || Lr: 0.000050
2024-02-11 07:50:40,028 Epoch 7423: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.39 
2024-02-11 07:50:40,029 EPOCH 7424
2024-02-11 07:50:55,983 Epoch 7424: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-11 07:50:55,984 EPOCH 7425
2024-02-11 07:51:12,112 Epoch 7425: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-11 07:51:12,113 EPOCH 7426
2024-02-11 07:51:28,201 Epoch 7426: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.19 
2024-02-11 07:51:28,201 EPOCH 7427
2024-02-11 07:51:44,094 Epoch 7427: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-11 07:51:44,095 EPOCH 7428
2024-02-11 07:51:59,918 Epoch 7428: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-11 07:51:59,918 EPOCH 7429
2024-02-11 07:52:15,806 Epoch 7429: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 07:52:15,806 EPOCH 7430
2024-02-11 07:52:31,841 Epoch 7430: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 07:52:31,842 EPOCH 7431
2024-02-11 07:52:48,391 Epoch 7431: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 07:52:48,392 EPOCH 7432
2024-02-11 07:53:05,073 Epoch 7432: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-11 07:53:05,074 EPOCH 7433
2024-02-11 07:53:21,449 Epoch 7433: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 07:53:21,450 EPOCH 7434
2024-02-11 07:53:26,358 [Epoch: 7434 Step: 00066900] Batch Recognition Loss:   0.000491 => Gls Tokens per Sec:      599 || Batch Translation Loss:   0.006337 => Txt Tokens per Sec:     1331 || Lr: 0.000050
2024-02-11 07:53:37,733 Epoch 7434: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-11 07:53:37,734 EPOCH 7435
2024-02-11 07:53:53,889 Epoch 7435: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-11 07:53:53,890 EPOCH 7436
2024-02-11 07:54:09,960 Epoch 7436: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 07:54:09,961 EPOCH 7437
2024-02-11 07:54:26,183 Epoch 7437: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 07:54:26,184 EPOCH 7438
2024-02-11 07:54:42,434 Epoch 7438: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 07:54:42,435 EPOCH 7439
2024-02-11 07:54:58,612 Epoch 7439: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 07:54:58,612 EPOCH 7440
2024-02-11 07:55:14,378 Epoch 7440: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 07:55:14,378 EPOCH 7441
2024-02-11 07:55:30,332 Epoch 7441: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:55:30,333 EPOCH 7442
2024-02-11 07:55:46,470 Epoch 7442: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:55:46,471 EPOCH 7443
2024-02-11 07:56:02,528 Epoch 7443: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 07:56:02,528 EPOCH 7444
2024-02-11 07:56:18,644 Epoch 7444: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:56:18,645 EPOCH 7445
2024-02-11 07:56:32,775 [Epoch: 7445 Step: 00067000] Batch Recognition Loss:   0.000562 => Gls Tokens per Sec:      299 || Batch Translation Loss:   0.006570 => Txt Tokens per Sec:      970 || Lr: 0.000050
2024-02-11 07:56:34,785 Epoch 7445: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:56:34,785 EPOCH 7446
2024-02-11 07:56:50,705 Epoch 7446: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:56:50,706 EPOCH 7447
2024-02-11 07:57:07,006 Epoch 7447: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:57:07,006 EPOCH 7448
2024-02-11 07:57:22,742 Epoch 7448: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:57:22,743 EPOCH 7449
2024-02-11 07:57:39,209 Epoch 7449: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:57:39,209 EPOCH 7450
2024-02-11 07:57:55,535 Epoch 7450: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:57:55,536 EPOCH 7451
2024-02-11 07:58:11,791 Epoch 7451: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 07:58:11,792 EPOCH 7452
2024-02-11 07:58:27,929 Epoch 7452: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:58:27,930 EPOCH 7453
2024-02-11 07:58:43,528 Epoch 7453: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:58:43,529 EPOCH 7454
2024-02-11 07:58:59,918 Epoch 7454: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 07:58:59,919 EPOCH 7455
2024-02-11 07:59:15,991 Epoch 7455: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:59:15,992 EPOCH 7456
2024-02-11 07:59:25,161 [Epoch: 7456 Step: 00067100] Batch Recognition Loss:   0.000294 => Gls Tokens per Sec:      600 || Batch Translation Loss:   0.005239 => Txt Tokens per Sec:     1756 || Lr: 0.000050
2024-02-11 07:59:32,325 Epoch 7456: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 07:59:32,325 EPOCH 7457
2024-02-11 07:59:48,432 Epoch 7457: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 07:59:48,433 EPOCH 7458
2024-02-11 08:00:04,609 Epoch 7458: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:00:04,609 EPOCH 7459
2024-02-11 08:00:20,354 Epoch 7459: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:00:20,355 EPOCH 7460
2024-02-11 08:00:36,374 Epoch 7460: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 08:00:36,374 EPOCH 7461
2024-02-11 08:00:52,450 Epoch 7461: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-11 08:00:52,450 EPOCH 7462
2024-02-11 08:01:08,622 Epoch 7462: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-11 08:01:08,622 EPOCH 7463
2024-02-11 08:01:24,625 Epoch 7463: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-11 08:01:24,625 EPOCH 7464
2024-02-11 08:01:40,840 Epoch 7464: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 08:01:40,841 EPOCH 7465
2024-02-11 08:01:57,037 Epoch 7465: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:01:57,038 EPOCH 7466
2024-02-11 08:02:13,256 Epoch 7466: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:02:13,256 EPOCH 7467
2024-02-11 08:02:24,214 [Epoch: 7467 Step: 00067200] Batch Recognition Loss:   0.000429 => Gls Tokens per Sec:      701 || Batch Translation Loss:   0.011402 => Txt Tokens per Sec:     2027 || Lr: 0.000050
2024-02-11 08:02:29,160 Epoch 7467: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 08:02:29,161 EPOCH 7468
2024-02-11 08:02:44,936 Epoch 7468: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:02:44,937 EPOCH 7469
2024-02-11 08:03:01,108 Epoch 7469: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 08:03:01,109 EPOCH 7470
2024-02-11 08:03:16,759 Epoch 7470: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:03:16,759 EPOCH 7471
2024-02-11 08:03:32,975 Epoch 7471: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:03:32,976 EPOCH 7472
2024-02-11 08:03:49,087 Epoch 7472: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:03:49,088 EPOCH 7473
2024-02-11 08:04:05,035 Epoch 7473: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:04:05,035 EPOCH 7474
2024-02-11 08:04:20,975 Epoch 7474: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:04:20,975 EPOCH 7475
2024-02-11 08:04:37,305 Epoch 7475: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:04:37,306 EPOCH 7476
2024-02-11 08:04:53,161 Epoch 7476: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:04:53,161 EPOCH 7477
2024-02-11 08:05:08,984 Epoch 7477: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:05:08,985 EPOCH 7478
2024-02-11 08:05:18,684 [Epoch: 7478 Step: 00067300] Batch Recognition Loss:   0.000229 => Gls Tokens per Sec:      831 || Batch Translation Loss:   0.010398 => Txt Tokens per Sec:     2239 || Lr: 0.000050
2024-02-11 08:05:24,961 Epoch 7478: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 08:05:24,961 EPOCH 7479
2024-02-11 08:05:41,346 Epoch 7479: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.43 
2024-02-11 08:05:41,347 EPOCH 7480
2024-02-11 08:05:57,809 Epoch 7480: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-11 08:05:57,809 EPOCH 7481
2024-02-11 08:06:13,897 Epoch 7481: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 08:06:13,898 EPOCH 7482
2024-02-11 08:06:30,020 Epoch 7482: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 08:06:30,021 EPOCH 7483
2024-02-11 08:06:45,842 Epoch 7483: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 08:06:45,843 EPOCH 7484
2024-02-11 08:07:01,964 Epoch 7484: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 08:07:01,965 EPOCH 7485
2024-02-11 08:07:18,239 Epoch 7485: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:07:18,239 EPOCH 7486
2024-02-11 08:07:34,458 Epoch 7486: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:07:34,458 EPOCH 7487
2024-02-11 08:07:50,346 Epoch 7487: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:07:50,347 EPOCH 7488
2024-02-11 08:08:06,218 Epoch 7488: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:08:06,219 EPOCH 7489
2024-02-11 08:08:21,988 [Epoch: 7489 Step: 00067400] Batch Recognition Loss:   0.000207 => Gls Tokens per Sec:      592 || Batch Translation Loss:   0.015772 => Txt Tokens per Sec:     1656 || Lr: 0.000050
2024-02-11 08:08:22,444 Epoch 7489: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:08:22,444 EPOCH 7490
2024-02-11 08:08:38,494 Epoch 7490: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:08:38,495 EPOCH 7491
2024-02-11 08:08:54,385 Epoch 7491: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:08:54,386 EPOCH 7492
2024-02-11 08:09:10,767 Epoch 7492: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:09:10,768 EPOCH 7493
2024-02-11 08:09:26,917 Epoch 7493: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:09:26,918 EPOCH 7494
2024-02-11 08:09:42,765 Epoch 7494: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:09:42,766 EPOCH 7495
2024-02-11 08:09:58,880 Epoch 7495: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:09:58,881 EPOCH 7496
2024-02-11 08:10:14,939 Epoch 7496: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 08:10:14,940 EPOCH 7497
2024-02-11 08:10:30,837 Epoch 7497: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:10:30,837 EPOCH 7498
2024-02-11 08:10:47,106 Epoch 7498: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:10:47,106 EPOCH 7499
2024-02-11 08:11:03,106 Epoch 7499: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:11:03,107 EPOCH 7500
2024-02-11 08:11:19,007 [Epoch: 7500 Step: 00067500] Batch Recognition Loss:   0.000195 => Gls Tokens per Sec:      668 || Batch Translation Loss:   0.007528 => Txt Tokens per Sec:     1848 || Lr: 0.000050
2024-02-11 08:11:19,007 Epoch 7500: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:11:19,007 EPOCH 7501
2024-02-11 08:11:35,096 Epoch 7501: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:11:35,096 EPOCH 7502
2024-02-11 08:11:51,049 Epoch 7502: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:11:51,049 EPOCH 7503
2024-02-11 08:12:07,028 Epoch 7503: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 08:12:07,029 EPOCH 7504
2024-02-11 08:12:23,019 Epoch 7504: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:12:23,019 EPOCH 7505
2024-02-11 08:12:39,123 Epoch 7505: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 08:12:39,124 EPOCH 7506
2024-02-11 08:12:55,040 Epoch 7506: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 08:12:55,041 EPOCH 7507
2024-02-11 08:13:11,478 Epoch 7507: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:13:11,478 EPOCH 7508
2024-02-11 08:13:27,359 Epoch 7508: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:13:27,359 EPOCH 7509
2024-02-11 08:13:43,354 Epoch 7509: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:13:43,355 EPOCH 7510
2024-02-11 08:13:59,199 Epoch 7510: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:13:59,200 EPOCH 7511
2024-02-11 08:14:15,268 Epoch 7511: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 08:14:15,268 EPOCH 7512
2024-02-11 08:14:18,572 [Epoch: 7512 Step: 00067600] Batch Recognition Loss:   0.000299 => Gls Tokens per Sec:      388 || Batch Translation Loss:   0.013078 => Txt Tokens per Sec:     1241 || Lr: 0.000050
2024-02-11 08:14:31,249 Epoch 7512: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:14:31,249 EPOCH 7513
2024-02-11 08:14:47,635 Epoch 7513: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:14:47,636 EPOCH 7514
2024-02-11 08:15:03,614 Epoch 7514: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:15:03,615 EPOCH 7515
2024-02-11 08:15:19,586 Epoch 7515: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:15:19,587 EPOCH 7516
2024-02-11 08:15:35,935 Epoch 7516: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:15:35,936 EPOCH 7517
2024-02-11 08:15:52,139 Epoch 7517: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:15:52,140 EPOCH 7518
2024-02-11 08:16:08,186 Epoch 7518: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:16:08,186 EPOCH 7519
2024-02-11 08:16:24,121 Epoch 7519: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:16:24,122 EPOCH 7520
2024-02-11 08:16:40,406 Epoch 7520: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:16:40,406 EPOCH 7521
2024-02-11 08:16:56,633 Epoch 7521: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:16:56,634 EPOCH 7522
2024-02-11 08:17:12,938 Epoch 7522: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:17:12,939 EPOCH 7523
2024-02-11 08:17:19,463 [Epoch: 7523 Step: 00067700] Batch Recognition Loss:   0.000378 => Gls Tokens per Sec:      393 || Batch Translation Loss:   0.029278 => Txt Tokens per Sec:     1272 || Lr: 0.000050
2024-02-11 08:17:29,135 Epoch 7523: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 08:17:29,136 EPOCH 7524
2024-02-11 08:17:45,418 Epoch 7524: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 08:17:45,418 EPOCH 7525
2024-02-11 08:18:01,615 Epoch 7525: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.09 
2024-02-11 08:18:01,616 EPOCH 7526
2024-02-11 08:18:17,933 Epoch 7526: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:18:17,934 EPOCH 7527
2024-02-11 08:18:34,177 Epoch 7527: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:18:34,177 EPOCH 7528
2024-02-11 08:18:50,198 Epoch 7528: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:18:50,199 EPOCH 7529
2024-02-11 08:19:05,858 Epoch 7529: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:19:05,858 EPOCH 7530
2024-02-11 08:19:21,876 Epoch 7530: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:19:21,877 EPOCH 7531
2024-02-11 08:19:37,840 Epoch 7531: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:19:37,841 EPOCH 7532
2024-02-11 08:19:54,053 Epoch 7532: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 08:19:54,054 EPOCH 7533
2024-02-11 08:20:10,115 Epoch 7533: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:20:10,115 EPOCH 7534
2024-02-11 08:20:13,985 [Epoch: 7534 Step: 00067800] Batch Recognition Loss:   0.000335 => Gls Tokens per Sec:      993 || Batch Translation Loss:   0.010080 => Txt Tokens per Sec:     2821 || Lr: 0.000050
2024-02-11 08:20:25,877 Epoch 7534: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:20:25,878 EPOCH 7535
2024-02-11 08:20:41,792 Epoch 7535: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:20:41,793 EPOCH 7536
2024-02-11 08:20:57,843 Epoch 7536: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:20:57,843 EPOCH 7537
2024-02-11 08:21:13,794 Epoch 7537: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:21:13,795 EPOCH 7538
2024-02-11 08:21:30,086 Epoch 7538: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 08:21:30,087 EPOCH 7539
2024-02-11 08:21:46,552 Epoch 7539: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 08:21:46,552 EPOCH 7540
2024-02-11 08:22:02,722 Epoch 7540: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 08:22:02,722 EPOCH 7541
2024-02-11 08:22:18,602 Epoch 7541: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 08:22:18,602 EPOCH 7542
2024-02-11 08:22:34,647 Epoch 7542: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 08:22:34,647 EPOCH 7543
2024-02-11 08:22:50,643 Epoch 7543: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 08:22:50,644 EPOCH 7544
2024-02-11 08:23:06,945 Epoch 7544: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:23:06,945 EPOCH 7545
2024-02-11 08:23:11,129 [Epoch: 7545 Step: 00067900] Batch Recognition Loss:   0.000126 => Gls Tokens per Sec:     1224 || Batch Translation Loss:   0.008098 => Txt Tokens per Sec:     3098 || Lr: 0.000050
2024-02-11 08:23:22,764 Epoch 7545: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 08:23:22,764 EPOCH 7546
2024-02-11 08:23:38,770 Epoch 7546: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:23:38,770 EPOCH 7547
2024-02-11 08:23:55,195 Epoch 7547: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:23:55,196 EPOCH 7548
2024-02-11 08:24:11,374 Epoch 7548: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:24:11,374 EPOCH 7549
2024-02-11 08:24:27,463 Epoch 7549: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:24:27,463 EPOCH 7550
2024-02-11 08:24:43,461 Epoch 7550: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 08:24:43,461 EPOCH 7551
2024-02-11 08:24:59,425 Epoch 7551: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 08:24:59,426 EPOCH 7552
2024-02-11 08:25:15,479 Epoch 7552: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 08:25:15,479 EPOCH 7553
2024-02-11 08:25:31,287 Epoch 7553: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 08:25:31,287 EPOCH 7554
2024-02-11 08:25:47,858 Epoch 7554: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 08:25:47,859 EPOCH 7555
2024-02-11 08:26:03,907 Epoch 7555: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 08:26:03,908 EPOCH 7556
2024-02-11 08:26:13,026 [Epoch: 7556 Step: 00068000] Batch Recognition Loss:   0.000110 => Gls Tokens per Sec:      603 || Batch Translation Loss:   0.012349 => Txt Tokens per Sec:     1771 || Lr: 0.000050
2024-02-11 08:27:24,693 Validation result at epoch 7556, step    68000: duration: 71.6648s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.24992	Translation Loss: 111715.64062	PPL: 70134.74219
	Eval Metric: BLEU
	WER 2.33	(DEL: 0.00,	INS: 0.00,	SUB: 2.33)
	BLEU-4 0.38	(BLEU-1: 9.90,	BLEU-2: 2.68,	BLEU-3: 0.90,	BLEU-4: 0.38)
	CHRF 16.17	ROUGE 8.31
2024-02-11 08:27:24,695 Logging Recognition and Translation Outputs
2024-02-11 08:27:24,695 ========================================================================================================================
2024-02-11 08:27:24,695 Logging Sequence: 148_186.00
2024-02-11 08:27:24,696 	Gloss Reference :	A B+C+D+E
2024-02-11 08:27:24,696 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 08:27:24,696 	Gloss Alignment :	         
2024-02-11 08:27:24,696 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 08:27:24,698 	Text Reference  :	*** siraj also    took four wickets in      1     over thus         becoming the record-holder for   most wickets   in an over in  odis   
2024-02-11 08:27:24,698 	Text Hypothesis :	the 2022  captain was  4    times   india's youth team participated in       the ************* world deaf badminton in ** **** 267 innings
2024-02-11 08:27:24,699 	Text Alignment  :	I   S     S       S    S    S       S       S     S    S            S            D             S     S    S            D  D    S   S      
2024-02-11 08:27:24,699 ========================================================================================================================
2024-02-11 08:27:24,699 Logging Sequence: 61_181.00
2024-02-11 08:27:24,699 	Gloss Reference :	A B+C+D+E
2024-02-11 08:27:24,699 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 08:27:24,699 	Gloss Alignment :	         
2024-02-11 08:27:24,700 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 08:27:24,701 	Text Reference  :	one other fan said it is babar's personal chat     we   should focuc on  this   cricketing career     and not  his personal life 
2024-02-11 08:27:24,702 	Text Hypothesis :	*** ***** *** **** ** ** on      15       february 2021 hotel  by    itc hotels through    bookingcom and when i   would    viral
2024-02-11 08:27:24,702 	Text Alignment  :	D   D     D   D    D  D  S       S        S        S    S      S     S   S      S          S              S    S   S        S    
2024-02-11 08:27:24,702 ========================================================================================================================
2024-02-11 08:27:24,702 Logging Sequence: 123_24.00
2024-02-11 08:27:24,702 	Gloss Reference :	A B+C+D+E
2024-02-11 08:27:24,702 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 08:27:24,703 	Gloss Alignment :	         
2024-02-11 08:27:24,703 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 08:27:24,703 	Text Reference  :	did you know that other than cricket dhoni       has another  passion
2024-02-11 08:27:24,703 	Text Hypothesis :	*** *** **** he   was   a    second  performance by  mohammed siraj  
2024-02-11 08:27:24,704 	Text Alignment  :	D   D   D    S    S     S    S       S           S   S        S      
2024-02-11 08:27:24,704 ========================================================================================================================
2024-02-11 08:27:24,704 Logging Sequence: 84_76.00
2024-02-11 08:27:24,704 	Gloss Reference :	A B+C+D+E
2024-02-11 08:27:24,704 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 08:27:24,705 	Gloss Alignment :	         
2024-02-11 08:27:24,705 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 08:27:24,705 	Text Reference  :	** the teams wanted to support   but were      refused 
2024-02-11 08:27:24,705 	Text Hypothesis :	if the ***** ****** ** situation is  extremely critical
2024-02-11 08:27:24,706 	Text Alignment  :	I      D     D      D  S         S   S         S       
2024-02-11 08:27:24,706 ========================================================================================================================
2024-02-11 08:27:24,706 Logging Sequence: 126_188.00
2024-02-11 08:27:24,706 	Gloss Reference :	A B+C+D+E
2024-02-11 08:27:24,706 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 08:27:24,706 	Gloss Alignment :	         
2024-02-11 08:27:24,707 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 08:27:24,707 	Text Reference  :	now he has  become a gold medalist at   the          2020 tokyo olympics
2024-02-11 08:27:24,707 	Text Hypothesis :	*** he also been   a **** ******** very disappointed by   these games   
2024-02-11 08:27:24,708 	Text Alignment  :	D      S    S        D    D        S    S            S    S     S       
2024-02-11 08:27:24,708 ========================================================================================================================
2024-02-11 08:27:32,298 Epoch 7556: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 08:27:32,299 EPOCH 7557
2024-02-11 08:27:48,532 Epoch 7557: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:27:48,532 EPOCH 7558
2024-02-11 08:28:05,117 Epoch 7558: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:28:05,117 EPOCH 7559
2024-02-11 08:28:21,158 Epoch 7559: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:28:21,159 EPOCH 7560
2024-02-11 08:28:36,632 Epoch 7560: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:28:36,632 EPOCH 7561
2024-02-11 08:28:52,573 Epoch 7561: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:28:52,573 EPOCH 7562
2024-02-11 08:29:08,364 Epoch 7562: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:29:08,365 EPOCH 7563
2024-02-11 08:29:24,340 Epoch 7563: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:29:24,341 EPOCH 7564
2024-02-11 08:29:40,350 Epoch 7564: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:29:40,351 EPOCH 7565
2024-02-11 08:29:56,460 Epoch 7565: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:29:56,461 EPOCH 7566
2024-02-11 08:30:12,380 Epoch 7566: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:30:12,380 EPOCH 7567
2024-02-11 08:30:21,907 [Epoch: 7567 Step: 00068100] Batch Recognition Loss:   0.000428 => Gls Tokens per Sec:      712 || Batch Translation Loss:   0.011468 => Txt Tokens per Sec:     1984 || Lr: 0.000050
2024-02-11 08:30:28,400 Epoch 7567: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:30:28,400 EPOCH 7568
2024-02-11 08:30:44,480 Epoch 7568: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:30:44,481 EPOCH 7569
2024-02-11 08:31:00,635 Epoch 7569: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:31:00,635 EPOCH 7570
2024-02-11 08:31:16,612 Epoch 7570: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:31:16,612 EPOCH 7571
2024-02-11 08:31:32,604 Epoch 7571: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:31:32,604 EPOCH 7572
2024-02-11 08:31:48,493 Epoch 7572: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:31:48,494 EPOCH 7573
2024-02-11 08:32:04,895 Epoch 7573: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:32:04,895 EPOCH 7574
2024-02-11 08:32:20,876 Epoch 7574: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-11 08:32:20,877 EPOCH 7575
2024-02-11 08:32:36,970 Epoch 7575: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:32:36,970 EPOCH 7576
2024-02-11 08:32:52,846 Epoch 7576: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 08:32:52,847 EPOCH 7577
2024-02-11 08:33:08,457 Epoch 7577: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:33:08,458 EPOCH 7578
2024-02-11 08:33:23,336 [Epoch: 7578 Step: 00068200] Batch Recognition Loss:   0.000330 => Gls Tokens per Sec:      542 || Batch Translation Loss:   0.015624 => Txt Tokens per Sec:     1490 || Lr: 0.000050
2024-02-11 08:33:24,380 Epoch 7578: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:33:24,381 EPOCH 7579
2024-02-11 08:33:40,000 Epoch 7579: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-11 08:33:40,001 EPOCH 7580
2024-02-11 08:33:55,905 Epoch 7580: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:33:55,905 EPOCH 7581
2024-02-11 08:34:11,616 Epoch 7581: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:34:11,616 EPOCH 7582
2024-02-11 08:34:28,020 Epoch 7582: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:34:28,020 EPOCH 7583
2024-02-11 08:34:44,119 Epoch 7583: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:34:44,119 EPOCH 7584
2024-02-11 08:35:00,384 Epoch 7584: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:35:00,385 EPOCH 7585
2024-02-11 08:35:16,426 Epoch 7585: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 08:35:16,426 EPOCH 7586
2024-02-11 08:35:32,611 Epoch 7586: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 08:35:32,611 EPOCH 7587
2024-02-11 08:35:48,385 Epoch 7587: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 08:35:48,385 EPOCH 7588
2024-02-11 08:36:04,330 Epoch 7588: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 08:36:04,331 EPOCH 7589
2024-02-11 08:36:20,194 [Epoch: 7589 Step: 00068300] Batch Recognition Loss:   0.000133 => Gls Tokens per Sec:      589 || Batch Translation Loss:   0.014991 => Txt Tokens per Sec:     1673 || Lr: 0.000050
2024-02-11 08:36:20,506 Epoch 7589: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 08:36:20,506 EPOCH 7590
2024-02-11 08:36:36,523 Epoch 7590: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 08:36:36,523 EPOCH 7591
2024-02-11 08:36:52,283 Epoch 7591: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 08:36:52,283 EPOCH 7592
2024-02-11 08:37:08,121 Epoch 7592: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 08:37:08,122 EPOCH 7593
2024-02-11 08:37:24,141 Epoch 7593: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 08:37:24,142 EPOCH 7594
2024-02-11 08:37:40,233 Epoch 7594: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-11 08:37:40,234 EPOCH 7595
2024-02-11 08:37:56,637 Epoch 7595: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-11 08:37:56,638 EPOCH 7596
2024-02-11 08:38:13,033 Epoch 7596: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-11 08:38:13,034 EPOCH 7597
2024-02-11 08:38:29,666 Epoch 7597: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-11 08:38:29,666 EPOCH 7598
2024-02-11 08:38:45,754 Epoch 7598: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.37 
2024-02-11 08:38:45,754 EPOCH 7599
2024-02-11 08:39:01,871 Epoch 7599: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.32 
2024-02-11 08:39:01,871 EPOCH 7600
2024-02-11 08:39:18,204 [Epoch: 7600 Step: 00068400] Batch Recognition Loss:   0.000891 => Gls Tokens per Sec:      650 || Batch Translation Loss:   0.117543 => Txt Tokens per Sec:     1799 || Lr: 0.000050
2024-02-11 08:39:18,204 Epoch 7600: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.47 
2024-02-11 08:39:18,204 EPOCH 7601
2024-02-11 08:39:34,149 Epoch 7601: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-11 08:39:34,149 EPOCH 7602
2024-02-11 08:39:50,668 Epoch 7602: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-11 08:39:50,669 EPOCH 7603
2024-02-11 08:40:06,973 Epoch 7603: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.26 
2024-02-11 08:40:06,974 EPOCH 7604
2024-02-11 08:40:23,191 Epoch 7604: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-11 08:40:23,192 EPOCH 7605
2024-02-11 08:40:39,168 Epoch 7605: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 08:40:39,169 EPOCH 7606
2024-02-11 08:40:55,285 Epoch 7606: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 08:40:55,286 EPOCH 7607
2024-02-11 08:41:11,429 Epoch 7607: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 08:41:11,430 EPOCH 7608
2024-02-11 08:41:27,477 Epoch 7608: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 08:41:27,478 EPOCH 7609
2024-02-11 08:41:43,673 Epoch 7609: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 08:41:43,674 EPOCH 7610
2024-02-11 08:41:59,932 Epoch 7610: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 08:41:59,932 EPOCH 7611
2024-02-11 08:42:16,034 Epoch 7611: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 08:42:16,035 EPOCH 7612
2024-02-11 08:42:20,544 [Epoch: 7612 Step: 00068500] Batch Recognition Loss:   0.000442 => Gls Tokens per Sec:       84 || Batch Translation Loss:   0.006912 => Txt Tokens per Sec:      301 || Lr: 0.000050
2024-02-11 08:42:32,103 Epoch 7612: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 08:42:32,104 EPOCH 7613
2024-02-11 08:42:48,246 Epoch 7613: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 08:42:48,247 EPOCH 7614
2024-02-11 08:43:04,235 Epoch 7614: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:43:04,235 EPOCH 7615
2024-02-11 08:43:20,447 Epoch 7615: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 08:43:20,447 EPOCH 7616
2024-02-11 08:43:36,154 Epoch 7616: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 08:43:36,155 EPOCH 7617
2024-02-11 08:43:52,398 Epoch 7617: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 08:43:52,399 EPOCH 7618
2024-02-11 08:44:08,375 Epoch 7618: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 08:44:08,376 EPOCH 7619
2024-02-11 08:44:24,612 Epoch 7619: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 08:44:24,613 EPOCH 7620
2024-02-11 08:44:40,759 Epoch 7620: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 08:44:40,759 EPOCH 7621
2024-02-11 08:44:56,709 Epoch 7621: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.28 
2024-02-11 08:44:56,709 EPOCH 7622
2024-02-11 08:45:12,757 Epoch 7622: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 08:45:12,758 EPOCH 7623
2024-02-11 08:45:16,744 [Epoch: 7623 Step: 00068600] Batch Recognition Loss:   0.000363 => Gls Tokens per Sec:      642 || Batch Translation Loss:   0.024736 => Txt Tokens per Sec:     2000 || Lr: 0.000050
2024-02-11 08:45:29,035 Epoch 7623: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-11 08:45:29,036 EPOCH 7624
2024-02-11 08:45:45,128 Epoch 7624: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 08:45:45,128 EPOCH 7625
2024-02-11 08:46:01,019 Epoch 7625: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.21 
2024-02-11 08:46:01,020 EPOCH 7626
2024-02-11 08:46:16,793 Epoch 7626: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.22 
2024-02-11 08:46:16,794 EPOCH 7627
2024-02-11 08:46:32,981 Epoch 7627: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 08:46:32,982 EPOCH 7628
2024-02-11 08:46:49,086 Epoch 7628: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-11 08:46:49,086 EPOCH 7629
2024-02-11 08:47:04,896 Epoch 7629: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 08:47:04,896 EPOCH 7630
2024-02-11 08:47:20,888 Epoch 7630: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-11 08:47:20,888 EPOCH 7631
2024-02-11 08:47:36,845 Epoch 7631: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 08:47:36,845 EPOCH 7632
2024-02-11 08:47:52,739 Epoch 7632: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 08:47:52,739 EPOCH 7633
2024-02-11 08:48:08,907 Epoch 7633: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 08:48:08,908 EPOCH 7634
2024-02-11 08:48:15,475 [Epoch: 7634 Step: 00068700] Batch Recognition Loss:   0.000750 => Gls Tokens per Sec:      585 || Batch Translation Loss:   0.017762 => Txt Tokens per Sec:     1665 || Lr: 0.000050
2024-02-11 08:48:25,059 Epoch 7634: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 08:48:25,060 EPOCH 7635
2024-02-11 08:48:40,769 Epoch 7635: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 08:48:40,769 EPOCH 7636
2024-02-11 08:48:56,783 Epoch 7636: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:48:56,783 EPOCH 7637
2024-02-11 08:49:12,999 Epoch 7637: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:49:13,000 EPOCH 7638
2024-02-11 08:49:29,065 Epoch 7638: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:49:29,066 EPOCH 7639
2024-02-11 08:49:45,128 Epoch 7639: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:49:45,129 EPOCH 7640
2024-02-11 08:50:00,719 Epoch 7640: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:50:00,720 EPOCH 7641
2024-02-11 08:50:16,879 Epoch 7641: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:50:16,880 EPOCH 7642
2024-02-11 08:50:33,101 Epoch 7642: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 08:50:33,102 EPOCH 7643
2024-02-11 08:50:49,087 Epoch 7643: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:50:49,087 EPOCH 7644
2024-02-11 08:51:05,249 Epoch 7644: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 08:51:05,249 EPOCH 7645
2024-02-11 08:51:10,043 [Epoch: 7645 Step: 00068800] Batch Recognition Loss:   0.000343 => Gls Tokens per Sec:     1068 || Batch Translation Loss:   0.015710 => Txt Tokens per Sec:     3171 || Lr: 0.000050
2024-02-11 08:51:21,219 Epoch 7645: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:51:21,220 EPOCH 7646
2024-02-11 08:51:37,310 Epoch 7646: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 08:51:37,311 EPOCH 7647
2024-02-11 08:51:53,315 Epoch 7647: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:51:53,316 EPOCH 7648
2024-02-11 08:52:09,430 Epoch 7648: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:52:09,431 EPOCH 7649
2024-02-11 08:52:25,312 Epoch 7649: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:52:25,312 EPOCH 7650
2024-02-11 08:52:41,214 Epoch 7650: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:52:41,214 EPOCH 7651
2024-02-11 08:52:57,415 Epoch 7651: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:52:57,416 EPOCH 7652
2024-02-11 08:53:13,439 Epoch 7652: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:53:13,439 EPOCH 7653
2024-02-11 08:53:29,739 Epoch 7653: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:53:29,740 EPOCH 7654
2024-02-11 08:53:46,173 Epoch 7654: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:53:46,173 EPOCH 7655
2024-02-11 08:54:02,247 Epoch 7655: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:54:02,248 EPOCH 7656
2024-02-11 08:54:10,715 [Epoch: 7656 Step: 00068900] Batch Recognition Loss:   0.000606 => Gls Tokens per Sec:      650 || Batch Translation Loss:   0.004843 => Txt Tokens per Sec:     1636 || Lr: 0.000050
2024-02-11 08:54:18,286 Epoch 7656: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:54:18,286 EPOCH 7657
2024-02-11 08:54:34,871 Epoch 7657: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-11 08:54:34,872 EPOCH 7658
2024-02-11 08:54:51,120 Epoch 7658: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:54:51,121 EPOCH 7659
2024-02-11 08:55:07,590 Epoch 7659: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:55:07,591 EPOCH 7660
2024-02-11 08:55:23,755 Epoch 7660: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:55:23,755 EPOCH 7661
2024-02-11 08:55:39,943 Epoch 7661: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:55:39,944 EPOCH 7662
2024-02-11 08:55:55,998 Epoch 7662: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:55:55,998 EPOCH 7663
2024-02-11 08:56:12,111 Epoch 7663: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:56:12,111 EPOCH 7664
2024-02-11 08:56:27,959 Epoch 7664: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:56:27,960 EPOCH 7665
2024-02-11 08:56:44,408 Epoch 7665: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:56:44,408 EPOCH 7666
2024-02-11 08:57:00,621 Epoch 7666: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:57:00,622 EPOCH 7667
2024-02-11 08:57:06,001 [Epoch: 7667 Step: 00069000] Batch Recognition Loss:   0.000144 => Gls Tokens per Sec:     1428 || Batch Translation Loss:   0.013479 => Txt Tokens per Sec:     3782 || Lr: 0.000050
2024-02-11 08:57:16,592 Epoch 7667: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:57:16,593 EPOCH 7668
2024-02-11 08:57:32,698 Epoch 7668: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:57:32,699 EPOCH 7669
2024-02-11 08:57:48,688 Epoch 7669: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:57:48,688 EPOCH 7670
2024-02-11 08:58:04,675 Epoch 7670: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:58:04,676 EPOCH 7671
2024-02-11 08:58:21,352 Epoch 7671: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:58:21,352 EPOCH 7672
2024-02-11 08:58:37,686 Epoch 7672: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.09 
2024-02-11 08:58:37,687 EPOCH 7673
2024-02-11 08:58:53,857 Epoch 7673: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:58:53,857 EPOCH 7674
2024-02-11 08:59:09,962 Epoch 7674: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:59:09,962 EPOCH 7675
2024-02-11 08:59:25,993 Epoch 7675: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 08:59:25,994 EPOCH 7676
2024-02-11 08:59:42,375 Epoch 7676: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:59:42,375 EPOCH 7677
2024-02-11 08:59:58,280 Epoch 7677: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 08:59:58,280 EPOCH 7678
2024-02-11 09:00:10,447 [Epoch: 7678 Step: 00069100] Batch Recognition Loss:   0.000606 => Gls Tokens per Sec:      663 || Batch Translation Loss:   0.004696 => Txt Tokens per Sec:     1759 || Lr: 0.000050
2024-02-11 09:00:14,177 Epoch 7678: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:00:14,177 EPOCH 7679
2024-02-11 09:00:30,005 Epoch 7679: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:00:30,006 EPOCH 7680
2024-02-11 09:00:46,127 Epoch 7680: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:00:46,128 EPOCH 7681
2024-02-11 09:01:02,443 Epoch 7681: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:01:02,444 EPOCH 7682
2024-02-11 09:01:18,599 Epoch 7682: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:01:18,599 EPOCH 7683
2024-02-11 09:01:34,585 Epoch 7683: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:01:34,586 EPOCH 7684
2024-02-11 09:01:50,871 Epoch 7684: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:01:50,872 EPOCH 7685
2024-02-11 09:02:07,146 Epoch 7685: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 09:02:07,146 EPOCH 7686
2024-02-11 09:02:23,271 Epoch 7686: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 09:02:23,272 EPOCH 7687
2024-02-11 09:02:39,385 Epoch 7687: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 09:02:39,386 EPOCH 7688
2024-02-11 09:02:55,740 Epoch 7688: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 09:02:55,740 EPOCH 7689
2024-02-11 09:03:10,915 [Epoch: 7689 Step: 00069200] Batch Recognition Loss:   0.000191 => Gls Tokens per Sec:      616 || Batch Translation Loss:   0.018255 => Txt Tokens per Sec:     1683 || Lr: 0.000050
2024-02-11 09:03:11,672 Epoch 7689: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 09:03:11,672 EPOCH 7690
2024-02-11 09:03:27,698 Epoch 7690: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 09:03:27,698 EPOCH 7691
2024-02-11 09:03:43,496 Epoch 7691: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 09:03:43,496 EPOCH 7692
2024-02-11 09:03:59,361 Epoch 7692: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 09:03:59,361 EPOCH 7693
2024-02-11 09:04:15,706 Epoch 7693: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 09:04:15,707 EPOCH 7694
2024-02-11 09:04:31,899 Epoch 7694: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 09:04:31,900 EPOCH 7695
2024-02-11 09:04:47,927 Epoch 7695: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 09:04:47,928 EPOCH 7696
2024-02-11 09:05:04,008 Epoch 7696: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 09:05:04,009 EPOCH 7697
2024-02-11 09:05:20,100 Epoch 7697: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 09:05:20,101 EPOCH 7698
2024-02-11 09:05:36,476 Epoch 7698: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 09:05:36,476 EPOCH 7699
2024-02-11 09:05:52,848 Epoch 7699: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-11 09:05:52,848 EPOCH 7700
2024-02-11 09:06:09,286 [Epoch: 7700 Step: 00069300] Batch Recognition Loss:   0.000166 => Gls Tokens per Sec:      646 || Batch Translation Loss:   0.017869 => Txt Tokens per Sec:     1788 || Lr: 0.000050
2024-02-11 09:06:09,286 Epoch 7700: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 09:06:09,286 EPOCH 7701
2024-02-11 09:06:25,244 Epoch 7701: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 09:06:25,244 EPOCH 7702
2024-02-11 09:06:41,070 Epoch 7702: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 09:06:41,070 EPOCH 7703
2024-02-11 09:06:56,911 Epoch 7703: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 09:06:56,912 EPOCH 7704
2024-02-11 09:07:13,040 Epoch 7704: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 09:07:13,040 EPOCH 7705
2024-02-11 09:07:29,260 Epoch 7705: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 09:07:29,260 EPOCH 7706
2024-02-11 09:07:45,446 Epoch 7706: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 09:07:45,448 EPOCH 7707
2024-02-11 09:08:01,481 Epoch 7707: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-11 09:08:01,482 EPOCH 7708
2024-02-11 09:08:17,589 Epoch 7708: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 09:08:17,590 EPOCH 7709
2024-02-11 09:08:33,802 Epoch 7709: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 09:08:33,803 EPOCH 7710
2024-02-11 09:08:49,899 Epoch 7710: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 09:08:49,899 EPOCH 7711
2024-02-11 09:09:05,922 Epoch 7711: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 09:09:05,922 EPOCH 7712
2024-02-11 09:09:06,620 [Epoch: 7712 Step: 00069400] Batch Recognition Loss:   0.000279 => Gls Tokens per Sec:     1839 || Batch Translation Loss:   0.019247 => Txt Tokens per Sec:     5628 || Lr: 0.000050
2024-02-11 09:09:22,107 Epoch 7712: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 09:09:22,107 EPOCH 7713
2024-02-11 09:09:38,256 Epoch 7713: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 09:09:38,257 EPOCH 7714
2024-02-11 09:09:54,230 Epoch 7714: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 09:09:54,230 EPOCH 7715
2024-02-11 09:10:10,289 Epoch 7715: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 09:10:10,290 EPOCH 7716
2024-02-11 09:10:26,274 Epoch 7716: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 09:10:26,275 EPOCH 7717
2024-02-11 09:10:42,184 Epoch 7717: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 09:10:42,184 EPOCH 7718
2024-02-11 09:10:58,268 Epoch 7718: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 09:10:58,268 EPOCH 7719
2024-02-11 09:11:14,707 Epoch 7719: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 09:11:14,707 EPOCH 7720
2024-02-11 09:11:30,899 Epoch 7720: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-11 09:11:30,899 EPOCH 7721
2024-02-11 09:11:47,071 Epoch 7721: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.29 
2024-02-11 09:11:47,072 EPOCH 7722
2024-02-11 09:12:03,340 Epoch 7722: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.28 
2024-02-11 09:12:03,341 EPOCH 7723
2024-02-11 09:12:04,428 [Epoch: 7723 Step: 00069500] Batch Recognition Loss:   0.000350 => Gls Tokens per Sec:     2357 || Batch Translation Loss:   0.032604 => Txt Tokens per Sec:     6623 || Lr: 0.000050
2024-02-11 09:12:19,571 Epoch 7723: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.27 
2024-02-11 09:12:19,571 EPOCH 7724
2024-02-11 09:12:35,599 Epoch 7724: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-11 09:12:35,600 EPOCH 7725
2024-02-11 09:12:51,560 Epoch 7725: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.23 
2024-02-11 09:12:51,561 EPOCH 7726
2024-02-11 09:13:07,915 Epoch 7726: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.24 
2024-02-11 09:13:07,916 EPOCH 7727
2024-02-11 09:13:24,280 Epoch 7727: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-11 09:13:24,280 EPOCH 7728
2024-02-11 09:13:40,113 Epoch 7728: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-11 09:13:40,114 EPOCH 7729
2024-02-11 09:13:56,228 Epoch 7729: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 09:13:56,229 EPOCH 7730
2024-02-11 09:14:12,521 Epoch 7730: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 09:14:12,521 EPOCH 7731
2024-02-11 09:14:28,458 Epoch 7731: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 09:14:28,459 EPOCH 7732
2024-02-11 09:14:44,849 Epoch 7732: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.17 
2024-02-11 09:14:44,850 EPOCH 7733
2024-02-11 09:15:00,795 Epoch 7733: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 09:15:00,796 EPOCH 7734
2024-02-11 09:15:05,901 [Epoch: 7734 Step: 00069600] Batch Recognition Loss:   0.000289 => Gls Tokens per Sec:      576 || Batch Translation Loss:   0.009199 => Txt Tokens per Sec:     1408 || Lr: 0.000050
2024-02-11 09:15:17,110 Epoch 7734: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 09:15:17,111 EPOCH 7735
2024-02-11 09:15:33,066 Epoch 7735: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 09:15:33,067 EPOCH 7736
2024-02-11 09:15:48,951 Epoch 7736: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 09:15:48,952 EPOCH 7737
2024-02-11 09:16:04,942 Epoch 7737: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 09:16:04,943 EPOCH 7738
2024-02-11 09:16:21,174 Epoch 7738: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 09:16:21,175 EPOCH 7739
2024-02-11 09:16:37,346 Epoch 7739: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 09:16:37,347 EPOCH 7740
2024-02-11 09:16:53,687 Epoch 7740: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 09:16:53,687 EPOCH 7741
2024-02-11 09:17:10,063 Epoch 7741: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 09:17:10,063 EPOCH 7742
2024-02-11 09:17:26,080 Epoch 7742: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 09:17:26,081 EPOCH 7743
2024-02-11 09:17:41,945 Epoch 7743: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:17:41,945 EPOCH 7744
2024-02-11 09:17:57,711 Epoch 7744: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:17:57,711 EPOCH 7745
2024-02-11 09:18:07,618 [Epoch: 7745 Step: 00069700] Batch Recognition Loss:   0.000100 => Gls Tokens per Sec:      517 || Batch Translation Loss:   0.007133 => Txt Tokens per Sec:     1518 || Lr: 0.000050
2024-02-11 09:18:13,924 Epoch 7745: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:18:13,925 EPOCH 7746
2024-02-11 09:18:29,821 Epoch 7746: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:18:29,822 EPOCH 7747
2024-02-11 09:18:45,933 Epoch 7747: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 09:18:45,933 EPOCH 7748
2024-02-11 09:19:01,956 Epoch 7748: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 09:19:01,956 EPOCH 7749
2024-02-11 09:19:18,211 Epoch 7749: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 09:19:18,212 EPOCH 7750
2024-02-11 09:19:34,451 Epoch 7750: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 09:19:34,452 EPOCH 7751
2024-02-11 09:19:50,594 Epoch 7751: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 09:19:50,595 EPOCH 7752
2024-02-11 09:20:06,833 Epoch 7752: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 09:20:06,834 EPOCH 7753
2024-02-11 09:20:23,032 Epoch 7753: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 09:20:23,033 EPOCH 7754
2024-02-11 09:20:39,185 Epoch 7754: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:20:39,186 EPOCH 7755
2024-02-11 09:20:55,200 Epoch 7755: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:20:55,200 EPOCH 7756
2024-02-11 09:20:59,921 [Epoch: 7756 Step: 00069800] Batch Recognition Loss:   0.000528 => Gls Tokens per Sec:     1356 || Batch Translation Loss:   0.011069 => Txt Tokens per Sec:     3485 || Lr: 0.000050
2024-02-11 09:21:11,399 Epoch 7756: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-11 09:21:11,400 EPOCH 7757
2024-02-11 09:21:27,341 Epoch 7757: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:21:27,341 EPOCH 7758
2024-02-11 09:21:43,218 Epoch 7758: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:21:43,219 EPOCH 7759
2024-02-11 09:21:59,008 Epoch 7759: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:21:59,009 EPOCH 7760
2024-02-11 09:22:15,110 Epoch 7760: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:22:15,110 EPOCH 7761
2024-02-11 09:22:31,027 Epoch 7761: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:22:31,028 EPOCH 7762
2024-02-11 09:22:47,156 Epoch 7762: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:22:47,156 EPOCH 7763
2024-02-11 09:23:03,123 Epoch 7763: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:23:03,124 EPOCH 7764
2024-02-11 09:23:19,139 Epoch 7764: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:23:19,139 EPOCH 7765
2024-02-11 09:23:35,292 Epoch 7765: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:23:35,293 EPOCH 7766
2024-02-11 09:23:51,436 Epoch 7766: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:23:51,437 EPOCH 7767
2024-02-11 09:24:04,663 [Epoch: 7767 Step: 00069900] Batch Recognition Loss:   0.006316 => Gls Tokens per Sec:      513 || Batch Translation Loss:   0.006897 => Txt Tokens per Sec:     1446 || Lr: 0.000050
2024-02-11 09:24:08,684 Epoch 7767: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-11 09:24:08,685 EPOCH 7768
2024-02-11 09:24:24,757 Epoch 7768: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 09:24:24,757 EPOCH 7769
2024-02-11 09:24:40,982 Epoch 7769: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-11 09:24:40,982 EPOCH 7770
2024-02-11 09:24:56,896 Epoch 7770: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 09:24:56,897 EPOCH 7771
2024-02-11 09:25:12,694 Epoch 7771: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 09:25:12,694 EPOCH 7772
2024-02-11 09:25:28,633 Epoch 7772: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 09:25:28,634 EPOCH 7773
2024-02-11 09:25:44,810 Epoch 7773: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 09:25:44,811 EPOCH 7774
2024-02-11 09:26:01,174 Epoch 7774: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 09:26:01,174 EPOCH 7775
2024-02-11 09:26:17,136 Epoch 7775: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 09:26:17,137 EPOCH 7776
2024-02-11 09:26:33,170 Epoch 7776: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.66 
2024-02-11 09:26:33,171 EPOCH 7777
2024-02-11 09:26:49,217 Epoch 7777: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.77 
2024-02-11 09:26:49,218 EPOCH 7778
2024-02-11 09:27:04,653 [Epoch: 7778 Step: 00070000] Batch Recognition Loss:   0.001161 => Gls Tokens per Sec:      522 || Batch Translation Loss:   0.214743 => Txt Tokens per Sec:     1545 || Lr: 0.000050
2024-02-11 09:28:16,277 Validation result at epoch 7778, step    70000: duration: 71.6228s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.31660	Translation Loss: 111226.12500	PPL: 66788.12500
	Eval Metric: BLEU
	WER 2.33	(DEL: 0.00,	INS: 0.00,	SUB: 2.33)
	BLEU-4 0.32	(BLEU-1: 9.06,	BLEU-2: 2.39,	BLEU-3: 0.90,	BLEU-4: 0.32)
	CHRF 16.49	ROUGE 7.28
2024-02-11 09:28:16,279 Logging Recognition and Translation Outputs
2024-02-11 09:28:16,279 ========================================================================================================================
2024-02-11 09:28:16,280 Logging Sequence: 129_90.00
2024-02-11 09:28:16,280 	Gloss Reference :	A B+C+D+E
2024-02-11 09:28:16,280 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 09:28:16,280 	Gloss Alignment :	         
2024-02-11 09:28:16,280 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 09:28:16,282 	Text Reference  :	however because    of      the     emergency games will  now      be held    without any ******* *** *** ** ** *** spectators
2024-02-11 09:28:16,282 	Text Hypothesis :	******* australian cricket council her       paul  pogba attended a  matches in      any thrower can use it at the 2020      
2024-02-11 09:28:16,282 	Text Alignment  :	D       S          S       S       S         S     S     S        S  S       S           I       I   I   I  I  I   S         
2024-02-11 09:28:16,282 ========================================================================================================================
2024-02-11 09:28:16,282 Logging Sequence: 179_378.00
2024-02-11 09:28:16,283 	Gloss Reference :	A B+C+D+E
2024-02-11 09:28:16,283 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 09:28:16,283 	Gloss Alignment :	         
2024-02-11 09:28:16,283 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 09:28:16,284 	Text Reference  :	these kids think that they are going to      the         olympics so           they've become some kind of    stars
2024-02-11 09:28:16,285 	Text Hypothesis :	***** **** ***** **** **** the deaf  cricket association idca     participated in      1st    day  her  match fee  
2024-02-11 09:28:16,285 	Text Alignment  :	D     D    D     D    D    S   S     S       S           S        S            S       S      S    S    S     S    
2024-02-11 09:28:16,285 ========================================================================================================================
2024-02-11 09:28:16,285 Logging Sequence: 162_20.00
2024-02-11 09:28:16,285 	Gloss Reference :	A B+C+D+E
2024-02-11 09:28:16,285 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 09:28:16,285 	Gloss Alignment :	         
2024-02-11 09:28:16,286 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 09:28:16,288 	Text Reference  :	**** not   only this    but they *** ****** ****** *** ** ** blamed mohammed shami's religion as   the reason for    india's loss    
2024-02-11 09:28:16,288 	Text Hypothesis :	they issue such threats as  they are lonely ignore all of it at     the      focus   on       keep the team   strong its     comments
2024-02-11 09:28:16,288 	Text Alignment  :	I    S     S    S       S        I   I      I      I   I  I  S      S        S       S        S        S      S      S       S       
2024-02-11 09:28:16,288 ========================================================================================================================
2024-02-11 09:28:16,288 Logging Sequence: 106_169.00
2024-02-11 09:28:16,288 	Gloss Reference :	A B+C+D+E
2024-02-11 09:28:16,288 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 09:28:16,289 	Gloss Alignment :	         
2024-02-11 09:28:16,289 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 09:28:16,290 	Text Reference  :	prime minister narendra modi also expressed his    happiness while congratulating the team on    twitter
2024-02-11 09:28:16,290 	Text Hypothesis :	***** ******** do       you  know amrapali  really amazed    by    people         of  by   third time   
2024-02-11 09:28:16,290 	Text Alignment  :	D     D        S        S    S    S         S      S         S     S              S   S    S     S      
2024-02-11 09:28:16,290 ========================================================================================================================
2024-02-11 09:28:16,291 Logging Sequence: 65_77.00
2024-02-11 09:28:16,291 	Gloss Reference :	A B+C+D+E
2024-02-11 09:28:16,291 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 09:28:16,291 	Gloss Alignment :	         
2024-02-11 09:28:16,291 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 09:28:16,292 	Text Reference  :	*** ******* **** indian team    travelling included 16        players
2024-02-11 09:28:16,292 	Text Hypothesis :	now tickets into a      tickets on         12th     september 2022   
2024-02-11 09:28:16,292 	Text Alignment  :	I   I       I    S      S       S          S        S         S      
2024-02-11 09:28:16,292 ========================================================================================================================
2024-02-11 09:28:17,213 Epoch 7778: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.26 
2024-02-11 09:28:17,214 EPOCH 7779
2024-02-11 09:28:33,925 Epoch 7779: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.25 
2024-02-11 09:28:33,925 EPOCH 7780
2024-02-11 09:28:49,876 Epoch 7780: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.73 
2024-02-11 09:28:49,876 EPOCH 7781
2024-02-11 09:29:05,874 Epoch 7781: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.44 
2024-02-11 09:29:05,875 EPOCH 7782
2024-02-11 09:29:21,957 Epoch 7782: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-11 09:29:21,957 EPOCH 7783
2024-02-11 09:29:38,068 Epoch 7783: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-11 09:29:38,068 EPOCH 7784
2024-02-11 09:29:54,273 Epoch 7784: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-11 09:29:54,273 EPOCH 7785
2024-02-11 09:30:10,263 Epoch 7785: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.20 
2024-02-11 09:30:10,263 EPOCH 7786
2024-02-11 09:30:26,214 Epoch 7786: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 09:30:26,215 EPOCH 7787
2024-02-11 09:30:42,078 Epoch 7787: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 09:30:42,078 EPOCH 7788
2024-02-11 09:30:57,888 Epoch 7788: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 09:30:57,889 EPOCH 7789
2024-02-11 09:31:13,673 [Epoch: 7789 Step: 00070100] Batch Recognition Loss:   0.000488 => Gls Tokens per Sec:      592 || Batch Translation Loss:   0.009243 => Txt Tokens per Sec:     1639 || Lr: 0.000050
2024-02-11 09:31:14,134 Epoch 7789: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 09:31:14,134 EPOCH 7790
2024-02-11 09:31:30,283 Epoch 7790: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 09:31:30,284 EPOCH 7791
2024-02-11 09:31:46,300 Epoch 7791: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 09:31:46,301 EPOCH 7792
2024-02-11 09:32:02,448 Epoch 7792: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-11 09:32:02,448 EPOCH 7793
2024-02-11 09:32:18,262 Epoch 7793: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 09:32:18,263 EPOCH 7794
2024-02-11 09:32:34,216 Epoch 7794: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-11 09:32:34,217 EPOCH 7795
2024-02-11 09:32:49,997 Epoch 7795: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 09:32:49,997 EPOCH 7796
2024-02-11 09:33:06,025 Epoch 7796: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 09:33:06,025 EPOCH 7797
2024-02-11 09:33:22,219 Epoch 7797: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:33:22,219 EPOCH 7798
2024-02-11 09:33:37,824 Epoch 7798: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:33:37,825 EPOCH 7799
2024-02-11 09:33:54,147 Epoch 7799: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:33:54,148 EPOCH 7800
2024-02-11 09:34:10,090 [Epoch: 7800 Step: 00070200] Batch Recognition Loss:   0.000645 => Gls Tokens per Sec:      666 || Batch Translation Loss:   0.016692 => Txt Tokens per Sec:     1843 || Lr: 0.000050
2024-02-11 09:34:10,090 Epoch 7800: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:34:10,090 EPOCH 7801
2024-02-11 09:34:26,483 Epoch 7801: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:34:26,484 EPOCH 7802
2024-02-11 09:34:43,471 Epoch 7802: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:34:43,472 EPOCH 7803
2024-02-11 09:34:59,768 Epoch 7803: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:34:59,769 EPOCH 7804
2024-02-11 09:35:15,803 Epoch 7804: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:35:15,804 EPOCH 7805
2024-02-11 09:35:31,785 Epoch 7805: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:35:31,786 EPOCH 7806
2024-02-11 09:35:47,991 Epoch 7806: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:35:47,991 EPOCH 7807
2024-02-11 09:36:04,071 Epoch 7807: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:36:04,072 EPOCH 7808
2024-02-11 09:36:20,106 Epoch 7808: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:36:20,107 EPOCH 7809
2024-02-11 09:36:36,104 Epoch 7809: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:36:36,105 EPOCH 7810
2024-02-11 09:36:52,146 Epoch 7810: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:36:52,147 EPOCH 7811
2024-02-11 09:37:08,101 Epoch 7811: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:37:08,101 EPOCH 7812
2024-02-11 09:37:11,237 [Epoch: 7812 Step: 00070300] Batch Recognition Loss:   0.003202 => Gls Tokens per Sec:      409 || Batch Translation Loss:   0.011540 => Txt Tokens per Sec:     1299 || Lr: 0.000050
2024-02-11 09:37:24,054 Epoch 7812: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-11 09:37:24,055 EPOCH 7813
2024-02-11 09:37:40,160 Epoch 7813: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:37:40,161 EPOCH 7814
2024-02-11 09:37:56,104 Epoch 7814: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:37:56,104 EPOCH 7815
2024-02-11 09:38:12,108 Epoch 7815: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:38:12,108 EPOCH 7816
2024-02-11 09:38:28,474 Epoch 7816: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:38:28,474 EPOCH 7817
2024-02-11 09:38:44,347 Epoch 7817: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 09:38:44,348 EPOCH 7818
2024-02-11 09:39:00,757 Epoch 7818: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:39:00,757 EPOCH 7819
2024-02-11 09:39:16,854 Epoch 7819: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 09:39:16,854 EPOCH 7820
2024-02-11 09:39:32,689 Epoch 7820: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 09:39:32,690 EPOCH 7821
2024-02-11 09:39:48,917 Epoch 7821: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 09:39:48,918 EPOCH 7822
2024-02-11 09:40:05,289 Epoch 7822: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 09:40:05,290 EPOCH 7823
2024-02-11 09:40:11,549 [Epoch: 7823 Step: 00070400] Batch Recognition Loss:   0.000374 => Gls Tokens per Sec:      409 || Batch Translation Loss:   0.007713 => Txt Tokens per Sec:     1227 || Lr: 0.000050
2024-02-11 09:40:21,560 Epoch 7823: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:40:21,561 EPOCH 7824
2024-02-11 09:40:37,959 Epoch 7824: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:40:37,960 EPOCH 7825
2024-02-11 09:40:54,181 Epoch 7825: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 09:40:54,181 EPOCH 7826
2024-02-11 09:41:10,537 Epoch 7826: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 09:41:10,538 EPOCH 7827
2024-02-11 09:41:26,711 Epoch 7827: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:41:26,712 EPOCH 7828
2024-02-11 09:41:43,010 Epoch 7828: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:41:43,011 EPOCH 7829
2024-02-11 09:41:59,399 Epoch 7829: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:41:59,399 EPOCH 7830
2024-02-11 09:42:15,520 Epoch 7830: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:42:15,521 EPOCH 7831
2024-02-11 09:42:31,732 Epoch 7831: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 09:42:31,732 EPOCH 7832
2024-02-11 09:42:48,069 Epoch 7832: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 09:42:48,070 EPOCH 7833
2024-02-11 09:43:04,319 Epoch 7833: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:43:04,320 EPOCH 7834
2024-02-11 09:43:05,676 [Epoch: 7834 Step: 00070500] Batch Recognition Loss:   0.000125 => Gls Tokens per Sec:     2834 || Batch Translation Loss:   0.012608 => Txt Tokens per Sec:     7257 || Lr: 0.000050
2024-02-11 09:43:20,153 Epoch 7834: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 09:43:20,154 EPOCH 7835
2024-02-11 09:43:36,371 Epoch 7835: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 09:43:36,371 EPOCH 7836
2024-02-11 09:43:52,540 Epoch 7836: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.29 
2024-02-11 09:43:52,540 EPOCH 7837
2024-02-11 09:44:08,283 Epoch 7837: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.73 
2024-02-11 09:44:08,284 EPOCH 7838
2024-02-11 09:44:24,517 Epoch 7838: Total Training Recognition Loss 0.00  Total Training Translation Loss 1.11 
2024-02-11 09:44:24,517 EPOCH 7839
2024-02-11 09:44:40,651 Epoch 7839: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.68 
2024-02-11 09:44:40,651 EPOCH 7840
2024-02-11 09:44:56,806 Epoch 7840: Total Training Recognition Loss 0.01  Total Training Translation Loss 3.49 
2024-02-11 09:44:56,808 EPOCH 7841
2024-02-11 09:45:13,373 Epoch 7841: Total Training Recognition Loss 0.05  Total Training Translation Loss 2.24 
2024-02-11 09:45:13,374 EPOCH 7842
2024-02-11 09:45:29,382 Epoch 7842: Total Training Recognition Loss 0.05  Total Training Translation Loss 1.64 
2024-02-11 09:45:29,382 EPOCH 7843
2024-02-11 09:45:45,739 Epoch 7843: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.63 
2024-02-11 09:45:45,739 EPOCH 7844
2024-02-11 09:46:01,861 Epoch 7844: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-11 09:46:01,862 EPOCH 7845
2024-02-11 09:46:07,380 [Epoch: 7845 Step: 00070600] Batch Recognition Loss:   0.001313 => Gls Tokens per Sec:      765 || Batch Translation Loss:   0.014737 => Txt Tokens per Sec:     1875 || Lr: 0.000050
2024-02-11 09:46:17,940 Epoch 7845: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.27 
2024-02-11 09:46:17,941 EPOCH 7846
2024-02-11 09:46:34,083 Epoch 7846: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.24 
2024-02-11 09:46:34,084 EPOCH 7847
2024-02-11 09:46:49,836 Epoch 7847: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-11 09:46:49,836 EPOCH 7848
2024-02-11 09:47:06,328 Epoch 7848: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-11 09:47:06,329 EPOCH 7849
2024-02-11 09:47:22,415 Epoch 7849: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-11 09:47:22,416 EPOCH 7850
2024-02-11 09:47:38,784 Epoch 7850: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-11 09:47:38,784 EPOCH 7851
2024-02-11 09:47:55,017 Epoch 7851: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-11 09:47:55,018 EPOCH 7852
2024-02-11 09:48:10,918 Epoch 7852: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-11 09:48:10,919 EPOCH 7853
2024-02-11 09:48:27,038 Epoch 7853: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.14 
2024-02-11 09:48:27,039 EPOCH 7854
2024-02-11 09:48:42,908 Epoch 7854: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 09:48:42,909 EPOCH 7855
2024-02-11 09:48:58,827 Epoch 7855: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-11 09:48:58,828 EPOCH 7856
2024-02-11 09:49:04,343 [Epoch: 7856 Step: 00070700] Batch Recognition Loss:   0.000391 => Gls Tokens per Sec:     1161 || Batch Translation Loss:   0.019970 => Txt Tokens per Sec:     3275 || Lr: 0.000050
2024-02-11 09:49:15,107 Epoch 7856: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 09:49:15,108 EPOCH 7857
2024-02-11 09:49:31,472 Epoch 7857: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 09:49:31,473 EPOCH 7858
2024-02-11 09:49:47,460 Epoch 7858: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 09:49:47,460 EPOCH 7859
2024-02-11 09:50:03,343 Epoch 7859: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-11 09:50:03,343 EPOCH 7860
2024-02-11 09:50:19,377 Epoch 7860: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 09:50:19,378 EPOCH 7861
2024-02-11 09:50:35,506 Epoch 7861: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 09:50:35,507 EPOCH 7862
2024-02-11 09:50:51,766 Epoch 7862: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 09:50:51,767 EPOCH 7863
2024-02-11 09:51:07,756 Epoch 7863: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:51:07,757 EPOCH 7864
2024-02-11 09:51:23,880 Epoch 7864: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:51:23,880 EPOCH 7865
2024-02-11 09:51:40,266 Epoch 7865: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:51:40,266 EPOCH 7866
2024-02-11 09:51:56,272 Epoch 7866: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:51:56,272 EPOCH 7867
2024-02-11 09:52:08,667 [Epoch: 7867 Step: 00070800] Batch Recognition Loss:   0.000179 => Gls Tokens per Sec:      547 || Batch Translation Loss:   0.007515 => Txt Tokens per Sec:     1475 || Lr: 0.000050
2024-02-11 09:52:12,895 Epoch 7867: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:52:12,896 EPOCH 7868
2024-02-11 09:52:29,101 Epoch 7868: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:52:29,102 EPOCH 7869
2024-02-11 09:52:45,133 Epoch 7869: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:52:45,133 EPOCH 7870
2024-02-11 09:53:01,065 Epoch 7870: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-11 09:53:01,065 EPOCH 7871
2024-02-11 09:53:16,906 Epoch 7871: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:53:16,906 EPOCH 7872
2024-02-11 09:53:32,837 Epoch 7872: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:53:32,838 EPOCH 7873
2024-02-11 09:53:49,165 Epoch 7873: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:53:49,166 EPOCH 7874
2024-02-11 09:54:04,987 Epoch 7874: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 09:54:04,988 EPOCH 7875
2024-02-11 09:54:20,728 Epoch 7875: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:54:20,729 EPOCH 7876
2024-02-11 09:54:36,875 Epoch 7876: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:54:36,875 EPOCH 7877
2024-02-11 09:54:52,871 Epoch 7877: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:54:52,872 EPOCH 7878
2024-02-11 09:55:08,544 [Epoch: 7878 Step: 00070900] Batch Recognition Loss:   0.000227 => Gls Tokens per Sec:      514 || Batch Translation Loss:   0.017060 => Txt Tokens per Sec:     1504 || Lr: 0.000050
2024-02-11 09:55:09,331 Epoch 7878: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-11 09:55:09,331 EPOCH 7879
2024-02-11 09:55:24,948 Epoch 7879: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:55:24,949 EPOCH 7880
2024-02-11 09:55:41,268 Epoch 7880: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:55:41,269 EPOCH 7881
2024-02-11 09:55:57,229 Epoch 7881: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:55:57,229 EPOCH 7882
2024-02-11 09:56:13,469 Epoch 7882: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.09 
2024-02-11 09:56:13,469 EPOCH 7883
2024-02-11 09:56:29,586 Epoch 7883: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-11 09:56:29,587 EPOCH 7884
2024-02-11 09:56:45,644 Epoch 7884: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:56:45,645 EPOCH 7885
2024-02-11 09:57:01,750 Epoch 7885: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 09:57:01,750 EPOCH 7886
2024-02-11 09:57:17,759 Epoch 7886: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.10 
2024-02-11 09:57:17,760 EPOCH 7887
2024-02-11 09:57:33,442 Epoch 7887: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:57:33,443 EPOCH 7888
2024-02-11 09:57:49,201 Epoch 7888: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:57:49,201 EPOCH 7889
2024-02-11 09:58:00,913 [Epoch: 7889 Step: 00071000] Batch Recognition Loss:   0.000184 => Gls Tokens per Sec:      874 || Batch Translation Loss:   0.010991 => Txt Tokens per Sec:     2394 || Lr: 0.000050
2024-02-11 09:58:05,173 Epoch 7889: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:58:05,173 EPOCH 7890
2024-02-11 09:58:21,308 Epoch 7890: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 09:58:21,309 EPOCH 7891
2024-02-11 09:58:37,363 Epoch 7891: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:58:37,364 EPOCH 7892
2024-02-11 09:58:53,508 Epoch 7892: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:58:53,509 EPOCH 7893
2024-02-11 09:59:09,503 Epoch 7893: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:59:09,503 EPOCH 7894
2024-02-11 09:59:25,452 Epoch 7894: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:59:25,453 EPOCH 7895
2024-02-11 09:59:41,282 Epoch 7895: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:59:41,283 EPOCH 7896
2024-02-11 09:59:57,345 Epoch 7896: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 09:59:57,346 EPOCH 7897
2024-02-11 10:00:13,534 Epoch 7897: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:00:13,534 EPOCH 7898
2024-02-11 10:00:29,509 Epoch 7898: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:00:29,509 EPOCH 7899
2024-02-11 10:00:45,106 Epoch 7899: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:00:45,106 EPOCH 7900
2024-02-11 10:01:01,042 [Epoch: 7900 Step: 00071100] Batch Recognition Loss:   0.000309 => Gls Tokens per Sec:      666 || Batch Translation Loss:   0.004449 => Txt Tokens per Sec:     1844 || Lr: 0.000050
2024-02-11 10:01:01,043 Epoch 7900: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:01:01,043 EPOCH 7901
2024-02-11 10:01:17,131 Epoch 7901: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:01:17,132 EPOCH 7902
2024-02-11 10:01:33,193 Epoch 7902: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:01:33,194 EPOCH 7903
2024-02-11 10:01:49,121 Epoch 7903: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:01:49,122 EPOCH 7904
2024-02-11 10:02:05,231 Epoch 7904: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:02:05,232 EPOCH 7905
2024-02-11 10:02:21,075 Epoch 7905: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:02:21,076 EPOCH 7906
2024-02-11 10:02:37,016 Epoch 7906: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:02:37,017 EPOCH 7907
2024-02-11 10:02:53,388 Epoch 7907: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:02:53,388 EPOCH 7908
2024-02-11 10:03:09,427 Epoch 7908: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:03:09,428 EPOCH 7909
2024-02-11 10:03:25,582 Epoch 7909: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:03:25,583 EPOCH 7910
2024-02-11 10:03:42,196 Epoch 7910: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:03:42,196 EPOCH 7911
2024-02-11 10:03:58,059 Epoch 7911: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:03:58,060 EPOCH 7912
2024-02-11 10:04:03,926 [Epoch: 7912 Step: 00071200] Batch Recognition Loss:   0.000462 => Gls Tokens per Sec:      218 || Batch Translation Loss:   0.013285 => Txt Tokens per Sec:      752 || Lr: 0.000050
2024-02-11 10:04:14,309 Epoch 7912: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:04:14,309 EPOCH 7913
2024-02-11 10:04:30,075 Epoch 7913: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:04:30,076 EPOCH 7914
2024-02-11 10:04:46,050 Epoch 7914: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:04:46,050 EPOCH 7915
2024-02-11 10:05:02,329 Epoch 7915: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:05:02,329 EPOCH 7916
2024-02-11 10:05:18,415 Epoch 7916: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:05:18,416 EPOCH 7917
2024-02-11 10:05:34,638 Epoch 7917: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:05:34,639 EPOCH 7918
2024-02-11 10:05:50,731 Epoch 7918: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:05:50,731 EPOCH 7919
2024-02-11 10:06:06,827 Epoch 7919: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:06:06,828 EPOCH 7920
2024-02-11 10:06:22,925 Epoch 7920: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:06:22,925 EPOCH 7921
2024-02-11 10:06:38,948 Epoch 7921: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:06:38,949 EPOCH 7922
2024-02-11 10:06:55,026 Epoch 7922: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:06:55,026 EPOCH 7923
2024-02-11 10:06:59,516 [Epoch: 7923 Step: 00071300] Batch Recognition Loss:   0.000204 => Gls Tokens per Sec:      370 || Batch Translation Loss:   0.004245 => Txt Tokens per Sec:      815 || Lr: 0.000050
2024-02-11 10:07:11,090 Epoch 7923: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:07:11,091 EPOCH 7924
2024-02-11 10:07:27,374 Epoch 7924: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:07:27,374 EPOCH 7925
2024-02-11 10:07:43,329 Epoch 7925: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:07:43,330 EPOCH 7926
2024-02-11 10:07:59,507 Epoch 7926: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:07:59,508 EPOCH 7927
2024-02-11 10:08:15,558 Epoch 7927: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:08:15,559 EPOCH 7928
2024-02-11 10:08:31,593 Epoch 7928: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:08:31,594 EPOCH 7929
2024-02-11 10:08:47,946 Epoch 7929: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:08:47,946 EPOCH 7930
2024-02-11 10:09:03,898 Epoch 7930: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:09:03,899 EPOCH 7931
2024-02-11 10:09:19,979 Epoch 7931: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:09:19,980 EPOCH 7932
2024-02-11 10:09:35,698 Epoch 7932: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:09:35,699 EPOCH 7933
2024-02-11 10:09:51,732 Epoch 7933: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 10:09:51,733 EPOCH 7934
2024-02-11 10:09:53,301 [Epoch: 7934 Step: 00071400] Batch Recognition Loss:   0.000358 => Gls Tokens per Sec:     2451 || Batch Translation Loss:   0.009136 => Txt Tokens per Sec:     7087 || Lr: 0.000050
2024-02-11 10:10:07,503 Epoch 7934: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:10:07,503 EPOCH 7935
2024-02-11 10:10:23,690 Epoch 7935: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:10:23,690 EPOCH 7936
2024-02-11 10:10:39,557 Epoch 7936: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:10:39,558 EPOCH 7937
2024-02-11 10:10:55,867 Epoch 7937: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:10:55,868 EPOCH 7938
2024-02-11 10:11:11,759 Epoch 7938: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:11:11,759 EPOCH 7939
2024-02-11 10:11:27,594 Epoch 7939: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:11:27,595 EPOCH 7940
2024-02-11 10:11:43,771 Epoch 7940: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:11:43,772 EPOCH 7941
2024-02-11 10:11:59,727 Epoch 7941: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:11:59,727 EPOCH 7942
2024-02-11 10:12:15,776 Epoch 7942: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:12:15,777 EPOCH 7943
2024-02-11 10:12:31,796 Epoch 7943: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:12:31,797 EPOCH 7944
2024-02-11 10:12:48,160 Epoch 7944: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:12:48,161 EPOCH 7945
2024-02-11 10:12:52,829 [Epoch: 7945 Step: 00071500] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:     1097 || Batch Translation Loss:   0.020036 => Txt Tokens per Sec:     2998 || Lr: 0.000050
2024-02-11 10:13:04,341 Epoch 7945: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:13:04,342 EPOCH 7946
2024-02-11 10:13:20,437 Epoch 7946: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:13:20,438 EPOCH 7947
2024-02-11 10:13:36,558 Epoch 7947: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:13:36,558 EPOCH 7948
2024-02-11 10:13:52,753 Epoch 7948: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:13:52,754 EPOCH 7949
2024-02-11 10:14:09,014 Epoch 7949: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:14:09,014 EPOCH 7950
2024-02-11 10:14:24,932 Epoch 7950: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:14:24,933 EPOCH 7951
2024-02-11 10:14:40,750 Epoch 7951: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:14:40,750 EPOCH 7952
2024-02-11 10:14:56,754 Epoch 7952: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:14:56,755 EPOCH 7953
2024-02-11 10:15:12,669 Epoch 7953: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:15:12,670 EPOCH 7954
2024-02-11 10:15:28,830 Epoch 7954: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:15:28,831 EPOCH 7955
2024-02-11 10:15:44,618 Epoch 7955: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:15:44,618 EPOCH 7956
2024-02-11 10:15:50,776 [Epoch: 7956 Step: 00071600] Batch Recognition Loss:   0.000165 => Gls Tokens per Sec:      893 || Batch Translation Loss:   0.013652 => Txt Tokens per Sec:     2451 || Lr: 0.000050
2024-02-11 10:16:00,743 Epoch 7956: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:16:00,743 EPOCH 7957
2024-02-11 10:16:16,871 Epoch 7957: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 10:16:16,872 EPOCH 7958
2024-02-11 10:16:32,964 Epoch 7958: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 10:16:32,965 EPOCH 7959
2024-02-11 10:16:49,055 Epoch 7959: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 10:16:49,056 EPOCH 7960
2024-02-11 10:17:05,395 Epoch 7960: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 10:17:05,395 EPOCH 7961
2024-02-11 10:17:21,634 Epoch 7961: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 10:17:21,635 EPOCH 7962
2024-02-11 10:17:37,448 Epoch 7962: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 10:17:37,448 EPOCH 7963
2024-02-11 10:17:53,806 Epoch 7963: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 10:17:53,806 EPOCH 7964
2024-02-11 10:18:09,814 Epoch 7964: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 10:18:09,814 EPOCH 7965
2024-02-11 10:18:25,775 Epoch 7965: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 10:18:25,775 EPOCH 7966
2024-02-11 10:18:41,682 Epoch 7966: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 10:18:41,682 EPOCH 7967
2024-02-11 10:18:47,228 [Epoch: 7967 Step: 00071700] Batch Recognition Loss:   0.000119 => Gls Tokens per Sec:     1385 || Batch Translation Loss:   0.009928 => Txt Tokens per Sec:     3743 || Lr: 0.000050
2024-02-11 10:18:57,856 Epoch 7967: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:18:57,856 EPOCH 7968
2024-02-11 10:19:14,035 Epoch 7968: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:19:14,036 EPOCH 7969
2024-02-11 10:19:29,947 Epoch 7969: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:19:29,947 EPOCH 7970
2024-02-11 10:19:46,240 Epoch 7970: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:19:46,241 EPOCH 7971
2024-02-11 10:20:02,172 Epoch 7971: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:20:02,173 EPOCH 7972
2024-02-11 10:20:18,083 Epoch 7972: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:20:18,084 EPOCH 7973
2024-02-11 10:20:34,075 Epoch 7973: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:20:34,076 EPOCH 7974
2024-02-11 10:20:50,060 Epoch 7974: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:20:50,060 EPOCH 7975
2024-02-11 10:21:06,118 Epoch 7975: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:21:06,118 EPOCH 7976
2024-02-11 10:21:22,258 Epoch 7976: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:21:22,259 EPOCH 7977
2024-02-11 10:21:38,120 Epoch 7977: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:21:38,121 EPOCH 7978
2024-02-11 10:21:46,284 [Epoch: 7978 Step: 00071800] Batch Recognition Loss:   0.000502 => Gls Tokens per Sec:     1098 || Batch Translation Loss:   0.015013 => Txt Tokens per Sec:     2930 || Lr: 0.000050
2024-02-11 10:21:53,970 Epoch 7978: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:21:53,971 EPOCH 7979
2024-02-11 10:22:09,755 Epoch 7979: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:22:09,756 EPOCH 7980
2024-02-11 10:22:25,950 Epoch 7980: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:22:25,951 EPOCH 7981
2024-02-11 10:22:42,004 Epoch 7981: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:22:42,005 EPOCH 7982
2024-02-11 10:22:57,688 Epoch 7982: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:22:57,689 EPOCH 7983
2024-02-11 10:23:13,769 Epoch 7983: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:23:13,770 EPOCH 7984
2024-02-11 10:23:30,054 Epoch 7984: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:23:30,055 EPOCH 7985
2024-02-11 10:23:46,381 Epoch 7985: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:23:46,381 EPOCH 7986
2024-02-11 10:24:02,468 Epoch 7986: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:24:02,469 EPOCH 7987
2024-02-11 10:24:19,343 Epoch 7987: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:24:19,343 EPOCH 7988
2024-02-11 10:24:35,572 Epoch 7988: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:24:35,573 EPOCH 7989
2024-02-11 10:24:45,820 [Epoch: 7989 Step: 00071900] Batch Recognition Loss:   0.000146 => Gls Tokens per Sec:      912 || Batch Translation Loss:   0.013894 => Txt Tokens per Sec:     2436 || Lr: 0.000050
2024-02-11 10:24:51,556 Epoch 7989: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 10:24:51,556 EPOCH 7990
2024-02-11 10:25:07,526 Epoch 7990: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.31 
2024-02-11 10:25:07,527 EPOCH 7991
2024-02-11 10:25:23,281 Epoch 7991: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.42 
2024-02-11 10:25:23,282 EPOCH 7992
2024-02-11 10:25:39,483 Epoch 7992: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.55 
2024-02-11 10:25:39,484 EPOCH 7993
2024-02-11 10:25:55,480 Epoch 7993: Total Training Recognition Loss 0.01  Total Training Translation Loss 1.48 
2024-02-11 10:25:55,480 EPOCH 7994
2024-02-11 10:26:11,554 Epoch 7994: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.55 
2024-02-11 10:26:11,554 EPOCH 7995
2024-02-11 10:26:27,215 Epoch 7995: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.93 
2024-02-11 10:26:27,215 EPOCH 7996
2024-02-11 10:26:43,309 Epoch 7996: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.97 
2024-02-11 10:26:43,309 EPOCH 7997
2024-02-11 10:26:59,121 Epoch 7997: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.72 
2024-02-11 10:26:59,121 EPOCH 7998
2024-02-11 10:27:15,094 Epoch 7998: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-11 10:27:15,095 EPOCH 7999
2024-02-11 10:27:30,964 Epoch 7999: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-11 10:27:30,964 EPOCH 8000
2024-02-11 10:27:46,943 [Epoch: 8000 Step: 00072000] Batch Recognition Loss:   0.000707 => Gls Tokens per Sec:      665 || Batch Translation Loss:   0.013807 => Txt Tokens per Sec:     1839 || Lr: 0.000050
2024-02-11 10:28:58,751 Validation result at epoch 8000, step    72000: duration: 71.8074s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.28297	Translation Loss: 112334.33594	PPL: 74605.44531
	Eval Metric: BLEU
	WER 2.33	(DEL: 0.00,	INS: 0.00,	SUB: 2.33)
	BLEU-4 0.66	(BLEU-1: 10.10,	BLEU-2: 3.07,	BLEU-3: 1.24,	BLEU-4: 0.66)
	CHRF 16.61	ROUGE 8.63
2024-02-11 10:28:58,754 Logging Recognition and Translation Outputs
2024-02-11 10:28:58,754 ========================================================================================================================
2024-02-11 10:28:58,754 Logging Sequence: 101_92.00
2024-02-11 10:28:58,754 	Gloss Reference :	A B+C+D+E
2024-02-11 10:28:58,755 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 10:28:58,755 	Gloss Alignment :	         
2024-02-11 10:28:58,755 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 10:28:58,755 	Text Reference  :	**** **** india had    to  score 190  runs to    win       
2024-02-11 10:28:58,756 	Text Hypothesis :	that time a     scored icc was   only in   quick succession
2024-02-11 10:28:58,756 	Text Alignment  :	I    I    S     S      S   S     S    S    S     S         
2024-02-11 10:28:58,756 ========================================================================================================================
2024-02-11 10:28:58,756 Logging Sequence: 164_412.00
2024-02-11 10:28:58,756 	Gloss Reference :	A B+C+D+E
2024-02-11 10:28:58,757 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 10:28:58,757 	Gloss Alignment :	         
2024-02-11 10:28:58,757 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 10:28:58,758 	Text Reference  :	if you divide these two figures you will be shocked  to  know  that each ball's worth      is rs   50    lakhs  
2024-02-11 10:28:58,758 	Text Hypothesis :	** *** ****** these *** ******* *** **** ** rankings are based on   the  team's perfomance of also thank matches
2024-02-11 10:28:58,759 	Text Alignment  :	D  D   D            D   D       D   D    D  S        S   S     S    S    S      S          S  S    S     S      
2024-02-11 10:28:58,759 ========================================================================================================================
2024-02-11 10:28:58,759 Logging Sequence: 177_160.00
2024-02-11 10:28:58,759 	Gloss Reference :	A B+C+D+E
2024-02-11 10:28:58,759 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 10:28:58,760 	Gloss Alignment :	         
2024-02-11 10:28:58,760 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 10:28:58,761 	Text Reference  :	the police also          said that sushil had  asked  his friend to record a video
2024-02-11 10:28:58,761 	Text Hypothesis :	*** ****** unfortunately this the  teams  were broken his ****** ** ****** * help 
2024-02-11 10:28:58,761 	Text Alignment  :	D   D      S             S    S    S      S    S          D      D  D      D S    
2024-02-11 10:28:58,761 ========================================================================================================================
2024-02-11 10:28:58,761 Logging Sequence: 124_62.00
2024-02-11 10:28:58,761 	Gloss Reference :	A B+C+D+E
2024-02-11 10:28:58,762 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 10:28:58,762 	Gloss Alignment :	         
2024-02-11 10:28:58,762 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 10:28:58,763 	Text Reference  :	however dhoni has said that     he   will continue to   play   for the team  
2024-02-11 10:28:58,763 	Text Hypothesis :	******* ***** *** my   daughter were all  a        huge number of  his jersey
2024-02-11 10:28:58,763 	Text Alignment  :	D       D     D   S    S        S    S    S        S    S      S   S   S     
2024-02-11 10:28:58,763 ========================================================================================================================
2024-02-11 10:28:58,763 Logging Sequence: 71_149.00
2024-02-11 10:28:58,764 	Gloss Reference :	A B+C+D+E
2024-02-11 10:28:58,764 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 10:28:58,764 	Gloss Alignment :	         
2024-02-11 10:28:58,764 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 10:28:58,767 	Text Reference  :	his coach sanjay had   suggested his name  for  the ******** **** ** **** ** **** ** **** ** **** ** ** madhya pradesh ranji trophy team  
2024-02-11 10:28:58,767 	Text Hypothesis :	*** just  like   there was       a   doubt with the umpire's call of 2020 as late as 1206 am part of us know   what    the   family agreed
2024-02-11 10:28:58,767 	Text Alignment  :	D   S     S      S     S         S   S     S        I        I    I  I    I  I    I  I    I  I    I  I  S      S       S     S      S     
2024-02-11 10:28:58,767 ========================================================================================================================
2024-02-11 10:28:58,772 Epoch 8000: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.23 
2024-02-11 10:28:58,772 EPOCH 8001
2024-02-11 10:29:15,785 Epoch 8001: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.20 
2024-02-11 10:29:15,786 EPOCH 8002
2024-02-11 10:29:31,779 Epoch 8002: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 10:29:31,779 EPOCH 8003
2024-02-11 10:29:47,381 Epoch 8003: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.15 
2024-02-11 10:29:47,381 EPOCH 8004
2024-02-11 10:30:03,444 Epoch 8004: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 10:30:03,444 EPOCH 8005
2024-02-11 10:30:19,274 Epoch 8005: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 10:30:19,275 EPOCH 8006
2024-02-11 10:30:35,205 Epoch 8006: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.12 
2024-02-11 10:30:35,206 EPOCH 8007
2024-02-11 10:30:50,840 Epoch 8007: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 10:30:50,840 EPOCH 8008
2024-02-11 10:31:06,731 Epoch 8008: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 10:31:06,732 EPOCH 8009
2024-02-11 10:31:22,514 Epoch 8009: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 10:31:22,514 EPOCH 8010
2024-02-11 10:31:38,555 Epoch 8010: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 10:31:38,556 EPOCH 8011
2024-02-11 10:31:54,705 Epoch 8011: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 10:31:54,706 EPOCH 8012
2024-02-11 10:31:55,301 [Epoch: 8012 Step: 00072100] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:     2155 || Batch Translation Loss:   0.012614 => Txt Tokens per Sec:     6214 || Lr: 0.000050
2024-02-11 10:32:10,975 Epoch 8012: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 10:32:10,976 EPOCH 8013
2024-02-11 10:32:27,133 Epoch 8013: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 10:32:27,134 EPOCH 8014
2024-02-11 10:32:42,778 Epoch 8014: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 10:32:42,778 EPOCH 8015
2024-02-11 10:32:58,839 Epoch 8015: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 10:32:58,839 EPOCH 8016
2024-02-11 10:33:14,766 Epoch 8016: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 10:33:14,767 EPOCH 8017
2024-02-11 10:33:31,100 Epoch 8017: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 10:33:31,100 EPOCH 8018
2024-02-11 10:33:47,221 Epoch 8018: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:33:47,221 EPOCH 8019
2024-02-11 10:34:03,117 Epoch 8019: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 10:34:03,117 EPOCH 8020
2024-02-11 10:34:18,921 Epoch 8020: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:34:18,922 EPOCH 8021
2024-02-11 10:34:34,985 Epoch 8021: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:34:34,985 EPOCH 8022
2024-02-11 10:34:51,960 Epoch 8022: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:34:51,960 EPOCH 8023
2024-02-11 10:34:56,593 [Epoch: 8023 Step: 00072200] Batch Recognition Loss:   0.000479 => Gls Tokens per Sec:      358 || Batch Translation Loss:   0.005055 => Txt Tokens per Sec:      788 || Lr: 0.000050
2024-02-11 10:35:08,297 Epoch 8023: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:35:08,298 EPOCH 8024
2024-02-11 10:35:24,466 Epoch 8024: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:35:24,466 EPOCH 8025
2024-02-11 10:35:40,261 Epoch 8025: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:35:40,261 EPOCH 8026
2024-02-11 10:35:56,485 Epoch 8026: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 10:35:56,485 EPOCH 8027
2024-02-11 10:36:12,563 Epoch 8027: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:36:12,563 EPOCH 8028
2024-02-11 10:36:28,779 Epoch 8028: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:36:28,779 EPOCH 8029
2024-02-11 10:36:44,658 Epoch 8029: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:36:44,659 EPOCH 8030
2024-02-11 10:37:00,903 Epoch 8030: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:37:00,904 EPOCH 8031
2024-02-11 10:37:16,822 Epoch 8031: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:37:16,823 EPOCH 8032
2024-02-11 10:37:33,047 Epoch 8032: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.08 
2024-02-11 10:37:33,048 EPOCH 8033
2024-02-11 10:37:48,952 Epoch 8033: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:37:48,953 EPOCH 8034
2024-02-11 10:37:53,151 [Epoch: 8034 Step: 00072300] Batch Recognition Loss:   0.000457 => Gls Tokens per Sec:      915 || Batch Translation Loss:   0.016004 => Txt Tokens per Sec:     2741 || Lr: 0.000050
2024-02-11 10:38:04,957 Epoch 8034: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:38:04,957 EPOCH 8035
2024-02-11 10:38:21,449 Epoch 8035: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:38:21,449 EPOCH 8036
2024-02-11 10:38:37,413 Epoch 8036: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:38:37,414 EPOCH 8037
2024-02-11 10:38:53,370 Epoch 8037: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:38:53,370 EPOCH 8038
2024-02-11 10:39:09,630 Epoch 8038: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:39:09,631 EPOCH 8039
2024-02-11 10:39:25,723 Epoch 8039: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:39:25,723 EPOCH 8040
2024-02-11 10:39:41,884 Epoch 8040: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:39:41,884 EPOCH 8041
2024-02-11 10:39:58,028 Epoch 8041: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:39:58,028 EPOCH 8042
2024-02-11 10:40:14,214 Epoch 8042: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:40:14,215 EPOCH 8043
2024-02-11 10:40:30,372 Epoch 8043: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:40:30,373 EPOCH 8044
2024-02-11 10:40:46,265 Epoch 8044: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:40:46,265 EPOCH 8045
2024-02-11 10:40:54,728 [Epoch: 8045 Step: 00072400] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:      499 || Batch Translation Loss:   0.008439 => Txt Tokens per Sec:     1494 || Lr: 0.000050
2024-02-11 10:41:02,249 Epoch 8045: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:41:02,250 EPOCH 8046
2024-02-11 10:41:18,449 Epoch 8046: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:41:18,450 EPOCH 8047
2024-02-11 10:41:34,435 Epoch 8047: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:41:34,436 EPOCH 8048
2024-02-11 10:41:50,605 Epoch 8048: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:41:50,606 EPOCH 8049
2024-02-11 10:42:06,967 Epoch 8049: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:42:06,968 EPOCH 8050
2024-02-11 10:42:23,176 Epoch 8050: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:42:23,177 EPOCH 8051
2024-02-11 10:42:39,111 Epoch 8051: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:42:39,112 EPOCH 8052
2024-02-11 10:42:55,484 Epoch 8052: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:42:55,485 EPOCH 8053
2024-02-11 10:43:11,278 Epoch 8053: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:43:11,278 EPOCH 8054
2024-02-11 10:43:27,465 Epoch 8054: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:43:27,465 EPOCH 8055
2024-02-11 10:43:43,361 Epoch 8055: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:43:43,362 EPOCH 8056
2024-02-11 10:43:53,609 [Epoch: 8056 Step: 00072500] Batch Recognition Loss:   0.000259 => Gls Tokens per Sec:      625 || Batch Translation Loss:   0.010892 => Txt Tokens per Sec:     1651 || Lr: 0.000050
2024-02-11 10:43:59,566 Epoch 8056: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:43:59,566 EPOCH 8057
2024-02-11 10:44:15,505 Epoch 8057: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:44:15,505 EPOCH 8058
2024-02-11 10:44:31,176 Epoch 8058: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:44:31,177 EPOCH 8059
2024-02-11 10:44:47,537 Epoch 8059: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:44:47,538 EPOCH 8060
2024-02-11 10:45:03,631 Epoch 8060: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.17 
2024-02-11 10:45:03,632 EPOCH 8061
2024-02-11 10:45:19,305 Epoch 8061: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.09 
2024-02-11 10:45:19,305 EPOCH 8062
2024-02-11 10:45:35,430 Epoch 8062: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.09 
2024-02-11 10:45:35,430 EPOCH 8063
2024-02-11 10:45:51,644 Epoch 8063: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:45:51,645 EPOCH 8064
2024-02-11 10:46:07,689 Epoch 8064: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 10:46:07,689 EPOCH 8065
2024-02-11 10:46:23,708 Epoch 8065: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:46:23,708 EPOCH 8066
2024-02-11 10:46:39,867 Epoch 8066: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:46:39,868 EPOCH 8067
2024-02-11 10:46:45,024 [Epoch: 8067 Step: 00072600] Batch Recognition Loss:   0.000156 => Gls Tokens per Sec:     1490 || Batch Translation Loss:   0.007338 => Txt Tokens per Sec:     3823 || Lr: 0.000050
2024-02-11 10:46:55,871 Epoch 8067: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:46:55,872 EPOCH 8068
2024-02-11 10:47:12,184 Epoch 8068: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:47:12,184 EPOCH 8069
2024-02-11 10:47:29,836 Epoch 8069: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:47:29,837 EPOCH 8070
2024-02-11 10:47:46,543 Epoch 8070: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:47:46,544 EPOCH 8071
2024-02-11 10:48:03,037 Epoch 8071: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:48:03,038 EPOCH 8072
2024-02-11 10:48:19,193 Epoch 8072: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:48:19,194 EPOCH 8073
2024-02-11 10:48:35,555 Epoch 8073: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 10:48:35,556 EPOCH 8074
2024-02-11 10:48:51,772 Epoch 8074: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:48:51,773 EPOCH 8075
2024-02-11 10:49:07,689 Epoch 8075: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:49:07,689 EPOCH 8076
2024-02-11 10:49:23,638 Epoch 8076: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:49:23,638 EPOCH 8077
2024-02-11 10:49:39,575 Epoch 8077: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:49:39,575 EPOCH 8078
2024-02-11 10:49:49,329 [Epoch: 8078 Step: 00072700] Batch Recognition Loss:   0.000138 => Gls Tokens per Sec:      826 || Batch Translation Loss:   0.009505 => Txt Tokens per Sec:     2182 || Lr: 0.000050
2024-02-11 10:49:55,624 Epoch 8078: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:49:55,624 EPOCH 8079
2024-02-11 10:50:11,345 Epoch 8079: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:50:11,346 EPOCH 8080
2024-02-11 10:50:27,231 Epoch 8080: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:50:27,232 EPOCH 8081
2024-02-11 10:50:43,584 Epoch 8081: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:50:43,585 EPOCH 8082
2024-02-11 10:50:59,737 Epoch 8082: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:50:59,738 EPOCH 8083
2024-02-11 10:51:16,037 Epoch 8083: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:51:16,038 EPOCH 8084
2024-02-11 10:51:32,252 Epoch 8084: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 10:51:32,253 EPOCH 8085
2024-02-11 10:51:48,296 Epoch 8085: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 10:51:48,296 EPOCH 8086
2024-02-11 10:52:04,425 Epoch 8086: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 10:52:04,426 EPOCH 8087
2024-02-11 10:52:20,652 Epoch 8087: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.15 
2024-02-11 10:52:20,653 EPOCH 8088
2024-02-11 10:52:36,972 Epoch 8088: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 10:52:36,973 EPOCH 8089
2024-02-11 10:52:52,785 [Epoch: 8089 Step: 00072800] Batch Recognition Loss:   0.000114 => Gls Tokens per Sec:      591 || Batch Translation Loss:   0.029604 => Txt Tokens per Sec:     1623 || Lr: 0.000050
2024-02-11 10:52:53,319 Epoch 8089: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 10:52:53,320 EPOCH 8090
2024-02-11 10:53:09,476 Epoch 8090: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 10:53:09,477 EPOCH 8091
2024-02-11 10:53:25,522 Epoch 8091: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 10:53:25,523 EPOCH 8092
2024-02-11 10:53:42,066 Epoch 8092: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 10:53:42,067 EPOCH 8093
2024-02-11 10:53:58,251 Epoch 8093: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:53:58,251 EPOCH 8094
2024-02-11 10:54:14,221 Epoch 8094: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 10:54:14,222 EPOCH 8095
2024-02-11 10:54:30,176 Epoch 8095: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 10:54:30,176 EPOCH 8096
2024-02-11 10:54:45,915 Epoch 8096: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 10:54:45,915 EPOCH 8097
2024-02-11 10:55:02,105 Epoch 8097: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 10:55:02,105 EPOCH 8098
2024-02-11 10:55:17,761 Epoch 8098: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 10:55:17,762 EPOCH 8099
2024-02-11 10:55:33,447 Epoch 8099: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 10:55:33,448 EPOCH 8100
2024-02-11 10:55:50,133 [Epoch: 8100 Step: 00072900] Batch Recognition Loss:   0.000134 => Gls Tokens per Sec:      637 || Batch Translation Loss:   0.009293 => Txt Tokens per Sec:     1761 || Lr: 0.000050
2024-02-11 10:55:50,134 Epoch 8100: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.18 
2024-02-11 10:55:50,134 EPOCH 8101
2024-02-11 10:56:06,025 Epoch 8101: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.19 
2024-02-11 10:56:06,026 EPOCH 8102
2024-02-11 10:56:22,398 Epoch 8102: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 10:56:22,398 EPOCH 8103
2024-02-11 10:56:38,365 Epoch 8103: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 10:56:38,366 EPOCH 8104
2024-02-11 10:56:54,753 Epoch 8104: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 10:56:54,753 EPOCH 8105
2024-02-11 10:57:10,914 Epoch 8105: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 10:57:10,915 EPOCH 8106
2024-02-11 10:57:26,877 Epoch 8106: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 10:57:26,878 EPOCH 8107
2024-02-11 10:57:43,110 Epoch 8107: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 10:57:43,111 EPOCH 8108
2024-02-11 10:57:58,993 Epoch 8108: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 10:57:58,993 EPOCH 8109
2024-02-11 10:58:14,928 Epoch 8109: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 10:58:14,928 EPOCH 8110
2024-02-11 10:58:31,337 Epoch 8110: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 10:58:31,338 EPOCH 8111
2024-02-11 10:58:47,664 Epoch 8111: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 10:58:47,665 EPOCH 8112
2024-02-11 10:58:53,417 [Epoch: 8112 Step: 00073000] Batch Recognition Loss:   0.000289 => Gls Tokens per Sec:      223 || Batch Translation Loss:   0.013872 => Txt Tokens per Sec:      768 || Lr: 0.000050
2024-02-11 10:59:04,185 Epoch 8112: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 10:59:04,186 EPOCH 8113
2024-02-11 10:59:20,573 Epoch 8113: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 10:59:20,573 EPOCH 8114
2024-02-11 10:59:36,627 Epoch 8114: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 10:59:36,628 EPOCH 8115
2024-02-11 10:59:52,315 Epoch 8115: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 10:59:52,316 EPOCH 8116
2024-02-11 11:00:08,308 Epoch 8116: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 11:00:08,309 EPOCH 8117
2024-02-11 11:00:24,363 Epoch 8117: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 11:00:24,364 EPOCH 8118
2024-02-11 11:00:40,727 Epoch 8118: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 11:00:40,728 EPOCH 8119
2024-02-11 11:00:56,865 Epoch 8119: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 11:00:56,866 EPOCH 8120
2024-02-11 11:01:13,026 Epoch 8120: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.16 
2024-02-11 11:01:13,027 EPOCH 8121
2024-02-11 11:01:28,777 Epoch 8121: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.22 
2024-02-11 11:01:28,778 EPOCH 8122
2024-02-11 11:01:45,039 Epoch 8122: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.30 
2024-02-11 11:01:45,039 EPOCH 8123
2024-02-11 11:01:45,699 [Epoch: 8123 Step: 00073100] Batch Recognition Loss:   0.000225 => Gls Tokens per Sec:     3885 || Batch Translation Loss:   0.017953 => Txt Tokens per Sec:     8932 || Lr: 0.000050
2024-02-11 11:02:01,056 Epoch 8123: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.35 
2024-02-11 11:02:01,056 EPOCH 8124
2024-02-11 11:02:17,308 Epoch 8124: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.25 
2024-02-11 11:02:17,309 EPOCH 8125
2024-02-11 11:02:33,424 Epoch 8125: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.36 
2024-02-11 11:02:33,425 EPOCH 8126
2024-02-11 11:02:49,059 Epoch 8126: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.51 
2024-02-11 11:02:49,059 EPOCH 8127
2024-02-11 11:03:05,188 Epoch 8127: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.39 
2024-02-11 11:03:05,188 EPOCH 8128
2024-02-11 11:03:21,363 Epoch 8128: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.30 
2024-02-11 11:03:21,363 EPOCH 8129
2024-02-11 11:03:37,565 Epoch 8129: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.43 
2024-02-11 11:03:37,565 EPOCH 8130
2024-02-11 11:03:53,837 Epoch 8130: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.62 
2024-02-11 11:03:53,837 EPOCH 8131
2024-02-11 11:04:09,960 Epoch 8131: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.48 
2024-02-11 11:04:09,960 EPOCH 8132
2024-02-11 11:04:26,065 Epoch 8132: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.84 
2024-02-11 11:04:26,066 EPOCH 8133
2024-02-11 11:04:42,028 Epoch 8133: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.66 
2024-02-11 11:04:42,029 EPOCH 8134
2024-02-11 11:04:52,721 [Epoch: 8134 Step: 00073200] Batch Recognition Loss:   0.000347 => Gls Tokens per Sec:      275 || Batch Translation Loss:   0.284586 => Txt Tokens per Sec:      844 || Lr: 0.000050
2024-02-11 11:04:58,328 Epoch 8134: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.45 
2024-02-11 11:04:58,329 EPOCH 8135
2024-02-11 11:05:14,476 Epoch 8135: Total Training Recognition Loss 0.01  Total Training Translation Loss 2.82 
2024-02-11 11:05:14,477 EPOCH 8136
2024-02-11 11:05:30,571 Epoch 8136: Total Training Recognition Loss 0.02  Total Training Translation Loss 2.58 
2024-02-11 11:05:30,572 EPOCH 8137
2024-02-11 11:05:46,948 Epoch 8137: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.65 
2024-02-11 11:05:46,949 EPOCH 8138
2024-02-11 11:06:03,197 Epoch 8138: Total Training Recognition Loss 0.02  Total Training Translation Loss 1.03 
2024-02-11 11:06:03,198 EPOCH 8139
2024-02-11 11:06:19,311 Epoch 8139: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.59 
2024-02-11 11:06:19,312 EPOCH 8140
2024-02-11 11:06:35,706 Epoch 8140: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.33 
2024-02-11 11:06:35,707 EPOCH 8141
2024-02-11 11:06:51,893 Epoch 8141: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.21 
2024-02-11 11:06:51,894 EPOCH 8142
2024-02-11 11:07:07,746 Epoch 8142: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.18 
2024-02-11 11:07:07,747 EPOCH 8143
2024-02-11 11:07:23,668 Epoch 8143: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.16 
2024-02-11 11:07:23,668 EPOCH 8144
2024-02-11 11:07:39,846 Epoch 8144: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.14 
2024-02-11 11:07:39,846 EPOCH 8145
2024-02-11 11:07:48,366 [Epoch: 8145 Step: 00073300] Batch Recognition Loss:   0.000382 => Gls Tokens per Sec:      495 || Batch Translation Loss:   0.011196 => Txt Tokens per Sec:     1386 || Lr: 0.000050
2024-02-11 11:07:56,286 Epoch 8145: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 11:07:56,286 EPOCH 8146
2024-02-11 11:08:12,304 Epoch 8146: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-11 11:08:12,305 EPOCH 8147
2024-02-11 11:08:28,212 Epoch 8147: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.13 
2024-02-11 11:08:28,213 EPOCH 8148
2024-02-11 11:08:43,837 Epoch 8148: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 11:08:43,838 EPOCH 8149
2024-02-11 11:08:59,815 Epoch 8149: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 11:08:59,816 EPOCH 8150
2024-02-11 11:09:15,914 Epoch 8150: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 11:09:15,915 EPOCH 8151
2024-02-11 11:09:32,090 Epoch 8151: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-11 11:09:32,090 EPOCH 8152
2024-02-11 11:09:48,367 Epoch 8152: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.11 
2024-02-11 11:09:48,368 EPOCH 8153
2024-02-11 11:10:04,671 Epoch 8153: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 11:10:04,672 EPOCH 8154
2024-02-11 11:10:20,579 Epoch 8154: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 11:10:20,579 EPOCH 8155
2024-02-11 11:10:36,685 Epoch 8155: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 11:10:36,686 EPOCH 8156
2024-02-11 11:10:51,425 [Epoch: 8156 Step: 00073400] Batch Recognition Loss:   0.000422 => Gls Tokens per Sec:      373 || Batch Translation Loss:   0.014230 => Txt Tokens per Sec:     1167 || Lr: 0.000050
2024-02-11 11:10:53,017 Epoch 8156: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.13 
2024-02-11 11:10:53,017 EPOCH 8157
2024-02-11 11:11:09,208 Epoch 8157: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 11:11:09,209 EPOCH 8158
2024-02-11 11:11:25,501 Epoch 8158: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 11:11:25,502 EPOCH 8159
2024-02-11 11:11:41,721 Epoch 8159: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 11:11:41,721 EPOCH 8160
2024-02-11 11:11:58,275 Epoch 8160: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 11:11:58,275 EPOCH 8161
2024-02-11 11:12:14,413 Epoch 8161: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 11:12:14,414 EPOCH 8162
2024-02-11 11:12:30,653 Epoch 8162: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 11:12:30,654 EPOCH 8163
2024-02-11 11:12:47,048 Epoch 8163: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 11:12:47,049 EPOCH 8164
2024-02-11 11:13:02,800 Epoch 8164: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 11:13:02,800 EPOCH 8165
2024-02-11 11:13:18,810 Epoch 8165: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 11:13:18,811 EPOCH 8166
2024-02-11 11:13:35,059 Epoch 8166: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 11:13:35,059 EPOCH 8167
2024-02-11 11:13:45,766 [Epoch: 8167 Step: 00073500] Batch Recognition Loss:   0.000310 => Gls Tokens per Sec:      717 || Batch Translation Loss:   0.012186 => Txt Tokens per Sec:     1923 || Lr: 0.000050
2024-02-11 11:13:51,321 Epoch 8167: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 11:13:51,322 EPOCH 8168
2024-02-11 11:14:07,355 Epoch 8168: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 11:14:07,355 EPOCH 8169
2024-02-11 11:14:23,611 Epoch 8169: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 11:14:23,612 EPOCH 8170
2024-02-11 11:14:39,715 Epoch 8170: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 11:14:39,716 EPOCH 8171
2024-02-11 11:14:55,959 Epoch 8171: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 11:14:55,959 EPOCH 8172
2024-02-11 11:15:12,034 Epoch 8172: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 11:15:12,035 EPOCH 8173
2024-02-11 11:15:28,335 Epoch 8173: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 11:15:28,335 EPOCH 8174
2024-02-11 11:15:44,348 Epoch 8174: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 11:15:44,348 EPOCH 8175
2024-02-11 11:16:00,311 Epoch 8175: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 11:16:00,312 EPOCH 8176
2024-02-11 11:16:16,222 Epoch 8176: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 11:16:16,222 EPOCH 8177
2024-02-11 11:16:32,335 Epoch 8177: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 11:16:32,335 EPOCH 8178
2024-02-11 11:16:44,681 [Epoch: 8178 Step: 00073600] Batch Recognition Loss:   0.000674 => Gls Tokens per Sec:      653 || Batch Translation Loss:   0.004582 => Txt Tokens per Sec:     1748 || Lr: 0.000050
2024-02-11 11:16:48,590 Epoch 8178: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 11:16:48,590 EPOCH 8179
2024-02-11 11:17:04,573 Epoch 8179: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 11:17:04,573 EPOCH 8180
2024-02-11 11:17:20,474 Epoch 8180: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 11:17:20,474 EPOCH 8181
2024-02-11 11:17:36,655 Epoch 8181: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 11:17:36,655 EPOCH 8182
2024-02-11 11:17:52,433 Epoch 8182: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 11:17:52,433 EPOCH 8183
2024-02-11 11:18:08,347 Epoch 8183: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 11:18:08,348 EPOCH 8184
2024-02-11 11:18:24,654 Epoch 8184: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 11:18:24,654 EPOCH 8185
2024-02-11 11:18:40,519 Epoch 8185: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 11:18:40,519 EPOCH 8186
2024-02-11 11:18:56,405 Epoch 8186: Total Training Recognition Loss 0.01  Total Training Translation Loss 0.09 
2024-02-11 11:18:56,406 EPOCH 8187
2024-02-11 11:19:12,638 Epoch 8187: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 11:19:12,639 EPOCH 8188
2024-02-11 11:19:29,142 Epoch 8188: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 11:19:29,143 EPOCH 8189
2024-02-11 11:19:41,990 [Epoch: 8189 Step: 00073700] Batch Recognition Loss:   0.000178 => Gls Tokens per Sec:      727 || Batch Translation Loss:   0.012068 => Txt Tokens per Sec:     1967 || Lr: 0.000050
2024-02-11 11:19:45,202 Epoch 8189: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 11:19:45,202 EPOCH 8190
2024-02-11 11:20:01,143 Epoch 8190: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 11:20:01,143 EPOCH 8191
2024-02-11 11:20:17,147 Epoch 8191: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 11:20:17,148 EPOCH 8192
2024-02-11 11:20:33,319 Epoch 8192: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 11:20:33,319 EPOCH 8193
2024-02-11 11:20:49,519 Epoch 8193: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 11:20:49,519 EPOCH 8194
2024-02-11 11:21:05,435 Epoch 8194: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 11:21:05,436 EPOCH 8195
2024-02-11 11:21:21,104 Epoch 8195: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.12 
2024-02-11 11:21:21,105 EPOCH 8196
2024-02-11 11:21:37,187 Epoch 8196: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 11:21:37,187 EPOCH 8197
2024-02-11 11:21:52,709 Epoch 8197: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 11:21:52,709 EPOCH 8198
2024-02-11 11:22:08,422 Epoch 8198: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 11:22:08,423 EPOCH 8199
2024-02-11 11:22:24,842 Epoch 8199: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.10 
2024-02-11 11:22:24,842 EPOCH 8200
2024-02-11 11:22:40,603 [Epoch: 8200 Step: 00073800] Batch Recognition Loss:   0.000193 => Gls Tokens per Sec:      674 || Batch Translation Loss:   0.027584 => Txt Tokens per Sec:     1864 || Lr: 0.000050
2024-02-11 11:22:40,603 Epoch 8200: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.11 
2024-02-11 11:22:40,603 EPOCH 8201
2024-02-11 11:22:56,564 Epoch 8201: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 11:22:56,565 EPOCH 8202
2024-02-11 11:23:12,850 Epoch 8202: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 11:23:12,850 EPOCH 8203
2024-02-11 11:23:28,853 Epoch 8203: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 11:23:28,854 EPOCH 8204
2024-02-11 11:23:45,258 Epoch 8204: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 11:23:45,258 EPOCH 8205
2024-02-11 11:24:01,588 Epoch 8205: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 11:24:01,589 EPOCH 8206
2024-02-11 11:24:17,936 Epoch 8206: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 11:24:17,936 EPOCH 8207
2024-02-11 11:24:35,160 Epoch 8207: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 11:24:35,160 EPOCH 8208
2024-02-11 11:24:51,008 Epoch 8208: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 11:24:51,008 EPOCH 8209
2024-02-11 11:25:07,000 Epoch 8209: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 11:25:07,001 EPOCH 8210
2024-02-11 11:25:22,675 Epoch 8210: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 11:25:22,675 EPOCH 8211
2024-02-11 11:25:39,030 Epoch 8211: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 11:25:39,031 EPOCH 8212
2024-02-11 11:25:39,658 [Epoch: 8212 Step: 00073900] Batch Recognition Loss:   0.000236 => Gls Tokens per Sec:     2045 || Batch Translation Loss:   0.017257 => Txt Tokens per Sec:     6198 || Lr: 0.000050
2024-02-11 11:25:55,091 Epoch 8212: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 11:25:55,092 EPOCH 8213
2024-02-11 11:26:10,948 Epoch 8213: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 11:26:10,949 EPOCH 8214
2024-02-11 11:26:26,643 Epoch 8214: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 11:26:26,644 EPOCH 8215
2024-02-11 11:26:42,909 Epoch 8215: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 11:26:42,909 EPOCH 8216
2024-02-11 11:26:59,123 Epoch 8216: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 11:26:59,123 EPOCH 8217
2024-02-11 11:27:15,360 Epoch 8217: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 11:27:15,361 EPOCH 8218
2024-02-11 11:27:31,657 Epoch 8218: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 11:27:31,658 EPOCH 8219
2024-02-11 11:27:47,518 Epoch 8219: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 11:27:47,518 EPOCH 8220
2024-02-11 11:28:03,754 Epoch 8220: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 11:28:03,754 EPOCH 8221
2024-02-11 11:28:19,626 Epoch 8221: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.09 
2024-02-11 11:28:19,626 EPOCH 8222
2024-02-11 11:28:35,777 Epoch 8222: Total Training Recognition Loss 0.00  Total Training Translation Loss 0.08 
2024-02-11 11:28:35,777 EPOCH 8223
2024-02-11 11:28:39,171 [Epoch: 8223 Step: 00074000] Batch Recognition Loss:   0.000175 => Gls Tokens per Sec:      755 || Batch Translation Loss:   0.013234 => Txt Tokens per Sec:     1898 || Lr: 0.000050
2024-02-11 11:29:51,367 Validation result at epoch 8223, step    74000: duration: 72.1940s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 0.28588	Translation Loss: 112910.52344	PPL: 79024.96875
	Eval Metric: BLEU
	WER 2.40	(DEL: 0.00,	INS: 0.00,	SUB: 2.40)
	BLEU-4 0.45	(BLEU-1: 9.60,	BLEU-2: 2.72,	BLEU-3: 0.98,	BLEU-4: 0.45)
	CHRF 16.48	ROUGE 8.02
2024-02-11 11:29:51,369 Logging Recognition and Translation Outputs
2024-02-11 11:29:51,369 ========================================================================================================================
2024-02-11 11:29:51,369 Logging Sequence: 77_172.00
2024-02-11 11:29:51,369 	Gloss Reference :	A B+C+D+E
2024-02-11 11:29:51,369 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 11:29:51,369 	Gloss Alignment :	         
2024-02-11 11:29:51,370 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 11:29:51,371 	Text Reference  :	he scored 2  runs on   the    fourth ball  on   the 5th 6th jadeja scored a 6 and a boundary respectively
2024-02-11 11:29:51,371 	Text Hypothesis :	** as     we have many medals ms     dhoni with the *** *** ****** ****** * * *** * brand    ambassador  
2024-02-11 11:29:51,371 	Text Alignment  :	D  S      S  S    S    S      S      S     S        D   D   D      D      D D D   D S        S           
2024-02-11 11:29:51,372 ========================================================================================================================
2024-02-11 11:29:51,372 Logging Sequence: 173_39.00
2024-02-11 11:29:51,372 	Gloss Reference :	A B+C+D+E
2024-02-11 11:29:51,372 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 11:29:51,372 	Gloss Alignment :	         
2024-02-11 11:29:51,372 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 11:29:51,373 	Text Reference  :	********* kohli will step down as          india' captain
2024-02-11 11:29:51,373 	Text Hypothesis :	similarly there was  a    huge controversy ms     dhoni  
2024-02-11 11:29:51,373 	Text Alignment  :	I         S     S    S    S    S           S      S      
2024-02-11 11:29:51,373 ========================================================================================================================
2024-02-11 11:29:51,374 Logging Sequence: 138_224.00
2024-02-11 11:29:51,374 	Gloss Reference :	A B+C+D+E
2024-02-11 11:29:51,374 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 11:29:51,374 	Gloss Alignment :	         
2024-02-11 11:29:51,374 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 11:29:51,375 	Text Reference  :	then people wrote positive messages and  stuck them     on the ******** plastic sheets    
2024-02-11 11:29:51,375 	Text Hypothesis :	**** ****** ***** and      the      goal is    defended by the opposing team's  goalkeeper
2024-02-11 11:29:51,375 	Text Alignment  :	D    D      D     S        S        S    S     S        S      I        S       S         
2024-02-11 11:29:51,375 ========================================================================================================================
2024-02-11 11:29:51,376 Logging Sequence: 128_98.00
2024-02-11 11:29:51,376 	Gloss Reference :	A B+C+D+E
2024-02-11 11:29:51,376 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 11:29:51,376 	Gloss Alignment :	         
2024-02-11 11:29:51,376 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 11:29:51,377 	Text Reference  :	with 8 wickets and 43  balls remaining they     won the      match in such  a   short time  
2024-02-11 11:29:51,378 	Text Hypothesis :	**** * ******* and was a     huge      surprise for everyone as    he still the huge  dating
2024-02-11 11:29:51,378 	Text Alignment  :	D    D D           S   S     S         S        S   S        S     S  S     S   S     S     
2024-02-11 11:29:51,378 ========================================================================================================================
2024-02-11 11:29:51,378 Logging Sequence: 126_159.00
2024-02-11 11:29:51,378 	Gloss Reference :	A B+C+D+E
2024-02-11 11:29:51,378 	Gloss Hypothesis:	A B+C+D+E
2024-02-11 11:29:51,378 	Gloss Alignment :	         
2024-02-11 11:29:51,379 	--------------------------------------------------------------------------------------------------------------------
2024-02-11 11:29:51,379 	Text Reference  :	despite multiple challenges and   injuries you     did   not give   up      
2024-02-11 11:29:51,379 	Text Hypothesis :	******* now      athletes   could not      winning while the entire olympics
2024-02-11 11:29:51,380 	Text Alignment  :	D       S        S          S     S        S       S     S   S      S       
2024-02-11 11:29:51,380 ========================================================================================================================
2024-02-11 11:29:51,383 Training ended since there were no improvements inthe last learning rate step: 0.000050
2024-02-11 11:29:51,386 Best validation result at step    22000:   0.86 eval_metric.
2024-02-11 11:30:17,557 ------------------------------------------------------------
2024-02-11 11:30:17,558 [DEV] partition [RECOGNITION] experiment [BW]: 1
2024-02-11 11:31:29,269 finished in 71.7101s 
2024-02-11 11:31:29,272 ************************************************************
2024-02-11 11:31:29,272 [DEV] partition [RECOGNITION] results:
	New Best CTC Decode Beam Size: 1
	WER 3.88	(DEL: 0.00,	INS: 0.00,	SUB: 3.88)
2024-02-11 11:31:29,272 ************************************************************
2024-02-11 11:31:29,273 ------------------------------------------------------------
2024-02-11 11:31:29,273 [DEV] partition [RECOGNITION] experiment [BW]: 2
2024-02-11 11:32:40,532 finished in 71.2585s 
2024-02-11 11:32:40,532 ************************************************************
2024-02-11 11:32:40,532 [DEV] partition [RECOGNITION] results:
	New Best CTC Decode Beam Size: 2
	WER 3.81	(DEL: 0.00,	INS: 0.00,	SUB: 3.81)
2024-02-11 11:32:40,532 ************************************************************
2024-02-11 11:32:40,533 ------------------------------------------------------------
2024-02-11 11:32:40,533 [DEV] partition [RECOGNITION] experiment [BW]: 3
2024-02-11 11:33:51,602 finished in 71.0674s 
2024-02-11 11:33:51,603 ------------------------------------------------------------
2024-02-11 11:33:51,604 [DEV] partition [RECOGNITION] experiment [BW]: 4
2024-02-11 11:35:03,755 finished in 72.1526s 
2024-02-11 11:35:03,757 ------------------------------------------------------------
2024-02-11 11:35:03,757 [DEV] partition [RECOGNITION] experiment [BW]: 5
2024-02-11 11:36:15,058 finished in 71.2995s 
2024-02-11 11:36:15,059 ------------------------------------------------------------
2024-02-11 11:36:15,059 [DEV] partition [RECOGNITION] experiment [BW]: 6
2024-02-11 11:37:26,216 finished in 71.1568s 
2024-02-11 11:37:26,218 ------------------------------------------------------------
2024-02-11 11:37:26,218 [DEV] partition [RECOGNITION] experiment [BW]: 7
2024-02-11 11:38:37,580 finished in 71.3625s 
2024-02-11 11:38:37,582 ------------------------------------------------------------
2024-02-11 11:38:37,582 [DEV] partition [RECOGNITION] experiment [BW]: 8
2024-02-11 11:39:48,820 finished in 71.2365s 
2024-02-11 11:39:48,822 ------------------------------------------------------------
2024-02-11 11:39:48,822 [DEV] partition [RECOGNITION] experiment [BW]: 9
2024-02-11 11:41:00,115 finished in 71.2938s 
2024-02-11 11:41:00,117 ------------------------------------------------------------
2024-02-11 11:41:00,117 [DEV] partition [RECOGNITION] experiment [BW]: 10
2024-02-11 11:42:11,541 finished in 71.4237s 
2024-02-11 11:42:11,543 ============================================================
2024-02-11 11:43:22,746 [DEV] partition [Translation] results:
	New Best Translation Beam Size: 1 and Alpha: -1
	BLEU-4 0.86	(BLEU-1: 11.24,	BLEU-2: 3.76,	BLEU-3: 1.63,	BLEU-4: 0.86)
	CHRF 17.26	ROUGE 9.55
2024-02-11 11:43:22,748 ------------------------------------------------------------
2024-02-11 16:21:34,503 ************************************************************
2024-02-11 16:21:34,507 [DEV] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 2
	Best Translation Beam Size: 1 and Alpha: -1
	WER 3.81	(DEL: 0.00,	INS: 0.00,	SUB: 3.81)
	BLEU-4 0.86	(BLEU-1: 11.24,	BLEU-2: 3.76,	BLEU-3: 1.63,	BLEU-4: 0.86)
	CHRF 17.26	ROUGE 9.55
2024-02-11 16:21:34,507 ************************************************************
2024-02-11 16:22:56,919 [TEST] partition [Recognition & Translation] results:
	Best CTC Decode Beam Size: 2
	Best Translation Beam Size: 1 and Alpha: -1
	WER 3.39	(DEL: 0.00,	INS: 0.00,	SUB: 3.39)
	BLEU-4 0.62	(BLEU-1: 10.53,	BLEU-2: 3.37,	BLEU-3: 1.28,	BLEU-4: 0.62)
	CHRF 17.27	ROUGE 8.75
2024-02-11 16:22:56,921 ************************************************************
